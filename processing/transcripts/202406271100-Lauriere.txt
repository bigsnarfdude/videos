Thank you very much for the introduction, Jenyun, and thanks a lot to the organizers for the invitation. I'm really sorry I cannot be there in person. But yeah, thank you for accommodating online presentations. So I'm indeed going to present this paper, which is a joint work with Asaf Cohen and Ethan Zell from University of Michigan. And you can find the preprint online. So I'm going to briefly introduce Mini. To briefly introduce meanful games and the master equation in the context of finite state meanful games. So, part of my presentation will be about that, and then two different deep learning methods for this type of equation. So, I will start with an introduction. And this is just a single slide to summarize what I'm going to talk about. So, the motivation is to compute Nash equilibria in dynamic games with many players. So, hopefully, it's a good fit for. Hopefully, it's a good fit for this workshop. And we are going to reduce the problem to solving a partial differential equation called the master equation, which takes the following form in our context. And I would like to stress that the third input of U is a probability distribution on states between 1 and D. So we are going to consider discrete, finite number of states. But D can be large. So this is a PDE in high dimension. And for this reason, we are interested in. Mentioned, and for this reason, we are interested in using deep learning methods. And I'm going to describe two algorithms, both of them using neural networks and stochastic gradient descent, but in a different way. And in each case, I'm going to present two results that we can prove. The first result says that we can always minimize the loss. There exist a neural network that will make the loss as small as we want in each of the algorithms. But more interestingly, we have the converse result. Interestingly, we have the converse result which says that if the loss is small, which is something we can observe numerically, then the neural network is a good approximation to the PDE solution. So it's a good idea to try to minimize this loss. And then there are many other questions that we leave aside, theoretical questions that could be interesting as follow-up works. And we conclude with numerical experiments, including with large D. So, in terms of motivation, we are interested in systems with many agents. In systems with many agents in nature or in human societies or other systems that we build, but the agents are strategic, so we don't look at these systems just as dynamical systems, but really we need to compute some kind of equilibrium in the spirit of game theory. And because it's hard to solve n-player games, we pass to the limit and we solve the so-called mid-field game. There are other applications that are less maybe physical, but nevertheless important. Physical, but nevertheless important, in particular in macroeconomics. So, there is a long history of connections between game theory and macroeconomics, and including recently some macroeconomic models have been revisited in the spirit of mid-field games. So I'm not going to describe that into too many details because, in many cases, these models have continuous state space. But here, I'm going to focus on the discrete state space case because we can State space case because we can really analyze everything when the state space is continuous, it's more difficult. But that's really a motivation of this work to solve, for example, macroeconomic models in which there are aggregate shocks, meaning that the distribution evolves in a possibly stochastic way. And in such situations, it's really important to obtain the value function and the optimal control of the individual agent, not only as a function of. Not only as a function of their individual state, but also as a function of the distribution, the population distribution, because this distribution evolves in a stochastic way. And so the tool that we propose to use is a master equation for that, I mean, as a numerical tool to compute this control as a function of the population distribution. And so in a joint work with Georgio Guest, Sebastian Merkel, and Jonathan Payne, we applied similar methods to macroeconomic. We applied similar methods to macroeconomic models. Actually, we compare three different methods. But here today, I'm going to talk only about the finite state case without common noise, and we are going to present the method and do the analysis. So just a little bit of backgrounds on Minfield Gans, because I know many of you in the audience are already familiar with that, but maybe not everyone. So they were introduced by Ladri Leons and Ken Swan and Malamir on 2006. 2006, and the goal is to study Nash equilibria in large population games by letting the number of players go to infinity. Conceptually, it is simpler, but nevertheless, we rarely have an explicit solution. So what we usually do is we phrase the problem in terms of optimality or equilibrium conditions, and then we try to solve these equilibrium conditions numerically. Typically, we can look at a PD system or system or SD system or the master equation, which is what I'm going to do today. And in terms of numerical methods, I would say classical numerical methods for PDEs or SDs or more recent machine learning methods. In terms of classical methods, I would like to stress that they have many advantages. They are relatively well understood, analyzed, and they are very efficient in low dimension. But when you want to go to high dimension, when the state High dimension when the state, which for us would be the mean field state, is in high dimension, they cannot really scale, so we need to rely on other tools such as deep learning and deep neural networks. So in this case, I would say that there are really impressive success stories in high dimension. However, it's much less understood from the point of view of the theory. So there is a lot of work for mathematicians. Lot of work for mathematicians. So let me briefly describe the type of model we look at, which is a continuous time finite state mean field game. The number of states will be denoted by D. So each agent intuitively is jumping from one state to the other in continuous time. So it's really a continuous time Markov chain from the point of view of one agent. And if I were to write the n-agent problem, were to write the n agent problem, which I'm not really going to use anyway. I'm going to pass to the minfield limit, but the idea would be to describe the state of each agent by x i t for player i at time t, and then to write the empirical distribution of states. We could do state action and more general settings, but for now we'll focus on the state-only distribution. So we basically count the proportion of agents in each state x at the current time t. At the current time t. So, is really like a histogram at every time t. And given this histogram or the evolution of the histogram, each player is going to try to optimize her individual cost, but then the evolution of the population should be the result of everyone using their optimal control. So, in the end, it's an ash equilibrium problem, is a fixed point problem. I'm going to come back to this and formulate it more precisely. To this, and formulate it more precisely, but there are many applications in economics, epidemics, such as the SIR model, cybersecurity, etc. Applications of specifically finite state continuous time influence. So if we want to write the dynamics, it could take this form. So the dynamics of one representative player after we pass to the mean field limit is, let's say, X C. Let's say xt for x at time t. Let's imagine that this player uses a Markovian control alpha function of their individual state xs. And basically, this alpha is going to influence the transition rate. So, I'm not going to use a stochastic point of view later on, but it's just to mention that you can write it, it's a continuous time Markov chain, and the player has an influence on the transition rate between her current state and the next. Her current state and the next possible states. And this player tries to minimize cost, which is in expectation as the integral over time of a running cost, which is composed of two parts. One part which depends on her state and her action, and her state and the mean field. So we assume this separable structure, which is common in the literature, and we take a terminal cost which depends on the mean field. Now, this means that you Now, this means that you give me the mean field and I try to compute my optimal control, but the notion of Nash equilibrium says that the mean field should be the result of everyone using the optimal control. So, this is really a fixed point problem. But I want to stress that this fixed point problem or the solution to the mean field game depends on the initial condition. So, here we assume that everyone, including this individual player, starts with a distribution eta, which we assume to be known. eta, which we assume to be known from everyone, and it is fixed. But if you change it, then you change the equilibrium. So the goals, one of the motivation of this work is to be able to compute the Nash equilibrium for any possible initial distribution. So how do we do it when eta is fixed? Well, when eta is fixed, we can solve the control problem for a typical player by using standard tools. Typical player by using standard tools from optimal control. So, standard optimal control theory tells us that under some assumptions, the optimal alpha is given by the minimizer of the Hamiltonian evaluated as a gradient of u. So this delta u is like a discrete gradient for finite state problems. And what is u? u is a value function of the typical player, so it solves a backward ODE. In this case, it's an ODE. In this case, it's an ODE because X takes only D possible values. So ut is a vector in dimension D, and it is coupled with the evolution of the mean field mu, which is also in dimension D. Mu T is a vector in dimension D, and they depend on each another. U has a terminal condition, mu has an initial condition given by eta, and so we have a forward-backward ODE system to solve, which is fine if there is just one to solve. Fine if there is just one to solve. But if we want to solve for any possible eta, then we have infinitely many possible forward-backward ODE systems to solve, which is not really feasible. So the idea of the master equation, the intuition behind the master equation introduced by Pierre Williams is to introduce this decoupling field, capital U, function of T0, X, and eta, and it corresponds to the solution. And it corresponds to the solution u, which is a value function for a typical player evaluated at time t0 and state x when the mean field starts with eta. And then you can evaluate you at any time t, not only the initial time, by looking at the current mean field. And in that case, you recover the value function of the representative player. So the motivations to study this capital U are Are different, but for instance, it could be that you want to solve the mean field game for any possible initial distribution. So, this will be done if you compute this capital U. Maybe it's also useful if you want to, for example, study the conversion from the end player game to the minfree game. And it's also useful if you want to study the minifree game with common noise, which is one of the motivations I mentioned earlier. But for today, let's just be satisfied with this one. Satisfied with this one. I want to solve my finite state continuous time infill game, but for any possible initial distribution. So I'm going to compute this capital U, which means I'm going to be able to recover the value function of a representative player for any possible mean field. And then I can find the corresponding optimal control. What do we know about this U? So, based on previous papers, we already know that U. We already know that u is c11 for every x under suitable assumptions. I'm going to give the main assumptions later on. And when I say c11, in particular, we need to say how we define the derivative with respect to the measure. But in this case, the measure is just an element of the simplex. So we can define the derivative with respect to eta in a classical way, just like this. So I will use this notation, d superscript eta. superscript eta the subscript yz just denotes the the direction in which we take the derivative but it's a first order derivative here then we also know that you solve the master equation which is the equation i had on my first slide so you see that there is a terminal condition which is a terminal cost a time derivative there is a derivative with respect to the measure and the hamiltonian so it looks like an ajb equation but it's not because it really characters But it's not because it really characterizes a Nash equilibrium solution. And so the goal now is to solve this PD. I hope I have explained and motivated the problem. I know the PD is quite abstract, so that's why I spent some time explaining the motivations. But now that's the goal. So I'm going to present two algorithms. The first one discretizes time and then goes backward in time. And the second one tries to solve the PDE globally in time. Globally, in time. So, in terms of related works, there are several papers on finite state meanfill games. Finite state continuous time mean field games is a master equation for such problems. And in terms of numerical methods, the two methods I'm going to use are the following. The first one is a deep backward dynamic programming method, DBDP, introduced by UVL Farm and collaborators. And the deep graking method introduced by Sir Nano and Method introduced by Sirignano and Spidiopoulos. And here the idea is to adapt these methods to solve our PD and to do the analysis. In terms of assumptions, earlier I gave some results on capital U and the main assumptions we need for that are that there exists a unique minimizer for the Hamiltonian, which can be reduced to some kind of convexity on the running cost. We assume that F and G are continuously differentiable with little Are continuously differentiable with Lipschitz derivative and their Las Radiance monotone. And we make some extra assumptions on the Hamiltonian, which can be satisfied by assumptions on the model. So the Ready cost and the dynamics. For the neural networks, in each algorithm, we use a different architecture. In the first one, the neural network is a function of x and eta only, and in the second one, it's a function of t, x, and eta. And we train the neural network as a function of t. We train the neural network as a function of time as well. You can have in mind feed-for-world fully connected networks. And well, in the implementation, we may use more refined architectures. So I will start with the backward algorithm, and then I will describe the deep Galarkin algorithm. Let me first rewrite the master equation. So here it is with a terminal condition in green and In green, and this is the PD. We start by discretizing time, so from T0, T1 to Tn equals capital T. Then we are going to try to approximate U at time Ti by a neural network. So Ti is fixed and U now becomes a function of only X and eta. So that's why the neural network takes as an input only the pair X eta. The question is, how do we? The question is: How do we train it at every time step? So, for the last time step, it's pretty easy because we know exactly what U hat I, so the neural network, what it should look like. It should be like the terminal cost. We don't even really need to train a neural network for that. We know G. But when we go one step backward, we need to somehow discretize this PDE or to do something that will let us catch the solution of the PD. Catch the solution of the PDE one step backward. So, actually, what we propose to do is to minimize the following term here, where we see that we use the neural network at the next time step, evaluated at the mean field at the next time step. So what we do is we sample or we take many different x and kappa. So let's take x and kappa as an initial point. Initial point. Then, starting from this kappa, you evolve in time, you get the next mean field at the next time step, and you plug that in the neural network at the next time step. This gives you this term. You subtract the neural network as a current time step and the Hamiltonian with a delta t. So, you see, this doesn't, it's not a naive discretization of the PDE residual with a naive time discretization because we do not have this term, which involves the derivative with respect to. Which involves the derivative with respect to the measure. However, this term is somehow included in the fact that we take the neural network at the next mean field. So, if you were to divide by delta t and let delta t go to zero, you would see by the chain rule the derivative with respect to the measure appearing. So, what we do with this algorithm is we prove the following results. First, by density of neural network, you can show that there is a neural network that will be close to the solution of the mass or. To the solution of the Master equation at time ti, simply because U is smooth enough, it is defined on a compact, so density of neural networks work fine here. And then if the neural network is close to capital U, the loss will be small. So this is a kind of straightforward result, just to say it is possible to make the loss small. And if you are close to the true solution, you will make the loss small automatically. But the converse result is also true. Also true. So if you define epsilon u as the maximum error you make, the maximum loss you have over all time steps, so this is the loss measured by the algorithm. Then epsilon u gives you a bound on the distance between the neural network and the solution to the master equation, up to a term 1 over n, which depends on the time discretization. So if you want to have a neural network close to capital U, it is sufficient to take 1 over n. It is sufficient to take one over n very small and then to take epsilon u very small as well to make epsilon u very small by using stochastic gradient descent. There is n here, so basically what you do in practice is really you fix n first in order to make one over n small, and then you try to get epsilon u small enough so that n epsilon u is comparable with one over n. The proof, I'm going to skip it for the sake of the Proof, I'm going to skip it for the sake of time, but the proof really relies on comparing the master equation and the loss that we define. In practice, we use stochastic gradient descent. We replace the max by not the max over the whole simplex, but we pick some samples. And it's important to note that because it's a backward algorithm, errors accumulate in time. Actually, that's why we have the capital N here. And so the more time steps you have, And so the more time steps you have, the more accurate you need to be, particularly close to terminal time. Otherwise, errors are going to accumulate in time. Let me move on to the deep Gherkin method. Okay, maybe I can take questions at the end. So the deep Gherkin method to solve this master equation is as follows. You replace capital U by a neural network. Capital U by a neural network, and you try to make the neural network satisfy both the terminal condition and the PD. What does it mean to satisfy the PD? Here, it will mean that we want to make the residual close to zero. So we are going to take this term, square it, and integrate it over the domain and try to make it equal to zero. And instead of integrating, we are going to use Monte Carlo samples. Can we compute this quantum? Can we compute this quantity if U is a neural network? Yes, we can compute the time derivative, this derivative, and this one by automatic differentiation. Actually, only the time derivative and this one require automatic differentiation. This one is like a finite difference. So the deep lengthing algorithm for this PD takes the following form. You define the residual for a batch of samples as As the residual of the PDE evaluated at these samples, plus the terminal, the difference between the value of the neural network and the terminal cost at time t. And you try to minimize all of this. So it's a very straightforward method. In principle, you could apply it to any PDE. What is not clear is that you can analyze the method for any PD. So what can we prove in this situation? We prove in this situation? The first result says again that you can approximate the solution to the master equation now globally in time, I mean for any time, not just for the discrete time steps, you can approximate it by a neural network because U is smooth enough and again defined on a compact. So this is by density of neural networks. And if this is true, if you find a neural network doing that, then the residual defined in the The residual defining the algorithm is going to be small and controlled by the same epsilon up to a constant. More interestingly, we have the converse direction, which says that this residual controls the difference between U and U hat. So, in other words, in this DGM method, if you manage to get the residual small by using stochastic gradient descent, and if at some point you observe after some training steps, Observe after some training steps that the residual is small, then you know that you have indeed a good approximation of the master equation solution. And the proof is not so straightforward. Actually, it does not rely only on the PDE itself. In order to prove this result, we had to really go back to the interpretation of capital U as the decoupling field. So, the system of characteristics is really the forward-backward OD system I wrote at the beginning. I wrote at the beginning. So I presented it at the beginning for the sake of the introduction, but also because we actually need that in the proof of this result. And so here I think this type of result is probably not true for any PD. Maybe you can take any PD you want and you can try to apply the T-Bi Rected method, but I don't think you can always get this type of results. All right, I will skip the proof for the sake of time, but I would like to show a few numerical results. Results. So we start with a simple example which has been studied in previous papers where the running cost, capital F, which is the part of the running cost which involves the mean field, is increasing with respect to the mean field. So here you see you take eta, you evaluate it as the individual state x, and it is eta x. So the more density you have, the higher cost you pay. So this will encourage people to spread. People to spread, and that's what we usually expect from monotonicity. So, this cost has nice properties, it satisfies our assumptions. So, the Hamiltonian that you can compute satisfies the assumptions I wrote at the beginning. And on top of that, you can use any number of states D. So, this model is quite convenient because you can increase d and the model still makes sense. So, the first experiment. So, the first experiments we did were in just dimension two because it's easier to visualize. And here, what we check on this plot is that when the density, which is the x-axis, the density at state one increases, the value, capital U, increases as well, which makes sense because you pay a higher cost when the density is higher. And for the other state, it's the opposite. The value decreases when the density increases in the other state, which means the density decreases in your state. Which means the density decreases in your state. We also looked at the loss function, and we can check that the loss decreases during training. This is the loss for the last neural network. So this one is the one that is the closest to terminal time. And for this one, we know that we need to be really accurate because if this one is not accurate, then the previous one will be based on that one. So the errors are going to accumulate backward in time. Backward in time. So we look at the loss of this one, we look at the loss of the other ones. You see that the loss decreases very quickly. And part of the reason is that you can initialize the neural network at time step I minus one by using the neural network obtained at the next time steps that is already trained because you expect that the parameters should stay relatively close. However, there can be a lot of surprises just by looking at the loss. So we have Looking at the loss, so we have this result which shows that when the loss is small, the neural network should be close to the master equation solution. But I recommend to always check the shape of the solution because sometimes there are some surprises. We have two methods. So what we did, we also checked that they give the same solution in this example where we don't have any analytical solution. But what we see here is that when the partition of the time interval is fine enough, then the two solutions Then the two solutions obtained by the two algorithms are indeed very close, which is a good indication that probably they are computing the correct solution. We also looked at the relative increase computational time when we increase the dimension. So the x-axis here is dimension, and we do the ratio between the computational time for one dimension divided by the computational time in the smaller dimension, dimension minus one. Smaller dimension, dimension minus one, and if this ratio is close to one, it means that the computational time does not increase too much. And so we see here that this is indeed the case by DBME, the deep backward method, and the deep barracking method. We have another example. I think I will skip it for the sake of time, but it's an example which has four states. It models a cybersecurity situation, a very toy model. It appeared in the literature before. appeared in the literature before. And so here we solve it. Again, we don't have any analytical solution. So the way we check that we get the correct solution is by solving the master equation using each of the two methods. Then we take the capital U learned, the neural network, and we evaluate it along the equilibrium flow of mean field, of the game, and we compare it with the little U. So actually the mu and Actually, the Î¼ and the U, we can obtain them by solving the forward-backward ODE system for one fixed initial condition. So we fix an initial condition, we compute the forward-backward system solution, and we compare with capital U. And then we take another initial condition, we redo it, we compare with the master equation solution. But the master equation has been solved only once. So that's the big advantage. All right, so I will conclude here. Finite state master equation for continuous time infield game, possibly a large. Continuous time influenced game, possibly a large number of states, two algorithms: one backward after time discretization, the other one global in time. I think they all have, I mean, they both have their own advantages and drawbacks. But in each case, we prove some kind of convergence. There is still a lot to do, for example, proving convergence of SGD and maybe finer rates of convergence, et cetera. Extension, so more numerical experiments. In particular, I would like to mention this again. I would like to mention this again this work with macroeconomic models which include common noise which include continuous space and so we need to discretize the distribution or to approximate the distribution and so we tried several approximations one of them is just to discretize a state space which brings us back to what I presented today but also using a finite number of agents and also using a projection method on a set of basis functions so I will stop here and thank you very much for your attention Here and thank you very much for your attention. Thank you, Matthew. I think we have time to take one quick question. Hi, Matthew. I would like to ask something about the convergence result you had. So there's like a coefficient when you prove the error. So I actually wonder a little bit about what that A little bit about what that constant, like I mean, the ratio would depend on. You mean this C, for example? Yes. Okay, yeah, so this C depends only on the data of the problem, by which I mean the running cost, terminal cost, and the dynamics. And that's it. And it depends on the dimension, D, but that's it. And capital T. Okay. I see. Well. I think. Yes. Hi, Matthew. A quick question related to page like 20. Is there a theorem about the estimate of the in this case, do we have some fire estimate or knowledge about the level comparison between and if silo here? Okay, yeah, thank you. So here, and so let me just recall for everyone that N is Let me just recall for everyone that n is the number of time steps and epsilon is the value of the loss, right? I mean, it is the max over all time steps of the loss at each time step. So, okay, in principle, the loss can be made as small as you want. By density of neural networks, I think you can make the loss as small, as close to zero as you want. There is a neural network somewhere that makes the loss as small as you want. And so, if you want to. And so if you want to have this smaller than this of the order of one over n, I think you should try to make your loss of the order of one over n squared. But it's a rule of thumb somehow. No further questions, but I thank Matthew for a very nice part. Thank you very much.