Right. Thank you very much. Thanks a lot to all the organizers for organizing this event. This place is amazing, every day more amazed. And of course, especially thanks a lot for inviting me here. First of all, I should warn slash apologize with the people who were in, I guess with the Mechmaster people who were in fields in September because this talk is. This talk is largely the same talk that they heard at that time, and to the people who were at Fields in 2022, because for them too, there will be quite some deja vuz. So, this is a work that has been carrying out together with Salma, and despite Dotmoon being my current affiliation, I should mention that all that I will present today was carried out entirely in constant. Was carried out entirely in constants. So here's the preprint from which this material is taken. And yeah, let's start. So what I'm going to talk about today is indeed Han groups and I'm going to study their automorphisms. So I do not do exponential fields and for what this talk is concerned, technically I don't even do fields. Technically, I don't even do fields. But these hand groups are tightly related to field formal power series, which, as we have already experienced often in these first two days, are tightly related objects. So at least in that sense, we can fit this material into this context. So what is it that I'm talking about? So Pan groups are defined as Are defined as in a similar way as fields of neuralized public series are. So what I do is we take a chain gamma, chain is totally open set, and over this chain gamma we take a family of ordered abelian groups. Sorry, just the family of abelian groups and not taking orders just yet indexed by gamma, so we call this an ordered system, and then we take elements, sequences in the Cutesian product. In the Coutesian product, and we use this power series-like notation here to just describe one of these sequences. And this bold phase 1 gamma is just to be considered a placeholder there. You can see this as the characteristic function on the group, on the gamma group, and then right. So, the support of such an element is the set of all the gammas. All the gammas that have a corresponding non-zero coefficient. And the first thing that we defined is we define this bold H of the A gammas to just be, I will call this the Han product or the maximal Han group. And it consists of all those elements for which the support is a well-ordered set. And as a subgroup, you have the Han sum. We'll call it this code product notation, which is a subgroup that can. Coproduct notation, which is a subgroup that consists of all elements which have fact support instead. Right. Go down like this. Okay. Then we define an operation, an addition, in the way that you expect, just addition is point-wise. So we get a group in this way. And for me, a Han group is nothing but any group that is comprised between the minimal and the maximal Hang. Right? And the order. The order system that I have here, I also give it a name, it's called the skeleton problem. So we also say that G is a hand group over this skeleton. I already mentioned that we call the first one and the last one groups in this equation inequality, but maximum and meaning hand groups. Alright, so what I want to do, I want to study automorphisms, and in particular, I want to study automorphisms of these objects that preserve. Objects that preserve some property, especially preserving orderings or preserving valuations. So the valuation that you have in that is defined on any Han group G with a given skeleton, so I'm fixing the skeleton. And gamma is my ordered set, so evaluation has values in this set gammas. This is simply given by the minimum of the valuation, mapping zero to infinity. And for evaluation preserving. For evaluation preserving automorphism, I simply mean an automorphism of the group, G as a group that respects the valuation in the sense that whenever two things have the same valuation, then so they can take images. And similarly, if all the A gammas are ordered groups, then I can endow the full of the maximal Han group and hence all the subgroups with the lexicographic ordering, and then I can look at the group of automorphisms that preserve. Group of automorphisms that preserve this ordering. If the A gammas are Archimedean, then the valuation that you have in there coincides also with the natural valuation. You will see that I order automorphism automatically is also a valuation. We don't need this fact too much, actually. But it is the case. So, right, and why are these things important with the object? Important, I mean the objects are important because, as Mikael mentioned the other day, they are universal domains for various notions. So, in the case of groups, they're universal domains for Han. One can always take an order, a Han group and embed it into as a Han group between suitable Han product and Han sum. Alright. So. So, and indeed, as I was mentioning before, we can draw a parallel with Hamfield. So, here comes the deja vu. Because on the left-hand side here, what I wrote down is what I have already said so far. On the right-hand side, we have the parallel with Hanfields, namely, if we upgrade all Upgrade all the elements that we have here. So instead of just having a chain, we have a total order debilian group, and we take a field, this small k that I have that. Then we can also construct a field in the same way just by taking formal power series indexed by this element in G with coefficients little k with well-ordered support, and we can prove again that with the usual. Can prove again that with the usual operations of pointwise addition, convolution multiplication, so the usual multiplication of power series, this object forms a field, I call it the maximum Han field. And we have this connection to that if we take the Han product over this G with all other components equal to K, what we're going to get is nothing but the additive structure of Additive structure on this field, bold phase K, that I have here. And again, right, so here the counterpart for the Hansum, here there is a little bit of an asymmetry, right? Because here I can just take elements to find support, I get my group. If I only do that in the field case, what I get is not fields, so I need to take the fraction field of that to get the minimal Hung field, a little technicality. And if I take the same definition for a Hung field, If I take the same definition for a Hung field, namely a field which is comprised between the minimum and maximum Hung field, we have similar results also in terms of embedding theorems, right? So like Hang groups constitute universal domains for order groups, then also Hang fields constitute universal domains for biofields. Biofield. Okay, so what about the automorphism group? So, the way we want to look at these groups is we want to relate the group of automorphisms of our groups or our Hand fields in the case of Hand fields to the invariants that are associated with this object. In the case of Hound fields, Hound fields, we have valuation invariants, which are the things that we use to describe our automorphism groups. So we will relate the automorphism group to the automorphism groups of these objects, some automorphism groups, some homomorphism groups. And the corresponding element that we use for the groups is the skeleton, so I need to say what an automorphism of the skeleton is. And an automorphism of the skeleton, or an automorphism of the Of skeleton, or an automorphism of an ordered system like this, will consist, as we can also expect, of one automorphism, so an automorphism that I'm going to call tau, consists of an automorphism tau gamma of the chain, so it's just an order-preserving bijection of gamma to itself. And then for every gamma and gamma, then I have an isomorphism between the two components that the previous The previous automorphism tau gamma gives me right. So if tau gamma maps gamma to delta, then A gamma and A delta are isomorphic. I have this isomorphism. This isomorphism as a data, part of the data in my automorphism of the skeleton. And yeah, usual notation for the automorphism of the skeleton, and I'm also writing them with the similar notation that. With the similar notation that I use for the skeleton itself. Okay, so what do I want to study? I want to study this group. I have already mentioned it before. And what happens is that whenever I have an automorphism here, if I assume that it preserves the valuation, then it automatically induces an automorphism whatself, right? So I have my automorphism sigma of G, it preserves the valuation. Of g, it preserves the valuation. Thus, I get an automorphism of gamma just by taking any element of gamma, viewing it as a value of some element, and then mapping this element into the value of its age. And similarly, for every typo, this should be sigma gamma. I forgot to backslash there. So for every gamma, Every gamma, I get an isomorphism between A gamma and A sigma gamma just by taking an element in A gamma, considering a coefficient of the simplest power series or simplest element in G that I can take that starts at a gamma, apply sigma, and then taking the sigma of gamma coefficient of Gamma coefficient of the given size. So I always have this automorphism of the skeleton as soon as I have an automorphism of my group G. And this gives me a group homorphism, right? So I'm viewing, of course, all the automorphism groups as groups with composition of functions. Groups with compositional functions. So I get this group homomorphism, Ï†g, mapping sigma to the corresponding automorphism of the skeleton. And I'm going to call the automorphisms that are in the kernel of sigma the internal automorphisms of G. So the reason why this is called internal will become a bit more clear when I define the external. A bit more clear when I define the external. Obviously, I'm going to get a normal subgroup, and they have this additional property that internal automorphism also fix the valuation. So not only they preserve the valuation, but every element will have the same valuation as its own image. Okay, and what I want And what I want to do is then I want to find a decomposition of my altomovsing group, having this group here as one of its factors. In order for that to happen, I need that my map here, I need to require that it has a section, right? That I can pull back automorphism. Automorphisms of the skeleton. So I'm going to give this property a name. First of all, we notice that if we have any automorphism of the skeleton, what we are going to get is we get anyway an automorphism of the maximum of that group. This we get for free by simply pulling back in the way that you expect. You take an element of your group and then you apply separately. You apply separately the tau gammas to the coefficients, and then you shift by the tau capital gamma. Now, what can happen is that, what could happen is that if you take a general Han group, that is not the maximum one, this automorphism here might map the subgroup into something that is not the group itself. And if this doesn't happen, so if whenever I take this. If, whenever I take this automorphism here, I restrict it to my group G, and I stay in G, thus getting an automorphism of G, then I'm going to say that G has the canonical lifting property. Canonical lifting property really means that I get a section of my map psi G by taking this automorphism and restricting it to G. And now, if I get a second. Now if I get a section just by applying group theory, I know that I can embed my group of internal automorphism into the correlation-preserving automorphism. Then I have my map, phi g that I defined earlier. There's a nanotypal, sorry about that. And I have my section here, right? And then Here, right, and then general group theory tells me that I can split the group in the middle as a semi-direct product of the inward images of the groups inside. So the image of this section I'm going to call the external automorphisms, and it is simply isomorphic. It's an isomorphic copy of the group. Group of automorphism on skeleton. And the reason why they're called external is simply that, oh, sorry, I want to go back. I get in these automorphisms by applying automorphism of my component, right? So I'm sort of getting this automorphism from outside, from the automorphism on scale. And the result that I basically already mentioned is that. Is that if I have a group G, if this satisfies this lifting property, then I can already find an initial description of the group of valuation preservation automorphism by just splitting them into this semi-direct product of the internal ones and the external ones. And the external ones are quite clear. I mean, they I can't really go any further than that because they are going to depend on what the skeleton is, right? Going to depend on what the skeleton is. Whereas for the internal ones, there is a bit less clear what they look like, at least if we look at them in this way. So that's where there is still actually some work to do. I will say something more, of course, about that in a second. But I left some space on the left hand side. On the left-hand side, to give also the corresponding results for the field case, which works in the same way. I'm not giving the details just yet. I might say something more later. But the only thing that I want to stress here, because it will become important, is that in the corresponding result for the field case, we have an extra word appearing, which is this first. Word appearing, which is this first lifting property, and the result looks pretty much the same. I can split up the group of political-preserving automorphism of a Ham field with a lifting property as semi-direct product of internal automorphism and external automorphism, where the external automorphism now are the product of the automorphisms of my base field K and the order-preserving automorphisms of the volume. Of the value. Pulling back works in a very analogous way as in the group case. I apply to a power series, I apply an automorphism of the base field to the coefficient, and then I shift by an automorphism of the exponent. And right, so this little note here, just to mention that that result appeared in this. This paper that we published with Salma last year, and here is there also a local bibliography, so to speak, because I mentioned the field case. So there were this paper of Schielding that was the starting point where he first started automorphism groups of formal power series in one variable, then One variable, then Salma, Nicole, and Francoise also worked in the valuation-preserving automorphism of this field from slightly different angles. They were looking at de-valuation that preserve an automorphism instead of the other way around, but there were some important results that we used there too. And these other papers of Salma with Alessandro, Michael and Vincenzo. And Vincenzo giving us some important information on the strong additivity of automorphism that we also use. Okay, so this was just an excursus on the field case, but let's go back for a second to the group case because we established a result here, and now we want to know, so this result holds for a certain category. Category of Hang groups, namely those satisfying the lifting property. And now we wanted to know: do any hand groups satisfy the lifting property? And in order to check that, I'm going to define a bigger category of hand groups, namely what I'm going to call the Reynolder groups. And I'm going to do it in the following way. So I'm taking a family of subsets of my value. Value set gamma. And this family of subsets needs to satisfy this few conditions: namely, that it must consist of all ordered families, over ordered subsets, because I want to use these subsets as supports for my elements. It needs to be closed under these two single operations, under unions and subsets. So, if I have such a family, then Family, then by a theorem of Rainer, and it's not quite this, I'm quoting this theorem, but this is a theorem for Hunt fields. But if you take all the elements in the maximal Hunt group that have as support some of this subset, then what you're going to get is again a hung group. And And now the theorem is that a Reyner group, which is a Hung group that erases in this fashion, so by just taking all elements that have supports in one of this family, has the canonical lifting property if and only if it's an equivalent condition, the family F is stable under the action of this group here. This group here. Now, I don't really need this whole group here. All I need there are the automorphisms of gamma. And in fact, not even all the automorphisms of gamma, I only need the admissible. So those automorphisms of gamma that can occur as automorphisms of the skeletons. And using this theorem, you immediately get some examples of hand groups that satisfy the canonical lifting property. So the maximum. Property. So the maximal and the minimal HUD fields both satisfy the canonical property and what we call the kappa-bounded HUD fields also. So kappa-bounded are just you fix an infinite cardinal and then you take all the elements in bold phase G that have support with cardinality smaller than this given cardinal. So we do have ways to construct such examples and this is again to draw the parallel that I had before. That I had before, we can extend the notion of Reiner groups to that of Reiner field extended specialized, maybe the better word. We need to ask, in this case, a bunch of more conditions, and that is exactly more precisely what the result of Rainer was about. So, in order to prove some properties of algebraic closure of freezes. Algebraic closure of Fuesus series in characteristic P, he came up with this construction here, and together with Salmon Sebastian, we also investigated much further these conditions in order to get similar structures when you give up some of the property, right? So, in order to get random fields, random groups, random rings, and the likes. Okay. Okay, so now, like I said before, again, now this is the follow-up of the correspondence of the result that I have for Han groups in the case of Hanfield. So there too, you get the same condition on the families in order for a Hanfield to satisfy the first canonical lifting property. Yes, canonical lifting property. And as a corollary, you get that the minimum, maximum, and bounded Hunt fields all satisfy these properties. Right, so this was a first portion of the story. Now, because in this decomposition, we get as one of our factors the group of order-preserving automorphism of the value group of. Of the value group of a Ham field. And by virtue of the embedding theorem that I showed before, we can morally always consider such a value group a Han group. So also to the purpose of getting a better decomposition in the case of Hanfield, it makes sense to also look not only at valuation preserving but also at order preserving automorphisms. Order-preserving automorphisms of hand groups. So that is what we are going to focus on next. And so now we're going to assume from now on that all our A gammas in the skeleton are ordered abelian groups. And they are, I'll also take the assumption that they are Archimedes, so that I have. Assumption that they are Archimedeans, so that I have my canonical valuation also coincides with the natural one as an order group. And in general, whenever I have an order-preserving automorphism of G, sigma, the ordering is the lexicographic ordering, then this always induces an order-preserving automorphism of R. So this we know from before. For example, because it For example, because it is in particular evaluation-preserving automorphism. So we have, again, similar to what we had before: a map from the order automorphisms of G to the order automorphisms of gamma. And what we will not do in this case the same trick that we were doing before, so trying to pull back. So, trying to pull back the order of the morphism because we simply fall back in the same situation we were before, we gain nothing new. But what we want to do is instead to provide a matrix decomposition, a matrix representation for these automorphisms. Now, I'm going to start and finish, unfortunately, for the time being, with For the time being, with G being the minimal hand. So G is just a handsome. Supports are finite. And I want to describe an automorphism like this in some way as a matrix. Okay, so in order to do that, what I'm going to do is for every gamma, I'm going to take the endomorphism ring of Ring of gamma, so ring seen with pointwise addition and composition. And then for every pair alpha and beta in gamma, I'm going to call H alpha beta the homomorphism group of A beta into A alpha, right? So I'm switching this order here. Hopefully, I'm doing Here. Hopefully, I'm doing this right in order to get lower triangular matrices. If I'm doing this wrong, then the matrices will have to be half a triangular, but that's the only thing that can go. So, okay, so this is what my H alpha beta are. And so, this is a homomorphism group, right? I mean, it's a group with pointwise addition in this case. And I'm going to denote by delta the set of all. The set of all infinite matrices, gamma times gamma, so they are indexed by gamma in both directions, and where I am requiring that on the diagonal I have endomorphisms of the corresponding component, and the entry alpha beta is an element of H alpha beta. Okay? And then I need to require. And then I need to require something else. So, this third condition here, I'm going to require that my matrices in delta for every beta in B gamma and every element A in the corresponding component, we have that sigma alpha beta of A is zero for all but finitely many alphas. So I need this. Alphas. So I need this extra finiteness condition. This means that whenever I plug in something into such a matrix, I compute all those automorphisms at my element of the group, then this matrix becomes column finite. So it's an infinite matrix, but every column only contains finitely many non-zero edges. All right. So the first proposition then is then that there is Then, is then that there is a ring isomorphism between the endomorphism group of G and this set delta, this ring delta that I just defined. Delta again is a ring of matrices and the operations that I'm considering are addition and product of matrices. So, given that I have this isomorphism here, then I also have a correspondence between the automorphisms of Between the automorphisms of G. I'm not requiring anything right now, no, all the preserving, relation preserving, nothing also, just all the automorphisms. So the automorphisms of G now correspond to the invertible matrices in this set, in this ring, delta. Alright, so and the correspondence is given in this way. So when I have an element An element A in G, and I have my matrix sigma alpha beta, I simply compute the image of the automorphisms that sigma alpha beta represents, the image of A under this automorphism, by just multiplying the row vector of the coefficients by my matrix. Multiplying means that in my multiplication, what I do is That in my multiplication, what I do is I take the sigma alpha beta of A alpha for every alpha, and then I take the sum. And I can be sure by my finiteness conditions that these sums that I have here are all finite sums, so everything here makes sense. And this row vector that I get here is just the vector of coefficients of my equation. Okay, so this is the automorphism, the isomorphism that I'm. The isomorphism that I was mentioning before. And once I have this isomorphism of this rings, then I need to look at the units in that ring to characterize automorphisms, right? I need them to be invertible. So, yes. Where do you use the principle? So, you So you have two finiteness conditions here, right? Because you need also that this support here is finite, right? So you have you need the support of each of your series to be finite, and then the next finite condition is this column finite, yeah, column finite class of the matrix. Of the matrix. I will argue that there is room for improvement to that, but we need to go a step further. So, okay, so what we wanted to do is we wanted to characterize the order-preserving automorphisms in there. So, what we do is now we look at matrices in delta, and then we ask for two more conditions, namely, we take lower triangular. Namely, we take lower triangular matrices and we require that the elements in the diagonal not only are automorphism, but they are themselves order-preserving automorphisms. So the order-preservingness, I only need it on the diagonal elements. And if we now take this set here, then we see that a matrix in here. That a matrix in here induces then an order-preserving automorphism. And in particular, invertible matrices in T are order-preserving automorphisms. Okay, so we have our order-preserving automorphisms, and now we want to give a bit of a better description. So, in T, we can, this is pretty easy, you can split. Pretty easy, you can split T into a semi-direct product, this lower triangular matrices, so I can split it into a semi-direct product of the diagonal ones and of the unit triangular ones. Diagonal matrices are matrices that have ones of diagonals, or identities in this case. And what we have then is that u, this group of units of the invertible matrices in T, looks like. In T looks like this semi-direct product that I have down here, and therefore we obtain this further decomposition. What does this mean? So, I have some decorations that I did not mention before here, but I need to take them so far. So, I'm taking all those order-preserving automorphisms of G that induce That induces the identity on gamma. So the older-preserving automorphism that I induce the identity on gamma, then those I can now split as semi-direct product of my internal automorphism that I had before. Those always induce the identity of gamma, so I don't need decorations in that times the automorphism of the skeleton that are the identity of the gamma. That are the identity of gamma. So I only apply as external automorphism permutations within each component. And... Sorry, so here G is finite support or G is still finite support. G is my, is the, is the Hansard component support. And what is T? Uh D so U ones, U one are the unit triangular matrices, U T and U D as the diagonal matrices, U T. Exactly. Diagonal matrix. Right, so for finite support we have this result and now we would like to go to infinite support. Now in order to go to infinite support, this is still work in progress. So what we need to do is, first of all, we have two missing pieces. Now, when we're talking about internal automation, Now, when we're talking about internal automorphism, in the group case, we know very little of what they look like. And when we are talking about this, the order presodic automorphism, we have a better description at least of the external ones as groups of matrices, but we only have that for uh for Hansa, so when we restrict to finite small some ideas to go for. Some ideas to go for. And to do that, I want to go back for a bit to the Hanfield case, because there, given that we have two operations, or we have a field, we have additional multiplication, we have more constraints, so we have more control of the automotive, and our results are actually better. So this is here I'm just summarizing again what I said earlier about unfields. About Hamfields, right? So my Ham fields will look like this: a power series that I denote in this way. G is a order-de-Begger group, set of supports, and where the supports belong. And I am looking at a Hahn field that is comprised between the mean one and maximum one. And those are my valuation invariants. And what we prove. And what we prove is that, first of all, we have two lifting properties. The first lifting property I mentioned before, it consists of lifting, so here I'm drawing the maps in the other way, consists of lifting automorphisms that come from automorphism of the base field and automorphism of the value group. And the lift simply consists in letting these two automorphisms. In letting these two automorphisms act separately on our series. If we have the first lifting property, then we have our first decomposition here. Here we can go further in decomposing also the internal automorphism, right? Because we have a second lifting property, or we can have a second lifting property, namely we can lift homomorphisms from G into the multiplicative group of K to into. Two internal automorphisms of the Hamfield simply by multiplying every term of a given series by the corresponding image of G under this home market. And if we have these two lifting properties, then we can have this further decomposition, we can decompose the internal into what we call the one automorphisms and These ones that we're going to call G-exponentiations, it doesn't matter for now. But even more, if we now restrict to strongly linear automorphisms, then we can also say something more about the one-automorphism themselves. So, one automorphisms are automorphisms that have not only do they fix the valuation, not only do they fix the constant term, they actually fix the first coefficient. Fix the first coefficient of every series no matter what the value of that series is. And so these ones we can describe in terms of this, if we want to call them valuation invariance of G, name it value group at the Rescue field. But we can also describe the strongly linear ones, so strongly linear means that they commute with infinite sums, as a particular type of A particular type of homomorphisms that we call summable homomorphisms between from G into this other important invariant, which are the one units of the migration. So the idea then is that maybe strong additivity. Yeah, okay, I didn't want to open cancel walls, but sure. So here, now in here, that Salmon is quite right. I mean, we need to define a suitable group operation on this set here in order to achieve this isomorphism. So it's not the group operation that this group naturally ties. Right. So now see, we have these two ideas that suggest. Ideas that suggest. Now, what happens here is that we have a second lifting property, we would like to have additional lifting properties of some sort in the group case. What goes wrong is that here the second lifting property is very tightly related to the multiplication, to the fact that we have a second operation in our field. But this part here gives some hope in the sense that. Here gives some hope in the sense that if we look at these conditions that I have, where do I have them? Here. Here. I needed a double finiteness condition, right, to get matrix description of my automorphism. I needed a finiteness condition on supports, and I needed a finiteness condition on the matrix. But I hope, despite the fact that so far I Despite the fact that so far I always ran into a wall, that looking at strongly additive automorphisms in the group case might allow to at least loosen one of the two finiteness conditions. So either looking at strongly additive automorphism, then that's getting the same or similar matrix description for. Matrix description for automorphisms of hand groups, not necessarily only finite supports, or maybe allowing to loosen up the condition on the matrix. And the other idea is to look at this admissible automorphisms of the chain gamma, so automorphism that can occur as automorphism of a skeleton, in order to. In order to get something that looks like the second lifting property, so this we probably need to get a lifting property, a recursive lifting property for each component of gamma. One important thing about second lifting property was also that when you are in a group, when your value set, value group is indeed a group, it has a distinguished element. It has a neutral element. Whereas when you have this Have this ordered set here, all elements are the same, that there's no one element that stands out and that you can chase for it to get your results. And I think I am pretty much out of time, so I'll stop here. More references here. Thank you very much. Thank you, Michaela. Uh thank you, Michaela. Any questions? Yes. Uh me? Okay. Um so for the field case, do you have uh a characterization of minor fields or else which satisfies the second lifting property? No. Uh sorry. Yes, the random fields. No, and yes, the sense of radar feels always good. Okay, and other. Okay, and other, like, strong problems, right? Yes, so yeah, I have to do something about this at some point because I always want to go onto the table when I get this question. Because I always do, I don't have an example of a hum field that does not satisfy some of the quantities. So we do have some of the first one, but the second one we don't. Oh, um, did that was a. Yeah, but I can't. I can't do anything about it. I can't move it. It's just a slide that looks like that. What was that? Oh, that's our paper. I mentioned it in the other slide. It was the title slide. Perfect. Any insight on on how the nodes from the additive uh internal things look like? I mean, that that there Maybe that that those are a bit more mysterious, right? Um so for for the field case, yes. So oh wait, the non-strongly additive. Okay, that is harder. Because it's okay, strongly additive, you basically they look like plug it in in the same way that you plug in, what is it? Yeah. Uh the same way you here you plug in a coefficient basically inside every power series in strongly additive case what you can do you can plug in the same way for every g a given power series, right? And you plug in a power series that looks like one plus Like one plus infinitesimal. Well, in the summable case, right? So here there is something hidden in here. And what's hidden is this plus that I'm putting on this home. This plus means that I'm taking homomorphisms from G to 1 plus by K with the property that, so this homomorphism must be summable homomorphism. Summable homomorphism, it means that the pure condition is long to explain, but the moral is that when I take all these images of elements in G, right, I take, if that homomorphism is called some psi, then I take psi of g for g in g. These are all bunches of elements in one. Bunches of elements in 1 plus ik, right? They are power series that start with constant term 1 and go on. They have the property that if I plug these guys into the support of some hover series, so if I plug them in into something which is which gives me something summable, then the family that I get that way is again summable. That I get that way is against someone. So, this is the condition that I'm asking on this homotopy. So, basically, this element here is going to be again a sensible homosis. And if I restrict to this type of homomorphism, then I do get an isomorphism here, provided that I. Moves in here, provided that I change a bit the operation here suitably, because they also need some commutativity tricks to work. But yeah, so what the non-strongly additive internal ones look like is religion. I have any like father. Yeah. So into like further work section, you mentioned like maybe just looking at arbitrary hand general hand group, but then only at strongly additive uh automorphisms. I'm trying to describe this right now. Okay, so like a strongly additive by definition is given by sort of a matrix, right? So isn't the task to just like sort of classify those matrices which actually do give right because you can you can propose If you have a conjecture which matrices will actually give you an automatic number. Wouldn't it be like in the hard circuits? I mean that's the reasonable thing to expect. But well I mean because you so the my feeling is that what that matrix should look like, so we we have matrix that are column finite so far, right? Column finite so far, right? So, the matrix that you should get are matrices where the columns, in a suitable sense, are summable columns. Sorry, can I ask a question about this? So, are you aware? I mean, you. Sorry, I do. I'm online. Yeah, yeah, yeah, I know. I was wondering where should I look, but okay. Uh so no, th there is a general result. You can there is a general result you can get two random structures in your language, right? I mean you define the random structure or I mean assume you have some feeling then you have a some objective structure which is given by a random structure. So mere random structure is, if I understood correctly, a recognition is just an ideal of substance. You allow only supports from that idea of substance to come. Okay? You hear the uh can you hear me? Yeah, yeah. Can you hear me? Yeah, yeah. Okay. So, and you can describe the strongly linear morphism from one strong vector space coming from a Reiner structure, another strong vector space, even as a Reiner structure. In terms of matrices, okay, so I said assume you have number one with some F1 and F11. Gamma 1 with some f1 and gamma 2 with some f2. Then the the strong linear of most matrix are a unique matrices on gamma 1 times gamma 2. Then you can n RN structure. So an ideal subset of gamma 1 of gamma 2 and you can really describe it explicitly in terms of the two ideals that one and the are you aware of it can have Yeah, probably do you have a reference for it? Yes, uh it is a paper of mine. Cool. Uh I think I can send it to you fine. Uh and I'll send the after you you should read the proof to make sure that it's You should read the proof to make sure that it's cost. Okay, I have to understand it first and then, yeah, sure. But thanks. I'll look into it. Where are you located, actually? Okay. Great to say Nikala again. And uh we now get to go to dinner and come back here at 7:30.