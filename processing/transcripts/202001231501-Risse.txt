Resolution or ones for the recognition of principle over sparse graphs. This is joint work by Susanna with Susanna, Jakob and Dmitri. And the paper is the same ten, but you'll find it if you care. Good. Okay, so let us fix some notation. So I'm sure we all know what resolution is. We've heard it a few times. I'll just like fix my notation for the length of a refutation and the minimum width of a reputation. And the minimum width of a replication. Then there is this length-width trade-off by Genses and Riggerson, right? So if we have linear width lower bounds, then we get exponential size lower bounds. And basically, all lower bounds go through this method. Yeah? So are we done with the resolution? Why am I talking here? Well, there are these annoying formulas where width is just less than the square root of the number of variables. And as we know, Again, and as we know that this relation, this length-width relation is optimal, we cannot hope to improve this. So, we need other methods to prove sizable ones for a solution. And examples of these formulas are K-blink formulas, pseudo-and-generator formulas, or the weak Pigeon Wall principle. Okay, and this talk will be about the later, and we'll show strong lower bounds for the weak graph Pigeon-Wall principle, building and refining Raspberry. Uh, building and refining Raspberry Sudowitz method. And my main message of this talk is this is like a generic method to prove lower bounds. Like we can prove random CNF lower bounds, random XR lower bounds, and even in some cases like for pseudo-random generator formulas, we can prove some lower bounds. Though there is some issue with ENCODE. Good. So let us start by introducing the Figeno principle formulas. And so just the vanilla version has So just the vanilla version has each edge has a variable with intended meaning the pigeon i goes to the whole j. And then we have the pigeon axioms saying that the pigeon goes to at least one hole. And we have whole axioms saying that at most one pigeon goes to a hole. And clearly these two kind of axioms together are contradictory. And now we can add other axioms. By adding axioms like you make it harder to prove lower bounds or like the prover gets more powerful. So like one way of adding axioms is like the Like one way of adding axioms is like the functionality axiom saying that a pigeon goes through exactly one hole or at most one hole. And then you can also add onto axioms saying that every hole gets at least one pigeon. And note that like if you have all the axioms and then you get the onto function petition all principle, then you just claim that there is a perfect matching in this graph. So it will also denote it by perfect matching of the complete by protected graph. Good. Good. Are we all on the same page so far? Okay, perfect. So let's do some history. So Haacken first proved this lower bound from n plus 1 pigeons to n whole, and that it's exponential in n. And then shortly thereafter, Vass and Turin came along and showed that you can actually increase the number of pigeons up to n squared and you still get exponential length lower bound. And then there was this open problem that Was this open problem that Paul already mentioned in the introductory talk, like to prove for a mildly exponential number of pigeons a lower bound? And Raspberry around 2000 showed that even for like weakly exponentially many pigeons, we still have exponential lower bounds. Rasp what did I say? I'm so sorry. Ras. But the next problem. Raspberry, so almost. Yeah. So then Rasparov came along and improved this result and also made it stronger. And if you want to know more about pigeonhole principle, there is a survey by Rasporov about pigeonhole principle improved complexity, and there is much more to this than just what I'm showing. Okay, so far, what have we done? We saw that we can add more axioms, or we can increase the number of pigeons to make it. Like, increase the number of pigeons to make it harder to prove lower bounds. So, then there's another thing you can do. Namely, we can limit the sight of pigeons, right? We can only give it some holes that it sees. So, the idea is to replace this complete byproduct graph by a sparse good graph. And what do we mean by good? Well, intuitively, we somehow want that all small sets of pigeons have many matchings, right? Because then there's Matchings, right? Because then resolution has to rule out all these different matchings on this small set of pigeons. So for a small set of pigeons, we want many matchings. And how can we enforce this? Well, one way is expansion, like boundary expanders, for example. So we want that every small set has many unique neighbors, where unique neighbor are like all the holes that have precisely one neighbor in this small set of engines. In this small set of pigeons. So, more formally, we want to have R-delta C boundary expanders, where we require that every pigeon has a degree at most delta, and all small sets S of size at most R have at least C times the set size many unique neighbors. Good. Now we can define the Pigeonhol principle over these sparse graphs, and then we get the graph Pigeonhol principle. And for these kind of formulas, there are also no lower bounds. No lower bounds. So the first one is by Benson-Wegerson, who showed that up to n squared, many pigeons, and like decent expander, we get exponential length lower bounds. And then Rasparov showed that we also get length lower bounds as long as the left degree of the pigeons is high enough. So as long as the left degree is like polynomial in n, like n to the n. Polynomial in n, like n to the epsilon, then we get meaningful lower points. And now you see that there is a gap, right? Like, so in particular, if we have more than n square many pigeons and like polylogarithmic degree, we don't know. We don't know anything. Good. So this is like one part of why this is interesting, because we don't know. And the other thing is that this method, this is the pseudo-width method, relies Method, this is the pseudo-with method, relies on somehow like symmetry arguments and very global arguments to prove the lower bound. And somehow resolution cannot argue locally, right? And like Benson-Wiggerson only require local arguments. And you somehow want these lower bounds to only use local arguments. Yeah? So this is somehow also a motivation of why we were interested in this. And yeah. Okay, so let me state our results. Let me state our results. They are as follows. So, as long as you're like slightly, you can go up to slightly super polynomial and like logarithmic degree in m and we have a random graph where every pigeon gets delta many holes uniformly at random, then we get exponential length rollouts. Okay, so this holds like for random graphs. Now, in contrast, if you go like into weak Go into weakly exponential manipions, then we require a certain expander construction by Guru Swami, Umans, and Vadan. But we can still prove exponential length low-rounds for these kind of formulas. Only for those expanders. Will you tell us what property of these expanders you use? But what what we require, like, yeah, these are all corollaries of like the smart general theorem. And we are basically saying that we want an amazing boundary expander. Yeah. But the problem is that random expanders in this regime wouldn't be good enough. Yeah. Yes. Then then why is it stated that there exist graphs? It's not for random graphs, right? It's like only for this expander construction by this paper. Expander construction by this paper? Well, for that theorem statement, you've got to have that strong boundary expander. Yes. Okay. And if you have this many pigeons, like a random graph is not that well expanding anymore. Like, yeah. Yeah, I mean, this, like, expansion factor is like. Okay, th that's what I was asking. What is a model of the graph? We need it. That's the one. And that that's like what we need. And random graphs with this many pensions is unsatisfied. Perfect. Okay, good. Okay, good. So, yeah, so the general statement is that if we have an amazing boundary expander, then we get length lower bounds that are exponential in this R. And this is somehow the dependency that we want, and we do not want the dependency on the minimum degree. Okay? Good. So let me also tell you that this is a theorem, but there is a lot of ways to improve this. So, first off, the loss in expanding. Like the loss in expansion, right? The maximum that we could ask for in expansion is delta, is the degree, right? And if n goes to infinity, well, we require this. And this is like, we really require very good expanders. So it would be really nice to show that this also holds for one minus epsilon times delta for epsilon epsilon like that. And then there is like some stuff down here that should go away. But yeah. Good. And then we also have. And then we also have general statements for the perfect matching principle over graphs. But there we also require some conditions on the degree. And this only holds for bipartite graphs. Good? Questions about the result? Okay. Now, so what? I mean, you know, you can push it a bit too far sometimes and why should we care? Sometimes, and why should we care about these formulas? Well, let's take a step back and look at what kind of how we can prove resolution size or not. And there are very few generic methods. So one of them is definitely random restrictions, or bottleneck counting, or the width argument. Another one is interpolation. And then I don't know of anything else but pseudo-width, right? So I think it's like a shame that we have not paid more attention to this method. More attention to this method. And in particular, by making these arguments local, this is now a much more general method. And so if it's the only message that I can tell you now about this talk, if you ever want to prove lower bounds for resolution and you're stuck with the other two, consider the third one, pseudo-width. And as I said earlier, we can prove lower bounds for random CNFs and also for pseudo-random generators. Okay. So I'll give you like a very hazy description of the proof. And yeah. Okay. So very high-level proof outline. First, we define somehow like the pseudo-width measure on clauses. And think of it as like a set of pigeons that are interesting for this clause. And what is interesting for a clause? Well, a set of pigeons where the clause has not made any progress yet, right? Where it still has to work on before reaching. Where it still has to work on before reaching contradiction. And then we're interested in the set size of this pigeon. Then we show that short refutations can be transformed into low-width refutations, or actually low-pseudo-width refutations. Then in the next step, we show that we actually require large pseudo-width, so we get contradiction, and therefore all refutations have to be very long. Okay? Good. Good. So let us consider a single pigeon. And what are interesting assignments for a single pigeon? These are all assignments where we assign a pigeon to a single neighbor, like to one hole, right? They look like this. They're all zero, but one edge is set to one. And all these assignments have the property that the axioms, like the pigeon axioms, are satisfied by these. Like this is the reason why we choose them like this. But obviously, Like this. But obviously, contradiction is not satisfied. So, somehow, the more such assignments satisfy a clause, the weaker or the less progress a clause has made on this pigeon. Yes? So an interesting number to consider is like the number of assignments from the set AI for this pigeon that satisfy the clause C. And this is like the ith pigeon degree of a clause. Pigeon degree of a class. Okay? Good. So state it again, like the IFC is the number of matchings of each and I that satisfy the clause. Now let's go back to like the Densels and Wiggerson method. How do they prove their width flow amount? But they're interested in all the pigeons that are mentioned in a clause, right? Then you get this pigeon degree notion, where you count the number of pigeons that are. Where you count the number of pigeons that are mentioned in the clause. And in contrast to this, pseudo-width should only be the set of pigeons on which there has not been made any progress. So let's look at an example. Here we have a clause that talks about three pigeons. And the last two, like pigeon two and three, are talked about very often, like three times and four times, big numbers. And pigeon one is only mentioned once, right? Mentioned once, right? So somehow this clause is already done with Pigeon1, and we should be able to just disregard it. So this is what pseudo-with does in some sense. So we define somehow a threshold vector. I'll not tell you how, but think of it as large, like close to the degree. And the pseudo-width of a clause is a set of heavy pigeons. And the heavy pigeons are all the pigeons. Pigeons are all the pigeons with high class degree, right? And then the size of this set is the pseudo width. Okay? And therefore I have a fat pigeon here, because we have heavy pigeons. Good? And yeah, I should mention, like, I'm not close to the right definition. Or I mean, I think yeah, I I don't want to go into the details of what it is precisely, but I think like like this I can like convey some intuition. Like this, I can convey some intuition. Good. So let us give a refinement, proof out line. So we start with a refutation and we classify clauses as having either high or low pseudo width. Okay. And as you may remember, our goal is to transform this original refutation into a refutation of low pseudo-width. So how do we do this? Well, we can take all high-width clauses and like strength. Clauses and like strengthen them into axioms that have lower pseudo-width. Yes? And then we can just remove these clauses from the proof and add them to our axiom set. Okay? That's something we can do. So now in the construction, we see that this set of fake axioms is not too large. And also we have a low-width revitation of the graph function pigeonhole with these fake axioms. Fake axioms. So, how can we see this? Well, whenever the original proof actually used a high-width clause, then we just substitute in the fake axiom. And the fake axiom is something stronger. So, therefore, the proof just gets shorter. Okay? Good. And now all that remains is to show that this formula, this new formula, actually still requires large pseudo-speaking. Good? Good. Okay. So now the first part I'll not really talk about for two reasons. First, I would need the proper definition of introduct. And second, it's not where the hard part of the proof is. Like the second, like number four is much more interesting. Okay? So let us move on to the pseudo-wheel forward one. So again, we suppose that we have an excellent boundary expander. And then we want to say that refuting the graph function pigeonholes. thing the graph function pigeonhole with these fake axioms added requires large pseudo-width. Namely like roughly r, right? Which is our expansion factor. Okay, so now for this talk we'll just ignore the fake axioms. So what is the idea? We somehow want to have a progress measure. We want to classify certain clauses as having Certain clauses as having made large progress and saying that certain clauses have not made a lot of progress. And previously, we defined these heavy pigeons in which we said that the clause still has to work. So a measure that one could cook up is to count the number of fractions that do not satisfy the clause. Okay? So so in particular, like axioms have like zero percent, right? Like they are always satisfied under these assignments. 5 under these assignments. But contradiction has 100% as the heavy pigeons is the empty set for the contradiction. So now the claim that you have to prove is that in low pseudo width, this fraction somehow does not increase. So I think as long as we are in low pseudo width, we still have nothing ruled out. And then once we leave low pseudo width, we actually start ruling things out. And thereby showing that we require our pseudo width. That we require our suitability. Good? Okay. So now let us observe that heavy pigeons have less than delta minus d, or d is this threshold vector, many available holes, right? Just by definition. Because a heavy pigeon has to be above the threshold. And on the other hand, light pigeons should be cons considered unconstrained. Be const considered unconstrained first, right? Because we said like the clause has made already so much progress that we don't care about these anymore. And also they have at least delta minus d many available holes per pigeon. So somehow all the tension that we're interested in is like between zero and delta minus d, right? Once the pigeon has more holes available than this, then we want to consider it as unconstrained, as light. Light. Okay, so now, as I said, I would like light pigeons to be unconstrained. And it turns out that in this claim, we sometimes want to extend matchings on the heavy pigeons to light pigeons. Okay? So so in particular, like we want always to have many available holes per light pigeon. Per light pigeon. So let us look at an example. So we have some heavy pigeons, these are the guys in blue, and these are all matched to these holes, which just happen to be in the neighborhood of a single pigeon. Yes? And now we have a light pigeon, but it looks kind of constrained, right? You cannot go anywhere. This is somehow like this heavy pigeon is not the right set to look at if we consider the graph version. Version. And the solution is to use this closure or support that originated in polynomial calculus, lower bounds, also by Alechnoich and Drasboro. So now I have a closure tilde because this is the intuitive definition, saying that our interesting pigeons of a clause are not just the heavy pigeons, but we add in all the light pigeons that also become constrained. In case we have some matching on the heavy pigeons. In case we have some matching on the heavy pigeons. Okay? And now you can show that this set of interesting pigeons is not much larger than actually the pseudo width. Okay? And this follows like by expansion. Good. So then let us go back to the last slide. And and now what do we want to do? We want to actually like s instead of counting the these fractions over the heavy pigeons, we want to count them over the interesting pigeons. Can you go back on when you say interesting are those other heavy plazols that may become constructed? Yes. They become on the what? So even what events? Okay, given a matching on the heavy pigeons, then like any matching, then a pigeon may not have enough holes to go to, like not enough empty holes. So in particular, if like there is a matching that occupies all neighbors of a That occupies all neighbors of a single pigeon. Okay. Okay. Yes, good. Thank you. Other questions? Yes. Is it really a positive property? So it's just one step? There are several ways of defining it. One of them is like iteratively, and you keep on adding things. You can also define it in as a way to do it. You can also define it as a one-shot definition. But there are, so I mean, at the first step, let's say that you can start that. So the region that is becoming interesting, at the step before it was less interesting, then are increasingly interesting. Yes, so so like while adding pigeons to the interesting clauses, you can The interesting clauses, you consider matchings on the interesting pigeons, not only on the heavy pigeons anymore. Okay? So, therefore, it may be growing. Okay? But again, this is like an intuitive definition. It's not precisely what we use. Good. So, as I said, instead of looking at the fraction of matchings that are ruled out over the heavy pigeons, we look at the fraction of matchings that are ruled out over the interest. Fraction of matchings that are ruled out over the interest pitch. Okay, so now let us look at this claim that in low pseudo width fraction somehow does not increase. Okay, and I have a beautiful picture of a resolution step. So each bar represents a pigeon. We have D here as a threshold and like these green or blue bars represent the pigeon degree of this pigeon in this clause. Okay. In this clause. Okay? So once you're above T, then you're a heavy pigeon. So, in particular, like the blue pigeons here are the heavy pigeons of the clause. And we resolve over this red guy. This just looks like this when you do a resolution set. And think about it offline. It's not very hard. Okay, so what I want to say is that if we consider again going back to pigeon width, and we have, we mentioned a pigeon down here. And we mention a pigeon down here, then we know that one of our premises also mentions this pigeon, right? It cannot be that just some variable appears. And somehow, with sudo, this may happen, right? We have like a light pigeon, which we consider unconstrained, and another light pigeon here. And here, it's suddenly a honey pigeon, right? So somehow, what we want is we also want some kind of a soundness property. We want to blame the premises. We want to blame the premises for why the pigeon suddenly gets heavy. And Rasporov tells us that we have to embed clauses into a linear space and then everything works out. Yeah. I will not go into details on this. This is all I want to say about the proof. Okay? Good. So I told you we also have lower ones for a perfect matching principle. The idea is by Raspborov to take your graph. By Rasprov, to take your graph and take a good cut and then simulate the function PigeonOld principle lower bound on this cut. Well, you know, let's do the same. I mean, it seems like there are some good ideas around it. And it worked once before. Well, then you try to do it and then you realize, well, there are no such good cuts. So you have to resort to like worse cuts. And it gets a bit messy. And then you actually get to this span lemma or this claim that I showed you earlier. Span lemma or this claim that I showed you earlier. And you again want that these light pigeons are actually unconstrained, but they just turn out to be constrained. And there's like no way around this. And then, yeah, you need entirely new ideas for this span lemma. I'm happy to talk about this offline if someone has questions. Good. Take-home message. Resolution is very well studied. We have a large toolbox, right? But many challenges remain, and well, for example. And, well, for example, k-click formulas to run new generators. We don't know how to prove these things. And as I said, I hope there is an upcoming paper at some point this year. And yeah, well, it would be great to extend this to Polynomial calculus. Like, in particular, to prove we pigeonhole principle size lower months in Polynomial calculus. That's it. Thanks very much. Yeah, I guess we've got time to Yeah, I guess we've got time for question or two. In one of the earliest slides you mentioned random formulas and now you don't mention random formulas. Yeah, I so what can you say about it? Can you apply them? Yes. Apply this method to get what? I mean we get a bit worse constants than Bensels and Riggerson. In the ratio of closest to variables or in in the lower bound? Yeah I Yeah I it's a ratio, is it? It's a ratio, she says it's a ratio. So you managed to get to n to the square minus norm? I don't remember, I'm sorry. But I think the point was not that I don't remember that we get anything, it's more like, oh, it works for the, and it seems to be different from vessels on Victor's side. So it's more like like th it's a different technique, but it can give you something. But I don't think we were able to squeeze anything interesting now out of that. No, also that. Okay. But more like it works. So as far as I know, the width lower bound standard uh gives you a dimension lower bound of 2 half or 3 cm. And it's open whether you can go all the way to n to the square minus epsilon or n to the 2 minus epsilon. Okay. I I don't know. It's kind of related because then it becomes more over constraint problem, like very weak. Oh uh I'll think about it. I don't right now I don't know the concept. Any further questions? Hey, let's go to copyright to use. Yeah, it's a question but