So our next speaker, we're very happy to have Georgia Garcia Pargas. I'm sorry I'm mangling the pronunciation, but please go ahead. Thanks. Yeah, so this is joint work with GS Banks and GS Plus 10, and of course. And Joseph, and of course, thanks a lot to the organizers for allowing me to talk about this. So, today I'm going to talk about, it's related to solving the eigenvalue problem for arbitrary input matrices. So, when you work with arbitrary input matrices and you care about the eigenvalues and eigenvectors, one has to be very precise on what it means to accurately compute these quantities. So, I'll be precise about that. So I'll be precise about that. And I'm going to tell you about rigorous guarantees that we got for the QR algorithm, which is the go-to method when one wants the full eigenvalue decomposition of the matrix, but you don't know anything a priori. So this is like, of course, if A has a pretty nice structure, then you can do something fancy, something very efficient. If you're just given any arbitrary input, what's the best you can do? And what's, this is also going to be some worst case. Is also going to be some worst-case analysis. So, we're going to give a running time bound that works with high probability on any input. And in short, and I'll be more precise about this, our result says that the QR algorithm can be used to solve the eigenvaluation problem in all tilde of nQ parametric operations. This tilde means there's some like polylog factors there. I'll tell you what those polylog factors are. Okay, so let's begin by saying what's By saying what does it mean to solve the eigenvalue problem in our case? So we will focus on the following version of the eigenvalue problem. So given an input matrix A and an accuracy parameter delta, find T and U with T upper triangular and U unitary such that A minus U T U star is small compared to norm A. So in other words, this problem is asking, find the exact short. Testing, find the exact Shur decomposition of a matrix that is close to your matrix A. And of course, the diagonal entries of this T are the eigenvalues of the matrix U star. And from the columns of U, you can recover the eigenvectors. So if you want, you can get a diagonalization. And as usual, if you are interested in knowing how good these eigenvalues approximate the eigenvalues of A. Approximate the eigenvalues of A, then that's related to the condition number of the problem. So we solve that up to accuracy delta, then this might get blown up by the condition number of the problem. But that's something balance. Okay. So in this case, something that's very related to the condition number of the problem is the eigenvector condition number, which is defined for an arbitrary matrix A. Defined for an arbitrary matrix A as the condition number of the vector matrix. So you have to take an infimum here because the eigenvectors of a matrix are not fully determined because you can scale them. So you take an infimum over the diagonalization of A and then you multiply this the number of B and B inverse. And so note that when A is normal, this is just one. This is just 1 because V is going to be a unitary. So the norm of the unitary and the normal of the unitary interest are both 1. And when A is non-diagonalizable, then this is just infinity. So this is some kind of measure of non-normality. And to be concrete, and related to what I was saying about getting forward error bounds, there is this theorem of power fik that says that if A and E are arbitrary matrices, K and E are arbitrary matrices, and you should think of E as a small declaration, then the spectrum of K plus E is contained in some disks around the eigenvalues of A, and the radius of those disks is epsilon times kappa V. So if kappa V is 1, then these disks are small, which means that you can predict what happens when you procure the matrix. If kappa v is large, then there's not much hope in doing this. Okay, so now I want to. Okay. So now I want to tell you what the shifted QR algorithm is. So when an input A, the QR algorithm does the following. So it first puts A in Hessemer form. So this means that it finds some unitary U such that U star AU is equal to matrix H0, and this matrix is Hessenberg. So if you don't remember what Hessenberg is, it's here. So this means that That it's here. So, this means that the non-zero entries of the matrix are, well, more like the entries below the sub-diagonal are always zero. And the other ones are to be zero or non-zero. But basically that the non-zero pattern is this. That's what it needs to be Hessenberg. And the point is that you can do this for any matrix A in all of n cube operations. And it's not mysterious at all how to do this. So there's a very simple algorithm. How to do this. So there's a very simple algorithm that works, that is stable, and if you're willing to pay an NQ factor, then this is a freebie. So you can start by assuming that your matrix is Hessenberg. And once you have Hessenberg matrix, what the QR algorithm does is generate a sequence of Hessenberg matrices, H0, H1, and so on, in the following way. So at time t, you choose a polynomial pt. You choose a polynomial Pt, and this polynomial may depend on what you're seeing in your matrix HT, so you can choose it adaptively. Then you apply it to HT, and you take the QR decomposition of that, and then you use the Q part to conjugate Ht. So this gives you the next iterative. And you can do this without having to arbitrarily, sorry, explicitly compute Pt of HT. So there's a way to implicitly compute A way to implicitly compute the next iterate that is much cheaper of what you would get by actually computing pt of h t. And the intuition here is that the roots of your polynomial pt are going to be like guesses for the eigenvalues of ht. And if your guesses are good, then good things will happen. So I'll be precise about these. So the first of all, so something that's obvious from this. So something that's obvious from this is that the original matrix is always unitarily equivalent to each of these iterates ht. Why is that? Well, at every point, your iterate ht plus 1 is unitarily equivalent to h t. So then if you unfold this, then you see that ht is always unitarily equivalent to a. So this preserves that property. And then the hope is that And then the hope is that if your shifting strategy is chosen appropriately, namely if you had good guesses for your eigenvalues, then what you will see is that Ht, which was Hesseberg, meaning that it might have non-zeros in the sub-diagonal, converges rapidly towards a triangular matrix, which means that the sub-diagonal entries start vanishing. So if this So if this happens, then what you see is that A was equal to some unitary conjugation of Ht. Ht is going to some triangular matrix. So then in the limit, this will give you assure the composition of A, which is what we want. Okay? So if you do this correctly, you'll converge to assure the composition of A. So of course, in practice, you cannot. Practice, you cannot run the algorithm to infinity, so you have to decide when to stop. And the following quantitative notion of convergence is useful. So we'll say that a matrix H, which is Hessenberg, is delta decoupled if at least one of its sub-diagonals is bounded above by delta times the norm of the matrix. The matrix. So if you have a decoupled Hesselberg matrix, that means that one of these edges is very small. What you can do is you zero it out, then you get something like this. And the point is that the matrix on the right is a block triangular matrix, which means that the eigenvalues of this matrix is the union of the eigenvalues of this one and the eigenvalues of this one. And each of those matrices are Hessingular matrices. Matrices are Hessenberg matrices. So then you can recourse your algorithm on these matrices and then get an eigenvalue, well, get eigenvalues for this full matrix. And because we only care about getting some backward approximation, then it's fine to move to a nearby matrix during your algorithm. So this step is called deflation and allows you to reduce the problem to smaller problems. And then, so the And then, so the goal standard here is to devise a shifting strategy that has moderate degree in each of the polynomial shifts that guarantees that you get delta coupling in some polylog one over delta time. And if you manage to do this, then you'll get a diagonalization algorithm that runs in this tolog time. Any questions? Um any questions? Yes? Maybe a silly question, but do we need the decoupling to happen like halfway through so we like get two bottles or it doesn't matter? Oh it doesn't matter. Actually in many occasions the only thing you can guarantee is that it happens here. Okay. Yeah. Okay. So what is known about this? Well in the Hermitian case it's shown by It was shown by Wilkinson and then Decker and Charlotte the rate of convergence that if you use the Wilkinson shift, then you get delta decoupling in log of one over delta iterations on any Hermitian input. Then there's work showing that a mixture of the Wilkinson shift and the exceptional shift achieve delta decoupling in log of one over delta iterations on any unitary inputs. On any unitary input. And beyond Hermitian and unitary, not much was known from the theoretical side for general matrices. And I wrote this down because I don't want to mess up my words. So, empirically, what has been done is that some reliable and practical version of the algorithm has been obtained over the decades by addressing non-convergent cases. Non-convergent cases with heuristic modifications and improvements. So, this means that there's some shifting strategy, then users realize that it's not working for certain matrices, and people go and check what happened there, and then you can always put something on top of your shifting strategy. Like if this you're seeing this behavior, then change to this other shift, or just like replace one shift with another shift, maybe it works better, and then eventually this has. Eventually, this has led to an algorithm that is quite reliable, meaning that in most matrices it works very well. It's not guaranteed to converge in all matrices, and it's quite complex because of this kind of batching. Okay, so one thing we can prove is that in exact arithmetic, if, well, okay, let me write. Well, okay, let me right now not talk about stability. So we can show that for every k we have some shifting strategy which achieves this delta decoupling in log one over delta iterations, but this is provided that k is large compared to the logarithm of the eigenvector condition number. So this is related to what Peter was saying in his talk. In this case too, the condition The condition number of the problem is quite related to the complexity of the problem. So if this inequality holds, then we get log of one of delta iterations. And our shifts at every point in time have a cost of k squared times n squared. Usually shifts of degree k one can do k times n. One can do k times n squared, but our shift has two cases, and then if you're in a bad case, you have to do some extra computations, and that's where this square is coming from. And as a conclusion, this allows to solve the eigenvalue problem with accuracy delta, provided well, in this number of arithmetic operations, provided that k has this relationship to the height. To the eigenvector condition. So, then what you do if you want a result for all inputs is that you can use some smoother analysis idea. And there have been a lot of research on this direction. Actually, the paper that Peter was talking about, not at all from 2015, it was a phrase like this, but you can use That you can use the balance that they get there to do something. And then there's other ways to prove this in different levels of generality. Let me just give you a concrete statement. If Gn is a normalized n-by-nger matrix, meaning you take in the entries independent Gaussians of variance 1 over n, then for any input a of norm at most 1 and any parameter gamma, we have Any parameter gamma, with high probability, the random literature proof matrix is going to have some controlled eigenvector condition number. So it's going to be controlled with high probability by a small polynomial in n. So then what you do is if you want to get, say, a delta battery approximation of your SURE decomposition, then you're allowed Decomposition, then you're allowed to move delta over 10 to a nearby matrix. And the point of doing that is that now with high probability, that nearby matrix is going to be relatively well conditioned, meaning that this quantity here that we cared about in the previous statement with high probability will become this over here. In total, if you can count out the formulas, this yields an algorithm. And this yields an algorithm that works with high probability and runs in this number of arithmetic operations. The best you can do in terms of complexity is here having a one and not having this term. We're kind of like block square in that regard. Okay, so I want to make some comments about the intuition or the design of the shifting strategy. Of the shifting strategy. So let me show you a particular instance, which is the case of normal matrices. So remember, the degree of the shift has to be large compared to the logarithm of the eigenvector condition number. In the case of normal matrices, the eigenvector condition number is one. So this doesn't really constrain your degree. So you can find a degree one shift strategy that works in this case. And it was That works in this case. And it goes as follows. Well, before looking at this, maybe it is a bit complex. The point, well, what we can show about this algorithm that I'm putting here is that it decouples the matrix in the following way. It aims to make this entry over here, which is the n minus one entry, very small, relatively rapid. So that's the goal. Yes? Oh, I was just saying one minute. Oh, one minute. Okay. Yes. So the idea is like first you do the Rayleigh shift, if you know what that is, which involves using the entry. And then you check if this entry over here has decreased by a factor of zero point eight. If it has, then okay, you're just going to keep doing this. If it hasn't, then it takes you to an exceptional shift. It takes you to an exceptional shift. And what we managed to prove is that if you didn't make progress, it's because there was some symmetry around the shift that you were taking. And moreover, we know the scale of the eigenvalues that are around that shift. And we can show that there's some annulus around that point where you'll find an eigenvalue. So our exceptional shift is basically take a net in that annulus where you're going to find an eigenvalue. An eigenvalue, with well we didn't try to optimize the number, but with 20 points it will do. And one of them will manage to decrease this by a complexity. Okay, so there are some insights, but let me just actually talk about the conclusions. So this is a theoretical contribution. And what we're showing is that a relatively simple shifting strategy Simple shifting strategy can achieve rapid decoupling, and we have some clear conceptual explanation of why it's working. We are not seeking to prescribe what should be done in practice, and so the LEPAC routines are pretty amazing in terms of performance. I'm sure that what, like, we haven't tested what we've done, but I'm sure that it wouldn't match what It wouldn't match what ElliPlack is doing. But ultimately, the goal here is to. Well, this work suggests that there might be a simple, efficient, and infallible shifting strategy for the QR algorithm. So what we're suggesting might not be efficient. It is infallible. It's relatively simple. What currently one has in practice, it's efficient, but it's not. It's efficient, but it's not infallible and it's not simple. So, yeah, hopefully, one day there is no vote for this. Okay. Couple quick questions. How do you decide what random perturbation to add your matrix, the size of it? It looked like a huge. It looked like if you added something of size like root epsilon, then the condition number of the eigenvectors becomes 1 over root epsilon. Yes, yes. How do you decide what size you make that? So the way we phrased it is that one has some target accuracy that one wants to meet. So then you take it proportional to that because it's like the biggest that you can take it and it's the best thing that you can So like you said, in practice, what people use this Wilkinson chip and then it fails and then they do some false. I think in practice, I think there are a lot of people more five, I don't need to say this, but like there's a like shift of large degree that is used at the beginning. I see, okay, so it's not. Yeah, I mean, so the Francis shift, for example, is something that is used. I'm not quite sure what point. So, I guess what I wanted to get at was towards your dream. Is there even a candidate shifting strategy that could fulfill the stream that we can't prove works? Okay, so I think it's clear that one thing won't work. So, you need some mixed sh shifting strategies. And I guess there's like three or four that are quite common. It is conceivable that some combination are Conceivable that's some combination or like modification of those. Junet, do you want to do the last question or ask myself? Yes, I'm not sure at the end of the day the shifting strategy that you're proposing is simpler than the one in LEPAC. I mean it's it's very similar at the end. I mean it's uh you do the standard shift in general and then you but doesn't it have like ten cases or something like that? Oh, it's pretty much uh the Do there is exceptional shift and a regular shift and so on. Where it gets more difficult. Which regular shift does it use? The thing that is very interesting in your work is when you work on the exceptional shift and you prove that you can find an exceptional shift that makes the thing move, that is nice, that's very nice. That is nice, that's that's very nice. So that's something like that that's very nice. Wait, no, I wasn't there no, okay, so so there are matrices where the Wilkinson shift doesn't work and then the exceptional shift that they had also didn't work. So it has something else after that. I don't know. The ignorant thing. Yeah, yeah, it's it tries to have like uh tons of strategies before the NAPA code gives up. So I don't think there's a counter example for the current code, but uh Example for the current code, but it can certainly not be proven that it's globally converged at this time. Right, well, thank you very much, Roshan.