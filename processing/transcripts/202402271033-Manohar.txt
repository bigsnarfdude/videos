Part two of the talk. I'm going to be telling you about this exponential lower bound from near the LCCs. And because Provesh has already given you a lot of the background of the context, I'm basically going to jump right into the proof. This is going to be focusing on the technical aspects of this work. So this is the main result. We've already sort of said it. But if we have a linear three-query locally correctable code, then we're going to prove that the block length n of the code has to be exponential in k, the number of bits in the message. And as per vascular, In the message. And as Provatius said, the main idea is that in the locally correctable code, we get more constraints than we had in the case of a locally decodable code. And so the idea is we're going to somehow use these extra constraints to derive more constraints via this long chains method, which is some form of structured low-width derivations. And then we'll use this to prove the lower boundary. And so the plan for this talk is going to be divided into two parts. In the first part, I'm going to show that the Cakuchi matrix method plus this method of Matrix method plus this method of long chains has the potential to prove an exponential load boundary. I'm going to do this using this density versus level heuristic that Pervesh kind of ended his talk with. And then I'll move on and give you a proof sketch of the k to the 4 lower graph, which is slightly better than the k cubed lower graph we know for LSC, and will contain pretty much all of the main ideas that we'll actually use in the final proof. Okay, so just Okay, so just to make sure we're all on the same page, remember Pervasia defined LDCs for us. We have this cognitorial viewpoint where we have these matchings, H1 through HK, one for each bit of the message. And we get the system of linear equations where the right-hand side is for the constraints of matching H sub I or B sub I. And then B is a message that we choose uniformly around. And in the LDC case, we have a very similar setup, as we've said, but the addition is that we have N matchings rather than K, and we get extra constraints. Than k. I'm going to get extra constraints. Now, these extra constraints don't come with independent right-hand sides, because the right-hand side is some variable x sub. But nonetheless, we're going to be able to make use of these constraints and do something with them to get a better lower value. So, I want to briefly just recap this density versus level heuristic that Pravesh told us about. Recall, we defined some matrix A sub i, and we came up with the following heuristic. We said that the lower bound we could try to prove. That the lower bound we could try to prove is k is at most L, where L is chosen to be the minimum value such that the row density of these matrices A sub i is at least one. And, well, Provesh defined these matrices when the number of queries or the size of the hyperedges was even. And turns out this applies for odd Q as well, at least the heuristic does. So the matrices become more complicated, so I'm not going to get into that, but you can do this. But I mean, you can do this. And you get an actual proof that k is at most L for this value of L if the A sub i satisfy this approximate regularity condition that probation should be really going through. And so if we compute the row density for Q L D C, it's L over n to the Q over 2, and then we have n constraints. And this tells us, if you go through the math, that L has to be at least n to the 1 minus 2 over Q. And so that tells us our method has the potential to prove a lower bound of k is at most n to the 1 minus 2. Lower bound of k is at most n to the minus 2 over q, or that k, or n is at least k to the q over q minus 2. And this turns out to be the best lower bounds we know for even query q LDCs. And we also got this trade-off for q equals 3 as well. And this says that the method has the potential to get this trade-off for all odd q, not just q equals 3, but we don't actually know how to turn this into a final proof. So this approximate regularity condition is not something we're able to show. So this is a heuristic. So, this is a heuristic. It gives us an idea of what we can hope to prove. It doesn't necessarily give us a proof, but if we have one additional condition, then we can turn this into a proof. Okay. Is this heuristic constructed? Like drives of LDCs, probably? No. Like, this is for proving lower bounds specifically. You can't get like an LDC. You can't even define the matrix. You can't even define the matrices without knowing the hypergraph, and that's like knowing the LCC. Okay, so this is the heuristic. Now let's go to three LCCs. So we have the equations that we started with for LDCs, and then we get this additional set of hypergraphs. And so a naive way to prove a lower bound is trying to show that this is unsatisfiable, right? So we can just forget about the extra constraints. So we can just forget about the extra constraints. And look at this polynomial psi that we had before. And if you go through the calculation with the heuristic, you get that l is n to the one-third, and you get that you're stuck at this cube and rho amount. So the heuristic sort of tells us that we're probably not going to be able to get better than this kq. So a very natural thing you can try to do is just write down sort of an analogous polynomial, but for all the constraints. But the issue is these right-hand sides are not independent. And this means that when we get down to that final matrix A, who's special. We get down to that final matrix A, whose spectral norm we want to bound, it's going to be the sum of mean zero but dependent random matrices, and that makes bounding the spectral norm rather challenging, and we just don't know how to do it. So instead, what we're going to do is we're going to derive new constraints that have the right-hand sides of the independent bits, b1 through bk, but we're going to derive these constraints using the additional hypergraphs that we have. And we're going to produce constraints that have larger arity. And you can notice that the density gets worse as the arity q goes up. Gets worse as the RDQ goes up. But on the other hand, we'll get more constraints, and that makes the density better. And you can hope that this trade-off is going to break in our favor, and we'll be able to reduce L and get better lower bounds. And that, broadly speaking, is the overall plan and what we're going to show. Okay. So let me tell you about chains, which are the type of structure low-width derivation that we're going to use. So we're going to form a new construct as follows. I'm going to start with I'm going to start with a vertex U and pick a constraint C and the hypergraph regions of U. And I'm going to pick one specific vertex in this edge to be special. I'm going to color it red. And this corresponds to this equation, xb1, xb2, xw equals x sub u. That's in our system. Then what I can do is I can now pick a constraint in h sub w. So I'm going to pick a constraint. of w. So I'm going to pick a constraint c prime that looks like xb3, xv4, xw prime equals xw. And now I can just multiply these together. So I combine them and I get a new constraint. The arity of this new constraint is 5 because w has cancelled of itself and I'm left with xv1, xv2, xv3, xv4, xv prime equals x subv. Okay, so the number of such constraints I get is going to be on the order of n squared. And that's because I have And that's because I have n choices for this hyper edge, because the matchings are a size of the border n. And then I also have n choices for this edge. So in total, I get about n squared constraints of error 5. And I'm going to use this notation, h2u, to denote the set of these two chains that have right-hand side h sub x. Is this clear? Like the most important thing of the entire talk. Like, if you don't look at this. Like, if you didn't look at this, you would not understand anything else. Because I'm going to do manipulations with this type of object for pretty much the rest of the entire talk. And here it'd be why there's n squared. Why the squares? Why are there n choices for the first one? Sure. As the W determines the hypergraph in fact. Yeah, so you fix U, right? So I know this constraint has to be an H sub U. There are delta N choices and then plus three choices for the W. So that's the three delta N. Now that I have W, there's again. Now that I have W, there's again delta N choices for this hyperage and then three choices for W. Okay, so now the strategy is I'm going to use H21 through H2K as the constraints. So I'm going to forget about everything else. And these are the only constraints I'm going to use. They have right-hand sides, b1 through bk, which are independent. That's what we wanted. And I can now just compute this density. The density is l over n to the q over 2. q is 5, so that's 2.5. And then we had n. And then we had n squared constraints. So in total, I get like l to the 2.5 divided by square root of n. And if you look at this, you see that you can take l to be n to the 1/5th, which tells us our predicted lower bound is n is at least k to the 5th. So we got a small win. We beat this k cubed, which we were stuck at for three L D C's. Also clear. Small improvement, right, by taking these two chains. And you can imagine that if I go. Taking these two chains. And you can imagine that if I got a small improvement by taking two steps, if I took a lot of steps, like log n steps, I would get a big improvement. And yeah, that's what we're going to do. Okay, we're going to do this r times. We're going to take a constraint and then just keep on going r times. And we'll call this hypergraph HRU. It's going to have three delta n to the r constraints. The arity is going to be 2r plus 1 because each link in the chain contributes these two blue variables. Attributes these two blue variables to the derived constraint. Plus, there's always this last red variable hanging off at the end, which is like where we would hook the next link in the chain if we were to keep going, but we're going to stop. So the annoying thing is this is always odd, and these Cakuchi matrices tend to work better when the instance is even airy. And so this is going to pose like a technical issue that we're going to have to deal with. Sorry, that was the issue. And then, okay, I just use these hypergraphs, HR. I just use these hypergraphs hr1 through hrk, and now let me just compute the density. The density is l over n to the r plus a half, right, because that's q over 2. Now I have n to the r constraints. You can see the n to the r is going to almost cancel with this n turn here, and I'm going to get l to the r divided by square root of n. And I want this to be at least 1. So this tells me I can take l to basically be as small as n to the 1 over 2r and still have this satisfied. And we'll get a bound of k is at most. And we'll get a bound of k is at most n to the 1 over 2. I might be a little worried, because if I take r to be very large, like log n, this is going to become constant, right? And that is clearly not a lower bound we can prove. So what's happening is there's actually some additional polylog n factor floating around here that we've been ignoring so far, which is now going to actually be very important because when I take r to be log n, this term here is indeed going to be constant, and the only thing left is this polylog factor. And we got this down to a log eight. You know, some small optimizations, and that gives us the lower value. Yeah, so it's like the high-level strategy clear. Okay, so now I think it's time. One more comment. So the number of bits of randomness in this instance is only log 8 to the n, right? So Provatio is telling us this is a method to certify unsatisfiability of systems of linear equations that are. Of systems of linear equations that are generated with very, very few bits of randomness. So, in particular, here we have basically arbitrary matchings, and the only randomness is coming from these b sub isa, which are only log 8 to the n of them. And if we get that many bits of randomness, then we're able to certify that this is indeed unsatisfiable. So, the amount of randomness we had is very, very tiny at this point. Yeah, David? So, L is going to be the size, the matrix is going to be indexed by sets of size L. Is going to be indexed by sets of size L. So that's like the, I guess the matrix is like an n to the order L size matrix. It's like some parameters calling us with a matrix. Yeah. Is there some reason to expect that if, I don't know, it feels like it should be if I had only a login, or constantly many bits of randomness or something, that now you're not really certifying randomness. There's not really any randomness at all somehow. And so is there some generic complexity? So is there some generic complexity theoretic argument here that would say, like, if you pushed this technique to deal with little O of log n bits of randomness, that actually you would just be proving, I don't know, that colon p equals p or something. I mean, at some point, there is an actual code. Yeah, you can't get below log squared. Below log squared, there's a code. Yeah, no, no, no, right. I guess I meant more broadly about, you know, this is an application to coding theory, but you've been discussing this technique as a general way to sort of. As a general way to certify unsatisfiability of CSPs with very few bits of randomness, and there's applications beyond coding theory. But it must be that you always need, there's some lower bound on the amount of randomness that's needed, otherwise you're really just doing worst case complexity instead of average case complexity, right? So here we're like making heavy use of the fact that the constraints have this like matching structure. Okay. Whereas like we can do a variant where the constraints have no structure but have independent right-hand sides and then there's some number of constraints we need. It turns out to be like end of the k or end of the cube over 2 where q. n to the k or end of the q over q where q is the size of the hyperg. So we this is like a I guess there's like some of squares lower bounds for that sort of thing. How is three query so important? So good question. If you actually try to do this strategy for larger queries and look at the density you barely get anywhere. You can't it like doesn't work. It gets slightly better than the Work. It gets slightly better than the LDC threshold, but like only by a very small polynomial factor. For example, for four queries, the Q would be 3R plus something, right? Yes, in the same slide. So I guess it's apparent why you look at it. Oh, yeah, yeah, yeah, okay, yeah. You put a 2B, yeah, get like an R to the 1.5%. L equals to n to the 1 1 over 2 times r or 1 over 2 r? 1 over 2 times r, yes. 1 over 2 times r. This is the way that you optimize this L? This pops out because why you optimize L so that the input Yeah, you kind of want to take L to be as small as possible, so that way this dimension factor you lose in the matrix kinchin is small. We're actually, in the end, we'll take L to be a bit bigger than this, but it won't. In the end, we'll take L to be a bit bigger than this, but it won't matter. Like, this is at this point when R is log n, this is some constant. Right? So, we're basically saying we can take the size of the sets of the matrix, potentially be like even constant size, and still be able to argue that the instance is unsatisfiable using the spectral model. Okay, so let's now move on to this proof sketch of the Let's now move on to this proof sketch of the k to the forward lower bound. And so we're going to do this using these two chains, which remember we had a constraint like this, a constraint like this, and these come together to get our new derived constraint. And so the first annoying thing, because I pointed out that this was an odd ARD instance. And so these matrices need to work with even ARD instances. So we have to deal with this now. And it's a pretty simple way, it's a pretty simple way to handle this. They had a list. Pretty well-known trick called the Cauchy-Schwarz trick, where what you do is you take two constraints that share the same last vertex, and then I just add them together and cancel that vertex. So I'm going to get some new constraint. It's going to be already 8, right? Because I get 1, 2, 3, 4, 5, 6, 7, 8. And it has right-hand side that's BIBJ, because I took one from HI and one from HJ. I get an 8xOR. I get an 8xOR. The number of constraints here is going to be n cubed. And you can get some intuition for this because each of these guys has n squared choices. And you can think of it as like the probability that they share this last vertex is going to be like 1 over n if it's like a uniform distribution, which would give you n cubed constraints. And it turns out this is actually the right answer. You're not going to get more, and you're also not going to get less constraints. Okay. Now there's another problem, which is I've looked at the Now there's another problem, which is I've looked at these derived constraints, and the right-hand sides are no longer independent. I'm going to skip over actually how we handle this. It's pretty straightforward, but basically you hard code one of these bits, BJ, to just be one, and you don't lose too many constraints, and it's fine. Okay. So now we have to form our matrix. And our matrix is going to be slightly different from the one Provash gave us. Sorry, that actually works. You can just take the matching. Take note of this. You can hard code each if you want. Yeah, you hard code like half of them. To B1. Yeah, you hard code like half of them. Okay, and that covers every pair. Yeah, you like, what we do is we match like i to j, and then we set b j to b 1, and then only take the constraints with i and j, but pair them. This drops a factor of k and the density, which is why I'm actually going to get a k to the 4 or k to the 5 here. But it makes things a bit simpler. Okay, yeah, so our matrix is going to be indexed now by four sets rather than one set. The sets are still going to have. The sets are still going to have size L, and I'm going to put a 1 in this entry if the following happens. I want the symmetric difference of S1 and T1 to be this pair here, then S2 and T2 to be the pair on this link here, and same for S3 and T3, and then S4 and T3. So, what Pravesh had was he had a matrix where the symmetric difference of S and T was like the full constraint. And here we're doing something slightly more fine-grained to make sure that, like, Slightly more fine-grained to make sure that each link here is split between the left and the right sides of the matrix evenly. And now I can compute the density for you, right? The density is going to be L over n to the 4 times n cubed, which is l to the 4 over n. And this, I guess, means we have to now do this approximate regularity step to argue that most rows indeed have this number of non-zero entries. And of course, this only makes sense. And of course, this only makes sense when L is n to the 1/4 or larger, so that way the average degree is at least one. Okay. And then we're going to use matrix kinchin and then get that k is at most order l and l we set to n be n to the one quarter, so we're going to get n is at least k to the fourth. And this whole like sequence of six steps is a fairly mechanical process at this point. Like we have to pick what constraints to start. Like, we have to pick what constraints to start with, and then we can kind of just go down here and get this bound. And this is like the whole reason we set up the heuristic the way we did. Because the heuristic basically is like if I compress all these steps into a very simple calculation of this row density, it's telling us what lower bound we're going to end up with. Now, the lower bound is a proof if this step is like actually holds. So, this step is, in general, not necessarily true, and we have to argue that. Necessarily true, and we have to argue this. And this is going to be like the main technical point that we have to argue in the proof: that these matrices that we construct satisfy this approximately related condition that the maximum number of entries in a row is roughly the average, at least for most of the rows. Okay, so let me explain this a little bit. What we do is we use some polynomial concentration bound to bound the number of entries in a random row. Entries in a random row. And essentially, what happens is we want to look at the following. We want to say that if I take a chain and I start telling you certain vertices in the chain, but the number of constraints that satisfy those, that have those variables in those spots, is somehow an expected amount. And what I mean by this is I can take this example here, and I can imagine hard coding V1 and V3, and I want to ask how many constraints do I have? To ask, how many constraints do I have that have i, j, v1, and v3? Say it's like t. What I can do is compute some conditional expectation, which is l over n to the 4 minus the number of things I fixed, which in this case is 2, times t, the number of constraints I have left. And then I want to compare this to the actual expectation. And if this is smaller than the actual expectation, then that's the condition that we need for this concentration inequality. That we need for this concentration inequality to work. And you can go through it. I'm not going to really go through this, but it turns out that if you look at this example, you can count the number of choices you have and determine the whole constraint by making only order n choices, which is less than this nl squared. That's the budget we have. So this works in our favor. But this isn't the only thing we have to handle. There's another example that's actually a problem. And it's when I fix v2 and v4 in That's when I fix v2 and v4 in here. And if you go through the math, what you'll do is you'll determine this whole part of the chain with order n choices. And now the problem is I don't have any budget left down here. So I want to basically, I mean, I could pay like order n to determine this part and then a constant to figure out this, but that's a total of n squared and I don't have that budget. Instead, what I want to say is that because I know two variables here, the number of constraints that have this pair of variables. Constraints that have this pair of variables should be actually kind of small. And this is not necessarily true. And this actually poses a big problem for the proof. So we don't know how to deal with this. And it turns out that this parameter is going to be what we call the heavy pair degree, which is going to be the maximum over pairs of vertices of the number of constraints in the whole initial set of constraints across all hypergraphs that contain this pair. So remember, this. So remember, this, if I look at one hypergraph, h sub, there's at most one constraint that even contains v, let alone v and w together. But across the whole set of hypergraphs, there could potentially be n constraints. Like, it could be that each hypergraph contains a detail and C that contains this pair. So this is actually going back to your question earlier, where you were asking about, like, what happens? Do we need to use that the hypergraphs are somehow uncorrelated with each other? And here this actually comes into play in a pretty big way. And here this actually comes into play in a pretty big way. And so we can bound this by d and then get a total of n times d, where our target was nl squared, which is n root n. And this is going to be bad if d is much larger than root n. And intuitively, this feels like a really weird thing to happen. Because what this is saying is that when I try to decode x sub u, I actually query xv and xw together. But not just for xu, for really all the other. But not just for x u, for really all the other, like for root n choices of u or more, maybe even n choices of u, there's like these two coordinates that I'm querying together a lot. And that seems like a very weird thing to have happening. It seems like something that should make it a lot harder to build a code. And it turns out that, well, actually all the constructions that we know of, like read muller codes, have d equals one. Okay, so like this, this doesn't happen. So somehow this is an issue, a technical issue, in the proof of the lower bound that. Bound, that feels like it almost shouldn't be there. It's something that feels like it makes your lower, should make it easier to prove your lower bound if D is large. Okay, so let me explain how to handle this very briefly. What happens is in the Cauchy-Schwarz trick, we say we should cancel one vertex. Well, it turns out if there are many heavy pairs, what we should do is we should really cancel two vertices rather than just one. And so we can do this. We'll get in six XOR instance, and we'll get some fewer number of And we'll get some fewer number of constraints, but there'll be a lower area. And it turns out that this is actually just better for us because d is rooted. And so that basically finishes the proof. I mean, there's like two cases. In this case, I gave you sort of a sketch of what happens. And in this case, we basically do the similar thing, just for these different types of constraints. So, to follow up on what you just said, so the matchings are still arbitrary, right? Yes. You're never kind of going back and saying, okay, there are some matches. And saying, okay, there are some matchings that just kind of a priori wouldn't work outside of this proof technique. You're kind of dealing with it all inside of this sort of thing. Yes, exactly. Okay, and let me just briefly say what happens with longer chains. You can do the same Cauchy-Schwartz trick. You form your matrix, you show that this is approximately regular. If there is no generalization of this heavy pair issue, win. If there is, then it tells you you should do a Cauchy Schwartz on some larger type of sets. Larger type of sets, and these sets look something like this: where this is the orange. Turns out that what you need to do is you need to put two variables in the last link, and then you need to merge back and only put one in all the other links up to some point where you stop. And figuring out that this was like the right type of sets to deal with actually took like six months. This is really like a very, very complicated part of the proof. Like, everything else is actually not so bad. Okay, so let me just conclude. Okay, so let me just conclude by talking briefly about some open problems. So, as Rachel has pointed out, you can ask what happens with LCCs for larger queries, and if you go through the heuristic, you get something that's n to the 1 minus 2 over q minus 1, which is better than the thing you get for LDCs, but only by a little bit. So it's not that much better. It's kind of like plugging in q minus 1 until the LDC lower bound. And then, well, for us, when q is 3, q minus 1 is 2, and we actually had exponential lower bounds for two query LDCs, which is sort of why we're getting exponential. LDCs, which is sort of why we're getting an exponential lower bound here. You can also ask what happens about non-linear codes, and it turns out that the entire proof pretty much works. Actually, the hard part is arguing with the set of chains that we form has some non-trivial, has an assignment that satisfies a non-trivial fraction of them. And with this conjecture that we had, maybe we can show that Reedmuller codes are optimal. We're pretty close. It's just a log A to the N versus a log squared. And there's only two steps in the proof to optimize. There's only two steps in the proof to optimize: this row priming step that loses a log to the four, and the hypergraph decomposition step that loses a log squared. And in fact, as Pravesh pointed out, there's a recent work posted like a few days ago that gets this down to a log to the four, and it's because they get this down to one log, and this also down to one log. So just to summarize the two talks, we presented this general method to certify bounds on degree 2 multilinear polynomials over the hypercube when the polynomial has some minimal. The hypercube when the polynomial has some minimal amount of randomness. And the center of this is these Cakuchi matrices. And there's a lot of things we didn't talk about, such as algorithms for CSP, some external combinatorics, and some number theory stuff. And the stuff we did talk about was this lower bound for locally decodable codes, and this exponential lower bound for locally curveable codes. I think this is a good place to stop. Thanks for your patience in listening to two back-to-back talks, which had some pretty technical material. Thanks. Yeah, I don't know if this made sense, but you talked about doing this resolution on these long chains. Can you potentially do resolution on other shapes that are not just a path, like trees? Yeah, like we've thought quite a bit about this, and it seems like the chain is the best one, as far as we can tell. Okay, so for this problem, but Alex, you have a great point there. Alex, you have a great point there. One of the works with Benny basically shows how to improve on this hydroclock mode bound that you might have seen in different talk. You actually gain improvements there by using some, we call it flower, but it is a tree and tree, whatever the reasonable name too. But it is a different structure, it's not a chain. Like somehow that turns out to be helpful there. But as Peter says, in this LCC and LDC case, I guess. I think it is in case I guess yeah. What data do you think is this ODC value? I think the right answer is like 2 to the root k. I think read molar codes are awesome. And it applies to what field size? So you can basically replace k with like a k over this field size. Like a k over this field size, if you want to go for larger fields. So, as long as your field size is not very large with respect to k, then you can still get an exponential lower bound. So, the Cauchy-Schwartz trick for getting from odd length to even length is roughly the same as taking, say, an n squared by n matrix of clauses and multiplying it by its transpose. Then you're also chaining two of these together to get a four. Chaining two of these together to get four, like four clauses? Exactly, yeah. And I'm sorry, really simple question, but I mean, if the XOR of three things gives a fourth, that's a clause of length four, right? We view it as a clause of length three, where like the fourth guy. The fourth thing is the thing that's randomly chosen and fixed. Plus 20% of me chosen. It depends, right? So if you're in the first k hypergraphs, then the right-hand side, x sub i, is one of the random bits. So then we should definitely think of it as separate. Okay. If you're in the other k. Of it as separate. If you're in the other hypergraphs, we still want to think of it as separate because it's like the thing we are joining the chain on. So we're actually canceling it every time we put it in. But yeah, morally, it is like a 4xOR that we're thinking of as a 3XOR at some point. So, I mean, you phrased it, you both phrased it as there's this big family of XOR formulas that all have to be satisfied. You're going to show that one of them isn't because many. Isn't because many of them aren't. But of course, it's also one huge formula, right? I mean, the and of formulas is a formula. But it's unsatisfiable for an interesting reason, right? Because a lot of, well, you know, really random formulas often have the property that any small sub formula is still satisfiable even when the whole thing is not satisfiable. But the clauses of that huge formula come in these bundles, and actually, one of them. Come in these bundles, and actually, one of those bundles is unsatisfiable. So, maybe, like, in this case, it's a little bit less surprising because, in the way you're building the formula, you're looking at disjoint variables. For every B, like, you use a new copy of X. So, in some sense, yeah, if you have a linear formulas on disjoint variables, then the only way it would be unsatisfiable is to point the five minutes of the same thing. Because you get to choose a different code route for each. Exactly. Dave. What was the reason why you constructed the Kikuchi matrices differently, like in this more stratified way? Yeah, so if you don't have. Let me try to go back somewhere. So here when I'm doing this like row pruning, I'm saying I want one VH per like link at most. If you split things incorrectly, you can maybe have to handle the case when you pick two variables from one link. And it turns out that this just completely breaks everything. What will happen is there'll be a lower rank term in your matrix. A lower rank term in your matrix that has a very high spectral norm will dominate your spectral line. So then the bound just completely falls apart. Yeah, there will be rows of very high degree, and actually a lot of rows will be like that, and this approximate regularity condition will just be false. Remember to tap the desktop recording. Also, maybe uh screen.