And with this work, I'm going to present the major findings of my past research work. I've been doing at Rama Columbia in Canada at Colton University during my PhD. So the work is about improving a data simulation system called the garmentary balance coherent filter, which is applied to methane estimation using POSA data, and that's where you see my. And that's where you see my quote model. So, before I start, I would like to acknowledge the co-authors and my teaching advisors, Archean Benard, Thomas Parker, and New York Khan. And thanks again for giving me the opportunity to present my work. Okay, here is an outline of my presentation. I start with a big picture of data assimilation inverse modeling and the importance of atmospheric methane, before we go. And then I'll talk about some gaps on the mutation that actually motivates. The completion that actually motivated this study. And before I go to the main part, like the main discussion and results, which is divided into three parts, I'm going to briefly talk about the research tools, including model, observation, and some assimilation techniques that we use. And finally, I conclude that some suggestions for future ports. Okay, so as you know, I'm going to briefly review some I'm going to briefly review some basic definition of data assimilation universal modeling to make some differences between the purpose and objective of these two. Okay, so model forecast concentration are the output of the chemical transport model which is derived from like inputs or parameters such as emissions. So model concentration, as you know, are usually created and provide a consistent and evolving concentration, time and space. You also have observation, and we know that they're more accurate than the model. And we know that they're more accurate than the month, but they're sparse in time and space. So, if we want to obtain a more accurate and at the same time involving photosystem concentration, we can combine these two using some statistical framework that we call data assimilation. And the result of data simulation is we call the analysis. So if we use the observation only for the purpose of improving the model inputs or parameter, then the assimilation technique is so-called in response. And the result of that is basically improved. That it's basically improved inputs such as emissions that we call posterior emissions. So, in this research, I used post-data assimilation and numbers modeling technique, aiming to improve methane concentration and emissions. Okay, so here I'm going to talk about the importance of atmospheric methane that you already know. So, methane has a big impact on both climate and air quality. In terms of climate, it's like Climate, it's a greenhouse gas with a large impact, with a large anthropogenic reinforcing, and due to a short lifetime and a large global model potential, it can have a significant impact on your term biodiversity. So, in terms of a quality, methane is a reactive species which basically oxidizes with hydrosteric radicals, and through production of water vapor, it can produce ozone, which is a problem. It can produce ozone, which is a powerful position. So, another critical point for methane is that, so you have seen this figure before in the last far. So, the global concentration of methane has been rapidly increased since 2007, as you see here. And as basically shows that there is a large impact from methane on both air, on both climate and air quality. So we actually need to reduce methane, we need some accurate and efficient esimulation system to estimate the methane emissions and concentration together. That's where that assimilation versus modeling can play a significant role. Okay, so here I'm going to a little bit talk about the gaps on limitation to past studies. Some of them have been adverse in past research and the researchers are still And the researchers are still continuing that direction, but there are some that we're focusing this study. So, first of all, most of the past methane studies are focused on methane inversion because of reducing methane emissions. But recent reports says that there is a large discrepancy between the result of the inversion of the different status, even those that they use the same data set. So, data simulation can be quite helpful, addressing some of the. Can be quite helpful addressing some of those issues. For example, when it comes to providing the accurate initial boundary conditions. But that isolation has its own challenges. For example, to estimate the state and uncertainty together, it's the bit the conventional method. It needs a computationally, it's expensive and it's very computationally heavy. And therefore, we need some development. We need some development methodology to be able to improve this aspect. Another limitation that exists is about the estimation problem itself. So past studies usually make a perfect model assumption. And they also assume that the error statistics is already optimal. But in this study, we showed that the input error covariance has a large impact on the quality of the assimilation. Yeah, simulation. And so, overall, with that, the literature still lacks a separate and objective computation of error variances, which can have a large impact on the inversion as well as the isolation. Okay. So, based on that, this research is divided into three main parts, which I put like the highlights in three questions. So, in part one, So in part one, we present the development of a low-cost estimation system called parameter dynamic filter and that actually provide an estimation of the state concentration as well as the uncertainty. And in part two, we actually discuss the impact of the error covariance parameter and how we can obtain the optimal analysis and corresponding observances. Uncertainties. And in part three, we actually use those information from the state assimilation for improving inversion. Kind of have tested for methane. Okay, before I go through the resulting discussion, I'm going to briefly uh talk about the tools, research tools I used. So Mobile and uh for the Mobile we use uh CNAR and we use Adjoint Model uh which is one which is an air quality and Which is one which is in air quality and post-prochemistry. So CMAC uses a methane for work and it takes the initial data recognition from global domain. And methane by default is not part of the chemical component in CMAC, but we add the reactions as fibrosymoretical into the chemical mechanics of gas-based chemistry in CMAC. For observations, we use GLOSA data that provides the reasonable surface sensitivity Reasonable surface sensitivity and kind of accurate measurement. So, GOSAT, following past studies, there are latitudinal bias in GOSAT data that need to be removed before assimilation. So, here we use accurate, step one, we use accurate surface measurement data from OUPSPAC to first remove the bias in model and then we use the unbiased model in step two and apply their Model in step two and apply to read the linear aggression model the latitude to remove the bias in coseta. So, as you see here, the last part, there is a difference between coset and unbiased CMAC before and after bias correction. Okay, every assimilation system requires information of error of some sort, and here you know that the observation errors and error, model error are usually uncomfortable. Usually uncorrelated, therefore, their corresponding error covariance are diagonal, and mathematically speaking, you don't have issues dealing with like diagonal matrices. But background error covariance and forecast error covariance, there are full matrix. And estimating every single element at that matrix is computationally very possible. So we use covariance modeling, like kind of a function, some acceptable form of a function. acceptable form of a function and really true that you start a computation of error covariances by estimating a few key parameters. And those parameters in these functions are like first order S6, second order and Gaussian correlation model are correlation legacies actually. In addition to that we can also estimate the observation error prevariance factor and model error prevariance factor. Okay, I can get back to the first part of this research, which is about the need for developing a low-cost and efficient estimation system capable of estimating the state concentration and their uncertainties. So here we propose the parametric variants formal clustering. So I'm going to briefly explain the main features of this escalation system. Okay, we have a quite large state space problem. Quite a large state space problem, and estimating state and uncertainty using conventional approach like 40-bar and some problem is quite expensive. But here we use the parametric form of problem flux rate that produce the forecast and analysis and explicitly evolve their variance using a continuum formulation. So, as you see in this flowchart, the simulation is divided into two main steps, forecast step, assimilation, and the analysis step. So, in the forecast step, the concentration is evolved using That concentration is evolved using a schematic finger transport model. And in a parallel forecast run, we basically evolve the error variance using the continuous correlation based on the advection of SEMI. So in the analysis step, so basically one of the things I need to go through before is that since the methane chemistry is very weak linear, we can implement Very linear, we can incidentally deal with the error variance evolution and the error correlation in this system. And then the analysis step, first we use the operator form of the function form of the observation operator and we are in this error point with a particular probation model. And here we use the SOAR and the second step we compute first the innovation observation is model and the corresponding curve for the variance to be inverted. One, we care for the variance to be inverted. And in the third step, we actually get the update of the concentration as well as the air variance. Okay, here we have the evolution of the assimilation analysis in the left compared to the model forecast on the right side, and both are started with the same initial similar initial condition. So, as you see in the esimolation, while satellites move along the orbit, I'm doing the escalation every hour. And there are two main differences between the CBT here that I want to take your attention. First is that we see like there are some hotspots in the left that they're not appeared in the model, and those are associated to like 2D emissions that are misduring particle. Emissions that are ministering correctly assigned to the model and are a simulation systems capable to capture some of those signals of emissions. Another point is that we only use the observation of the lab, but what we see the effect everywhere, including those regions that we don't have observations. And that's because we use the transfer scheme, the continuum formulation that evolved your variants. Okay, as I mentioned earlier, As I mentioned earlier, we get an update of the error variance in this assimilation, and that basically evolves the errors in a dynamic equilibrium ray based on CMI transport. So here I did an experiment where I start with the initial field of variance, the 7,000 pp squared everywhere. And when I run this, we see the reduction. Iran is that we see the reduction of uncertainty in some places to be the effect of observations, and those are getting evolved based on the dynamic. And so basically, those reduction means that we get more accurate in our estimation at those locations where the error variance is reduced. So this error variance is flow-dependent, and we see the effect on some regions that we don't even have observations. Observations. And the reduction goes down from 70,000 pp squared to somewhere like 40 pp squared, which is almost a quarter of magnitude of reduction. Okay, so we can basically verify our sum of our assumptions using the single observation experiment. So, for example, we can So, for example, we can test you and test the assumptions that we used for advection scheme that evolved the error. In this experiment, at the top, we have analysis increment. At the bottom, we have error variance reduction. And those are from two separate runs. The above from C-Mac with all chemistry transport, and the bottom is only a patch. And these are done for three days, which is equivalent for let the Or let the ghost have the result. So, as you see, after three days, the transport of the analysis increment and error variance reduction are quite similar in terms of their spread and spatial distribution. So it means that using the error variance to evolve the, using the advection to evolve the error variance, we're methane as a reasonable assumption to make. And we actually did the same experiment on the vertical correction and we see the same kind of behavior. We see the same kind of behavior. But I won't show the results here. See a better time. Okay, I think we're on to the second part of this talk, which is about the impact of error covariance parameter and how we can obtain the optimal analysis and corresponding error variance as well as certainties. And we tackled this problem by presenting the cross-validation method that you all have seen already in the in Have seen already in short talk. So, cross-violation is useful because it provides and we use independent data set for verification. And basically, we assume that those observations that use for a verification are coming from different data sets. Basically, instead of assuming that the analysis is already Of assuming that the analysis is already optimal, we use different sets of observations and we minimize the analysis error variance with respect to those independent data. And that actually comes in the form of a cost function with respect to, like, associated with cross-validation. And to make it a bit clearer, if we use with cost-validation, we get the analysis and observation uncorrelated. And if we don't use cross-validation, the analysis and observation. The analysis and observation are correlated. So, you have seen the speed before in short talk, so I'm not going to go over that. But I'm just going to mention that if we use cross-evaluation, we get an optimal point which is associated to the minimum of the cost function or minimum of the error-variance, the analysis. Whereas if you don't use cross-validation, the results go to the cases we covere estimations. We cover estimations. So, for cross-validation, we use basically K-fold methodology. Here we have three folds. We generate the analysis with two sets as active observations, one set delete, one set out for the evaluation as passive observations. And one important point for cross-validation is that you have to make sure that your observations are uncorrelated. And we try to make the observation uncorrelated by applying. By applying the teening technique, then found that the 10-kilometer cutoff distance is appropriate to make both side data uncorrelated. And we actually verified that using the T-Con observations, but I didn't show the result here for the sake of time. And okay. In relation to the past study findings, here we have three parameter therapical variances to be estimated and those. And those parameters are horizontal correlation length scales, vertical correlation length scales, and observation error comparisons factor. And this table shows the iterative minimization of this scheme, and these figures shows the result of the final iterations. So in the left red curve, we have the minimum of the ERP variance parameter associated with the minimum of the cost function as point. The minimum of the cost function is 0.5. And the values that we found for the correlation lenses are 350 kilometer horizontally and 7 sigma vertically. That correspond to the minimum of the cost functions or the minimum error variance of the analysis. So the objective of this part is to find the optimal error equivalence parameters corresponding to the optimal analysis. So here we're going to test Here we're going to test those values that we found in our cross-validation scheme, the impact of those on the assimilation results. So the figure in the left, we have model concentration on the y-axis versus the TCON observation, which is the accurate surface measurement that we usually use for evaluation. And the middle, the red dots are Middle, the red dots are the analysis produced with optimal error comparisons parameter, and the green dots are the analysis produced with non-optimal but commonly used values with error parameters. So as you see, the analysis with optimal error covariance is much better than those two cases to fit the G1 observations. And interestingly, here, the analysis with non-optimal parameters is Non-optimal parameters is even worse than the model forecast. And that actually implies that we need to estimate the optimal error covariance parameter along in our data assimilation scheme. Okay, we're going to move to the third part of this work, which is about the result, the effect of the assimilation that we found in part one and two for improving methane estimation. And here we proposed. And here we proposed a framework that linked the PBTF assimilation to a 40-bar standard of emulation. So we applied the LSE framework and here we generate the simulated observation with known emissions given observation error in the green boxes in nature one. And in the control one, we actually perturb the prior emissions in different ways and also provide different initial and error prevention. And their covariances to our cost functions, and all those given to the 4DR optimization framework, and we get the posterior emissions that can be compared easily with the two remote emissions. So here a simulation window is the prior to an inversion window. And so basically, previous study assumed that the state of the system is out. system is already optimal and basically it's basically the model is perfect and therefore the effect of for there is no basically there is no effect from the uncertainty on uncertainty of the state on the inversion and so at time t1 in a classical term they use model forecast Model forecast and assimilation analysis, but not the optimal one. But in this method, we use more information from our PTPF assimilation, including our optimal analysis and corresponding analysis error variance. And in addition to that, we also, in the inversion window, we also account for the evolution of those errors, which does not mean accounted in the previous part. Okay, here are the four different cost functions. And this cost functions that account for all the escape information that we propose in this study and we compare with the classical form that they didn't account for basically the error variance as well as the optimal analysis. So I'm going to jump to the results. And here's our Aussie results in the form Here is our Aussie results. In the far left, you have prior with 50% extra emissions everywhere, and those cases B2E shows the post-air emissions. So I start by type 1, where we don't get any information from the state, and we assume that the model is perfect. And as we see, the posterior emissions show the overestimation and underestimation in different locations. But when we use But when we use the optimal analysis as our initial condition to the inversion, we get an improvement in some locations, but there is still some miscorrection in some regions. And in type 3, when we provide basically the evolution of analysis or invariance, we get more improvement in our evolution correction. And finally, when we compare type 3 and type 0, where we only account for the model. where we only account for the modeling error, we see a small improvement in some location like East Station. And this shows the scatter plot corresponding to previous slides. And as you see, the y-axis shows the prior and posterior, whereas the true emissions and the x-axis. So type 1, which only relies on the model forecast with perfect model assumption, we see that the posteriors are biased downward. Bias downward, mainly because of the effect of extra emissions in the prior that shows in the initial state. Type two, we use the analysis, optimal analysis, and we see that the bias of the posteriors must be removed, but there is still a variance in the posterior emissions. In type 3, where we account for the analysis, there are variance in our recursion. The variance of the posterior emissions also decreases, means that our emissions getting closer to move. Getting closer to move. And finally, I'm gonna, the type 0 when we compare with type 3, there are statistics also in proof. And this is another case of our Aussie, but with non-uniform perturbation of the prior, which is a little bit closer to the real case. I'm not going to go into the details, but the trend of the improvement by providing more information from instead is similar to the uniform perturbation I showed you before. I showed you before. So, in general, we can conclude that if we provide more information from our simulation, including the optimal analysis as well as the error variance corresponding to that, we can even get a better correction on our region inversion. So, the hotness of this talk comes into three parts. In the first part, I show a development of a low-cost isolation system capable of estimating the stay and. Estimating the state and uncertainty together. In the second part, we showed that the impact of the aerial covariance parameters to be optimal is quite important. And part three, we show an application of using those state estimation for improving machine version. So, suggestions for the future work, for example, we can extend this framework for the joint source state estimation to basically get an estimate. To basically get an estimate of the emissions as part of the solution, we can also apply this system to other species with similar properties like CO2 or even carbon monoxide. But for other species with complex chemistry, like ozone, we probably need further development in the methodology. And finally, it would be interesting to apply that to other satellites for the purpose of For the purpose of high-resolution inversion constellation. Thank you so much for attention. Wow, this is a really nice work. I learned a lot from you. And so I have a question, and this is a question that's been building over all the talks this afternoon. So you're using GOSAT data to try to estimate a variance. Covariance, error covariance measures, right? And you're reserving Tcon as your evaluation data. Do I understand that correctly? That's right. Okay, so, but those are column measurements. What if your error covariance matrix has, you know, structure in the vertical? Like, then what? That's right. But we check out actually for methane. For methane, we don't get any complex structures. Like complex structure in the vertical, it's kind of smooth, as far as I know. But maybe for other species, if you want to apply the system, that's right. That's the things that we need to consider for the framework. When your estimate didn't have any vertical structure? The real estimate is pretty complicated, and you'd think there would be some vertical structure. Yes, but we can kind of. But we can kind of take the vertical separate, like the correlation of length form vertical separate from the horizontal. And that is structure in the vertical, we assume that is uniform and suit. That's it was verified against aircraft observations, for example. Yes, and other type of observations, not just yeah, I've Yeah, I verified that with T Con, but yes, there will be a cases verified with aircraft. That's right. Okay. Yes. I have a two-part question. So the first part comes where you're part two when you're estimating your solar correlation function. So during this process, are you only estimating one like optimal correlation? There are actually two horizontal and vertical. They're actually two horizontal and vertical. That's right. Okay, so you can only get more and less PCs. That's homogeneous as formula. Okay, so then my follow-up question to that is then in the PKF formulations that I've been working out with, that also Olivier introduced earlier, typically in the forecast set not only involve invariance, but also another parameter, say like a correlation length. It's a correlation length field that allows you to construct anisotropic correlations of all kinds of Correlations evolving when you're doing it. So I'm wondering if in this version that you're talking about, you're only involving the variance? That's a bad question, actually. We test that for methane, and methane, since it has a linear chemistry, the evolving multi-variance, it works for Both equal invariance, it works for methane, and we can keep the SOAR correlation kind of external attached to the variances. But yeah, if you want to go with species, with complex chemistry or a non-linear, yeah, that's the case. We have to evolve other modes in our code. Question of time, I mean, you have to construct all the systems, it takes years, and so that's the That's what he was able to accomplish. Another last question, yes? When you evolve the error variance, how does it work with convection? Convection, you mean that actually, there is a advection scheme in CMI credit model model that we only make that active when I run the error variance. Are variants. So there's no diffusion, no like other transport. I'm not quite sure convection. But in sub-grid scale convection, you don't have that. I'm not quite sure. If it's part of the advection, it has to be included. But yeah, if it's yes, we have like two two schemes and in the model, like uh diffusion. In the model, like the fusion vertical horizontal abstraction vertical horizontal. So I keep the abstraction in modeling. So yeah. Okay, thank you. We have an extra talk by