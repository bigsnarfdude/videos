Alright, thanks so much for the invitation and thanks to the previous speakers for giving a really good introduction. So the senior researchers are always really good at explaining the motivation and the background. So it makes our life much easier. So I'm going to talk about using Biobank EHR for genetic research. So I think most of these biobanks are very Most of these biologists are very familiar with everyone here. So, there's a lot of biologists that are used for genetic research. And this slide, I think the first speaker really covers the opportunities and challenges of using the bio banks for performing research in general. So, I'm just going to give a brief overview of these opportunities. So, of course, in file banks, we have very extensive phenotype measures that are linked to the gene data. That are linked to the genetic data, and there's potential linking multi-modal data with the clinical data and genetic data. And in general, we can also access the larger sample size compared to with just traditional GWAS studies. And we can also integrate multiple EHR data. I think the second talk about the federal learning algorithms. So, with the different EHR data, we can develop methods to integrate different EHR data. And with the integration of different data sets, we can improve. Different data sets, we can improve the reproducibility and the generalizability of the results. But with different opportunities, we also have challenges. First, is the data quality because the EHR data was not collected for research purposes. We have to be careful the quality of the EHR data. And also, I think field type extraction was touched on by the previous talks. How do we define the field types? There are different field typing algorithms. And data access and privacy of. Data access and privacy, because for different data sets, there are varying degrees of accessibility, and there's also privacy concerns when we talk about EHR data and GNA data. And lastly, the EH data are observational, so we can typically only infer correlations but not causations. But there are, of course, methods that are trying to infer causations from this observational data. So, today I'm going to basically talk about a couple methods we have developed using the biobank VHR. So, one type of method is about developing privacy preserving distributed algorithms to integrate multiple VHR data. Another method is we are developing a generous prediction model with this type of data. So, specifically, why do we want to integrate multiple EHR? So, with multiple EHR. So with multiple EHR, integrating multiple EHR, of course, we can increase our sample size. And with the larger sample size, our results could be more generalizable. So for example, if we have EHR system that serve different regions, I think some of the questions were asked, like, how do you, what's the advantage of integrating EHR? So if one EHR serves a local region, by integrating EHR from multiple regions, our results, our training data will be more representative. Our training data will be more representative of the whole population. So the results could be more generalizable. And with the larger sample size, we could also have greater sensing power. And for the rare events, so for example, for one particular EHR, you don't have enough sample size for a particular disease. By integrating multiple EHR, you can have increased case counts for a rare event. And also, I think multi-ancestry is a very Multi-ancestry is a very important topic right now. So, with the different EHR systems, because there are several different areas, there could be different demographic compositions. So, by integrating them together, we can increase the sample size for underrepresented populations. And of course, we can also bring in different investigator resources. But the challenges of integrating EHR is one is a concern because the data are protected, the clinical data and GI data, and the data access and communication concerns. And the data access and communication cost is also concerned. So, one of the more recent challenges that I got last week is: I got this email from UK Bob and is starting today. All the UK Bob and data are going to be spooci on the cloud. So I'm also interested in people hearing people's thoughts on this, because that's definitely a challenge going forward of how to analyze these different bioband data more effectively. Because previously, you can actually download part of the YouTube BioBand local. Actually, download part of the UK album locally to do the analysis, but now everything is going to be on the cloud. And also, same with the OFAS data, the entire OLAPAS data are already on the cloud. So you can't download any data from the OFAS. So if you want to integrate these different data together going forward, you really need methods that can effectively integrate all this data without access the integration downloading the integrated document. Look at the so of course there are multiple applications when you integrate charts. So the scientific questions we want to answer by integrating this data is something called pleiotrophy. So quite simply, pleotrophy is we want to detect association of one gene or one SNP with multiple phenotypes. So if a SNP is correlated with multiple phenotypes, we say this SNP is potentially player-tropping. Pleiotropic. And we know there are many pleiotropic SNPs across the channel. So if you look at the GWAS catalog of all the previous associations, so each dot represents a single significant association of one SNP and one phenotype. So we can see that many of the loads that have association with multiple different phenotypes. So we know that pleiotropy exists across the genome, and there are many of them. And there are many of them. And I think the earliest and most commonly used method to detect pleiotropic effects is called phenome-wide association study, or PHWAS. So in PEWAS analysis, what the analysis is doing is looking at multiple GWAS-associated results. So if you have a GWAS for one disease, hypothiabetes, obesity, and hypertension. And hypertension. So, what FUS does is look at the common significant associations across model disease. So, if a SNP is associated with multiple disease, we say the SNP is potentially platonic. Of course, it's not causality, but we say it's potential. But with the VOS, there's a couple of challenges. One is computational time. So you want to add one additional field type, you have to run another GWAS across many. Across many, many SNPs. And there's also a challenge of increased multiple testing because with additional phenotypes, you are adding additional comparison across entire state. So if you want to look for pleiotropic effects across 10 different disease, you are basically going to increase your bowel testing 10 times compared to just one disease. And it's also unclear how to integrate multiple EHR data. Integrate multiple EHR data when we detect clayotropic effects. So, a little bit of background of different methods for performing data integration. So, the best way to perform data integration is the goal setting approach, where we can just combine the individual level data to a larger data set. So, in this case, the results are lossless, so that means we don't lose any information. That information, and also because we are only combining the data one time, so it's not incurrive. So, in general, with the data integration methods, there's a trade-off between losses and the number equations. And there's some other methods that have already been developed for other couple regression methods. You can see there's normally a trade-off between the lastness and iterativeness. So, for linear regression, you can also achieve lossless, and it's only required. And it's only required communication of one or a few times. But with other methods, I think Boulorn was mentioned in the second talk, there's a trade-off between losses and equations. So for the analysis of query drop effects, we design a new algorithm called sum share. So sum share, we try to achieve both lossless and non-intuitive with the analysis of binary outcomes. So what this means is for So, what this means is, we don't want to lose any information by combining the different data sets, but we also don't want to communicate many, many times between different EHR data sets. So, I'm going to skip most of the methodology part, so just give you an overview of the sum share algorithm. So, the sum share algorithm, there are three different steps. So, the first step for each of the EHR data will calculate a Calculate some summary statistics of the EHR data. And the second step, we are going to exchange the summary statistics with the central analyst. Then the central analyst will calculate an overall summary statistics that combines different information from different HRs. Then in the next set, the central analyst is going to return the information back to each HR data. And each of the HR will update its own summary. Update its own summary statistics based on the overall statistics. So, Sumshare has two rounds of communication. So, back and forth from each EHR to the central analyst, then back to the EHR, the factors analyst. Then, the central analyst will calculate the overall test statistics that combines all the information from all the EHRs. So, just to make clear of what type of information, some share. Type of information some pair is transferring. It's not the typical GWAS summer statistics, such as odds ratio or standard error. So, some share is actually transferring summer statistics of data. So, we're transferring, for example, the prevalence of the outcome, the minor low frequencies, and some sort of product of the outcome and the China data. So, we are transferring more information and competence to us, but we are not transferring the individual-level data. Data here. So to validate our algorithm, we applied the sum share to a consortium of multiple EHR data from the US called eMERGE. So eMERGE contains EHR data from multiple health systems and EHRs in the US. And the sample size is about 80,000 individuals in eMERGE data. And we also independently validated our results in the Kaobank data. And so, first of all, And so, first of all, we want to demonstrate through simulation data that some charity is actually lossless. So, in this analysis, we used the MERS data and we randomly extracted some SNPs from the across the genome and we subset the SNPs into different minor allele frequency categories. So we have a set of SNPs with a low allele frequency, some of the SNPs with a a little bit of common frequency and some of the SNPs with a much more common frequency. The much more common latency. And we did two types of analysis. So, in one type of analysis, we combine the individual level data from the eMERGE into one larger data set, and we perform one type of analysis. And in the second analysis, we use some share and calculate the summary statistics from different EHRs, integrate the different summary statistics, and do the analysis. And we compare the p-values from the two types of analysis. And the basis. And basically, the figure shows that p-values are exactly identical between the two types of analysis up to the 10th digit. So basically, essentially, they are identical. So, this simulation shows that some share is losses. We don't lose any information, even though we're not combining individual data. And the second analysis we want to demonstrate is that some share is better than the existing methods to us. So, in this analysis, we also We also simulated two types of phenotype pleiotropic associations. So, in the top simulation, all the phenotypes are independently associated with the SNEN. And the bottom simulation, the phenotypes could have correlations between them. And we compared some share with the VWAS. And the VWAS, we also did two comparisons. One is VWAS mega-analysis, so we combine the individual. So, we combine the individual level data and we do the overall few others. And we also combine with the VOS meta-analysis. So, in this case, each of the data, we did the VWAS, but we meta-analyze the odds ratio and the standard error across different data sets for the VWAS. And the sum share, similarly, we did two types of analysis. One is combining the individual-level data, and the other one is just using the summary statistics. So, if you notice, there So, if you notice, there are only three lines on this power curve. Basically, the sound share distributed and sound share pulled give us exactly identical results. So, these two curves for the sound chair exactly overlap with each other. And the different simulations represent different scenarios. So, when we do the simulation, we also make sure that some of the phenotypes could have positive correlations. Have positive correlations, some optimotypes could have negative correlations, and in some other cases, all the correlations are in the same direction. And in some other cases, there are only maybe one or two of the phenotypes are actually correlated with the SNPs, but the rest of the phenotypes don't have any correlations. Basically, through simulation, we show that some here are better than the existing approach in all the different simulation scenarios. So, after making sure So, after making sure that some chase losses and has better power, we apply it to MERS data. So, this is a kind of distribution of the case control standards across different MERGE data. So, we apply the sum share to five different cardiovascular diseases across eight different EHR data in the eMERGE. There's different distributions of case and controls for the different EHR data. So, there's definitely some hydrogen. Data. So there's definitely some heterogeneities across different EHR data. And the main result is shown in this table. So what we did is we analyzed different EHR data separately. So we only apply the algorithm to each individual data set. So what we see is when we only apply the algorithm to each individual data, we don't really see any significant social stress. Significant association. So, only one of the EHR data produced one significant association. And when we combine the two largest EHR data together, and we see there's 171 significant associations, and when we integrate all the EHR data together, so the sample size is much larger, and we have over 1700 significant associations. So, this kind of demonstrates the power of integrating different VHR data for integration. Different data for an integrated analysis because we have a lot more improvement in terms of our power to detect association. And some share is only designed to analyze binary outcomes, but we know that in the EHR data there is also continuous outcomes such as blood measurement, HDL, LTL, or cholesterol. So we also want to analyze them in addition to the binary. In addition to the binary outcome, and there might be also heterogeneities across different UHR data. We also want to handle the heterogeneities. So, we designed an extension of the sum share. We call it mix was. So, the setup of the mix was very similar to sum share, but the main difference is the mix was can model both continuous and binary outcomes together to detect the potential play trouble effects, and you can also handle data. And you can also handle data heterogeneity. So, I can provide one example of heterogeneity. So, when we detect SNP and phenotype associations, we have to adjust for the age as a compounder because the age is a very important predictor of whether someone gets a disease or not. But we can't just adjust for one age across many diseases when we analyze different diseases together. So, we have to adjust a different age. Have to adjust a different age for different phenotypes because the same individual could get diagnosed for one disease at a younger age, but for a different disease could be at older age. So that's one type of hydrogenities we have to consider when we model different phenotypes together also across different EHR systems. And similarly, we can show that on the left-hand side is the My Haddon plot when we analyze. Plot when we analyze each EHR data together. So, in this case, we have a few associations when we only analyze one EHR at a time. And on the right-hand side, this mechanic plot shows if we integrate the different EHR together, we see a much more number of significant associations, and they're more easily validated in an independent development idea. So, oh, I think I have two minutes left. Okay, so the remaining time, I will talk about some of the one of the methods we try to develop to improve the existing polygenic score method. So, this is one of the recent plots from a recent paper to compare different polygenic score methods. And you can see a similar comparison across different PRS methods. So, the general consensus is there's no one single PRS method that is uniformly best across OTCs. Uniformly best across OTCs and traits. So it will be very hard to develop methods that are better than all the existing methods. So our thought processes can be to simply improve the existing methods. So we don't have to completely replace the existing methods, but come up with something that can improve the current PRS method. So broadly speaking, the PRS methods are, there are three different steps when you do the PRS methods. Develop the TRS methods. The first step is to perform a QA to do variable selection to identify important variables. The main component of the PR is to reduce this correlation because the many SNPs are highly correlated. So we want to adjust the correlation. The final step is to do dimensionality reduction. So most of the PRS methods are focused on the middle part, is how to best induce the correlation among the states. But what we want to do is, can we improve the other parts of PRS? Improve the other parts of PIS calculation in terms of variable selection and dimensionality reduction. So, in terms of variable selection, most existing GUIs are performed under the added coding. So, we convert the number of LEOs of each person into this year 1,2 coding. But there could be other codings we can incorporate when we perform the TWAS, such as USS coding or download coding or cold encoding. So, that's one area we. So that's one area we can improve. Another area is right now, all the PRS methods are kind of reducing the entire genetic data into this one score. So that might be too restrictive to represent the whole genetic data. So can we have a representation that's not just one-dimensional score, but a multi-dimensional risk vector? So we develop a four-dimensional risk vector framework. So in the first step. So, in the first step, we're wrapping up now. Okay. Yeah. Okay, okay. One more slide. So, in the first slide, basically, you can use any of your favorite PRS method to calculate the PRS score. So, the main thing is, once you calculate the, after you calculate the PRS score, you can use the PRS score as input into the model. So, you can find potential interactions among the different PR scores. So, it's not a replacement of the PRS, but using some comparative methods to develop. PRT methods to develop different PR scores. And the main conclusion is: when we look at the multiple-dimensional PR scores, it actually performs better than just the individual score based on the added PRS. Okay, so that's the main conclusion. So these are some references. I can't take any questions. Thank you very much.