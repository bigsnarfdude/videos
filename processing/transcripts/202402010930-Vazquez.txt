I also want to thank you all for making this long trip and to be here. I feel truly honored to be able to share my research with some of the best people out there in sensorful areas. And on that note, I'll begin my presentation on evaluating robustness and efficiency of estimators for informative sensoring. And this is a joint work with a colleague of mine, Marissa Ashner, that used to be a PhD student in our department. She's right now a She's right now a scientist station at Duke, Tanya Garcia, Yan Yuan Ma, and Karen Margar. So I'll go ahead and begin the background and motivation for this study. Then I'll present a set of estimators that work under informative sensory. I'll wrap this with a simulation study and then an application to Huntington disease. So what is Huntington disease? Piggyback running from Sarah's presentation. From Sarah's presentation, it's an inherited neurodegenerative disease caused by a triple repeat expansion of the cytosine Adden 919, better known as CAG, in the HTT gene. It affects motor, cognitive, and behavioral skills. There are currently no treatments that delay your preventing onset of symptoms. And research in this area is difficult just because there's very high sensoring rates. These are the sensoring rates of five major studies of. Of five major studies of Huntington disease, and they're all around at least 80%, especially in VolHD, which has 88%, which is the study that I'll be using my message today. So to begin with, we're interested in this model, Y, which is a function of X, which is subject to random right sensory, Z, a set of fully observed covariates, and beta, which are the regression. And beta, which are the regression coefficients. m can be a linear on non-linear function and then epsilon is just the normal distribution mean zero sigma square and so the problem is that we observe w which is the minimum between x and the sensoring variable c and delta which is the indicator that x is less than or equal to c. So we want to find a way to estimate data using only the assertion. To estimate data using only the observed data. So, just a little bit of background: when I first started working with Tanya, I had heard what missing data is, but I didn't know how to tie it back to missing data. And so I read this paper, and I think that the first line does a very good job at explaining what coarse data is, which is what she said. Coursing includes, as special cases, rounded heat sensor, partially. That he sensor, partially categorized, and missing data. So we can see sensor data following or belonging to the same family, which is like you have coursing data and within that coursing data, you have missing data, sensor data, rounding. But they all belong to the same family. It just happens that missing data is the most popular form of coaching data, it was taken over the literature. Literature. And here I have an example. Let's say the true value of x is 10.56. With missing data, we don't really know anything about that. With centering, we at least know that it's less greater than 10. And with rounding, or some of you may call it measurement error, we know that the value is around 10.6, but we don't really know how much. But as you can see, missing data and sensory. Data and sensory, they're both forms of course and data, but we can extract a little bit more of information with sensory data, which opens an opportunity for us to be able to propose methods that might not work with the missing data literature, but we take advantage of that gain of information into our estimation process. So I'm not doing it justice here, but there's a wide variety of methods that work on the Variety of methods that work under uninformative sensory, meaning that C and X are independent or that C and X are conditionally independent given Z. We have complete case estimator, inferability, weighting, which was first centered by Roland on Monday. We have imputation, maximum likelihood estimation, and there's a review of papers that describe these methods. But the motivation for my project is that we can't just really. For my project, is that we can't just really rely on the assumption that c and x are conditionally independent on z. And do we always observe like everything that, let's say for example, this was true, do we always observe all of the z that would make this condition true? So it just seems like it might be a little bit difficult for this assumption to hold at times. So this leads to the assumptions under. The assumptions under this study, we assume that y and c are conditionally independent of x given z, which is a typical assumption that is used in this area. And then we assume that c is not independent of x conditional of z. And we can kind of represent this using DAGs. But we can see we have a lines going from Z to C and X, and we also have an arrow going to C to X. An error going to C to X. If we condition on Z, we still have a path from X to C. So this is kind of like how we can represent these assumptions. And I guess on the first assumption, if we condition on X, then we plot all of the paths from C to Y. And this is how we can visualize this assumption. So, going from 10 years professional development workshop, we have Workshop: We have a problem that Huntington disease develops slowly over time, and it's evaluating early symptoms of Huntington's disease as a function to time to clinical diagnosis difficult. Our proposed solution is to provide a framework of regression estimators for individuals to be able to choose from under informative censoring. And this will allow researchers to leverage potential for. To leverage potential for bias in exchange for exchangeability. So, in some cases, you might just be interested in effect estimation, not so much in efficiency. Or at different times, you might be more interested in efficiency rather than high. So, to begin with, we can represent the log likelihood in this form, which we have the contribution of Z, the contribution of the uncensored data, and then the contribution. Data and then the contribution of the sensor data. So when we observe delta is equal to one, then we need to integrate the component of c that we do not observe. And when we observe delta is equal to zero, meaning that x is greater than c, then we need to integrate this component of x that we're not observing. We have these set of nuisance parameters denoted by alpha, which basically govern the different. Which basically govern the distribution of F of C in X, conditional and Z. And we'll be going back to these news parameters. So the complete case analysis basically takes the partial derivative of the likelihood of the observed data with respect to theta. And this is the estimating equation. We can show it some bias. It's good because this robust specification of these three distributions. Specification of these three distributions, but the price that you see is inefficient. Then we have the reminding, because I know we've talked to a few people about when complete case will won't be valid. So I was wondering, so your assumptions are allowing for informative censoring without making the communication valid? It would. So you can still misspecify this and get correct if. Get correct effect estimates even under the presence of informative sensory. Okay. Does that answer your question? A little, but I'll think about that. If you go back one slide, one other one, go back, okay. Right there. So we're assuming Y and C are independent, so that allows that's that if we didn't have that, then we wouldn't violate the consistency. So we're Okay, the consistency. So we're but what's changed from maybe the the conditions we saw before is that now that's given like now X is allowed to be C and X are allowed to be not independent given C. Sorry. So in uninformative, is it true that you only have the one condition of C being independent of the first one not? Typically we still make the first assumption even under uninformative sensors. All right, then we have inverse probability weighting, which is we have the same exact thing from the complete case analysis. We just inversely weight every contribution proportionally to the probability of being observed. And the main difference between this IPW estimator to the one that Roland was presenting on Monday was that here are probabilities that. Here, our probability does not depend on y. It just depends on w, which it's only evaluated when delta is equal to 1, so it'll be x and z. And we can show that if we take the expectation of this thing, we don't really need to correctly specify the distribution of x given z for c given x and z to be able to get unbiased estimates for theta. Now, the problem. Now the problem is like knowing how to calculate this probability because typically you would just get the logistic linear regression using x and z as their covariance, but you don't really observe x all the time. So what we do is we take the maximum likelihood estimators of alpha using this log likelihood of W, delta, and z. And in that way, we can get an estimate for alpha. We can get an estimate for alpha and then just plug in that in the estimation for these probabilities. So, do we want this method is equally as robust as a complete case estimator? So, we can miscalculate this probability. And through conditional expectation, we can still show that even if we misspecify the probability, we can still get unbiased estimates for data, which is different than when. Different than when the probability pi depends on y. Because if you misspecify that probability and you have y as one of the predictors, the probability in the numerator and denominator are different and you will get bias estimates. Then we have this modified augmented complete case estimator. And for this one, Tanya and I, we saw this, well, it was initially proposed by Maurice. Well, it was initially proposed by Morisso, but we saw this paper and they had this documented component estimator. And we were just like, oh, well. And naively we thought, well, we can just exchange the rules of R, which in the missing data literature is like an indicator for missing. So we can just exchange the rules of R and delta and be able to compute this probability. So this was our claim going from this to the second line. From this to the second length, but we tried so many different things, and we were like, Well, the implementation is not right because we're seeing like it's less efficient than the completeness estimator. The variance was super high, so somehow we were getting like very bad coverage. And the coverage was decreasing as a function of n, which I thought was just totally ridiculous. So, we decided to like take. Totally ridiculous. So we decided to take a step back, go back and understand how you augment an estimator. And through that process, we define that we can augment the complete case estimator by this component here. And the most efficient form of this function is equal to this. And I won't talk about how we derive this right now, but after you can explain more. But I guess I didn't really. But I guess I didn't really explain this part here. So, this is the complete case estimator. Then we have this augmentation part. This phi can be any function of yz. It just has to be the same dimension as theta. But only by taking this form of phi, yz, you see gains in efficiency. So I can just put like anything here. I'll still get unbiased estimates, but that's not going to guarantee efficiency. That's my point to guarantee efficiency. But through some derivations, we show that by using this form of phi yz, we see those gains in efficiency. So it improves efficiency over a complete case estimator, but it's not robust to misspecification of FC given X and Z. Because if we misspecify that, when we take this conditional expectation, the probability in the numerator and denominator. The probability in the numerator and denominator are going to be different, and we just don't get a zero in front of this augmentation term. And it's called modified because we modified it from the missing data literature. Then we have the modified augmented IPW estimator, which has a very similar form. We have this IPW component, and then we augment it by the same structure. The same structure, it just happens that fee happens to be different. If we try using the fee from the augmented complete case estimator, we're not going to see efficiency gains, even though it works for the complete case. But after some derivations, we find that the optimal form of this AIPW estimator is going to be very similar, except we, instead of the expectation, we And inside of the expectation, we additionally divide it by this probability and then we take the expectation follow. So it improves efficiency over the IPW estimator. It's just not robust in the specification of f of C given X and Z. Then we have the full likelihood estimator, which we basically take the partial derivative of the okay. Do you always observe the C or is it only observed? Or is it only observed? That's an excellent point. We only observe it when we have a sensory observation. Then I don't understand why you can identify the And I think on your project you use the same likelihood and that's because we can use this joint likelihood. Joint likelihood since we have this joint distribution of f of c x and z and it's a function of alpha, we can maximize this lock likelihood to find these estimates for alpha. So even though we don't observe x and z simultaneously, we can still make use of this log likelihood to find those parameters. And at the end of the day, we just plug in x when delta is equal to x. plug in x when delta is equal to 1 and c when delta is equal to 0 to find the estimated x and c values. So would that also work if you have sensoring on the response? This is something that works under sensoring and exon. What makes it different from the case with Oh, this we're only investigating the case when we have sensory on X, not the response. Okay. Go ahead. So you did have kind of four estimators that are really kind of built on each other. And I was wondering if it would be fair to say, like, could you rank them by efficiency? Is it like complete case modified up by? Is it like complete case, modified augmented, complete case, IPW, modified augmented IPW? Yeah, and that's an excellent point. And I'll show it a figure that I made. Sorry to jump. Oh, no, you're good. Thank you. Cool. Do you have a question? No? Okay. All right. So, yes, the partial derivative of the luck likelihood of the absorbed data, and this is a form of phi of the MOV estimate. The MOV estimator. So you see, so we know that if it's correctly specified, we're going to achieve the highest efficiency from all estimators. So your point is like, oh, I want the highest efficiency, and you're pretty sure about these conditional distributions, go for this. But it's not robust enough specification to these two distributions. And jumping on what Sarah was asking, we can so there. We can, so this is a figure. So, the higher it is, the more robust they are, and the more to the right they are, the most efficient they are. So, we can put IPW and complete keys at the very top. So, they are the most robust, but IPW is less efficient than the complete keys estimator on this scenario. Then we have the MAC estimator, which is robust to f of x given. Robust to f of x given z and f of z, but is more efficient than the complete case estimator, but less efficient than the MOE. And the MOE is not robusting a specification of F of C given X and Z, F of X and Z, and yeah, only those. And then we have the AITW estimator. We can only show that it's more efficient than the AITW. That is more efficient than the AIPW estimator, but it lies somewhere between these two points. So we can't really make any efficiency claims between how efficient this compares to the complete case analysis and the modified augmented complete case. So in a theory, so we did a small simulation study in which we simulated X, C, and C using a multi-their. X, C, and C using the multi-direct normal, we pick the covariance matrix in this way such that the partial correlation of X and C given Z is not equal to 0. Then theta is equal to this, and our interest is in theta 1, which is equal to 3. Number of observations was 500, and the simulations was 1,000. So the red line here indicates Red line here indicates the oracle value of three. We can see that the oracle estimator, in which we just plug in x, it's unbiased and it's like pretty narrow, so very efficient. Then we have the naive estimator, in which we plug in w and run the linear regression, which as you can see will lead to bias estimates. We have the complete case, which will lead to Lead to unbiased estimates. But it is a little bit wider than the oral estimator. Then here on blue, we have the MAC estimator. And as we can see, it's a little bit narrower than the complete case. But when it's correctly specified, it's unbiased. It gains efficiency. When alpha is worn, that's the distribution of X given Z. As I was explaining, we can still achieve unbiased. It does not depend on. Achieve unbiased. It does not depend on that. But when we misspecify alpha 2, which is the parameter for the probability, we see that our estimator becomes some bytes. And we misspecify both X conditional and Z and C conditional on X and Z. We see that the bias gets a lot worse. This is the IPW estimator, so even when you specify So, even when we misspecify alpha2, we still achieve unbiasedness. And the NAIPW, which augments the IPW. So when everything is correctly specified, we see some slight efficiency gains here. We can specify alpha 2, the same thing as the complete case estimator, and still achieve unbiased efficiency is not guaranteed. And then we can specify alpha 2. We see some slight. specify alpha 2 we see some slight bias and when we misspecify both distributions we see a lot more bias and finally we have the the MOV and as you can see when it's everything is correctly specified that bot spot is very narrow which in the case that is the most efficient estimator when we use the observed data. And when we misspecify that joint distribution, we see some bias. We see some bias. This is a web plot of the 95% coverage. So the red-dotted line indicates 95% coverage. So the only estimators that deviate from that is the MOE. When they're both misspecified, we see that we don't achieve that 95% coverage. The same thing goes when, on the MAC estimator, when we misspecify alpha 1 and alpha 2, we see 1 and alpha 2, we see a decrease in coverage, but for the most part, we see that all of these estimators under this simulation study they're performing like pretty well. So, application to Huntington disease. So, enroll HD study. Go ahead. Sorry, I was wondering what were your censoring, right? So, you remember that state? 50%. Okay. Because I guess I'm concerned that when you get rid of 100% censoring, that the estimating distribution of X given Estimating the distribution of X given would be really challenging? Yeah, it would be like a lot more difficult. That changed as much? I mean, well, surprisingly, under the simulation study, beta 1, which is the coefficient related to the centering variable x, we observed this, but when we observe the results for beta naught, or the intercept, like the results are a lot more dramatic. A lot more dramatic. But yes, with higher sensoring rates, we do see some of these results being a lot more dramatic. So we applied these methods to a MOLHD study, which is a non-interventional observational study, designed to identify the earliest symptoms of contacting disease. And so we decided to fit this model, which is the SVMT score, which is basically a score between Which is basically a score between 0 to 124. And the patient comes into a clinic, they're given a legend with shapes and figures with a number assigned to them. And they have to match those figures to another legend that only has numbers. So they give them five minutes, and in those five minutes, those four is determined by the amount of correct matches that you can use. Correct matches that you can use. So someone that is like healthier would get a higher score. And then we have time two diagnosis, which is basically evaluated at a clinic visit. What was your age? And then we subtract the age at clinical diagnosis. So time 2 diagnosis will be negative because your age at diagnosis will always be higher than your age at clinic visit, assuming. Clinic visit, assuming that we're only evaluating those people that are at risk, meaning that they have not been diagnosed. And then we have our CAP score, which is a measure that is indicative of how far along have you progressed on Huntington disease. And it's basically a product of your attack score multiplied by age. And for this study, we decided to extens it at 1500 and scale it by. At 1500 and scaled by 100. So, inclusion criteria is individuals who are genetically confirmed to have Huntington disease, not diagnosed with Huntington's disease at baseline because we only want to evaluate those at risk, and no missing SDOT or CAP scores. So, by applying these, and I guess the last one is we only analyze Only analyze observation map basically. Okay. So we're missing STMT and CAP scores. Do you know like how many people had excluded? After it wasn't that much. I think I knew the exact number, but it was in like, oh my god, we had 10,000 people and then we narrowed it down to like 3,000. But it was like something small. 5%, perhaps. Yeah, maybe like even less than 5%. So centering rate after applying all of these inclusion criteria was at 85%. The mean SDMT score was 48. So already pretty low. Time to diagnose was 1.9 years. So this means that at baseline, on average, individuals were two years away from a clinical diagnosis among those that were diagnosed. And then the mean CAP score was 1700. CAP score was 1700. It was a majority white group. This is the distribution of SDMT score, time to diagnosis, and the CAP score. Among those that observed HIP clinical diagnosis, this was the age. Among those that were censored, this was the distribution of the age. And this is the distribution of the type score. Of the type score. So looking at this, I was like, well, it seems like this is fairly normal. This is fairly normal. Or at least it's not screaming. I'm not normal. And then this is the cap score, which also screams. I'm not normal. So I decided to employ a multi-bay normal distribution to model X, C, and Z using that observed likelihood that I was explaining for the X W case. When you put the IPW kicks and these are the results. So we can see that the effect estimates for the intercept are all like very similar. When it differs, there's like this beta coefficient for time 2 diagnosis. So the complete case analysis is telling us on average, an individual that is 10 years away from being clinically diagnosed, their SDMT score. SDMT score is about 20 points higher than someone that was currently diagnosed at that claimed visit. And as you can see, all of these estimates are very similar. The MOE is telling us that that value is actually only 10 points. And the AIPW does not make a lot of clinical sense. They're saying that actually someone that is closer to being clinically diagnosed, they have a higher F. Clinically diagnosed, they have a higher FDMT score. Which here it shows that there is a leverage for efficiency versus bias. If you are only interested to getting an effect estimate, just go for the complete case analysis. You can say less inefficient, but it is one of the more robust methods. If you're trying to plan out a study and sample size is an issue, then you can make a lot of stronger considerations. Make a lot of stronger consideration for use and efficiency. But you can see how providing this estimate to a research could be harmful and really affect the research for future generations if you're, okay, I just want to employ my modified AIPW estimate. Luckily, it's not significant, but as you can see, you might have a wide variety of estimates if you could mispecify the distribution. Is specify the distribution of any of these estimators. So, in conclusion, I feel like most of the conversation around the complete case is focused on whether or not you make consistent estimates on the censored covariate. So, I'm mostly wondering, the MLE is super different in a CAPS core, even though clinically you would have expected that, maybe. So, are you aware, do we trust the complete case for the additional covariate? I could maybe see. Covariate, I could maybe see, I'm not sure. Yeah, I think that's like a valid question. And when you look at these numbers, I'm not, no one knows what is going on behind the scenes and which assumptions you might not be satisfied. And that's like the importance of working very closely with a collaborator that might be more knowledgeable about what assumptions you can make in that meeting. But just by looking at the knowledge, But just by looking at the numbers, it's a puzzle that might not have an answer. All of these are estimators trying to give you an estimate for that one beta. And as I was explaining before, they're just varying in assumptions. And I guess going back, if you're only interested in the relationship, does it increase or decrease? And you're not interested about inference or confidence intervals or efficiency, then Or efficiency, then complete cases like the way to model. I guess my question, though, is: is that true for the entire model or just for time-to-diagnosis? And that maybe doesn't have to just be a mystery question, but I don't look into the complete case all that much. Okay. Do you have an idea what might be violated or why the MIAIPW is cornered source? AIPW and the score. So first two parameters? Well, straight off the bat, my answer is that I'm not sure. But I can say that typically when the estimates for the complete case and the IPW are very different, it's because on that probability, there might be like some lingering information about why that is making your That is making your estimate for the beta slightly different. But through proofs, we can show that if the probability is only dependent on x and q, then your estimate for complete case analysis in IPW should be relatively this. Well, going through this process, I have kind of thought about a specific way, a test that we could use, but I still need to write it down. But this idea just came up to me a couple of days ago. But excellent question. For the regression models, so why do you treat the assessment scores as the response and the time diagnosis? At the time of diagnosis and the requirements? Is it at the time of diagnosis if the target is wrong estimate? Well, for clinical trials, we kind of want to evaluate the progression of the condition as a function to a time-to-diagnosis. And that's because we want to, well, one, we want to use time to diagnosis as opposed to age because we want to eccentric everyone along the same time. Center everyone along the same time scale. Because someone in the study might be like 40 years old, they joined the study, and they might be like closer to the time two diagnosis. Whereas we can have a 19-year-old that joins the study, and their time scales are very different. So the interpretation of this beta coefficient might be could lose precision and mean. So, one, that's why we decided to use time two clinical diagonals and then going. Diagnosis. And then going back to your question, we decided to use time-to-diagnosis as a predictor because we want to see how the progression of the condition is related to how close you are to being clinically diagnosed. And that's useful because then we can do an interaction for those studies that are interventional and see how the And see how the disease progresses for individual somatic treatment versus not treatment. Just point back to assumptions. There's any kind of weight cross assumptions that we, I don't know, like do you or maybe like cover all the cases in a way? So we try to like identify like So we tried to identify ways about how to make these estimators doubly robust, but because of the relationship between X and Y and how X is carrying information of the censoring process, that additional augmentation term, the expectation is maybe for the service. So, one, that's a dropback of like this sensory field that we're interacting in. And second, interacting and second we can apply imputation techniques like defining like the defining a model for the probability distribution of x given z and then use those values and then just plug it in and just do a regular linear regression. They do have some benefits and drawbacks. That's the type of work that my Kyle here is doing. And so yeah, there's a So yeah, there is a wider we're right. I know these are designed for the informative censoring case. I may have missed this, but is that the quote-unquote harder case? And so these are robust if it is actually uninformative censoring, or do these work only under informative censoring and would not work under uninformative censoring? From my dissertation, I first present. My dissertation: I first present these estimators, and under uninformative sensoring, we can relax some of the assumptions. So, it might be easier to get unbiased estimates with less assumptions. And I guess just to conclude, and then I'll open it to questions, we were able to establish a framework of estimators for regression with the sensor covary on the informative sensory. The informative sensory. Two, there is a robustness efficiency trade-off that we were able to establish between these estimators. And this will allow researchers in Huntington disease and other neurodegenerative diseases to have a bigger toolbox either to plan studies of Huntington disease or just analyze observational data like how I did in this project. Further research needs to be done making these less computationally difficult. Computationally difficult because, as you saw for the augmentation terms, there is expectations that need to be calculated for each individual. So that can sum up pretty quickly. And second is to define a totally robust estimate. In this case, we were able to augment estimators that were robust. We were able to augment them with, to make them more efficient, but we added a little bit more of like. But we added a little bit more of like assumptions that we need to meet. And finally, I want to thank my collaborators and my dissertation advisor, Tanya Garcia. Thank you again. The Garcia loves each member's my funding sources. And yeah, if you have any questions and would love to stay in contact, please reach out. And with that, I'll take any questions. One question, and then open or break my questions.