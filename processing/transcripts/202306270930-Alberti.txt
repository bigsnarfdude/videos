Thanks a lot, Demetrio, for the introduction, and thanks to you and Tatiana for the invitation. I'm very, very happy to be here. So, today I'm going to talk about the joint work with Matteo, who is here, Alessandro, who is in Genoa too and is one of our PhD students, and Ivan Trapaso, who is now at the Politecnico di Torino and is a former postdoc of ours. So, the outline of the talk is as follows. I'm going to first introduce the problem, namely the sparse. The problem, namely the sparse Radon transform, which basically means the Radon transform with a finite number of angles. Then I'm going to do a quick section on the classic, basic ideas of compressed sensing. And then basically I'm going to sum one and two and get three, namely compressed sensing for the sparse Radon transform. So let's start with the Radon transform. So basically, we had three talks that touched upon the Radon transform. The first one was by Radon transform. The first one was by Alan Griff, very you know complicated, non-linear in a PDE setting. Then there was Filippo yesterday, on still linear and classical radon, but in a generalized settings with group. And then there's me, very basic 2D lines. So the super standard thing. So the easiest thing you can think about, you have an image U and you integrate it along lines across. integrate it along lines across it so more precise sorry so basically this is the sinogram theta is the angle that's one and s is the position okay so you integrate the function along many many angles and for many s's so that's radon i suppose all of you know what this is so i'm gonna be quick here so i suppose my functions to be compactly supported in the unit ball in r2 and if i fix theta so i fix an orientation theta and then r2 Orientation theta, and then r theta is simply the Radon transform along all projections for a fixed angle theta. So this quantity here, this is going to be called R theta. And then if I, so that's fixing theta moving S. And then if I let theta change as well, what I get is the full Radon transform, namely simply R. Okay, it goes from L2 of the ball to L2 of S1 times minus 1, 1. All right. So, you know, there could be many. So, you know, there could be many things. I could say many things about the Radon transform. Let me just say one thing that I think is key and it tells many things about Radon. And that's this formula here that tells us two things. The first one is that indeed the Radon transform problem can be solved because R is injective, because R is linear. And if R of U is zero, then U is zero as well. So you can solve this problem. But there's a smoothing effect of the radar. There's a smoothing effect of the Radon transform because of those integrations. So you are killing some singularities. And this is codified in this power minus one half. So basically, if you have a function in a negative sub-level space, its image through the Radon transform becomes in L2. So it means that you're smoothing a little bit singularities out. And so the inverse problem is imposed. And I'm going to use this thing later. To use this thing later. All right, so what's the sparse Radon transform? Well, here, basically, the idea is that I don't consider all possible angles, but I consider only a finite number of angles. Okay, so for example, you consider theta one equal to zero. So you consider vertical lines, and basically this means this dashed portion of the cynogram. Then you can consider maybe 30 degrees, and this is this line here, 60 degrees. Here, 60 degrees, 90 degrees, horizontal lines. Okay, so you pick a few angles, and then now I picked four. Now think about picking M angles. So I consider M angles, finite number of them, and I sample them at random, uniformly on the sphere, okay, on 0 pi, 0 to pi. It doesn't really matter. Okay, think about 0 pi. So that's that's the idea. I sample them IID with the uniform distribution on S1. On S1. And then the inverse problem is as follows. This is the data. This is just, I'm repeating things. U dagger is my actual function I want to recover. The unknown, yeah, the unknown is u dagger in L2 of the ball. And since I have subsample measurements, right, I only have those projections with a finite number of angles, I cannot think to recover all possible u dagger in L2. So I need Dagger in L2. So I need some sort of a priori information on New Dagger. So this, of course, could open a long discussion on what this a priori information should be. And I follow a classical path in signal processing where sparsity plays a crucial role. And so I assume U dagger to be sparse with respect to a certain family of functions that I'm going to discuss, I'm going to mention later. Okay, that they're going to be wavelets anyway. Okay. So u dagger is. Okay, so your dagger is sparse. So let me see what's been done. Let me say what's been done for this problem. So, again, here is just a summary of the problem. Sub-sample radon, I want to find you, Dager. That's the problem. And now, on this problem, there are many what I call like empirical results. Namely, you know, you do some numeric simulations where you actually try to understand what you can recover from M. You can recover from M measurements, maybe assuming some sort of sparsity. Okay, these are several works. For example, I just see this title, which is quite intriguing, how little data is enough. Basically, it tries to understand what's the link between the number of angles and the complexity in some sort of way that is measured in some way of your dagger. So, these are, let's say, empirical works. Let's say empirical works, and then theoretical works where we tried to dig in in the literature and we couldn't find much about the precise link between number of measurements and number of angles and Eudagger and complexity of Eudagger. And so we try to dig in a little bit more. And to us, the main question we would like to address is: as I said, a link between the number of angles. A link between the number of angles, what's called the sample complexity of the problem in mostly machine learning, and the sparsity of Eutager. So, this is our main question. And immediately, you know, the theory of compressed sensing comes to our mind because somehow compressed sensing is indeed about linking sparsity with number of measurements. However, it turns out that things for the Radon transform are not that simple. And for example, That's simple. And for example, let me just mention two papers here. This one says compressed sensor connects the critical number of projections, so this one to the image sparsity, this one, but does not cover computed tomography, namely radon. Empirical results, that's what I showed you in the previous slide, suggest, however, a similar connection. And then there's another reference that says basically the same thing. We use simulation studies to provide the foundation. Simulation studies to provide the foundation for the use of sparsity in computed tomography, where, unlike compressed sensing, it is not possible to give rigorous proofs. And so we were somehow intrigued by these statements, according to which it was not possible to do some theory of contrast sensing for CT. And so we said, well, is it really true? Can we do something about it? And well, the answer is yes. And I think that's going to be the The following of the talk. And now, maybe you would expect me to give like a main result now, and then maybe comments, ideas of the proofs, and other extensions and simulation or whatever. But you know, I'm maybe mean now. And so the main result is going to be at the end. So you will have to suffer with me a little bit. But let me just give you a very quick spoiler on what the main result is going to say. More or less, it's going to simply. More or less, it's going to simply say that the number of angles is going to be directly proportional to the sparsity of the image, as more or less it happens in standard and basic compressed sensing theory. All right. So, this brings us to the second section of the talk on basics of compressed sensing. So, here, this is all, let's say, classical material. It's not related to Radon, but I'm going to need those ideas mostly and tools to then. Tools to then deal with the problem of the compressed sensing for router. All right. So the setup is the following. I have an unknown signal, U dagger, more or less as before. I'm describing here the finite dimensional setting. U dagger is in Rm, and I have a linear forward map with little m measurements. Okay. And this linear map is, of course, each component is simply a scarab. Course, each component is simply a scarab product between u and a certain function, psi l, since it's linear. Okay, now the number of measurements is, of course, little m is smaller than capital M, as it's obvious. And as an example, you can think of A being a sub-sample Fourier transform. So psi L are some trigonometric polynomials, not all of them. If you take all of them, then you just get the FFT basically. So there's not much. The FFT basically, so there's not much you want to say about the problem. If you take only some, then you're missing some frequencies, and you have to think about how to invert them up. Okay, so that's the idea. That's the U-dagger. And assume you measure only those white pixels and the black, you don't measure them. Okay, so it's sub-sampled isometry. In this case, a sub-sampled Fourier transform. Okay, so what's the problem? Well, you know A of U dagger, so you know partial in this example. Partial in this example Fourier measurements, and you want to find your dagger. So, what's the issue here? Well, of course, the issue is that in its generality, this problem is impossible as soon as little m is smaller than capital M, because this map is linear, you have a kernel, and so it's impossible to find a unique solution. Okay, so the idea of compressed sensing, well, the two main ideas of compressed sensing are: first, use sparsity. First, use sparsity. So you assume something on U dagger. You're not interested to recover any possible U dagger in R capital N, but only those that are sparse. And the second solution is that the inverse map is not linear, even though the map A is linear. Okay, so you want to find a way to recover U dagger in a non-linear fashion by a minimization problem I'm going to mention later. Okay. Later. Okay. Now, one of the main ingredients, as I said, is sparsity. So let me say a few words about it. Sparsity, in my language, is always going to be, at least in this talk, with respect to a fixed orthonormal basis of the space. So you fix phi m, an orthonormal basis of Rm. And you take the analysis operator, namely the change of basis between the canonical basis and the basis of the phi's. You take just color products with the phi n's. Color products with the phi n's. And then you consider the set of S-sparse signals. So this is the S0 norm, namely the number of non-zero coefficients of the vector phi u. And then you take a sigma s, the set of vectors, so that the zero norm of phi u is smaller than s. Okay, so in the basis given by phi, you only have up to s active coefficients. Okay, these are s-par signals. All right, now in practice. All right, now in practice, you can imagine that real-world signals will not be exactly sparse because this condition is nice mathematically, but not very practical. The zero, the equal and different, you know, they're not very precise concepts. So in practice, you only have compressibility, which means that your signals u will be something sparse plus something small. Okay. And the compressibility concept is very well seen in this example. This is Seen in this example, this is the JPEG 2000 compression standard where you just take a wavelet basis, so the phi's are wavelets, basically, and you kill all coefficients that are below a certain threshold. In this case, you kill all, I mean, you kill the 99% coefficients that are small or less. You keep only the first percent largest coefficients in this image here. And, you know, for you, it's going to be impossible to distinguish the two from here. Yes, you see the difference. From here, yes, you see the difference, but still you're saving 99% of the space. Okay, so this is compressibility. It's not exactly the same. The difference between these two images is more. Okay, U is the original one, the left one, V is the right one, okay, the sparse one, and the difference is more. This is compressibility. All right, and now, okay, let me just to summarize, I have two ingredients so far. The psi l's are the functions. The psi ls are the functions with which I take measurements, and the phi n's are the functions with respect to which my u is sparse. So I have two families. Okay, in MRI, you may think of psi l as being trigonometric polynomials like Fourier functions and the phians like wavelets. Okay, you can suppose that to have a theory, you need to understand the link between these two families. These are the two things I have on the table, these are the ingredients. Things I have on the table. These are the ingredients. And so, most likely, there has to be a sort of relationship between the two, okay, that has to be satisfied. And this is coherence, because in general, sparsity alone is not enough for compressed sensing to work. Let me just give you a very simple example. Suppose that both psi s and phi n are the same basis and they are the canonical basis. Okay, so basically, sparse vectors here will just be a train of deltas. Will just be a train of deltas, okay? Many, many zeros, only a few active coefficients, okay? So, suppose this is the u you want to find, okay. And now, suppose that you are unlucky enough that your measurements A are given exactly with the same basis. So, again, the psi l are the canonical basis. So measuring in the canonical basis means that you just take pointwise values of these signals. Okay, so this signal has sparsity one, two, three, four, five. Okay, how to say. Okay, out of say 64 maybe coefficients, you can imagine that to recover that signal with pointwise measurements, you need 64 measurements. You need all of them. Okay, because try to take a few random measurements here, here, here, here, and here, you measure only zero. There's nothing you measure. Okay, and if you're lucky enough, maybe you hit one of some of these, but most likely you will miss some others. Okay, so you need to measure everywhere. Okay, so in this case, it's not possible to do. That's not, it's not possible to do compress sensing. So, this is why the πN and the ψL, they are the same basis, so they are fully coherent. So, in order for compress sensing to work, you need some sort of incoherence between the two bases, which can be measured by the absolute values of the scalar product between the two families. Okay, so in this way, it's easily calculated that ideally you want to have one over square root of m as the root of m as the as the coherence between those two families. For example, if you have Fourier and a canonical basis, namely deltas, then you actually get this bound here. So this is the ideal setting. All right, so now let me go and finish this section on compressed sensing with the main recovery estimate in the classical setting that goes as follows. So u dagger is the unknown signal. Again, it starts with recovery. Signal again is sparse with respect to the phi n's, and those are the linear measurements. Suppose that they come from a sub-sample isometry. Okay, for example, in the Fourier case, this corresponds to a sub-sample MRI problem, just as a simple example. The measurements are y equals AU dagger. And as I was mentioning before, this non-linear optimization problem, sorry, this. Problem, sorry, this non-linear recovery map from Y to U. And this is given by this minimization problem here, where among all possible solutions of this equation, recall that you have less measurements than the number of unknowns. So this equation here has a big kernel. So this map here has a big kernel. So this equation here has infinitely many solutions. Among all these infinitely many solutions, you'll pick the Solutions, you'll pick the sparsest one. Okay, the sparsest one would be the one with a zero here, but then you relax the problem and you put a one to make it convex. Okay, but this is not the focus of my talk. So here, putting one is a standard thing, and you pick the sparsest solution among all those possible solutions. And the theorem says that if you measure enough, and you see there's a precise relationship between the number of measurements and the complexity. And the complexity of the sequence, namely its sparsity. And the constant here is b square, b is the coherence, m is the dimension of the space. So in the Fourier case, b is one over square root of m. So these two things kill each other, and you're left with only s. Okay, so this is the ideal setting in Fourier. So if this is true, then with high probability, indeed, use use star, namely, U star, namely the unique minimizer of this thing is exactly u dagger. I mean, this thing has a unique minimizer and is u dagger. Okay, so that's the theorem. Again, there's a precise relationship between complexity and number of measurements. So this is exactly what we would like to obtain also with Radon, a link between number of angles and complexity of the image. All right, so now let me go to the key section of this talk. Section of this talk, namely how to use these techniques to the sparse Radon problem. Okay, let's go back to the sparse Radon transform. Again, you have the Radon transform of U dagger only in theta1, theta m, and you want to find u dagger. That's the problem. Now, let's see why, in my opinion, somehow, you know, those works claimed that it was impossible to apply compressed sensing to radon. Apply compressed sensing to Radon. Indeed, somehow with Radon, you go outside the framework, the classical framework of compressed sensing. Let's see in what ways you go outside the classical framework. First of all, Radon is with u dagger in L2. So you are not in RM, you are in a function space. And Radon itself is modeled in an infinite-dimensional setting. Okay, so of course you could discretize everything from the beginning, but somehow it wasn't our aim. So we wanted to work. Wasn't our aim, so we wanted to work with functions, and so this was the first obstacle that is not covered in the theory I discussed to you before. But actually, it turns out that this extension of classical compress sensing to the infinite dimensional setting was indeed done. Here, I'm just putting basically, I think, the first two works doing this. For example, Clarice, her PhD student, is like exactly in this direction, as a key development of this. Right, as a key development of this 2016 result. So, somehow, infinite dimension, it was easy. Well, it was easy. You know, we could borrow ideas from these works. So, this aspect here is fine. Then, second obstacle. Here you see we do not have scalar products with functions. Like in Fourier, you take scalar products with Fourier polynomial, with trigonometric polynomial. trigonometric polynomials. Okay, here you have the sinogram and you set you just take pointwise values. Okay, so you evaluate a function in one point. So you have pointwise values and not scalar products. It turns out that this has been indeed investigated in the compressed sensing theory. Here I'm just putting one work. I apologize for missing maybe some relevant references, but this has been done. So this is possible to move from compressed sensing and interpolation. And interpolation easily, let's say. Okay, next, we have vector-valued measurements. For each angle here, I suppose, so I rotate the machine, I fix an angle, for each angle, I suppose to measure all corresponding translations given by S. Okay, so each angle gives me an infinite-dimensional measurement in L2 of minus 1, 1, namely all possible translations. Okay, so this means that I have vector-valued measurements while the Measurements while the setup I discussed before was only in the scalar, in the scalar case. But it turns out that indeed you can write things down and basically everything works the same. For example, here I'm just citing one paper that does actually much more than what we need, and in which this structured acquisition, this was applied, I think, with MRI. But again, this structured acquisition means that indeed for each measurement, you take not only one scalar quantity, but the full value. One scalar quantity, but a full vector of data. Okay, so this was still okay. Okay, so these three problems: infinite dimensionality of the domain, infinite dimensionality of the measurements, and the fact that you have pointwise values somehow by putting all these together was okay. These weren't big issues. However, there were two big issues that I'm going to focus on now. And the first one, and they are both related to the And they are both related to the presence of R here. Okay. So the first one is that the forward map R affects sparsity. Okay. What do I mean by that? Well, I mean that I assume U dagger sparse, but then I don't measure U dagger directly. I don't want to interpolate U dagger directly. I don't measure scatter products of U dagger with something else. There's R in between. So R transforms my U dagger. So this affects sparse. So, this affects sparsity and it has to be dealt with in some way, and I'm going to discuss how. And the second issue that is again related to the presence of R, and this connected to what I discussed in one of my first slides, is connected to the fact that R is ill-posed. So R is a smoothing operator, is a compact operator, that there was this one-half parameter, and this is expected to create some problems for the inverse problem. Okay, typically. For the inverse problem. Typically, in all compressed sensing results, the map A was a sub-sampled isometry or at most a sub-sampled isomorphism, let's say. Not a compact operator as in this case. So we have to understand what happens here. Okay, so now before getting to the main result, let me address one and two. So the forum up R affects sparsity. A paraffect sparsity. So, as I discuss, the typical a priori assumption is U-dagger sparse or compressible. However, here there's a map in between, namely the Radon transform. Let's call it F, for a general map F. If you transform U dagger with F, F, U dagger might not miss parts with respect to a reasonable dictionary. Okay. However, the solution, as I'm gonna detail in the next slide, As I'm gonna detail in the next slide, the solution is that many dictionaries and operators are some sort of compatibility between the two. Think about F being diagonal. If F was diagonal in the basis given by the sparsity of U dagger, then you apply a diagonal map to a sparse vector, and then it's still sparse, the output. So the diagonal property may be useful, and that's exactly what. It may be useful, and that's exactly what we see here. It's not exactly diagonal. We call it quasi-diagonal. And let's see how it works. So this is just the estimate that I gave you in the first slide, where B is one half. It tells us that radon, there's a smoothing effect of weight one half with radon. So the L2 norm of the radon of U is comparable to the H minus one half norm of U. Okay, so it's an. Norm of U. Okay, so it's a negative Sabolev space, and that's it. That's okay. Then this is about radon. Then let's see about the family of phi's with respect to which our U dagger is sparse. I assume that this family is a double index family. Think about wavelets, for example. J is the scale. So J is the scale. So I consider finer and finer scales, and N is a translation parameter. And I assume that this family. And I assume that this family and wavelets they satisfy this property. I assume that this family satisfies a little vulpely property. So that's the little vulpely property. So let me explain what this is for those who are not familiar with it. Think about B being zero. If B is zero, this is just L2, and this thing is not here. Okay, so this is just the Parceval identity for an orthonormal basis, okay, with L with B0. Now I'm saying, okay, let me take a look. Now I'm saying, okay, let me take a negative power here. And then I'm just saying, okay, I have to weight the high, the low scale, sorry, the fine scales with a weight. Okay, so think about in Fourier, this would be natural. If you have a Fourier expansion, you want to calculate the norm of the gradient, you just put a frequency here, a suitable power of the frequency. Here, it's more or less the same. Instead of putting the frequency, we are using. Instead of putting the frequency, we are using wavelengths, and so you put the scale with the typical exponential weight. Okay, so basically, if you have a negative subl space, then high frequencies matter less, okay? And so you weigh them less. If B is positive, think about B being, sorry, if B is negative, so in H1, okay, so B minus one, here you have weigh, you put higher numbers to the higher frequencies. Numbers to the higher frequencies because you're measuring gradients. Okay, so this little good Paley property is natural and it works for wavelets. And now, if you put these two things together, well, you remove this and you immediately get this. Okay. And if you look at this, this doesn't say that the Radon transformer is diagonal in the wavelet basis, also because it goes into another domain, so it wouldn't make sense. Into another domain, so it wouldn't make sense. But in terms of norms, it acts more or less as a diagonal operator. Okay, because you see that each component in the phijn is multiplied by this function. Okay, so in terms of norms, the radon transform acts more or less as a diagonal operator. Okay, the action in terms of norm of radon is comparable to that of a diagonal operator, in which the coefficients in the diagonal depend only on the scale. Depends only on the scale, on the wavelength scales. And so, somehow, this is useful because, as I was saying, our problem was that the format R affected the sparsity. But for the discussion, for what I explained before, in this way, the sparsity of U-dager is somehow moved to some sort of information on the sparsity of the Radon transform of U-dager. So, that's good. All right. So, the second. All right, so the second issue that I mentioned was on the ill-posedness of the Radon transform, which is expected to create some problems. So let me just be very quick and be cryptic here, maybe. But for those of you who know what the RIP is, the restricted isometry property in compressed sensing, this tells that think about delta being small. Think about delta being small. This tells that the action of A is more or less an isometry, okay? Because A of U is almost the norm of U. Okay, but the restricted word here means that this is not an isometry everywhere. This is impossible. Remember, A has a kernel, so it cannot be an isometry because A has a kernel, but restricted means that it's an isometric only for the sparse vectors. Parse vectors. Okay, so this is one of the key ingredients on which the compressed sensing theory is based. So basically, in compressed sensing, typically you show that this property here is satisfied for M sufficiently large with high probability. And once you prove that, everything works. Now, we have a nil-pose problem. So it's very unlikely that we could show a property like this. You can imagine that a property like this is, you know, being an isometric. Is, you know, being an isometry here, it would correspond to when, in the full measurement case, to an isometry of A. Okay, but we don't have an isometry, we have a compact operator. So how do we fix this? Well, the ideas that we borrowed from this paper was to use a generalized RIP in which you cannot aim, as I said, of having an isometry here because of the imposeness of A. Remember this. this inequality here. So there is an intrinsic filposeness of the operator. So instead of putting, sorry, instead of putting u here, you put G U, where G somehow makes high frequencies smaller, let's say, large frequencies smaller. And G is exactly this operator here. So G is, let me simplify, G is the Radon transform itself. By G is the Radon transform itself, more or less, up to simplification. Okay, because you just need to make it self-adjoint, truncate, and take the square root. Okay, so just as an intuition, we think about GS being the radon transform. Okay. And in this way, it's possible somehow the ill post-ness, the issue of ill post-ness is cured, okay, is removed. All right. So now I've collected all the groups. Something like this, yes, yes, exactly. Um, the main result, so having collected the ideas that are behind the results now, it's time to state it. So, it says the following. So, suppose, so this is the setup again. Eudagger is the unknown. I measure the Radon transform only at M angles, theta one, theta m. Uh, Eutager is an L2. Eutager is an L2. Now, I assume that Eutager is S sparse with respect to an orthonormal basis of wavelets φjn. Here I'm a bit unprecise, but those wavelets have to satisfy the Littlewood-Peley property. So, for example, you cannot choose any wavelets. You need to have some smoothness, okay, because you need to take derivatives in some sense. Okay, anyway, so this has to be satisfied. It has to be an orthonormal basis, at least as far as. Basis, at least as far as our current theory allows. Then I choose theta1, theta m between zero and pi uniformly at random. So I consider m angles uniformly uniformly at random between zero and pi. And I assume that the number of angles is directly proportional to s times log factors. I haven't said anything about those log factors. These are somehow unavoidable in compressed sensing. In compressed sensing, the only thing that you can try to optimize is the power here. The reason why they are unavoidable is the fact that, you know, think about without log factors, it would mean more or less that you knew where the support of the vector u dagger is. So namely, you would know where the non-zero entries are. Okay, if you know where the non-zero entries are, then it becomes a linear problem. And in a linear problem, you just need to measure as much. To measure as much as the size of the domain. Okay, no linear problem here. You also need to locate where the non-zero entries are. That's a hard bit. And then you have to pay a price. It's small because it's log, but still it's a small price. Okay, then as I said before, the inversion problem is non-linear, even though R is linear, but the inversion problem is non-linear. And among all those possible solutions that are consistent with my data, Are consistent with my data, I select that one with smallest L1 norm in the wavelet basis. Okay, and then the statement is that with high probability, indeed, this problem here has a unique minimizer, and the unique minimizer is exactly U-dagger, namely the original unknown. Okay, so let me just say a few comments, and then I'm gonna conclude. So, the few comments are as follows. Here in this talk, for Folllows. Here in this talk, for simplicity and for reasons of time, let's say I focus on one particular case, namely Radon. But actually, in our work, Radon is only, let's say, an example, a particular case of an abstract result that deals with compressed sensing and interpolation simultaneously, Hilbert space-valued measurements. So for each theta in this case, the measurements are vector-valued in Hilbert spaces, and possibly Hilposi. And possibly Ilpaul's inverse problems. So the map R or the map F that underlines, that creates the measurements, has an unbounded inverse. And the second thing, let me say that I only gave you here the exact recovery estimate. So no noise. I had no noise in the data here, exact data. And I was assuming U dagger exactly sparse. Okay. But I said to you that. But I said to you that typically the vectors are compressible, and of course, you may have noise. So, somehow in the paper, you will find explicit estimates when, you know, that somehow that measure U star minus U dagger. And this will be small if the noise is small and if the compressibility is good, let's say, okay, as you would expect. And there's actually a nice way of putting all these together. You can relate all these things together with noise. All these things together with noise, and you can obtain an optimal number of measurements depending on noise, so that when noise goes to zero, the number of measurements goes to infinity and the recovery is exact. So you can do these things as well. All right, now let me conclude. I tried to explain that on the one hand, there's a rigorous theory of compress sensing that is valid for many different domains, for instance, for Domains, for instance, for sub-sample isometries or for fully random measurements. I haven't mentioned those here, but the other area where compressed sensing has been applied is in case of fully random matrices, for example. And also there was empirical evidence that compressed sensing could be used for Radon, but somehow a lack of theoretical understanding. So, what we tried to do in this work was to build the Ricolus theory of compressed sensing for the sparse Radon transform. Radon transform, and this follows as a corollary of an abstract theory of sample complexity for inverse problems, ill-posed inverse problems. Well, we have the impression that this can open interesting directions for investigation. For example, we looked at only the parallel beam geometry. It would be interesting to look at the fun beam geometry as well. It would be interesting to move from wavelets to other families. So far, this does. Far, these families don't fit our current assumptions. It would be interesting to generalize this to other ill-posed problems, possibly even non-linear. And again, this is a big question mark. When you say non-linear, you mean A is non-linear. And compressed sensing with generative models that somehow have a similar difficulty, namely there is a non-linearity involved because sparsity is still a non-linear. Sparsity is still a non-linear concept because you have a union of subspaces, but those subspaces are still linear. And so things are relatively easy. As soon as you move to generative models, as Mateo discussed yesterday, there's a big compress sensing with generative models. It's a big theme today. But again, if you have subspaces, easy. If you have sub-manifolds, well, things become much more complicated. All right, so now you will excuse me. Let me just give you a quick. Excuse me, let me just give you a quick advertisement. You know, I see many young faces here. I'm always looking for interesting, you know, interested people, either as PhDs or postdocs. So if you know somebody who's interested to come to Genoa and to Malga, let me know. And they can freely contact me. There's no fixed open call, but I'm always interested to hear from interested people. All right. Thank you very much. Questions? Questions? It's off, I think. There's a little switch there. Does it work now? Yeah, perfect. Yeah, thank you so much for this interesting talk. Much for this interesting talk. I have a question about the log factor. Usually, in this log factor, if I remember correctly, you have the dimensionality of the space in which you embed the signal. Since here, the signal is infinite dimensional, what's in there? That's a good question. This is part of something that I've completely hidden from this presentation. Somehow, it's something that we haven't developed ourselves, and it was contained. Was contained mostly in this paper here and in the papers that followed. Somehow, yes, the problem is infinite dimensional, but you have to start from a truncation of the U dagger in, say, think about the wavelet case, you truncate U dagger up to a scale capital N, let's say. So you suppose that your U dagger is supported on in the wavelet basis up to a scale. Basis up to a scale capital M. So in this problem, you make the problem somehow finite-dimensional, even though in a function space setting, but still with a in a finite-dimensional subspace. And so in that log factors, this M appears. Okay, that makes sense. Thank you. Space is already compacted. What's happening here is that we assume that U dagger activates. You assume that u dagger activates only wavelet coefficients up to a scale capital M. Then, of course, what you have is the compressibility business. So, you will have a remainder. Of course, we assume that there's a tail in the wavelet scale. So, the fine scales that will contribute to a small error that appears in the estimates. So, that's something that I haven't discussed, but it appears in the estimates. Yeah. Okay. Thank you. Yeah, thanks, Giovanni, for the nice talk. Yeah, thanks Giovanni for the nice talk. I have actually the same question about the log factors but from practical perspective. Did you compare it maybe to classical sampling theory? So I wonder, so in order to reconstruct a function of a certain, I don't know, sparsity, you need a certain number of measurements. So what is the relation? I mean, what do you mean by the relation? I mean, let's say you take 100 Take 100, yeah, sparsity, like 100 biggest wavelet coefficients, and whatever, and then how much, what is exactly the log factor there? There is s times something. So, is it like the same size or is it double? Or have you any idea? Well, I mean, I cannot answer on the practical point of view. We haven't done any experiments here. On the theoretical side, there's you know the log factor for large values of m and s, the log factor. Of m and s, the log factors here will be substantially smaller. Well, it's not here anymore, anyway. That's s log m, let's say. So, for large values of everything, the log, the log factors will be substantially smaller of the other quantities. Of course, in realistic settings, this I can't guarantee. Yeah, I think it's a proof of concept estimate. Yeah, I don't know. The true answer is I don't know. Okay, thank you. Thank you so much for a very nice talk. I mean, I'm not in the field of compressor sensing and radon transform, but your first part is very, very clear. So I understand very well. Thank you. So I have only one question for the noise. When you did some comment at the end, right? So the noise, what type of noise that you can take into account for your theory? Can take into account for your theory? So, far we've included deterministic noise. So, basically, what you have is you assume that you have R u dagger T time plus a function, let's call it epsilon M. And you assume that the soup of across the M's of the norm of epsilon m in L2 of minus one, one is smaller than some constant. And this constant appears in the estimates, in the recovery estimates. So, any noise that has So, any noise that have about it out to exactly. So, it's deterministic here. So, basically, there's no statistics on the noise. We've done a fully deterministic worst case scenario, let's say. Yeah, because if you're technical probabilistic or statistical property of a noise, then it may affect your result, right? Because your result is with probability. So it's going to take into account the other one as well, right? Absolutely. I mean, it can get. Absolutely. I mean, it can get better because, as I said, this is with deterministic noise. So, this is the worst case scenario. So, it is not impossible that with statistical type noise, it can get better. Yeah, so I work a lot with different type of noise. So, the noise who L2 actually bout it, is that only limited to Gaussian noise or any other noise? Is okay. I mean, I don't, I typically can take any random variable in L2, it doesn't have to be Gaussian. Variable in L2, it doesn't have to be Gaussian. You cannot have white noise, for example, here, because white noise would throw you away the space, out of the space somehow. But it's infinite dimensional. But any other noise, even non-Gaussian, you can take it. I see. I see. But it meant to be in like L2, right? So because. So is any work that actually beyond L2 setting? Work that actually beyond L270? I don't know. Oh, no, okay. Thank you so much. Yeah, thank you. You mentioned that what you showed here was a special case of a more general result, but then you said you'd like to extend it to fan beam, which I assume means sparse fan beam because the full fan beam is true. So is what's the work involved in the sparse fan beam case? Is it establishing this, what's it called, GRIP or whatever, that that holds for the basis that you have in mind or whatever? Exactly. Exactly. That you have in mind, or whatever. Exactly, yeah, and also the quasi-diagonalization. These two concepts, yes, exactly. You always have to somehow put together on the one hand the sparsity, say it's always wavelets, fine. The forward map is different. I mean, in the full measurement case, it's the same because you measure all projections. But in the sub-sample case, it's different. So you have to look at that. Do you think it's pretty straightforward? It's just work or it takes. I think so, but yeah.