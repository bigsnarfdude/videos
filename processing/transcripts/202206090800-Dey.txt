So, today I'll be talking about how we can try to use some of these deep learning models. And specifically, I'll talk about genomic sequence-based deep learning models with complex trait GWAS for different diseases and traits. And so, in the just to give a little bit of a background, in the past Background in the past nearly 16 to 17 years of GWAS studies, we have been able to discover close to 10,000 GWAS hits for different traits. And 90% of these GWAS hits have been found in the non-coding regions of the genome, suggesting more of a regulatory impact on genes rather than a direct effect on protein structure and function. And this is where. This is where the deep learning models can be really useful because we believe that a lot of this regulatory logic that drives how a variant is affecting a gene and how that translates into disease is driven by the sequence-based architecture. And these deep learning models, given the fact that they are predicting based on the sequence patterns, can actually inform us or hope to, we can hope that they can inform us about. We can hope that they can inform us about this regulatory sequence-based logic. So, what we are primarily interested in is that whether we can actually predict the regulatory activity at a particular variant based on the sequence data and whether we can predict how it affects the gene expression patterns and overall the disease architecture. So, what we do here is for a particular genomic A particular genomic sequence, we can treat this as a sequence image, which is a bit of a monochromatic image in this case, where we kind of color those cells or those pixels in that image, you can think of it that way, which are corresponding to that particular alphabet or nucleotide alphabet, you know, basically. And now if we have Features like the DNAs, histone mark features, transcription factor binding site events, or cage data, which and we have a ton of that from ENCODE, from roadmap, epigenomics. And we can try to fit a deep learning model where we throw in a lot of the sequences from the different regions of the genome, and we also tell the neural network model what are the chromatin features at that particular. Chromatin features at that particular site of that sequence. And we hope that once the model has trained itself well enough, we can now throw in any sequence and not necessarily a sequence that is part of the genome or the reference genome, which has been seen by the model, but even something that has not been seen by the model. And it can still give us a prediction of what would be the chromatin feature affinity at that particular location or like DH. Location or like DHS accessibility prediction or TF binding site affinity prediction for that region. And there have been several models that have basically tried to do this thing with different sets of chromatin features. Two of the models that we are going to use are the deep sea model from Olvet Roanska's lab in Princeton University, and the other is the Bayes NG model built by the Calico labs in Google. And so the nice thing about these two models. The nice thing about these two models is that they actually encompass a lot of features. So, the deep sea model is around 2,000 features, the BaseNG model is close to 4,000 features. And the fact that they can actually bring in so many features can actually help us better understand the regulatory logic just by the fact that we are using a lot more data sets. So, now we can take a particular sequence. take a particular sequence and we can look at the contiguous we can we can take a particular SNP and we can look at the contiguous sequence to that particular SNP and we have a sequence for the reference LV of the SNP and we have a sequence for the alternate LV and now we can generate a prediction of the different features that were part of this deep learning model training once for the reference basic Reference base sequence and the other for the alternate allele base sequence, and which we term here as the PF and the QF. The PF, we are calling it as the variant level annotation because it's like a prediction of how much affinity of the DHS or transmission factor binding we expect at the particular region based on just the reference genome or reference sequence, basically. But the more interesting thing is. The more interesting thing is this delta F term, which is the difference between the prediction based on the reference sequence and the prediction based on the alternate sequence. So what the delta actually gives us is an allelic effect on the prediction of that particular feature. Like by changing the allele of the SNP from the reference to the alternate, how much do we expect to see a predict? Expect to see a predicted change in the transmission factor binding or in the DHS accessibility, for example. And what we did was we generated these delta F anyelic effect scores for close to 10 million common and low frequency variants across the genome. And then we wanted to understand whether these SNPs, which have a higher allylic effect scores in general for many regulatory For many regulatory features, are they enriched for disease heritability for different complex traits and diseases? So, to perform such an enrichment, we use the stratified LD score regression framework. So, this particular method takes in as input Diva summary statistics, then an LD reference panel, and then the function annotations, genome-wide function annotations, basically. Genome-wide function annotations, basically. And then it can learn what is the enrichment of the GUS signal. In this particular case, the heritability enrichment of the particular trait at the SNPs that are annotated by that function annotation. And on top of that, there is also another metric called the Tauster metric. And the Tauster metric can be used to Metric can be used to provide an estimate of the disease signal for one particular function annotation, conditional on other annotations that you can incorporate into the heritability model. So we wanted to make sure that we are capturing some unique information in these deep learning functional annotations, the allylic effect annotations of the variant double annotations, conditional on some of the other predictive annotations, like the Chrome HMM or like the roadmap annotations and many other. Roadmap annotations and many other like coding conserved and deliver annotations. So what we did was we, as I mentioned, we considered these two deep learning models, deep C and Base NG, and we computed these allylic effect scores and the variant effect scores genome-wide for each of these models, specifically for four chromatin marks, the three histone marks, HTK4, Me1, Me3, and HTK27AC. And these are the histone marks. And these are the histone marks that are associated with activated enhancers and promoters in general. And then also we looked at DNA and we considered an aggregate of these delta F scores for this particular chromatin marks either aggregated across all biosamples or all tissues or the blood-related biosamples or the brain related biosamples. And this aggregation was performed by either taking the average across all the biosamples or taking the maximum value. The maximum value. In this way, we obtained overall 32 different functional annotations. Some of them were kind of aggregated across all the tissues. So they were kind of tissue agnostic, some were kind of brain-specific, some were kind of blood-specific. And so we evaluated the heritability enrichment for each one of these 32 annotations. And what we observed was not very encouraging. Out of the 32 function annotations, only one annotation. Function annotations: only one annotation, which was this Bayesian GS3K4ME3 allelic effect annotation, which actually gave us a conditionally significant tau star value, conditional on the roadmap, Chrome HMM, and all the other functional annotations that have been typically used in heritability models. And so, this was an analysis, meta-analysis across all the tricks. Now, if you do a meta-analysis across, say, brain. You do a meta-analysis across, say, brain-related traits, once again, HTK4Me3 in brain was found to be the only informative annotation. We didn't see much signal otherwise. So it kind of seemed like we were getting some signal, but it was not a lot of signal that we were getting out of that. And one of the reasons could be that these deep learning annotations, overall, when you are looking at them genome-wide, at many SNPs, they may very well be noisy, and which is probably diluting the overall signal that we. Is probably diluting the overall signal that we are seeing. At the same time, a lot of these variants are basically common and low frequency variants. So they have smaller effect sizes in general. So that might be harder to kind of delineate which are the variants that are really important based on this regulatory information from these deep learning models. So now the question was: Is there a way in which we Was that a way in which we can improve on the amount of information that we are getting out of these deep learning models? So, I will briefly talk about three such approaches that we followed. One was combining the deep learning models with GWAS fine mapping data. The other was integrating SNP-to-gene linking information with these deep learning models. And finally, how we were using these deep learning models to predict gene expression. Predicted gene expression. So, I will first talk about how we can integrate fine-mapped GWAS causal SNPs with the deep learning models to actually improve on the functional annotations. So, for this, what we did was we took a set of causal SNPs, fine-map causal SNPs from autoimmune disease devices, and then we used that as a positive set in a logistic regression kind of a framework. Regression kind of a framework. And then the negative set was a set of SNPs that were not found to be fine-mapped across this autoimmune across any of these autoimmune diseases, basically. And we performed this kind of classification task, separating out this positive set of SNPs from the negative set using these deep learning features from the Base NG and the deep C models. And here we use the XG Boost classification model. Now, one of the Now, one thing that we did to kind of come up with a new predictive annotation was: let's say that we want to generate a new annotation or improve deep learning-based annotation for all chromosome one SNPs. So, what we do here is that we split the chromosomes into two groups: one is the odd chromosomes, and one is the even chromosomes. So, if we have to predict on the SNPs in the odd chromosomes, we will. In the ot chromosomes, we will look at the model that is trained only on the even chromosomes. And then the fitted model from the evin chromosome space, we use that to predict on the test NIPs from the odd chromosomes. And we do the same thing again for the evin chromosomes where we use the fitted model on the odd chromosome. The reason why we kind of do this splitting is just to make sure that we do not have the overfitting issue by using the Issue by using the same data for the training and then for the prediction. So this predicted score that we got out of this, we call this the deep boost score because it was kind of like a like boosting the deep learning annotations using information of Jiva spine mapping. And one nice thing was that what we observed was that this deep boost core can actually be used to To resolve GWAS loci, which are hard to resolve just based on the disease GWAS-based information. So, here's an example of a chromosome 8 locus, which is a very high LD region locus. LD average D prime is close to 0.99. But this is a locus which is filled with GIVA states for the lymphocyte count GIVA strait. Now, if you perform Now, if you perform a JIWAS fine mapping of lymphocyte count for this particular locus, you will find that the fine map posterior probability of being causal for all of the SNPs in this locus are really small. And that is just because this locus is really big, it is very hard to specify or pinpoint which variant is actually the causal variant. But now you can overlay the deep boost scores on top of this variant. On top of this variance in this particular locus. And here you can see that there are some variants, especially like the three variants in the middle, which have much higher dip boost scores compared to some of the others at the tail ends of this particular locus. And the one with the highest dip boost score is this RS113142693 variant. And interestingly, what we found was that this particular SNP was not just implicated by the deep. Implicated by the deep learning models, but there was also additional validation from experimental assays like DNA's alleged imbalance data or TFG alleg imbalance. And even if we look at GTEX fine-map TQTLs, then this variant gets picked up as a final slip. So there are multiple layers of evidence which support that what the deep boost model is actually picking up is possibly a causal variant or a functionally important variant. So one thing that we did was we compared these the deep sea and the BaseNG annotations that were kind of based on the analytic effect models and in part of the published literature with that of this new boosted deep learning annotations. And now we started to see more information out of them where the Bessingi boosted model and this was basically trained on This was basically trained on a set of find map SNPs from across 30 autoimmune diseases. And we started to see some disease signal out of them. This was for blood-related traits. And we were previously not seeing anything significant for blood, but based on just the island effect annotations. But now we are seeing some signal based on the boosted annotations. However, the amount of signal that we were However, the amount of signal that we were capturing was still kind of small. And so, this leads us to the second step where we are combining these deep learning models with SNP2 gene links. So, the idea of this is that we can look at the deep learning annotations genome-wide, but as I mentioned, that a lot of these annotations could very well be noisy. But what we can do is instead of looking at them genome-wide without any other information, we can restrict ourselves to only SNPs that we know are linked to. Only SNPs that we know are linked to some genes. Now, when I say that linked to some genes, there are many ways in which you can actually define how a SNP is linked to a gene. Some simple ways could be just to look at SNPs that are in a particular window around a gene, like a 5 KB or 100 KB window, or maybe like the gene promoter or transcription start sites. We can also look at some expression-based typed to gene linking strategies where we can look at an SDQTL to link a particular variant to gene, or we can. Particular variant to gene, or we can look at SNPs that are in regions where the ataxic peak is correlated with the gene expression. Or the same thing we can look at for roadmap epigenomics data sets as well. We also use a couple of high-C-based SNP2 gene linking methods. One of them, particularly, that we'll talk about is this ABC method, the activity by contact method, which basically combines the DHS, the H3K27AC, which are kind of Which are kind of signatures of active and answer marks with the high C information in different tissues. And especially in this particular case, we're interested in blood because, I mean, that's the data set for which we have a really nice set of fine-mapped data sets. So now, when we looked at these deep learning models kind of restricted to the SNPs that are linked to genes, we started to see a lot higher magnitude of the tauster. A lot higher magnitude of the tau stars and a lot more signal, also. And especially, we observed that when restricted to ABC for this base NG boosted model, or when the Base NG boosted model is restricted to TSS, or the deep sea boosted is restricted to QTLs, we get significant tau star values. And also, another thing to note here is that the boosted models do once again better than the general published allylic effect-based models. We also performed a joint analysis of these three annotations that we picked up as marginally significant, having marginally significant tau star, and we found that all three of them are also jointly significant, providing unique information. So, now what we are interested in is not just to understand the regulatory logic of these different SNPs, but to also understand whether To also understand whether how we can use this regulatory logic to predict gene expression patterns. So, what we did here was that we took the gene expression and we fitted a linear regression model where the predictors here were these PSF quantities. So these were variant-level deep learning predictions. But we were kind of combining these variant-level predictions for all the SNPs that are. For all the SNPs that are linked to the particular gene, particular gene G in this case. And this linking information can be based on multiple SNP-protein linking strategies that I discussed here. However, we focus primarily on the two gene window-based methods, the 5KB and 100 KB windows, and then TSS proximal SNP2 gene linking, as well as the ABC and roadmap, which were more digital enhancer gene kind of linking. So we were kind of covering a broad spectrum of different. Kind of covering a broad spectrum of different types of sniplogene linking in this way. And so this particular method as such is not very new because this was actually done to some degree in the Expecto paper from Ju et al. 2018 Nature Genetics. But in the Expecto paper, they primarily focused on TSS proximal regions to predict the expression of the gene. Whereas here, we are looking at Whereas here we are looking at bigger windows and also more specific enhancer gene kind of links from the ABC and roadmap, which were not used in the expected paper. So one thing that we first noted was that if we use this particular model to predict gene expression and we held out a particular set of genes and use this fitted model to predict the expression of those held out genes. The expression of those held-out genes and compared with your actual expression, we got a fairly high correlation, like 0.72 for the deep C model and the 0.76 for if we use the base NG variant level predictions. And comparatively, if we look at the expecto method, the correlation was 0.79. However, what we can do here is besides fitting this particular model, we can also use this model to get a sense. Also, use this model to get a sense of the expression effect of a particular allele. So, when the particular SNP, the reference allele changes to the alternate allele, what is the overall effect of that on the expression of the gene that it might be linked to? And this expression effect can be summarized by this complicated expression, but it can be derived basically from the linear regression model that we have. Basically, from the linear regression model that we have apart from. So, this expression effect now we actually generated for all the variants across the genome, all common and low frequency variants. And we did that for this for the deep C model, and we also did that for the base NG model. And we are calling these annotations as the Imperio annotations, Imperio Deep C and Imperial Bess NG. And we are comparing that now with the EXPECTO annotations from the GETL 2018 paper. And what we observe here is that we see We observe here is that we see both higher enrichments for the imperial scores and, more notably, higher tau stars. And you can see that we actually get an increase in the tau star when we go from the expector model, which is based on the deep sea deep learning model, to MPO deep sea. So this jump that we are seeing, the plausible reason for that is that we are using a much better SNP2 gene links, like the enhancer gene links. To gene links, like the enhancer gene links, for that matter. But the jump that you are seeing from the imperial deep sea to the imperial Bessengy, it is more likely that it is a difference between the two deep learning models, the deep sea model and Bessengy model, with the Bessengy model being more powerful than the deep sea model. So finally, just to highlight what I discussed, we first observed that the published deep learning annotations, the allylic effect annotations, if we look at them. Adelic effect annotations, if we look at them genome-wide, they have limited information for complex diseases and traits. However, there are ways in which we can improve on their information, either by integrating with fine-mapped GIVA SNPs from different, we looked at different autoimmune disease fine-map SNPs, and also by restricting to regions that are linked to genes. And finally, we can also look at how they predict gene expression patterns and use that. Expression patterns and use that to derive these expression effects, which can be informative. And one of the future directions that I'm interested in is to extend this deep boost model to not be just restricted to deep learning features, but also to take into account other variant level effects like the allylic imbalance and EQTLs and all of that. So, finally, I would like to thank everyone who has been involved in these particular projects, and here are In this particular project, and here are the two relevant publications or preprints that are related to this project. And also, I would like to mention that I'm joining as an assistant professor starting to 2023 in Memorial Sloan Kettering Cancer Center, and I'm looking forward to those docs and grad students. And I would like to thank all of you for listening. I'll try and take questions.