Thanks again for joining. So for the first talk we have Sean Palett and Shahno Nassarasp. So Sean is a professor in the Department of Mathematics at University of Virginia. And Shahler is an assistant professor in School of Mathematics and Statistics at Russia Institute of Technology. And they're going to talk about inverse eigenvalue problems for graphs. Thank you, Messa. Thank you, Messa. So, yeah, so I'll see what I can do about trying to make up time. I have had three coffee and an espresso, so I think it'll be okay. So, we weren't given much of a plan. I mean, in-person eigenvalue problems is quite a broad field, so we sort of narrowed down, roughly speaking, to these five topics. And I'll sort of talk about part one, and then my colleague Shala will do the bulk of the work two through five. Bulk of the work two through five. So the idea of zero forcing as presented in this context is a propagation game through a graph. You start with initially coloring some vertices and then you're allowed to color more vertices as you move through the graph with the objective of trying to color all the vertices. Okay, so I'll explain how you propagate through a graph associated with this color change rule as we move forward, but roughly speaking, in the context of In the context of eigenvectors or null vectors, the idea is that if you're standing at a colored vertex, if you're on top of a colored vertex, and all neighbors but one are colored, then you're allowed to color that last uncolored neighbor, and you can continue the propagation through the graph. And then the goal is to try to color all the vertices in a graph by somehow starting with some sort of nice initial coloring. And then the idea is you want to minimize the size of that initial coloring. Minimize the size of that initial coloring as we'll see in a moment. So, we've just tried to demonstrate here with the, I'm not sure which one's the pointer, I'm scared to do it. So, vertex one is colored, a chunk of its neighbors are colored, and then it has, it sort of has one uncolored fellow that's attached to it, and so you're allowed to color number two and then move forward. That's kind of the idea. Okay, so very sensitive. There we go. Okay, we're not going to do it anymore. It anymore. So, you start, as I said, all vertices are uncolored. You color a certain subset of the vertices initially, and they cost you tokens per colored vertex. And then you sort of, at no cost, you can apply that color change rule about coloring a unique uncolored neighbor. And then the idea is you're trying to do this by minimizing the cost of the number of tokens you've paid up front. And then that minimal cost is known as the zero forcing number in a graph. In a graph, and it's sort of denoted by tap rules. And why is it related to inverse eigenvalue problems? Well, I haven't officially told you what A and S of G is. I'll get to that in a moment. But let's imagine you start with a graph, you associate a collection of matrices to that graph. In this case, these matrices are symmetric. And then among all those matrices, you calculate all the nullities of those matrices. And then you take the largest possible nullity you get. And I'll explain what S is exactly. I'll explain what S is in a second. That zero forcing number, that cost, that initial cost of coloring vertices, is always an upper bound for the maximum nullity. And so it was sort of just a game, I suppose, or a mechanism for us to try to bound the nullity. And the idea is that maybe this number is maybe sort of not so easy to calculate because you've got a heck of a lot of melodies to sort of go over. But somehow, maybe this number is maybe not so bad to calculate. Maybe this number is maybe not so bad to calculate because now you're just coloring the graph. It just depends on your perspective. The other thing that you should pay attention to, although I haven't personally addressed this very much, is this class here, as I said, was symmetric latency, so they're inherently have real entries. So then you might ask yourself, well, if I change the field, you know, and then I recalculate this capital M number, maybe it changes. And indeed, it does. Like if you go from the reals to the complexes or finite fields, this number varies. But this number This number varies. But this number here has got nothing to do with the field because you're just coloring a graph. So no matter what field is your favorite, and you calculate this capital M, this is always on top. So that's, in turn, that's useful. So then the question, here's a little mini proof of it, but it's not, I'm not going to waste time now, I mean, because it is straightforward. But if you have questions about why this inequality is true, I mean, there's the reason, but I can sort of explain it in more detail at the conference. I can sort of explain it in more detail at the coffee break when we have more special. Excuse me, I should go back. So then the question becomes, right? Like once you find this kind of cool inequality, the question becomes, well, when is it strict? When is it not strict? So when is there an equation, you know, and you play this game back accordingly. And of course, it's not hard to see that it's not strict all the time. And here's just one example. There was an American Institute of Math workshop. American Institute of Math workshop where zero forcing was, I guess, formally sort of written down, although that process has been around for a while. But its connection to capital M, I suppose, was kind of more made more clear. And this graph, for example, this graph on 10 vertices, the zero forcing number is 3, but the maximum nullity over the reels is 2. So there's a gap. And I can make the gap as big as I want in some sense. Assuming I can make the number of vertices obvious. Number of vertices always. And so the question, one open problem, which is rather open, is just to find all those graphs where there's a coincidence between capital M over the reals and Z. Now, if you go back to this concept of forcing, you can really dissect things. And so, as I said, you start with an initial set of colorings, right? You color an initial subset of vertices. And now you imagine you propagate this forcing process. What ends up happening is if you follow each vertex that you initially... is if you follow each vertex that you initially colored, you actually end up constructing a path through the vertex. I mean, that's clear because you just color a neighbor, then you color another neighbor, and you color. So you're just, so what ends up happening is you start with a bunch of vertices, and then you just get a bunch of paths going through the graph. And the hope is, is that those paths cover all the vertices. So then you end up with something known as a path covering. And those paths have been known, they're referred to as forcing chains, but that's just following the chronological list of forces. Chronological list of forces. And if you're lucky enough and you've covered all the vertices, then you sort of refer to that as a forcing chain cover. And then the number of the paths correspond to the initial coloring that you had, right? Just follow that. And so you talk about a path covering, which is just a collection of vertex disjoint-induced paths that sort of make their way through the graph that cover all the vertices. And then, because I want to minimize my initial coloring, I'm very naturally I want to minimize the number of paths. I want to minimize the number of paths that sort of do this covering. This is referred to as the path cover number. So, as you know, if you followed along, although I don't see it here, but nonetheless, every forcing process induces a path covering, so there's a natural inequality between the path cover number and the zero forcing number. And so then the question becomes: like, well, okay, so when are they all equal, or when are they, are there gaps? And it's known for trees that. And it's known for trees that the maximum nullity of a tree is the same as the zero forcing, is the same as the path covering number. And as noted here, that the inequality between these two things was known, you know, as it says there from 1999. Okay, so this is the point I wanted to make, is that there's this obvious inequality because the path cover number is the minimum over all set of paths, and one of those paths might be a set of forcing chains. Maybe it isn't, but one of them might be. So you have But well, it might be. So you have a natural inequality here. And again, you ask the same kind of question. When is it an equation? When is it strict? You know, all these kinds of things. And here's just a simple example where it can be quite a gap, because it's really hard to sort of zero force a complete graph unless you cover a heck of a lot of vertices, just because it's just vertices have too many neighbors. So the zero forcing number is very high. I claim it's n minus one. But the path covering number is basically a magic, right? Basically, matching, you know, with roughly speaking, right? Because you can't, there's no way you can get a path of length three because you've got, it's not induced. And so the question becomes, you know, when is this, when is, for what graphs do you have this equation? And sort of more recently, I discovered this paper by these folks here in 2016, where they actually studied this class of graphs. So they called it ZP. I mean, it's this curious thing in mathematics. I mean, it's this curious thing in mathematics. Like, I don't know how to answer a question, you know, like, what graphs do I have this equation? So, I ask myself a seemingly more complicated question: like, okay, suppose I ask that this is true for all induced subgraphs. So, on the face of it, that seems kind of ridiculous because I can't answer the first one. But you know from mathematics that when you impose more conditions, often you learn other things, right? And so that's the idea. And these folks in 2016 defined this graph Z P as presented there, and they showed that under And they showed that under the condition that every cycle is induced, you know, so basically you're avoiding cords and things like that. They showed that there's an equivalence between these two classes. And in case you don't know what a cactus is, because it might be known by many different things, this is just basically like a tree where, at worst, cycles can intersect at a vertex. Like cycles are not allowed to intersect at an edge or beyond. So you have this sort of, it's basically like a tree of cycles, if you wish, where two cycles. A tree of cycles, if you wish, where two cycles are allowed to just be touch at a vertex. I mean, there's a more formal definition, but if we may be able to visualize that. So, the plan or one possible problem is to say, okay, let's sort of scratch this condition out and let's just ask ourselves, like, geez, what are other ZP graphs, right? And, you know, natural places to start are places like Planar and Outer Planar where you can start to get some information. And some things are not, but that's a And some things are known, but that's a sort of a place where we can start. Now, like a lot of times in mathematics, like once somebody discovers something is working, then people come along and start to apply variations. And there's like 101 variations in zero forcing, and I'm not going to go through them all. Here's just a mini list of the different types of variations of zero forcing, all with the intention to bound some kind of double. Okay, so that's what they were sort of there. I mean, I think power domination was sort of around the I think power domination was sort of around before formal zero forcing. And there, you're, you know, domination in a graph. We have experts in domination here that coordinate. But the idea of power domination, there is this color change rule buried in it, right? So power domination is you have a set of vertices and you dominate the neighbors, and then you're allowed to keep dominating as long as you have this color change rule applicable. Let's pass this colour. And there's all there. I'm not going to explain them all to you, but. them all to you but if you're interested I'll talk about some of the I'll talk about ZQ and some of these ones up top that are available but there are papers out there that I can point you to if that's something you wish to work on no problem but just going forward this is the game you've got a bunch of tokens and you're allowed to spend tokens to color a pile of vertices to start with and then you're allowed to invoke the color change rule and then the variations of these games is just basically modifying how you invoke the color change How you invoke the color change. And I'll explain some of those. So, this is the conventional zero forcing game, and this picture here is just an illustration of the process. And we'll stick with this picture when I go through the variations. So, I claim here for this sort of straightforward tree that a zero forcing number is five. I've illustrated a process here where it costs you five tokens. Of course, I have to prove you can't do it fewer than five, but I'm not going to do that. I'm not going to do that. And so the darker colored vertices here, if that's visible to you folks, are where I'm spending tokens. Okay, so like I've spent a token here, and now I look at all its neighbors. If there's exactly one uncolored neighbor, I'm allowed to color it. And so that's what the little modified blue is about. And at this point, I stop. I can't do anything. Like all the any colored vertices have more than one uncolored neighbor, or no uncolored neighbors. So then I spend another one. I spend another one and I play the same game and I keep going. And eventually, it turns out you can see you're going to spend five in this context. And five is the best. But I just wanted to illustrate a picture there. Now, when I defined that class of matrices, I said these were all real symmetric matrices and a very natural subset of symmetric matrices of the positive semi-definite matrices. And for that subclass, there's a different type of zero force. And it's presented up there for you. Up there for you, but let me try to explain and I'll do it on the board to try to illustrate, but I can sort of see here. So the idea is you've colored, you've spent tokens on some initial set of vertices, and then you apply the color change rule as much as you can. You go as far as you can. And now let's suppose I've stopped here because I can't apply the color change rule anymore. So now I remove this set of vertices from the graph. And when I do that, And when I do that, I'll create components in theory. And these components are all uncolored, so I just refer to them as white. And now the idea is you take one at a time, you take a particular component, and you look at this induced subgraph, and you apply the color change rule here alone and see if you can go forward. So you get to forget about these other ones, and then you get to go. And if you can, And then you get to go. And if you can, if you're lucky and you can apply the color change rule, then you update B and you play this game again. So there's a lot more power to the positive semi-definite because it's not like you have to worry about all the neighbors here. You only have to worry about the neighbors when you attach one little component over time. And that's what this is trying to say here. You just get to do this component at a time. And the reason this works in the positive semi-definite game is there's this beautiful property for positive semi-definite matrices that talks about. Semi-definite matrices that talks about, it's called like the inclusion principle or something, however you want to think about it. But if I hand you a positive semi-definite matrix and you look at a principal sub-matrix in that positive semi-definite matrix, and then you sort of scan out here and you look at a column, the result is that this column is always in the column space of that thing, no matter what submatrix, principal submatrix. That's called inclusion, and that's the reason I get to sort of pick on these little parts, is what's going on. Little parts is what's going on. But we can discuss that later. And it turns out for trees, for example, this positive semi-definite zero-forcing number, which is the fewest number of tokens I need to spend to play this game, is always what? Because trees always bust up into nice little pieces. It's easy for me to play this game on trees. And then it gets worse. So this positive zero-forcing game and the zero-forcing game are one-player games, right? It was just you against the graph. It was just U against the graph. And then the optimization problem was to figure out what the best set of initial colorings was. This ZQ game is a two-player game. So there's a player and then there's an oracle, and they're sort of opposing one another. But the premise is sort of the same. You get the color of a pile of vertices by spending tokens. You play the color change rule, and then as far as you can go, and then you have this adapted rule here. Rule here. So in the positive semi-definite game, you remember, I was allowed to pick one component at a time and play the game. The ZQ zero forcing game, you need to select Q plus one components, if such number of components are available. It's possible there may not be that many components. So what you have to do is you have to select Q is fixed, like you decide what Q is ahead of time, some positive feature. And then you have to pick that many components and you pass it over. That many components, and you pass it over to the oracle. So the oracle knows which vertices are colored and which components are handed to them. And then the oracle looks at that picture and says, okay, how best can I sort of hand back components so that you're not able to proceed in the game? So they're like opposing one another. But the Oracle is required to hand back at least one of those components. That person has to give back at least one. Maybe all, maybe a pile, maybe some, but must give back at least one. But must give back at least one. So you hand the oracle q plus one, the oracle looks around and says, okay, I'm going to give you that pile back, and hopefully that sort of is going to mess you up. And then so then the fewest number of tokens you need to spend in order to get through the graph, that's called ZQ. And then you have this progression between these ZQ numbers, where, by the way, Zn, where n is the number of vertices, is conventional zero forcing, and Z0. Zero forcing and Z0 at the bottom, if you've been paying attention, is the same with a positive definite zero forcing. So there's like this bridge of zero forcing that was in between. And ZQ, this ZQ game, is really quite tricky. So here's that same tree where I showed you that capital Z was 5. So here it turns out that Z1 is 2, or excuse me, Z1 is 4. You save a little bit. But I'll show you, for example, like I covered this, I spent a token on that same leaf, and I apply the color change rule as much as I can, so I get to here, right? And so now this is my set B. I remove B, and I have to hand the oracle two components at least, right? Because Q is one. So I got to hand the Oracle at least two components. But you see that which, so, and there are how many components, right? One, two, three, four components. Whichever pair I pick, I'm in trouble. Pick, I'm in trouble because the Oracle is just going to give them back, and then there's always going to be at least two white neighbors dangling about this guy, and I can't proceed. Okay, so then I, like I did before with zero forcing, I spend another token, and I do it again. Now here, I'm in a pretty good spot. But I still have to hand the Oracle at least two components, but you know what I do? I hand one leaf on the left and one leaf on the right. And now, whichever leaf is handed back, I can force. Right? Force. So now the Oracle, instead of giving them both components back, is going to say, okay, I'm just going to give you one back because at least I'm only going to let you force one more vertex. The Oracle is trying to optimize as well. And so that's what this, so if the Oracle hands you back this leaf, you do the same thing again. He's just going to give you that one. Because he's going to let he or she'll let you color that side. Because the oracle wants to hang on to these guys for as long as possible. And then that works until the very end when there's two damages. Works until the very end when there's two dangly there, and then we just play the game where I give two, he gives the back, I give, I gotta spend two. So there's the there's the ZQ game on that particular tree. And the, oh yes, so here's all these numbers that are associated with the ZQ game. Yeah, so maybe you're, I understand I'm going fast, I feel like I'm going fast, but why do you care about ZQ? The ZQ, there's also Care about ZQ, but ZQ, there's also a nullity problem there. So there's a thing for symmetric matrices, for example, but not just symmetric, there's this concept called the inertia of a matrix. The inertia of a matrix is the number of eigenvalues with positive real part, the number of eigenvalues with negative real part, and then the number of eigenvalues with zero real part, if you're not symmetric. But if you're symmetric, it's just sort of chopping up the real line. And so the inertia of a matrix is really important. And if you're interested in inverse eigenvalue problems, inverse inertia. And inverse eigenvalue problems, inverse inversion problems are also something you might worry about. And so here, this Q in that ZQ game was actually fixing the number of negative eigenvalues you required. And so if you look at the set of symmetric matrices in this context that have exactly Q negative eigenvalues, and you choose the maximum nullity over all those, then that's called MQ, and there's always a bound between ZQ and MQ. Between ZQ and MQ. Right, so you get this. This was the one from 2008, and then a couple years later, it was a semi-definite version. And then a few years later, they cleaned it all up by showing this is true for any Q. And remember, Q equal to 0 is this, and Q equal to N is that. So they sort of cleaned it all up, these folks of 2015. So if you've sort of played around with symmetric matrices, for example, Sylvester's inertia theorem. You know, Sylvester's inertia theorem is really important, and so studying the inertia is actually quite valuable. And so, this zero-forcing ZQ number does that. It turns out that having the oracle involved really does make a difference. So, in the Z game, remember I told you, you just have to cover a bunch of vertices and let the game go. And if you cover everything, you clap. But in the ZQ game, it's actually not ideal to color all the vertices first, right? Color all the vertices first, right? Like initially color thumbs and then let it go. You actually want to mess around with the oracle a little bit. And so this picture here, while I won't waste your time, this picture is actually showing you that if you actually color four vertices right up front, the oracle, depending on which one you color, of course, and which way the oracle, the oracle can actually force you to get stuck. So what you want to actually do is just color a few, play the game, see what the oracle gives you, color a few more over here. So that's different than those deterministic games, right, where there wasn't a Those deterministic games, right, where there wasn't a second player, where all you had to do was just sprinkle colors and see what happens. Here, there's actually a game, right? Like you color a bit, try to force the oracle over there, color some more. And that makes it, of course, trickier to find that ZQ number, but it makes it more interesting as well. Okay, so I'll skip that. And so then the question becomes: how do you find this ZQ? And it's not easy. And so there's a bunch of results in sort of straightforward cases. Straightforward cases, depending on the nature of Q or the nature of the graph. We have these papers online that I can provide for you, so I'll just, you know, I'll leave them there. There, right? I mean, anytime you play in this game, right, you sort of say, what the hell's the answer for trees? And there's the answer for Z1 of a tree, right? Like, don't even look at it, right? I mean, I can't even look at it. I mean, it's just, it's ugly, right? So remember, Z0 of a tree is always one. Z0 of a tree is always one. Z of a tree is always the path cover number. And Z1 of a tree is this awful mess. Is it computable? Like in terms of the number. It's polynomials, I think, yes. But I mean, if you handed me a tree on 11 vertices, I wouldn't know what the hell to do with it, other than ask a computer. So, you know. So, you know, let's work on Z2 of a tree. Like, I can only imagine that, you know. But, you know, it's not all hope is lost because, of course, if you ask about other kinds of graphs, so these are threshold graphs. So these are graphs that are created by, you know, your tinker toys are vertices, and you sort of, you add a new vertex to your existing graph by either joining it to everything or just leaving it as a union. Or just leaving it as a union, and you just keep doing this. That's a threshold graph. For threshold graphs, there's a correspondence between Z and M. Remember, I asked, I told you that was a possible problem. For any chordal graph, you have a correspondence between Z0 and M0. This is, sorry, this number here is called the clique cover number, so the fewest number of cliques needed to cover the edges. And so, obviously, this is still true for threshold graphs. And then it turns out that very Threshold graphs. And then it turns out that very recently we were able to show that actually all these numbers are the same for threshold graphs. Not just 0 and n, they're all the same. And not only are they all the same, like here's the formula, where you have this thing that's computable based on the creation sequence of the threshold graph. And so a natural problem for us to sort of ask, I suppose, is to try to figure out what other graphs have this correspondence between that nullity with fixed negative eigenvalues and this z-cube. And this is a cube. Okay, so because we are short on time, and I do want to give Shalla her another time, any questions I'll happily ask at the break. I know I went really fast, but I'll let Shalla have an opportunity to talk, and she'll talk more about matrices instead of forcing stuff. Okay, thank you for your time. Thank you, John. Thank you, John. And thank you, all the organizers, for inviting us to this beautiful place. Gary and I had a really nice hike yesterday and make Lee's picture from yesterday. Thanks again for having us. I had forgotten how much I spent Canada. It's good to be here. Sean gave a really nice talk about the zero forcing variance of grunts. I'm going to talk about a different aspect of the inverse eigenvalue problem, which is basically on matrices and the multiplicities of. On matrices and the multiplicities of the eigenvalues. So I'll start with a very brief history of where this started from. In 1857, Cayley modeled some molecule by a graph, and then he discovered that the adjacency matrix of that graph is invertible and only if there's some kind of unstability in that molecule. And from then, the invertibility of the The invertibility of adjacency matrices became a really nice problem in the graph theory and the minor algebra. And of course, invertibility means having the novelty to be zero. Chris Dotson is looking at it. I'm sure he knows way more than that, but that's all I know about this theory. So, what's an adjacency matrix? It's basically simply the square 0, 1 matrix when the empty ij is 1. If I know the perfect size is 0. Is one, if I know it, vertex i is adjacent to the vertex j, and everything else is zero. There's only one matrix here associated to a graph, so the rank of that matrix is the rank of the graph, the nullity of that matrix is the nullity of the graph, and we are wondering about the nullity of a graph, which means the nullity of that matrix. And of course, we have this very simple, nice theorem from Beneral algebra that says nullity plus length is the number of columns. In this case, the matrices are squares, so the nullity plus length is the order of the query. Rank is the order of the graph. So, what that means is that studying the novelty is kind of equivalent to studying the rank, because if we have one, we have the other one as well. So, again, since then, this problem has become the center of attention to many researchers, and there are a lot of graphs for which the eigenvalues are known, not just novelties, like paths and cycles and many other graphs, but the question in general is open. And for these, if you want to know these, And for these, if you want to know these numbers come from, the eigenvalues are completely numb, they're just some functions of cosine, which is what the periodic things are. So the goal here is to give results about the eigenvalues using combinatorial methods. So relating the eigenvalues to the combinatorial properties will help us to understand them more. And one way that has been done is to study relationship programs. Study the relationship of the novelty to let's say maximum degree or matching number or pad cover number and so on. I only listed one here: the relationship between the novelty and the girth of the graph, the length of the shortest cycle in the graph, which is G. And in 2007, Chen and Liu showed that the novelty of graph is at most m minus three plus two of g divides, I guess G is divisible by four radar microscopes, at most m minus three. At most, n minus 3. And the way that they did it was to show that if the graph, they actually characterize it, if the graph is either a complete bipartite graph minus the stars or a cycle of length because of y4, then it's going to get normal t this, and otherwise it has to be a minus g. So a minus g plus one never is achieved in power for the normal t. And then later on, these guys show that. These guys show that which graphs are minus G and the minus G minus one. These are some finitely many graphs that they show if your graph is those or obtained from those in some way, then the graph is going to call these otherwise it's not providing a finitely many graphs that satisfy these. Everything else is completely open. So the graphs, the null ta minus g minus k for t at these two. And we said the study of knowledge and rank are almost the same. So, how do we look at the rank problem? So, if you look at the vertex duplication, which simply means like the blue and gold one, the two vertices that are not adjacent and their neighbors are exactly the same, you see that a matrix, the two vertices give us exactly the same rows. So, in terms of rank, it doesn't matter how many of those we have, we can just have one of them. Rank is going to be the same. We talk about Going to be the same. We talk about those as reduced graphs, which means that's not okay with vertex. And at the same time, we also know that for any positive matrix of R, there are finitely many reduced graphs of rank R. So finding graphs with rank R is equivalent to finding those finitely many reduced graphs of rank R. And this has been studied for graphs of rank atmosphere 6. Everything up to rank 5 is completely characterized. These are graphs of rank. Characterize these other graphs with rank 5, or these graphs with rank 5. We can obtain any other graph by duplicating the vertex in any of these. But for rank 6, we know to a angle-free rank 6 graphs, they're completely characterized, but nothing else is known. So there are lots of other questions here at all. So far, we are only talking about 0-1 matrices, but at this point, we are going to talk about the SOG, which is the matrix set that Sean was talking about, the generalized form of the 0-1 adjacency matrix. Generalized form of the zero-one adjacency matrix. So the same type, but instead of ones, you are going to have non-zero entries. And the matrices are going to be symmetric, and the diagonal entries are going to be free. They can be zero or non-zero. And of course, now instead of just one matrix, we have infinitely many matrices. And the question, and then we're segmental problem is, what are the eigenvalues of these matrices? Which is obviously going to be a difficult question. So most of our questions are extreme questions. What is the maximum of something? What is the minimum of something? Maximum of something, what is the minimum of something, and something. And one thing that we could ask is that instead of talking about the eigenvalues themselves, what about the multiple C's of the eigenvalues? So if I look at all these matrices and set F sub G, they each have some eigenvalues. Those eigenvalues could be repeated. That gives us the multiplicity list. We list them from the largest to the smallest. The multiplicities, not the eigenvalues, and we put them all in. And we put them all in a set called L of G, L for list. And the question is: if I have a graph G, what is L of G? This is known for some few families of grass, such as packs, config grass, and linear trees, I believe, and some other very small families of grass. But other than that, again, not much is known. There are lots of really nice open fashions here. Some of the related questions here, again, extremal questions. What's the maximum? What's the minimum? The maximum only makes sense for the maximum normality. So, what's the maximum multiplicity of eigenvalues in all of the matrices in L sub G? And because we let the diagonal entries to be free, talking about maximum normality is the same as talking about the maximum multiplicity. So, we just simply look at maximum normalities, which is the same as looking at the minimum rank in a way, because minimum rank and maximum normality are exactly. Rank f x multi exactly the order of the matrix. So, again, there are a lot to know about either one of these parameters, but at the same time, there is not much to know. So, there's a lot of open questions here. Characterizing graphs with minimum rank equals k or any of these, for any of 1, 2, n, and minus 1, minus 2, those are completely characterized, but everything else is completely open. Lots of open questions there as well. Another related problem in the multiplicity area is that if you look at accept L of G, we talked about the lists of multiplicities. The question that we class is that what's the minimum length among those lists? And that's exactly called the minimum number of distinct eigenvalues of the graph. So looking at all the math PC lists with the smallest K, then that's it. Another way of looking at this parameter is the uh we are looking at the minimum degree among all minimal polynomials of matrices. Polynomials of matrices. This parameter, we started working on it with the Speak Map Research Group of Manchina in 2011 or something like that. Yeah, you were there too. And it turned out to be a really, really nice parameter. Like a very small example in this case solved sensitivity conjecture without me knowing about it. And especially in the case of Q equals Q, is equivalent to finding. is equivalent to finding patterns of symmetric orthogonal matrices, zero pattern of symmetric orthogonal matrices, which has turned out to be really, really interesting at the same time, really, really difficult questions. So there's lots of really good questions as well. I'm just mentioning the big pictures of questions just so we can talk about them in more detail. The next thing in the area of looking for the small queue. Cube. So when we want a small cube, we want the minimum number of distinct eigenvalues, right? So that means we want the eigenvalues to be repeated as much as possible. And that brings this question that how many eigenvalues the original matrix shares with the differential plot submatrices? And that becomes a very interesting question by itself. So I've introduced all these notations, but I think I explained much better with this picture. Things better with this picture. The question is basically: if you look at this matrix and look at the eigenvalues of the matrix itself, and then the eigenvalues of this principal submatrix, and then that one, and then that one, and keep going on through the end. If you put them all in one set, the coordinate of that set is called the Q, the big Q of the matrix, and minimum over all those is called the principal spectra or big Q of the graph. Of the graph and then we want to know about this because it answers a lot of questions about the lid over here. And at the same time, there isn't much known about this at all. This is a problem introduced by Brian Schader a few years ago in one of the aim groups. I know some groups have worked on it, but they give up on it because it becomes really difficult. And only now there are really a few things, such as the Q. A few things such as the Q, the Q of paths are colored by concept multiple of n to the 3 over 2, and it really just becomes more interlacing as you're forcing, which we'll talk about. But other than that, we even don't know big queue of complete buff and stuff like that. It's a very, very new and open question. My last thing that I want to talk about, yeah, so one thing that I want to mention is that this big queue, in the little queue, we didn't care about In the little Q, we didn't care about the order of labeling and labeling at all, because in either case, we will get the same number. But in this big Q, because we are looking at the principal sub-matrices in this way, the label limits are each different. So the way that we label the graph is going to make a difference in the dramatic Q. That's something that makes it even more interesting and more difficult. And this is the last second topic that I want to talk about. So generalized Laplacian matrices. So in that set, S of G, there are many. Matrices. So, in that set S of G, there are many families of graphs that we can look at. One of them is the Laplacian matrices, in particular, the generalized Laplacian, which are the matrices when they're off-diagonal, non-zero entries are negative. And their diagonal entries are chosen so that the row sums are all zero. When the row sums are all zero, obviously zero is an eigenvalue. And if the graph is connected, that eigenvalue zero is going to have multiplicity one. The question is, again, Is again what is the list of all possible policies for geothermal repository agencies? Again, this is a very new problem, there isn't much known about it. Sean and his colleagues, Jeffy and Himancho, just are going to submit, right? I'm not submitted yet, a paper on this, that they give some of the multiplices of some graphs, but that's, as far as I know, the only work done in this area. I know the only work done in this area. So, given that zero is going to have multiplicity one, we could ask the question of what's the minimum length among the multiplicities or even more general question, what's the entire set of multiplicities for a given class. So, lots of opportunities to work on questions here because, again, not much gets done. And the last thing that I want to talk about is something that I just learned about this Friday, so I don't know much about it. And one of the aim groups, Brian Shea. And one of the aim groups, Brian Sheager introduced this group, I don't know any of people here attended that meeting, the what is it called? AIM something? ARC, yes. So it's about symplectic matrices, which is defined in this way that the matrix S of size 2MY2N is symplectic if it satisfies its relationship with I and its identity matrix of size I. Size 9. And Williamson has shown that if that's the case, then there's a matrix D, a diagonal matrix D, such that for a positive semi-definite matrix A, we have that relationship. So this matrix can be looked as D from the diagonal, everything was zero. And again, this problem almost hasn't been studied at all. And one thing that we could ask is that for a given graph, if you start from A, where the simple A, so the eigenvalues, so the eigenvalues of B are how the symplectic eigenvalues of S, and the questions, what are the symplectic eigenvalues of matrices associated with a given graph. Lots of open questions here because this is like just introduced on Friday. And that's it for me. Thank you. One thing that I want to say is that because there are so many problems, Sean, and I wanted to see if anyone is interested in working these, and then we will give a lightening talk based on that. Yeah, sure. Yeah, happy to. By we, you mean you or me? Is there any question? Is that uh Z and G, is it um can it be computed in planomal time before any g? 'Cause I understand the m of g is difficult to compute. It's difficult to compute. I don't think so. I forget what version of NP it is, but I think it's known to be like NP complete or something like that. You basically have to check every subset of vertical to know if it's a model. It could be a little smarter about it, but it's these not I mean, I think even for like cubic graphs, it's it's like hard to do. It's not hard to do. There's a paper by a colleague in computer science, Bo Ting Yang, who I think proved that for cubic graphs, zero forcing is, I believe it's either NP-hard or NP-complete. I always forget what these all mean, but could it be one or two pieces of the same thing? Same answering? I said there's always seven minutes to say NPR that somebody right, okay, so that's probably the same. So to answer the question quickly, it's hard. Right, so then it's NP complete because you can just check all the Complete because you can just check all the exponentially any. I believe