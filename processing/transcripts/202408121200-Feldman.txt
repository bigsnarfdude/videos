I will speak about the work we have been doing in the last two, three years with Anna Maria Mantanari from the University of Bologna. Can you hear me? Okay. So there are several results I want to mention. And so let me give you. Okay, thank you. So, a brief outline of the talk is the following: I will explain the motivation for this problem we consider from optimal transport. Problem we consider from optimal transport. And the results concern monotone maps. And monotonicity here is with respect to a cost, which will have some assumptions. And we are going to prove that the monotone maps with respect to this cost are single value almost everywhere. And then from this, one can integrate inequalities because the inequalities. This is because the inequalities hold almost everywhere, and then we can get L infinity, local L infinity estimate for the map minus affine functions, and that will lead to differentiability properties of the map. So, let me just briefly say the background which Background, which two metric spaces, two measure metric spaces, excite the same measure. And it doesn't, I have many of these, so I got it's going to be like this all the time. So a map from X to Y is measure-preserving if. Serving if you take a set Borel set in Y and they take the image inverse, you measure the image inverse with the measure mu, and this is exactly the same as the new measure of the set E. It's the right button, right? I press it and doesn't. I press it and doesn't have to do it. It vibrates doesn't go back. I have my thing I can connect, but I know it works. So you omit because it will be. Because it will be oh, you use this okay and then what is the purpose of the input back this is back and this is forward okay so we have a a cost function A cost function, which, and as you all know, the Monge problem is to find team measure-preserving map, minimizing this integral among all the measure-preserving maps. And under general conditions on the cost, there is a C-concave function such that the optimal map is a C sub-differential. The C sub differential of this C concave function, which is given by this expression, and this P super C is the C transform given by this. Okay, so this implies very easily that the optimal map satisfies this inequality. Certainly optimal. So, if you switch the order, you will increase the cost. And assuming this single value, we can write this inequality almost everywhere. The point is to prove that this, if the optimal map, if the map satisfies this monotone, satisfies this inequality, then it's almost Almost everywhere is single value. So here is a focus. We take multi-valued maps from Rn to parts of Rn that are not necessarily optimal and not necessarily measure preserving, just satisfy this inequality. This inequality here for all points C and T X and Z and T. C in Tx, Zeta, and T of Y, and X and Y is in the domain of T, which is a set where the T of X is non-empty. And here, we are going to assume here are the assumptions on the cost. The cost is H of X minus Y. And the assumptions on the cost are the following. It is convex, it's non-negative, and this C2. And this C2, and this positive homogeneous of degree P for some P bigger than or equal to 2. So the cost we have in mind are powers, P powers of X. And an extra assumption, Thank you. So we are going to assume the following property: that the Hessian of the function h on the unit sphere is away from zero. And of course, the other inequality is free because the function is C2, right? Now, this, if the cost is a quadratic cost, this C monotonicity is a standard monotonicity property because if H is a quadratic cost, then you write down the inequality in X minus Y squared and you expand with the squares and you get the inner product. So, and this standard monotony. So, and this standard monotonicity is something that has been studied extensively and for many, many decades. It has many applications to PDE, to optimization, functional analysis, etc. So, this is a notion that is very well understood. So, the motivation for our research is the following. So, in Following this, so in 2020, Goldman and Otto proved partial regularity of optimal maps for the equality cost with Herler density. This is a, I think, it's a remarkable paper they did. They prove that there are open sets containing the support of Support of rho zero and row one. This is the rho zero is the measure on the on the space X, and rho one is the measure on the space Y. Full measure, so that the optimal map is C1 alpha diffeomorphism between the resulting sets. This is a very useful result. We have in mind, there are some applications. I'm not going to talk about that, but these are some applications that are used. But these are some applications that are useful for optics because you can remove a set of measures zero and then get the right inequalities and apply decompositions that are called for smooth maps. So this is a very, very interesting and important results. But this result was originally proved. This result was originally proved by Figali and Kim using the regularity theory for the Montgomery-Perry equation. But the point is that the Gold-Manoto result is a result that is independent of maximum principle and the Montgomery-Perry equation. They used the formulation of Benamu and Brenier, optimal transfer, the fluid formulation of optimal transfer, which is pretty well known, has many applications. Many applications, estimates for harmonic functions and Campanato iteration technique. So this was the got interested into this result, especially in the proof by Goldman and Otto. And what we tried to do was to prove regularity for when P is not true. Excuse me. Excuse me. And I think this is a very from the point of view of this method. Now, in this endeavor, in this work, we obtain L-infinity estimates for C-monitor maps and interpolating maps. And so, I'll. And so I point out that to prove the L-infinity estimate, the map is neither optimal nor measure-preserving. We just use the monotonistic C-monotonistic, not even the cyclical monotonisticity, it's just the C-monotonistic. And from the technique, we obtain differentiability properties of C-monotone maps. And there are results, the point is this cost that I explained before. Before they are different from zero outside of the diagonal, the diagonal is zero, and there are results, partial regularity results, pretty recent results, assuming that this is different from zero everywhere. Or they can assume that the targets are far apart. So we didn't obtain the regularity results for peak. irregularity results for peak was two but i will explain at the end what are the the things that are missing in the analysis many many items in the analysis of the of golden alto proof for work for p different from two using this l infinity estimate but there is a part which uh which is um it's not clear what is the replenishment they call the almost orthogonality property so let me begin explaining what are the replenishments Let me begin explaining what are the results. The first result is that the H monotone maps are single value almost everywhere. So I'm going to assume all along the talk that H satisfies these properties. It's homogeneous of degree P is C2, is convex, and the Hessian in the unit sphere is bounded away from zero. So the theorem says that the That tx is a single term for all points in the domain, except the set dimensions zero, possibly a set of measures. So the proof of this result is not trivial, but it's inspired in an idea of Caffarelli, an older paper of Caffarelli, I think it's from the 90s. And I will explain the proof in the case of proof in the in the case of using this Caparius idea, the proof in the case of when H is the quadratic cost. So in other words, when the map is standard monotone. And the idea is to prove, to use the following, this is a setup, like call S to be the set where T is not a singleton. And then SK is the collection of. Is the collection of points where the diameter is bigger than 1 over k. And then cover all the space by balls of radius 1 over 6k, just in many ways. And define the set BJ star are the points where the image of TX intersects the ball. And then obviously the union of this set is. Of this set is the domain of the map, as this set where Tx is not non-empty. And let's call SKJ to be SK intersection Bj star. So S is the union of SKJ. It's straightforward. The point is this. So we prove that SKJ has no points of density. No points of density. Therefore, the measure of the set must be zero, and therefore the measure of each of this SKJ is zero, and therefore the measure of S is zero. And here is the argument. So take a point SKJ, X0, in SKJ, and let Y be a point in the TX0 intersection. In the Tx0 intersection Bj, which is possible because of the definition of Bj star. And then there is a point of view. So I think do you think it would be better if I use my computer or because if this were to happen every five minutes, I mean we it's okay. And then I need connect the in the Zoom session in the laptop. Okay. Okay. At least this is not going to happen. At least this is is not going to happen anymore. I mean, if it will happen, you think it's going to happen continuously or or yeah. Okay, let's try. Okay, I will not. Okay, I will just point out with my finger then. So there is a point y2 in tx0, so that the distance between the y1 and y2 is bigger than 1 over 2k. Because otherwise, if this point doesn't exist, Tx0 is containing the ball of radius one over two k around y one, and this implies that then the diameter of tx0 is less than one over k, and this is impossible. And this is important to point this in next kill.  Well, I'd be happy to give you the slides. I mean, you don't need to copy, but this slide is important. This slide is important. Yes, I have enough time to copy. You can take a picture.    Okay. All right, so this is not going well. Okay, so the last thing I said was this: that this is the I said was this that the this is the this is the we can pick a point y2 in tx0 so that this is okay and we also have another important thing is that this y2 is in is not in the double of bj and this is because of the choice of this of the radius and the choice of the the case and so on so this the point um y2 is away from from uh this is out Is outside to be J. Okay. You are moving this from there, right? You are moving this from there because I'm not touching this. I think it's low. Oh, you have to give it maybe two seconds and it'll be or maybe more than okay. Okay. So this means that if you take a point so that you have these two points, take the vector difference between the two points and consider the cone vertex at x0 and axis e and opening delta. It's just a cone at x0 with axis E. X0 with axis E, small opening. And then by the monotonicity, so if you take a point X bar in this cone and Z in Tx bar, since Y2 is in Tx0, you have that Z minus Y2 X0 minus X0 is bigger than or equal to 0, standard monotonicity. So this means that the angle, the point Z is in the cone which has angle. The cone which has angle less than or equal to pi over two plus delta. So the angle between x0 minus x bar minus x0 and e is delta is less than delta, and the angle between c minus y2 and e is less than is less than pi over 2 because of this. The angle between these two vectors is less than pi over two. So the angle of the combination is is l so it's it's a it's a cone that has opening a little bit more than pi over two. Opening a little bit more than pi over two. So this means that T of gamma is containing gamma zero, but the y two is not in 2Bj. So for delta small, this cone does not intersect Bj. So here's the picture. So this is the configuration. So this cone, if delta is small, if delta is very big, it's going to cut, but if delta is small, this doesn't intersect. If delta is small, this doesn't intersect the ball. So this means that TX bar intersection BJ is empty for all points in gamma in the initial cone. And then that means gamma intersection Bj star is empty. Okay, so since SKJ is SK intersection BJ star, we have that you write SKJ intersection Br, now Br is an evolved around X0, and you have that this is empty. So there is no intersection with the cone gamma. So this means SKJ intersection, the ball BR is just the intersection with the complement of gamma. Section with a complement of gamma. But that implies that if you take the outer measure, this point here, you don't know the SKJ is a measurable set, so you have to take out Lebesgue out the measure. So if you take the outer measure of SKJ intersection of the ball, this is simply the outer measure of this set, but this set is contained in the ball intersection the complement of gamma, but this gamma has opening delta. Opening delta, so this is small than one minus the constant depending on delta, which means that this ratio is always less than strictly less than one for any r so this means that the point x0 is not a density point. Okay, so this is the proof in the standard monotonicity case. So the implementation of this proof for general cost is non-trivial. Is non-trivial and it requires a formulation of the H monotonicity used in the inner product. The definition of monotonicity used in the H is not very friendly. So we can write what the monotonicity before is because is the standard monotonicity. The standard modernicity, so is the x minus so I can write here. So the is within a product, we use that. I'm talking about the quality cost here. Okay, so moving forward to the general cost. So, this is the definition of monotonicity. Now, we write this in terms of integrals, in terms of gradient. We only want to use our condition with on the Hessian. So, we write the first difference as an integral of the gradient, the same the second. Same as second. And then you put together the two, and then you form a second cation. And so you switch here, just the smallest thing, switch the signs. And then you get that you have a quantity here, which is a matrix, depending on the. Depending on the four points, x, y, c, and eta, and zeta. And this inner product is on negative. So this matrix is the identity when you are in the standard monotonicity case. So here is a matrix which is given by this formula. And then the map is H monotone is if this inner product with this matrix is bigger than or equal to. With this matrix is bigger than or equal to zero. And this is the matrix. The matrix is symmetric and satisfies its properties. If you switch X and Y and you switch C and theta, they change. It is the same matrix. And since the H is homogeneous of degree P, the Hessian is homogeneous of degree P minus 2, so then you can have this ellipticity condition on the matrix A. Ellipticity condition on the matrix A, you just write, multiply and divide by the absolute value of the argument and use the p minus two monotonicity, and you get this kind of elliptic condition with this function. Okay, so far is okay. I mean, the okay, so this is the notion we use. The notion we use, and then we have to do re. I mean, you have we have to figure out how to do the argument with this cone, cones that depend now on this matrix that varies. And this is complicated, but I'm not going to explain this here, but I'd be happy to share the paper with you. Okay, so some extra consequences of this is that if T is H monotone. Is that if T is H monotone, the inverse is H monotone, and the set of points where the set of points that belong to two different, the images of two different points by the map in T is as measure zero. And then you can define a push-forward measure in this way. But this is just for a monotone map. And there is an extra Extra consequence, which is the set of points xc, where c is in tx, x in the domain is rectifiable. This was pointed out to me by Robert McCann. And this is the ideas are in a paper with Warren and with Brendan Pass. You consider sets that are monotone and you prove rectifiability, but the map. Rectifiability. But the map, this doesn't imply the map is a single value because you can have a map. You can have a map, and then if you add K, so you have a pile of maps, and then the map is multi-valued, but the sets are rectifiable. Okay. So here are the L infinity estimates. So p is greater than or equal to 2, the function is. are equal than two the function is homogeneous of degree p the Hessian is strictly positive in the unit sphere and it's a non-negative and it's convex also and we have a map of this have some we need some some integrability on the map is Lp minus one to this monotone and then what we estimate is how far is Tx from a fine combination. Tx from a fine combination. So we take ux is tx minus axb, a is a matrix, b is a vector, and then for each point and each radius, we have this estimate. I will come back to the estimate in a writing down in a more friendly way. So the supremum of a smaller, slightly smaller ball is controlled by this quantity. quantity by sorry by this quantity if the p minus one average is small compared with r and is smaller than this quantity if the p minus one average is big compared with r and the constants that depend only on the on the ellipticity of uh i mean the hessian of of uh of h and the norm of the matrix And the norm of the matrix and the parameter beta, of course. So this gets worse when beta is close to one. So here is a rewritten of the inequality. So if you write the p minus one average of the u, u is tx minus the affine function, then the bound can be written in this way. The supremum is bounded by Is bounded by this power of r and the p of p minus one average to this power, but you see, if if a p minus one average is smaller than r so this is the kind of a convexity inequality because the sum of these two exponents is one. So if the a p minus one is smaller than r so you gain something here you can put this quantity and if the the r the a p minus If the AP minus one is bigger than R, it's just controlled by the average. The second inequality is kind of the reverse Kelder inequality. Okay. So I don't know if I have 25 minutes. So okay. So here is the idea of the proof. I will the proof is not. I will the proof is non-trivial, okay? It uses the homogeneity. I mean, you have to, it's hard work to do it. But let me just point out: the starting point of the proof is this potential theory formula. So if you have a function t2, you can write always a function at y as the average of the function on the ball of radius r plus this term, and the gamma is the fundamental solution of the Laplacian. Solution of the Laplacian. Okay, and then you have to introduce some quantities. So the main idea is you introduce some quantities and rewrite the monotonicity condition in a way that you have all the variables x on the left and no u of y, no u of x on the right. Y, no u of x on the on the left, and you have the u of x on the right because you are going to integrate this inequality using this formula. So, there are there are terms you have to estimate. I think I will just briefly there are you apply the potential theory representation formula with this function. You have to adjust the radius of the ball, you have to. You have to look at what point do you use the formula and then you substitute, and then you have to estimate this from below and these two terms from above. And well, this can be done. And so one that the first term is not difficult to prove is bigger than. Remember, we want to bound u of y, the absolute value of u of y. So the homogeneity estimates the term two. And to estimate the term one, we use the inequality, we use the monotonicity. And in the end, there are. And in the end, there are quantities you have to optimize, which when you optimize, it's not surprising that you get some convexity inequality. Okay, so this is here is the application to differentiability. So I'm going to I'm going to this definition was introduced by Calderon Singman in the 60s. The problem they were studying was how differentiability is preserved by singular integral operators. And the standard notion of differentiability is not preserved by singular integral operators. But this is, this is. So they found the notion of differentiability that is preserved. So here it is. So you have a number P. So, you have a number p and you have a real number bigger than or equal to minus n over p for figurative reasons. You have a function in L P and then you say they introduce the classes T K P. I don't know if you have seen this before. Probably you did. So, K a function is in the class TKP if there is. p if there is a polynomial p of x0 of the degree strictly less than k if k is negative the polynomial is zero so that when you take the average this is model on the taylor formula right the average of f minus this polynomial is big of r to the k as r goes to zero so this is the space p kp and there is And there is the little TKP in which the polynomial has degree less than or equal than k, and the big O is replaced by small O. So these two notions were introduced by Calderon Sigmund. And of course, if P is infinity, you replace the average by the essential soup. And they prove a And they prove a remarkable result. They prove that if you have a function in TKP for all points in a measurable set E, then the function is in the little TKP for almost all points in E. And these order magnitudes are not uniform, so it's just at the point. And this is an extension of a well-known theorem of Stepanov. Stepanov proved this when P is infinity. And then we combine these results to obtain the L infinity estimate to obtain differentiability. Okay, so here one corollary was the differentiability. Suppose you have a T H monotone map. H monotone map, and you have t in L P minus one of a small ball, and there is a matrix, an assumption there is a matrix and a vector, so that the p minus one average is small over. Then the function is in tp one, t one p minus one. So this means the function is differential in the ordinary sense. And this uses the L infinity estimate. It's very simple. L-infinity estimate. It's very simple, the proof. So to set ux is tx minus axv, and the call phi r is the p minus one average. And then from the assumption, p r over r goes to zero. And then we are in the first situation with the L infinity estimate. We can estimate the supreme. We can estimate the supremum of the ball by this convex combination of these two things, which you write in this way. So you divide by R, and this goes to zero, so you get differentiability at x0. What? They are given. Suppose there is a matrix and a B. Yeah, your assumption. Lead alone, leave it alone. This is a little this little thing, yeah, yeah, yeah. Yeah, this has a big oh, but this has a little for the hypothesis the corollary thing. The hypotheses are p is bigger than 2 and h satisfies all the properties at the beginning. No, no, no, I'm not using color and sigma here. But I mean the you have to assume that there is a point zero. Yeah, yeah. No, no, no. No, no, no. This is just this is just the, it doesn't use color and signals. Color and sigma. It's just this small O condition implies differentiability from the L infinity estimate, just from the L infinity estimate. But you need integrability of the T okay? I just mentioned the colour and signal result. I'm going to use the Stepanov actually in a moment. I'm not going to use the full Cadder and Signal. So here is a corollary. Another corollary. So again, is log in p minus one. So the other one is the pointwise result, right, at the point. This is that. So it's big O, the average of Tx minus B, T minus one is big O of R for some B. And this is true for almost. And this is true for almost for all points in a measurable set E. So this proposes. Then you have that Tx is in T1 infinity, little T1 infinity, for almost all points. This will use Stepano. Okay, so here is the proof. Okay, so here is the proof. So for each x0, by the assumption, there is a constant and a radius so that, and a vector so that this p minus one average is smaller than m of x0 times r. Okay, so this is just the assumption. Now we ought to use this L infinity. Now we ought to use this L infinity estimates. We have to test. So, given R, we have either the P average smaller than R or is bigger than R. The U is Tx minus P. So in the first case, from the L infinity estimate, we are in the case in which you have a convex combination, but this is smaller than mx0 r. M x 0 r so altogether is less than r constant times r depending on x 0 in the second case we have to use just the supremo is bounded by the p minus 1 average but the p minus 1 average is bounded by m x 0 r. So for at each x 0 we have this inequality. So this means the supremum supremum is more is big over r that means that the tx is in t1 infinity but now uses the panov stepanov implies that this is differentiable almost everywhere okay so the l infinity estimate makes it okay so the Okay, so the result for, so this is related to there are some consequences of this estimate for monotone maps and bounded deformation. So a locally integral mapping is a bounded deformation if the symmetrized gradient in the central distribution is the radon measure. This is the notion that appears in elasticity. Now, if you have a standard monotone map that is locally in L1, then the map is of bounded deformation. And this is because the standard monotonicity implies that the distributional derivatives are non-negative and therefore they are addon measures. Now, there is a Now, there is a very nice paper from 1997 from Ambrosio, Kochi, and Dalmasso that they prove that if T is in bounded deformation, then T is in T11, little T11 for almost all points. But if E, the map T is in the space T11 at X0, this implies in the big T1. This implies in the big T11, which is the condition of the second corollary, and then we obtain the differentiability of T. So this combination of this corollary before and the Ambrosio Cauchian Delmasso result implies that the map is differentiable almost everywhere. And this generalizes the map. This is for standard monotone, right? Monotone, right? This analyzes the result that proved earlier by Mignon, but it was proved for maximal monotone maps. This is just any monotone map. Okay, so what happens if you consider the cost? The question is, if you consider the cost, what can you say about the differentiability? Okay, so the differentiability for general cost. General cost is that this matrix, now T is H monotone. This matrix is positive semi-definite in the sense of distributions. This can be proved just from the monotonicity. And then the matrix A can be represented with a matrix value radon measure. And when the H is quadratic. when the when the h is quadratic this agrees with the notion of bounded deformation this is just this extra term which is harmless but if if t is locally bounded which is a consequence of the l infinity estimate the matrix there is the matrix value distribution of order zero And therefore, the matrix the matrix here can be represented as a s a sine radon measure. Therefore, this is in T one one of x zero. In T one one of x0, little t one one of x0. So, the colours are a little bit more difficult. So then we have the following result. Just to wrap up the result, if you have a monotone map, H monotone map, which is L P minus one, locally L P minus one, then dH of X Tx is abandoned deformation. So the T is inside this expression. This expression and ideally would be good to take it out of this expression, but we have this combination and belongs to T11. Now, this, I have how much time. 10 minutes. Okay, so let me finish with this. This is the consequence of the result I just explained. There is a Keller continuum. There is a Helder continuity result for H-monitor maps. So if the maps are locally in L P minus one, they satisfy a Helder time condition. So T is in the space T1, P minus one, infinity. P is bigger than two, okay? Bigger than or equal to two. So this can be proved using first a couple of lemmas. The first lemma is a calculus lemma, which is the variant. Which is the variant of the lemma that we use to prove the L infinity estimate. So, in the proof of the L infinity estimate, you need some calculus lemmas. And this is a simpler situation of those lemmas. So, we have this quantity which is bounded, is comparable to the maximum of the vectors V1, V2. Some constant and also, um, it follows that from the previous formula and you write down the using the integral representation, h representing with the gradient, this follows from so the difference of the gradients at two different points is bigger than a minus b to the p minus one. Okay, so then here is the theorem. So we have an H-monotone map, which is in Lp minus 1 locally, and H satisfies the assumptions I said at the very beginning, then the supremum of Tx minus Tx0 is big O of R of 1 over P minus 1 for almost all points. This is the kind of Helder continuity condition. For T and the proof uses the fact that the gradient of H evaluated at X minus TX is T11 from the previous theorem. And then at each Lebesgue point of this function, it follows this. Because if it's T11, the polynomial is constant. The polynomial is constant. And in the definition of T11, if there is a constant, there is a polynomial, it's unique. Polynomial is unique. You can prove it's unique. And this satisfies that similar thing with the different, with the Lebesgue, because it's a Lebesgue point. You got this, the difference of the gradients is big O R. Okay. Now, applying triangular inequality and the previous lemma, so that the gradient of h at a minus b is bigger than a minus b to the p minus one, you estimate tx minus tx0, p minus one, this triangle inequality. Put the p inside, you lose the power constant, and then And then here, this is bounded by this, and this is just calculation: O R to the P minus one, but P is bigger than two, so this is big O R. So this is O R. So this altogether is O R. Then for each, for almost all points, you have that the P average of Tx minus Tx0. average of tx minus tx0 is bounded by a constant depending on the point times r to the one over p minus one and now apply the l infinity estimate so you have to test either this is smaller than r or this is bigger than r so in the first case if it is smaller than r you have the convexity inequality which all together gives you Which altogether gives you a different constant times R. And in the second case, you just get by the L infinity estimate the quantity, which is r to the p minus one. So, but this is this is bigger than this. So, you get the supremum is big O of R to the one of. Go of r to the one over p minus one. So you get the failure continuity. And the final remarks. So L infinity estimates for optimal maps were proved. So there are L infinity estimates available in the literature. They were proved by Bushite, Jimenez, and Mahadevan. Mahadevan, but they use the P-vases transdistance. And now, as I said, so an important ingredient in the proof of the partial regularity of optimal maps are L-infinity estimates. And several of the steps that Goldman and Otto proved for quadratic cost can be extended, but as I said. Extended, but as I said, there is a missing part still in the analysis, which is the positogonality condition that we don't know how to handle it for posts that are not quadratic. Also, another observation, we prove a first paper with an Amaria in which we have LISD estimates. Which we have L-infinity estimates in terms of average of p averages of Tx minus X P. And here we have estimates in terms of P minus 1. So the P average and the P minus 1 average are related. It seems that the P minus 1 average is more fine. So you might get finer estimates. So you might get finer estimates with p minus one than with p. But I don't know how what is the impact on that in the argument of Goldman and Otto. Maybe this is something to be checked. And then the argument we use to prove L infinity extends to power costs in the Heisenberg group. So there are some optimal transport made by Heisen. were made in by in the heisenberg group this pioneering paper by ambrosio and rigaud and some of these uh the things we prove can be extended in that that set and here are the references so the first paper gi case the the estimate of tx minus x which they call the excess Sex, which they call the excess. I don't know what they call the excess, but we prove it. Also, all these papers I with Anna Maria Montanari here in 22. And this is in our key. This is version 2. Version 1 was a more condensed version, contained differentiability properties. Version contained differentiability properties also, but we decided to split the version one into two papers because it was too long. And so, this contains just the fact that monotone maps are single value almost everywhere. And this paper is the concerns about concerns about the differentiability. And that's all I have to say. Thank you very much for listening. Thank you very much for listening. Okay, so thanks for a great talk. So you're getting the linkedin D estimates just from sort of two models. Just from sort of two monotonic two h two monotony, yeah. So, I guess when p is two, there's that result by Cafali and Millman, where if you have like two plus epsilon monotonic plus bounded information, then you get C alpha. So we're going to say it again. So if you have more monotonicity, oh, two plus epsilon, two plus epsilon, maybe for like very You get, I don't know, I didn't think about that. Yeah, I know the paper by Cafaria Millman, but I didn't realize the connection. Good that you point it out. Yeah, yeah. Thank you for the talk. You proved that this. You proved that this S set, this singular set, has measure zero. Do you expect more structure? Can you prove? Yeah, I think that's a good question. I think the proof, or at least in the standard monotone case, right? I think with the argument with the balls, if you move the balls in a certain way, you probably can estimate the household dimension of the set. Would you expect that to be equal dimension one, rectifiable? One rectifiables like that? Uh, well, I think these are probably n minus one, but I don't know. Thank you. Thank you for the very nice talk. Uh okay, so I believe the author and others result uses the the information of the passport. I wonder if you can use that formation Q in the or P. Yeah, we use you mean the with the Brainy approach. Yeah, yeah. So we proved these things in the first paper. Yeah, this is proven the first paper. We prove estimates of this type. Yeah, yeah, yeah. Thanks for the nice talk. I just have a kind of naive question about the homogeneity assumption. So, is it true that the only place that this assumption plays is like the kind of getting the elasticity constants with the A's? So, you remember the P homo, like the cost, like H having to be homogeneous? Yeah. So, I was just curious as to how. I was just curious as to how much you would expect to be able to relax. Like, do you expect this to hold work? Well, we use heavily the P monotonistic very much because we are modeling the case of the P power. I don't know what color. Yeah, you probably you can extend, but I don't know what would be the interest. I mean, it's a class that is pretty clean, the T monotone, but I don't know. D monotone, but I don't know. Yeah, but probably button.