I'm going to talk about a model that is often used in the economics literature, the dynamic panel logit model. And I'm going to focus on, most of the talk, I'm going to be focusing on identification with a little bit discussion on estimation and inference. So this is joint work with Chris, who used to be a PhD student at Toronto, now at Google, and Korea. At Google and Kurio King at Michigan State. Alright, so the model that I'm going to focus on is specified here. So we are observing a panel of binary choices. So T here denotes how many choices that I observe for each individual. The binary outcomes is pinned down by this model, which contains latent effects. So in econometrics, we call this fixed. So, in econometrics, we call these fixed effects. So, this is capturing the individual heterogeneity, but it's unobserved to the researcher. We model dependence. So, in applications like firm entry and exit decisions, so we understand that if a firm is already entering the market, chances are that in the next period he's gonna stay in the market, right? So, this is captured if beta has a positive sign. This is saying that the payoff. This is saying that the payoff for those that have already entered in the last period is going to be bigger than those that have not. Okay, so this part is building in state dependence of modeling this dynamics. Okay, there might also be covariates that influence payoffs. Those are captured by gamma prime XIT. And epsilon YT is an idiosyncratic shock that is assumed to be independent over time and follow a logistic distribution. And follow a logistic distribution. So you can view these as some form of a mixture model as well, where the base density is the logic distribution, while as the mixing part, the latent part, is introduced through the fixed effects alpha i. Okay, so this model is used and to add on, we are not going to make any parametric assumption on the latent effects. So this is just, we are just assuming that this is the latent. We are just assuming that this is a latent variable, follow some unknown distributions that could possibly depend on the value of the covariates. So, in nutshell, the takeaway message is this is modeled very flexibly in terms of the latent effects distribution. So, this model is used very, very often in industrial organization as a branch of economics. Oftentimes, this is brought to bear to analyze consumer purchase decisions over time or firms' entry and exit decisions. Or firms' entry and exit decision over time, or in general, as binary longitudinal data models that can be used to analyze those kinds of data. Okay, so the parameter of interest are either structural parameter, right? So beta captures state dependence, and gamma would tell me how these covariates contribute to the payoff. At the same time, we might be interested in the latent effects distribution, partly because a lot of Distribution, partly because a lot of the average marginal effects of understanding how entering or not entering the market in the last period might influence choice probability. So these would involve functionals of Q that we would also try to cover and discuss how to identify those quantity in the paper. There are variants of this model in cases where more dependence wants to be introduced. So we can think of AR2 models where both the two periods. where both the two periods, past choices, will be entering and influence the payoff for individuals' choice in time period P. All right, so now what's the key challenge of analyzing this model? Partly we have, we are facing the incidental parameter problem, right? Because each individual has their individual specific effects. If I just simply introduce individual dummies, then we know that because of the nonlinearity, And we know that because of the nonlinearity of the model, those structural parameters will not be consistently estimated. And also, regarding the functionality of Q, one key challenge for this model is the distribution Q is not point identified due to the binary nature of the data. So both of these would kind of show up as the key hurdle that we try to bypass in the identification analysis. Identification analysis. All right, so let me speak briefly about how this model is traditionally being handled. One key method is Chamberlain 1985. You can view this as some sort of generalization of the Anderson method to handle rash model. So here, the difference between this model and the rash model is we have dynamics. Okay, so the idea of the chat. Okay, so the idea of the Chamberlain 1985 is: okay, we have incidental parameter problem rooted in individual specific parameter alpha i. So, one way to bypass that is to look for sufficient statistics of alpha i. And in this model here, this will take the form of the initial choice, the termination choice, and the sum of choices in between. And once we have the sufficient statistics for individual alpha i, we can factor out Alpha i, we can factorize the likelihood of the model by a piece that is free from alpha i by conditioning on the sufficient statistics and a piece that would depends on fixed effects, which would have the mixture model form showing up here, right? Where I'm integrating out the distribution of Q. Okay, so the idea is that these piece of the likelihood still gonna give you enough identification power and also convenience for estimation. Also, convenience for estimation. So, this is the so-called conditional MLE method that is often being used for handling this model, right? Because this part now is free from alpha i, so I no longer suffer from the incidental parameter problem. And there is lots of extensions trying to extend this beyond the Chambling model, which does not have covariates. For instance, a well-known paper, Honori and Curiazido, tried to extend. 2000, try to extend the sufficient statistics idea to allow for covariates in this model. Now, the bad news is, at least in this paper, they show that for this method to work, one would have to incorporate restrictions on the covariance. So for the second and the third period, if you have a model with the initial choice and three more choices observed, then we need to require that x2 equals to x3. So this is restrictive in the sense that a lot of Restrictive in the sense that a lot of the data cannot be used, right? So, think of example where x2 is not equal to x3, those cannot be brought to bear for estimation. And also, these would rule out models including time trend, because with the time trend as the covariates, this by construction is not satisfied. And beyond that, the sufficient statistics method also have limitations in extension of this model. For instance, for AR2 model, the sufficient Model, the sufficient statistics for alpha i is the whole choice sequence, meaning that if I conditional on that, this part of the likelihood is flat. So it contains no information about the structural parameter. So this method cannot be used for the AR2 mod, for instance. Okay, and also if the panel lens is very short, if you only observe the initial choice and two more choices, which is oftentimes the type of data that one have with sperm entry and exit decisions. Entry and exit decisions, then the sufficient statistics method also do not provide any identification. Okay, so what we are trying to do in this paper is to put the sufficient statistics method aside and to come back to the basics and ask what is the identification for the parameter of interest. What we mean by that is given that the model that we have specified plus the logic distributional assumption, what are all Assumption, what are all the model restrictions regarding parameters or this latent distribution Q? And the key result that we show is the identification problem shows up to have a connection to the so-called truncated moments problem in the mass literature. So this has a long history, dates back to Chebyshev 1874. And in a nutshell, the truncated moments problem is essentially the following. Okay, so think of any random marriage. Okay, so think of any random variable x, say a scalar random variable. Suppose that I give you the first k raw moments of a random variable. The truncated moments problem literature is essentially trying to characterize the set of probability measure that x could have by matching these first k-raw moments. So you can imagine that there will be results about existence. Does there exist a probability measure that could Exists a probability measure that could match this k first row moments? And also, results on uniqueness. Is there a unique probability measure that can recover these first row k moments? So we're going to show that these actually give rise in the dynamic logic model, which helps a lot in the identification analysis. Okay, so use this connection. Let me briefly introduce the type of results that we obtained in this paper. So the first type of So, the first type of result is regarding structural parameters. So, we showed that all the model restrictions for the structural parameter can be characterized by a set of moment equality condition and some moment inequality conditions. And the number of moment equality conditions can be or may be substantially more than those that can be found by sufficient statistics approach. So, this brings some good news. So this brings some good news for models like AR2 model, where one can actually find moment equality conditions. And also for models of AR1 with covariates, the limitation of having to assume x2 equals to x3 can be lifted. So there are actually more moment equality conditions available in this model. On the other hand, the set of inequality conditions can sharpen the identified set. Sharpen the identified set for the structural parameter when they are not point identified by only using the moment equality condition. Okay, so this is showing that these inequality conditions are also useful identification information for handling models. For instance, the AR1 model with time trend, I will have an illustration example where the inequality conditions can actually sharpen the identify set to a singleton. Set to a single term, while only using the moment equality condition still gives you multiple roots, which you don't know which one is the true parameter. Regarding the results on identification of the latent distribution Q, we show that one can basically characterize the identified set of the latent distribution by a finite vector of generalized moment of very peculiar form. So I'll be explicit on that. Peculiar form. So I'll be explicit on that. But essentially, one way to understand this result is the data information is basically reduced down to a lower dimensional moment vector for this latent distribution. And one can show that the number of moments for this Q grows linearly in T. So this gives you some appreciation as how the panel length increases, gives you more information about the latent distribution. Latent distribution. And using this result, we're going to provide sufficient conditions on point identification for some functional of Q. So remember that I was mentioning that in the binary choice panel data, Q is often not point identified. There are multiple Q's that are observationally equivalent. But it turns out that there are some specific functionals of Q that happens to be point identified in some setting. And if so, these sufficient conditions. And if so, these sufficient conditions are going to help researchers to pin down exactly what functionals benefits from those point amplification results. And beyond that, we talk about estimation and inference if we have time. All right, so I'm going to start off with a very, very simple example with two periods, no covariates, just to pin down intuition. And then we're gonna generalize those results to general t and And then talk about both identification of structural parameter and functionals of Q, and then give some examples of how it can be used in practice. All right, so let's see what the model plus the logit error is giving us through as a model restriction, right? So what we know is basically this script y here is the set of all possible choice history. Of all possible choice history, since I have binary outcomes, so the j would be 2 to the power of t, right? Because each period has binary choices. PJ is going to denote the choice probability. This part is identifiable from the data, right? Imagine you have infinite number of data. You can identify what's the choice probability for a particular choice history. The Logit model, combined with the IID assumption of the Logit Indios increase. Assumption of the Logette idiosyncratic shocks is going to tell us that the likelihood takes this form here. So, this is the familiar Logett distribution form, and because of the IID, they are product form over time periods, t. Okay, so just to introduce some notation, I'm going to stack the choice probability as a vector of lengths 2 to the power of t, and also the L as the likelihood vector, which is going to be a function of the latent effects. To be a function of the latent effects and other components in the model, right, like parameters and value of the covariance. Okay, and one quick remark is because of the logit distribution, alpha, which is the individual latent component, is always entering through the exponent transformation. So for convenience of notation, I'm going to do a change of variable and call capital A as the exponent of alpha. As the exponent of alpha. So nothing is changing, right? Because it's always entering through the exponent. I can as well imagine my latent variable to be the capital A. And now the distribution of that, I'm going to abuse notation to call it still Q. And this will be a random variable supported on the positive real line because of the exponent transformation. Okay, let's talk about our key object of interest, the identified set for the structural parameter. For the structural parameter. To do so, let me first introduce the set scriptQ. So, this set is basically if you fix a value of theta and also covariate x, this contains all the probability measure such that for that particular value of the theta, it can recover the choice probability that one identified from the data. So, this is basically saying that the set contains. The set contains all latent variable distributions that are observationally equivalent, that can recover what you see from the data. Then the identified set for the structural parameter are essentially those values of SATA such that this set is not empty. This line basically says that all values of theta such that there exists at least one probability measure that can recover the choice probability, those values are. Probability, those values are in the identified set, meaning that all values of zeta in this set are observationally equivalent because I can always find a latent distribution Q that match with what I see from the data. Okay, all right, so as promised, let's look at a simple example to try to see the link to the truncated moment problem. So, with t equals to 2, I would have four distinct choice history: 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 001001 or 11. And the logit form is telling me particular form of the likelihood functional form. So these are written corresponds to the particular choices right. Now what we do next is to take a common denominator. So this G here is the common denominator of these four components, which is of the form of 1 plus a squared, which appears here. plus a squared which appears here and one plus a b which appears here. Once you do that on this part of the vector here are nothing but with their components as polynomial function of a. This is a polynomial of order one. This is a polynomial of order three and all of components here in this vector are polynomial function of a okay so and in t equals to two these polynomial t equals to 2, this polynomial order is up to 3. And more generally, in general t, we're going to show that it's going to grow linearly in t and take form 2t minus 1. Okay? All right. So then what happens is introducing the mixture model, which we're going to link to the choice probability, what happens with my choice probability is I'm going to collect all the coefficient. All the coefficients in these polynomials into the matrix G. Now, this matrix G is only going to depend on parameters. A does not enter here, right? Because these are the coefficients of the polynomial. Then I have the polynomial sitting here. I have the scaling. And then I integrate with respect to whatever probability measure I have. So in a nutshell, what the logic model is giving us is the choice probability. The choice probability can essentially be written as some matrix that only depends on structural parameter multiplied by a vector of moments. Okay, but these are not moments of the original probability measure. These are the moments of a redefined measure, q bar, which is defined particularly in this way. One can easily show that 1 over g is belongs 0 to 1, therefore, One. Therefore, if Q is a probability measure, then Q bar is going to be a non-negative Borel measure on the positive real line. So, what this result is basically saying is the Login model, the choice probability, can be written as a matrix multiplied by some moments. And this is gonna pave the way towards the connection to the truncated moments problem. How does this help us in identification analysis? In identification analysis, recall that the identifying condition is if the beta is in the identify set, it means that I can find a probability measure Q to recover my choice probability. And now because of the way that I reformulated the likelihood and hence the choice probability, I can move G beta inverse to the left-hand side and equivalently say that beta. Equivalently, say that beta is in the identified set if and only if there exists a non-negative Borel measure such that these vectors can be rationalized as the first three raw moments of some Borel measure Q by. So that switch towards establishing a connection between characterizing the identified set to checking whether a particular vector can be rationalized. Particular vector can be rationalized as a truncated moment vector for some non-negative Burial measure. And here with the definition of the moment space, which collects all vector R that can give rise through, that can be rationalized as the first raw k moments for some non-negative Borel measure, nails down the form of the identified set in this simple t equals to 2 example, essentially. Two example, essentially, we are going to collect all beta values so that this vector r beta belongs to this moment space. Does this introduce restriction on the set of values? It does, right? Because moment space has some unique geometric structure. First of all, it's a closed convex cone in Rk plus 1 space. And second, moment space is not going to spend the whole R K plus 1. Spend the whole R K plus 1 space, it has restrictions. It needs to respect Cauchy-Schwarz inequality because variance has to be non-negative. And also, more generally, Holder's inequality has to fold. So, in the sense that this restriction here does provide non-trivial constraints on beta, and we proved that these are all the restrictions that is available in this model for t equals to 2, and more generally for any t that this is going to carry. That this is going to carry out as the feature. Then, making use of a result in Carling and Studden. This is a classical result in the truncated moment problem literature. The restrictions through this essentially boils down to checking non-negativities of two matrix. This is often called the Henkel matrix in this literature. So, this matrix here basically contains some elements of the vector R, and similarly, this matrix here. matrix here and non-negative non-negativity of matrix essentially corresponds to non-negativity of all principal minor and this gives us the moment inequality that I was mentioning in the introduction. So for our particular example here, the characterization of the identified set is essentially all these inequalities which are going to involve choice probabilities. Choice probabilities as well as structural parameter, and these would give us the characterization of the identified set. So, just an illustration example of whether these can be informative or not in real example. So, here I'm considering the latent effects to be a discrete distribution with half of the probability it's going to take value minus two and the other half of the probability it takes value one. Of the probability, it takes value 1. The left-hand side of the figure should be viewed accordingly as on the x-axis, I'm varying the true parameter value beta 0 from 0.1 all the way to 2. These are after you taking the exponent. And on the y-axis, it gives me the bounce for the structural prime, right? And the green curve here is the true value. So you can see that for these DGP at least. You can see that for this DGP at least, it does give quite informative bounds of what the structural parameter is, despite of the fact that we have an extremely short panel, only two periods, and yet we could identify some bounds for the dependent structures for this model. The right-hand side, I'll come back later. All right, now, what can we say about the distribution of Q? Okay, recall. Okay, recall that this is the reformulation that we get from the logit likelihood. So I have a vector r beta, which can be written as some form of moments. Instead of looking at q bar, now I have to focus attention on q. So this is what I would call the generalized moments. So instead of the usual raw moments, this will always have the 1 over g carried around because of the logit form. So, but we can say. form. So, but we can say that the identified set for distribution q is basically going to be characterized by this vector r, which I call generalized moment. So for each value of beta in the identified set, this set here gives me all the probability measure that are observationally equivalent. Okay, so in a sense, this vector here is the dimension reduction that transforms from the data information to information about the latest. To information about the latent distribution. And later on, we're going to have a sense of how information accumulates as panel length increases. Okay, so this is the simple t equals to 2 example. The key takeaway message is the logit parametric distributional form gives a polynomial structure that paves the way towards the connection of the truncated moment problem. Of the truncated moment problem. And this structure persists for any finite t and for models with or without covariance. Okay, so more generally, for any t and introduce x back to the model, what we're going to see is the likelihood can always be written as some matrix that does not depend on the latent effects, only depends on theta as well as the covariates that one is focusing on, as well as As well as the vector of what I call the generalized moment. And more specifically, this matrix here, or the length of this generalized moment, is going to be of length 2t. So now, what's the difference between t equals to 2 analysis and general t analysis? For t equals to 2, 2 to the power of t is going to be equals to 2 multiplied by t. But if t is greater than 2, But if t is greater than 2, this matrix here is going to be of dimension 2 to the power of t multiplied by 2t. So what does this give us is in addition to the moment inequalities coming from restrictions that the vector has to belong to the moment space, these all of a sudden give us a rich source of moment equalities that is available in the model. And in particular, the moment equality is going to be, first of all, the To be first of all, the number of them is going to pin down by the dimension of the Left Now space of this matrix. And in addition, we can find the analytical form of the moment equality condition by finding a basis of the Left Now space of this matrix G. Bearing in mind that once you have the model written down, this is of known form to us. So we could analyze this analytically and try to find all the moment equalities that is available to identify. Qualities that is available to identify the structural parameter. And here I'm just defining the left-now space of this matrix. So this vector V here is going to pin down what exactly is the form of the moment equalities that can be brought to use in a GMM framework. For instance, a few minutes. Okay. All right. So since I have a shortage of time, I think I would skip the identification of queue and give you. Identification of Q and give you some example on why inequalities might be useful. Okay, so here I'm considering a T equals to 3 model where the only covariates is time trend. This model is important because there are plenty of empirical examples where age is an important covariance and age can be viewed as a time trend. Okay, so if you look at this model, we can find two ways. This model, we can find two moment equality conditions for two parameters here: one parameter for the dynamics, and the other parameters tell me how important is the time trend. However, we can analytically show that the two moment conditions is going to always give me two solutions. I'm gonna have two roots for this vector of parameter here. However, using the moment inequality, which also our model Inequality, which also are model restrictions, we're going to demonstrate that these can rule out one of the four solutions and restore point identification. So, as a graphical illustration, I'm again focusing on the two-component distribution for alpha. Here on the left-hand side figure, here, I have two curves which collects the value of parameters that all satisfy the two-moment equality condition. If both of them have to satisfy. If both of them have to satisfy, it's phishing out the green dot and the red dot here as the two solutions coming from this two-moment equality condition. However, we're going to see that the red dot here violates one of the inequality conditions. So here the yellow area collects the parameters that perfuse one of the inequality and the green area of collections of values for the other inequality. They are intersected. Inequality, their intersection is going to nail down possible parameters that fulfill both inequality, right? So these clearly rule out the false root here and help us actually restore point identification for this model. Okay, and we then, there is estimation and which I will skip. In empirical application here, it's a data example which looks at children's respiratory conditions. Respiratory conditions over time, where age is an important factor, right? Because, as you would imagine, as children get older, their conditions usually get improved. So, we can see that if you use the usual GMM framework, incorporating only moment equality conditions, there are two roots that are indistinguishable. But one of the roots is very weird of having a negative coefficient in terms of the dynamics. So, this is weird, and we can easily show that these roots. Can easily show that these roots here is the false root because it violates moment inequalities. So then we did some comparisons with models that either introduce fixed effects through dummy variables, which clearly gives inconsistent estimates, as well as models which do not pay attention to the individual heterogeneity, which would overestimate the latent effects. But anyway, let's close here.