Yeah, for now, but for the future. Yeah, we have to do that 20 times this week. We'll forget, for sure. Okay, thanks, yes, for inviting me to this workshop. Yes, I'm Fred. I'm currently at the University of Chicago. So please visit your board. And if you're in the area, okay, and let's talk. I was told by Guy that I can have a very, very informal talk. Like, no math, no nothing, just second math. He said I could read the Unibomber Manifesto if I wanted. We're supposed to be out, but you're being recorded. Anyway, that's not what's going to happen. I'm just saying, if it's anyone's fault. So, today we're going to talk about some serious mathematics. We're going to talk about learning Gaussian-Gauffen models. And that's some progress and some frustration in this area. And this talk, okay, please let me know if I'm writing too small. And yeah, feel free to interrupt. And this talk is largely based off two joint works. So one joint work was with John Kilner, myself, Ankar Moitra and Raghu Mecca. Moitra and Raghu Mekka, and this was a couple years ago, so maybe 21. And another much more recent work, his fund work was also John Kilner, also me, also Ragu Mecca, but also a very promising young graduate student named Jubro Hatti, who's at MIT and this is at 2024. And finally, I need to mention that there is actually overlap between this work and some other recent work. So there's recent work. Recent work, so there's recent work by Marunais Kuhai, Ding, and Steven Tiegel, which is also 24. So I'll mention which part is overlapping with that. Yeah, so it's not so much just our brilliant idea, but also their brilliant ideas. Anyway, I mostly, I think, wanted to make sure the problem statements are clear, and then I'll tell you about some of the state-ups to art in this area. Data. And so, what we need to start with is what is a Gaussian graph involved. Okay, so I'm worried that my body will get in the way, so please let me know what you can't see. So, a GGM, this is our abbreviation, with precision matrix theta is just a Gaussian distribution. So, its p of x is So its p of x is proportional to x minus x theta minus. Okay, so this is just a Gaussian distribution. And for simplicity, we're going to have the mean as zero. Like you could have the mean not being zero, but then we'll have to deal with that everywhere, right? So let's just make our life simple. And we're going to, in general, assume that this thing is sparse. We say it's a, say it's, say it is k sparse, or K-sparse, or maybe I'll call this sparse, but that's okay. If every row of theta has, at most, t plus one minus one. Okay. And so And so, hopefully, from now, it's like kind of starting to make sense. What's learning in Gaussian graphical models is a class of distributions. I guess if we're being very precise, we should write that they're parametrized by this matrix data. But I might not write that all the time. And it's kind of a natural class of distributions. I'll explain in more detail why we're assuming sparse, as I was maybe here already too many people. And so we want to learn, right? And so that's kind of what we do. Right, and so that's kind of what we do: the computational complexity of specific numbers. So, that's what this talk will be about: it's like how well can we do this task? And it turns out it's actually kind of complicated. Now, before diving into the results, I want to talk about an important class of examples, which is useful for building intuition and which is better behaved than the general problem. Okay. So, this is like a good example to think about. So, this is like a good example to think about. And this is called the discrete Gaussian free field. And I'll define this on a general graph. So, Gaussian free field. And um, okay, so what is this going to do? So So, what is this going to do? So, what we're going to do is we have a graph D. So, this is a graph where it's max degree D. And then I'm going to define the following precision matrix. Okay, so the precision matrix is going to be the Laplacian of the graph. And what is the Laplacian of the graph? It's the degree sequence. The degree sequence. Okay, so it's the degree. So it's, let's say it's a graph on the vertex set N. And the Laplacian is just the normal graph Laplacian. So you look at the degrees and you subtract the adjacency matrix. Okay. Okay, and so I would say the discrete Gaussian free field on a graph is the Gaussian graphical model with this precision matrix. Now, probably some people are thinking that there's something wrong, because if you think about it, I think if you hit this matrix on both sides, which is all ones vector, then you get zero. And actually, you know, for this definition to make sense, we need that this should be a positive definite matrix. Because this is like a Gaussian distribution where the covariance sigma is theta inverse, actually. And so if we have a zero eigenvalue, that corresponds to an infinite variance direction, which doesn't necessarily make sense. So there are various ways to fix this. Like you could add a small copy of identity to the matrix, or you can Or you can pick a particular vertex and condition on that vertex is this is going to define one way to deal with this is I could have a sample of x from p theta and say I could condition x1 equals 0. And the reason is like if you think about, oh, why is there an issue here? It's just this is supposed to be kind of a translation invariance probability distribution. So if I like add a constant. So, if I add a constant to x, it shouldn't change the probability of it. But that creates a problem because if you make a Gaussian translation invariant, it doesn't necessarily make sense mathematically. So that's why this is the common way of dealing with this position. But it's just like a technical detail. Is the Laplacian your covariance matrix or the precision matrix? Yeah, so it's the precision matrix, right? So that's a very important distinction. So for example, So for example, if I have my graph as a path, then let's say this is G. Then what is the process X going to look like? So X would be basically a simple random log. And if I take the spacing between the vertices to zero in the correct sense, then zero in the correct sense, then this would converge to a Brownian motion. Okay, it's important to think that this is different from assuming the covariance is sparse, because if I look at a simple random walk and I look, this is time, this is time, so it's t equals 4, the value at t equals 4 is correlated with the value at t equals 5, with t equals 6, with t equals 7, t equals 8, t equals 9, right? So it's a covariance matrix for a simple random walk is a dense matrix. A simple random lock is a dense matrix, right? And this whole talk is about sparsity, so the thing that's a sparse is the inverse of the covariance matrix. So, Fred, just to make sure I answered correctly, if your graph had a spectral gap, then you could do some Taylor Space expansion of the inverse and get a sparse approximation. Is that equivalent to what you just said? Be free to ignore this. Okay, yes, you're saying if Okay, yes, you're saying if things are well conditioned, then you can approximate the inverse of theta. I think, yeah, that's kind of true. I don't know. Yeah, this may be used in some works, I would guess. It's not going to be useful for what we're talking about today. Because we'll talk about mostly be worried well, we'll be worried about the worst case. Worst average case. Yeah, the the worst average case, exactly. Because Exactly. Because, you know, yeah, we are very pessimistic. Okay. And let me just kind of remind you, yeah, what's the reason that this is like a natural thing to look at? Okay. Well, you can just think about like in this one-dimensional example, like why would you want to look at each minus x Laplacian x? Well, there are different ways to think about this. Well, there are different ways to think about this, but if you think about you have some kind of graph, right? What's like the natural Gaussian distribution to look on this graph? Well, the most obvious one is that you just have IID Gaussian noise at every spot. That's the most obvious Gaussian process on the graph, but that doesn't depend on the graph. And then what's kind of wrong about that is like, perhaps we think it's like some kind of field on the graph. So it's a value. The graph. So the value and neighboring guys should be kind of related. So we think we want a smooth function on a graph. And realistically, what are some cases where people try to use this kind of model? One is they want to do some kind of Bayesian statistics. And they're like, oh, I need a prior for a function on a graph. And then they're like, what? On the graph. And then they're like, what's a good prior? And they're like, oh, maybe this is a good prior. And so, you know, very informally, like Brownian motion, it's kind of like you look over all functions and to encourage them to be smooth, like you look at the LT norm and then you look at f. So like, it's just the same story. We brought in motion in one dimension. If brought in motion in one dimension, you were like, oh, can we come up with a distribution over functions which are kind of continuous and smooth in some sense? The natural way would be to look at all functions. All functions don't have any kind of geometric structure, so try to downweight the ones which have very big gradients, so they're not very smooth. And then this could be made formal, and then you get Brownian motion. If you look at this measure again, it's supposed to be translation invariant, because if you add a constant, the gradient doesn't change. Constant, the gradient doesn't change. So that's the exact same problem we have here. And then the way we fix it in brown motion is you condition that, generally speaking, you condition that FR0 equals 0. So this whole story is like the same story. Okay? But then we like graph theory, so we're not going to consider it just on one-dimensional graph or two-dimensional graph. We'll just consider arbitrary graph. Oh, yeah, just to say, well, here I wrote to grade and then, and then, you know. That. And then, you know, if you think about integration by parts, if you have the integral of the grad f grad f, then I guess under like some conditions on the boundary terms, I think when you integrate by parts, you can get the Laplacian. And then there's some concern about the sign convention for Laplacian, but we won't worry about it because we did define it correctly. Like if you negate it to Laplacian here to measure, it doesn't make any sense because the state will negate it. So that's one reason that people were interested in Gaussian free field. We were interested in Gaussian free field in various communities. They had a bunch of actually names for it. Like there's some very well-cited paper, which calls like Gaussian harmonic crystal or something, and I don't know why. But yeah, there is some relation between this harmonic functions. Okay. Okay, so now let's talk about the learning. Okay, so now let's talk about the learning task. And you could think, yeah, and maybe you don't like the general model, it's like too general, but in many cases, you could think, oh, there's just some graph. I observe a process, but I don't know what the graph is, and I want to get the graph. So it's just kind of a natural question, at least mathematically, to ask. Okay, so it's going to be a learning problem where we give in. We'll be given samples, i.e., and then I'll generally write m for the number of samples theta. And then my goal is to recover, well, either to recover theta or perhaps just to recover G. And after I'll just focus on the setting of trying to recover G. Focus on the setting of trying to recover G. Because, in principle, if you could figure out G, it's very easy to estimate zeta pretty much optimally once you know G. So, in some sense, this hard part is like, you know, selecting the graph that is supposed to correspond to the data. That's really the hard part. So, that's why we kind of might focus on this. And that's sometimes called structure recovery. This probably also sometimes called inverse covariance selection. I know. I know. For the reason that the precision matrix is the inverse of the covariance. Okay? Okay, I wrote some notes about cases. Like, what are these models good for besides, oh, you want to look at smooth functions on graph? I just mentioned a couple other places where, I mean, there are a lot of books in something about this, right? But a couple other cases where people were finding out. Where find Daussen graphical model useful? One would be something like a collaborative filtering. And so here what's going on is that let's say I'm Netflix, for example, and so I have n movies and I have m users. Well, it's not exactly true. Well, it's not exactly true that users are independent, but we could just kind of pretend that they are. And then we say, okay, we want to predict for a new user based on what movies they've liked, what other movies they've liked. And so you might imagine building a model where you have these movies, and this is like, you know, Batman, and this is like Harry Potter, and this is like Hunger Gates. And then you want to understand, oh, like, you know, if I know. Like, you know, if I know what people think about some of these movies, what are they going to think about other movies? And, you know, once you have the distribution, then in principle, you can calculate all that stuff. It's just calculating with Galileo's distribution. Okay? And yeah, it turned out that this actually works pretty well. So there are a bunch of papers from Netflix on this. So it's like I think it's essentially the same thing. There's they also call it embarrassingly shallow autoencoder. Shallow autoencoder, because you can kind of use this like a neural network, but there's no line comparisons. So, yeah, I'm not making it. That's actually, in that case, actually, I think I went to a poster about this, and the sparsity is like, there's actually some compromise. You want the model to be sparse because, for example, that makes the inference faster. Like, oh, given a new user, you want to recommend to them. If it's sparse, then it's. If it's sparse, then it's easier to calculate these things. But as you make it denser, it does become like a better model of reality. So there are some, in the real world, there are some compromises like this. Reality is not exactly as far as Calcutta model. But yeah, this is a case where you can use this model and it does work pretty well. Yeah, so that was a one-fixed example. Okay, so that was kind of Okay, so that was kind of quote-unquote motivation. So now we can kind of just talk about this problem. It's just, I guess, kind of like a fun math problem, and we don't need to worry about the real world anymore. Okay? Good. What time am I supposed to end the talk? Oh, really? Oh, I thought there would be like nine fifty, even the other ten minute break. Uh no, we have a coffee break out. Oh, okay. Wow. From the I guess you guys won't be mad if you end early. It never happens. Never been to Top Revolution. Yeah, you never need to a Top Friday. Even though I always say you might end for Friday. Only 10 times. Okay. So now we have this task, okay? And we want to figure out how to solve it. And basically there's kind of like two main approaches. Basically there's kind of like two main approaches, two main challenges to solving this task. So one is like, okay, it's like a statistical model. How do you learn a statistical model? Maybe if you know that you can try to maximize the likelihood, right? And in fact, for example, And in fact, for a Gaussian distribution, right, like, okay, I erased the definition of a Gaussian distribution, but Gaussian distribution is quite explicit, right? The theta is like proportional. It's not even just proportional. Density is like 1 over maybe 2 pi to the d I think it's his determinant of sigma. And then x. So, if you think about it, you could take the log of this, and when you look at the log likelihoods, it's actually, you know, you want to maximize the likelihood. It's a concave function, so you might try maximize over theta that are sparse along p theta xi is one theta. Okay, so this might be one natural approach. You're like, I want to learn this model, I should try to maximize the likelihood. This ends up to be a convex, concave function, like the kind that you like when you maximize, right? It's not a coincidence, like it's always true for exponential family. So you might try to solve this problem. You have this issue that this is sparse, but maybe we'll discuss more later. You know, there's standard ways to deal with this, like add a penalty. This might add penalty corresponding to the sum of the entries. Positive definite. So, this is actually a class of approaches. So, for example, this algorithm is called graphical associ and it's like a reasonable class of approaches, but it's not what we're going to analyze today. And one reason is just that, at least for mathematical analysis, like having this big Like having this big joint optimization with this positive definite matrix is not so fun to analyze. But there are papers which analyze. But then you have to make kind of strong assumptions. And so it's not going to be, this so far has not been a very fruitful route to getting like good results for the worst case. Okay, now, like I said, we're going to be worried about the worst case. So the worst case graph, worst case thing as well. So this is one approach. Then what could you do if maximum likelihood doesn't work? Well, there's kind of a second option. But there's kind of a second option. And the second option is, I think, a very smart idea. It's called a pseudo-likelihood. And like many good ideas, it was invented a long time ago. This one was proposed by this guy named Julian Lusag in the 70s. And he was one of the big guys pushing this corrupted models. So a lot of data. Problems. So a lot of data. I think if you look at his paper, he had some cool examples, maybe involved farming or something, like some real-world thing. And then he was like, oh, let's try pseudo-likelihood. And then he wrote a paper about pseudo-likelihood and Gaussians, which is very similar to what we're studying. So yeah, he was doing this stuff. And this is what we're going to focus on today, because this ended up to be nicer to analyze and to think about how to modify. It just seems like a more fruitful approach so far. Okay. Okay, and so I need to tell you what is a super like with it. Okay. Well what about the dumbhead approach where you just test for conditional independence? That works for all graphical models. Yeah, but like how do you do the test for conditional independence? That's kind of the issue here. Like still like in some sense it's kind of like would be one way to do the test. Okay, maybe let's say what's it because continuity or what we Yeah, it's like yeah, it's like how would you construct the like okay, we're going a bit ahead. But like what if edges in the graphical model mean? It's actually the meaning of this is that let's call this i and let's call this j. The meaning of this is that covariance xi, xj, given all the others, is bigger than zero. So in principle, to check for edges like you just To check for edges, like you just need to compute this thing, but that thing is not so easy to compute. I mean, if you have an explicit description of the model, it's easy. What you need to construct an estimator, because you don't have an explicit description of the model, and then when you think about this, this will involve things like estimating xi given, like if I try to build an estimator for this, I would need to be able to estimate quantities like this. And this is the kind of quantity that's going to show up in COI. So that's why I'm saying, like, if Thing like if your idea is like, oh, we should test for initial correlations, this is called a partial correlation, then you're basically going to be led to pseudo-light marks. So let's say what pseudo lightning is. And so, like, with one word or one sentence, sort of likelihood, let me write it up here. So, the very good idea. And the idea is like the way you'll fit a model of P of X, P of X is by matching. The conditional laws p of xi equals thought given x given all the other coordinates, we're all i. I guess one thing that I didn't say explicitly, but like, I mean, there's many ways you can solve this problem if you have as much data as you want. Problem if you have as much data as you want. Like, we're really concerned with, like, oh, let's do this with as little data as possible. Okay, so this is what pseudo-likelihood means. Instead of trying to maximize the joint likelihood, which in many cases might be kind of difficult, we'll just try to look at these levelized likelihoods and try to optimize as a strength. And what's the reason for this is because, you know, under kind of weak conditions, if you match all of the conditional laws, Match all of the conditional laws exactly, then you sort of match the overall distribution. Like, you can think a bit more about what does this mean. Like, let me just make a comment. If you have a distribution where the so-called Gibbs sampler or Glabber dynamics, which is based on resampling from this law, converges to the stationary distribution, it's like ergodic, then that would be exactly where this is true. And that's true for many, many distributions. So, in principle, if you can solve this problem, then If you can solve this problem, then you do, there is a real distribution. But that's just a common, it's not an important thing. And so, like, now let's do the first calculation, let's calculate what the conditional law is. What's the conditional law in the GGF? You know, and like, I guess for some people, this Dallas and graphical model is like paradise because it's all just a linearity model, right? Like, basically, we're just trying to invert a matrix. We're just trying to invert a matrix. You have this covariance matrix, sigma, and you kind of observe it from data, and then you're like, how do I invert this? So it's just going to be a bunch of different matrix. Okay, so let's do this. So what's the probability that, or the density of xi, given all the other coordinates of xi? Well, we can just go from the definition and keep all the terms which involve xi, right? So we're going to have the exponential of the negative. the negative sum over j not equal to i x i theta i j x j, right? And so this is kind of like the relevant part of the off-diagonal terms of the matrix. And then we also get this non-diagonal term. Theta ii xi squared over 2. Okay, right? And then you look at this like a Gaussian distribution, so Like a Gaussian distribution, so let's make it look more like a Gaussian distribution. So, what we can do is complete the square. So, let's complete the square, and you get this is x of minus sigma ii, xi plus the sum, take not equal to i, theta ij over theta ii Square root. Okay? So I just completed a square. If you expand this, you get this, plus another term which doesn't. Let's ask a high-level question. So here isn't the goal to try to recover slash slash keys. I guess how do we know the user's effect side? Yeah, we don't. That's one of the hard parts. So we don't know them. One of the hard parts, so we don't know them exactly exactly. And so that's why this problem is kind of hard in these issues, right? Because there's like a combinatorial search over all possible graphs, which have sparsity. And then we're trying to figure out how to do this in an efficient. Yeah, that's the fundamental reason this problem is hard, because the log likelihood itself is concave, everything is good. So it's only trying to constrain theta from use bars, which is all about problems. Yeah, and if you know the graph, it's very easy to think about it. If I have as much compute time as I want, is it known what the right number of samples is for the problem? Yeah, actually, it's interesting. Okay. I'm going to talk about this problem for structure recovery. So it's like you're going to make some assumptions that the edges are non-degenerate, which is function links to. And then, okay, under that assumption, you would think that we knew about the impact information theory to sample complexity, but worrisomely we don't. So there's a gap between upper bound and the long-pound. Gap between aquapel and moral bounds. That's what I'm saying. So that doesn't involve any computation. That's just this random question, which surprisingly, nobody figured out the answer to. Yeah, it's not trivial, actually. The analogous thing Freisen model is. If that takes you too abstract, yeah, it's like people have all sorts of different formulations on the problem learning e-sing model, I-Cing model. Model. But if you state it in terms of the same conditions, like for graphical models, this is actually kind of like, in some sense, very natural thing to look at. And usually in VC models, people are not looking at this. So if you phrase the question in the same way for VC models, it's not going to happen that we know the exact answer. Okay, so what was the takeaway from this is that this is just a Gaussian. This is just a Gaussian distribution, and this is just a normal distribution, whereas the mean is the sum of the j not i. This is negative theta ij over theta ii xg, and this is 1 over theta ii. Okay, so that's just the Okay, so that's just a summary is that this is the conditional law. And so what you can do is you can try to match this conditional law with what you see in the data. Yeah, so I mean, okay, there's argument, I know like there's some choice. Like, for a pseudo-lihood, you're trying to match this conditional law. Like the original pseudo-likelihood, you would try to match all these joint laws together at once, but in many cases, Ezra at once, but then in many cases, you do a variant where you match them one at a time and then figure out how to stitch the laws. So that's what I'll talk about, but it's like, you know, it's conceptually about that. So basically, if we're trying to match the law at no high, like what are we doing is we want to maximize the likelihood of the theta to this model. Of the data under this model. So it's just going to be something about maximizing the log likelihood of a Gaussian distribution with a rise of business. Maybe let's just write it explicitly. So like it's just the same basic expression over here. You want to solve some kind of optimization where you minimize over theta. And then you have this term from the log likelihood. And then you have a bunch of samples. Okay, maybe you should call this a different letter. So it's called S i plus sigma ij over sigma ii xj squared. And then there's like gonna be some other term for normalizing constant. This is not gonna end up to matter for us. This is not going to end up to matter for us, but I think it will be this. Okay, so this and I get this, I was like looking at the problem of maximizing the log of P beta Xi gross. I was like thumbing over to samples and then I was trying to maximize The conditional log likelihood. I mean, it isn't like a bunch of math, but like, you know, it's, and there's a log. Okay, and then I'm not sure if this is a plus or minus, but you can check yourself. Okay, so that's where I love math, but like, all it's really saying is like, I mean, there's this fitting this beta ii term, but if you kind of look at the subjective, fitting theta ii doesn't really interact in a very interesting way. Interact in a very interesting way with fitting the theta ij. So, like, basically, the main difficult part of this problem is like, I want this to be sparse, that the rows are A plus one sparse, and actually I'm only looking at i of theta. So the main difficult problem is solving the optimization. I mean, I can ignore the term involving theta i i. The difficult part is really solving this optimization or this squared loss where this term is where I have a squared. We're going to have a sparsity constraint on theta i. So let's just say that maybe written in a different way, there's an expectation of xi and all the other guys is a sparse and then a d sparse linear function. Yes. So uh the model comes with the promise that that theta is sparse itself. That theta is sparse itself. So if you don't impose it in the minimization problem, that makes it quadratic minimization easy. Why not drop the sparsity? Yeah, but then thinking about, yeah, I guess how many samples are you going to need? You're going to need like the dimension number of samples, and we're trying to get less than that. But is that a fact that without overfit somehow? Well, you overfit if you don't have enough data. If you have enough data, you won't overfit. That's all okay. Coverpint yes. That's all okay. Even with a promise. Yeah, you can analyze directly. I mean, basically, if you look at the maximum likelihood estimator, like when it exists, for it to exist, you kind of need to have dimension number of samples. It's just like ordinary squares doesn't exist until you hit the dimension number of samples. And there's a bunch of research. Like, if you add additional constraints on theta, then there are cases where it exists before that. Yeah, but then if we go into that. Um yeah, but then you know if we go into that that's quite a whole different topic. Target for a noise list. So you just solve each of these problems per I and you stitch your estimates together? Yeah, exactly. And that would be like a version of pseudo-likelinhood. And then the only question is, how do we solve this optimization? So, you know, maybe like let's change the notation for a bit. Like basically, we want to optimize over D sparse coefficient vectors, and it's like you want to optimize just this normal regression squared loss. Right, so if I, here I have all the notation from the Gaussian Herbal model, but this is just a problem that I basically want to solve, and then the next thing to worry about. And then the next thing to worry about is like, well, what's the best way to solve this problem? So this is what we call a sparse linear regression problem. Yeah, and then imagine you solve this for all of the i's, then you could do something like pick the average of the, like you're kind of gonna, each w is going to give you an estimate for theta ij, but then the theta is symmetric. So there's like some arbitrary choices about how to stitch together, but these are not, as for statical analysis, like they're not that. And yeah, the next thing to say is: if you can actually solve this optimization, you can imagine this is non-convex, but if you solve it exactly, then this is actually a very good estimator. Okay, so there's a lot of talking, and then finally, yeah, there's like one more definition. So the definition I want to make, which is kind of important, has to do with, I wanted to focus on structure recovery. I wanted to focus on structure recovery, so learning the graph. So I need to make some assumptions that the edges are not too. Otherwise, you cannot see them from a reasonable number of size. Okay, so here's the definition. We use some previous work, and it's like kind of the right definition. We'll say that the model is haploid non-degenerate. It's theta ij divided by squared. by squared theta ii theta jj. I think this ends up to be equivalent to the thing I wrote before. So this is some fact about two by two matrix inversion. If this is at least kappa, it should be greater than zero. Okay, another thing to note from this calculation, kind of trivial but important thing, we see that the conditional law of node i Law of node i I conditioned on everything, but only depends on its neighbors. So that's what we call the Markov law, right? And so that's to say that these coefficients, these partial correlations, they're only going to be non-zero for say neighbors of J and then for I neighbor of J and G. I'm going to assume that's a conditional, this is a correlation. So correlation means covariance normalized by the standard D. Covariance normalized by the standard deviations of the topic. So the covariance divided by the square root of the variance, square root of the variance. Just asking, so in the Gaussian free field case, are you allowing the graph to be weighted? Yeah, it turns out that's not going to change the analysis. And you're allowing the weights to be negative as well? No, no, for Gaussian-free fields, they have to be non-negative. But for the general Gaussian graph model, you could. But for the general Gaussian graph model, you could put negative. Okay, so let's just say that this being positive, yeah, this is just a definition of having an edge. So it makes sense to assume that this is lower bounded by a number which is bigger than zero. Okay? Okay, so now that we have all the definitions, we can talk about what are the results and the open questions, and then, yes, given the time, yeah, what. Yeah, we'll talk maybe just a tiny bit about some of the results. But I think sort of the most important thing is the problem statement. Because, yeah, as you see, it's like not entirely true. Okay, so yeah, now we can ask: how many samples? But you learn a kappa non-degenerate D sparse GGR. And like I said, the game here is like we are very pessimistic people. So, you know, or it's min-max theory, right? So we're going to look at the worst case for the graphical model and try to understand what's the best estimator in the worst case. A different game would be pick your favorite graphical model and then Be pick your favorite graphical model and then see what happens in that case. And that's just going to end up to be a difficult thing. So let's talk about the worst case. And then, this is called computational complexity. So we have two different kinds of thresholds, right? There's algorithmic and information theoretic. And when I write algorithmic, you know, going to be forever lower bound, just assuming some kinds of contentions, right? Well, let's talk about the case on the Gaussian free field. What do we know how to do algorithmically? Algorithmically, we know we can achieve recovery with d over kappa squared in log n samples. Examples. And yeah, we love computations to feel gap, so we don't know if this is real or not. But information theoretically, you can get away with one over kappa squared log n. So there is a gap, which is interesting. And then you'll, yeah, you can go home and try to prove this gap. It's supposed to be there, but so far, no one proved it. It's supposed to be there, but so far no one worked. Okay, so second game is to try to improve this to match that, and I don't know which one's correct. Okay, now that's just specifically for Gaussian free fields. Or more generally, if off-diagonal entries of theta are negative, that ends up to be basically the same model. I'll go on to explain that in detail. So, this is some linear algebra effects. Now, for general GGM, what do we have? Well, yeah, it's like we don't really know, but just like David was saying, like. But just like David was saying, if you get to the dimension number of samples, then you can do maximum likelihood estimation. So I think you can get away with something like order n plus 1 over kappa squared log n samples. Maybe this is not exactly correct, but I think it's basically correct. That there's an upper bound. But then you can compare information theoretically. You can get a rate of d over kappa squared. And I should say who someone's whose results are due to. Say who some of these results are due to. So, this one is due to some people at Los Alamos, Frey, Misra, and Paul. And there's, yeah, there's a bunch of, there's like some previous work designed by there's a lot of other works that I'm not mentioning, but these guys prove these. And then, yeah, it's kind of weird because what do we know is a lower bound? We just know something like one of our capital. We just know something like one over kappa squared plus. So this is a purely statistical question, but somehow maybe it just avoided people's attention. It doesn't seem entirely trivial, actually. We don't know what the problem is. Okay? And yeah, the thing I wanted to briefly mention today is that actually now we have some computational lower bounds, so you need at least d squared log n under some kind of a reasonable assumption. And so you can compare. And so you can compare, and this is when kappa is close to 1. So when kappa is close to 1, you should only need, definitely you should only need d log n samples. It's possible you're going to need less. No, you shouldn't need d log n samples, but you actually need d squared. Okay, so this is like new results. And like I said, this you obtained, and also independently it's obtained by Ronesh, DFUN, and Bing. Okay. And And this is just for GDF. This is not for Gaussian free fields. Yeah, yeah, yeah. So Gaussian free field, there's not such a free Gauss field. Gaussian free field, there's only this question of whether D is required. But for the general one, there's how much. This question of what the exponent of each. Yeah, that's right. Yeah, exactly. Actually, okay, this one is sharp. So we know for Gaussian free field exactly how many samples, but for the general Gaussian-Gaffici model, somehow it has evaded our model. There's no dependence on the channel. D. Well, because you can prove in this model that D and kappa are literacy. It's actually not. Do an exercise. Okay. Wait, sorry. So, Fred, you're saying that the information you write threshold occurs at 1 over kappa squared is not n. Yeah. And then it's not clear if there's an algorithmic lower back. Like, if there's no alpha. Yeah, four or four gauss and three fields, we don't know if it's integral. Okay. Just try our best and see what we've got. I mean, this is hiding some maybe log factor. I mean, this is hiding some maybe log factors. Yeah, basically, this is what we got, and then we try it pretty hard. We're going to get better. But then maybe we just need to try harder. So the t-square robot is conditional, or is it a bit? Like I said, all this stuff is conditioned. So this is based off. There's like a low degree of hardness. And not rock solid. Okay. I guess there's a lot of other things. So you can assume, in most of the literature, you try to. In most of the literature, you try to make some further assumptions of theta. So you can assume theta is well conditioned, for example. Then we know how to solve sparse linear regressions. You're going to get much better results on this. And so there's a lot of work on that, so I don't have time really to talk about all the papers that are studying that, but this is like, you know, this is what I'm going to say. If you are optimistic, you're in a very good setting, these things are not so scary. And if you're very pessimistic, then you have bankales. Why do you think that people? Why do you think that these well-conditioned assumptions are more optimistic than the capital non-degeneracy? Yeah, the well-conditioned will apply the cap and non-degeneracy, but just capital non-degeneracy doesn't apply condition. I mean, just look at, yeah, simple random walk is an example which is kappa non-degenerate for a very large kappa, but it's not well conditioned at all. Like you look at the covariance matrix, and you know, the variance of the last time of the simple random walk would be like n. The simple random walk would be like n, and the variance of the first time what. So you see the condition number and certain data is automatically. Sorry, I think I'm still stuck on the same free field. You're imagining weighted graphs. Is that why you're keeping track of T and Kappa separately? Because in the unweighted case. Yeah, yeah, that's right. I'm thinking of weighted graphs, so that's why I'm keeping track of them separately. That's right. But if you only want to do it in terms of kappa, this would be like one over kappa to the third. Then you don't have to have to keep kappa separately. I mean, they're also I mean, they're also or maybe, yeah, I think. Okay. And then there, yeah, there's another setting called block stummable, which is more general than Gaussian free field, but not as general as this. And we have also a lot of questions there. I thought it was that. So it's kind of look kind of similar to Gaussian-free field, but the gaps are different. So we have kind of algorithms and some lower bounds. So let me just Lower bounds. So let me just explain quickly. Okay, well, what's the lower bound, and what's it got to do with everything else that we study? So it seems to be quite related. And that's also the research stuff, so maybe it's more exciting. Okay. Lower bound. Okay, yeah, we want to prove a lower bound. Okay, yeah, we want to prove a lower bound and then like, oh we have to. So triangle line, this is, at least for me personally, what the history is, is we're like, oh shit, there's a, I shouldn't curse. There's a gap. I don't know if that's bad that occurs, but there's a gap between these two. And like, yeah, what I really wanted to do is like prove that the gap is supposed to be there. Okay. And So we were like, well, what sounds kind of similar to this problem? It sounds kind of like a sparse PCA, right? So we're like, okay, maybe we can do sparse PCA, because we know that one is hard. And in fact, sparse PCA fits my definition of the Gaussian field. So at least with the weights. Okay, so what is this problem? Is I get samples from normal 0i plus beta indicator s, indicator s. S, indicator S transpose over size of S. Let's say it has D. Okay, and then I get ID samples, X1, XM, and this. And what you want to do is you want to recover to set S, right? So that's spores PCA. And you can check that this is basically a weighted Gaussian free field where the graph is a Gaussian free field where it's a graph is a clique. You have this set S, you have a clique here with all equal edge weights, and then you have a bunch of other variables which are just not correlated with anything. So that's why Sparse PCA is isographical model. You can calculate that by inverting, because it's a rank one perturbation of identities. You just use rank one thing. Okay? And so, like, yeah, we tried to do this, and it seemed like a good idea. Okay, so then. Yeah. Okay, so then maybe, okay, just I'm not going to do out the kind of lowering algebra, but like if you use the rank one, update formulas, and this is what you would get for theta. Okay, and yeah, the important thing about sparse PCA is it has a k to k squared gap for constant theta. Under planet leak assumption. So, what you say is that you can solve this. Basically, you need d-log n samples to solve this problem information theoretically, but you actually take a d squared log n to solve it with any efficient algorithms, as far as we know. And so, you can try to go from the hardest as far as PCA to try to figure out this gap. And so, then what you do is compute kappa, like you know, it's not. Like, you know, it's not hard to do. And you seem that, yeah, you're okay, something like halfway is around beta over k, beta over d. So then there's kind of like an odd lower bound, which is going to be, well, we need d squared sample to start PCA. But what does it translate into in terms of capital? But what does it translate into in terms of kappa? Then it's just omega 1 over kappa squared. Okay, so then it actually didn't work because this is a statistical threshold. Okay, so the algorithmic threshold for sparse PCA translates is the statistical threshold for this problem. And now you can think for a moment, like, why did this happen? Like, this is really bad. Like, that I wanted to get a lower bound, but it didn't work. But then you think about what is the moral of sparse PCA? Like, sparse PCA, if you think about the estimators for sparse PCA, there's like diagonal thresholding. BC, there's like diagonal thresholding. And that's kind of like trying to look at the empirical covariance and only use the fact that it's like a sparse perturbation, right? That's what a diagonal thresholding is doing. And there's also just PCA without any sparsity. And like that estimator is saying like, oh, okay, well, this is a rank one perturbation. Okay, so you can have estimators which look over sparsity and take advantage of that, or which look over low-rank structure and take advantage of that. And then the lower bound for sparse PCA is kind of something you can optimize simultaneously. PCA is kind of saying you can optimize simultaneously with low rate and sparse. And so if you think about GGM, there's no notion of rank. So there's only sparse. So it totally makes sense that this reduction doesn't work. Because this story is about low rank. Gazing up the model doesn't have any straight level. But then here's the stupid thing that actually works. And then, of course, once you see this, you're like, oh, we should have noticed it earlier. But what you can do is like what's bad here is that kappa ended up to be pretty small. But what can I do is imagine I take beta to minus one. So this is called a negative spike as far as PCA. And actually, the negative SPCA problem, this is a beta lesson zero. This had already got some attention because it was studied by Guy and Matt. I don't know what year it is. I don't know what year it is. That's a study by Bandera, Ding, Uniski, and Wine at some point. So yeah, actually, it turned out that taking beta negative is a good idea. But actually, okay, then again, if you just think, oh, let's take beta negative, it seems like you didn't gain any advantage because actually when you analyze this problem, almost everything is symmetrical in beta. Okay, so low degree calculation towards this problem, basically it's symmetrical in beta. Basically, it's metrical in beta. So it doesn't make really much sense that, like, oh, taking beta and negative is going to help us. But actually, it fixes this problem because as beta goes to negative one, then this number goes to zero. So I invert it, so it goes to infinity. So actually, as beta goes to negative one, then kappa goes to one. And so that was the big problem here, is that kappa is small. If I take beta very, very close to negative one, kappa is big. Then if you think about it, like there's a dgd squared gap here, and I exactly. A DGD squared gap here, and I exactly get the DGD squared gap that I claimed here. On the other hand, we don't notice this type, right? There's like there's no upper bound that looks like that. And also, it doesn't fix the range problem really. It's still range on perturbation. Yeah, yeah, yeah, right. So then that's a very interesting thing. So maybe this problem was near critical. Maybe a sparse PCA. Like, we didn't fix the conceptual issue that, oh, sparse PCA is about a low-rank structure. But it just turns out when beta is very close to minus one, then actually only optimizing for sparse structure. Then actually, only optimizing the verse bar structure actually does work information theory. So that's kind of a strange thing that this problem starts to behave differently when beta gets close to one. And that is not so courage intuitive to me. And the math is just kind of trivial, right? So that's just how it is. When beta gets close to minus one, the behavior of this problem does actually change. I don't know what to make of that, but that's just how it is. So you actually don't sort of care about the lowering structure anymore. So I don't know what the deeper significance of this is, but that's. What the deeper significance of this is, but that's why we didn't expect this to work, but it does work, and it's actually pretty easy to make this argument. Then you get a low-degree, you can calculate it, you get a low-degree hardness for this problem, you can also analyze the natural STP relaxation, and it doesn't work. So it seemed like this problem is still hard, even if you take beta to minus one. If beta equals minus one, this problem is actually not hard because exercise, because there's going to be some special structure. But if you take beta to negative one not too quickly, it seems like. Too quickly, it seems like the problem is hard. And so that gives us this gap. It's sad because we're hoping to solve this gap, but we didn't. So we got something here, it doesn't seem to match. It doesn't match the upper bound. There are special cases of sparse linear regression, and you can actually test between an empty Gaussian graphical model and a non-empty one with d-squared log n samples. So there are actually upper bounds. You can see which matches. Your lower bound would be like for this testing problem. Would be like for this testing problem. Exactly. And the deeper reasoning is that this is the correct threshold for the testing problem, empty, because it's not empty. And then this perhaps bigger gap has to do with estimation. Estimation gap is harder. That's why we don't exactly know what's going on. You're not sure. I mean, there could be a hard testing problem. Yeah, yeah, like a different testing problem than we are. Just that testing problem. Which we think is natural. That's the kind of question. Okay. I think that was. Let's do one question while everybody's still here. Ready? Yeah. Sorry if you said this already. For the gasoline-free fuel case, does the IT threshold have to be achieved by the inefficient just constraining the MLE? Or you don't know if that works? That almost firmly works. Yep, almost thermally works, but the analysis is for this pseudo-likelihood approach. That one works. We have a coffee break. Brief announcement. One, there have been some small modifications to the schedule. I'll write them on the chalkboard during the coffee break so that everybody knows. That's just for today, just because people's flights and stuff have freed out switched moving talks around. Thing two, there was this business about lightning talks for this evening. I sent out a sign-up sheet and then I didn't. I sent out a sign-up sheet and then I didn't allow editor privileges for anybody. So I basically said, please don't give a lightning talk, but actually do want you to give a lightning talk, so please sign up. I fixed the editor privileges thing. Enjoy your coffee. Yeah, I thought people might realize that I was gonna play the other one. So this is all the questions already. Oh, it's not so bad for you, I guess. You've probably got a copyright property through. Oh, yeah, yeah, no, that that was that was that was the correct point. It is like one stop on the way back because like the plate that like the Black Friday is is to uh Is to have they would have had since leave early Friday morning. So, anyways, okay, I gotta write this up before I forget what this is. See you in a second. Let's see. Is it a bad new hairstyle? I'm boycotting, uh, boycotting haircuts, yeah. I decided I don't like it. Judge forever. Oh, why say? No, no, no, no, no, no, no, no, no, no, it's not a cheap, it's not a cheapest chip. I don't know about that. I don't think all these algorithms, all their parts are like, yeah. This is the new one? You think I'm writing the old schedule? Yeah. Yeah, let me just write it now. Yeah, but we just written out a big schedule. What is this? So David is not making it. No, he's making it in the afternoon. That's all.                    