For inviting me, for having me here. So, today I want to talk about uncertainty-aware calibration of FI models, joint work with Evan Grubman and Torson Schmidt, both from the University of Freiburg. And yeah, actually, what we want to do is we want to talk about calibration of these types of models of FIM processes. So, we were looking into the specific class of processes, but actually, the Of processes, but actually the approach is not restricted to this class of models. It's only because of tractability. I mean, F1 processes are usually parameterized by some set of parameters theta and they turn out to be particularly tractable. But actually, the approach is also applicable to any other type of model. So if we talk about parameters of an affine model, Affine model, then it's the question is how to choose this specific set of parameters, right? So we can estimate them statistically, but if we do this, usually this comes with uncertainty. In statistics, we talk about confidence intervals. So we rather want to talk about a set of possible parameters. And also, if we do the calibration every day, the results will be different. So if we want to consider the process. If we want to consider the process over a time period, it's more reasonable to talk about a set of parameters leading to the notion of non-linear processes. And then if we have a set of parameters instead of a single parameter, instead of fixed parameters, the question is how to calibrate them, how to determine this set of parameters. This is what we want to discuss. And then in the end, we will And then in the end, we will talk about an application to a specific example, which is the US yield curve. And then, in the end, we will see: okay, how we can apply after having applied, after having calibrated the parameters, how we can apply this to a specific hatching example. Okay, there's a vast literature on exactly this type of or this branch of research, particularly about parameter uncertainty. About parameter uncertainty, robust hatching, robust option pricing, and deep learning approaches. So, I don't want to go to the details here, but just to show that this is very current research that is done here. Okay, so affine processes usually are characterized by the characteristic function, which is often exponential affine structure and affine processes. Affine structure and affine processes, if we talk about a one-dimensional case without chumps, usually are simply like can be understood in terms of this STE, where the trif component has an affine structure as well as the volatility component. I mean, they're very prominent examples. So, if we consider log prices and the Flexolts model, we are in an affine world. Or, I mean, in terms of interest rates, the VasterCzek model, Koxinus or Ross models are. Model Coxinus or Ross models are also other examples of FM processes. And what we want to determine here are the parameters. So we fix a parameter set theta consisting of the intervals in which the corresponding parameters of the affine process fly in. And then we to define a non-linear process. So this means a process that can attain parameters within this. Parameters within this set of parameters, we simply require that the corresponding differential stream multi-generation characteristics. This means the drift and the volatility component are contained in the corresponding affine interval spanned by the corresponding parameters. This means we require here, for example, for the drift component that B0 and B1 are contained in the corresponding intervals defined by the set theta. Okay, but this is. Okay, but this is not the correct notion of an affine process if we talk about interest-rate calibration. And if we look, for example, at sulfur as a spot rate, then we see, okay, if there's a meeting by Fed and they decide on changes in the interest rate, then there will be a jump in the spot rate, right? So we need to include jumps here. And because the Fed meetings are known in advance, the jumps will not happen at random times, but at Will not happen at random times but at deterministic times. So, what we want to include is addition in addition to the continuous structure of the affine process, we want to additionally include some kind of chump component where the jumps happen at deterministic times. But the chump distribution itself is unknown, right? The times are fixed, but the jump distribution is unknown. And also, we want to include uncertainty with respect to the Uncertainty with respect to the chunk distribution here. Okay. But the main question is: how can we calibrate these parameters? Okay, so we consider, again, we consider here interest rates as our working example. And what our idea is of the approach we propose is to minimize the distance between observed bond prices on the US bond market. Bond market and between model prices. So, this means we have our underlying FI model that produces bond prices. And I mean, in FI model, these model prices, they are of a particularly nice structure. So usually they can be written in this exponential affine form for some functions, phi and psi, that can be solved by usually ODE methods. So if we talk about linear affine models. Linear FM models. And non-linear world is a bit more complicated, but still doable. And we can compute these model prices here. And now what we want to do is we want to find the set theta, so this non-linear set of parameters, where the idea is actually that the model, the set theta should be rich enough in the sense that the observed In the sense that the observed bond prices in the US market should lie within the maximum and minimum produced prices by our model. So this means if we minimize with respect to bond prices among all parameters from the set theta, this should lead to prices that are below the observed market price in the same way for maximizing, right? Because the model should be, the parameter should be. The model should be the parameters should be rich enough in the sense that the parameters in the set should at least contain, be able to replicate the observed multiple versus. But we don't want to consider a set theta which is absurdly large. Therefore, we want to minimize the corresponding set theta in the sense that the price bonds are, in a certain sense, tight. Okay, this leads to the following minimization function. So the first So the first, so we sum over all maturities that we observe and overall bond prices that we include. These are the two sums in the beginning. And then we want to minimize with respect to the distance between market prices and observed prices. These are the two summons you see here. And then we penalize simply if we go beyond the upper bound and or below the lower bound. So we don't want. So, we want to enforce the constraint that I mentioned on the previous slide that the market prices are within the range of produced model prices. Okay, and I mean, even in the non-linear case that we talk about here, we have the possibility to compute the model prices. At the model prices efficiently in specific models, for example, in the Coxingers or Ross model, that we want to consider the solutions are simply Riccati equations. So that's very easy to compute. And then we minimize with respect to the corresponding set theta. That's what we want to do. Again, we go back to the case of sulfur. And we see in this time period that we consider in 22, if we look at the chump sizes, they are either 50 basis points or Are either 50 basis points or 75 basis points. So, this means the jump size distribution that we want to consider is of a very easy type. This means either we don't have a jump, this can happen, we know in advance there will be a meeting of the Fed, but maybe they will decide not to change the interest rate, right? There might be a jump of 50 basis points with probability P0 or 75 with probability P1. We don't know the probabilities in advance. Know the probabilities in advance, so we only require this sum up to one, something like that. Okay, and then we minimize the corresponding distance function that I explained before. Then we can show in the limit if we let the penalization parameter go to infinity, it produces exactly the result we want to have. This means we minimize the corresponding price bounds. They are as tight as possible and the corresponding prices. Prices, the observed market prices lie within the range of model prices, so to say. And this is what we get then for after calibration. So we calibrate the whole yield curve. And this is the example of a two-year bond calibrated considered time period 22. And we see the black line are the market prices. And then we see three further lines. So the one line is simply calibrating all. One line is simply calibrating only with the current market prices. And then we also consider some kind of look-back period where we also consider recent market prices to calibrate the model, which makes the whole calibration a bit smoother, of course. The same for the five-year bonds and the figures look similar for other maturities. Okay, and then if we look at the parameters calibrated, the vertical dashed line. Dashlines are the FAP meeting dates. And we see that before a FAP meeting date, the probability for a jump P1, P2, this should be P0 and P1, increases. This means only from observing the bond prices and calibrating them, we see already that the bond prices include. Prices include the probability for a jump. So it's already priced in the bond that there might be a jump in the future. So we see that the probability happens already a long time before the Fed meeting already. But on the other hand, the prices are not able to tell you what kind of jump will happen, whether it's 50 basis points or 75 basis points. So this is very hard to distinguish here. Okay, this is standard calibration. This is then the calibration. So, if you look at the bond prices, we see the calibrated bond prices lie somewhere between the upper and lower bound of what we get, even though the calibrated price points are quite large in the end. But what we get is a set of parameters that we rather not use for pricing, but for other purposes, such as hatching. So, this means if we want to hatch, for example, a caplet, Hatch, for example, a caplet written on the sulfur rate that we consider here. Then we can use a non-linear model, run the usual deep hatching strategy, get some kind of robust deep patching. We compare it with a non-robust procedure where we don't have the parameter interval. And then, as it turns out, if we compare the relative hedging error, it's much smaller for the robust procedure simply because the parameters change a lot over time. The parameters change a lot over the time period. So, these are the values in the table. Of course, it's too much for this short time, but if you take your time to look at them, you see that, for example, the mean row, that's the mean hatching error is much smaller, smaller variance. The reason for that is simply it's more resilient against chumps and parameter changes, which happens a lot in this period. In this period. On the other hand, in contrast to what we thought in the beginning, the length of the look back period, even though it's smoothing the parameters and everything, it doesn't have a strong influence for the robust results. Okay, so thank you very much for listening. So the project is still in a very early stage. So if you have any suggestions, you're very welcome to give them to me. Thank you very much. Different people. Thank you very much. Many thanks, Julian, for this nice presentation. So, we still have some time for questions.