Frederick, so I got it. Thank you to the organizers for giving me the chance to speak here. And hello, good morning to OHAKA. It's lovely to be here, to be there at least virtually. It would be even more lovely to be there in person. So maybe next time. So what I want to So, what I want to speak about is joint work with Skennady Avalkov. And so, I try to give an understandable and non-technical talk. So, I hope we'll succeed more or less. The background is semi-definite programming optimization SDP. And since this is not the main focus of this concept, At the main focus of this conference, let me give a brief warm-up on SDP. The idea is to, or the goal is to optimize linear functions over convex sets in, say, R to the N. So I have a convex set and I want to have a linear function. I want to optimize it. That means to find the minimum or maximum and also the one optimizer. So one point where the So, one point where the optimum is attained, for example, when the set is compact. And this is done by solving linear matrix inequalities, LMIs. So it is what you see on the screen. The A sub I are real symmetric matrices of some size and the linear matrix inequality. matrix inequality describes the set of all points in Rn for which this linear combination A0 plus sum X I Ai is positive semi-definite as a symmetric matrix, which means you all know a symmetric real matrix has only real eigenvalues and positive semi-definite means all the eigenvalues are non-negative. So the curly greater equal sign means positive semi-definite for symmetric matrix. Semi-definite for symmetric matrices, or they could also be emission complex. And so, the optimization, it's not of importance for this talk, how it works. It's of course a numerical process, an iterative procedure. And it can be performed up to any prescribed precision, at least in theory. So, you give yourself an epsilon. You give yourself an epsilon, a positive epsilon, and then, at least in theory, you can find both the optimum of your linear function and an optimizer. And you can do, so the solution set of a linear matrix inequality is called a spectrahedron. That's what is written there. But if you know how to optimize a linear function over such a spectrahedron, you can also do the same for For a linear function on the defined on a linear image of the spectra each, because I mean, just pulling back a linear function by a linear map is again a linear function. So you can immediately, if you know how to do it, you can optimize over linear images of spectrahedra. Those are the spectrahedral shadows. And sort of the standard way to describe a spectrahedral shadow, K is like. called shadow k is like the one you see on the screen. So there is an existential quantifier over additional variables, slack variables that come from the projection on the linear map. Okay. So just briefly, I mean semi-definite programming is a very efficient tool. So If you give yourself a precision, you can do it, perform it in polynomial time. And at least in theory, we come back to this. It has tons of applications and so has become a very useful, flexible tool the last 20, 30 years. Of course, there is a but I mean about the performance and the complexity and what you can actually do. The complexity and what you can actually do because the larger your matrices get, the more slow and resources eating resources eating with the process be. So you resources consuming with the process B. So if you start with a convex set K to perform an optimization, we first of all need a semi-definite representation. A semi-definite representation of k, by which I mean a representation by concrete symmetric matrices as a spectra e to a shadow. And what is intuitively very clear is the larger the matrices get, the slower the performance will be. So you will be happy if you can do it with small matrices and if you have to use large matrices, it will. Large matrices, it will be destructive for the performance. But the total matrix size is not everything that counts. Because in many application situations, it happens that you model your semi-definite representation. It is a matrix composed of block matrices arranged diagonally, like in the symbolic left-hand. The symbolic left-hand picture. So we have a large total matrix, but in fact, the matrix may consist of many small blocks of much smaller size along the diagonal. And if you compare such an LMI, such a semi-definite representation with one of the same total size, which is densely filled, I mean, where more or less the entries are all non-zero and no block diagonal block structure is present. Diagonal block structure is present, it's a huge difference in performance between the two. So, what actually matters for the performance, for the time and for the resources in terms of what actually matters is not so much the total size, but the size of blocks if you are able to break the thing into small. To break the thing into smaller blocks. So, this is supported both by there are lots of practical experiments that people have carried out and that gives this evidence, but also there are theoretical considerations that point to that direction. And so, that was the background and the reason for Gennady Aberkov to come up with this definition. This definition, the semi-definite extension degree of a convex set. So, this definition is meant to capture somehow the minimal complexity of a semi-definite representation if you pursue the idea that this block diagonal, the maximal block diagonal size that you or the min yeah that you can use for this set is Use for this set is sort of the decisive entity. So the definition is the smallest integer that you can describe k by a block, semi-definite representation with diagonal blocks of size at most d by d. So the smallest integer d for which you can do this is as x degree of k. Or in more geometric terms, it means It means that you can describe K in as a or that K has a semi-definite representation means it is a spectrahedral shadow, it's a linear image of a spectrahedron. And this degree is the smallest D, such that you can write it as a linear image of a slice, of an f n linear slice of a direct product of many copies of the Many copies of the PSD cone of D by D matrices. So that's my notation. Here, S D without the plus is simply the space of symmetric real D by D matrices. And with the plus in the index, it's the positive semi-definite symmetric matrices. And to complete the definition formally, we make it. We make it infinity if k is not a spectroequal shadow. So, if there are any questions or anything that remains unclear, please feel free to interrupt me anytime or to write in the chat. So, I won't observe the chat, but maybe in that case, somebody will notify me. So, a few examples. So, a few examples. This is X degree. What does it mean? It is one. That means you can describe your set K by a linear matrix inequality consisting of diagonal matrices, right? One by one blocks. So it's diagonal matrices. And of course, a diagonal matrix is positive, semi-definite if and only if all its diagonal entries are non-negative. So since the entries in So, since the entries in an LMI are linear functions, this simply means your set is describable by a bunch of linear inequalities. So it's a polyhedron. So that means Sx degree one. Sx degree two, or at most two, means it is second order con-representable. So it's a product of a section of many of a direct product of. Product of it's sorry, it's a linear image of a section of a product of Lorentz cones. And these second-order representable sets are, I mean, second-order LMI, second-order cone LMI are also extremely efficiently solved because it's just two by two matrices. So expressing that it's PST means solving a quadratic. Solving a quadratic equation or inequality. So that's much, much faster than larger sizes. Okay, so Gennady made this definition and of course then we proved a couple of results. So his central result is a channel lower bound for this SX degree in terms of some combinatorial kind of conditions of the given convex set K. So I won't describe. K. So I won't describe this general condition here, or maybe in an example a bit later, but let me outline two concrete applications. So he has this general theorem and then he applies it to various celebrated cones and gives lower bounds for their SX degree. So for example, you may look at the cone of symmetric PSD matrix. cone of symmetric PSD matrices itself. So SXSD plus, so symmetric D by D matrices. And a priori, you could imagine that there's a way to describe for symmetric D by D matrix PSDness by a large linear matrix inequality depending linearly on the matrix, but which involves blocks of smaller size than D. Of smaller size than D. I mean, that would be wonderful because it would automatically give you a means to reduce from size D by D to smaller sizes. So he shows as one application this is not true. So you cannot do with block size smaller than D. As another example, consider the sum of squares cone of polynomials. So we fix a number n of polynomials, of variables, and an even degree 2. And an even degree 2d and sigma n 2d consists of all homogeneous polynomials of that degree that are sums of squares of polynomials. And it is very easy to see that the dual cone of this SOS cone, so the dual cone generally consists of all linear forms on the vector space that are non-negative on your given cone. Your given cone. That this dual cone is a spectral etron because so that's really elementary. If you have a linear form lambda on the space of homogeneous forms of degree to D, it gives rise to a symmetric bilinear form on forms of degree D v sublumter, so called the catalecticant by Sylvester. Namely, you take two polynomials of degree D, multiply them, then you have degree two. multiply them then you have degree 2d and apply lambda that gives you a symmetric bilinear form and then by definition lambda is non-negative on sums of squares if and only if it's non-negative on squares so that exactly means that this symmetric bilinear form is PSD so this dual column is the solution set of an LMI of size Of size, well, what is the size of this LMI? It's the size of the vector space on which the bilinear form is defined. So the space of forms of degree d and this dimension is, well, this binomial coefficient n plus d choose d. So that's the size. And so from Gennardi's results, it follows that that's actually the minimal size, minimal possible block. size minimal possible block size so sx degree is this number and is not less for the for the dual sums of squares cone but then by a general observation a cone and its dual always have the same so it's also the sx degree of the sums of squares cone itself okay and uh then uh a result of myself from last year Myself from last year, when you have a closed con any closed convex semi-algebraic set in the real plane in R2, SXD3 is at most two, so it's second-order cone representable. The proof is technical and complicated altogether, but essentially it's constructive. So you give me a complete such set and give me. And give me arbitrary much time and patience, then I can work out such a representation for you. Okay, so this was sort of the situation, and we wanted to extend this last result to convex hulls of curves in Rn for higher n or arbitrary n. And to get an idea of what one can reasonably expect, let's Expect. Let us look at an example at the rational normal curve. So just the curve parametrized by a real parameter t, t square, t to the n as a curve in Rn. And the closed convex, if you take the closed convex hull of this curve, it turns out to be a spectral etrum. So that's sort of an accident, that's in general not true. That's in general not true, but here it happens, so it is described by this linear matrix inequality. So this convex hull consists of all n tuples x1 to xn. I'm writing just for appropriation k for the integer part of n over 2. So given any such x1 to xn, you form this matrix here. This is the metric matrix. matrix here this is a metric matrix of size k plus one and that this matrix is psd is true exactly if you are in the convex hull of this curve that's not hard to see and so the matrix size of this lmi is the size of this matrix which is k plus one so roughly n over two and so a workoff's result mentioned before implies Mentioned before implies that this is indeed the SX degree of K. So you cannot do with a smaller block size. So maybe I can very briefly try to indicate here how the argument goes to show that this condition applies here means you have for any k points in S, K different. K points in S, K different points in S, that means k different real numbers. You have to show that there is a that there is a supporting hyperplane of S touching in those K points, but nowhere else. So that means you want to find a polynomial that is PSD, I mean supporting hyperplane means. means it's I mean or translates into a polynomial of degree n that is PSD on R has double zeros in the K given points but no other zeros and I mean this is elementary if you just write down the factorization so this is what this criterion translates into concretely in this case so I mean just as a very naive immediate Naive immediate application of this criterion, you get this lower bound, and therefore, this is the precise degree. So we learn from this that we cannot expect anything better than n over 2 integer part plus 1. So for n equal 2, it's 2. For n equals 3, it's also 2. For n equal 4 or 5, it's 3, and so on, in steps of 2. And so that's actually. And so that's actually our main result. This is true in general. So for every, you take any one-dimensional semi-algebraic set in Rn, the SX degree is at most this number. And note that the consequence we see in particular this degree is finite. So there is, there exists a semi-definite representation of any such. Of any such convex hull, closed convex hull of a one-dimensional set. That's a fact that was known before, but with a very complicated and abstract and non-constructive proof. And I think our approach is at least halfway constructive and is altogether should be much easier. So it gives a new proof to this fact, also. So, I will now not attempt to give a sketch of the general proof because it's too technical. Instead, I want to consider only the very particular case of monomial curves. And this case is completely explicit, so we will arrive at an explicit definite representation of such a convex Hull of a monomial curve with blocks of only that size that we want to achieve. Achieve. But also, this case shows some key concepts that play a major role in the channel case as well. So let's just talk about monomial curves. And so here's the setup. We fix monomials t to the m0, tm1, tmn. We order the exponents in decreasing. The exponents in decreasing order. So, m0 is the largest, mn is the smallest. So, we allow mn to be zero. So, the last maybe just constant. And so this, and we take this curve with parameter t between zero and one. So, this is essentially for convenience. Yeah, I mean, let me let's just do this compact case. Just do this compact case, so t between zero and one. Let C denote the closed konic hull of this s. So all conic, all positive, all linear combinations of these points with non-negative coefficients and take the closure of that. That's the cone C. And we will, well, we will not construct an explicit representation for C. Representation for C of the desired block size, but for the dual of C. The dual cone of C consists of the linear forms that are non-negative on, well, on C, but that also means non-negative on S, because the cone is generated by S, by the subset S. And if we have calculated the S exactly of C Q. X degree of C dual, we get the same for C by duality and also for the convex hull instead of C conical. So we work with C and in fact that can be translated again to a different setting as we did before. So let I don't want this pen, I want this, let V be the linear subspace of the polynomials spent just by the monomials that we have fixed. By the monomials that we have fixed. And inside that space, let capital P be the cone of all polynomials that are non-negative on the unit integral. And then a linear function on Rn plus 1 is non-negative on S if and only if the corresponding polynomial is non-negative on S. So, in other words, we can identify the cone C star with this cone P, and now we are speaking about. Cone P, and now we are speaking about a cone of polynomials with in which only a given supply of monomials are allowed to occur. So this is a setting. So on the next slide in the right top corner, I have repeated just for our convenience the data that we are working with at the moment. And now recall. Now, recall Descartes' rule of science. That was the point where Frederic on Monday started with. I think you called it the arguably the oldest piece of real algebra or real algebraic geometry in mathematical history, or at least one of the oldest. So it tells us in our situation, our polynomials have at most n plus one. Have at most n plus one monomials. So, by this rule, there are at most n many sign changes in the sequence of coefficients. So, that means there are at most n strictly positive roots. There is a question by Timu. Perhaps Timu, you can ask your question, Jackie. I don't know what I'm saying. Can you briefly show again in which way the monomial curve case specializes? Okay. Curve case specialized, okay. So that was two slides back. You mean this case, yeah, exactly. It was just a little too quick for me. So, like, you said you had the main theorem and then you show it for the monomial. So, the main theorem is which is below here. And so, I mean, this is so this example, we are in Rn, right? It's a one-dimensional set, it's a curve in Rn. We take the closed convex hull and And yeah, I mean, by, I mean, here we see one representation of size k plus one, right? It's a symmetric k plus one matrix. And from our Velkov's criterion, we see that we cannot do better. This is one plus k. So this is the SX degree. This is this number. So certainly we cannot do better because this is a particular example that requires this number. That requires this number. And so the main result shows this is actually the true bound for every one-dimensional set in Rn. Okay? So that's what Descartes Descartes' rule of science. Then a little bit more less immediate but also not hard is the next observation. Not hard is the next observation. So we have this P cone of non-negative of PSD polynomials in this linear space, linear system, if such f spans an extreme ray of the cone. So extreme ray means you cannot write f as a sum f1 plus f2 with both fi in the same cone p other than in a triple way. So f1 and f2 scalar multiple. Two scalar multiples of f itself. I mean, that you can always do trivially, but if there's no other way to write it as an as a sum, it is said to be to span an extreme ray. And so, if this happens, if it spans an extreme ray, this polynomial is determined by its roots in the unit interval. So, roots, by roots, I mean we consider them with their multiplicities up to scaling. So, that means Scaling. So that means if any polynomial in this linear system in this linear space V, which has at least the root of F, is a scalar multiple of F. Or in other words, there's only this one-dimensional space with these roots, or at least these roots. So that comes, of course, from the extreme condition here. I mean, that you prove without difficulty. And now to proceed towards our goal. Towards our goal. So let's start off with an F that generates an extreme ray, and we assume that F does not vanish at the left-hand border of our interval. So F of zero is not zero, but positive. And then combining these two facts, we see we have precisely n roots. We have precisely n strictly positive roots, and they are all between zero and one. Just combine the two. Just combine the two. So let's call these roots alpha one to alpha n. Well, they will be multiple roots because our polynomial is PSD. But let's pretend for a moment that they are different, pairwise different, just to make the next argument to come a bit easier to digest. So let's make this assumption for a moment. Then we can express the polynomial f in terms of these roots as this determinant. As this determinant up to scaling. Right? So, because you see the first row are the monomials that span our vector space. So we see this determinant is a linear combination of those monomials, so it lives in V. And if you put T equal to one of the alpha i, you have two identical rows, so the determinant is zero. So this polynomial vanishes at the root. And the determinant is not. And the determinant is not identically zero. Of course, that's also something you have to check, but that comes from this condition here. So the determinant is not non-zero. So it's up to scaling. It's our polynomial. And well, let's look at the determinant again. So if I put t equal one of the alpha i, you get zero. And if two alpha i and alpha j coincide, we also get zero. So this determinant, consider it as a Consider the alphas as variables as well. Then the determinant will be divisible by a fundamental determinant, right? What of t minus alpha i times alpha i minus alpha j. So consider it as a consider the alphas as variables as well. So we have a product decomposition of f as this fundamental time sum cofactor, and this cofactor is known. And this cofactor is known classically. It's a Schu polynomial. So that's something I, of course, I should have known, but I learned only in the process of our work. Shoe polynomials are, so for every partition, there is a shoe polynomial. It's a homogeneous polynomial, symmetric polynomial that has only non-negative coefficients. In our case, the partition that won't play a role here, but the partition that is at work here is this one. So M0, M1 up to Mn was a strictly decreasing sequence. And so here we take this modified sequence. So it's non-strictly decreasing. So it's a partition in the sense, yeah, I mean, it's just a non-strictly decreasing sequence, a partition of some number. A partition of some number, namely the sum of all those. And so, this general construction of true polynomials are constructed in a combinatorial way from this. So, you can calculate the coefficients in a combinatorial way. And this is exactly this cofactor. What will matter for our argument is that this cofactor has only non-negative coefficients. Now, in our situation, it's a little bit different because since f was non-negative. Because since f was non-negative, the roots will have even multiplicities. I mean, otherwise, I would have a change of sign, except maybe at the right end of our interval. So let us now list the distinct roots together with their multiplicities. So take the beta one to beta r, let that be the distinct roots, positive roots. They are all between zero and one. And these are the multiplicities. And these are the multiplicities. And then a variant, a variation of this previous argument, gives the following expression. So that looks huge, but I try to explain briefly what happened. So before, so we have the same first row. These are just the monomials here, the given monomials. And before I had such a row for each root, I just plug the root into the or the alpha into the monomial. The or the alpha into the monomial now. Here, for example, beta one has the multiplicity b1, and so I have b1 many rows where beta one occurs, but now not always with a monomial, but with a monomial, with a derivative and second derivative, and so on, just as long as you have multiplicity. And that you repeat for every root. So this is sort of the This is sort of the correct generalization. And then, by again, you have a fundamental argument, you have some fundamental kind factor that can be extracted. I called it u sub t here. It is the product of the linear factors with these roots, with these multiplicities, times some non-negative, some positive constant that doesn't matter. Positive constant, that doesn't matter for us. And the cofactor, that matters again. The cofactor is again a true polynomial, actually for the same partition. We just repeat the roots as many times as the multiplicity says. And so note that all the multiplicities here were even because our polynomial was PSD, was non-negative, right? So these are all even. So these are all even numbers, except when beta i is equal one, when we are at the right end of our interval, then that may be an odd number. And so what stands here is essentially a square, right? The square of a polynomial of degree, well, sum of all the bi, or square times one minus t. So how does this help? How does this help? So, we have a different goal. We want to arrive at a linear, at a statement-definite representation for the convex hull of our curve. How does this help? I claim we are essentially finished. Because what we showed, I'll repeat it again, so we made the assumption our f is extreme real and doesn't vanish at the origin. We showed it's a square, that's the square is from the The square is from the shoe polynomial. So not the shoes, that's the square here. That was the square part. A square times possibly one factor of one minus t. And that is, sorry, that is the true polynomial, a polynomial with only non-negative coefficients. All the degrees are under control. So that's for extremal p. So if we have a channel So, if we have a general member of this cone, well, by Klein-Millmann for cones, convex cones, we are in finite dimensional vector space, so no problem with topology. By Klein-Millmann, every member of the cone is a sum, finite sum of extremal members. And the length of the sum, the number of summons, is bounded by Cartiodori theorem, right? Theorem, right? It's a cartio to rebound. And so there's this condition that we used before, but this can be, we can get rid of this because, I mean, if f if an extremal f vanishes at zero, then what you do is you divide out the factor t from your linear system. So all the exponents you decrease by one and decrease by as many times as the order of vanishing is here. Times as the order of vanishing is here. And then you get sort of into an induction situation. So that can be handled with. And so we get every element in the cone is a sum of such things. And that means this representation. So I have some here are the squares times polynomials with positive coefficients. And here is the same with vector one. Here is the same with spectrum one minus t. And let me rewrite this one last time. So I introduce again the integer part of n over 2, write k for it again, and let sigma less or equal to k be the sums of squares of all polynomials of degree at most 2k. Sums of squares or non-negative polynomials in one variable is. Polynomial in one variable example is the same. SOS polynomials of dictionary at least to k. And then I can rewrite this in the following form. So here before it was saying every f in p has this form. And here I say p is contained in right-hand set. So I have a sum of squares of this degree plus t times such a sum of squares plus and so on. So going up with. And so on, so going up with the powers of t plus another bunch of summons with factor one minus t additional factor. I mean, if you compare, so these polynomials have non-negative coefficients. So I sort them by the power of t and then I get this sort of expression. But now this is. But now, this is a semi-definite representation. So, to make this more complete, I know my time is up, but the talk is also finished in one or two minutes. Let phi be the map. You have a k plus one symmetric matrix. You associate to it this polynomial here. Then this polynomial will be a sum of squares. polynomial will be a sum of squares if and only if your matrix was PSD. So the image of PSD matrix cone is just the sums of squares here. And therefore this linear map, I have a large power here of k plus one size symmetric matrices. That's just the number of times it occurs up here. And what is written here is just copies from that expression. So the image of that So the image of that linear map phi intersected with our space V will just be the PSD polynomials in P. And this is a semi-definite representation of block size k plus one. So exactly what we wanted to do. So that's I have one last slide with the outlook to the general case. If you allow me half a minute. So in general, you So, in general, you just very roughly what you do: you have a given semi-algebraic set in Rn of dimension one. One to give a representation for its closed convex hull. This is the bound you want to achieve at. So, you can make a few immediate or initial reductions because you can decompose your set S into finitely many pieces. And okay, so you can. And okay, so you can reduce to an irreducible curve or a piece on an irreducible curve. The curve may have singularities, but what you really will work with is again a linear space of polynomial functions on the curve. And so that you, from the singular curve, you can go to the normalization, and then you have a non-singular curve and just work in the non-singular curve. So these are sort of simplifications. And then one. And then one main principle is that it suffices to, I mean, you have to make this precise, to work very locally. And what you do is you the elements, the basis elements of your linear space, you expand them into power series around a given point and kind of mimics the monomial case before. But you have, you need to use more properties of true polynomials than we used before. Before. Okay, so I'm over time already. Thank you very much. Thank you very much, Klaus. So very nice talk. Someone has a question? Yeah, I have a question. So in the in, let's say you have a question. Let's say you have a rational curve, but not the monomial case. Instead of the t to the mi, you have some polynomials fi, say. So is then the representation that you wrote down the same? But just okay. No, no, it's not because I mean here since we work with monomial. Since we work with monomials, we got the Descartes upper bound on the number of positive roots, and that sort of made life very easy because we know there's exactly n many roots in our interval. And that does not work, of course, in general. So you have from your given linear system, you have to shrink down your interval. You will find there is a positive interval, they write of. positive interval say right of zero to the right of zero on which it works but you may have to do finitely many steps to to fill the entire thing so it it's it's it's not the answer is no to your question i see so you can you always shrink down the interval so small that you can characterize the extreme rays yes that's what you do okay i see yeah but otherwise uh But otherwise, actually, yeah, but you need the Taylor series expansion of your polynomials around zero. And then you, I mean, at least without further work, you only know it will work on some positive, on some epsilon neighborhood of the origin. But you don't know if you have shrinked in advance, you don't know if it will fill your entire interval. So it only really gives the Really gives some interval around your point. And so, of course, you have to do some work to see that. I mean, it looks like you may need infinitely many small pieces, but you have to make precise what you mean here, but that works. Okay, thanks. Thank you very much. Other questions? Uh, other questions? So, can I also ask a question? So, so Klaus, very nice talk. Um, so you mentioned that in two-dimensional space, uh, you would be able to construct things kind of constructively if you have enough time. Can you elaborate a little bit on that in particular in how far are you using, say, the statements of Helpnet and Vinnykov in that respect, or what are the techniques to construct these representations? Construct these representations. Just some words. No, Helton Binnikov is not. I don't see a connection. Of course, I mean, in hindsight, the procedure is this particular case or has to be, should be a particular case of what I. Should be a particular case of what I out what was outlined here. But in that paper, I proceeded differently. I just proceeded from the yeah, it's hard to tell. You take some point and I mean, what it boils down to, you need uniform sums of squares representations of non-negative elements in your cone and uniform in some special sense. Uniform in some special sense because I mean uniform should encode this thing here, so I won't elaborate, but and you you arrive, you can arrive at your, say, you start with some point where you start off, you arrive for such a representation around this point. And then you can give an estimate how far will this get you. I mean, say I write down a concrete representation valid at that point. Valid at that point, I can tell that works for maybe epsilon at most one. So, in an epsilon equal one neighborhood of that point. And then I can do the same business at the next point. So it's in that sense, it's semi-constructive. I cannot do not know an a priori upper bound on the number of patches, but I know finitely many patches are enough, and I can sort of construct a patch around. Can sort of construct a patch around every point. So it's semi-constructive. Okay. Okay. Thank you very much. Thank you for your questions, of course. So thank you very much. So there is no other questions? Yeah, maybe a brief one, if I may. So it's general. So maybe you said it also in your proof, and I did not get it. But I mean, you showed in your proof that there were sure polynomials appearing. Were sure polynomials appearing at some point, and you mentioned that they are symmetric polynomials, of course, and play a role in representation of what are symmetric group, etc. So, like, from an abstract point of view, is there any kind of intuition like why symmetry appears here in this entire approach or which role it might play? So there's certainly, I mean, this. I mean, the strategy is, so where does this number, this bound here come from? It comes from, I mean, it came from, we have n over two roots in the relevant part, in the relevant interval, and we try to factor out those roots with their squares, or maybe they may be repeated ones, but so the total number with multiplicities is this one. Multiplicities is this one, and we factor those out, and it all comes down to understanding the cofactor. And of course, there is a symmetry because, I mean, the question is symmetric in the roots. So the cofactor has to be a symmetric polynomial in with respect to the root. So the roots are treated as variables themselves by themselves, right? So don't treat the roots. Treats the roots as variables, and then, of course, the cofactor has to be symmetric in those roots. And yeah, I think that explains why, at least, why you have to get an asymmetric principle. Yeah, okay, thanks.