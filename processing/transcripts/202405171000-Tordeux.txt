To show you can improve it, okay? And what are the blocking questions? And what are the actual answers to it? So for the application, we are looking to application where you have a plane, 10 meters, and there are some wavelengths, 10 centimeters. And you have to solve minimally problems in the three-dimensional that is 100 wavelengths. The three-dimensional that is 100 wavelengths times 100 wavelengths times 100, and that's the minimum. Okay, so minimally, for example, until 200 wavelengths, you have 8 million elements, which makes 100 million unknowns with P4. And LU server, it is impossible. I will show you. This happens for submarine scattering, this happens for 4G propagation. 4G propagation. Here you have Manhattan City, okay? And you would like to know the impact of some antennas. This happens everywhere. Normally, you say, okay, I would like to solve the wave equation. Let's do the easiest one. The easiest one is LMOS. And you all know that this is an elliptic problem, weakly elliptic due to the minus sum, minus sign. And this minus sum, that is a plus here. Is a plus here. I mean, it's your minus here. Cause no problem in the theory. This problem is well posed. The final element is convergent. So it is the end of the story, no, because you have two problems. The first one is due to a paper of Ilanburg and Mabushka, which is a numerical dispersion. The more the domain is large, the more it is difficult to approximate the solution. And the second one, And the second one is that if you would like to do a divine and compare logarithm, domain decomposition, it is very hard for this type of problem because every point communicates with every point. This is not like for strongly elliptic problem. So I take my high-order nedelic element and I ask myself until which size I can compute. Okay. And And mainly the memory increase with a power 4 with the size of the domain, with the best algorithm with value factorization. So it means that on your PC you can solve 10 lambda, okay, with uh some open end pieces with a low supercomputer. You can solve until twenty four, something like that. Solve until 24, something like that. But with supercomputing, you will use energy like crazy. Okay, it will cost your salary for every run. Okay, so mainly we have to develop something, okay? And you cannot address the larger problem, okay? Even on the computer. And due to this powerful, even if the things will uh improve, you will always be be blocked. Always be blocked. Okay? But the matrix is like this if you if you code on a hexilader mesh. So it is parse, okay? Even column okay. And so when you have this sparse structure that is block sparse, okay, for DG, the idea is to refuse to do LU and just to do multiplication and to do generous, okay? Because LU, it fills the scale term of the matrix, then you can do permutation, gains a little bit. Okay? So. Okay, so mainly the main problem is memory. It is not time computing, time computation. It is memory, the problem. Okay. So the idea is to do a U, to approximate the inverse by a polynomial. You know all these things. And the problem is the following. The convergence of the Emirates problem is related to the spectrum of the matrix. And minimally, And minimally to lambda mean or to the ratio lambda mean of a lambda max. If you have a large ratio between lambda mean and lambda max, you are dead. And we will see it. Look. Continuous finite element in green, okay? And DG with a print free flux in blue. None of them are convergent in a reasonable time. In a reasonable time. Okay, none of them. This is one of the curses of the large dimension. Okay, the due to the propagation, the information cannot go to one side, to the other side of the domain. Yeah? There is domain decomposition methodology. There is a domain decomposition method, okay? That exists, and especially what you are saying. The problems that the best are non-local. Okay, the best preconditioners are non-local. And when the domain is large, you get the memory problem. Okay? So you are forced to use local preconditioner. And that's hard to make work. Okay? So mainly you need to do this to have a team that is able to deal with. That is able to deal with mesh. And this is why we do not do this for TREFS. We do algebraic domain decomposition. Okay? TREFS is an algebraic domain decomposition maybe. Okay, what is classical TREFS? You know all, you have LMOLS, you take basis functions that satisfy L-Mols. Okay? You have a domain, anterior faces, exterior faces, F is a faces. F is the faces, the boundary, etc. Okay? I think. And very strange. I mean, the physics, you don't do two integration by part. You do just one when you do energy. Never two in your physical lecture. For traffic, you do two. Change. You obtain this, okay? Which is Which is monkey no, measure it is illustration. I will use the arrow. I will okay, and I obtain this. And this is this is R2 interface at this time. It is just a gradient times P Is just a gradient times p, nothing appears. And then mainly you say, okay, I rearrange my things, I go to the face, I have to jump, this very clever jump that you all know, okay? And then the next step, it is what I replace the value on the interface by what we call fluxes, but by the way, it is not fluxes, it is traces. Okay, I will explain. Okay, I will explain you later. Okay, and then you have to choose parameters gamma, delta, alpha, beta. Okay, you go back to Lax theory, increase by major, et cetera, et cetera, et cetera. And you say, okay, I will write this coefficient with respect to the incoming and outgoing flux. That is really flux. That is really flux. Okay, this is flux in the theory of flux. Before it was not flux, it was just trace. Okay? When I go back to the old theory, okay, this is flux in the standard Digit theory. Okay, I just picking what they were saying in the beginning of the numerical analysis, okay? And so I am writing my trace, okay, my numerical flux, like a combination. Flux like a combination of these two ongoing flux from the left and from the right, and I have no choice. There is only two coefficients which give to a consistent approximation of P. Only one. If you change the flux, you change this coefficient. But if I take the flux from Lux, I have only one R. Okay? I do the same. I do the same for gradient P, and what happened is that this formulation is coercive and adapted to iterative solvers. This is the work of Bruno and Olivier Ossessna. Okay, we say that. But this approach is difficult to generalize to other equations. The second problem is related. The second problem is related to the anticipation of the basis that is numerically correct, especially in 3D. In 2D, there is no numerical problem. This appears in 3D because you have too many unknowns, and the runt of error becomes really a problem. Okay? So, the variation of formulation is perfect. This is the basis that has a problem. Okay? This is the representation of the basis that has a problem. The problem. And if we look at the local level on one element to the conditioning of this colour product, not of the problem, you really see the problem. The conditioning problem is terrible, okay? And you know that this will amplify the rundown error. So the idea before some year it was to have some heuristic. Some heuristic to choose the right number of things to avoid this render ferror. This is what is doing Peter in the Parmax code. We discussed this. So it's very hard to know at which point you should do because K can vary, etc. So he had to develop to make work ParMax a restrict strategy to any domain. And that is wonderful, this heuristic. Full, this heuristic. Okay, so the idea is all we can do to automatize this process. Okay, and the last things, I do not want to assemble my matrix. If I have hexahedral mesh, I will use HPC to have no matrix, just to have interaction between elements, because I do not need the value. Okay, that's the point. No value, you are not forced to assemble the matrix. Okay? This is a classical lecture. Not Margot, it is my PhD, and not just advertising PhD. But I like her work. Okay, so I changed my point of view, and that's also a point of view that is defended by some of you. This is clear in the thesis of Olivier, for example, that this was a point of view that was under. And this list appears also in some. And this list appears also in some paper of Peter, for example. This equation is quasi-elliptic, but it is equivalent to this hyperbolic formulation. What I call hyperbolic, this can be rewritten like this, by supposing that V of x3 is of this form. This is really an hyperbolic problem, this one. Okay? So I will recall what is the Friedrich problem. What basic information Uh, what basic information does it contain, and how we build a Riemann solver, and what is the numerical flux? Okay, a Friedrich system is a matrix M, okay, and some matrix Fg that applied to Y. The important things is that M is constant, and F that F is constant, and that M can vary. Okay, associated to this, you have Associated to this, you have with one integration, not two, energy conservation that is written like this, and that we will use for TREFS. The first term is the total energy, and the second term is the work on the boundary. For example, for acoustic, you identify to each direction one colour, and F1 is a matrix associated to the color end. matrix associated to the color and okay F2 to the color green and F3 to the color green. Okay, here we get the matrix, the acoustic metric, okay, the things that we study when we are young, okay? Here we get the work that you all know, okay, and you know what? This is the same for every For every wave propagation problem. For example, for Maxwell, we say that Maxwell is much more difficult. No, in this framework, it is the same. The same difficulty. Just the matrix is larger. For example, F1 is like this due to the rotational. Then you have F2, then you have F3. You have the electromagnetic energy, you have the pointing vector. Okay. This uh this this uh goes to elastic, to anisotropic, uh uh and in some situations to aeroacoustics. Okay, when every it is much more difficult to do something. Okay, that's the paper from Peter, where he defines this point of view for 4DG, for example. And I advise you to read the two old paper, the paper from Friedrich, there is a theory of paper. From Friedrich, is there a theory of paper of Friedrich? Okay. So, and now what do we do? We take this formulation with y and y' in the trif space and just one integration by part, and you get your trust formulation. That's this one. This is what we call the flux operator. That's really a flux, because f applied to a divergence, and in physics, what is given to a divergence. What is given to a divergence is a flux. And this is why we call P hat and V hat.n the flux. Okay? And you rearrange the things by phase, okay? And you replace the flux by the numerical flux. Okay? Here it is really the numerical flux and no more what I call the numerical trace, even if I am the only one. Okay? This was to sort. This is just so that you understand my point of view. When you have this, you can do a Riemann solver. A Riemann solver, you put N, you put a constant here, you put a constant here, and you let do the Riemann solver. It gives you only one value on the interface, not for y, but for f of n y, okay, which is the auxiliary variable for the hyperbolic problem. Okay, and Okay, and this defines exactly the value. This works for anisotropic media, heterogeneous, and you have a clear way to define all your flux without any thinking. Okay. Then you can modify this to do better. Okay? So the first step: if you take this point of view, everything becomes direct. Okay, you have just to apply a Apply a good strategy to have your flux. Okay? I mean, to resolve 1D problem, it's easy. It's just a transport equation. You have to diagonalize F. And that's the numerical trace that you obtain. And this numerical stress should be seen only when you apply F to it. So the only thing that has a sense is F times this numerical trace, which are the numerical flux. Which are the numerical flux. And so at each time, you have that your numerical flux is this plus the operator that act on the jump, okay? And you get your rationale. And if you do this, at each time you obtain a coercive, weakly coercive, in the sense of troughs, formulation where you can do an iterative formulation. Okay, for me, it's the first improvement. Second improvement, okay, we go to the basis. Okay, in the theory, it is convergent, 12 pose, that's the theory of Peter and the Perugia, etc. Okay, and but this is the difference between practice and theory. In practice and in 3D, it is occasionally non-invertible, okay, and often non-convergent. Okay, you have your block at some percent. Okay, you have your block at some percent error. Okay, this is due to the facts that I was explaining. The idea is to decrease the computational cost, to reduce the rundown error by either considering better bases that we call quasi-trefs, okay, or by applying principal composite analysis to reduce the trough space. You remember, I have a hyperbolic matrix M. Okay? This hyperbolic matrix defines a scalar product. Okay? Where in my equation I have M dy over dt M is symmetric positive definite. Okay? This defines a scalar product. You have also the UW VF scalar product defined by Olivier and Rudeau that is integral of X. Olivi and Drudeaux that is integral of X6 prime in this paper. So you have many scalar products where you can do a PCA of the basis. Okay, so you dragon analyze, and the theory of PCA is just you take the eigenvalue, you select the largest one. This is the important information. And then you truncate, okay, like we do in statistics, and you have a much better basis that I've erased. What is not important and causes. What is not important and causes the correlation. Doing this, you apply a two reduction and you reduce also the size of your linear system. Okay, you have some criteria like this, which depends of H. You have still two things to solve to some heuristic. Okay, everything is not solved, you have still a lot of heuristic to do. And since you are doing a gram shift, And since you are doing a Gram Schmidt, okay, your basis is orthonormal, and you can choose it or to normal with respect to the iterative process, okay, so that the decomposition, the block Jacobi decomposition is written not with M minus N, like in the classical theory, but with I minus N. So you have not any more M to invert. Okay. And then we can have a look to four different size of the mesh, okay, with different epsilon. And clearly, there is a trade-off. Okay? The largest is epsilon, the less accuracy you have for the approximation theory, but the more you are stable. Okay. So, and also if So if the domain is large, you truncate less than if the domain is smaller. So there is some automasticity that appears. And also, you reduce your memory cost. You reduce more and more, okay? In function of epsilon, here it is much less. Okay, here it is much less than here. Okay. So, this is for a large epsilon, you lose everything. This is for an optimal epsilon, okay? And that's for a small epsilon when it works. It is 3D and just it is cut. Okay? It is, you know, this experience when you send a wave in a glass. In a glass, okay? This clear? So you have to choose uh in which point you are uh in between. Okay, epsilon is uh the cutoff fact. This was the first solution. You apply a reduction, okay, and you improve your basis. And you can do it with all your bases for a very low cost. Now everything is local, even if you are not an exhaedral because uh it is adapted to MPI. To MPI. The second strategy is to go back to the theory of Cessna deprived and to say that the good variable is x here, which is the Fourier is equal to phi. Because you have an operator that is unitary, which associates to these Fourier traces the other one, the conjugated one, and this one is unitary. So the idea is to. So the idea is to develop a discretization of phi and not of p. Okay? This defines an exact solution and that you approximate with some other code, with an approximate variable. You can also approximate it, like he's doing like we are doing with Fanrear, with some polynomial, with some other things for other models. For other models, okay? So, the question is really to find bases that have a better quality. And here it is example in 2D, okay, where you have funny domain. It is a pentagonal domain, okay, with a curved boundary, okay, and the files are on the border, okay, and you get the classical solution that we know. Uh, solution that we that we know analytically. Okay, that's the second uh where you have a quadround triangle that couples, okay? So we have mainly two paper on this: one that is doing with uh boundary element method to do quasi-trace method, and one other that is using uh continuous element okay to do this type of things. And what is interesting. These type of things. And what is interesting is that quasi-trace functions are superconvergent. Okay? Okay. You have the discretization of field that is of order n, okay? And you have the discretization of the entire, which can be of order k. Okay? If k is smaller than n, sometimes it works. It's a miracle. Okay. When k is equal to n, you get the expectation. n you get the expected order for q is equal to one you get order one and when you do a super resolution inside okay you get one half of degree of convert and this is really the h one half h one so q here perhaps it was something else uh before q it is q q it is the degree of approximation of a phi of the x variable and this the order it is the degree of your your finite element p and q here you see that that if you take p p zero and q q equal you you get uh something and you get one half more if you If you take one, two, or three, okay? So you have just to be one degree higher. This is confirmed by numerical analysis. So for HPC, you have two things. You would like not to assemble, okay? That's the idea just to store the things like you do in finite in-time stepping. So this is easy. This is a type. This is a type of one thing before I forget. Even without doing iterative, okay, since you have condensate your two minutes, you degree you are less costly than Nedelec element, okay, just because you are doing pref, even in LU. And when you are doing gemrest, you have this type of structure, that's interaction media. That's an interaction mill against me, me against right, me against left, etc. Okay, so you have 12 matrix, 12 small matrix to store. Okay, and when you do Jam REST, you have to do a small restart. Restart 10, restart 50. That's the message that I would like to deliver. Okay? And this is uh then you have the question of the preconditioner. So that's when you do nothing. That's when you do Cessna des Preconditioner, okay? And then that's what you obtain when you are doing the reduction. You have less eigenvalue close to zero, and the spectrum is much more is less wide. So it increased the quality of the interactive method. And like ParMax, we were Parmax, we were happy to be able to do a test case with one billion unknowns. Does it convert, does it converge? Nobody knows. But we have a shadow and some nice scattering phenomena here. Here you have a three hundred times one hundred times one hundred lambda. Okay. This this was what I was seeing at the beginning. What I was seeing at the beginning. Does that work? Thank you very much. Do we have a question? First remark was okay. Uh killing high white is white. Although the spelling of Killing was not quite existing. So exactly the same. Now the second thing is that the formula that we showed just a slide before the completion of the job is indeed a general command. No first section second section at the end of the No? Yeah. Okay. Yes. And actually, okay, some years ago, yes, I had to work and put in this. I hope so. Make the connection. Okay, so. Make the connection. Okay, so I have to make the connection using this fancy algorithm take it for away. And it's just for steering. Actually, it's formula for stairs and here. It's very stable. And it is the book of Google and Venture behind this formula. Related to the fact, related to the fact that there is this. Yeah, related to the fact that there is this uh lax technique and you have to to to to to check the the to decompose the the these matrices and that you get the Riemann invariant and all. So yes, I think it's very natural and for for a in a compressible fluid then y is the acoustic impedance like that. And this formula is super or very then this formula is very important in scientific computing because it is used in many domains. Because it is used in many domains. Even for non-linear equations, this is what I wanted to say. Even for non-linear equations, it's super important. And I discove okay. I discovered that some years ago, and for me, it was just fantastic also. Because then it makes a bridge between different technique and techniques. And it's so, okay. So I agree. There is this thing, this is the work of Godunoff, and there is something that is also wonderful, is the work of. Also, wonderful is the work of Hermonder for hippoelliptic operator. Yeah, yeah, when you make the connection between ellipticity and hippoellipticity, that's the two things that was making me very fond of these things and the work of a mandor about hippolytic operators. I just wanted to comment on that because for me it was also very important. We have another equation. The line, the line equation. This will be the equation. I never break it.