To talk about limits for sub-critical limitable claushing growth models. Thank you, and thank you to the organisers for giving me the chance to come for the first time out of this beautiful place. Coming to the Zoom version was not quite the same. Okay, so we'll be talking about both models. Both models, random growth models in the plane, and I'll explain what the plastic means in that context in a moment. This is joint work with Vittoria Silvestri and Amanda Turner. So I suppose kinds of growth models which have maybe been studied. Kinds of growth models which have maybe been studied the most are lattice-based growth models. In this case, we'd take the Z two and you have a seed particle, then you'd have some rule for adding further sites to the cluster, and it would grow as a connected cluster. And so this could include examples like the Eden model, where you just use a random point on the ground, you uniformly add it to the cluster, or TLA, where you pick a random walker coming with affinity and the point next to the cluster, which is first. To which it first hits, it's the place where the next part gets added. Now, these models have both proved extremely resistant to any kind of mathematical analysis, and they also appear to depend on the masses based on physically unpredictable. If you think that you really try to model random growth in the continuous plane, there's something wrong with the plane of ones. The K-1s. There's another family of growth models which rely on complex analysis, rely on the use of conformal mappings. And this approach was introduced in mathematics by Carlson Makarov and also by two physicists, Hastings and Levitov. And they're known as Hastings-Levitov type models. The ALE or aggregate learning evolution family is a related family, but it's kind of within this Hastings. But it's kind of within this Hastings-Lebatov aggregation of world. So on this slide, I've tried to describe: so it's a two-parameter, well, in fact, maybe a four-parameter family, but two of the parameters, C and sigma, are asymptotically going to zero, and the remaining of these other two parameters, alpha and eta. And I'll try to describe here how this tomology works. So it's going to be a Markov process. Process with values in compact subsets of the plane, which grows at a finite rate, multiple time. So, all we've got to do is say kind of how long is the holding time until the next arrival, and what is the next arrival, and we'll describe the process. It starts from the closed unit disk. That's my starting point, and it grows in discrete jump. And it grows in discrete jumps by the addition of little particles to this sort of macroscopic seed. So, if I take the example, I'm not going to consider all sorts of shapes of particles to be added, but one example might be where the particles added were little disks, and the size of the disks will go to zero in the asymptotics I'm interested in. So, gradually over time, it will build up some kind of random structure, because the rule for adding disks. The rule for adding disks is a random rule. Now, I'm going to arrange, in fact, that the complementary domain outside the cluster, so this picture now I guess is Kn, having added n particles. The complementary domain is something called dn and this should be simply connected if you regard it as a Subset of therapy sphere. And given that, the Riemann mapping theorem assures us that there's a unique conformal mapping which will take d0, meaning the complementary set of the closed unit disk, to dn. Time's going to be continuous, so it will be. Continuous, so it will be evolving in discrete steps, but it's like a continuous time lock of chain. So there's a unique conformal map which takes the d0 to the dt and which fixes infinity and doesn't rotate the whole picture at infinity. And so there's a one-to-one correspondence between the sorts of compact set KT we're interested in and conformal maps. Conformal maps. And this is convenient because conformal maps are functions. Doing mathematics with functions is much easier than doing mathematics with sets, with rounded sets. Okay, so the data for this process will be the two real parameters alpha and eta and a choice of a capacity parameter C, which will tell us how big the particles are that we add. The particles are that we add. In fact, the radius of these disks would be about the square root of c, because c will describe a notion of capacity for these particles added to the cluster. There's another parameter, sigma, also positive, which is, and this will kind of details of this will emerge later, but it describes a scale over which there's feedback. Scale over which there's feedback in the model. And then you have to decide what sort of particles you're going to add. So, if, for example, we were adding slits, then we would have a map FC for every length of slit that you might add. So, I've drawn this picture with discs, and maybe I should draw this one. So, you have a map FC for each size of disc that you might add. So, C1, C2. might add so C1, C2, C3 and for each of these examples you'd have a map, so this would be C F C, which takes the complementary domain of the closed unit disk to the complementary domain of the unit disk, but with that extra little piece taken away. So we're equipped with this family of maps, and each of these maps is going to be used to add a particle. It adds a particle of capacity C. Spent some more time discussing the model, so hopefully, it's important that you get hold of what the model is. So it's going to be a Markov chain, and I'll tend to switch the And I'll tend to switch the focus in describing it to the conformal maps. The conformal maps defined on the exterior unit disk into the exterior unit disk. And it starts from the initial state is just the density map, which takes the exterior disk to itself. And the rule for jumping is this. And the rule for jumping is this. At a certain rate, which for every possible angle, what we're going to be doing is rotating the place, the angle where this particle is attached, which will be called theta. If I do this to the map Fc, I effectively add the particle at e to the i theta. And the simplest example of these models is where we just take a composition of these randomized maps with a random theater, a uniformly random theta, and see how that composition evolves. We take more and more functions in the composition. If you do that, you will see pictures emerging like this. Each time you compose with one of these little FCs, you add a particle in the You add a port hole in this trust. And that model, where they're just independent and they're dentally distributed, is called HL0. That's a sort of vanilla, simple-to-analyse model, which we understand very well. But for good reasons, we want to make it more complicated, and we want to choose the size of the particles that get added. We want to choose the size of the particles that we add in a way that depends on the current cluster and on theta, and we also want to choose the rate at which we add particles at theta in a way that depends on theta and the current cluster. So when the cluster is in state little phi, we're going to jump by composing, pre-composing with Fc, Fc and C With f c, f c and theta at rate lambda of theta and phi. And here the key thing is that the capacity that the part will get added depends on our basic capacity parameter, little c and it also depends on the derivative of phi at e to the i theta or some slightly smoothed version of it, because this pushes points. Of it because this pushes the point that we're measuring the derivative at out. So instead of measuring actually on the unit circle, we're measuring it a little bit outside the unit circle. E to the theta, theta is positive, so e to the theta is greater than one. So these points run around that slightly bigger circle. So why is this an appropriate function to modulate the dynamics by? The dynamics phi? Well, the derivative on the unit circle of phi has a clear meaning in terms of the cluster. It gives the rate of change of arc length with respect to the harmonic measure on the boundary of the cluster. The harmonic measure on the disk is just uniform. It's just uniform. So it's kind of the theta parameter. And when that gets mapped over here, you can imagine as you make a sort of transit round the cluster here, you're making a transit round all the boundary over here. And the larger this five primed is, the more arc lengths you get per unit harmonic measure. And in fact, what will happen is that you'll get lots of arc length. What will happen is that you'll get lots of arc length when you're kind of in a fjord of the cluster for little additional harmonic measure. And when you're on the tip of some finger of the cluster, where harmonic measure tends to be concentrated, you'll get less arc length for harmonic measure. And it's the fact that the model depends on this physical quantity which makes it a Laplacian growth model. What is AK of C? A K. Oh, sorry. So I wrote this down simply to indicate the generic form of these conformal maps. So they will always have this form. It'll be a positive, it'll be a constant greater than one, whose logarithm is the capacity of the little particles added. And then it'll be of the form the density map plus The density map plus some convergent Laurent series. So it will always, so if I give you the F, you always be able to expand it in Laurent series, and that these won't actually reappear. So this is the shape of the conformal maps. They're all of this form. So what we're allowing ourselves to do is to take some power of. is to take some power of this quantity phi primed and use that to modulate the size of the particles being added. So this will allow us to add sort of bigger or smaller particles at the tips of the cluster or in the fjords of the cluster. And it'll also allow us to change the rate at which we add particles. Similarly, depending on the shape of the cluster. Notice that I put a C here. I'm sorry, C is slightly. I'm sorry, C is slightly overworked here. C is on one hand kind of basic parameter of the model, which is going to go to zero. And it's also a free parameter which describes all the possible capacities of particles. So there's perhaps a little room for confusion there, but there will be a particular fixed C associated with the model. And that C is going to go to zero. So what's going to be happening is that we'll be adding particles of small capacity very rapidly. Very rapidly, because C minus one going to infinity. We're looking at the kind of law of large numbers regime, where the kind of total amount of capacity being added at angle theta is like the product of phi prime to minus alpha by phi prime minus eta. Because the c's, and the c's minus ones will cancel. Okay, so that's uh that's the model. That's the model. And so, in fact, this phi prime, any given time, will be of this form, f1, for those with fn, where n is the number of arrivals in the model by time t, and each of the f's are one of these randomly rotated single. Single particle maps. Okay, a little bit more about the clusters recapitulation at the beginning. So there's this unique encoding of a cluster by the conformal map in one-to-one correspondence. When I take a jump in this model by precomposing by Precomposing by F of C and theta, what this does, it takes the cluster that you had before and it adds a new piece to it. So my cluster had reached a shape like this, okay, then I would have some point on the boundary, I'd add a little bit more at the time of the jump, and that will be the image under the conform map which is associated with K of Associated with K of e to the i theta P P C, where P C was the particle that you added to the unit disk by F C. So it it it rotates this round to e to the L V7 V C and then it maps it to Then it maps it to this. I guess if it's a little ball, it's going to look slightly more like over here. It adds that new piece to the cluster, and you can just keep on going on particles. So the basic question we wanted to understand is, can you take a scaling limit of c goes to zero? Or at least to some extent, we'd answer that problem. What would the scaling limit look like? What would the scaling limit look like? So I've mentioned before that we're sort of adding capacity at theta at a total rate of phi prime to the minus zeta, or to the minus alpha plus eta. I'm going to call that zeta. And it's well understood in complex analysis how that should look in terms of an evolution equation. If you want to kind of add And if you want to kind of add capacity at theta according to some measure on the boundary, then what you use is the so-called Leewner-Kufereff equation, and this is the radial-Leebern-Kufereff equation. You may have seen this in the context of SLE, where radial SLE, you don't have here some kind of diffuse measure on the boundary, you have a unit mass concentrated at the Concentrated at the place that the Brownian motion is on the boundary of diffusivity kappa. So this is kind of within that same framework. Instead of having that Brownian motion driving proceedings, I've got feedback from the current cluster coming in here. So generally speaking, you'd have some sort of given measure, a driving measure for the Cliffore F. But I'm taking the current state and I'm feeding it back in and saying that the driving measure has got to be. And saying that the driving measure has got to be this function of the current state of the system. So it's not too difficult to convince yourself formally that this should be the fluid limit for the model I've described. And indeed, if you're operating at a physics level, taking zeta equals two will give you the Healy-Shaw equations, which are supposed to be the continuum limit. Supposed to be the continuum limit for DLA. Taking zeta equals one, this should give you the continuum limit equations for the Eden model. And there's this family of models which interpolate between Eden and DLA and beyond, called the dielectric break-time models. And so those would also be examples of this equation. But this isn't a straightforward equation. Equation. It is when z equals zero, you solve it explicitly by that formula. That's related to HL0. HL0 is easy to understand, and I kind of did all that already. As I say, when zeta equals zero and sigma is zero and zeta is two, this is the Healy short flow. It's been quite extensively investigated. The fact that the modulus here is sort of algebraically nice allows Allows for some quite cute ideas in understanding how this evolves. And it's known to develop singularities in finite time. Even if you start with analytic initial data and you insist that the solution is analytic, it only has a finite lifetime. It's otherwise very poorly understood equation. In particular, the case when z equals one, which I think is a clear interest. Which I think is a clear interest, has not really been studied. Right, where the Z parameter comes from, the initial description of the model. Oh, so it's alpha here, is that right? Maybe I didn't write it. Zeta is always going to be alpha plus Zeta. Okay, there you go. That's what I think. Sorry if that was a mistake. A little bit more on this equation. We know that if you start with by zero being analytic, then you'll get a unique analytic solution for a short time. You can just mimic the theory which is done for the z equals two case, which is sort of classical. You can spot solutions to this equation. If I if I If I look at the function of this form, then this equation just reduces to a simple ODE for Tor, which you can solve. So there's a family of solutions which correspond to growing disks. There's always a family of solutions which are growing disks. We expect, for various reasons, we really don't have a good theory. Don't have a good theory, that for zeta less than or equal to one, this equation is well posed. I don't think it's known. For zeta equals two, it's known that it's not well posed. And for zeta bigger than one, we strongly suspect that it's not well posed. In part, because of the stochastic models, which are supposed to correspond to them, we can simulate the stochastic models. And for example, with DNA, you get this fractal type. DLA, you get this fractal type growth, and that's not going to correspond to the solution of the nice differential equation. So at some formal level, our ALE processes are stochastic discretizations of the Levner-Kufrev-Alpha Moseta process, Avnak equipment solution. And we're trying to show that that's true. If you simulate our processes with Z2 bigger than 1, you will see the characteristic fractal growth of DNA. So it looks like Z2 equals 1 is the threshold. Okay, so this is the result I wanted to announce or talk about. So if I take one of these ANE alpha eta processes, and remember there's a C and there. Processes. And remember, there's a C and a sigma in there as well. And I assume that the zeta is less than equal to one. So on the right side of the thread, that's what the subcritical is in my title. Assume the zeta is less than or equal to one, and take c and the capacity and the regularization parameters go to zero, but with a constraint. And when zeta is less than one, it has to be slightly more. Zeta equals one, it has to be slightly more constricted. More restrictive than when zeta is less than one. If sigma is much bigger than c to the half, or acg equals one much bigger than c to the one-third, then we can show that if you start with a disk, then the ALE process as a process will converge to a crowing disk, and the radius of that disk is given by solving the block-of-rough equation. So, yes, there's a So yes, there is the expected correspondence between the random model and we don't know much about the differential equations, but we know they have these disk solutions, and those disk solutions are the relevant ones for this limit. Orders were Well, we've tried very hard to make them as small as possible, and I'll come back to really one-third later on in the talk. But I don't believe that they are, but I think that they are the limits of our current approach, to be honest. Now, if you make some slightly stronger conditions on the way that the regularization parameter is allowed to go to zero, then in fact the In fact, the fluctuations of the ALE process around its deterministic limit with the kind of Gaussian rescaling converge to an explicit Gaussian limit. So these are going to be holomorphic functions in the exterior disk. So they'll have Laurent expansions. You can look at the coefficients in those Laurent expansions. In the limit, Expansions. In the limit, those coefficients do independent on-site limit processes. And we can calculate exactly what the relevant parameters are. So we've got an explicit description of the Gaussian limit. Is the initial condition of the unit disk very important if I started with another arbitrary convex domain with some maximum diameter? Would that change the limiting shape? It changes what we can set. It changes what we can say. I strongly believe that, and it would be very nice to know. It's on my last slide. Things I would like to know include the answer to your question. So, in the rest of the talk, I would like to indicate a little bit about how the techniques we use to prove this. And firstly, just kind of step right away from the specific. Just kind of step right away from the specific problem and talk about if you have a Markov chain in continuous time and you arrange that it makes a very large number of very small jumps, then often one can approximate it by the solution of a differential equation. Space, but and the way that we try to identify the differential equation is that you Is that you try to do some sort of Martingal decomposition like this of the process? It's always going to be possible to do that because it's Markovian. And in the sort of situation I've described, this is going to be small, so you might suppose that x was pretty close to the solution with the differential equation. So let's look at So let's look at that in some generality. Markov chain and bit bit of freedom here. B you can think of as close to the quantity b to the appears here, the dip of the of the Markov chain. It doesn't necessarily have to be exactly the same. Now it turns out that although this works quite well in finite dimensions, the sort of infinite dimensional problems that one's looking at here, it's really not the best approach. And this is Not the best approach, and this is better what I meant to describe. We imagine that our Markov chain is kind of close to the solution of our differential equation. So, what matters is not the differential equation, but its linearization around that trajectory of the solution of the differential equation. So, let's linearize the differential equation around the limit path. So, the propagator for the linearized equation is going to be called PTS. So, that's obtained by software, the linearized equation. This is the linearized equation of that one. So, I want to write a formula, and I want to kind of highlight this formula, because I think it's a good way to think of things, that we can consider the difference between the Markov process and the solution to differential equations, the sum of two terms. Equations of the sum of two terms, the first one of which is got by taking the compensated jump measure of the Markov chain and then integrating the jumps, so y minus xs minus, will be the jump that x makes at time s. But then they get hit by this linear operator, P T S, sort of, to move them all to time t. And this will be a martingale. Well, it will be the value of time t of a martingale, there won't be a parameter. Of the Marticale, there won't be a Martical in T because of this dependence on T here. And there's a second term in this decomposition, which I'm calling the interpolation formula, which is somehow a quadratic term in the fluctuations. Because if you imagine that beta is going to be close to B as a function, then this difference between beta of xs and b of xs is close to being the gradient of b at xs. Of B at xs minus little xs. So this is somehow close to the second derivative of b at xs minus xs. But this is the exact formula. And this gives a tool to stare at the terms in this formula and you try to show that you'll give an example. I mean, if you're trying to show that you've got a fluid limit, you're trying to show that both of these terms are small, negligible in the limit that you want to consider. The limit that you want to consider. If you want to prove convergence to a Gaussian limit, then you try to show the AT is negligible, the limit you consider, and try and pull a Gaussian process out of this particle. Now, to me, this whole thing is very suggestive in certain contexts of the KPC equation, because, as I said, this integral dt here is quadratic. Here is quadratic in the fluctuations of the market process around its scaling limit. More on that later. But this will be the approach, and we found this an effective approach, particularly when this whole analysis has to take place in functions rather than in RD. So, in order to start doing some analysis, So in order to start doing some analysis with the formula, we have to agree what sort of state space we're going to consider the ALE process. And it turns out that the good way to do it is to peel off the capacity of the process at time t, which is kind of the single parameter, real parameter, which describes how big it is. And then the second factor is kind of normalized because I've taken away the capacity from it. So our state variable. So, our state variables, which will be kind of equivalent to the original state variable phi t, are the capacity and the difference between the normalized map and the identity map. So, this phi map will be a holomorphic function which is bounded as infinity. It's kind of flushing these in front of you. Just kind of flushing these in front of you, I I guess. I'm I'm I'm it it it turns out to be the good norms to control functions like this are obtained by just integrating the function, p to power, around a circle, and looking at this kind of hardy norm, p norm. And one of the reasons this is convenient is that you can sort of move the p around in sort of the wrong direction for For the usual inclusion of p norms, you can go from p to infinity if you just move the r parameter a little bit out any little price, just because we're talking about holomorphic functions. And you can estimate the derivative of the function in terms of the norm of the derivative in terms of the norm of the function itself, again by moving the house a little bit. But those turn out to be useful forms. Forms. One thing that came along which I'd not had to learn about before in probability was the use of norm estimates for multiplier operators in Fourier series. So if I have an operator m which acts on the roll series in this way, so what's happening is it's acting diagonally. acting diagonally in the in the z to the minus k basis by just multiplying each of the z to the minus k functions by m k. So multiplier operator. And if you just use Plancherel's formula it's easy to see that the 2 norm, the 2r norm, you can estimate in this way of the operator. So m phi 2r is less than or equal to that quantity, that supremum, times That quantity, that supremum, times the 2R norm of phi itself. We want to work in LP for various reasons for arbitrary large P, and so it's convenient that there's a sort of L P analog of this inequality, which is known as Martin Kievich's multiplier theorem. So, under some conditions on the multipliers, you get this estimate on the p-norm of M. On the p-norm of M from L P, to itself. You saw in that interpolation formula that the key thing to do was to linearize the limit dynamics around the solution. So here's the vector field which drives the LKZeta equation again. Normally, if I linearize that, I'll get this equation. Equation. So actually, the linearized dynamics is obtained by solving this, and what that comes down to is a semi-group or a propagator acting on functions as e to the minus tq, where q is this multiplier operator. It multiplies z to the minus k by q of k, where q of k is given by this formula. Particular is one of the operators which Partienkevich's estimate will enable us to handle. So we were lucky to be able to spot a solution which has these disk solutions. This relates to your question. We only really know very little about these Alkinsiti equations. We do know one solution, one set of solutions, and we can linearize around those solutions, and we know that the linearize. The linearized equation is well behaved, and the operator driving it, Q, is a negative operator. That's why this Q is a positive operator, so this minus Q, which is the linearized equation, it's a negative operator. So it damps down, it damps down fluctuations. Oh, and yeah, and when zeta is bigger than one, it doesn't dampen down. And down. I think so. This kind of starts to reveal why the zeta is zeta being less frequent, one is important. Okay, maybe I'll just won't say much about this. Burkholders, Martin Geel estimates, is something which every stochastic analyst has met at some point. Probably more often with continuous. Probably more often with continuous martingales, but there are, you know, they're there for discontinuous martingales if you want them. And what it says is you can control the size of a martingale in terms of its quadratic variation and the maximum size of its jumps. Running short of time, I'm not going to read the way through that, but for those. The way through that, but for the sorts of martingals which come up in this theory, it's very easy to compute the quadratic variation and the maximum size of jumps well enough that you could make effective use of this inequality. Okay, now I wrote down the sort of general case of this interpolation decomposition between the Markov chain and this. Between the Markov chain and its fluid limit. Suppose now our state space is no longer called E, but it's now a set of continuous functions on E. And in the example of A and E will be some sort of continuous functions on the exterior disk. But remain slightly abstract for a moment. I suppose that x jumps when it's in state x, which is a function on E, suppose it jumps by this continuous function delta of x and theta at rate lambda of x and theta. Theta at rate lambda x of the digit theta for every theta and multiplier. So this, you know, this sounds a little bit like A and E, but it's a bit more general. Well, then I can write the evolution of the process X in this way. It's kind of, as it were, driven by its joint measure. And then the interpolation formula is kind of rewritten in this format, and you see the The PTS, which is this kind of propagation from the linearized operator appearing here, acting on the jumps. And so I'm going to take this in the case of ALE when you can calculate the delta explicitly for the dynamics which I described and the lambda. And, well, no, okay, so this slide is still in the slightly general framework, but what I wanted to show was the way some of these pieces fit together. So, by using Burkelder estimates, I can combine that with the Mark Sikievich LP estimates, but also the way that these hardy LP norms. These hardy LP norms allow us to control holomorphic functions. So there's LP everywhere, and I can do lots of Fubiniering and get some useful estimates out of all of this. So I can take, for example, Burkella's inequality and integrate it around the circle, and that gives me control of this new triple p-naum, which is defined by taking the integral of the The integral over the whole space, or say around a circle, of the p-norm of the martingale. So there are kind of functional versions of Burkholtz and the quadrature that you get by integration. And then in the case of ALE, it is actually possible to get good enough control on the quadratic variation which appears in this formula. So this is at a very So, this is at a very sketchy level how we control the Martigale term in that interpolation decomposition and show that it's small. And then there's another story with the A term, but that turns out also to be small in the C goes to zero limits, and that's what allows us to show the fluid limit. Okay, so what would we like to do? So, what Ty asked is the first thing we'd like to be able to do. I would like to be able to start from an arbitrary shape and evolve the limit equation. And I imagine that if zeta is less than or equal to ones, it's going to sort of regularize towards a disc. That's completely unknown. And we're kind of not really started on that. We've got some a priori estimates on how. We've got some a priori estimates on h how solutions might behave, but that's still a long way. Perhaps the most exciting thing at the moment for us is if we take z2 equals one and tune the regularization parameter to exactly z to the one-third, then at least formally the interpolation decomposition in the limit becomes a KPC equation. So, this not only is interesting because KPC is of wide interest, but it also somehow justifies introducing this local regularization parameter. I mean, it's kind of known that the KPC equation is somehow you have to get everything balanced up nicely in scaling to get KPC. But the regularization parameter is giving us the freedom to balance things up so that we can get that equation in the head. Get that equation in the middle. So, I mean, as is well known, it's not the friendliest object to analysis with, but so that's something that we are trying to do. Finally, well, if zeta is bigger than one, we've got lots of nice simulations which show very interesting fractal type structures emerging from the ALE processes and actually producing similar phenomenology. Similar phenomenology to the kind of classical continuum DNA sequences. It seems pretty clear that the Healy-Shor equation is not the description of random growth in two dimensions. It seems to be one of those examples in PDE where the P the sort of current PDE doesn't carry enough information to to to propagate. To propagate the dynamics. But of course, the stochastic process is well defined. Now it's written down. So we have uniqueness, but the PDE is a reduced description of the dynamics, which is nice to have when you can have it, but maybe when zeta is bigger than one, you can't have it. But then what does it okay so suppose we had a scaling limit for ALE with zeta bigger than one? With zeta bigger than 1, it would still be a growing family of compact sets in the plane. So it would still be the solution of a Leubner-Kufereth equation, because there's a one-to-one correspondence between growing families of compact sets with simply connected exterior and driving measures, driving measure families for living Kufa Earth. So there's still driving measures. What on earth? Driving measures. What are them? What on earth are they looking like? Imagine yourself that you've got this scaling limit. Can you describe the driver measures? I think that's an interesting question. Oh, the little the little target or what I'm talking about is uh that's That's two relation of ALE with Z3 equals 1. Reasonably disclike, but maybe a little bit rough around the edges, as you would expect the critical case to be. Questions? Is intro a lot of showings that this Also shows that this are but can you estimate like a wider region of the galaxy on the gaps depending on oh you mean you're seeing that the little gap? The wider return. This this this widening. Yes. Um so um a long time ago with Amanda With Amanda, we looked into this with HL0 and we found that if you look at the fingers of the cluster from the outside, of course, the fingers form a tree as you go in. They join up from time to time. And you can take a step. And you can take a scaling limit of that, and what you're seeing is, in a suitable sense, a ratio's coalescing Brown and flow. So you kind of refer the question to so we can describe what these fingers look like, and actually, you also get the dual flow in this, because it's Because it's if you imagine these were slits, then for any point, so the slits occupy sort of Lebesgue measure zero. If I pick random points within this figure, then I ask what's the shortest distance that I have to, what's the shortest path from that point out to freedom, right? Escape routes from the cluster. And if I take a bunch of those, the escape routes will sometimes join together as you move further. Join together as you move further out. And those are again our coalescing Brandon notions. And they're the dual Lauratier flow. So in the continuum, there aren't any white spaces, but we can say something about the internal structure of this cluster. The shorter answer, that didn't give me the chance to explain what we've done about it, is that the white spaces within the cluster are an artifact of the simulator. That are an artifact of the simulation. I guess, well, maybe you could prove a theorem that at some given level of discretization, there will be holes. And you're asking the question, how big are the holes? We've shown that the holes go to zero as you let C go to zero, but we don't have to care about it. Jonathan Manning has watched. Hi, John Flumi. Thanks. How are you? Nice. How are you? Nice to see you. I have two questions, and so you can take them in any order you want. The first one was: you made this comment that when the parameter was bigger than one, you had this fractal slash turbulent behavior. And I guess I wanted to understand a little bit more what you meant by turbulent. I mean, is that something about how certain correlation functions are scaling or some multifractality? That's question one. And the other question. And the other question is: You said that the fluctuation is for KPZ. Is that scaling such that it essentially flattens out to be KPZ on the line, where the curvature goes to zero, or is it KPZ on some circular double, on some circle? Yeah, okay. I'll ask the second question first, and I'll try to remember the first question. So, Jonathan, I didn't say it, but I was carrying on the slide. I was carrying on the slide to say closely related model. And what we've done is we've unwrapped this disk model into a kind of periodic cylinder. So we put ourselves on the line. So you have some curvature that remains in the element? No, no, we so the the version of KPZ which um which emerges is the periodic KPZ equation. Of the KPC equation. I see. Okay, okay. So it doesn't expand out to the whole line, but it does get flat. I mean, what could take suitable limits to kind of produce the, at least formally, to produce the KPC on the line out of the periodic one. But yeah, but no, the natural KPC is the periodic one, which is kind of nice. I mean, that's going to be easier than doing the other one, I think. First question was about the turbulence. First question was about the turbulence, kind of in what sense do you mean turbulent slash fractal? Yeah, I didn't mean anything but that. I use the word turbulent because that's what Hastings and Ebtoff call it. I think in mathematics we call it fractal. There's nothing more to it than math. So it's not that it has some kind of multi-fractality or some weird scaling laws. It's just kind of, you'd say it's straight up fractal. Well, we don't have a limit theorem, so we don't have a fractality. We don't have a limit theorem, so we don't have a fractal. Okay, fair enough. You know, if we use slits, then the clusters are all one-dimensional. But they're maybe asymptotically not one-dimensional in scaling. But that's the kind of fact I'll picture. Thank you. Well, uh I think if there are further questions, maybe ask uh things in the the lobby as we eat our eat or eat. Obviously, eat and drink some coffee. Let's take the speaker one more time. So 3:30, we'll start again.  To some extent, it may be very appropriate. One thing that will be interesting to understand that seems very clear perhaps is unlikely to give universal statements. Will we take significantly? I'll reorient the maps. Okay, cool. Okay, we're close. Hey, but I'm going to climb up there. And you're saying all out. This height, which is one difference, is about. So, if I could take two minutes of weeks, then this this is the boundary. It's not gonna work there. So this line is getting mapped to the boundary cluster. So if I type signal to a thing that is an obviously performance. There's a good reason for wanting to type just here. I understand. Yes, so we kind of I think if you want to pick out things which are too much higher. That approximation is closest if you turn it off. 