                           The next symbol will be zero because I have two followed by one. Next symbol will be zero. If that one step back, I have zero, so I'm here. I have one and I look. Here, I have one, and I look a step back. I have zero. Next symbol would be two. Now, if the last next symbol is zero, I look a second step back and situation could be more complicated. It before zero, I have two. Okay, I'm here. I have zero, before I have two, and then the next symbol is zero for sure. If it's one, then The next symbol will be either one or zero with probability because I'm in this situation. I have zero. One step back, I have one. Next symbol will be either zero if I decide to erase, or it will be one if I decide not erasing. Now, finally, if one step back, I got zero, I need to go and Need to go a further step back. Okay, so this is very elementary. Now, observe that I can encode all this in this tree. So, this is the present. This is one step back. This is two steps back. And this is three steps back. So, this is something which is quite useful. So this is something which is quite usual when you do information theory. You encode the data using a tree. So it's like a dictionary, but a dictionary is organized using the prefix, and this tree is organized using the suffixes. So one step back, I have two. This is enough. If you one step back, I have one. This is not enough. One, this is not enough. I need to go a further step, and this is enough. If I step back, I have zero, this is not enough. I go and step back. If I find two or one, that's enough. Otherwise, I got zero. I need to go a further step back. So the leaves of the tree, this tree tell us what is the suffix of the past that we need to do the best possible prediction. To do the best possible prediction. It's useless to go a step further in the past. This will tell us nothing that we don't have already with these suffixes. Associated to each leaf, I have a transition probability. If I am in this leaf, I know that the next symbol will be either a 0 with probability 0.2 or a 1 with probability 0.8. 0.8. And now, if I am in this leaf, one preceded by two, I know that next symbol will be zero. If I am in the leaf in which is last symbol is one and the previous zero, I know that next symbol is one and so on. So this is a way to encode data. So you know how to encode data. You assign a model. You assign a model to the data, and then at each step, you see the portion of the path you need to predict the next one, and you associate a very short code to things which occur with a high probability, and a longer code to things which arise with a smaller probability. That's why Risenin proposes such a model. So, this is very grid, is much better than you. Is much better than using Markovic chain. For instance, you could say that, well, this only depends on the three last steps. That's true. But you see, here you have only eight leaves and the number of parameters is very small. It's very small. If you have a Markov chain with memory of length, fixed length tree, you have many more parameters. Okay, so this is the basic idea. So, this is a basic idea recently gave to encode data. Now, we can use this to propose a experimental protocol in neurobiology. So, my question is the following. Many people had observed that the brain makes models and makes prediction. The classical example is present, so I can think the end tells is they present five tunes, the same. The same tuner, and so on. And you measure EEG data in the voluntary scalp, and then suddenly the last tune is different. And then you see that something in the EEG data that shows a mismatch negativity phenomena and the even stronger. And even stronger if the last symbol is just erased. So we know this. Now, life is more complicated than just reproducing the same tune five times. So the question is, can I imagine an experimental protocol in which I expose volunteers to a sequence of random acoustic signals? Of random acoustic signals generated by a context tree, that's the structure. And then looking at the EEG data, is it possible to retrieve the Samba context tree from the EEG data? That's my problem. Okay, so this is the context tree. So I didn't put the transition probabilities, but imagine it there. Imagine it there. With this transition probability, I generated a sequence of auditory stimuli: strong bit, weak bit, unit, silent units. A volunteer receives this. In the experiment, I don't tell the volunteer anything. I don't tell try to understand what's going on. I only tell the thing, do not sleep, do not move. I'm collecting EEG data with you. And then I got. And then I got this EEG data. I'm using 18 electrodes. My question is: Is it possible to retrieve from these 18 electrodes the samba context tree? Okay, now I need to use some formalism. So we already call the X one, X two, Xn the sequence of auditory stimuli. The sequence of auditory stimuli. Call Yn, the segment of EEG data recorded while the volunteer is exposed to the auditory stimulus XN. The question is, is the law of Yn a function of the string of the past stimuli? Is it a function of the entire sequence of past stimuli? What what is the relationship between the past stimuli and the Past stimuli and the EEG data. Well, recall, that's the context tree. Let me call CN the, okay, I forgot telling you. The leaves of the tree, which are the suffix of the past that you use to predict next one, have been called contexts by. By reasoning. So let me call Cn the context associated to the string of past symbols up to Xn. For instance, if my sequence of past symbols is 0, 0, 2. So I come here. This direction I have 0. In the vertical, I have 1. In the horizontal, on the right, I have 2. So I follow the brand. I follow the branch, starting from the last symbol. So the last symbol is two, and two is already a lith. So that's enough. So that's the context. When the last symbol is a two, the context is two. So you see, the context tree, what it does, it gives you a partition of the set of all possible past, all possible strings of past symbols. Path symbols. So here we have two, which is a context. If the sequence, the last symbol is one, so I come here. The next one is two, two, one. And that's enough. I don't need to go back the past. So 2, 1 is a context. If the last symbol is 0, so I came that 0. The next one is 0. Oh, now I must go back until. I must go back and choose the third step, it is zero. Conjecture: the law of yn is a function of cn so I like to tell you that this is something which is more or less the same idea of the one which appears in evoked potentials. So, is this true? Well, to address this question in Ariguero's point of view and to do statistics, I need to View and to do statistics, I need to do some mathematics, and that's it. So, that's all the mathematics I use. So, you have the sigma-soft auditory stimuli. So, I recall CN is the context associated to this, is a suffix of this, which belongs to the tree. I call Y1, Y2 the sequence of segments of EEG, which have been recorded. You see, YM. You see, Yn is a real function. You can see it as, well, I put L2 because I'm using Hilbert spaces in my proofs, but it's a continuum function with lots of regularity. And this interval T, the time distance between two consecutive auditory stimuli. And now that's a new element. I call Q of W, where W is a context. Where W is a context. So I have a family of probability measures on L20T indexed by the family of context. So this is a model. This is a model putting together the sequence of stimuli and the sequence of responses. And the basic assumption of the model is the following. If I take any integer n and any sequence of Boridian sets, Sequence of Boridian sets, Boridian sets in the space L202. And then I ask, what is the probability that my first AEG belongs to B1, my second to B2, my last one to BN, given the sequence of audio story stimuli. And the assumption is, well, they are conditionally independent, and the law of why. Of Y and Y in Menzone only depends on the context which ends in the auditory unit XJ. Okay, that's a model. This is not a Markovian model. This is something different. But the question is, is this a good model for the sequence of the samba auditory stimuli and corresponding EEG data? So I use this model to make statistics. make statistics. So I need to make a new a new obtain a new result to answer this. And in particular, I want to know if the laws of the EEG segments are really a function of the SAMBA context. So that's my problem. Okay, the mathematical model has been introduced in this paper with the CRM. So these are Riglos results. We say, why Riglo's out? Well, that's my way of doing it. I'm a mathematician. And the analysis of the EG data has been done in this paper, which appears last January, last year. Okay, now let me explain to you how we proceed. How we proceed. So, for each string of symbols, weak bit, strong bit unit, for each finite sequence, I identify in the sequence of the auditory units all the times N1U, N2U, such that the string U appears ending at NJU. And then I collect together in the set Y of U. Together in the set y of u all the segments, EEG segments corresponding to this to this index. You see, I'm only considering the EEG which appears here in this last symbol. And then now the question is, what is the dependency between the law of the E of the EEG segment and the sequence of pass strings. The sequence of past strings. So I test this in the following way. For any string V which coincides with U in all the points except the last one for any string U, I use the sets I collected and I test the new assumption that the A G segments correspond to U and V have the same law. So I tell you this way, it's quite complicated. Testing, testing. The testing testing functional equality of the laws of functional data is a very complicated problem. We use the projective approach. So that's the kernel of the paper I told you. But this can be done. This can be done. And then what we do is the following. So recall, we are testing the assumption that the EEG corresponds to U. The EG corresponds to U and V have the same law, while U and V are coincide in all the symbols except last one. Well, if the new assumption is not rejected for any pair of final symbols which are different, then you conclude that these last pairs do not affect the lot of the G segment. So that's a classical way to prune. Prune past symbols. So if the laws are always the same, no matter what are the values of V and so this last symbol is not important. I prune then and I start again the procedure, but now with the one step shorter sequence. But if the new hypothesis is rejected for at least one pair, then you conclude. Pair, then you conclude that the last symbol is important. And then we stop pruning at that point. And I call T hat the tree. So this is a way to constitute trees. The tree constituted by the strings which remaining after the pruning procedure. The question, is tau hat equal to the samba tree? And then the answer is the following. So here you have the 18 electrodes we used, and you have, well, this is the results obtained, making the mode across participants, across 18 participants. When you do this, you see that's the tree of the summer. And you got exactly. And you got exactly this tree in all the prefrontal, in all the prefrontal electrodes, which was expected. This one, well, we don't succeed identifying the last branch here. So you see, this is a measure of the distance, how close the obitane tree is from the one. Well, the results are not bad. Well, the results are not bad because what we expected is that the tree should be retrieved here in the pre-frontal. Well, this one is not good, but for the others, it's good. And as a matter of fact, we retrieve the right tree even in other points. So that would be the end of the story. But it's not. I still have three minutes. And why is not the end of the story? Because a colleague of mine kept asking me, but how the... Kept asking me, but how the hell do you know that the brain uses context trick to encode data? And he was right. I had no way. So finally, we decided to address this question after our paper was published. And then what we do did was the following. We tried to cluster the EEG segments, putting together segments generated by the same law, but not using the tree framework. Tree framework. Because in the tree framework, I only compare sons of the same father. Now I'm comparing pairs. Events don't have the same father. Okay, now look, if different contexts in the Samba tree have different laws, with this procedure, we would obtain eight clusters because you have eight leaves in the Samba tree. But we didn't. We only found four clusters. Four clusters. So the green one, the blue one, the red one, and the black one. So this is a synthesis of the 18 participants. In another, if you invite me for a longer period, I tell you how to do it, but it's in the paper. And then I put here the clusters, the first clusters. Now the question is: so in these clusters, for instance, I'm very happy with 2, 1, 1, 0, 2, 1, 1, 2, 1, because they correspond to the same leaf. I'm not that happy with 0 to 0 and 0 to 1 together. So in my previous analysis, what I found that 0 to 1 and what 1 should be together. Together, we found that two zero zero. So this is compatible, but I have extra information. Now, that question is how to interpret this result? Well, I put then again here. So I eliminated two strings. Zero, zero, zero, because it does not appear that much in the experiment. So, okay. Experiment. So, okay. And one zero one, I don't know why. So it turns out this appeal a lot, and something is strange. If you have ideas about it, please tell me. I don't know. But for the remaining one, I know what to say. The first cluster contains a string in which two, the strings, in which two appears two steps back. Equivalently, okay, eliminating the zero, zero. Equivalently, it contains the strings ending. Give a link leach contains the strings ending by the constitutive silent unit. Not bad. The second cluster contains the strings in which two appears one step back. Equivalently, it contains the strings ending in a position that can be occupied randomly by one or zero. Not bad. The third cluster contains the strings which two appear as a last symbol. Of course, with the exception of this one. Exception of this one, I don't know what to say about. And finally, the first class contains strings which two appears four steps back, with the exception of this one. But I have an alternative description. It contains all the strings and in a position that can be occupied randomly by one or zero. Okay. The partition for clusters suggests that instead of context. That instead of context tree, the brain could use an alternative approach to encode data. Namely, the encoding could quite well be done by identifying a basic fixed recurrent structure in which a strong bit always appears in the first position, a constitutive sound unit always appears in the third position, and in the second and fourth position appears either a zero, one. Appears either as 0, 1, choosing randomly IID way, which probably 0.2 and 0.8. As a matter of fact, to compress data, this model is better than the context tree. I have less parameters. Well, this is not contradictory with the context tree description. It only retrieves the extra information which is not provided by the context-tree approach. It was recently proposed by a very Recently proposed by a very bright PhD student, Fernando Nasman, in his PhD dissertation. Okay, that's the team who wrote the papers. So Alini and Gidyim were postdoc, well, PhD students, North Professors, sim professors. Noslin is a postdoc fellow. Antonio Najma is a PhD student. And these old people. These old people like me, and this was done inside Neromat, which is a long-term project, 11 years, supported by FAPESP, which is São Paulo State Research Foundation. And we have available postdoc fellowships and also PhD fellowships. So, if you have interest in students, send them to us. Thank you very much. Thank you very much. Awesome. Thanks, Antonio. That was a really interesting talk. I have a couple of questions for you. And yeah, the first of these is, it seems like your pruning procedure that you were using to try to find the context trees ends up being really sensitive to the noise level in your data. And that's because you basically keep going. And that's because you basically keep going so long as you cannot reject the null hypotheses. And if you're, you know, if your EEG data was in principle just like extremely noisy so that it was impossible to reject any of them, then it would just kind of keep going forever. Is that a problem in practice or is the data, yeah, go ahead. That's a problem. Well, every time we wanted to retrieve a tree, so yes, we need to control the error of the problem. Error of the probability of false positive and no-negative, but we did it. We did it. So this could be a problem. Actually, because EEG, as you know, is a terrible data. So during the experiment, all my PhD students, I'm a mathematician, they always spend two weeks in the laboratory, EEG lab, collecting data in order that they can face. Data in order that they can face the difficulties. No, but that's a that's it's it's a quite real problem. But since, well, in the data I showed to you, is the so I have the individual data. If you go to the article, we have the individual data, then you can evaluate the fluctuations. But for sure, this is a problem. In particular, because it's very difficult to do EEG data for a long time. So you people stayed 10 minutes more than this, it was difficult. But yes, it's a problem. It's a real problem. Thanks. Other questions for Antonio? Yeah, Tara, please go ahead. Hi, I was just wondering, so for the EEG data, when you're sampling from, you know, Sampling from you know your volunteers. Do you consider experience? Because something like Samba is obviously going to be relevant. If you're a musician, you're going to have more experience with that, which may change whether or not you have sort of then. They were all students of the university, but then if they have a previous training as musician or not, half of them have one. We didn't find any difference. We didn't find any difference. Let me tell you something which occurred to me in the beginning. So, the stimuli have been synthetized in order to avoid that during the experiment. So, synthetized, but in the beginning, I was not sure about what kind of experiment. So, besides the samples, I was also about then, I was not sure about it. I was with a student, I asked her, listen to this. What are you listening? Listen to this. What are you listening? Can you understand this? You say, Some say, Well, I don't know. Say, I'm not listening. And she said, Oh, you did, which was not what I did, but she was unable to reproduce, but she was able to identify the difference. Well, this is not a very complicated. So if you listen to the stimuli, then You listen to the stimuli, they are not that I called samba to make to give flavor to the thing. So it's just a quaternary thing, it's not as complicated. We did recently, but we did, we stopped due to the COVID epidemic. We did experiment with musical notes and tried to see. And we also did the experiment with We also did the experiment with behavior in a video game called the Goalkeeper Game, in which you asked the people to predict whether the penalty kicker would send a ball left, center, right. And we realized that it was much more difficult video gaming because people are anxious, people try, and the brain is a better statistician when you don't bother him. And I was reading these days again this wonderful book about the Zen Asher. I don't know if you know this book. It's a book written in the beginning of the 20th century by a German philosopher who went to Turkey, to teach philosophy, and who want absolutely to learn then. And then the teacher said, No, you cannot, the sedentars are not able. And then he insists so much that. And then he insists so much that someone said, well, go to learn the Japanese traditional arc technique. And then he describes it. And he describes the fact that you need not to worry about. So the brain does it. If you worry about, it's catastrophic. And so what we give to people is to say, well, don't sleep, don't move, and you stop after 10 minutes to do this coffee. Minutes to do the same coffee. Well, in our data, it did not show any difference. And I've already been reading some papers. So, if you are interested, I have been reading recently some papers, Pennsylvania in the PhD dissertation of my students, in which apparently, so I don't have the data, but I can't send you hyperlinks. All the other people ask this with respect to listening. Yes. Thank you. Thanks again, Antonio. Thank you. Thanks again, Antonio, for a really fun presentation. And we'll move on now to our