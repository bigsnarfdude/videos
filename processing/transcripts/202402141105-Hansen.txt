Private restrict data, despite the fact that the that the original data is sort of open and available to everybody. And that creation also certain policies is about how we're going to handle that restriction. So we have a preprint on the technical side, some funding. This work was really done by the two students, Fruz and Chris, Ciro and Sarah Wang helped a little bit at the beginning. My co-conspirator is Jeff Liegel up at the HOTS. And then for sort of some of these policy implications, and I'm going to talk. Of these policy implications, and I'll talk about at the end. We have written some of a policy paper that is not online right here, and then we've deeped up a two bioethic system paper to do that, which is pretty illuminating because GFNI knows a lot about logistics and genomics, but some of these privacy issues, policy issues and stuff, we don't really know that much about. Okay, so first a little bit of background. So genomics is somewhat unique in science, I would say, because there's a wide array of totally open data. Array of totally open data. That's really a big change. There's a lot of fields in science where data is sort of semi-private. You have to talk to the right people, you have to be unknown. But in genomics, you just go to a website, download, and everything is filled in, profiting. So we're working with RNA sequencing data, which is a gene expression data. And we're working with some data sources. So when you process RNA-seq data, you have to go through a rather long processing pipeline. Rather long processing pipeline. We have to make a lot of choices. All different stories make their own choices. Very hard to compare data. So, some years ago, we said, okay, well, if we want to analyze all this data, we should just take all of the public available data, about 740,000 samples, and just process it in the same way so we can make comparisons across it. Okay? And that's basically mostly a big computation. We decided sort of the right pipeline, we think, follow the right alignment pipeline. And it took sort of a fair amount of time. This is not a Took some fair amount of time. This is not a big computation for those of you who are used to sort of Jet GPT training, but this is like a really big computation when you're sort of a small lab like VR. So we're using data, basically all the open data in Soviet Archive, and then we have two big restricted access data, the Cancer Genome Atlas and GCHEC that we just heard a little bit about. So we process this data, we summarize it onto different levels, it's publicly available, and we and other people have used that. Available, and we and other people have used that to sort of do biology. Here's a PCA plot. This is sort of one of the things we get out of it. This is a PCA plot of like hundreds of thousands of samples in human. And well, I'm not really going to go into it, but because we do this uniformly processing, it's somewhat surprising that some of the first PCs are going to be jiscoty. Okay, so we're going to talk about genotypes. I'm just going to remind all that not geneticists, but genotypes are right. So we have the DNA, we have in the olosomes, we have two chromosomes. In the older somes, we have two chromosomes, two partial chromosomes. And if you take people, most people are only different from each other at a small set of locations for a single nucleotide polymorphism, which is a size and a genome that vary across individuals and populations. Because we have two chromosomes, we have sort of two alleles, and we can either be pink-pink, pink-blue, or blue-blue. That corresponds to sort of A, A, B, and D. We are only going to talk about bio. We are only going to talk about biolinic SNPs, so that's where we just have two things. Okay, so when you do DNA sequencing, you basically genotype all SNPs in an individual. That's sort of the goal standard. We're going to work with RNA sequencing data. And RNA sequencing data, we generate data on genes, which are sort of isolated occurrences along the genome. And we are going to be able to genotype SNPs that reside inside a gene that is expressed in the sample. Expressed in the sample. So when we do DNA, we can get the entire genome. With RNA, we can only get the genes that are expressed. So roughly speaking, 1% of the genome is inside a protein-coding gene, and roughly, 50% of genes in an organism are expressed in a given sample. So then later on, when we use this for something, there's going to be, in some sense, a lot of missing data. It's going to be sample-specific, what steps we can do. Okay, so because. Okay, so because we did this sort of big processing of recount 3, we throw a lot of data. It's like the input data to the processing we did is absolutely enormous. There's no way we can store it. So in our processing pipeline, we get rid of a lot of junk, a lot of stuff that we don't really want to keep around. And that stuff is pretty important if you want to use existing pipelines for genotyping analysting data, which is also really so. So we have to come up with a method that kind of adapts it. With a method that kind of adapted to the data we had available. We did something really simple. We basically have the RNA sequencing reads, then we have genes, and then we basically just count up for each position, how many reads do we have that covers the four different nucleotides? That's going to be some of our input. We are going to start off only genotyping samples on a fixed set of known SNPs that cuts in. Of known SNPs, so that cuts in that really helps with our sort of accuracy here because we're not going to genotype all places in the genome, we're just going to genotype places where we know there's variation and we know the two influence. So we know there could be four different nucleotides. We know that two of them are going to be sequencing errors and two of them are going to be the right ones. So we can structure our data. So we go through this set of node SNPs. We count up how many, in this case, orange and purple things we have. And then we form this like really simple. And then we form this like really simple uh uh uh representation. S here is basically going to be the coverage, so that's going to be uh how many reads do we have covering. So here we have five reads covering. And M here is going to be basically the log ratio between the amount of purple and orange we have. So really, really simple, right? And what we're going to see is that what we expect is that samples that are AB are sort of an in-value around zero, and samples that have that Zero and samples that have that are AA orange or AA purple, they're going to be either at the top or at the bottom. So the real data looks like this in a set of a set of test steps where we can see that our expectation holds pretty well. We make a really simple model. We just model kind of the mean and the variance. And then we basically use the map predictor. We get the posterior chance of being part of the three different groups. Three different groups. And well, this is like this takes like two seconds to compute learning. And we get these prediction boundaries. So now we can take a single sample and put it through and get sort of the inferred genotype sample. How well does it work, right? I mean, this is prediction, so all that matters in some sense, how well does it work. So we do that on a set of, we take T-Check, we train it on G-Check. T-Check has about 900 individuals. Each individual contributes somewhere between 20 and 50 tissues. Because it took these post-communications. Tissues because you took these post-mortem individuals, chopped them out into small pieces, and did RNAs as you are. So we take a set of holdout individuals and then we take some tissues there. And then we get something that looks pretty impressive, right? We have an accuracy of 99.5%. That sounds pretty good. But how good is 99.5%? So in order to compare that a little bit, we can compare this to sort of a made-up estimator. So we can say there's sort of two very simple estimators we could come up with. One is One is, I'm just going to output the reference genome. Every time a new sample comes in, I'm just going to output the reference genome. Okay, that is really stupid. Of course, you don't learn anything from it. But it actually works pretty well because the reference genome is going to have the major allele on a lot of sites, and therefore there's going to be a high chance it's going to be correct. So we're going to compare this to a slightly different thing that's a little bit more sophisticated, still kind of stupid. We're going to take the sample individuals. We know the population. We know the population, so we know whether I like the minor, but we know what allele is a minor and what allele is major. And when we genotype a SNP, we're not going to use the data from the sample, we're just going to predict the major allele. And this is nice, and this works pretty well because the amount of error, every individual who has a major allele is going to be correct, and every individual who has a minor allele is going to be wrong. So basically, the error is going to be bounded by the minor allele factor. So if you do that, So, if you do that, you get 95.7%, right? So, that's pretty good, even though this is really stupid. So, this tells us a little bit that when you read papers about genotyping, you really got to be careful about these very highly languages. But it's also, it hides a lot of stuff, right? Because this one here is high because of the composition of major and minor allele. So, we like to report our accuracy a little bit more sophisticated. We condition on whether or not we are predicting where the truth is the major and the minor allele. And in that case, The minor allele, and in that case, the major allele model here is always correct when we have the major allele, and it's always wrong when we have the minor allele. Our model does a little bit worse than perfect on the major allele, where our gain is that we are much, much better predicting the minor allele. But because the minor allele is in some sense a small proportion of the alleles we are predicting, you know, the difference in performance between 0% accuracy and 90% accuracy translates only to a few kind of percent. Translates only to a few kinds of the scent button of water. We can also do this in a true auto study sample. So we have other studies in SIA where we have genotype information available. So this is like completely different study, different processing collected like 10 years ago and so on. So this is like a true out-of-sample comparison. And we basically actually do better on the minor allele, we do a little bit worse on the major allele, but pretty much we have a comparable performance. So this is pretty nice. It really suggests that it. Nice. It really suggests that it generalizes well. We can also be worried a little bit about tissue. And for tissue, we go to our teacher exams, and they have each dot here as a tissue. And basically, there's relatively little performance differences from tissue to tissue. Actually, one of the tissues that does the worst on the main huge is the tissue that's being used here for the tube artists. So that's a little bit surprising. In some sense, we do better in the other sandwiches. Okay. Okay, we also want to come with some prediction uncertainty. And you know, it's sort of intuitive based on our sort of super simple model where convex really matters the most. So we have a little model for like, for accuracy, given convergence and minor allele frequency, which we can output to the user. Okay, so my first sort of little interloop, and I'm going to rank a little bit. I really think we need a much better and more sophisticated vocabulary when it comes to talking about. Carefully when it comes to talking about predict the accuracy, right? In some sense, prediction is really nice. You're just going to predict, you're just going to report the accuracy. But when I read papers, I have a really hard time, sometimes a really hard time, understanding how is this prediction accuracy estimated. Like does the test sample that it's used to estimate, does that really represent a realistic application of this in the real world? That's where a lot of prediction algorithms fail because they work on the test data when people develop this. Data when people develop the algorithm, but then they go out and use it in the wild, it doesn't work anymore because it's not really representative of the real world, right? And I think we need like a whole ontology and vocabulary, and I'm going to say straight up that this stuff we see most people doing, where they just take a huge data set, they split it up, perhaps at random, perhaps in a more supervised brain, you'll adjust in a training set and record and fit the model and then record it. That's not really a big, that's not really useful for most biologists. That's not really useful for most biomedical data sets where we see a lot of light distributional shifts when we move across data sources. And if your prediction model doesn't hold up on the distributional shifts, it just doesn't work. I mean, it's not like it just doesn't work in practice. Okay, so can we use this to stuff? I mean, we can use this for stuff. We have some ongoing work about where we use this to study this specific expression, but this is sort of in our preprint where we do like the easiest thing you could. Where we do like the easiest thing you could imagine, which is estimating the population composition of SIA. So we know that when we take DNA sequence data and we do a PCA, the PCA represent geographical or population composition. So here we take the 1,000 genomes DNA data that we've already somehow used in some talk, and then we do a PCA, right? So this has like about 1,000 samples, and in 1,000 genomes, they had five superpopulations indicated here. Indicated here, and we can see that the price of our separator corners can move on the basis. So, this was the experience. Now, now we're going to use a trick that I've used a couple of times that I think is really quite nice. So, when we do a PCA, we get a PCA, but we also get a projection operator. So we can get new data and we get projected into the same PCA states. So, now we take the Giovannis data, and this is the RNA-seq data, that we basically project. And projection is sort of a single sample operation, right? So, we take each sample, we can put it through, and we can put it on the same PCA. True, and we can put it on the same PCA plot as the thousand genome data. And you can see here, this here lines up really well. Geovarinas is a European project, so they have a lot of European populations. They have one on top of the European thing, but they're also genotype some individuals from Europa and Nigeria. They land right up in the African population in 1,000 genomes. So, this looks pretty good. Unfortunately, there's this issue with missing data, right? The whole genome data has nothing. The whole genome data has nothing missing, or what happens when we do when we have missing data? Here we have just taken the SIA samples and projected it down 1,000 genomes where we only retain around 65,000 samples, which has very little missing values in our genotypes. So, what happens with missing values? Well, you can simulate that a little bit here. We take the thousand genome data, project it into the cell, but we start by removing SNPs. SNPs. And what happens basically is that this piece of air can kind of linearly deform down to here. And you can see that also fits with what we see with single-cell RNA sequencing theta and voltage RNA sequence in theta. So using this, we can make sort of a pretty easy simple variant of k-means cluster, where we take account for how much we would expect these points to move according to the amount of missingness, and we can infer the superpopulation composition. That gives us the population composition here of the Volgar data and the signal. Composition here of the Point RA data and the single-cell RA data. The single-cell R data, you've got to be a little bit careful with that because a point in this distribution is a cell, so it's not an individual. But that's what we can do. Okay, that's all on the application. We're pretty excited about this because it seems to work pretty well. We are doing a little specific expression right now that I think this data is particularly well suited for. And our allele ratios look really, really good, surprisingly good, because, frankly, Surprisingly good because, frankly, this here is like so bloody stupid as you could imagine, right? This is like the project Jeff and I, when we set up coffee, we're like, this is a project where everything works. We do like the stupid simple thing with the most simple model, and it works pretty well. It's pretty amazing. Okay, so let's talk a little bit about the policy issues. What we have really done is we've taken open publicly available research data. It's not just data you scraped off the internet type, but sort of research data, and used it to infer genotypes without. And used it to infer genotypes without which is widely considered to be identifiable data. And how should this be had? We want to share this data. But so we just put it up on the internet. The original data we took is basically completely freely available with no restrictions. But clearly, there's something fizzy, I think, about taking the genotypes and just putting them all onto the internet. So there's a couple of policies that are supposed to handle this, that we can get some guidance from. There's a recently adopted data sharing policy, MNIH, that says. Data sharing policy from NIH, as research data can be shared unrestricted if the data is de-identified and pose very low risk. But as most policies from NIH, it doesn't actually define what it means by de-identified or very low risk. Like, you know, we don't think of DNA data as we think of genotypes as being, you know, identifiable data, but we don't have a link to the individual samples. We don't know what these individual samples are. We still have the. We still have the data. We're also not genotyping all stuff in the genome, we're just doing it in the code instance, right? So, how we handle this in genetics, right, is that in genetics we tend to use this repository that is supported by NIH called DBGAP, where we can deposit data as restricted access. So, restricted access means there's a data access commitment, and you can get access to the data by filing an Can get access to the data by filing an application. If you belong to an institution, typically, you describe what you're going to do with the data, and then the data access committee decides whether or not the intended use you have for the data aligns with what people have been consented to. So basically, this process may be handled privacy by saying there are some people who can get access to the data and some people who cannot get access to the data. There's a similar repository in Europe. So, when you share data through the open part of SA, which is what we've been using, right, you upload data and you click a little box when you upload it saying that your permissions do that. But it's a little bit unclear what the resources who uploaded data actually thought their permissions were right. You have inferred genotypes from the data. Have they really thought about that this was possible to do with INA-seq data? I mean, there's no doubt that there are samples in SIA that just come from even alive human beings and Human beings in North America who I've used with the podlip, and now there's a plot all the materials to be used and who probably haven't been fully consented on this. But this is just my belief. I don't know that for a fact, right? On top of that, if you want it to be restricted access data, we need a DAC and we need some data used in the JSONs, but we have no idea what these people but I mean there's 9,000 starters, there's 9,000 different consent forms, like we don't even have access to the consent forms, like We don't even have access to the consent forms, like what are they being used for? How are we going to handle this? So, this is a little bit complicated. So, we talked to Bedford and IH about it, and we decided that we will deposit in DPGF, and then we will have some kind of general access rules that hopefully Jeff and I are not going to be involved with because I'm not going to spend my time reviewing applications to access this data. But that's just the genotypes. There's also sort of the original input data. So, in some sense, we took the original data. So, in some sense, we took the original data, we did this out there. And this is pretty hard to do if an interview out there wanted to do this. Writing this through recount, processing these 400,000 human samples, that's not a small thing. But if you really want to, and you have a lot of money, you can do it. So should that be restricted? Should that be taken away? And here we've got to also think a little bit about the fact that these RNA-seq data are kind of widely used in genomics, and there's a lot of bits. Widely used in genomics, and there's a lot of benefit to the public and to the research enterprise by having them freely available. And we spent some time thinking about that, you know, perhaps we should be using this new concept called registered access that's been proposed by the Global Alliance for Genomics and Health. And here you're replacing kind of an application process with sort of a registration process where you say all researchers at an institution and a given department or a given sort of group of people just have access to the data. Of people just have access to the data. For example, you could imagine that all genetic counselors at Hopkins just have access to variant data when they diagnose patients. I don't think few people in this room would think that's a big problem. When you diagnose people with rare genetic diseases, you spend a lot of time looking at their genomes compared to what is in the pot. We need to think to see if these different variants could be causal for the syndrome they might have. Okay, uh the problem is of course this is not actually implemented right now. Actually, implemented right now. I'm also going to say some broader thoughts, and we're not really this sort of on my own plate, but all of this is about limiting data access to sort of a select group of researchers, but we really don't have any good process. It's sort of assuming that if you have access to the data, you have benign things and you're going to follow the rules and you're just going to do the things that are okay. But this is actually pretty tough to police, right? There's been a couple of situations here in the last five years where people have done. Last five years, where people have done what I think is a little bit suspicious with genetic data, and there's really no repercussions. The only hammer we have in the US is restricting funding. But if you're not NIH funded or you don't care, there's really no way to really bring the hammer down right now. And I think that's a vital problem. Okay, but that's not something we're going to solve. Okay, so that's what I wanted to say. Any questions? I'm just wondering, what's your perspective in terms of protecting genetics data in general? So now in Genexia, I can now tell the, for example, in a typical medicine data set or data set that you work with, what are the concerns? Like Matrix mentioned, membership attack. Attack, you don't want people to find out that your data is in the data set. I'm just wondering your yeah, I mean, so I think, okay, I think that I think this is very complicated genetic data. For example, I have many years ago, I sent in my data to Trinity University get testified. And now I have kids. And now I realize that because I have kids, I've shared a fair amount of their genetic data with this company, right? And they were not asked because they were not alive, but I mean, I need the assistant, right? But sharing data also impacts families, right? Impacts families, right? And we have these examples where police have gone through and used, you know, sort of ancestry data to identify perpetrators. And, you know, when somebody has done something good back, of course, we can't probably all support it. But what happens, you know, 50 years from now if somebody wants to do this with a very season twice. So there's some long-term things about just having the data outlook that are really hard to think about. It's really complicated, but I'm not sure what to have. Really complicated, and I'm not sure what that I've thought it through. I do think that some of the privacy attack papers we have read are rather academic in nature, right? I mean, so how often, so typically they require something like, I have the full genome for a person and I want to understand, was this person part of this particular story? Like, really, like, how hopefully somebody is going to do that, right? I mean, you can do it. I mean, of course, if it's a sensitive phenotype, say, you know, in the 80s. You know, in the 80s, we wanted to figure out if people had HIV, right? I mean, but still, it's really esoteric, right? But the fact that the data is out there, and you could imagine companies or governments using it to doing stuff in the long run, it's a little bit hard to take away what's up, what is out there, right? I mean, eventually Trend and GMB is probably gonna be brought by somebody else, and my data is gonna be handled. And my data is going to be handled by somebody else that I have no control over. I mean, I can't really take it back now, right? I mean, that's sort of a real concern, right? But on the ground, right, a lot of people are just following this sort of NIH or this sort of thing with like protecting yourself. This is a little bit like, if we have genetic data, we're going to put it behind a portal demeanor, right? Without really having a good discussion about the trade-offs between nefarious use and non-nefarious use. And non-nefers, which this is complicated, right? Like, you know, the policy that I coded doesn't really say, doesn't really make a difference between do I have the whole genome or do I have three SNPs? Like, tree SNPs are probably not, I don't think anybody would have a quick talk with Searing Tree SNPs, right? Searing the whole genome is a little bit different, right? And I also don't think that this entire solar structure with like projecting it and then say there's only some people who have access, I mean, there's no real consequence that will follow up right now. There's no real consequence that will follow up, right? I mean, when people are doing stuff with it, like, you know, it's really hard. I mean, if I choke something and there's something crazy with genetic danger, I mean, there would be a few consequences, right? But not that much, I think. Yeah, I appreciated how you pointed out that NIH doesn't define very important terms like de-identified and very low risk. I'm wondering if you said that to them when you were working with them and how they responded if you did. Working with them and how they responded if you did. Well, I mean, they also want to like cover that a little bit, right? I mean, so there's a little bit like they also want to be a little bit big because what if you know some researchers 10 years down the line figure out that you can do something with it? You know, there's a lot of, you can basically say, so a lot of what they do is like, don't share it if it doesn't fall under these categories, but we are not going to tell you if the data you have. So, for example, it's very unclear right now if INA-seq data. Unclear right now if INA-seq data is considered protected data and not protected data. Somewhat unclear. Perhaps a little bit less unclear after we've done this work here, but I mean a lot of people are treating it as undiscriminated data. There's no clear policy implications, right? So it's a lot like the NIH tells you some very vague guidelines and then you have to make some decisions. And those people are not really quite ready. I mean you're finishing up your paper, you're uploading the data to some archive just to make a decision, right? I mean that's not the time. To make a decision, right? I mean, that's not the time to think about these complicated things. But I mean, they also don't want to say too much because they don't want to get some, they also don't want to get in trouble, right? So. But that's, well, that's not my opinion, not a question, but that's their job, right? Like, the reason people, people would be able to do it if they actually told people what to do, right? Because you're right, people at the end of the day, when they're finishing their study, don't have time to think about that. But that's what the NH should be telling them. That's why they set policy. Yeah. Policy, yeah. I mean, uh, sure, I mean, but you know, we've talked to some people who are involved in this stuff, but I mean, you know, some of these policy things get decided at a higher level, even than the data gets quite. So, I'm just a bit curious, because I'm not familiar with the policies, like, so is it just genomic data that is got to be put behind the, you know, the behind the login? And maybe it's, are they just behind times and understanding that we can do a lot? Understanding that, like, we can do a lot, like a lot of exactly what you demonstrated with RNA-seq data now. Like, I guess RNA-seq data used to be like smaller bits and wasn't easier to put together? Well, I mean, some of these policies apply to all biomedical research. I mean, some of the policies are set by the HHS, right, and apply to all biomedical research. In the U.S., I mean, that's the other problem, right? I mean, different countries have different rules, right? I mean, I'm from Denmark. Like, if you're saying you're using for research, you can do anything. It's like totally crazy what you can do with genetic data. Do it genetically. I mean, I think. I mean, I'm a little bit shocked by it. I mean, but as long as you say I'm a researcher at university, you're basically allowed to do whatever you want to do. And yeah, I mean, except this applies to all research data, right? But I mean, if you have some research data, what does it mean that there's like low risk? And do you think that what does it mean for people to be identified using it? I mean, that's a little unclear sometimes. Sometimes like following up on your response to I guess the academic threat model, privacy threat model, I guess I couldn't follow the argument. On one hand, you said academic model is because it's special can't participate. And you seem to argue that the fact that they're individual, you know, data available means that there's some. Available meet that there's some risk of individual targeting. Where do you see the actual well? What I'm saying is that, for example, I think you can do stuff, but if you put genomes out in the wild, you can do stuff with it, right? But sometimes some of the attacks are assuming that the attacker already had the genome of the person who they're interested in, and they want to assign, say, disease label to that person by seeing if they were part of the study. So I think if you already have the genome of the person, then Person, then you know, you don't really need to protect the genome anymore, right? Because that kind of assumes the genome is in the hand of the attacker, right? I mean, but you don't necessarily have, you don't necessarily know that they have ADHD, right? And then, I mean, but yeah. Is there a better idea? I'm not saying there's a better side. I think it's really complicated, right? I think it's really complicated, and I don't think the guidelines are that great. But there's also problems with making guidelines, right? And there's some of these things about once. And there's some of these things about once the data is available, you know, it's hard to retroactually close down the data. I mean, that has happened. So, some of the summary data that we have heard about that used to be complete open access, and then people figured out you could attach to these things, and then suddenly the data put it all behind sort of projected, restricted access sort of, right? Which also has some consequences. Thank you all for the great talk. And next about so we uh discuss the um the uh issues related to uh privacy protection. So how would you have one group just for that discussion? So uh of course you can see that uh privacy protection nowadays is incrementally critical, especially with all the data. Especially with all the data collected, stored in a digital form. And there are lots of issues with privacy protections. One of such is related to policies, kind of policy impose to protect that. And there are also technology solutions. You have seen the presentation, you know, researcher talk about federal learning approach. There's a data identification approach. Approach and the encryption approach, and also something related to differential privacy standards. So, what are the challenges in this area? So, that is something we should talk about it. Of course, that is heavily related to the policies in place for privacy protection. But, of course, from our perspective, we got to provide some kind of technology solution if we could. So anyone can help me to take a note so we can begin our debate and discussion on this topic. Okay, great. Anyone has any comments? Any things you see from your applications? Actually, I've been struggling to, so for example, devirtual privacy to us is just injecting noise, right? Just injecting noise, right? It's just as simple as that. And as a statistician, our intuitive reaction to that is just I'm noise, then I'm trying to get rid of the noise and then do a bias correction. But then the confidence interval, so the uncertainty, if again, statisticians are interested in doing statistical inference about the population value. So that means we have to do the bias correction, but then because of this bias correction process, then Then you still have to take into account that kind of randomness of the injecting noise. And then what happens at the end of the day is that company symbol is so wide. So this is something that has been bothering me for a long time. We have tried all kinds of methods and it seems that for finance sample size, none of the work methods that I have tried actually works well in the sense that you still get similar inference results compared to the one without To the one without DP protection. So I'm just wondering what other people are. Let me give you some partial answer to that question. You have seen this generative AI has impact on everything. I probably should say something. This has a potential to have a huge impact in privacy protection. For example, instead of releasing the whole genome of the human, Genome out to a human, I can actually release synthetic data, try to replicate the distribution of the real data, and that can be shared. And that synthesic data will impose less risk concern, especially for the things that you mentioned. But they are still risk associated with synthetic data. First of all, there is a representative attack. You can try to look at my synthetic data. So, can my sincere data try to recover the true original data? So, but overall, it has less risk of concern compared to the original data. On the other hand, when you're doing this, you try to replicate the original distribution. So, in a sense, the one you have, if you use that to do downstream analysis, you're supposed to deliver the similar result. In terms of inference, in terms of estimation, whatever you have. Yeah, supposedly, but yeah, so I kind of want to pull back on that. Like the consensus in the privacy subfield is exactly the opposite of that. Like synthetic data is not privacy preserving at all unless you improve the purpose. But since then, there can be privacy preserved. It can satisfy the trends of privacy standards. But we don't know how to do it. For example, For example, if you use diffusion model, you add the noise to stochastic gradient between stochastic distance, you're going to show that the satisfied standard doesn't know without satisfying different standard privacy standard. So the satisfied DP in the sense. No, they have to use some way to, it's like adding noise to the gradient. It's like adding noise to the screen, but in certain ways it's different than adding noise to a reading. But you have to click, you need to truck it. Sure, sure. There are something technical issues similar to that one. So it doesn't work, right? This doesn't work. But this is just trying to protect the model so that try to address the reverse engineer attack. Because for really, there are so many attacks, okay, you can do the membership tech, and this is. The membership tech and this or that, but that's just mainly reverse engineering type. So, in this sense, it poses less risk concern compared to the original data. But then you have to repeat. I don't think any differentially private researcher will agree. Sure, but that's what they don't agree is but this is a this is a this is a topic. This is a topic debatable, but this is something may have potential. This requires further study for sure. So, this has a potential to address some of the concerns. So, I think the consensus is that it's a very dangerous thing. Like, just saying, well, it helps. No, no, this is something we don't quite understand, but something you certainly need to investigate more for sure. Yeah. Yeah. What is the concern of uh Keepra tracking the uh piracy? So you can essentially regenerate the training data and know it's the training data. Essentially there's no if you try to perfect your generating micro I don't care how you generate. That would make your a task much harder. Well again it's like separated by obfuscation, which is like the worst practice in security, but Worst practice insecurities. But that compares to you. It will come back to byte. Yeah, you can say I added noise to a regular data. I have issues with accuracy. There's a trade-off in accuracy and also the protection you have. But doing this, at least I don't have that concern. Everyone? And you can see, I have the same issues as the real data. As the real data. But on top of that, for example, if you think about this, it's not real data anyway. So I release that. So if I don't release any, associate any real data in the obvious way, that will be better than releasing the real data, right? Um put it this way, it's relative speaking. So this is not this is not perfect, okay? But compared to the real data, But compared to the real data, it has a less risk. As long as you don't claim that it's private, then that's something else. But yeah, it's fair to say it has a less risk concern. I think there's a simple issue with synthetic data. Even if you can guarantee it to preserve differential privacy, on the accuracy side, basically, if you give the data, if you give them synthetic data, if that You give them synthetic data instead of real data, and you do whatever machine learning you run on it, like the accuracy or the performance of their algorithms is limited by the inductive bias of your synthetic data generator, which may be how well you generate the data is another issue. But that, you know, if you have a better technology, you can do just that issue. And that's actually also my comment for here. So, in general, so the set is a matter. Of success is a matter of focus, not a matter of effort. So, the question for here is: are we going to make the horse running faster and faster? Or we need to think of the directions to make the engine. So, in the traditional approach, kind of continued to improve the traditional kind of differential privacy. I think data works, probably in the small, sample-sized situations, but in the with the data more and more, I will rethink about, like, for example, use a generative AI, directly understand the distribution. Directly understand the distributions of the direct understand the distribution of the data. That always means that right on the engines, or we have some people to think about interpreting. That's also another question, whether dandemic data is really understanding the. I really, I can make him a density expert. But that's a proven success in the image generation inviting the areas. And image generation contains about even all the cases with highly correlated data. And we can handle. The data and with a handle should it generate full images, but it may not be representative for the underlying distribution. That's, I mean, generally samples from the high density region is very different thing from the quality of the generation. That's another issue. This is a technical aspect. Just this I can allow the generate uh uh generated model so everything to be screwed up. A better one are we doing better. A better one, probably doing better. So that's that's always the issue, that's a technique issue. But from the way the area will move on, this is a potential, it has some potential, may have some impact. This is something we do not consider. This is not the very privacy or the privacy protection area we have been working for a long time, but you still have issues with respect to data sharing. So that's heavily protected. So if that's heavily protected, then you should be able to give it a very formatical research because you can see that you have to put this in a highly security environment. And so that data access is limited by researchers. Yeah, I mean, I think we've seen the issue recently in the privacy side of like kind of like mixing up the distribution or average gate and and adversarial setups. and adversarial setups, which is what you have to care about privacy. Because there was this work like trying to do privacy based on learning theory and principle. It got like completely broken really fast because the proofs were like some kind of like regret style proofs. But then in privacy you care about an adversary and it's not quite the same thing. So I think that's why I was trying to argue that it's important to be careful in the statements. To be careful in the statements you make about the anti-privacy thing. As long as it's clear that it's a heuristic that essentially may completely fail in ways that you don't predict against an adversary, then... That's for the research, for the future research. Let me just say from the direction kind of perspective, I think that's something for looking at. But yeah, I mean, I work on private generated data. On like private generated data. And I agree that if we can solve it, it's amazing. As long as we don't think it's solved yet. Of course, now I'll talk about why we won't talk about it. So this is just something for future consideration. I do think we can address some of the concern you have. That can be accomplished. Of course, that's for the future research. But I just want to tell you that and that you realize that this thing has a potential. That this thing has a potential. The recent call since Saturday Generation and AI. Yeah. I've got to tell you that. It's a product recent. A couple of proposals for that. Certainly, I know researchers, we are all right to choose which direction to go. If I invest my time, I would rather to think about, I know the engines kind of development is still in a very individual stage, but I would rather to put my time, put invest better engines and the new engines, instead of putting my time to hold kind of basic horse. To both have basically the horse very fast. Let me phrase it in this way. There's basically two potential paradigms. You either design algorithms in an ad hoc way. You have a task and you solve this task, biocarranty and differential biofuels. Or you have a unified solution. You have a way of generating synthetic data that pregurs. Synthetic data that preserves differential privacy. And you can release the synthetic data, and the user can do anything they want. Apparently, the second approach looks like amazing, right? But on the other hand, it is kind of limited by it's too good to be true if you have a synthetic data that works for any potential task. Task. That means you are learning the entire underlying distribution correctly. That may be too good to be true. And then you have to be biased. So that certainly will have a bias, but it depends on application. I can apply a specific, for example, genome data can generate this second one and do HIV study. If you know HQ already, that what you want to do with the data. Well, because you see. Well, because that's a huge area. Because I'm doing the infectious disease, I'm doing something else. So I certainly would not you can't do universal analysis. But that could be a tough thing to do. But for the specific applications, you certainly have. One of the questions I like, because someone is not an expert in this area, you have grounding principles that's a huge. Grounding principles that seemed thoughts like the definition of conventional privacy, make good progress on some of these foundational ideas. I was also impressed by they seem to be well motivated by specific applications quite different technical projects. Actually, you had to work with someone who has worked with several hospitals and federated learning or So I thought each of these broad collaborations added a lot of nuance to what I see foundations of potential effects. Maybe the thing that I'm still unsure and maybe I didn't quite see how some progress in one of those applications could translate. Like here to you whether anyone transferable problems between It's I mean I think we've hit the nail on the head like it's you have to develop specific applications because you're solving different problems, right? Like I mean you just you just said the same thing. Like there's no such thing as a general privacy solution that solves all problems. Like that just doesn't exist. And so you have to create specific methods for specific statistics for specific outputs that you're trying to create. Specific outputs that you're trying to create. And even the concept of what privacy means, right, is going to be different. Like, multiple people talked about the difference between, like, are you protecting individuals in the data? Are you protecting groups in the data? Like, what are you protecting? Are you protecting the identification of people? Are you protecting different attributes? Like, there are many different definitions of privacy as well. So, you're not going to just have one thing that covers all of that. Covered part of that. Even differential privacy, right? There's like 20 different definitions of differential privacy. Or other actually connections issues I think there are connections. Yeah, I think there's Yeah, like there's a general use. Not an answering part, but I guess the lingo within the field for folks who work on RAPI, particularly both, actually both apply and theoretical work is to do a very specific model. It specifies exactly what environment you're in, exactly what the attacker can do, exactly what you want to protect. Many of these definitions that you saw earlier are sort of instantiations of a particular threat model that people think of. French privacy, for instance, is a type of the attacker that's maybe all knowing outside of one data point alongside the Python data point. And if you're in a different setting, you're out of the threat along with maybe more. So I guess some of the design work for translating theory practice comes down to some things. Some thinking work on specifically what threat modeling on techniques trying to come up with a fencing that might work well for that threat model. They generally don't have guarantees for the threat model. Something, if an attacker is more properly anticipated, there is some like uh I guess you know, I guess academic uh literature Literature that like fully fledges out the movies. So there is some, I guess, translation between here, but there's a really good paper I can give you that does a really good job of breaking down some of the differences in the way people approach this problem because sometimes those can be a bit obscure, right? So like one really big one that sometimes gets lost in the conversation is differential privacy is fundamentally thinking about relative risks. Fundamentally, thinking about relative risks and what people might call classical statistical disclosure control is thinking about absolute risks. So they're actually fundamentally thinking about different types of risks to the people in the data. And there are other nuances between the way people are approaching this problem that sometimes get lost. But it's really helpful to get that. So I can give you that paper quick. Yeah, we possibly have to share it with that book. Any other comments, questions? Maybe I can jump in one comments. To discuss the brainstormings, I was thinking about what the future looks like for the data sharing. And there are a lot of technologies, I think, that they can put a piece together. So, one part is kind of related. The data sharing of the automated trust issues. How can I better use decentralized solutions to sharing the data? The solutions to sharing the data. And also, when the data is updated, kind of think about generating AI. How can we automatically? There are no common interactions updating. There are already some of the technology could potentially happen. For example, in the blockchain technologies, and right now for each of the person they're sharing the data, I can put my digital, kind of individual digital signatures on top of this data. After putting the chat, nobody else can look at this. And after this, also there's kind of a really certain moment. There's kind of a really certain moment, and they can use the smart contract technologies to update the kind of distributions. And if the people query this and get the synthetic data, I feel like that's a very elegant solution. If they have 10 years from the day, 20 years from the day, no humans can behave this and nobody has a high cost influence. That's why I feel like what the futures could potentially look like. Sure, that would be exciting. So, solve that problem. That'd be great. I think one of the biggest issues we have is that for scientific research we have limited access to many type of data. And that becomes the privacy concern. And the data are distributed only for access for other parties. So it's expensive to it's not easy to generate data, but once you generate data and you have But once you've generated data and you have enough data, you're better to make a good use of it. So that the privacy issue must be addressed so that they can have a better way to share. That's going to improve everything. This certainly requires a general API or this new technology probably will help in this area, but certainly we need to explore it. Just yeah, just one other point I want to make. Yeah, just one other point I wanted to make because I think is a really important statistical concept for everyone to think about who wants to work on this topic, which is, and this goes back to something he wrote at the very beginning, there's kind of two different ways that people think about this. One, people think about obscuring the model, right? The downside to that is if you want to do statistical inference, not knowing what was done to the data means you can't do it, right? So like you're basically some bias that you just don't know how it was generated. That you just don't know how it was generated. You don't know the model. So if you don't know how the noise was added, that's an assumption. Conversely, the methods that allow you to quantify the noise end up giving you a lot of noise. So basically, what you end up with is some kind of bias-variance trade-off, where you either have some bias that you can't measure, or you have these huge confidence intervals. And that's kind of where we are with the methods that we have right now. And obviously, neither of those is that desirable. But it's not like one or the other. It's not like one or the other is perfect. You either kind of have this unknown bias that you can't measure or this huge variance. Yeah, but for the typical way of injecting noise to protect data, you know that there is a trade-off between the accuracy and the demand of protection you have. But if you use another kind of approach, of course, the generation accuracy is met as well. If that accuracy is Matters a lot. If that accuracy is high, then the bias may be smaller, and in that situation, whatever the synthetic degenerated would roughly replicate the real data in terms of what it is, right? You don't know the biology. You can estimate it. There are ways to do that. Well, I think it's contradiction. So for a research field, For research field, I feel like we should be able to, the privacy shouldn't be, we don't identify the individual phone address, phone number, et cetera. We only have the DNA, boundary, etc. And that's for the development of new medicines to help human beings. But the privacy part should be in cost. They can usually collect persons well. Collect persons blood and can go sequence, can easily get this result. And people cannot say, I want to protect my privacy, you can't test my blood because they go to hospital, I want to get cured of their disease. So the privacy side should put on the hospital side or on the insurance company side rather than on the research side. But that is bounded by the policy. Policy, because the patients will sign the confidential agreement. So whether or not your information will be released for research purpose. So whatever the thing we do, we probably find it by that. So if you want to change it, then you have to go back to send another agreement with the other patients. But that is a touchy issue, I suppose. The research side, we are trying to better serve human beings. And on the hospital or insurance company side, we are trying to certainly help patients, but they can optimize their profit by knowing patients. Yeah, but the privacy actually is protected by law. So even though the insurance company has access, but the question is whether I'm going to release for data sharing for other purposes, for research, for other things. That happens pretty tricky. Yeah, it's abounded by the law, so you cannot, yeah, so unless you address a publication. Yeah, so unless you address public issues, otherwise okay you don't won't have the uh access as their power. Yeah, but I mean what you say is that it's not clear what the limit what the limitation the hospital wants to do is not limited. I mean and what's the let's say the author get called by an insurance company or some other company. It's not clear that there's any restrictions on what they can do with the point. I mean in the US we basically have all Pitch you get almost no restrictions on driving the hill thing as long, I mean, as long as you go to the airplane. Any other comments? How much should I hear anyway? So we can continue our discussion. This is the end of this discussion, I guess, if you really want to get into this topic. But nevertheless, thank everyone for participating. Thank you very much.  