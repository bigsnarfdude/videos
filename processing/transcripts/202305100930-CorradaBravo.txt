Take us off maybe two and a half years ago, and I'll show you some of the work that we've been doing for the last year and a half or so. So, but a few months after we got started there. I don't think I need this, but I always got a kick of people saying disclaimers before, so I just added it just because. But anyway, the important point is to place this project in context, and I'll start really at the beginning. This is a This is Sid Brenner, Nobel Prize-winning biologist, won the Nobel Prize for the genetics of apoptosis, like intrinsic cell death and animal development, like the most reductionist genetic research you can imagine. Knock out a gene, see what it does to the worm. Sometimes you get dumpy worms when you knock out the right genes, right? So then won the Nobel Prize in 2002. And like every Nobel Prize winner, oh, actually, this is a fun picture. This is in a Silomar meeting for recombinant. In a Silomar meeting for recombinant DNA, important meeting for biotech industry. I'll quiz the Genentech folk later as to why that's important. That's it, making notes. So why we want to do that. All right. So as every noble laureate, he gets to give a lecture, right? It's available there. And he gives a talk about his years and years of genetic research and everything he had to do to do that. But pretty nice also, he sets out what. He sets out what, you know, if he were to start his research career at that time, what would he be working on? And one of the things that he says in 2002 is that he would create a self-map. And he makes a little comment along the way that at that time, so genocentric modern biology has become that we forgot that the real units of function and structure are cells and not genes. Our cells and not genes, right? So, you know, go out and map cells everywhere. And again, somebody receiving the price for the most reductionist genetics research at the time said, yeah, maybe that's what we should be doing. Now, it took a while for technology to catch up to the ideal, right? It was only about 15 years later or so in 2017 that the idea of a human cell atlas technologically became more feasible. Technologically became more feasible. That was five years ago. So we're at the time where we think: well, great, we can do this. Why would we want to do this? And at least for some of us, the important bit of it is that we think as we want to find more efficacious and more safe therapeutics, understanding the basic units of function, cells, will allow us to do that a lot better. Allow us to do that a lot better. So, for example, in neurodegenerative disease, if we can understand what renders some neurons resistant or not to toxicities in the brain, well, that would be a helpful piece of knowledge to have. In the tumor microenvironment, if we understand the complexity of immune cell landscape and how that relates to tumors in the microenvironment, that would be super helpful to understand. Helpful to understand. If we try to understand how pathogens and cells that are involved in the epithelial barrier, both immune and epithelial, that would tell us possible ways of targeting immune disease or infectious disease. So, and that's well understood. I think a lot of our experiments now involve a pretty substantial amount of single cell. Amount of single cell expression analysis, or at least data generation. Our question is: how do we use, now that we're creating these maps, large-scale genomic data to understand the cell states and their organization? And that's the goal of our project. And I would like to give a little bit of a framework as to how we're thinking about this. It's not very different than, I think, how I think about knowledge in About knowledge in our own experience. We are definitely at the infancy state here. We're just little babies. We're just collecting and curating and harmonizing data sets. I think we're now moving into our little toddler years where we are starting to find or think of ways of finding appropriate data sets in useful and meaningful ways. And that goes along with a little bit more maturity. Along with a little bit more maturity, where we synthesize what we're seeing in terms of representing cellular state. Ultimately, we want to make predictions as to what a chemical or genetic perturbation might do to a cellular state. I won't say anything about that at all, partly because a lot of this is observational data, and who knows how much of this kind of perturbation or prediction-based workflows we can do with this kind of data. At that, with this kind of data, but eventually, and that we're working on this now, combining this kind of observational data with more direct perturbation data from which we can start understanding some of the effects of those perturbations, I think will get us there. But for the moment, let's concentrate on the observational regime for now and talk about what we do for as a baby on the observation side. So, all of this that I'll Um so all of this that I'll yes any resemblances all right um so uh everything that I'll show is is uh stuff that's behind a system that our scientists are using. We've seen spinning that up in the last year or so. Um, it is It is, you know, by design, we collect a lot of public data. So, if you look at what the human cell atlas has, we have it. So, we're going to have more than the human cell atlas and the broad single cell port. I'll explain how we do that. And, you know, people use it. There's the vagaries of seasonality or so on. But on a given month, about 300 or so are scientists will be looking for stuff here. So, how do they use it? How do they use it? You know, there was a discussion on the panel about, you know, in industry, we kind of have a tendency towards decision making. So if we can use external data to make quick decisions about potential hypotheses, that's great. So one of the ways our scientists use it is to find publicly available data sets, prosecute specific hypotheses, a preliminary step, and then decide what else you may need to do an experiment for. Do an experiment for, or should you pursue something else? Other ways that is being used, a lot of single-cell analysis, because we don't really observe individual, like we don't know exactly what each cell is, we kind of have to reconstruct cell identity. All right. Many of analysis are comparative. So you have a reference data set of, say, colon epithelial. Say colon epithelial cells, or colon tissue in general. And you want to understand how, after a certain chemical perturbation or under a certain experimental condition, that relates to the normal tissue, we usually have a lot of kind of reference atlases for each of these disease areas. And our scientists kind of find data sets, integrate them, and use them for their own experiments. And of course, we want to. We want to do that kind of comparison analysis. So, there's a lot of workflows that involve this kind of mapping between reference data sets and specific experiments. And that's another use that are being very commonly used. And something that we're starting to do a lot more often is meta-analysis of cell states across tissue and disease contexts. Find a particular cell state of interest that you might see across different tissue and disease, and try to understand what's the And try to understand what's distinct or common across these contexts for those cell states. So, I'll show you a little bit of how we are enabling some of these work. So, I'll tell you a little bit of how we get data in. The first, we have ingestion and curation. We have a few sources of data. Every once in a while, we go and scrape the gene. We go and scrape the gene expression omnibus and just grab everything that we think is of interest. We do some amount of curation to say, you know, what tissue, what disease context it might be coming from. We use cell by gene by the Shell Zuckerberg, or Chen Zuckerberg Institute, which they publish lots of data from large consortia, which has some amount of curation there. We also have that. But also, many of our scientists see a paper that may not be in either of those. In either of those, so we have a place where people say, Well, this is a good data set. We send that out to curators, but we come back with some useful data. So, put that all together, we have about 130 million cells worth of data. And the first thing we try to do is like our scientists would like to operate on that, is that we put it on the same kind of structure. So, I call that structural data set harmonization. And if you're You know, familiar with a lot of the kind of genomics, Bioconductor, and other frameworks, we try to place everything in this kind of three-table construct where we have a matrix of cells and genes, cells and genes, along with information about each cell. And this is where, for example, annotated cell types, or is it coming from a sample of a particular disease, is captured. So we put everything in that structure, regardless of where it's coming from. That structure, regardless of where it's coming from. And also, there's semantic data setter harmonization where metadata for each of these types is placed into certain attributes of those metadata is placed into standard terms that are part of an ontology. So, for example, cell types are placed within the cell ontology, et cetera, et cetera. Much was like Robert was discussing. So, with that, how do we then? How do we then use this to find data sets? We have harmonized cell type, tissues, disease, experiment types, organisms. We represent every data set based on these metadata attributes and then map each of these metadata attributes to an appropriate ontology so that data sets that have been annotated to contain a particular cell type we can trace down into whichever level of the ontology. Whichever level of the ontology we might care about. For example, if we're interested in finding all myeloid cells using the ontology plus our data set graph, we can find any data set that has annotated for any myeloid cell. So I won't show very much more about the engineering aspects of it, but we do have a pretty substantial AWS usage for storing our data sets and indexing those data sets based on that. Indexing those data sets based on that metadata. And we're very careful about how we duplicate information or not in each of these things. So with that, we let our scientists find data sets. So for example, you can search for a myeloid cell that gives you data sets that contain myeloid cells. You can find it in a dimensionality reduction plot of that cell sum. Take that big matrix. So, take that big matrix, reduce it in dimensions, and it allows you to kind of take a look at that data set. We also are able to use this for certain spatial data sets. So this is, for example, cell types organized in a particular tissue slide. We can also show the expression of a specific gene, facial configuration. Specific gene visual configuration. We don't see that because my GIFs are not working, but we can do that. So, for example, we could look at the expression of estrogen receptor or something like that in this spatially. We also have some chromatin accessibility data. So also we're able to show chromatin accessibility profiles in specific genomic regions on the same application so that you can see cell type-specific. Cell type-specific candidate regulatory elements for each specific sample of interest. And all of the engineering behind this is in different levels of open sourcing, but most of it is available. It's built on a lot of our previous work. And finally, we kind of instantiate a view of, if you want to search, you find an overview of data sets and allow you to kind of drill down. And allow you to kind of drill down. We then allow you to go from a set of experiments to a visualization tool that allows you to do some more deeper analysis and so on. And that's something that Aaron and Jay have published. Great. So that's all the engineering stuff. But I wanted to spend a little bit more time on the recall and synthesis, right? Because we want to. Synthesis, right? Because we want to kind of find appropriate data sets in useful, meaningful ways, and that means kind of finding cellular state. At least we interpret that to mean finding cellular state in context. So I'll give you two examples, one simpler, one more, slightly more involved. So, one of the things that we found immediately that we thought would be interesting is the case where. Interesting is the case where a scientist would perform an experiment, compare a particular disease context in, say, a mouse model, identify a signature of genes that they think might be distinguishing a particular disease state, and ask, have we seen this disease state or this cell state elsewhere? All right, so that might be one of the things that we want to identify, our data sets in which a particular cell state, as defined shortly. Shortly, it is present. So instead of that, I'll show you a more kind of delightfully uninteresting example of amyloid beta clearance. So we also have a particular pathway. So a scientist might say, okay, can we find data sets in which the pathway for the positive regulation myeloid beta clearance is present? That's essentially a gene set including TRIM2, APOE, LARP1. Can we find data? Can we find data sets where we think this pathway is active? And I'll show you kind of how that works by example. This is a survey of the developmental survey of the human prefrontal cortex. We do know that microglia and astrocytes are cells involved in the breakdown and clearance of. The breakdown and clearance of a beta, microglia through phytocytosis, astrocytes by clearing into the vascular system. So if everything works well, we would hope to find microglian astrocytes in these data sets. So we could ask, well, we know trend 2 is involved in this. Where do we see expression of trend 2? We would see there microglia. We could ask the same one for LARP1 and APOE. We could see expression on those two. APOE, we could see expression on those two. All of this would visualize in the portal button. That's not quite useful. So we need to define some way of identifying those populations of cells across our 130 million cells. So we want to do this in a query form of this. So we want to define a scalar function per cell defined by a query here being just a list. A query here being just a list of genes. All of this data is particularly noisy, so I don't trust high expression of a particular pathway in a single cell. So we want to discover cell sets that are enriched for high function values. We want to control discovery rate across cell sets and then return those states. And we want to do all of this quickly. So, how do we get there? So, we aim for simplicity. We want to identify We want to identify high expression of these genes. So we pre-compute a cell by gene background expression distribution. So every time a new data set comes in, we pre-process its signatures so that per cell and per gene, we have a measure of normalized expression. We want to compute a function per cell set. We do that by taking the average of those z-scores. That gives us a distribution. That gives us a distribution over cell sets. And we can then use some enrichment method for detecting those that are enriched for high signal. We want simplicity for speed. So Rafa and Terry are masters of simplicity and speed. So we borrow some ideas from that to find those cell sets that are enriched. We do multiple testing correction eventually, but the point is. Eventually, but the point is, once you want to say, I want to find data sets where we think there are cell sets positively enriched for the regulation amyloid beta clearance, you can do that. We'll query our pre-processed data set, compute cell set statistics on the fly, and report those that we think are enriched for that. Right, and then from that point, you can. And then from that point, you can do everything else as if you had searched for a particular disease. So instead of just metadata querying, now we query by properties of the transcriptome itself. Now, if you noticed, my definition of cell states were cell types, which is not great because when we're looking for more finely defined cell states, the cell type is a fairly broad or potentially fairly coarse. Fairly broad or potentially fairly coarse definition. So, cell type labels as cell sets are really not quite that helpful. So, as an example, suppose this is a low-dimensional representation of our cell population. These colors represent our a priori defined cell states, and what we were looking for are enriched sets of high signal. And the way I just described it, we would be able to find a set like this where we have. Set like this, where we have fairly global enrichment of pretty much a lot of cells in that set, we would miss things like this, where we might have multiple populations within the cell type that show enrichment for that particular signal, but it's not captured in the set that's given by that cell type. So, some of the work that we've been looking at is more topological enrichment, less just Topological enrichment. That's just a fancy word for saying we use graphs as a representation of the data set. Instead of a low-dimensional representation of the data, we measure distances, find neighbors, and then construct a graph that says two cells are connected in this graph if they share very high degree of neighbors in the original data set. All right, so then instead of defining these per cell type, we essentially define these cell neighborhoods a priori. We borrow an idea from John Mariani's group for differential abundance testing on the k-nearest neighbor graph. Essentially, we randomly choose nodes in the graph, construct neighbors around them, and cover the graph based on that. And how many neighborhoods you get depends on the number of Neighborhoods you get depends on the number of neighbors you of index cells you send. Okay, so then at that point, we have a bunch of new potentially overlapping neighborhoods. We can do the same thing. We have a signal within each neighborhood, find those that are enriched, but now it's not defined by cell type, but rather by the connectivity of the Keynes neighbor graph. Unfortunately, that definition of the neighborhoods doesn't depend on the signal. So we tried something. So, we tried something a little bit more in which we don't predefine the neighborhoods, but rather define the sets based on the signature value itself. Again, data is noisy, so we do some smoothing along the graph using kind of Laplacian methods, and then find those sets where we think we might see high signal. So, with that, we're now able to identify kind of sets that. Sets that are at lower resolution than cell type. There's trade-offs as with everything, but we can go. Sorry? Oh, and in this particular case, we take this space, right? Or something like this space. We look at the nearest neighbors of each cell and an edge. And an edge in the graph means that two cells have a similar set of nearest neighbors, right? So instead of cutting by distance in this space, it's based on the exactly right. So in principle, a lot of So, in principle, a lot of the kind of predefined cell clusters that a lot of the analysis is done. So, why biologically, right? It means that the expression profiles of these two cells are relatively similar, right? And that's fine. The real trick is finding those sets that are not, so that you can then find distinct clusters. That have similar integration, then will like globally globally, yeah. Okay, so physically, they will be physically closed. No, no, no, this is all mathematical, mathematical. Yes, it's not spatial data. So, an interesting question is in spatial data, how do you translate this kind of similarity over to the physical actual co-location? So the smoother happens at query time. We essentially decide ahead of time how many exponents we'll use for now. Yeah, but it matters because. But it matters because again, we want to do this quickly, and eventually we might expose this to the user. The number of exponent is essentially tells you the size of the neighborhoods because it will be a smoother signal. So then you'll see. So that does say something. Also depends on that, yeah. There is no no free no no free thing. No, no free thing in here. Okay, so with that, and then I'll show you one more way in which we're using representation learning to kind of find data sets. Similar idea. We want to define a space in which similarity means something. This is work by Graham. So I'll show you very, very, very briefly. The idea here is to use, well, learn a lower dimensional. Learn a lower-dimensional representation of each transcriptome, but to do so in some way that preserves what we think we know about cell types. So essentially, it uses a triplet sampling idea so that two cells that we think should be nearby and a third cell that we think should be far away is used as input to a network so that the output layer tries to respect how close this relationship that the Close this relationship that this distance between these two cells is going to be smaller than the distance between either of these to that negative. And the idea is that in that space, distance has something to say about transcriptome similarity. And how do we use this? If a scientist finds an interesting population of cells in their data set, so for example, in our portal, they can circle cells. We can then find other We can then find other cells in our corpus that have cells that are similar to that. Same way that we would have done either by typing a cell type or by typing a gene list. Now we do it by indexing into this lower dimensional space. The statistics of this are a little bit wonky. We haven't really worked this out because the signal we're looking for is kind of similarity already in that space. Already already in that space, which defines the same thing we're querying for is the thing that defines the topology of the space. So it's a little bit wonky. So if you have ideas for this, that'll be great. All right. So with that, what's next? We want to jump from the idea of finding data sets to actually summarizing what we know about cell states in different contexts. I told you about a very simple representation about is a gene expressed or not. A gene expressed or not. We saw a neural network representation of it. One of the things that we're doing is for this kind of more simpler question, given a gene, do we want to find the top K tissues and cell type with highest median expression? So for example, is ACE2, where is ACE2 expressed? We want to kind of summarize that across all the data sets that we've found. And we want to do this in relationship to the ontologies that we've done. Relationship to the ontologies that we have. So, that we can find that or define that at different levels of resolutions for cell-type labels and/or for disease definitions. We're also thinking about it for gene pairs. That gives us some ideas for synthetic lethal relationships, et cetera, et cetera. We talked about representations of state that seem to be doing well in that kind of similarity search. Of similarity search. I'm very interested in kind of understanding how we bridge this gap about representation of state that we can reason about, like the average expression of a gene set versus whatever this network did. So things that we're thinking about, and it would be interesting to get some ideas, is maybe instead of a gene list, we can start thinking about network-based representations of how these genes are regulated. You might want to think about learning. You might want to think about learning the neural net representations that are already aligned to kind of predefined pathways, signatures, or programs. Or can we use attribution methods to then query signatures and programs within like representations? And something that we're very interested in thinking about is how do we think about co-occurrence of particular cell states in a population. So, for example, we might see microglia that are doing their job in eating a beta, but astrocytes might not be. But astrocytes might not be doing their job in clearing out pieces of that. And finally, we want to start thinking about how this works in spatial data sets. So same principles, we have to think about the structural harmonization part of it. How do we represent these data sets across, or how do we represent this data across data sets? What information about tattoos, spatial, and location should we be thinking about harmonization? And then finding. And then finding the idea of recall and representations. How do we define data sets based on this spatial information, based on their spatial organizational cell states? And if we want to do something fancier, how do we define these representations in ways where similarity captures both something about physical proximity and something about cell state? Yeah. Yeah, so we don't have a lot of data here. So we haven't, I don't think we'll do as good of a job of just capturing data. So if the intent is to, you know, at least see cell states, we won't see them from spatial. So we'll have to somehow map that to the, from what we see in single cell RNA to spatial. And that's where things like deconvolution would come in. All right, so with that, I'll thank the team that has been working on it. We have an engineering team, a data curation team, folks thinking about infrastructure, folks thinking about integration, and some of the work that we did with on topological enrichment was an intern from Berkeley and a couple of our scientists as well. And with that, I'll take some questions and back. Thank you. That was great. Thanks, Lisa. Nice work. I think going forward, right? And so one of the sort of two problems that are. There's sort of two problems that are, I think, in this space that are really big that need some thinking about, which is one, how do you sort of get a sense of completeness of a query, right? So I come in and I say, find all the myeloid cells for me, and I get an answer, right? Like I do from CZI or whatever, but I have no idea if it's accurate. I have no idea if there were data sets. If there were data sets that I should have got, that you have a bug in your code or there's an incompleteness somewhere. And I think that on some level, it's going to be important for these kinds of things as we sort of develop more of the integration stuff to be able to find ways of sort of independently giving some feedback on like how many of those do I think I should have found? Does the response look about right? Look about right. So that's one thing, and you know, going to be interesting to know your thoughts because I think it's important to start to be able to deal with that question. And then the other one is quite related, which is, so you're enabling a ton of interactions with this, right? And some of them will work and we can give people good ideas. And some of them, the stuff that comes back is going to be, you know, a little more problematic, right? And so, how Problematic, right? And so, how do you think about helping your users start to understand, you know, yes, you can do this, but it's not going to be that effective, right? Yeah, I think it's, as you mentioned, it's related and I think a byproduct of the fact that we're trying to do two things, one, like query by structure, right? So, how people have annotated their data sets, but also query by sales. But also query by cell state and query by cell similarity. I don't think, I think to your point, I don't think we can guarantee completeness in either, in any of those three. Some understanding of the complementary information you get from all those three or the consistent information that you get from all three of these, right? Because I can define cell search by a signature based on cell type markers that same data scientists might have. Same data scientists might have done that. So we haven't done it, but I think a certain analysis of the consistency between labels, expression of markers, and similarity in the space for those annotations, at least will give us something about the, I don't know, completeness is a little tough to think about, but at least something about correctness by consensus, let's say. A consensus, let's say. So that's one way I would think about this. And in fact, I think it is important. And the reason why we dove in into kind of transcriptome-based search rather quickly, because if we only concentrate on annotation, I think we get closer to the or faster to this kind of problem. And in terms of showing what's wrong, I think that's a general. I think that's a general problem a lot in interactive analysis. We've done, you know, things that we try to do are standard kind of enrichment type of false discovery control and so on. But those are kind of instantaneous records of control and it doesn't really capture the history of interaction of a user in a system. That's a little bit more challenging. There are some ideas for kind of streaming ways of Streaming ways of doing control. Pulse discovery control, but I guess I think it's still an open question. Oh, nice thought, Hector. So I had a quick question about, like, I just want to hear your perspective about how much upfront work you think people need to spend on the integration before these tools that you're developing on top of it actually make sense to you. To use so, like, so where does this come from? Is I had this talk with Multi Lucan about all the HCA integration efforts and building atlases for the six core tissues organs. And, you know, the way he makes it sound is that for each one of these organ sites, you have this, you know, you have these like armies of people with the main expertise of the different types of cells who are just battling it out in terms of figuring out what are the right labels or like, you know, what is the effect of like different perfusion protocols on the data, da, da, da, da. Who follows on the data, and you know, they're still not done. And a lot of them, even for the like long antlers, people spend tons and effort. And you know, I guess I just imagine when I look at, when I saw your slide about, you know, integrating these 130 million cells across tons of data sets from tons of publications, do you like, do you guys have some sort of similar background effort in that integration process? Or are you just hoping that large numbers just kind of wash out there? There was an insidious strategy in my calling out what we mean by integration, right? So, structural, meaning we put it on the same shape, semantic, meaning we're using labels. And then the last bit is where we do measurement, right? Can we actually integrate measurement in some reasonable way? And I think part of the big challenge that this kind of similarity deep neural network idea tries to get at is that, you know, at a At a, not necessarily per cell type community, but a growth level, you're able to kind of try to average some of these effects out, which may or may not be doing it well. Now, do we have a systematic effort of curation and designing these things? Essentially, right, so the way we kind of are organized is by disease area, right? So we have our By disease area, right? So we have our lung experts, we have our people really thinking about colorectal cancer. It's essentially we help them come to consensus and come to opinion, right? So we are, for example, starting to capture some of, so for example, they build their own atlases that they're comfortable with because they've seen it work. Where essentially per DC's area, letting each team kind of Each team kind of figure how they want to use this. The other thing I would say is by design, we try not to bake a lot of opinion into the tools, which might be a problem as users that we are. We lean towards access at all times rather than waiting for our high degree of curation to then give resources to our scientists. Our scientists. So that could have its problems, but at the end of the day, we kind of assume each of these teams is going to do the right thing. When they do, then we incorporate that into our own. I don't think there's a technical solution that works for everything. It's as much a sociological problem as a technical problem of this organization. At least we can say that the folks who are using these data sets to understand their experience. Data sets to understand their experiments in the context of, say, lung fibrosis, have come to some sort of consensus on how they want to do things. And if you disagree, then just grab your own data sets and do your own integration. Yeah, thanks. Great talk. So you have 100 million cells. That is a lot. And maybe it's a philosophical question, but could you comment on the value of a single cell? Because I don't think I've ever seen anybody actually make a scientific conclusion based on a single cell. Scientific conclusion based on a single cell, but yet we keep them around for all of this analysis and it's very expensive. As I said, right, so all these query results are not based on single, on the expression on a single cell because we don't buy that. So the whole problem of topological and predefining and smoothing and all of that is defining the population, which you want to perform a particular. And that's the challenge of our. That's the challenge of our part of the reason why we keep them around. And this is something I was discussing with Robert. Like, I'd say, well, definitely the first day. I'd say 90% of the inferences that we were discussing on the first day are performed over things we don't observe directly, but rather that we construct computationally. This could be deconvolutions, this could be trajectories, this could be topological graphs. We don't actually observe any of these things. We don't actually observe any of these things. We construct them computationally, inferentially, and then perform another inference over that. And part of it is because we don't agree on what it is that we want to perform inference over, because we don't observe the things we might want to do inference over. We always have to deduce them in some way. So the reason why we keep the data around is because we don't know, right, what are the components of the populations we want to perform inference over. If we did, then of course we can just summarize that those statistics for those populations. Those statistics for those populations and be done with. But in a lot of what we do, we don't and we kind of reconstruct our populations along the way. No, because I well, unless I can isolate, yeah. No, I don't think the, well, I mean, we can tell you value in terms of similarity, right? We now have a network that every data set that comes in tells us if it fills a new part of the. Tells us if it fills a new part of the space or not. And there are many examples of data sets that don't. And that's part of why we're kind of advocating this idea of, you know, check first and see what it does. I don't think it's a, I don't, again, to the point of completeness. I don't think we're near the point. I think there are many data sets that are sequencing the same cell. I think in perturbation data sets, my friends who are My friends who are doing perturbation experiments say we're going to generate 10 million cells. And I tell them, yes, great, but about 100,000 of them will actually contain anything because you're going to be doing the same experiment 990,000 time because the perturbation didn't do anything. So there is, I think we don't know yet the information density of these spaces, but until we do more. Until we do more. I don't know how else to do it other than to just make sure we have the populations. Yeah, I guess I would just want to jump in quickly and say, yeah, we might have 100 million, we might even have a billion cells. We have pretty much zero genetic diversity. Right. And genetic diversity actually matters. We have, with these single cell and other ohmic style things, think that somehow by measuring a whole bunch of stuff within one individual tells us anything at all about what happens in the next individual. Happens in the next individual, right? We don't know what normal looks like, right? And so we think back to Pei's talk yesterday about what proportion of the cells are some type. Well, that proportion has probably got a fairly decent distribution across the population, and it could depend on a gigantic number of different things in different people. Do they have an infection? Do they have a particular disease? And so, you know, I think that there isn't an upper band. There isn't an upper bound yet on how much data we need to be able to explore that because we need to, especially in understanding disease and drug discovery, you have to have a much better idea of what happens in the population, right? It's just really worrying to me that we might decide that, hey, this tells us a lot about something, right? It's a tiny little picture, even though it's a ton of data, right? Ton of data. Right. It's a tiny little picture in the space. For the single cell thing, is that because biology we believe happens at a single cell level. So when the ambition is of trying to understand the biology, single cell appears to be a natural starting point. Natural starting point, and I guess that's the overall ambition that is there: that we'll understand biology from the single cell, although it might look like too much data, no information, and so on. But at some point, maybe we will. Thanks. Frankly, my interest is to understand how we're going to. Understand how we're going to use that. I agree. It's going to be useful. Let's put it into practice. If there's somebody who has a vested interest in making sure we put it into practice, it's us. So let's do it. So you're using data that come from both like relatively unified pipelines as well as just whatever some dude put up on GEO or some dudette. How much effort do you go to try to reprocess? Like, are you going to start? To try to reprocess, like, are you going to start from fast queues or just kind of take whatever count matrices you can find? Yeah, we are using count matrices at this point, and I think to the I don't, I don't, I can't quite formulate it concretely yet, but I think the real value of a lot of this is going to be an analysis across multiple experiments, potentially distinct processing. Um, processing is actually a good thing potentially because we want to find inferences that are somewhat consistent across all of these things. So, I do think that there is some value. I do think that the most information we'll get will be when we start doing more meta-analysis, rather than the ability to really line up every single data set so that we can look at signal across all of these signal at the expression level. At the expression level across all of these samples. Thank you very much. We can continue at the coffee break now. We'll take 30 minutes break. Thank you, Hector, again.