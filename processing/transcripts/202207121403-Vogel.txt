It's been there 2018 once, and it's magnificent. I mean, it's like I'm really very jealous. Okay, so my talk is going to be on eigenvector localization, and it's partly based on joint work with Anirban Bazak offers Aytuni and a different part that I'm going to talk about also with Johannes Schestran. So, I didn't put Johannes in the abstract that I wrote because originally I had something sort of different outline. Something sort of different outline for this talk in mind. But I changed a bit, so I put in there as well. So also, it's a bit late here. So if I start saying nonsense, please interrupt me at any point. I think I should be able to hear you, or this hopefully should be fine. Okay, so let's jump into this very usual motivational example of this sort of Of this sort of left shift operator Pn on Zn. So n is a very large dimension. And we're just looking at this sort of matrix here with the ones on the upper diagonal. So just a classical Jordan block. And the dimension n is very large. And so what we know is that the spectrum, I mean, it's just in zero, very degenerate. And there is one eigenvector, one, zero, zero. And so in this picture here, there is red is just the red circle, is the unit cell. Units. And so the question is: what happens if we add a little bit of small random noise? So, meaning that to this Jordan block, we add a small random perturbation. So, meaning that Qn here is a complex Gaussian random matrix. So, whose entry are standard complex Gaussians? And small means that this coupling constant here, n to the power minus gamma, is with gamma always larger than one half. So, this means that the norm of this perturbation is. The norm of this perturbation is with extremely good probabilities, extremely close to one, as n goes to infinity. It's of order n to the one-half minus gamma. So as gamma is larger than one, this really is small. And you can imagine gamma to be any arbitrary large fixed constant. And so what happens with the spectrum? And if we ask Matt, it would tell us this. So we get that the spectrum. So this point here isn't there anymore. So, this point here isn't there anymore, it's just from the previous picture. So, actually, most of the eigenvalues will immediately move to the unit circle and sort of accumulate there. There's just finitely many inside. And was there a question? Oh, no? Okay, perfect. And so, what this means is that there is always on my screen, there's always a black. There is always on my screen, there's always a black thing appearing here. Can you see that? Ah, okay, that's annoying. I kind of want to get rid of that. What happens if I do this? Okay, now it's not there. Well, so in any case, so one way of quantifying what happens with the spectrum here is a sort of a while law, and in a very weak form, we can say that if we look at this normalized experimental A normalized experimental spectral measure here, so meaning one over the dimension, and we put a mass of Dirac mass onto each point of the spectrum of this matrix, then this converges weakly in probability or even almost purely to the uniform measure on the unit circle. So we get a sort of equidistribution of these eigenvalues. And so, but another question is: what happens with the corresponding eigenvectors? So we know the original one that's just one degeneracy, which is just one, zero, zero. Degeneracy, which is just 100, and the eigenvectors of the perturbed matrix. So, again, if we look at simulations, so for n, for the dimension to be 1000, and gamma, this coupling constant here at delta is n to the power of minus gamma, is two, then we get a sort of on the left, a sort of localized eigenvector here that has a sort of lots of this mass here on the left-hand side, but then has an exponential tail. And if we increase gamma a little bit by one-half, then we still are. Then we still are quite localized on the left. It's a bit broader, but we still have a sort of exponential tail. And indeed, so this is what this talk is going to be about: to tell you how these eigenvectors for these matrices and more general than the toplets matrices localize to either the left or the right of their support. Just a little bit of an outlook, what this talk is not going to be about, and which might be more interesting, is that there seems to be a phase transition. So remember, as long as gamma is larger than one. As gamma is larger than one, sorry, larger than one half, we are in the domain of very of a small perturbation, right? Because this is this unperturbed deterministic part is of order one. And this perturbation is, as long as gamma is larger than one, strictly larger than one, it decays to zero in norm as algae and goes to infinity. So, but as long as gamma is larger than one, we see that it kind of behaves like sort of this exponential decay with a localization to the left here. But at gamma, To the left here. But at gamma one, something changes, and gamma larger than one, definitely we move towards the sort of delocalization behavior. And at gamma one half, there the perturbation is of size one. It's comparable. We're definitely in the regime of just a sort of deterministically perturbed random matrix, and we expect sort of the usual delocalization of eigenvectors of random matrices results to hold. But the interesting part is also before there seems to be. interesting part is also before there seems to be this really even already in the small perturbation phase there seems to be this sort of phase transition so unfortunately this sort of delocalization part here between one and one half this is not what i'm going to talk about this is sort of what we're currently working on and maybe at the next round of this conference i can tell you what happens there so um let me just for a bit of motivational reason so why we're interested in spectra of non-sulphogen operators and i mean luckily at this conference i have to say nothing because all the talks I have to say nothing because all the talks are about this, and the crowd is very motivated for months of term problems. But let me just mention a few. We have seen already in the talk of Tanya Christiansen the relation to scattering theory and resonances, or in the talk of Borobala Guerra, the damped systems relation, or simply the damped wave equation. But there are many other more interesting kinds of problems where non-self-retront spectral problems appear, and therefore they're interested. And spectral problems appear, and therefore, they're interesting. So, let me just remind you briefly of the sort of driving force behind the results that we're going to exploit, which is this phenomenon of spectral instability. So, meaning that due to the intrinsic non-self-adjointness of the operators that we're considering, we have a generic problem of controlling the resolvent norm. So, meaning that it can be very large, even if we are far away from the spectrum. So, in general, we only have this lower bound on the resolvent norm, one over the distance. Norm over the distance of the spectral parameter to the spectrum. If we were in a self-adjoint or normal case, we'd have equality here. So, and this can lead in general to this sort of pseudo-spectral effect, meaning that the spectrum is very unstable with small perturbations, which led in due to bi-numerical analysis such as Trefetam and Embry. So, I'm sure you all know this book of Treffetan and Embry of 2005, which is an excellent reference for all of that, to this definition of this epsilon pseudosport. Of this epsilon pseudo spectrum. So, meaning that this epsilon pseudo-spectrum, meaning is that for a little small positive epsilon, we add to the spectrum of the operator, we add all the points in Z where the resolvent norm is larger than one over epsilon. And this definition, this notion is completely equivalent to this notion of instability with respect to small perturbation. So, this instability of the spectrum, meaning that a point is in this epsilon pseudo-spectrum, if and only if. If and only if there exists a perturbation, so a bounded operator of normal epsilon, such that Z can be realized in the spectrum of this Q perturbation of P. So this of our initial operator P, we have a perturbation of size epsilon. So this kind of really captures where in the complex plane we are or where we can go with perturbations of size epsilon. Okay, so let me just carry on and Let me just carry on and tell you about the setting that we're interested in, which is this nice setting of tuplets matrices. I mean, it has been studied since a century, essentially. I mean, it's interesting work by Goberg, and there's two excellent books about summing up all you want about known spectral theory, about topless matrices by Betcha Silberman and Betscha Grutzki. And so the setting is kind of simple. So we look at the right-shift operator on functions. shift operator on functions on complex valued functions on z so this tor is just the shift by one unit to the right so tor of u at the index n is just u of n minus one so one shift to the right and so these these uh if we take powers of tor so new powers of tor positive powers is just a shift to the right by new units and if new is negative we shift to the left by new elements and so we can take linear combinations p And so we can take linear combinations. PÎ½ here is just a complex number. And we get a nice, so if this is finite here, we get a nice operator, a bounded operator between on L2Z. And indeed, I mean, more generally, many things I'm going to say work also for P, which is just an infinite sequence, as long as, so the sequence of coefficients here, if we put a weight in front of it of one plus the modulus of the index, this should still be L. This should still be L1 summable. And in general, this remains that this then remains a normal operator. So on L2Z. And with the Fourier transform, we can associate to this a symbol. So this is just this complex function. So zeta in Z without zero goes to B zeta, which is just where we've replaced the toe by the symbol of TO, which is one over Zeta. So some complex number. And how do we get taplets matrices now? It's just by truncating operators. Operated to a finite interval 0 to n, and this natural space L2 on the interval 0 to n. So n is not included, so that we have n units on this interval. So this gives us a tablets matrix from a symbol. And so there's one quantity that will be important, which is the symbol curve Ps, which is just the image of the unit circle under this P. So this is, if, in fact, if we assume this assumption here, this will be a nice, the symbol will be a nice. Nice, the symbol will be a nice on the unit circle, it will be a nice C1 function. And so we get a sort of closed bounded, nice curve in the complex plane, like this, for example. Finally, because so this is a normal operator, indeed, the spectral theorem just tells us that the spectrum of this operator here will be purely absolutely continuous and on this curve. But if we start truncating, we destroy non-self-adjointness, and it will be in general a non-self-adjoint problem, and the spectrum will. Salphogenic problem and the spectrum will be can look then like this, which will be, however, still inside this kind of domain here. And there is one other sort of object that I would need to mention, which is these circular matrices. And it is if we kind of instead of truncating this operator to a finite interval, we truncate it either to a finite interval with periodic boundary conditions or simply to a discretized unit circle. So we consider this operator. Consider this operator as on L2Z model NZ, okay? Z. So this is then, which I'll note by Pn with this little C here for circulant. So this again is a normal operator. So spectral theorem tells us everything about its spectrum. It's just P evaluated at the points of the discretized circle. And the eigenvectors are perfectly known. They're just at the columns of the discrete Fourier transform. So they're just these Fourier modes here. They're just these Fourier modes here, normalized by one of a square root n. Okay, so let's go back to the small random perturbations. In this case, we have a probabilistic while law as slightly indicated before. So meaning, so here is an American simulation. So we have one certain symbol that gives us this curve, this right symbol curve, and inside is the spectrum of the unperturbed tablets matrix. And once we add a little perturbation, the spectrum immediately starts moving to. Immediately starts moving to kind of this mimic the shape of this symbol curve. And more quantitatively, we can say that this goes back to the theorem of Shastrand and myself in various guises. So this means that if we take some omega, some closed, simply connected set with a smooth boundary, and we want to count the eigenvalues of this perturbed operator or this perturbed doublets matrix inside omega, then we get the while law in the sense that. In the sense that as long as the coupling constant is sufficiently small, so I'm going to comment on this, but it should be larger than sub-exponentially small in n, so delta being between 0 and 1. Then the number of eigenvalues inside omega is given by n times the volume of the sort of segment of omega intersected with the symbol curve. So if omega doesn't intersect the symbol curve, this leading part is zero. This leading part is zero times integrated against this push forward of the uniform measure on the unit circle, this push forward by the symbol of our tablets matrix. And this holds with probability that it's sub-exponentially close to one. So here, I mean, this definitely holds also for a coupling constant up to n, something that for n minus gamma with gamma larger than one, sorry, one half. It's just that when we wrote this theorem, we never thought of going in the This theorem, we never thought of going, and that it would be interesting to go to the sort of limiting case. Luckily, however, there's probabilists Bazaka Paquet and Saituni in various papers who gave a sort of weaker version of weak convergence, but it's essentially the same theorem, but for more general classes of random perturbation, not just Gaussian. And however, they gave this improbability. For just the Jordan block, there is a longer story. It was actually first done by probabilistic Schnyady. Actually, first done by probabilistic, which used techniques from stochastic differential equations and free probability to prove something very similar. There's also work by Bortenaf and Capitane, David Hager, Guionet, Wooden, Saituni, and Wood, which all with various techniques essentially prove something quite similar, always for the German block. So, but why are we essentially interested in tablets matrices? And the reason is simply because they are a good toy model for pseudo-differential operators, especially. Differential operators, especially semi-classical pseudo-differential operators, because a lot of the calculus one usually does for these tablets matrices is essentially one can do similar things for pseudo-differential operators. And so in general, this sort of why law that we get for non-self-adjoint tablets matrices also holds for certain for a rather large class of non-self-adjoint pseudo-differential operators in the semi-classical limit. So we can perturb this also, these pseudo-differential operators with random matrices. Differential operators with random matrices, and this was done by Hager and Bordeaux-Montrieu in a sort of toy model case, and then generalized by Hager and Chestron to pseudo-differential operators on Rd. And also in the case of Bettis and Tablets quantizations, first for Tauri, for functions on the Taurus, even dimensional torus, it was done by Christians and Sworsky, and then myself, and also by Bazak and Paket and Saituni from a probabilistic point of view, and very recently also by. Of view and very recently also by Isaac Altman, who did it for general Tuplets, Berryson-Tupletz quantizations of compact Keller manifolds. So essentially, if you perturb your non-self-adjoint pseudo-differential operator or Berryson-Tupletz operator, some suitable, but very small perturbation, you will get a Y law, a probabilistic Y law. So I'm going to skip that and just jump directly to this eigenvector. To this eigenvector discussion. And so let me remind you: maybe I'm going to go back to slides. So, here, what we see is for these tablets matrix is that once we add a small random perturbation, the spectrum of this perturbed tablets matrix seems to really imitate immediately the spectrum of the circular matrix. So, it essentially converges, or you can prove that they are extremely close. And so, it's natural to ask: well, what about the eigenvectors? But of course, for Well, what about the eigenvectors? But of course, for the circulant versions, I mean, this black blob here is just the moduli of the Fourier mode, right? So they are all the same size, and I mean, they're perfectly delocalized in a sense. Whereas the eigenvectors of these perturbed Duplets matrices, I mean, they're strongly localized to one or the other side. And so that's what I'm going to talk about now. So how do we get the sort of something about this localization phenomena? And so from now on, this coupling constant. And so from now on, this coupling constant, so this constant kind of giving us the size of this coupling constant, so n to the power of minus gamma here, from now on be always with a gamma larger than one, okay, to avoid this regime that I talked about before where we start to see delocalization. So to study these eigenvectors, the first thing we do is something which is now by now very standard in this business, which is to set up a well-proposed Gusian problem. So this means Gushen problem. So this means first we look at the singular values and the singular vectors of our tablets matrix. So for z being a spectral parameter, we look at pn minus z times pn minus z star and look at its eigenvalues and eigenvectors, which are the singular values and singular vectors of pn minus z. So its eigenvalues will be just t1 squared to tn squared. It's an n-dimensional matrix, so we have n. And if we Have n, and if we take the square root of these eigenvalues, we are interested in all of the eigenvalues up to the nth, such that they're still smaller, but so that the m plus first is slightly strictly larger than some parameter alpha, where alpha is still larger than one over n. So alpha can't be too small. So we're taking all the singular values up to essentially one over n. And corresponding with these singular values or eigenvalues, if you want to, we have. Singular values or eigenvalues, if you want to, we have singular vectors, which are an orthonormal system of singular vectors. And we have a second orthonormal system of singular vectors, which are the eigenvectors of Pn minus Z times Pn minus Z star with the adjoint from the right. So these essentially will be a second orthonormal system of eigenvectors corresponding to the eigenvalues t1 squared tm squared of this matrix, this self-adjoint matrix. So we've Matrix. So with these guys, we can set up a Grushian problem, which consists essentially of enlarging our problem to an operator-valued matrix again. So we put our matrix minus Z here, and we try to find two kind of additional operators here, R minus, R plus, such that this problem here becomes invertible for all Z. And so here is a bug. This should be little L2 of C. And I'm sorry, this is the Hilbert space that obviously this works on is little L2 of. Is little L2 of finite interval zero to n. I'm sorry for that. So the way we do this is this R plus here is what we'll do is essentially we'll project onto a space that is kind of larger and containing larger than the kernel of Pn minus Z. So here we project, so R plus goes into Cm. goes into Cm. And what we do is the kth element of this vector r plus applied to u is just the projection of our vector u that is a vector in cn if you want to project it onto this kth singular vector of our matrix pn minus c and similarly r minus just add these these these the vectors that essentially are this of normal system of the eigenvectors of this Vectors of this guy here. So the interesting thing is, again, I mean, for very small singular values, or even when they're zero, this space here spanned by the vectors F1, Fm, definitely contains the co-kernel of Pn minus Z. So since we're adding here the projection onto the kernel and onto the co-kernel, this guy immediately becomes invertible. It's not hard to show. And so we write the inverse, but it's E calligraphic with E here, E minus, E plus, and E minus plus. Here, E minus, E plus, and E minus plus. And so the first observation that one can do is essentially by the short complement formula, as we've learned yesterday, that E minus plus is invertible if and only if P n minus Z here is invertible. And this gives us a nice way to kind of already to study easier the spectrum. But that's not what we will need, but just a little remark. So next we will try to study this Brushian problem. try to study the discriminant problem for our perturbed matrix. So we add here delta qn, so our perturbation, and we leave the r plus or r minus the same. And essentially, since the perturbation is still small, you can show by a Neumann type perturbation argument that this guy is still invertible with an inverse, but we just denoted by delta that it's now of the perturbed operator. And all of this holds with very high probability. It's a probabilistic perturbation. And so. And so, what is the interesting part? Why does the solution problem tell us something about the eigenvectors? And it's so here I just copied from the page before what we just said. And the important observation is that this guy here, the E plus delta, indeed gives us a bijection between the kernel of E minus plus delta and the kernel of P and delta minus Z. This is a nice bijection. And furthermore, since this is a, we use the Furthermore, since this is a we use the perturbative argument here, we have a sort of perturbative expansion to express E plus delta by the one of the unperturbed operator, which is just a linear combination of the singular vectors Ej of our matrix Pn minus Z, so the unperturbed matrix, and plus something that is a sort of rest in this perturbative series. And we can show that with high probability, this will go to zero as n goes to infinity. So this. So, this means essentially that if we have an eigenvalue to this perturbed operator with an eigenvector, then with high probability, we can express this eigenvector as a linear combination of these singular vectors of our unperturbed matrix times some coefficients. So, m coefficients here we need because this the m was the number of singular vectors we needed to invert our Grushian problem or to get a well-posed Grushen problem. Posed cushion problem. And this coefficients psi here, they are simply the coefficients of the kernel element of e minus plus z hat. So this again has a nice perturbative expression where we see that our perturbation, so to say, comes in. So these three elements, so E minus plus, E minus, and E plus here are deterministic and you're only in leading order in the coupling constant. It's only Q here that appears as a perturbation. And so what Perturbation. And so, to study these eigenvectors of our perturbed problem, what remains is the things, two things. So, study these singular vectors and these eigenvectors of this kernel here. But this is a sort of finite dimension. So, finite matrix, even as n goes to infinity, it remains finite dimensional with some random elements. And with a bit of analysis, we can fairly easily show that if this is a kernel element here. If this is a kernel element here, then these vectors psi j put a little bit of mass onto all of its elements. So it does not put zero onto any of these vectors Ej. So we have a minimal bound on all of them. So it really remains to study these vectors EJ here, these singular vectors. And to study them, what we need is two things. So we need to know very precisely where the eigenvalues are. So for that, we need to do the same. So, for that, we need a sort of local law of eigenvalues. So, what we do is, first of all, we avoid problematic points. So, one problematic point is where the curve intersects itself. So, it has auto-crossings. So, there we can, our analysis doesn't work. So, we put a protective ball of radius epsilon around it to avoid this. Also, if we are in the case where the curve has a cusp, so where the derivative of the symbol is zero, we get a sort of cusp. We get a sort of cusp, and there we also don't know what to do. Things are more difficult, so we put the protective ball of radius epsilon around it and avoid it. And then we say that, well, so we put this set, which of given this epsilon characterizing this safety ball around our problematic points, then it's just the set of all points Z, which are at the distance log over n, log n over n to the spectral curve. And we are Spectral curve, and we are avoiding these special points. Okay, so we are really inside this zone here that is dotted. So, at a distance log n over n to, but not farther than log n over n. So, really bounded from above and below this distance by log n over n inside this curve here. Okay, so we are essentially at this dotted line. And the result that we can show is that the probability that for any given mu, the probability that the number of eigenvalues of this The number of eigenvalues of this perturbed matrix inside this, so that is intersected with this dotted line here is smaller than one minus mu n goes to zero. So with this, we can say that with the probability going to one. But also, I've started five minutes late, I think. But let me okay, let me just take one minute and tell you the results. Take one minute and tell you the results. Okay, blah, blah, blah. So the result is essentially this: that what we can prove with the one additional step that I had to jump is that the following. So we take Z is an eigenvalue and V is a sort of corresponding eigenvector. Then if the winding number of our curve around this eigenvalue, so if the winding number around our eigenvalue here, dotted as X, is positive. Here dotted as x is positive, then we are localized on the left-hand side, meaning that if we put the L2 norm on the interval L to n, so L is in one to n, we get that this is bounded from above by some constant, e to the power of minus C L times log n over n. So this means it is bound, nothing happens until n is of size log n over n. So we go at most to the breadth log n over n, and then we see an exponential tail. And the same for winding number negative, it's just that we're localized on the other side. Negative, it's just that we're localized on the other side of the interval, so the same from the left, from the right. And to say that we have exactly a breadth in this guy here of log n over n, we need the corresponding lower bound. So if we look at the L2 norm and an interval L to L prime, where they are L and L prime are bounded by n over log n, and they have to be at the minimal distance of something slightly larger than square root of n, then we can say that this L2 norm is bounded from below by L2 norm is bounded from below by the breadth of this interval, the length of this interval times log n over n. So this means that this breadth of this localization here is really of size log n over n, and then we see an exponential decay. Okay, so sorry for that one minute too much, and thank you very much for your attention. Thank you. Are there any patrons, remarks? I'll send them for emails. I think it is too, too long. Okay, so you get the question by email. Okay, great. Well, if not, then let's thank Martin again. Thank you. Okay. Thank you. And well, enjoy the rest of the conference. Thank you. Thank you for joining. See you. Bye-bye. Bye-bye.