Okay, so I would like to talk about something related to the neuron collabs. So this is actually very new work we just posted to the archive last two weeks ago. And this work is jointly done with my very smart students, Wani Hong, on this topic. Okay, so let me first introduce what is neuron collapse and what's a state-of-the-art result on this topic and what is new for us. Is new for us. And in particular, most of the state-of-the-art results on neuron collapse focused on the so-called unconstrained feature model, which I explain later on. And our main contributions try to study the neuron collapse phenomenon for nonlinear model, in particular for shallow network. Okay, so we'll start with some basics related to machine learning, also training of a neural network. So we basically think about neural network of this form. W is a linear classified. W is a linear classifier, and H of theta X would be the features on the last layer. And this feature would be a very complicated function. For example, this looks something like this. It could be much deeper. Okay, so in this work, we'll mainly focus on two type of neural network, real neural network. One's a two-layer, the other one's a three-layer neural network. So typically, give you the data, we try to train the model. The data: we try to train the models by minimizing empirical risk minimization, and we choose a loss function L, it could be cross-entropy or L2 based on the regression or for classification problem. And we minimize the over theta and try to find out a good model. So that's actually the standard pipeline right now for the training of a model. So, what is interesting about this is that for a very large over-parametrized neural network, if we train the model Overparametrized neural network, if we train the model longer enough, some inductive bias actually occurs, which is called a neuron collapse. First discovered by Pepe and Donahoe in 2019 and 2020, around that period of time. So in a nutshell, basically, it says that when the training evolves, if you train a neural network for a longer enough time, the feature of non-last layer, which is a H, right? The feature corresponding to the data in the same class would actually. To the data in the same class would actually collapse to a single vector, which means, so here I use the HKI means the ice points in the case cluster, it will actually converge to the sample average of the case class. Okay, so we typically call it the variability collapse or NC1. There's other notions of neuron collapse. One of the famous one is related to the ecoangletide frame. Basically, it says if you have a balanced data set, which means you have Balance the data set, which means we have, like, let's say, K classes, and each class contains the same amount of data points. If we train the model long enough, the mean feature, which is obtained from the first class, actually would exhibit those equal angular type frame, which means the pairwise angle between this mean feature would be the same. And also, the same applied to the linear classifier W. Okay, so that's actually quite an interesting thing because which means, so they observe these phenomenons on a large family. Phenomenon on a large family of neural networks, it seems to be a quite universal thing. So that's actually sparked some of the research interest into why that happens, why the neuron collapse happen. Okay, so that would be illustration. Basically, it says for three classes, finally you would have this Vasidi-Benz type of economic just illustration. Okay, so the question here is for the mathematical perspective, why it happened? Why it happened. So, an immediate challenge would be a neural network is typically highly nonlinear, which means that try to understand it directly would be very tricky. But one way to simplify it is try to use an idea from approximation theory, right? So even for a very, for a two-linear neural network, a non-linear neural network, as long as the width is sufficiently wide, it has exceptionally strong expressiveness, which means it's able to approximate. Expressiveness, which means it's able to approximate functions on compact sets, let's say, to absolute error, as long as the width is sufficiently large. So that's actually sparked one of the idea of the consider those surrogate model called the unconstrained feature model, which basically says that things the neural network is able to interpolate data points. So how about I just replace the features, which is a complicated neural network, by arbitrary functions. So here you'll be replaced by arbitrary vector, let's say HKI. Say HKI. Okay, so the idea here is that neural network has a very powerful approximation power, but on the other hand, doing this would actually decouple the output and the data. Because when we're assuming these features are arbitrary vector, you actually have nothing to do with the input data. But the interesting thing is that if a study, this sort of unconstrained feature model, also known as a layer period model. also known as a layer period model. It is shown that for both this objective function under some regularization function, for both the cross entropy loss and L2 loss, it's able to confirm this finding in the neuron class, which says that under the unconstrained feature model, indeed the global minimizer would have this NC1, NC2. The mean feature would have equalangular tight frame, and also all the features in the same class would collapse to a single factor. So, in the same class, it would collapse to a single vector. Okay, so this has been done by a long list of papers. It's incomplete, but that's some of the most influential work in this area. And also, this has been extended to imbalance data set, which means that if the class is not, if the points in each class are not the same, what would happen? So, in that case, we will have this minority class, which means a small group would be completely ignored. Would be completely ignored by a large group. So that's actually to do that. So why that happens? So I can give a very quick tool why that happened. So from the optimization perspective or from low-rank matrix vectorization perspective, it's very simple to understand why that happened. So for example, if we look at an unconstrained future model within the unconstrained optimization problem, then in fact, we can let this W times H as the Can let this W times H as the one variable, Z, let's say. Then, if we fix this Z and try to minimize the regularization parameter, we find that that's exactly the nuclear norm minimization rate. Rube actually mentioned that at the very beginning of today's talk. So, in other words, the objective function in the over-parametrized regime, so it has to have to emphasize, it has to be over-parametrized regime. So, the objective function is essentially this convex problem. So, it's a convex and So it's a convex empirical risk plus nuclear norm regularization. So this sort of problem is quite often used in, for example, denoising, matrix compilation, et cetera. It's essentially the single variance thresholding. So then many things actually can be explained. So one of the some of the most commonly seen results would be on the both across entropy loss function and z. Okay, so I didn't touch. Z, okay, so I didn't talk about W and H because if you get a Z, you also get a W and H. I would have this the equangular tight frame. So Y would be the label vector. So it's a 1, E1, the first label and the second label. So you have this. So exactly, you have these equangular type frame structure. So the proof is essentially using the complexity label symmetry. Okay. Okay, so it's all good, but also explain the NC to some extent. But of course, it has its drawback. That's actually motivates our work. So first, as I mentioned earlier in the talk, the unconscious feature model actually breaks the connection between input data and the output. But on the other hand, it's extremely important to try to connect the output and input if we want to understand, for example, generalization. Generalization. So, which means what we want to do is try to extend this framework at least for some toy example to see how the neural network, how the training of the neural network, in particular the neuron claps, when the emergence of these neuron claps depends on the data structure, which means the statistical property of the data, the neural network architecture, let's say width, input dimension. Let's say width, input dimension, how many samples you have, how those factors would interact with the neuron class. Okay, so in particular, we'll first start with the two-layer reroot network because it's a simplest nonlinear model. So theta would be the pair parameter. In this case, we consider there's only the in total k classes, and then d would be feature dimension. Feature dimension, actually, the width of the first layer, and little d would be input dimensional data. So, in particular, we think about the empirical risk minimization with activation regression. So, it's used in some of the training of neural network, but it's not the most commonly used. Usually, people would use weight cake, but here we've focused on this because it's much easier to analyze. But we find some of the But we find some of the phenomenons also extend to the WikiK, which means it explains something to the actual training of neural networks. So which means we actually regularize the features of the output. Okay, so the core question is that when does a neuron collapse occur? In particular, for these objective functions I just proposed, in general, it's non-convex because you have these removed. Convex because you have this remove activation function, there's also matrix vectorization. And when this non-convex objective functions, a global minimizer, would have this NC configuration, which means the feature would have this equilangular typeframe and how it depends on, let's say, the number of data point, input dimension, and the architecture of neural network. So following these questions, we focus on three sub-problem, very concrete, and we try to explain this one. Concrete, and we try to explain this one by one. The first one is that for a sufficiently wide shallow network, does the neuron claps always tap or not? Why we think about it is because remember when we talk about unconstrained feature model, the idea of using the unconstrained feature model is that we assume if for sufficiently wide neural network, it has very nice approximation power. So the So the reason, so which means that the whole foundations of uncontrolled fission model rely on approximation theory, right? So that's why we want to think about whether for sufficient or laywide sharing network, whether NC would happen. The second question is that whether NC would be more likely to occur on structured data. So for example, let's say you have a data A data class, okay, and the class is very separated, and the variance of each class is very small. Okay, then we apply a non-linear map to it. It's very likely that the features, which means the output of this non-linear function applied to the data, it would still cluster, right? Which means they're still preserved those approximity. And if we train that further, it would still have this NC property. Okay, so when I say the NC property means When I say the NC property means that it has a NC1, the variability collapse, et cetera. So, whether it would happen. The third question is that whether the emergence of NC necessarily implies the accident generalization. So, does it, if NC indeed occurs, does it mean the model can generalize to a larger data set? So, we try to understand one by one. Send one by one. So we'll first focus on the simple first question, but before that, before think about the nonlinear model, we will still think about the unconstrained feature model. But it's different in the way that we also have this, we add this positive feature, because you have a remote activation function, which means the feature on the last layer will be positive or non-negative. So in that case, the unconstrained positive feature model would basically focus on those objective functions. We try to minimize over the function. So we try to minimize over the linear layer w and also the feature which are non-negative. So this function is non-convex and we cannot not actually using the idea before to do it because you have this non-negative constraint. But at least that's something to start with. So what will be the global minimizer to it? It's too non-convex. So to do this, actually we can use some idea. Do this, actually, we can use some idea and learn from low-rank matrix estimation that we do a convex relaxation. So, what do we do is that we do some sort of lifting by introducing two more random variable, two more variable, not random variable, variable, and u and v, so that you have this form, and a z would be the matrix in w times h. So then you actually have this positive semi-definite constraint. Positive definite constraint. And also, W and also this V would be non-negative. And the objective function will be this. So the things of doing this, the good side of this is this function is actually convex. So which means we can try to guess what the solution looks like. And then the next step will be: if we get a global minimizer of this program, we can try to verify whether it will actually imply the global minimizer of the original program. That's the idea. So it turned out to be this problem is quite simple to solve, and we are able to get it. So on the cross-entropy loss function, this feature, H would actually have this NC1. So H bar would be main feature, Y would be the label. And also, instead of you have this ETF structure, you have this orthogonal frame. So the feature actually they're perpendicular. The feature actually they're perpendicular to each other instead of their form ETF. And on the outer loss functions, even for the weight, it also have orthogonal structure, not instead of equangular type frame. So adding this non-activity constraint to the feature indeed would create something different. Yeah, besides that, we don't know because up to a rotation, I mean a positive rotation matrix actually would they're all equivalent. So this H bar would be something like every column, you only have one non-negative entry. But besides that, we don't know anything about it. Yeah. But in the actual training, it can be arbitrary. So next, we try to answer the first question. So after we are able to understand the unconstrained feature model, we study the actual nonlinear model. So we study this two-layer neural network first. So now let's think about the following question. When, for the original program, it would have NC1. So this question would actually correspond to the following. So whether we're able to find a matrix that So whether we're able to find a matrix W1, right? W1 is actually weight in the first layer. So that output, the features on the first layer, would have this H bar times Y. In particular, this is a non-negative perpendicular to each other. So whether this W1 exists or not, it's interesting in the way that it's independent of network width. Basically, the question is that if we have a large data set, okay, and then Large data set, okay, and then what you want to do is try to find out a factor and you multiply this factor to this matrix so that we are focusing on this. On the case cluster, when you do this matrix multiplication, you will get a constant vector. And the rest of them will be negative, right? Because then if we apply the rerule, it will be zero. So then you have the feature class. Okay? So the interesting thing is that it's indeed a Interesting thing is that it's indeed an independent width because we only have to look at each individual feature instead of all of them. So that's actually reduced to a linear visibility problem. So given the data set, whether there exists such a W for each K, right? So that the feature collapse would happen. Okay, so actually from linear algebra, we're able to get some convenient results. Easy result. First result is Easy result. First result is that if we look at this expression, right, we can see that if there exists such W, the data set has to be separable, which means that for each class, it has to be, has a hyperplane able to separate the case class and the rest of the data point. So, which means if the X case is not separable, the NC will definitely not happen for the renovation. Second thing is that if the dimension, input dimension is sufficiently large, which means that data dimension is sufficiently large, compared with the data point, k times n will be the number of sample, then we know that by linear algebra, we know that we can always find out a w so that this would hold, which means it's always visible. Which means if the dimension is large compared with the number of sample, global minimizer would have this NC configuration. Have this NC configuration. And of course, on the other side of the story, if D is too small, it's never going to happen. We can never find that W so that the first inequality constraint would hold. That's basically the idea. So that's a general thing. So the conclusion here is that even for a sufficiently wider neural network, NC1 won't happen unless the input dimension is sufficiently large. A dimension is sufficiently large. That's a general hold for general data set. Okay, so now let's go to the second question that five minutes. Okay. Okay. So let's think about the structure data set. So let's think about, for example, if you have this Gaussian mixture model, whether how the sigma would play a role on the emergence of NC. On the emergence of NC. Okay, so the intuition that this would have depends on the mean and also the variance in the input dimension, et cetera. So first we get a very simple result for the two classes cases that if the theta has the angle between the two mean vector, as long as they're greater than 90 degree, which means they're pointing in different direction, if the noise level is sufficiently small compared with the dimension and also the mean vector, et cetera, then the Et cetera, then the neuron collapse will happen, which means the global minimizer will have an NC configuration. Yeah, here in actually in the sum login. You mean for the signal or for it? Yeah, you have login. Right, right, right. Here, actually, we're using union bound. That's why you would have this sort of Have these sort of yeah, I'll later expand this. That's one of the first results. Okay, so if we look at this, so this is actually the phase transition plot we actually plot is try to solve the linear visibility problem on the Gaussian mixture model. That's actually what we get for difference data. You actually see the black region actually gets larger, and the red curve would be the bound actually gets from the Be the bound actually get from the lasso slide. But you see, if we just look at the red region, it's not able to capture the whole picture. Because you see, here there's a large of the white region, which is visible. So how to capture that part? So then I just, I first go to the general, let's say, K-classes case, and then go back to the question. So let's think about, let's say, you have K-classes, and I present a general result. And so the first result. And so the first result will be similar. Let's say you have k classes. It depends on the smallest singular value of the mean vector. That's a similar result as the one I just presented. And the case two would be interesting that, so if the dimension d over n, right, input dimension d over n is greater than k plus one divided by two, k is a number of classes, actually with high probability, it would happen. Okay, and also if d is sufficiently large, of course, it would happen with probability one. Of course, it would happen with probability one. So, where did this come from? Actually, we can formulate this as a Gordon's type of argument and show that it's related to the Gaussian comparison and able to show that this would make the feasibility. Yeah, I tried to explain. Yeah. Okay. So, the key argument is that, so that's actually the phase transition for like multi-class, and then you can. phase transition for like multi-class and you can see if we look at if you use a combine the red and the blue curve it's able to capture most of the region okay so key argument for the first one would be using the the union bound and for the second one is that we can formulate this feasibility problem as a try to find out maximum of a Gaussian field and if we try to understand the maximum Gaussian field we can actually use the Gordian's inequality but I have to find out the the good comparison To the good comparison, reference the Gaussian field to control it. And this actually gets this blue curve. Okay, so I'm running out of time. Okay, maybe two minutes. I'm done. Okay, so far, what I do is that just try to characterize what the global minimizer looks like. What if, what happened to the actual training? And we see that if we train the neural network long enough, let's say 10 to 6 epochs, you can see the NC1s actually decrease to, let's say, 10 to the power. ones actually decrease to let's say 10 to the power negative three and 10 to the power negative four but actually in different experiments we find that it depends on the the noise level okay so similarly uh for if we pick up a particular instances we see that these nc would actually decrease uh compare uh versus the epoch okay so now let me just finish the part on the three layer for three layers layer for three layers uh the first result is uh interesting is in the way that uh for three layer if we look at this uh first layer output if d1 right so if so you're using the result general result we can see that if d1 is sufficient large um global minimize always would be the nc because you have this linear algebra argument and in particular if we look at this uh random feature model we see that if d is greater than n log n using this concentration inequality we can concentration inequality we can show that the global minimizer would have a have a have a um would have a this nc configuration so the so so the thing why a three layer would actually have these nc configurations because you can all we can always make the first layer sufficient wide so that it's able to have a global mineral would able to have a nc configuration okay so but actual training would be slightly different because uh using stock of gradient descent it actually has a lot of fluctuation Decent actually has a lot of fluctuation, but we're able to make it down to let's let's say 10 to the power negative 2, etc. Okay, so let me just go through it quickly. For the generalization, what we do is that we assume, let's say, a Gaussian mixture model, but it can be actually much more general. And then we think about, let's say, a simple, like a two-neuron classifier, and then study if the NC happens for this. If the NC happens for these two neuron classifiers, what would be the best misclassification error you can achieve? And what we get is that the following. So we're able to reduce this to a single neuron case. And that's actually boiled down to try to estimate this optimization program, which means you try to find out beta so that the NC happens and this error bound would be as large as possible. So the story would be this: if the noise is small, If the noise is small, you can always find out beta one, beta two, make the neural net, make the model actually have the NC. But at the same time, the misclassification rate is very small. But that's provided that the noise level is small. But on the other hand, even if D is large, even if D is large means that NC always happen. But on the other hand, if the noise is very large, which means the mu divided by sigma is sufficiently small, then actually, Small, then actually the misclassification rate is always greater than some constant, which means even NC happens, we cannot get a good generalization. So which means the generalization more depends on the data itself, at least for this example. So that's the conclusion. And the details actually are in this paper we posted like two weeks ago. And if you're interested, we could take a quick And if you're interested, you could take a quick look at it, and I'm happy to talk about this offline. Thank you. I thought that the neurolapse phenomena is a phenomena that you observed during training. And during training, there is no regularization. I think the results you presented are much more about characterizing the landscape and the global optimum of a regularized problem. So what is the connection with the actual phenomena when you train on it? In the training on the neural network, actually, we still add the generalization, the regularization. The generalization the regularization without regularization, actually, we don't observe the neuron class. It depends. I think you did have the questions: what are the data structures? So, if the data structures are sufficiently separated, like some of the examples and the initialization of the network is small, that you do observe the actually we did lots of experiments. We actually didn't see. We find the regularization actually crucial, even for the training to have the Training to have the neuron class. For example, in the algorithm, we added VDK, et cetera. But without the regularization, we didn't often see the, and see usually with the objective function sometimes also explodes, have very bizarre phenomena. Yeah. Yeah, I think this is different from maybe what some of the results that were presented this morning uh about the online. But we can remind you, remember