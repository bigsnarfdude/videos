Behind time. So, firstly, thank you very much to the organizers for the invitation to give this presentation. And also, apologies for not being there. I'm very jealous, of course, as everyone else is. So what am I going to talk about today is I'm going to talk about advances in gravitational wave inference. And really, what I'm sort of going to try and do here is give some historical perspective and talk about some. Historical perspective and talk about some of the work that's been going on and maybe some sort of predictions for the future. I did plan to start off with a sort of overview of parameter estimation, but Siong has done an excellent job of doing that. So all I'm going to do is flick through these just to show what notation I'm using because my notation is slightly different from Xiong, but I'm not going to spend too long on this. So apologies, this is going to go a little bit quick. But so notationally, right? But so notationally, right, we have data that our experiment gives us. And for me, that's, you know, throughout this talk, it's going to be interferometer, H of T. And then we have theory, right? So that the theory gives us some model, which I'll use M, and that model will have some parameters, theta. And as Xiong was telling us, from Bayes's theorem, we can perform parameter estimation. So we can identify our posterior, which I'll use P. Which I use P, using the likelihood, which I have a little curly L, and a prior distribution. So, with those two things to find the likelihood and prior, the job of parameter estimation is to figure out the posterior on the left-hand side. But of course, we also have another thing that we do in inference, which is our model selection. So, when we go and add a normalizing evidence in, so I'm just taking that. Evidence in. So I'm just taking that proportionality on the last slide and dividing through by Z, which is what we call the normalizing evidence. And how do you calculate that? Well, it's just the thing, right, which integrates over the numerator to make sure that the right-hand side is a normalized probability distribution. So once you can calculate evidences, you can do model selection in exactly the way that Xiong was telling us. You have base factors, you have prior odds, and you have odds. Factors, you have prior odds, and you have odds and all of these things. Okay, so that's our notation out of the way. So in gravitational wave astronomy, why is it that we need, you know, why do we use Bayesian inference? Why aren't people doing maximum likelihood analyses? You know, why do we spend so much time on computers burning out clusters to get posteriors? Well, there's a few reasons, and I'm going to highlight the three, which I think are key. I think are key. The main thing, right, is that we have very non-linear correlations between parameters. So, for example, here I'm showing the mass posterior for 1708.17, and you can see that there's a sort of large curving banana, which is all of the points which are equally likely to within some credible level. And if you just do a maximum likelihood, you'll say it's here, maybe it's somewhere up here. You know, maybe it's sort of somewhere up here, but you won't realize that there's a huge sort of space of points that could equally well explain the data. And that turns out to be really important when you're trying to measure the maximum mass of a neutron star or something like that. The second reason is we have informative astrophysical priors. So a good example of that is when we talk about the distance to a source, we know that that distance, you know, the prior distribution for that distance is not uniform. Distance is not uniform, and over short, you know, small redshifts, it's something like a power law distribution that goes like the distance squared. And we can fold that information in to improve our parameter estimation. And then the final part is that, you know, our single event inference, which is predominantly the topic of this talk, fits into a larger body of sort of hierarchical frameworks for population modeling. And Zion gave a really nice overview of how to do that for EM counterparts, but of course, within. EM counterparts, but of course, within gravitational wave astronomy, we do that as our sort of population models as well. Okay, so then what's the sort of bread and butter of this field? It's all about stochastic sampling, at least for now. What is stochastic sampling? Well, that posterior distribution we approximate with a set of samples. So, in particular, what I mean by this is this here is the thing that we want, right? It's you know. Is the thing that we want, right? It's you know, it can be a closed-form solution if you can do the maths, but it turns out it's much too hard to do the maths in most interesting cases. So instead, what we do is we draw a set of posterior samples which have a, which are drawn in proportion to this posterior. And what that means in practice is that when you bin them into a histogram, they're at the peak, you know, the peak is at the sort of maximum of. Peak is at the sort of maximum of this distribution and so on and so forth. And you can do things, right, like you know, take the average or take a credible interval, you know, whatever you need to do. We also estimate the evidence numerically, usually using some sort of Riemann sum. So just summing up things and adding things together to get you one number and getting that evidence out. And there's a variety of different ways to do that. I'll touch briefly on it throughout this. Briefly on it throughout this. Okay, so then what's the sort of history of this field? Well, it goes back a really long way. It's been fascinating for me actually to go digging back to the literature as I got more and more involved in this field and finding the first papers and sort of reading about it. I think it's actually really quite illuminating, especially seeing where we're at today with it. So, as far as I'm aware, and if I'm wrong, please do let me know. I've been wrong in the past. The first approach. The past. The first approaches came from Nelson Christensen back in 1998 using what's known as a Markov chain Monte Carlo approach. And I'm going to talk a little bit about that in a second. So I'll just sort of name it for now. And then there's, you know, many papers doing MCMC in the meantime and looking at different kinds of gravitational wave events. And then in the next sort of development, I guess, in terms of sampling came in 2008 when Came in 2008 when John Beach introduced the idea of nested sampling. Now, these two, MCMC and nested sampling, have been the sort of two dominant ones used throughout the field for, well, since they were introduced. But I will also add that there's a sort of grid and sampling hybrid approach that's named Rift. And that was introduced in 2015 or so and is used to this day. Now, I'm not going to talk a lot about it because it's not my wheelhouse and I don't understand. Because it's not my wheelhouse and I don't understand it, but it certainly exists and is useful. So, why, you know, why are MCMC and Nessa Sampling so well used? Well, it's predominantly due to the development of what's known as the LATA inference software, which was introduced by John in 2015, or at least the paper came out in 2015. There'd been a lot of development, of course, before that. And Laminference was used as the sort of flagship software between 01 and 03. Between 01 and 03. And it offered sort of MCMC and nested sampling packages in tandem. There are actually other packages in there as well. And I'm sure that there's many people in this audience who know a lot more about this history than I do. I sort of learned it by piecemeal and digging through the code and finding references to things called Bambi and all sorts of interesting ideas. Now, LAN inference remains in active use to this day, and it's a really To this day, and it's a really well studied piece of code, very robust, and will continue to be used for many, many years, I'm sure. There's reports of this to sort of do TGR tests that I know will remain sort of in use by the collaboration. I know there's some people sort of tinkering with the idea of developing on that, but these are quite well-developed codes as well. So, my guess is, I mean, I certainly can't say this, I don't speak for any of the team. This, I don't speak for any of the TGR people. My guess is we'll see that hanging around for quite some time because it gives you a real sort of framework to sort of go back and test things where you know you can trust it. Okay, so I mentioned I was going to sort of explain a little bit about nested sampling versus MCMC, and I do it as a sort of very high level. I don't want to get into the nitty-gritty details of this, but roughly speaking, they're two different approaches to the same problem. To the same problem, which is you want to find the posterior distribution, and you also want to find the evidence. That's actually what's not written on this graphic here. The MCMC route of doing this is you sample directly from the posterior and you sort of do a walking about of the space and you pick points in proportion to the posterior density. And so that directly gives you samples. Directly gives you samples, the output of it is a set of samples by itself. Nested sampling takes a different approach, in which, sort of, roughly speaking, you break the space into sort of slices, and then you sample from each slice, building up as you go the likelihood. So it's actually the likelihood contour which defines these different slices. And then at the end, what you can do is you can combine all the samples. You can combine all the samples that you've got into a posterior distribution by weighting them appropriately. Now, what's really interesting about these two approaches is that they sort of flip what they're trying to do on their head. So, MCMC is all about approximating the posterior. And in its sort of naive version, it doesn't compute the evidence. Now, there's a thing called thermodynamic integration that you can do and other methods as well that allow you to calculate that evidence, but it's kind of That allows you to calculate that evidence, but it's kind of a byproduct, if you like, and not the main focus. On the other hand, nested sampling, the main focus is you calculate the evidence, and then the byproduct is the samples. And this has an interesting sort of consequence, which is when you might ask, well, which is better? Which one should I use? And my answer is both. It's actually really important that we have both of those types of codes so that we can cross-check. So, that we can cross-check between the samplers and ensure that whatever we're finding is not, we don't have sort of errors from the samplers themselves. And that's been really vital throughout the field, just to make sure that we're not hitting some stochastic sampling issue. There is a good argument that the Lao inference MCMC was definitely easier to parallelize, looking sort of historically, and it was used in a highly parallel environment and has been right up. Environment and has been right up to the most recent publications. So, what you can do is you can run sort of tens or even hundreds in some instances of independent jobs and just combine them. And this fits the high-throughput computing model used by the LIGO data grid, which is one of the reasons why this is a well-used method of getting samples quickly. As I mentioned, though, the two different ways that they approach the problem turns out that in fact Turns out that, in fact, when you sort of get down to it, they're actually, you know, in some sense, one is better than the other, depending on what you want to do. If your core goal is to use the evidence estimates to say do model comparison, then you really need to use nested sampling. You can get sort of robust evidences with MCMC, but it's a lot more inefficient for most of the codes that we're going to talk about today. Meanwhile, if you really just want to know the posterior. If you really just want to know the posterior as efficiently as possible and also as robustly as possible, MCMC tends to be better. Nested sampling has a sort of failure mode where it can overconstrain the posterior. And if your sampler is well set up, that won't happen, but it can happen. So those are my sort of pieces of advice. If someone's doing one thing or the other, do what it's designed to do, I guess. Use the code that's designed for what you want it to do. Ideally, Want it to do. Ideally, calibrate both of them and make sure that both your evidences and just your posterior samples are robust and accurate. Okay, so that's the sort of history. You know, land inference is still a well-used piece of code, but you'll know, and people have talked about this already, that there's sort of new codes on the table. And the first question you might want to ask is: you know, why? You know, why bother with any of this? Well, sort of from a Well, sort of from a user's perspective, it's C-based and it's pretty hard to get your hands into and change something. You want to go prototype a new way of sampling or you want to add some parameters in. It's kind of a headache and you kind of have to know someone who knows someone in Lalinference to get some help with that. Okay. The second thing, which is kind of pragmatic, and I put this almost sort of as a joke, which is that the developers of Landfranc, the core people who really know what they're doing. The core people who really know what they're doing now largely have permanent positions, and sort of it was difficult to get people to develop things. So there's a pragmatic aspect to it as well. So what happened then? Well, there was a host of new approaches which have been developed. So there was PyCVC inference, which of course grew up the PyCVC search pipeline. There's a code risk called Buildy, which is my own project. So that's what I'll mainly. So that's what I'll mainly be talking about here. There's also one which is Bahez. I'm never quite sure how to say it. I think that the Y is a soft Y. And also GW model, which has not yet come out, but as certainly something which has existed for some time, which has come out of Walter's group. So there's probably other things as well that I've missed, but largely speaking, these codes try to develop a land inference-like solution. Like solution, but with either using sort of off-the-shelf samplers that have been around for the last 10 years or so, or developing things sort of in-house. But mostly they're written in Python with the sort of goal being this is something which you can flexibly use and improve on. So let me say a little bit about Bilby. Not too much. I'll try to be brief. So this is now the package which the LVK has adopted for use in 03 onwards. Three onwards. It's sort of, you know, canny. I think you can rightly point a finger at it and say it's really just a big wrapper. It's a wrapper between sort of Bayesian inference concepts like likelihoods and priors and off-the-shelf stochastic sampling packages. So some of those you'll know, like MC, PyMultinesk, all of those different things. When we built it, we tried to keep a couple of design principles in mind. So things like, you know, making it modular. So there's a separate GW and a separate core package. GW and a separate core package, and you can take things out and put new things in without breaking everything else. Consistency, generality, and usability. So, those are the kind of things that we try to focus on. And I think that this has worked out well for Billby users and the developers in that it's kind of lowered the barrier to doing this kind of stuff and doing new things. And it's also enabled people to sort of rapidly develop new ideas. And some of those I'll talk about at the end of this talk. And some of those I'll talk about at the end of this talk. So far, it's been used to analyze compact binary coalescences, continuous gravitational waves, fast radio bursts, GRBs, and all the other acronyms that you can think of. And so in this talk, I'll focus on the CBCs though. Okay, so what have we learned with Bilby? And the thing that I've come to very much appreciate is that very much the devil is in the details. It took about a few weeks to write. It took about a few weeks to write something which could sample from a 15-dimensional CBC problem. But it took really about two years of development before we got to the point where we could confidently put our hand on our heart and say, we really trust the results. And how did we kind of get to that? Well, we had to go through a process of testing and validating the software. And there's three kinds of tests, which are really important. So you've got the trivial sort of standard inference problem. The trivial sort of standard inference problems. So, a good example is something like the Rosenbrock likelihood, which is just a very curved, sort of banana-like likelihood, which is pretty hard to sample. You've got CBC inference problems. They might be going looking at real data or simulated signals. And then finally, you've got something called parameter-parameter tests, which essentially tell you how sort of confident you can be in your credible intervals. Now, there's a paper from led by Isabel Romero-Shore. Well, led by Isabel Romero-Shore and a few other PhD students at the time who were involved in the project. And that paper really did the legwork of validating Bilby and saying, you know, this really is something that we trust. Now, I want to be clear here, there's several packages which Bilby sort of gives you access to, but only the Dynasty package has so far been sort of fully validated. We expect that this is going to grow in the coming year or so, but right now that's sort of where things are at. It kind of goes to show how hard. At kind of goes to show how hard this can be. And the key to it, the sort of devil is in the details, if you like, is actually you need to show that your posteriors are what we call statistically identical to another sampler. And in this case, we, of course, used the Lalinference sampler. Now, I say this because if you look at this posterior on the right here, you'll see that the source chirp mass, you know, the histograms overlap, but there's little gaps, right? And if you look at the luminosity, Right, and if you look at the luminosity distance, you'll actually see there's a little sort of shift there. And you might sort of, you know, you might sort of look at this and say, oh, well, you know, that's not right. You need to rerun it. And then you'll rerun it and you'll get something slightly different. And it turns out that, you know, just with the number of samples that you have can, you know, can tell you, if you like, how close those two histograms should lie on top of each other. It's sort of Poisson statistics, if you like. And we developed a method to actually determine when these two things are really statistics. When these two things are really statistically identical to within sort of sampling errors, and when actually there's something more pernicious going on, and maybe you have sort of a bias or something like that in there. And in doing that, we really found a lot of bugs in Bilby and one or two in Lalanference as well. So that was a fun sort of game to play of digging through those things. Okay, so now I want to move on to, I guess, you know, now we have Bilby, what do we want to do with it? What do we want to do with it? And mainly, how do we prepare for O4? And really, this comes down to we need to optimize everything. And I'm going to try and go through sort of how we're going to do that and how we might get to something which is a little bit faster and produces results a little bit in a shorter time. But first, you know, why do we need to do this? Well, typically, if you want to sort of evaluate a CBC signal, you need to evaluate the likelihood about 10 to the 8th. Likelihood about 10 to the 8 to 10 to the 9 times. That's a rough ballpark. It's pretty hard to get below that, but I'll talk about some ways you can try later on. Now, the likelihood itself takes about a millisecond at best to evaluate, but it can take as long as a few seconds. And that mainly depends on the waveform model. So if you're using sort of a rapid waveform model, milliseconds, if you're using the really, really expensive ones with all of the physics in them, it takes a few seconds. Physics in them takes a few seconds. Now, if you just multiply those two numbers together, what that tells you is that the amount of time that you need to analyze one of these problems is something like days to years. And I try and always keep this in the back of my mind because often we have students who will start running an analysis and they sort of say, oh, it's been running for a few days. And I ask them, well, how long was the evaluation time? And they say, oh, it was about 100, 200 seconds. And that means that you're going to wait. And that means that you, you know, you're going to wait sort of most of your lifetime for the thing to finish. So you have to be careful and keep an eye on that evaluation time. Why is this important? Well, as has been mentioned in several talks already, when CBC systems contain a neutron star, the observers want the best possible sky map as soon as possible. Now, Deep has talked about this and others. We need those, you know, we talked about. Though you know, we talked about, I know there's been sort of a talk about the rapid sky maps which are produced. Here, I'm talking about the fully calibrated kind of inference sky maps, which would take something like a day at the moment. And we'd like to try and reduce that so that we can actually produce something which is, you know, the most reliable sky map that we can, but ideally a little bit quicker. Another reason that we want to optimize is that, as I already mentioned, sort of better waveform models, you know, sorry, waveform models. For models, you know, sorry, waveform models with better physics tend to be more computationally expensive. So we need to optimize so that we can use better waveform models. And this actually sort of relaxes the constraints on waveform developers. If we say, well, you know, you can get away with a half second or so. They might chuck a bunch more physics in their waveforms. And as a consequence, we'll squeeze a little bit more science out of the data. So that's why it's good to optimize there. And then finally, in 04, we're going to have maybe hundreds of CVCs. Have maybe hundreds of CVCs to analyze. And we really need to optimize just so that we can analyze them all in a meaningful way and do all the things that we want to do. Okay, so how do we optimize? The most obvious thing to look at is, well, the likelihood, that was the thing that you said dominated the wall time. So what can we do to speed that up? The waveform, as I mentioned, tends to be the dominant effect. So for binary black hole signals, For binary black hole signals, it's typically more than 90% or so. So it's entirely all about the waveform generation. And that's because you have sort of a smaller amount of data. So the overheads with the detector and summing things, they don't tend to dominate. For binary neutron stars, it can be more like 50% for a rapidly evaluated waveform. So a good example is Taylor F2, which is a very rapid waveform with. Rapid waveform with not as much physics as some other waveforms. But if you sort of look at the timing of this, you can see that the per-likelihood evaluation can be 60 milliseconds and the waveform is only about 15 seconds or so. So about a quarter. So in that case, it's actually 25%, but more typically, so for something like IMRPANOM PV2 in our tidal, it tends to be around the 50% level or so. So there's a few approaches available to try and Approaches available to try and reduce the time it takes to evaluate the likelihood. The first is known as reduced order quadrature. I'm not going to go into the details of how you do any of these, by the way, just sort of pointing to them. The nice thing about reduced order quadrature is it's very well developed. It's been around for quite a long time and been used to quite a significant extent. And there's real development in this even in the last year or so. So Sichiro Murasaki has been. Tichiro Murasaki has been creating what he calls focused reduced order quadratures. And these allow you to actually speed things up to almost a factor of a thousand or so, which allows you to take the sort of a likelihood evaluation times for BNSs down to the milliseconds or so. And so we're actually seeing people analyze binary neutron stars in something like a few minutes, which is just wild and really exciting. And we're yet to know if that's going to translate into something we can. If that's going to translate into something we can do online, there's lots of tests and verification needs to be done, but that's, I think, something super exciting. The downside to these ROQs is you have to generate them in advance, but there's lots of work in trying to develop that. So I think that that's a good option for the future. The other one, which is really interesting, is something called the heterodyne likelihood or relative binning approach. Now, this was sort of popularized recently by Zaki et al. from the IAS group, but in fact, From the IAS group, but in fact, the sort of method goes back to Neil Cornish from 2010. What's really nice about this is that you don't need to do anything in advance. You sort of take a mean likelihood, sorry, mean waveform, and you look at perturbations around it. And that really speeds things up quite significantly. But we're not in a place yet where that is sort of robustly tested. So I think it's promising, but not quite there yet. So I think there's one to watch, but it needs some work. There's one to watch, but it needs some work. So, both of them sort of no silver bullet because you either have to sort of develop things in advance or you know, well, they need verification, let's put it that way. Okay, what else can you do? Well, the other thing which we've looked at most recently is, well, you can actually optimize the priors. So, this is not the time it takes to evaluate the prior, but rather which prior you choose. So, for an astrophysical application, your priors, you know, they really need to be your prior below. They really need to be your prior belief about the object before you see the data. But if actually, if you sort of look at Bayesian inference through a different lens and say, well, I'm using Bayesian inference to produce a sky map as soon as possible. I don't really care what the prior is, so long as my sky map is robust, then we can optimize the prior. We can actually, you know, pick a different prior. And so this was work done with a PhD student from Monash, Tixing Yu. So we looked at essentially, you know, what can So we looked at essentially, you know, what can you do with the prior to speed things up? And it turns out there's a few different things you can do. And the first is you can, for binary neutron stars and NSVH systems, where we think sort of astrophysically that they're not going to be, you know, massively spinning, they sort of can't be, and they're also very likely to have nearly equal masses. You can choose a prior which says, okay, well, forget about spin and set the two things to have an equal mass. And if you do. Have an equal mass, and if you do both of those two things together, you get a speed up of about 50 percent. Now, this is, of course, you know, it's only a factor of two when the relative binning and things, they give you a factor of a thousand. But the nice thing is, is you can do both of these things in combination. And in fact, by optimizing the prior, you make it much easier to do the reduced order quadratures and things like that. So, this gives you an extra factor of two, in effect, onto the factor of a thousand you get there. So, that might mean that rather than it. So that might mean that rather than it taking an hour, it might take half an hour. And from what my friends with telescopes tell me, that's important. For binary black hole systems, what we found is that it's pretty unwise, in fact, to do this. And that's mainly because we think that, you know, binary black holes, well, we know in fact that binary black holes can have significant spins. And so if you start to sort of, you know, monkey around with the priors, you end up, in fact, giving yourself quite bad biases in the sky map. But that's not. In the sky map, but that's not the case for binary neutron stars. You really can get the exact same sky map using this sort of optimized prior approach. Okay, so then the other thing that you might want to do is parallelize. And I want to sort of first give some context to this. So there's more than one type of computer which is available. And especially when we think about sort of big clusters, there are two kinds which you sort of want to keep in mind. kinds which you sort of want to keep in mind. The first is high performance computing, so HPC, and the second is high throughput computing. Now, typically for those of you sort of within the LIGO collaboration who are used to using the LIGO data grid, that sits in the HTC environment. So it's, you know, I want to run lots of jobs that are completely independent of each other, but I want a thousand of them to go through at once. And the open science grid is completely high throughput, there's no communication between the jobs. No communication between the jobs whatsoever. By comparison, HPC, it's a large cluster, but with lots of interconnect. So lots of, sorry, a good interconnect speed. And this really suits sort of applications where you need lots of communications between the nodes. So these two things are different and we can use them in different ways. So one way to speed things up is just let's just chuck everything, you know, all of the cores that we have at the problem. Have at the problem. And it turns out that the nested sampling algorithm really benefits from this kind of parallelization, which is a HPC kind of parallelization. So you're actually using a large cluster, if you like, all at once. And so we introduced this with some work with Roy Smith, this piece of software, which is called Parallel Bilby, which is a kind of hacked together version of Bilby, which can use an entire cluster at once. And it uses the message passing. And it uses the message passing interface or MPI to do this. And we actually managed to sort of optimize it to the point where we can get near linear scaling. So, what that means is, you know, if you have 600 cores available, you get a speed up of almost 600 cores. It's more like 500 or something like that, but it's pretty good going. And sort of what it means in practice is that for an event like 1904-12, which was on our first sort of asymmetric mass events, the best we could do. Events, the best we could do using sort of standard analysis would be to use something like 16 cores. That's what's typically available on a laptop or a high performance laptop. That analysis would take about three years, which obviously, if you sort of put that to the outside community, they wouldn't want to wait three years for LIGO to analyze the data. But instead, by using an entire HPC cluster with 640 cores, and we managed to do that in 12 days. And one of the things That in 12 days. And one of the things that I think was pretty cool about this was they actually used a cluster in Australia called the Austar cluster, but it wasn't the main part of the cluster that we used. It was an old sort of student environment that was used for testing, but they were happy for us to take over the entire thing. And that really, you know, sped things up quite unbelievably using this pretty old architecture. So really nice use of sort of existing environments. But of course, the downside. But of course, the downside with this is you need to take over the whole cluster, and that can be pretty hard to do. And it can upset your colleagues who maybe don't want you to take over their cluster for many months at a time to do your analyses. So this is where the sort of second type of parallelization comes in. And it's sort of very key because, you know, for a lot of people within the gravitational wave community, they have access to the Open Science Grid. And if you're part of LIGO, And if you're part of LIGO, you know, I know I say this is completely LIGO-centric, but I know there's Virgo clusters and things like this as well. Those are predominantly HTC environments. So we can't take over the whole of those clusters. So what do we do there? And what did people do before they had parallel Bilby? I mean, we certainly have already mentioned this. So MCMC, as I already discussed with the Lanference MCMC, is trivially parallelizable. You can just run the same job. You can just run the same job many, many times, and each job produces a number of samples. And that can be pretty small. You can just get 100 samples or 10 samples from each, or even in the extreme case, you can just get a single sample from each and then just combine them all together. And this is being used with great effect by Lanon from 7 CMC, as I mentioned. I will say that there is a limit to how much you can do this, or maybe let me put it as there's a limit to sort of. Let me put it as there's a limit to sort of where the efficiency comes. So, if you go to the extreme case where each one runs and only produces a single sample, that turns out to be very, very computationally expensive. You effectively waste a lot of computing time burning through sort of bits, what we call the burn-in, and then just getting one sample and closing. So, it's much better to go for something which is a bit more balanced. So, produce something like 500 samples from a few. Samples from a few chains rather than run 100 chains which only produce 10 each or something like that. Now, by comparison, nested sampling, I've written that it's just nested there, apologies for the typo. This is typically parallelizable. So you can do the same game where you just run them independently. The trouble is, is you can't stop them early. So that's what you can do with MTMC: you can just stop it when you have 10 samples, right? With MTX, with nested sampling, With MCX, with nested sampling, you really need to run it until it's completed to believe the result. And so the problem then is that yes, you can triplely parallelize it, but it doesn't reduce your overall runtime. Your overall runtime is still the runtime of one single analysis. There is some ways to do this, but it turns out that they're not great. So if you're using nested sampling, what you really want to do for parallelization is the sort of HPC method, which I talked about with parallel Bilby, that can reduce the wall time. That can reduce the wall time, but this kind of HTC environment can't really help you very much. Okay, so that's a way to optimize with parallelization. And then the final kind of parallel optimization I want to sort of discuss is improving the convergence. And what that really means in effect is reducing the number of likelihood evaluations that you need. So I said at the start of this section that it takes like 10 to the 8 or so likelihood evaluation. Or so likelihood evaluations. If you can reduce that to say 10 to the 7, you know, you speed up your whole analysis by a factor of 10. And it turns out that this is, you know, the way to do this is to use your brain and not just sort of, you know, chuck things on clusters and try to power through, right? So there's a few ways you can do this. The first is what's known as analytic marginalizations of parts of the likelihood. So there's a long history of this that goes back quite far. And there's so many references that I've linked. And there's so many references that I've linked to a review instead, which does go through all the references. But these are things like phase marginalization, time marginalization, and distance marginalization that can allow you to sort of reduce the dimensionality of your problem. And the nice thing about those sort of tools is that you can analytically marginalize over them, but then at the end of your analysis, you can resample and pull them out. And so that's a trick that's been used quite a lot recently. That's been used quite a lot recently. And I think in the most recent TWTC3 catalogue, we at least use distance marginalization, but then the posteriors have the distances in them. So it's not something that we just throw away. We do recompute those. The other thing you can do is improve the sampling efficiency. Now, there's a few different ways to do this, but most of them come down to changing the way that you propose new points when you're sampling. And we've already heard Socion did. And we've already heard, so Sion did also a great job of going over Nessie, which is Michael Williams's code, and that introduced a really novel way to do this, which is to use machine learning. And I'm going to talk a bit more about how that works in a few slides time. The other thing you can do is to use sort of proposal densities which know about the problem that you're looking at. And so, in our case, for CBCs, there's huge sort of degeneracies in, for example, the sky, which you can actually tell the sampler. Which you can actually tell the sampler about. And you can say, well, when you're picking a new point, rather than just jumping randomly in a sort of Gaussian or something like that, go along this curve, which is, you know, you analytically calculate, and that will improve things. And that was something which is actually, you know, developed early in the loud inference process and is used with great effect by the MCMC sampler to improve the efficiency. Okay, so some takeaways then. So in O4, you know, I think that we could achieve. Before, you know, I think that we could achieve posteriors with binary neutron stars and NSDHs. So these are full posteriors using calibration and everything in less than an hour. We're not at the level yet where we can confirm this. That's ongoing work. But the main speedups, so the reason I'm sort of confident we can do this is the main speedups come from the likelihood optimization. So either the focus reduce order approaches or the heterodyne approach. And then there's further optimizations by being clever with your choosing. Optimizations by being clever with your choice of waveform and your choice of prior as well. So that's something that I think we should push on and as a community, try to drive to reduce the times from the day or so that we saw before to something like an hour. The other thing, which I think is worth saying is, you know, we have new modern inference packages, but these modern infranc packages lack a, they should have said Lalinference MCMC here. Apologies. CMC here. Apologies. I added this in the last few hours or so. So, we, in our new sort of approaches, we don't yet have something which is like LAL inference MCMC. And that's what I'm going to try and talk about now. And the reason this is important is that we can use MCMC to sort of take advantage of HTC parallelization and also improve convergence with sort of GW-specific jump proposals. So, you know, how can we sort of fix this? How can we right this wrong? This? How can we write this wrong? Well, what we did when we tried to write this was we went and looked at the typical packages which are out there in the community. And there's MC and TEMC are the two which have been used quite considerably within gravitational wave astrophysics. And when you look down the list of sort of features that they have, so Lalin, for instance MCMC doesn't have what's called the ensemble component. And that's sort of an indication that you don't need an ensemble to sample gravitational waves. To sample gravitational waves, but it does have what's called parallel tempering and these GW-tuned proposals, and it has, of course, been validated against the NEST version. When we look at MC and TEMC, which are the two, as they say, the main packages which we were under consideration, they were both what's known as ensemble samplers, but only TEMC gives you parallel tempering. And most importantly, neither of them give you these GW tuned proposals. Now, I will say that, as I say, neither of them. I will say that, as I say, neither of them, when we tried to cross-check them against other samplers, neither of them, at least I could get to produce results which were unbiased. However, it's worth saying that PyCVC Inference has been using TEMC for quite some time. And so that has been validated by PyCvC inference. But here I'm talking about the Bilby implementation, let's say. So what do we do? Well, we developed what's known as Bilby MCMC, and this was sort of conceived to fix these issues, right? Fix these issues, right? So, what does it have? It does have an ensemble, but in fact, we don't use it because it's not very effective. It does have parallel tempering. We know that we need that. And it does have these GW tuned proposals. And it has been validated in a limited sense against the dynasty sampler and using the sort of cross-checks that we used when we validated that. A full validation is yet to come, though, of that sample. But okay, so what is it sort of introducing? So, what is it sort of introducing? Well, it's introducing effectively, you know, it focuses on this idea of better proposals, and it does this in two ways. So, one is it implements the GWTune proposals, which Landon France MCMC has. And the other thing, which we did with it when we wrote it, was to introduce machine learning proposals. And this was very much inspired by Michael Williams' Nessie code. It sort of came out whilst we were in the development phase, and it was an obvious, oh, let's give that a try. And it turns out Oh, let's give that a try. And it turns out these really improved things. So, we actually use a mixture of different machine learning approaches: normalizing flows, kernel density estimates, and Gaussian mixture models. And what those do is they use the past distribution of the Markov chain, Monte Carlo chain, to actually learn an efficient proposal density. And formally, this breaks the Markovian property, but it turns out that if you do it sort of sparsely enough, it's okay and you can check the bias. It's okay, and you can check the bias and things like that. Now, what I think is really important with this is mainly that this is going to allow people in a HTC environment to repeat, you know, to use low inference MCMC-like approaches where we spread the load over many hundred jobs. And that's what I'm sort of most excited about with this, I guess. Okay, so now let's talk about a few sort of takeaways from the sampling discussion. So there'll be Discussion. So, Bilby is a new sort of flexible Python-based code to do sampling. It has long had a validated nested sampler, but we now have an MCMC as well, which I think it does the job as we need it to do. And so I'm hoping, at least my sort of work for myself is in 04 to try and push that ahead so that we have both of those operating in a sort of cross-checking way. Way. Now, the thing that I want to now move to in the last, I think I've got sort of five minutes or so left, is to talk about the fact that, you know, there's more to just writing a flexible code like Bilby than just using it, you know, with sort of the methods which have been known about for a while. The real motivation to do this was to develop new ideas and prototype those sort of quickly. And so in the rest of this talk, I want to talk about a way that To talk about a way that we're using Bill BMCMC to develop a new approach to modeling uncertainty. So let's just have a little second about modeling uncertainty. So going back to my first few slides, we had in that, you know, posterior, you have P of theta given the data and given a model, right? So whenever we're talking about Bayesian inference, we need a model which makes quantitative predictions about our data. And where do we get those models from? And where do we get those models from? Well, you know, numerical relativity is the really only way to sort of fully model a CBC merger right from the in-spiral through the merger and the post-merger. Of course, you could break those bits up and do some of them without numerical relativity. But it's not feasible to model the entire signal. So it gets broken up and then different approximations are used in different parts of it. And there's different sorts of ways to stick those. Different sorts of ways to stick those bits together. And so, what you end up with is different families of models and multiple different models which make different predictions and are validated in different ways. And so, what I'm going to refer to this as systematic waveform uncertainty. And so, this is kind of the, I think for me, this is one of the most exciting things that happened in 03 was we started to see events for. We started to see events for which the different waveforms actually didn't quite agree on the properties of the signal. And that tells you that this uncertainty is now actually resolvable. And it's starting to tell you something about the signal and the physics of the signal itself, because you can go and dig into those waveforms and say, well, what did they actually do to stitch all this together? And, you know, is one of them better than the other? And maybe we can learn something from that. So, how is that current? So, how is that currently done? Well, as of the latest catalogue, even the current approach, and this has been true right from the first catalogue, the current approach is that you analyze every event with at least two waveform models, and exceptional events get more waveform models, but for the catalogs, they do two. And in practice, actually, because of the difference in cost of the waveform model, so you tend to do one of the Tend to do one of the IMR, so interviral merge or ring down models, and one of the EOB-based models. And I know that for some of you, that won't make much sense, and for some of you, you'll be very familiar. So apologies for the slipping into the LIGO lingo for a second there. But it turns out that the IMR model is much faster, and the EOB model can be a lot slower. And so, in order to sort of have both waveforms analyzed, we tend to use two different stochastic samples. To use two different stochastic samplers to do that. Now, once we've analyzed both, you know, the event with both waveform models, you then sort of compare between them to look for cases where they disagree. And that's referred to as sort of waveform systematics. And that's, you know, trying to find some sort of difference between them. Once you sort of say, well, okay, you know, there's no particular reason why one of them, you know, you rule out a bug or something like that, you then say, well, okay, they don't. You then say, well, okay, they don't agree. And so, in order to sort of capture the waveform systematics, what we'll do is we'll combine equal numbers of samples from each of the waveforms, and we'll produce a mixed posterior, which captures the uncertainty. And most of the plots from our catalog papers use this paradigm. This is, you know, the plots are produced from these sort of mixed samples. The idea being you're marginalizing in a naive way over this uncertainty. Over this uncertainty. But is this the best that we can do? Well, it turns out no, we can do a little bit better quite easily, which is, well, actually, you know, as I introduced at the start, right, if you're doing your Bayesian inference properly, you also have an estimate of the evidence, right? The evidence tells you how well the model explains the data. And so you can use this to actually weight posteriors by. Weight posteriors by how well they fit the data. So you can, of course, calculate base factors between two models. But if you want to combine posteriors together, the thing to do is you define a weighting parameter. So it looks like this. So psi i. So this is for the ith waveform, if you like, that you want to fix together. You calculate the evidence. So m i here is the waveform model. And then you divide through by the sum of the total evidence. And so, you know, if they're equal. Evidence and so you know, if they're equal, you know, if both of them explain the data equally well, then these xi's will be 0.5, and you end up sort of putting equal numbers of samples in together, and that's the standard approach. But it turns out that, in fact, for almost all of the events that people have done studies for, there are differences. The base factors are almost never exactly one. And so you're throwing that information away, and you can not throw it away by using this approach. So you calculate these size. So you calculate these size and then you just mix them together with the right with these weights. Now, this is an improvement and something that I think would be useful to do and improve our estimates of the source parameters. But there's still in fact a few pragmatic difficulties with this. And the first one is, which is quite critical, which is that it relies on you being able to estimate these evidences very robustly. And so this is, I think, the This is, I think, the key one, right? Which is when you tie it into the fact that we use two different stochastic samplers often to look at the waveforms, do you really trust the evidences between them, let alone between the same sampler? I think those are sort of hard questions to answer. And then on top of that, there's really purely pragmatical issues, which is like, did you analyze exactly the same data? Did you use the same likelihood, the same priors, and the models, right? And all of those things can be. And all of those things can be small changes, but they end up with systematic effects, which would bias all of your results. So these are pragmatic difficulties with this approach that we'd like to fix. And how can we do that? Well, I think the approach that, well, at least the approach that I want to push forward is something which is called, which we're going to call hypermodel, or the use of hypermodels. So this is an alternative to evidence-based approaches. So at no point in here, do you calculate evidence. No point in here do you calculate evidences, but you do calculate odds and base factors. The way this works is you define a hierarchical hypermodel. So this is very much like what Siong was talking about earlier, but here it's a little bit different in that your hypermodel M tilde is a superset of many waveform models. So this one might be, you know, IMR phenomen PV2, and this one might be IMR phenomenon XPHM, you know, whatever it is. You stick them all in a bag and you What is you stick them all in a bag and you call them that's my hypermodel? And then during the analysis, you calculate the posterior of the waveform parameters, theta. And in this case, they all need to have exactly the same waveform parameters. That's what this green box tells you. And you perform analyses which effectively jump between these models. And it turns out that this is a specialization of what's called a reversible jump MCMC. And this is what we've implemented in built. What we've implemented in build the MCMC, and the flexibility allowed us to do this. Now, there's a few benefits to this. The first is it's an all-in-one approach, you don't have to worry about having different data. The second is you marginalize over M tilde kind of by definition. You can pull out individual waveforms and you can calculate odds between the waveforms as well. So, what have we done? We've applied this to the binary neutron stars that we've seen. So, binary neutron stars are a great place to use MC. Neutron stars are a great place to use MCMC because it's nicely parallelizable. And so, this actually hit the archive today. We got it out just in time for this presentation. And in this work, we looked at 1708-17, 1904-25. The typos are killing me, I'm sorry, and also 2003-11, which is a sub-threshold BNS. So it's not a confirmed BNS, which is reported in the GWTC3 catalogue. Now, the results which you found, we weren't expecting this. We weren't expecting this, but it turned out to be more interesting than what we thought it was going to be, which is we thought it were going to all come out sort of equal. But it turned out that when we looked at the four sort of cutting edge aligned spin BNS models, which are available right now, we found a consistent preference for the TEOB resum S waveform model. Now, the odds, and this is, by the way, not log odds, these are raw odds, go from about 1.7 to 2.3. So a factor of two. Now, Two. Now, the Bayesians in the room will be shaking their head and saying, Yeah, this isn't even worth reporting on. But in fact, the reason that we kind of got interested in it was we found this consistently between 1708-17 and 1904-25. So both sets of data told us the same thing. And that was, and in the same order as well. And that was sort of, it's not conclusive, but it's certainly tantalizing. We did some digging to figure out, you know, what part of the waveform model is doing this. And it looks like it's the tidal set. This and it looks like it's the tidal sector, but that's yet to be confirmed. The main sort of output of this is it actually tells you that the tidal deformability is a bit larger than what we think it is because TEOB Resum S predicts a larger tidal deformability. Okay, so then, you know, this isn't conclusive, but we need to wait for 04. You know, once we have a few more events, we're hoping with about 10 events, we can cement this result. So I think that's going to be something to look forward to in the future. To be something to look forward to in the future. Okay, so I'm actually going to leave that slide because I know I'm over time now. So let me just jump to the outlook. So I think this is a really nice, interesting, fast-evolving field. In 04, I'm expecting that those wall times that people look at as day or so are going to come down. And I hope this brings a lot of joy to our EM, to EM observers. Now, I think that's something I'm sort of going to signpost for a talk later on today, which is everything I've said is stochastic sampling. Everything I've said is stochastic sampling, and this is sort of remains the gold standard for doing analyses. But there's lots of interesting work in machine learning-based approaches. Um, Chris Nessinger is going to talk about this later today, and I'm looking forward to that. But for my take on it is these need to be demonstrated to work out the box before we can trust them. You know, once they can, stochastic sampling is going to go in the bin. We're not going to do it anymore. And we will use machine learning, but we're not there yet. At least that's my understanding. But maybe, Chris. That's my understanding, but maybe Chris will tell us differently later on. And then finally, my biggest concerns for the future are really about waveform systematics and non-Gaussian noise, which I haven't discussed and I hope to say more of, but I'm out of time, so I'll stop there. Thank you and apologies for going over. Thanks a lot, Greg. So do we have any questions on Zoom? On Zoom? Actually, I could ask a quick one. So, Greg, firstly, thanks for the great talk. Thanks for developing Bilby. I personally have used Bilby and paralleled Bilby in a few of my projects. And I would like to say that it's very well written and it's very easy to customize. And so that's so. Thanks again for. That's so thanks again for that. I was wondering that regarding your comment on the testing GR side, you say mentioned that the testing GR team will most likely be using LAL inference for maybe a few more years. And I'm wondering, is it because of the waveform development that is needed or something else? Or something else. And secondly, maybe a follow-up to that is: so if we, I know that in Build B, you can develop your own waveforms. So for example, if I have a new waveform in modified gravity, I could code it up in Build B, but one could also code it up in, say, LAL simulation. And I'm wondering which approach would you is recommended, or if you have any experience with that? With that, yeah. So, I think with the TGR part, I mean, the reason I say that, and again, I wish to say I'm not at all doing TGR work, and the people which are doing that, um, you know, I don't want to speak for them. This is completely an opinion, right? Um, but they've already invested in converting LAL inference to do TGR work. And so they've paid the cost, if you like. And I don't, I mean, I think that there's sort of, it would be good to port some of it. To port some of it to Bilby, and I think it would be a worthwhile thing to do. But it's a lot of work, you know, there's a lot of things that need routing through and sort of piping and things like that. So that's at least my sort of perspective. And I sort of put it in there mainly just to signpost that sometimes people sort of say to me, oh, you know, no one uses Land Inference, but they do, and we should use it. It's a really great and well-written piece of code. And I think it will be used for some time yet. Not really to talk about the TGR. I mean, I don't know if they will or won't. TGR. I mean, I don't know if they will or won't. Maybe they have some effort to move to Bilby. I don't know. In terms of why it is, I think that it's probably just the startup cost of doing it. I think that the waveforms and things are done through LAL simulation. So I'm sure that that could be done quite easily. And then to your second question, it's again one that I don't know a good answer to because I'm not a waveform expert. You know, I'd like to see a sort of high-level Python interface that people can develop waveform. An interface that people can develop waveforms in a lot easier, and that would be super useful. If you're sort of developing something, you know, I think that by putting it into Lou Simulation, you get a much larger exposure than what you would if you just sort of put it out as an independent package, if you like. But both would be possible and interesting. Right. But in terms of, say, speed or efficiency, would you recommend the other? Oh, so I think in terms of efficiency, I think if you're a way. In terms of efficiency, I think if you're a waveform developer, at least from what they tell me, you need to code in C or you need to code it in a low-level language in order to get the waveform speed. But then there's lots of ways to sort of wrap that into Python. And Laos Simulation does this already. You can call the waveforms directly from Python. So in terms of writing a package, you would do it in some sort of low-level language and then have something which just allows you to call from it. Sounds good. Thank you so much. So, do we have any questions in the room? Yeah, go ahead. Hi, Greg. This is Sean. Great talk. I just wanted to ask you quickly about some of the optimizations you guys are doing. So with this ROQ and the heterodyning, you get an enormous speed up, right? And you can possibly do it within tens of minutes. Tens of minutes, the first layer of PE. I got the feeling that this was mostly targeting sky maps to get the best sky map as early as possible. How accurate are the intrinsic parameter estimations in this fastest PE that you get? And what is the kind of time scale at which these intrinsic parameters will be something we can rely on? Will be something we can rely upon that these are the, you know, these are good for source classification. So, hi, Sean, thanks for your question. In terms of using the relative binning and the reduced order coordinators, you get both the extrinsic and intrinsic to the same, it's exactly the same if it's done right. It's exactly the same as if you'd run the whole program, you know, not using. Program, you know, not using the likelihood speed up, if that makes sense. So that's one of the key things that we sort of insist within the PE group: is that when you make one of these approximations, your posteriors shouldn't change and you need to show that it's identical. So those measurements would be exactly the same. Now, the question though is, do you, in doing so, make some assumptions about the waveform? So a good example is it's a lot easier to develop an ROQ for an aligned spin waveform. For an aligned spin waveform rather than a processing one. And so often you'll see people talking about speedups, and those speedups you'll realize them so long as your source is aligned spin. When in fact, if it's not a line spin, you'll analyze it and you'll just get out something which is not quite right. So you need to sort of be careful about what you expect your source to be. And then perhaps you need to make sure that you've sort of covered the region of parameter space outside. Space outside of where that might be, just to sort of guard against something interesting coming along. Great. So great, thanks. And I just, and what kind of time scales are we talking about for the alliance being run in O4? So, like, in O4, that's something that's ongoing work is to actually. Ongoing work is to actually demonstrate how fast we can really get this for realistic, the kind of signals that we expect. And also trying to figure out all the other bits, like how do you make the power spectral density quickly and all of these sort of things that we need to do. Now, I can tell you that you can, if you go and speak to Sichiro, you can run a binary neutron star in a matter of minutes quite easily. And it comes down to basically. And it comes down to basically being able to get the choke mass frame accurately and sort of using some other tricks. So I think that the best you'll ever see is minutes. You're never going to get below a minute or so. But realistically, I think that's where maybe an hour could be possible. But again, this is complete speculation and yet to be demonstrated. I think that's where it's really hard. You know, it's easy to run something which only takes a few minutes, but it's hard. Only takes a few minutes, but it's hard to demonstrate that that's really the right thing, and most importantly, that we trust the posterior as much as if we'd used the regular method. And that's what we need to do is the hard work to verify that. Thanks, Greg. Any last questions before the coffee break? No. Okay. Well, let's thank Greg again. 