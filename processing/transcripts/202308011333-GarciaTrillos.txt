Very happy to be here. So, yeah, okay, so I just want to start mentioning that, you know, this is, I'm going to present doing work with a lot of people, many of them, one of them is my student, another one is my brother. I have a lot of friends here. And it's kind of funny that I have a lot of friends and yet this talk is about adversarial training. But anyway, so okay, let's get it started. So, you know, by now, I think that we all know that. I think that we all know that there are tons of success stories of AI and this we can see scientifically but also just like the mainstream media. So I think that for language recognition, natural language processing, also even just our generation or say some creative process has achieved a lot of success. So here is one example of that. I just used David, one of the forums by OpenAI. OpenAI and this is like you it transforms text into images and I just put this as an input generating image representing the success stories of AI and it just produces. I don't know if it's good or not but it's supposed to be a creative thing, right? So anyway, so there are a lot of success stories, but of course there's also, I mean, as we hear good news, and probably we should also be concerned about it. And I think that especially in the past year with ChatGPT out and every With ChatGPT out and everything, it has become, I think, a little bit more mainstream that the dangers of AI, or at least some controls that you should have on AI. So I'm taking this example of the University of Bonn, maybe I think a year ago or two years ago, I was there, we did this workshop, and it was a mathematics workshop, yet we had a session where philosophers and law experts came just to talk to us about, you know, AI, etc., and then they're thinking about. etc and then they're thinking about you know kind of like how we can regulate or what is the kind of law framework or the philosophy framework that is more appropriate to you know in the in the in the context of of AI and machine learning data analysis it said two weeks ago you know if you go to the New York Times two weeks ago there was this announcement so Biden was meeting with some of the leading with the some executives of some of the leading AI companies and Some of the leading AI companies, and some of the things that we're talking about is: are we going to put some guard rates on AI? Now, of course, this is a little bit more like you know, some promises, there is nothing like, you know, regulation yet on site, but at least it's something that is jumping into mainstream E. So this idea of making AI controllable and putting guardrails is something that is kind of important at all levels of society, and of course, scientifically, it should be as well. As well. Now, I just want to highlight three things, like the three keywords that in that meeting of Biden and these AI guys. So they highlighted these three concepts, this concept of safety, security, and trust. And you can have a lot of examples of what it means a little bit more concretely. Last week, there was a workshop here on collective dynamics, and I was talking about privacy and federated learning and a little bit about. Federated learning and a little bit about systems being robust perturbations of data. Today I'm going to talk about systems, how to make them robust perturbations of data. What does that mean, mathematics? Okay, so let's say you can think of it as, I'm going to just talk a little bit about one of these things that we should try to think about if we want to deploy AI around. Okay, so a brief introduction of adversarial machine learning. So, you know, this is in the context of So this is in the context of supervised learning. So maybe you've heard a lot about neural networks and maybe you've heard that they're really great at classifying clean data. Sorry, at classifying things when they're trained with clean data. But I don't know if you've heard that they're also very, very sensitive to atmospheric attacks. And maybe this picture has already kind of become very popular. The idea is that although you may have a really accurate neural network, so that Neural networks so that you can predict input in a much degree of confidence, especially given the number of classes that you have. And yet, you can build a structured noise. It has to be structured. So it doesn't really matter what this formula is, but suppose that you compute this thing, it gives you some perturbation that is small in the sense that we cannot even distinguish the second image, the second panda, from the first panda. We know that both of them are pandas, yet the neural network. Them are paranas, yet the neural network is thinking very confidently that this is not a parameter but a given. So, this is kind of something that was noticed more or less around the same time as when neural networks became the main tool to do image recognition. And, you know, so this was in this workshop here. Okay, so this is just an observation. More bad news. Well, it's not just something that we can do like, you know, in the computer, like the question. Know in the computer, like the question that these guys were asking was: Can these attacks be deployed in real life? And so, then this experiment where they essentially located patches on stop signs, actual stop signs, and then of course they would try to locate these patches so that to full serial payments classifier. And they were successful at being able to do that. And of course, this was an example to say, well, this is not just something virtual, this is something that can happen. Virtual, this is something that can happen in the real world, especially in an application where you can imagine a self-driving car or something like that, not being able to recognize the stop sign because of some silly patches. This is potentially very problematic. And then, of course, people just keep playing with these things, right? And then they say, oh, how about we try to put some sort of camouflage on top of a car and see if the system that is trying to recognize objects in a self-directed car can be fooled. Can be fooled by this camouflage. And indeed, they can essentially make the classifier in this case believe that that car is a cake as opposed to a car. I don't know what that means, but in a sense, it's bad, right? You want to avoid cars. I don't know what the car should be with the cakes. Anyway, so this is just to say this is serious, this is important. And of course, we ask ourselves, well, okay, it's from a mathematical perspective. Case from a mathematical perspective, what can we do? And so let me start, and I guess this is the beginning, really, of the math talk with a model essentially. And this was actually a model for computer science. So the question was, if we want to train models to be robust to adversarial perturbations of data, how should we do that? And again, keep in mind, this is a structured noise, it's not random noise that we've put enough, depending against. And so then what Madrid and his team. Madrick and his team at MIT, what they came up with was: okay, let's write down explicitly this problem over here. And I just want you to notice the difference between the new kind of objective function that you're trying to minimize versus the standard one. So this is the standard one. We're just trying to do risk minimization on a model that is able to predict correctly the labels associated to the inputs that you have. But now we're That you have. But now what we're saying is: well, potentially, although the data point is x, y, there could be an adversary that changes the location of x and moves it to location x tilde, not too far away from the original one. So we're modeling that with this ball of radius epsilon. And now the idea is, as learners, you have to choose this data. We try to choose that data so that this worst kind of attack is we're protecting. Is we're protected against. Okay, so this is kind of the idea. Now, just to really, really quickly say why this talk is going to be about geometry and specifically about perimeter, let me just highlight the differences between the two objective functions. So the objective function number one in a binary classification problem just cares about how many mismatches there are between colors. So here the background could be what the classifier is predicting, say blue or red, in this binary case, and then the actual Binary case, and then the actual columns for the points are the actual labels. So you look at this classifier, you say how many mismatches there are, and that's essentially what this is keeping track of. Whereas now, when we look at the robust risk, when we add the action of the adversary, we not only care about mismatches, but we also care about points like this one that although have been accurately predicted, it's supposed to be blue, and it is predicted as a blue, still, Predicted as a blue, still, it has, you know, when you look at the ball of radius ups and went around it, it touches the other class. And what that means is that the adversary could move that point to the other side and achieve in this way a mismatching classification. Okay, so now we should care about boundaries and especially how big they are. And this is essentially conceptually why perimeter shows up in adversarial facts. Okay, so this is kind of like my short explanation of the talk, in a sense. And okay, but anyway. And okay, but anyway, so roughly speaking, of course, we care about how to find solutions to this problem. Again, the purpose is to be robust to adversarial attacks. If the problem is too difficult, at least get a sense of what upper and lower bounds we have in terms of the values that this thing can take, and maybe some guidelines as to how to construct those optimal robust classifiers. And then more explicitly, kind of like, you know, like. Explicitly, kind of like you know, like the comment that I just made about how adversarial training is related to regimeization methods, making this connection with perimeter minimization more explicit. Okay, so we'll see a little bit of that. And then just in summary, what I will be discussing for the rest of the talk is, you know, essentially I will be exploring this connection between adversarial training and geometric. I mean, just from a geometric perspective, I will highlight some connections with optimal transfer and perimeter minimization. And then also. And then also, I will be talking about two families of, you know, kind of two general families of models that somehow contain this adverse training as a specific case. So you can think of these guys as more general families that will have a structure, and we'll talk about those. Okay, so the first one is something called this regionally robust optimization. The other one is something relatively new that is called for realistically robust. And we'll see that all of these things have a lot of parameters, a lot of interesting optimization. Lot of interesting objects. Hopefully, I can motivate you to talk to me and investigate a little bit this type of stuff. So then to describe the first family, this DRO, and kind of give you some connection with optimal transfer and also talk about lower bounds, I'm going to introduce this problem over here. So I'll explain what I mean by agnostic learner view. I'll explain that. I'll explain that. So, the first thing is, I will introduce right away what do I mean by distributionally robust optimization. So, the idea is: if this is the original adversarial training problem, you can see that essentially the objective inside the integrand is like point by point. You essentially take x, y and then you produce for each point x, y, one, you know, this object over here. But now, what we're going to think is that the adversary is not just attacking point by point, but Just attacking point by point, but essentially, what we're going to think is that the adversary transforms the entire data distribution. So instead of having one fixed, I mean, the clean data distribution is moved here and also here, what I'm thinking now is that the adversary can change the distribution move to an entirely new distribution mu tilde. And the idea is that now, in order to do that, the adversary will have to pay a cost. So then we will model that. But this is a cost that has two inputs, two probabilities. Has two inputs, two probability distributions. So that's the first thing. And then the other term is exactly the same as before. This is just the risk associated to the new data distribution. And so what does the adversary want? The adversary wants to construct a new tilde that makes this risk the largest with the smallest cuts or something like that. And of course, the learner is trying to minimize that super. Okay, so this is the arrow. And now I'm going to give you the specific setting that I'm going to be interested in. So one thing that I should mention. Interesting. So, one thing that I should mention is that I was modeling the learner with parameters there, but just think about them as classifiers. And maybe I wrote this down over here, and you'll see some notation. So, when I write F1 through FK, I really mean soft classifiers. Those are things that assign to each point a probability vector, essentially. So, it's not hard classifiers. The hard classifiers would be you build partitions of the space, and essentially, you only give one to one class. One class and binary, well, binary, you just have to select the region because once you have selected, you know, how do you classify in class one, then the complements is classy. So, anyway, so that's that's what I'm thinking of here. Like we are looking at self-classifiers, but I'm also making one assumption. And I'm saying this is an agnostic learner, which just means mathematically, I'm just looking at the entire collection of measurable functions with this property. So you could say, well, but this is not the problem that people solve. Not the problem that people solve with neural networks, and I would say yes, but I'm trying to find lower bounds and I'm trying to find the analogue of what in statistics you would call the base classifier. Base classifier is essentially the thing that you would do if you had the model, the model, the data distribution, and you're just trying to find what is the best classifier without adversary. What I'm going to be talking about here is what is the analog now that you have an adversary. And this will always give you a level. Okay, so that's just one observation. Loss function just to determine how a classifier is. function just to determine how a classifier is performing at you know classifying a given point is just a relaxation of the 01 loss now that you have soft classifiers essentially if there is a data point x with label y you would like to give high probability to that label at that point x so this thing if this thing is close to 1 the difference is close to 0 and that's exactly what we're trying to enforce okay so this is the the setting and now our problem you know this the arrow formulation would look just like that Would look just like that. And now, what I'm going to make a choice now is, the choice I'm going to make is how to model the cost function. So, the cost function, and you will see kind of like some of the nice things that come with this, is modeled as an optimal transport problem. And now at this stage, I'm comparing two data distributions on pairs X and Y. So I have features and labels. But I'm going to make a choice for what is the marginal cost. Or, what is the marginal cost to going from Z to Z tilde? So, think about Z as a clean data point. Z tilde is what the adversary would like to do. So, we would like to move from Z to Z tilde. I'm going to model it in this way, where I say, look, if the adversary tries to change a label, it will not be able to do that. It would have to pick cost infinity. But if it keeps the label, the only thing I'm going to charge is how costly it is to move the feature vector x to x theta. So in the example that I was. So, in the example that I was considering at the beginning, where I had the two pandas, I'm changing the x's, right? So, x, the first panda, is one, and then x tilde is the other panda, but the labels are the same. They're both, they should be pandas at the end of the day. So I'm modeling it this way. And one of the advantages of doing it like that is that you can essentially try to understand the problem with what is happening in each of the classes. So, essentially, So essentially, this reduces to what happens for each of the classes 1 through k. So this is just from the fact that you cannot transport mass across different layers. Simple observation, so again, the agnostic problem always provides a lower bound. If we had chosen a different class of classifiers, then of course, well, personally, we're working with measurable functions, otherwise I can only do something very strange. And so this inf will always be smaller than this. Will always be smaller than this. So this thing is always a lower bound. So we can try to compute this quantity over here. And the question is, how to do that? Now, let me really quickly mention, I mentioned that the original adversarial training problem, the one that was modeled by Madrid and his team, is, you know, it was this expectation and then supremum insight. So what I'm going to claim now is that you can recover that as a specific example of this more general TRO set. The RO set. So informally, what you can do is say, well, as before, we have our cost structure like this, but now I choose my cost or features in this way. And essentially, what I'm saying is, if the adversary is trying to move point x to x tilde, it will be able to do it as long as it's trying to move it within distance epsilon. Otherwise, it cannot do it. And so, if you make this choice, then automatically you would recover the adversarial training problem. So, what I'm saying here is that this family So what I'm saying here is that this family of models, the DRO family of models, is much more general than what the original AT is. And so anything I can say about this family of problems, it will allow me to say something about this, at least formally speaking. Okay, now, on the other hand, what I want to highlight is the geometric structure of that problem over there. And here you're going to start seeing the connection with optimal transformers. So the idea is that you have the distributions of the different classes. This is class one. The different classes. This is class one or red, this is supposed to be yellow, and it's supposed to be blue. So, in this example, we have three classes. Let's just think about intuitively what the adversary is trying to do. So, you have those three classes, and as an adversary, to make the problem the hardest for the classifier, what you would like to do is put together, stack on top of each other, a lot of points from different colors. It would make it more difficult to distinguish. So, if I manage So, if I manage to move a red point, a blue point, and a yellow point into the same exact point, that would be the most difficult for the classifier. Maybe I cannot do that, but maybe I can move a red point and a blue point together, and maybe the green point or yellow point would be too far away, so it would be too costly to do that. So, you see, essentially what the adversary is trying to do is make a lot of overlaps, but of course, you will have to pay for them. And so, when you start thinking in this way, So, when you start thinking in this way, then, and if you know a little bit about optimal transfer, in particular about bar center problems in optimal transport, this sounds like you're trying to find a measure that you're trying to couple to these three guys, and somehow you have to pay a cost for them. Now, it's not exactly like that because, first of all, these measures don't even have to have the same total mass. And in fact, again, I'm not supposed to just map to a single measure, I will just pair. Measure, I will just pair things as much as I can. So, in fact, what I'm looking for is maybe a measure lambda, and I will be matching those three measures to parts of lambda, and then that will give me essentially what I would like to do. So, when you do this kind of analysis, you discover that the problem that I wrote before, this DRO in the agnostic setting, can be rewritten in this form. And this is what we call a generalized variation. And then from here, And then from here, you say, okay, so how can we solve this? So the thing is that what we show in our work with my student Jeff Jacqueline and my colleague Mark Jacobs is that this problem over here admits a representation as something that is called multi-marginal optimal transcript problem, where the number of marginals, in this case, is the number of classes that we have. And what turns out is that this is kind of in resemblance to a paper that was written about 10 years ago, like the morning, on 10 years ago, or even more, on the connection between the standard bar-center problem in optimal transport with multi-marginal optimal transport. So somehow these are analogs of each other. But now, again, remember, this is connected to this DRO formulation of adversarial training. So why is this useful? Because once you rephrase, once you make this connection between this adversarial training problem and MLT, then all the confusion. MLT, then all the computational optimal transport tools that are out there now become part of your tools, essentially. Now you can use computational optimal transport to compute these things. In this case, this level works. The specific problem, okay, it admits a certain expression. I don't think that I should go into the details of what is happening here, but the point is that we fully characterize solutions to the agnostic DRO problem in terms Problem in terms of the primal solution and the dual solution to a certain problem in optimal transfer. Okay, so maybe don't focus too much on the formulas. All I'm saying is that there's this bridge between these two things, and those two bridges would allow you to say if you solve a certain emotion problem, then you can just apply some formulas to obtain what your solution should be in the adversary of turning. So, this is the only thing that I'm involved here. Okay, and then with this, then you can. Okay, and then with this, then you can take arbitrary data sets, MNI, cyclar, etc., take specific norms, you know, distance to measure what the adversary can do, and then you can just, for example, build these plots that are over here, where essentially what this is representing is what is the minimum value. So basically, this is that lower bound that I was talking about for different values of adversarial budgets. So, how large these radius are for the costs that I mentioned at the beginning. So, you can do this. Mentioned at the beginning. So you can do this, and you can do this with computational optimal transport. And okay, so this is kind of like the first message that I wanted to. Okay, so then so far what I've told you about is this equivalence between adversarial training and computational OT. Keep in mind that this is in the setting, the agnostic setting. I'm not solving the neural network problem. This is only giving me some lower bounds that you can use for sure, but it's not that I'm solving the original neural network problem, for example. And I just want to And I just want to point out at this moment that this result that we have is a generalization of something kind of similar that had been done in the case of two marginals. But the case two, as many of you would know, and generally in life, right, from going to two, two, three, that's the bigger gap than many problems. So here is so from kx equal to two, it's very immediate. What you have to do from two to three. From 2 to 3 is not so clear. So, we consider the multi-marginal. Okay, so that's the first story that I wanted to tell you. So, you already see some connection here, right? Like adversarial training, and then suddenly it became like a very center problem in optimal transfer. Okay, this is interesting, and this is something I don't think obvious at first. But let me give you more geometry in these types of problems. Okay, so here I'm going to consider the binary. Are you going to consider the binary case? So, only two classes, and it's going to be the hard classifier problem. I mean, in fact, we can consider both of them, but remember that when you have the binary case, then if you're looking at the hard classifier problem, you can just reduce everything to a problem about finding sets, right? Just finding regions. And so it turns out, and this theorem is almost like a one-line proof because there is not much to it other than rewriting something. It other than rewriting something, but still, I think it's worth making this observation. If you look at the original adversarial training loss function, so the expectation of some supremum in the binary case, what you can do is rewrite it as a problem where in the first term I have the standard risk, the thing that you would do without any adversary, and then I just collect the rest of the terms and I call them epsilon times pair sub epsilon, okay? Pair sub epsilon. Okay, so I'm just giving it that name. Then you can, you know, a problem for sets, you can relax into a problem for functions. And now, you know, same thing, some sort of like risk or fidelity term, and then something that is Tb sub epsilon. So then what turns out is that this perimeter, epsilon, is a non-local perimeter. So it has all the properties of perimeter. TB epsilon is the associated total variation for functions. And somehow this is already telling you an Already telling you, and this was the observation I made at the beginning with my two pictures: well, there is some sort of parameter involved when you define this admission training problem. And well, here is more explicitly what is the penalization charts. Now, again, this is not super difficult. You just have to write it in this form. But of course, you can do analysis. And when we do analysis, we just understand what it means to divide zero by zero, right? So we say, okay, we have. So we say, okay, we have something that looks like this. Presumably, I'm taking epsilon to be small, and I have this functional over there. The total variation will be, you know, we're considered with a parameter for indicator functions. More geometric picture, really, really, what you're doing is you're looking just at the boundary of a set and you're looking at two alert neighborhoods of radius epsilon and essentially measuring according to each of them kind of a condition. Of them kind of conditions for each of the classes, class 0 and class 1, you're just measuring different parts with different measures, essentially. But essentially, this is what this parameter is doing. Now, of course, when you start thinking in those terms, what happens when absolute is going to zero, then you can ask questions like this. So, well, you can change a little bit of perspective and say, well, if I had the original decision boundary without adverse, and I wanted to robot. Adversary, and I wanted to robustify it a little bit. How would I do that? Well, that would be equivalent to saying if I start, if I consider like an initial value problem, I mean, this is formal, but imagine like an initial value problem where I start with the decision boundary of the best classifier without adversary. So you say, how should I evolve that boundary so as to construct solutions to all the adversary problems for at least some interval? So you can ask that question. So, you can ask that question, and of course, not super surprisingly, you would see that the way that you should be evolving those decision boundaries would involve the gradient of the density. So essentially, where do you have less data? You should head that way, because that's where the adversary wouldn't have opportunity to attack. But also, you should move in directions where you're reducing the most the curvature of your decision boundaries, because by making By making by moving in that direction, you're essentially considering decision boundaries that have shorter surface areas actually. So, this is something that you can do just formally. And then, of course, there is no surprise if you take the adversarial training objective function and you just write it in this form that I mentioned earlier, and just try to expand, at least for smooth sets A, of course, what it's going to give you is some local. Going to give you is some local perimeter plus some correction term that probably will have some curvature terms. So then this perimeter would essentially have the marginal and the x-coordinate, so it puts together the two classes. At least in this model, it cannot distinguish, it cannot really separate the two classes, but this is the type of regularization that is enforced. And so you see perimeter over there. Now, this is kind of more like a point-wise analysis, but you can do a more calculus of variations type. More calculus of variations type convergence, and you can say, well, what can we say about minimizers? What can we say about the minimum value when you take now this as optimization problems? And so one of my colleagues, Leon Brunberg, with one of his PhD students, they have a result in that direction. So it's kind of like what in Calcutta's operations you would say is a gamma convergence result for this non-local perimeter. For this non-local perimeter, towards some sort of local perimeter. One thing that is interesting from those results is that, from a geometric perspective, essentially, you're capturing what it means to do a trade-off between accuracy and robustness. So essentially, this is what you would obtain if there was no adversary, and then the slope depends on the perimeter of your base classifier. So this is this third star over here. So this type of analysis allows you to say things like that. When epsilon is small, you're doing analysis. Is small, you're doing analysis, you're sending things to zero, then stuff like this you can save. And of course, if you can expand even further, then it will give you more and more information about this problem. But again, in short, what we're saying here is that adversaries are enforcing regularization of decision boundaries through perimeter regularization. Okay, now that's forestimo epsilon. So now let me tell you one thing we know about fixed epsilon in terms of existence of solutions and regularity. Solutions and regularity. Okay, and I think that this is one of the things that I want to highlight the most, especially in relation to the last topic that I will present in two minutes. So, one thing that one can show, and again, it's not so difficult given the model that we have, is that you can always find solutions to the adversarial trendy problem. There exist solutions. It's not completely trivial because there is measurability issues here. You're taking a soup and you're minimizing over Borel sets, so there are some things. Said, so there are some things to be very careful with, but let me put this aside. Now, what you can do is given two solutions, sorry, you can find these two solutions that have some geometric regularity, that you have epsilon interval condition, epsilon outerval condition, depending on whether you're looking at this one or this one over here, and that you can find essentially anything in between would be a solution to the adversarial training problem. This is pretty general, we don't have to assume much about. You don't have to assume much about the type of distance function that you're using. But if you do a little bit more of analysis and you say, well, let's just consider the Euclidean case, for example, then you can actually deduce that there always exists solutions with some regularity of their boundaries. And I mean, more or less, the picture suggests why this is the case. Essentially, you know, in those regions where you have a space, you can do anything you want. You can be very smooth. And in those regions where this sets A minus and A plus came together, you have control. Came together, you have control from outside and inside because you have this epsilon wall condition on the control on the curvature. Now, why is this useful? Well, because when you're training with neural networks or whatever, depending on how regular are the objects that you're trying to capture, you will have to ask for more neurons or bigger architectures and so on and so forth. So, this type of result, although first is still very small in a sense. Still very small in a sense, it is kind of telling you: look, you can always find solutions that have some level of regularity. So maybe you should be tracking Taffeta, and maybe you should be trying to build your neural level so that you heat that regularity, and you will be able to find solutions involved. So this is the type of thing that we are trying to do over here. So, okay, so far I've told you about adversarial training and this family of DRO, and I've told you a little bit about And I've told you a little bit about connections with perimeter mechanization. Now I'm going to tell you about another class of, you know, if you like, generalizations of adversarial training. And I will tell you why people start thinking about this. So let me get it started. So here is the thing. So this is again in the binary phase. This thing is called probabilistically robust learning. And what it is, is you just define a new objective function. So now I think in So now, I think instead of looking at this formula, I just want to explain what is happening here. So, before, a point like this one in the model that we have before immediately would be penalized because the bulb radius epsilon would be touching the other side of your classifier. But in this model, it's not just like that. You essentially look at what is the volume of that kind of intersection of the ball with the complement, and if that volume is large enough, When a volume is large enough, then you penalize, otherwise, you don't penalize. So, this is why there is, you know, you have an indicator of is that volume large enough? Yes or no? If not, then you don't penalize. If yes, then you penalize. So it's a relaxation a little bit. And okay, so this was introduced like one year and a half, two years ago by another group in computer science. And as I said, this is kind of some sort of probabilistic thickening. So it's not just that you're So it's not just that, does your ball touch the complementary or not? No, it's it's enough. Okay, so this is the adversary thinking: do I have enough attacks? Yes or no? If I don't have enough attacks, I don't have enough attacks, then I'm not going to do anything. Otherwise, I do something. What is the reason for people considering this? Because you can imagine in high dimension, you know, you have the supremum over a ball, and you're trying to force your classifier to be robust to maybe a very, very large set. You're trying to, you know, you're Large set. You're taking a superiority. So maybe if by enforcing minimization of that adversarial criminalization, maybe it's too strong. Maybe you're asking too much. So you could end up with classifiers that are a little bit trivial and your accuracy may drop quite substantially. So this is a way to relax that a little bit. You're saying, well, I don't have to be robust to every single possible attack, I just have to be robust against enough attacks, more or less. This is the the logic that they're using. Or less, this is the logic that they're using. So they want to cure a little bit the trade-off between accuracy and robustness. So this is why they introduce this. But just, you know, formally, you can say what happens if you take this parameter that measures whether there are enough attacks or not, you send them to zero, then formally you do recover the adversarial training problem that we were discussing earlier. So this is, you can think of this family as, again, another bigger family that contains adversarial training as a specific example. Okay, now unfortunately this, at least the way that it was formulated, is not a great model conceptually because it doesn't rule out situations like this one, where this point over here is misclassified, and yet you compute the ball over in sepsilinum, you see, you look at, do I have enough attacks? You say, well, this volume is very small, so no, no problem. But here, it's kind of silly because the classifier was already misclassifying this point. So this is one of these situations where So, this is one of these situations where if your enemy is already doing something stupid, just let your enemy do something stupid. Don't try to do anything. And you can see that by essentially subtracting the standard risk, you get something like this. And this is non-zero. Sorry, this is non-possibility. I mean, it could have either sign, which means that sometimes the adversary may help the learner. This is the problem. So, what is the solution to that? We'll just try to force this time. Try to force this type of thing, but only when the classifier is doing things correctly. If it's doing things correctly, then you kick in as an adversary and try to do something to fool the adversary. So we introduce this thing that we're calling prop pair. And again, my notation is suggesting something. So you have something like this, and you can be a little bit more general. You can say, well, A little bit more general, you can say, well, let me just put an arbitrary function ψ that essentially is going to tell us how big it's going to penalize depending on the amount of attacks that are available around a given point x. Something like this. And two examples, you can take psi t to be this step function. If you do this, then you recover precisely the thing that I was talking about before. Or you can consider this concave piecewise linear function like this. Piecewise linear function like this, something very curious that comes when you do that, you recover something that in mathematical finance people call the conditional value at risk. So essentially, you can do adversarial trending with conditional value at risk. And there are some interesting properties that come from choices of psi. So we have a structure property of this prop pair function. If psi is concave and non-decreasing, then automatically prop pair function function function function function fun Everything then automatically satisfies this submodularity property, which essentially means that it can be used as a generalized form of parameter. That means also that if you define the total variation in terms of query or formula, automatically you get something that is a convex function on functions. So this is something that is, I think, surprising to us. It was not expected that it was going to satisfy this property, but for concave functions, it does. So in particular, this thing that gives you the C-bar satisfies this property. Satisfying this problem. So you have a perimeter in the kind of a structure sense. You also have some point-wise consistency. So if you have a set with a nice boundary and you rescale things a little bit, then you recover forms of local pyramid. So here you see housing measure and so on, weighted by densities and maybe some surface tension depending on the kernel for the distribution for the adversarial attacks. And then there is also something more. And then there is also something more. One can prove the existence of solutions to this problem. So we're trying to find robust classifiers. So we're trying to minimize the probabilistic risk, which is standard risk plus this perimeter that I was mentioning. And this problem has solutions. Now you would say, oh, you're going to just apply the standard direct method of separations. And actually, that's not the case. And there is something very interesting. The case, and there is something very interesting about the structure that comes in this problem. And so, I just want to spend my last minutes to highlight that. Okay, so here's the thing. So, this is the probabilistic perimeter. So, again, I said the website is concave. This thing over here is a submodular function. Now, it has admits two relaxations. So, the first relaxation is the standard total variation, which is just you do the Coria formula. You just integrate the perimeter over level sets. Perimeter over level sets. But there is also a natural relaxation, which is instead of working with this indicator functions, you just put arbitrary functions over there. So here you have the indicator of the complement, just take 1 minus u. Here you have A, then just take U. Here you have A, then you have U. And then here you have A complement, just 1 minus U. This is a valid relaxation, right? And now this tuple of four things satisfy some interesting properties. Satisfy some interesting properties. So we have the regional perimeter, the total variation, this j functional, and weak start convergence. Weakstar convergence, why? Because this is the only thing, the only compactness you can get in this problem is weak star. There is nothing else that you gain. It's non-local parameters. But what is interesting is the following. So first of all, those three things agree on sets. That's why they are relaxations. J psi is sequentially lower than the continuous with respect to wheat star. And the reason is because essentially, Is because essentially you look at this thing over here, weak star is fine for this term, and then it's fine for this term because inside weak becomes kind of strong convergence kind of thing. So you have something that is converged strongly and weakly, then it's going to be fine. This is the reason why this functional j is lower semi-continuous. But proc TV is not. And this is the reason why the direct method does not work. However, what one has is this inequality over here. Is this inequality over here? When psi is concave, okay, you have this inequality, and so you have these four objects that satisfy these properties. And now, kind of like a modified direct method would allow you to prove existence of solutions. The idea is, as always, you have a minimizing sequence. Up to subsequence, it will converge something. It may not be a set, but then you apply the properties. So, you have this thing agrees with J in sets. You have lower semic continuity for J epsi. You have this inequality. Psi, you have this inequality relating j with prop, and then prop has the coercion formula. So, here you get the set that you were looking for. So, it's kind of interesting because I at least don't have other examples where you have kind of these four objects that kind of like work together to give you proof of existence in your non-tree logo. Okay, so I think I'm running out of time. I just want to summarize a little bit what we did in this talk. So, I told you about families of adrocious. Families of adversarial training problems, DRO and PRL. We talked about connections between the agnostic version of DRO and multi-marginal optimal transport. So this is useful because it builds a bridge that you can use for computation. I told you about adversarial training as perimeter minimization. And I told you about existence of solutions to all these problems. I don't know anything about final regularity for solutions of AT. The only thing that I have is what I told you at some point. I told you at some point. I have no idea about the existence of regular solutions for this new model that I was telling you about. There are solutions, but what about regularity? I don't know. Gamma expansions for all these models. Well, there is something for the adversarial training, but not for this PR model. But you would expect something similar, although maybe a bit more difficult. And new notions of perimeter, therefore, there are new notions of curvature. And who knows if this is structured something that. Who knows if this structure is something that one can explore in other sets. Okay, so all this talk was based on a few words. This last work is what contains the probabilistically robust learning. Thank you so much for your attention. Further questions? Um could you say a little bit about how um Can you say a little bit about how you have envisioned this to work in practice? In a sense, that it seems to me like if you were to try to use this for a deep neural network, you don't have a lot of control over what the boundaries, the decision boundaries look like. And that's one of the things. So do you have an envision more of something that you have more control over that? Or is there a way to get more samples? How would that work? Yeah, so that's a good question. So the first thing is you have to. So that's a good question. So, the first thing is you have a lower bound. You could be computing the adversarial risk for your neural network. If you know that you're within, I don't know, epsilon of the lower bound, then you could stop there and be happy. That's it. So that's one potential way, right? The other potential way is that, so this result also gives you a classifier. In this case, if you have finite data, the classifier will be, what it will be doing is selecting a few of those data points and putting some balls around it. Some balls around it. And now, if one combines it with this kind of regularity result, what it will say is: well, now that you know what are the kind of, so maybe I can give you this feature. So now that, so say this A minus and A plus, you can construct with this optimal transport, let's say. So now what you can do is do some sort of data. Do some sort of data augmentation around those points. So, essentially, to fill this space over here and then force your neural network to be accurate on those. So, in a sense, you would be translating an adversarial training problem into a standard training problem. And if you are able to feed that with a neural network, then you're done. Because you know that this lower boundary would be reaching a lower bound, and you're hacked. So, this is one way, this is two ways in which this could be used. So, you mentioned this string of