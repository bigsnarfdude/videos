Padricia, thank you. Chen, and let's see, did I forget somebody? Oh, yes, my dear colleague Sami, Sami. So this talk is about some applications of probabilities to some problems, singular integrals where probability has really had a tremendous success, I think, in addressing some of these questions. In addressing some of these questions, in fact, there are quite a few results in this subject that have no, at this present time, no analytic proofs. That it doesn't mean that we are doing better than they, they are doing better than us. It's all us, in my opinion. So, but it is nice to have some applications of this. So, let me Where is the advance? Well, it doesn't seem to be doing anything. I can just press this for the time being, yeah. So I'm going to start with a so it's somewhere here. Here is not the pointer, but rather this you had it working. Okay, so um So I'll just press this. There's a very beautiful paper that was written by Gandhi and Garopoulos that is three pages long. And in this paper, written in 1978, they wrote a probabilistic representation of Ries transforms. So the Ries transforms are these operators that arise in many settings. They are singular integrals. This is the right-hand side formula. They can be. right-hand side formula they can be obtained from the semi-group of the Poisson from the Poisson semi-group by differentiating and in the k direction integrating from zero to infinity uh the the y variable uh and they give you the the risk transforms they also query multipliers so they have been extremely well studied and studied in many many different settings manifolds vector bundles you know you name it and they really have been around uh for for for a long time Around for a long time, and they have been very well studied. The formula is this formula. Okay, again, it doesn't work, but this formula is, let me just elaborate a little bit on this formula. So you take Branian motion that we call Bt, and then we have the Y component, and we look at this Branian motion until it leaves the upper half space, until Y becomes zero. That's the exit time. You take this function, which is again just the convolution with the Poisson kernel, the Cauchy. With the Poisson kernel, the Cauchy distribution. And then you take the starting measure to be the Lebesgue, the starting measure is the Lebesgue measure. So you take the level Y, you integrate against X, and so that gives you the expectation Ey, the probability measure. And then you have this formula, right? So this is the limit as Y goes to infinity. So you start very, very high up and you condition to exit the process to exit at X. And this gives you the. And this gives you the formula for the Riesz transforms. If you knew the first formula, you would guess this formula probably if you were thinking probabilistically. And the proof is extremely simple. So one line proof, essentially, you integrate against another function, you compute the occupation time of this process, you convert it into an integral over the operational space, and then you do Fourier transform, and out comes the singular integral. The singular integral Hilbert transforms. But, you know, this paper sat on the bookshelf for a while. Gandhi Varopoulos gave this formula. Actually, it's not this formula. They gave a more complicated formula with a process that they called the background radiation, which was a little bit mysterious to a lot of people at the time. But the paper anyway sat on the shelf for quite a while. But then Stein wrote a paper, Eli Stein, that was also a three. There was also a three-page paper. So, this is the story of my, how three-page papers changed my life. But this paper had a tremendous interest. So, he proved this theorem, which is called some results on these results in harmonic analysis, the same goes to infinity, is a research announcement. So, that's why it was very short. And let me say what Stein really proved in that paper. He actually gave the Proved in that paper. He actually gave the proof of this theorem. So that if you take the vector of Ries transforms, these Ries transforms have a constant that an LP bound that is independent of the dimension. So you can understand then maybe some of the interest on these questions, right? Now, here's something that happened that was very curious at that time for these new results. So Stein corrected the dependence on n, which a Calderon-Sygman theory tells you that is exponential. Calderon-Sygman theory tells you that it's exponential in n, but then ruin the dependence on p. See, so this as p goes to one, these singular integrals are not bounded on L1. They are what I call weak L1. And this is, you can see that from the behavior as p goes to one, right? And but they are automatically bounded on L P. So there's an interpolation theorem that tells you exactly how the P the growth of P happens, right? So this is the growth of P if you take the constant dependent on N. The constant dependent on n is exponential in n and is linear in p, if you will. Now, Paul Andrew Meyer in 1984 wrote his also very famous paper that people use in stochastic analysis, namely the proof of the Ornstein-Ullenbach Ries transforms, right? Also in LP. The theorem there, the key point is to prove that in Rn, the Ornstein-Ullenbach Ries transforms are independent of N. But he had the same problem that Stein had. The Orange and Rullenbeck operators were already known to be bounded in Lp, of course, with a constant independent of dependent on N, which of course for the infinite dimensional setting is not useful. But he had the same problem. So the question that I asked my former advisor or my advisor, Richard Durrett, at the time, do you think it's possible to do both? And he said, well, if you do. Book to do both, and he said, Well, if you do both, I give you a PhD for it. And I said, Well, that sounds like an appealing thing to do, right? And so I actually look at Varopoulos' paper and I rephrase it in the way that is usually used now. So you take a, but you can do this very, very generally. You take a matrix which is variable coefficients, and you construct these operators, right? And you can prove that these operators are also bounded in LP using. In LP use and marking out inequalities, right? See, it's interesting that Gandhi Varopoulos had this paper and wrote this beautiful formula, but didn't give any applications. So I think this probably was the first paper that brought this sort of subject out, the applications. And you can prove then that you have the right dependence on n, right, independent on the dimension, and also that you have the right dependence. And also that you have the right dependence on p okay. So these were almost just kind of so right. So these were kind of just in some sense remarks. Of course, I didn't call them remarks because I wanted to publish my paper, right? But it's not incredibly difficult. You have to keep track of things very, very carefully. So unlike Mayer and also Stein. And also, Stein, who used Littlewood-Paley theory, I use the Sharp inequalities. Well, let me first of all say a little bit more here. So what happens if you have a single Ries transform? For a single Ries transform, these are the matrices that give you the Ries transforms. In one dimension, you only have the Hilbert transforms. The Ries transforms are extensions of the Hilbert transform to several dimensions. And the matrices have a special property, namely that. Have a special property, namely that if you take h applied to a vector v, then it is orthogonal to the vector v itself. Okay, so this is an interesting property that was explored a bit later on. But in any case, with these, the proof that I had was that you can do well with both of those problems. You can remove the dependence on N. Can remove the dependence on N and get the right dependence on P. And most importantly for me is that I got a PhD from it, right? So I'm going to say, of course, of course, yes. So let me tell you how the proof of this goes. So the proof, you use Davis' best constant inequalities. Burgess Davis found the best constant in the Berholder-Gandhi inequalities. Holder-Gandhi inequalities. The quadratic variation compared to the P normal of the martingale, right? For P larger than one, right? So these are the Berholder-Gandhi. The Berholder-Davis-Gandhi is the discrete martingales for P equals one. Burgess always, my colleague Burgess always objected to calling them Berholder-Davis-Gandhi because he says these are Berholder-Gandhi. And if you want to do discrete, you put Davis. And I told him. Davis. And I told him, Burgess, you better, you should just give up on that objection. I don't know if he has or has not, but I haven't talked to him recently. Any case, so but he did this beautiful theorem long ago, right, 1970 something or 80, 87 something, in which he proved the best constants. And if you then use the representation that you have, right, for this terms of stochastic integrals, you remove the conditional expectation, that doesn't ruin anything. Expectation. That doesn't ruin anything when you take LP norms. And then after that, you do Davis. And then after that, you do all the classical inequality. And you get P star minus one for one operator. If you want to do the whole vector, then you have to do what is a good Lambda inequality, a sharp exponential square dec good Lambda inequality. And this is what gives you that. What gives you the that inequality was a new inequality, but in any case, a little bit later on, um, the Ries transforms, the actual norm was discovered to be this constant, which doesn't matter, but it will be, it will come up throughout the talk, all right, at this point. And that was a paper with myself and Gan Wang, and a bit around the same time, a little bit published a little bit longer by these other two gentlemen, Ivanich and Gavin Martin. Ivanich and Gavin Martin. And this constant is actually smaller than this p star minus one, which is again the maximum of p and q when p and q are conjugate exponents. But this inequality is sharp in the general case, and that was also proved a bit later on after Bergholder and Ganwang and I extended very sharp inequalities. Now, let me tell Now, let me tell you what is known in the people that work on discrete singular integrals as a quote-unquote tantalizing problem. So, I want to tell you a little bit of history and how the Gandhi-Varopoulos strategy came to be very useful in this setting. So, a problem that was around early 1900s was how does the size of a function control? Of a function controls the size of the conjugate function, okay? And you can just look at it elementarily as a trigonometric polynomial. So you take the trigonometric polynomial, you take a conjugate trigonometric polynomial. So these are boundary values of conjugate harmonic functions, which is why they're called conjugate functions. So this was a problem that was of great interest because of applications in Fourier series and many other problems. And there was a great competition between Between Hardy, Tishmarsh, Tishmarsh was a student of Hardy, and Marcel Rees, okay, so for this problem. And this is very well documented. It's a very interesting history on this problem, but I don't have the time to go through it. But let me just tell you what the problem is. So Rees actually proved this inequality in 1923, and the paper was published in 1927. So if you 1927. So, if you are complaining about referee reports, right, taking too long, just think of Rees. All right. So, the constant, right, dependent on P. Rees also proved that this theorem is equivalent to the so-called continuous Hilbert transform. Now, where did the name Hilbert transform come from? So, of course, the Hilbert transform is just a one-dimensional version of the Riesz transform, right? Where does that come from? It comes from From it comes from David Hilbert's introduction of quadratic forms, and he had this quadratic form which he called the singular quadratic form. So many people know, you know, the quadratic forms on integers of David Hilbert. So this is the exact analog, if you will, of what we call now the continuous Hilbert transform. So this is what Hilbert introduced. Okay, and Tish Marsh, who was a student of Hardy. Who was a student of Hardy at that time? So, what Rees proved is that there's a control of one of these norms by the other, right? The discrete is larger than the continuous one. The discrete is up to a constant less than the constant for the continuous one. And Tishmash proved in his paper the other inequality, okay? Right, so he turned it around. So, Hilbert first proved the continuous version. Tischmash proved the discrete version. Tish must prove the discrete version, and then he said, Okay, my proof also gives that they are equivalent. And unfortunately, the next right after he published the paper, he had to write a correction, okay? He said, my proof that these two things are equivalent is incorrect, right? And so that became a quote-unquote tantalizing problem, and it had been studied by many, many people, right? By many, many people, right? The very first proof of the p equals two theorem, by the way, for the Hilbert transform, appear in Andrew Weil's thesis. Okay, who was Andrew Weil was a student of Durham Hilbert. But in any case, there are many proofs and the tantalizing name, I think, comes from this guy right here. But people had used it. He said they used it before. I don't know. I never saw it. Used it before. I don't know. I never saw it before. But in any case, there are many, many of these papers, you know, proven LP inequalities for these, but somehow, and some of them, by the way, have been used, have been proved using discrete martingales, which random walks, okay, or discrete objects. But random walks, discrete objects do not give you the constant, unfortunately, because of this. There's some technical issue with the difference between orthogonality for martingales that make. That make it impossible to apply the techniques for discrete martingales in this setting. So, what we proved using again the machinery of Gandhi Varopoulos is that in fact you do have the exact same norm. So the discrete one is less than this constant. And so in particular, because of the continuous, this is the constant. The continuous, this is the constant for the continuous Rhys transform, the continuous Ries transforms, right? And when dimension one is used the Hilbert transform, and Rhys's inequality in the other direction, then can be used to prove that these are actually the same. Now, well, this was fun. But there is a more tantalizing problem in dimensions, in several dimensions. So let me tell you a little bit of several dimensions. So, Calderon and Zygmunt wrote this famous. So, Calderon and Zygmunt wrote this famous paper, in which they introduced the Calderon-Sigmund theory, right? And they proved the Calderon-Zygmunt kernels, which you can define them in certain ways, are bound in an LP. You can then define, these are convolution singular integrals, you can define the discrete versions of these. Again, not just the discrete versions by putting values of discrete variable in the kernel. Variable in the kernel, but also taking the function to be just discrete value, right? A sequence, okay, and we will use here f mapping the z d to z. And they proved that these discrete singular integrals are less than a constant that depends on d dimension and depends on b times the L P norm of the singular integrals. And in fact, they were so brief, this is the only thing they wrote, okay? Only thing they wrote. Okay, this remark is due to M. Rees, and the proof in the general case follows his proof. That's all they said, right? And they put it in the appendix and somehow, you know, stay there. But a lot of people noticed it, okay? And certainly people have worked on it. Let me give you a little bit of a refinement that is needed for this subject that we're going to do about how the constants here depend. About how the constants here depend if you take certain types of singular integrals. So, if you take a Calderon singular integral, you can prove that the discrete version of it is less than the p norm of the continuous plus a constant that depends on d and on beta. If the kernel is what is called homogeneous of degree zero, then you can get exactly Marshall-Royce theorem, that the discrete one is larger than the continuous. Discrete one is larger than the continuous one in P nor, right? So an example of that are the discrete Ries transforms. When d equals one, this operator is actually equal to Hilbert's, you know, the quadratic form, singular quadratic form, which you call, which is now called the Hilbert, discrete Hilbert transform. And these problems on discrete analogs of harmonic analysis have been studied by a lot of people, a lot of analysts. Of people, a lot of analysts. I wrote various people here, various names. There's a beautiful thesis that was written from which I learned, started to learn about discrete harmonic analysis by Lillian Pierce in 2009. And that's a good place to start. But anyways, there has been a lot of work on this. Borgan, for example, brought in the theory of so-called Holly Littlewood circle. Uh, Holly Littlewood circle method to study some of these problems in analytic number theory and so on. There's a lot of connections, but let me go on to formulate the tantalizing problem in higher dimensions. So we know then that these Riesz transforms from the earlier theorem that we proved earlier, and we also know then that NP, the Hilbert, that the continuous one is larger than the discrete one, the discrete one is larger than the continuous one. The discrete one is larger than the continuous one times the norm of the continuous one plus this constant C D. I don't need a beta because the beta depends on D, right? So the question, the conjecture is that in fact, this inequality, which is what we proved in one dimension, also holds in any dimension, right? Okay, so it's a very, very appealing question. And again, And again, let me tell you: we don't have to give you any suspense here. We do not know how to prove this inequality in higher dimensions, this identity in higher dimensions, but we know a lot, and we believe it really is true. What we do is construct a collection of operators PHKs on Z D, allagandi Varopoulos that have norms and P. The construction uses a suitable Dubai H transform in the up. H transform and the upper bound of these operators is obtained from martingale theory and the constant is exactly this. The lower bound that these operators have exactly the conjecture for the Reese is proved by somewhat different method, which is I don't really technical that I don't really need to want to discuss here. Of course, you see, the beauty of the restructuring of the construction, Gandhi Varopolo's construction, is that the Is that this probabilistic representation gives you exactly the operators? And so you don't have to, you just transfer everything to the stochastic setting and apply the Martinego inequalities and you got it, right? For the Risk transfer here, the question is, are these related, these probabilistic objects, are they related to the discrete singular integrals, right? Now, this is something that Fabrice and I and Lee and... And I and Lee found somewhere also that you can do constructions of Candi Varopoulos type for operators you want, and you don't come to the exact thing that you want. But you know, those other problems were very different than this one. Okay, so let me tell you very briefly what the Duveich process is. So this is a Branian motion that is conditioned to exit only on the integers, okay, only on the lattice. Okay, only on the lattice. How do you construct it? All right, well, you take what is called the periodic Poisson kernel, right? So this turns out to be a positive harmonic function, right? Then you construct this function u sub fx, y, right, to be equal to that quantity. And you can prove that that function is positive. Of course, it's positive, right? Not positive, but these quantities are positive. Quantities are positive, and this function is a solution to this equation. Okay, so these then allow you to construct the two-bes process that goes with that equation, right? Okay, I'll tell you that. But here's another observation that these sum equals one. So these are probabilities on the lattice. So this is the probability, in fact, that the process, this process ends or Process ends or X's the latter's at the integers. These are the probabilities, right? And the boundary values under this process, okay, not all, not as a PDE, are just the integers you started, right? So you take the integers, you construct a process that has these boundary values, right? Okay, with respect to these measures, and that solves this equation. But the boundary values of these... But the boundary values of these, of this function, are actually a singular integral itself. They actually, as it turns out, we wrote this paper and then we discovered that the analysts also have found, study this kind of equations and study the boundary values of this. Okay, so here's the process, right? So you take the Dubesch process, right? So this process, you know, you know what the probability it is, it's got finite lifetime almost surely. Lifetime, almost surely, you know what the probability of exiting on the lattice is, and you take the expectation of this as if you were a bath 416, whatever the course is, 416. What is the gene? 416 in our course. Some is an expert 416. And then you find the expectation of this random variable, right? And then there is this normalizing thing. This is a theorem, okay, that we had to prove, a lemma, but you can get. But you can get, you know, integrals as expectations from these transfers, right? And then you construct your two martingales, you construct the continuous martingale, you take the martingale transform of it. And then you have the martingale inequalities. So the first one is the Martingale inequality, which comes out of the work of Berholder. The other one is the Martingale inequalities that Gan Wang and I proved. Equalities that Gan Wang and I proved as an extension of the theorem of Berholder in the case when the martingales are orthogonal with the orthogonality property, the matrix A has this orthogonality property. All right, well then from that you take the conditional, you take the conditional expectations, right? So you take, let me see, so let me go back and so here's the operators. The operator is just the conditional expectation, exactly as in the situation of Gandhi and Varopoulos. And then you have to prove something about what happens as the limit goes up to infinity, as y goes to infinity. As y goes to infinity, you get a collection of operators, which we call TA's. By the way, somebody asked me, why did you call these operators TA in your original paper? And I said, because. And I said, because I was a teaching assistant, if I had been a rich assistant, I would have called them RAs. But any case, so these are the inequalities, right? And you can compute in the general setting, right? The kernels of these very explicitly. And then the question is: are these related if you take the special matrices of Matrices of Gandhi-Varopoulos, they gave you the Ries transforms in the classical sense. Are they related to the discrete operators that we want, the discrete Ries transforms, right? And here is, so here are the kernels, right? They're kind of ugly, but you know, they're workable. And so what we proved is that with these matrices, these operators, these TH sub H k's, have H case have exactly the same p-norms as the original Hill Ries transforms, right? And of course, so here's what that is. And of course, the question is, and this is what will be absolutely wonderful, if these operators really gave us the discrete objects that we want, right? And unfortunately, that is not the case. That is not the case. See, in one dimension, this is the periodic Hilbert transform, and you can compute it explicitly. People in analytic number theory use this object very often. And there's many beautiful representation of ways of finding these in all sorts of magic ways. If you want to see a probabilistic magic way to do this computation, you can look. Computation, you can look at Margior and Jim Pittman examples of so-called Jacobi beta functions, okay? Just magic, right? You know, formulas, okay, expected exit times of complicated objects and the boom, this pops out. But here, you know, we can use that to compute the kernel of the operator in one dimension is that you see that misses. That misses for the Hilbert transform, you have one over pi n. This misses by that factor. So we worked all this time and then we got something, and we were just, okay, we're done. But here's something that happens. You can compute the Fourier transform of these, the discrete Fourier transforms, and then using this theorem of Buckner, right, that tells you when a function is the Fourier transform of a probability measure, which is almost impossible. Which is almost impossible to check because it's got this positive definite condition. So, in practice, it's not that easy, but we were able to check it here. And what we showed is that this discrete Hilbert transform is equal to this operator convolution with a kernel. And therefore, you get exactly that the discrete object is less than the continuous. Continuous object in pinhorn. And the question, of course, in higher dimensions is: are there kernels, probability kernels, for which you have the continuous, the discrete Hilbert Ries transforms as convolutions of these probabilistic objects that we actually call probabilistic discrete Ries transforms. That we do not know how to. That we do not know how to do because what involves there is very explicit computation of the Fourier transform. And we are not able to compute the Fourier transforms of this object as nicely as one would need to test anything like the Bugner's theorem. Okay. When you have an explicit kid, oh yeah, yeah, yeah. Well, I mean, it's over explicit, but it's not explicit, I think. But you can bound the norm. It's a probability kernel, right? So it's a convolution with the probability kernel. So when you do that, the p-norm of this is just the p-norm plus the p-norm of this, you know? So that's, you know, that's right. And in higher dimensions, we're not able to do that. Of course, let me just plug in here. So this theory of the theory of Gandhi Varopos has been used again, as I said, to study Ries transforms in all sorts of setups, geometric setups, and the constant are independent of the geometry of the ambient space in many respects. Okay, and with Fabrice and Lee and many other collaborators, I have actually studied the second-order discrete Reese transforms, where things sometimes are very, very nice. And here, as it turns out, you know, the periodic Hilbert heat kernel has been studied a lot in one dimensions, again, in analytic number theory. It's very closely related to the Jacobi beta function. And using that, we're able to construct a collection of operators in the same style. They give you norms, which are like the norms of the continuous versions of the second order transforms. There's something here for when this is when j is not equal to k, right? So rj. K right, so RJ K, the Reese transform of order J applied to the Reese transform our order K. Okay, so and anyway, this is work in progress. Okay, you know, why one thing that, okay, so I put this out to as a sort of a speculation, but one thing that is really interesting, or that was interesting, at least to me, when I began to study this, is these applications, right, of Applications, right, of the Mayer's inequalities in infinite dimensions, truly infinite dimensions, right? Where they come from? They come from Mayer's inequalities in finite dimensions with constant independent of n. After that, you let n go to infinity and things normalize properly and you get the Riesz transforms in Wiener space. Are there analogs? their analogues of Wiener Ries transforms that are discrete. And in fact, you can certainly in Rn, you have integral formulas for this, for the Gaussian Ries transforms, and you can write it down. Unfortunately, those are not, the kernels that occur are not nice. Nice called their own sigma kernels. And I don't know how to prove, for example, that the discrete analogs of the discrete Gaussian Risk transforms are bounded by the continuous ones, even with a constant that depends on dimension. It's probably doable, but I don't know. And of course, the next question would be: if you prove that it's independent of dimension, what did you get? Are these of interest? Are these of interest? You get something in infinite dimensions, I assume. I don't know. Any case, let me stop here. Thank you very much, very, very nice board. Well, you mean for the second order, for the second order? Oh, no, no, that's the problem. Actually, that's the problem. We do not have one of those theorems like the one that I mentioned, which is extension of Stein of Of Stein of Calderon Sigmund, because that uses this homogeneity of the kernel and it uses oddness of the kernel and the fact that the kernel integrates to zero on the sphere. Yeah. And for the even reach, you know, Rxk, this is square, right? It's not one of those. So that lemma doesn't apply. That's the reason why we're not able to prove that these two things are equal. That these two things are equal. Yeah. Thank you.