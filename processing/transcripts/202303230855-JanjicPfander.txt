Actually there in person, my visa arrived too late for me to travel. Yeah, and I would like today to talk about the perspective on magnet scale data simulation. So this has been a joint work with actually a postdoc at the time within my group, which is Yufei Tsang, who now moved to be a professor at the Nanjing University. Be a professor at the Nanjing University of Technology in China, and also with Yvonne Lukstul, who is lucky there and who will also give her talk a little bit later. Alright, so what we are interested in in the convective scale data simulation is to improve the location and intensity of the convective storms. Storms and we all include the prediction of convection intensity of the convective storms. We know all that this is not completely perfect and just as a motivation I am putting a plot here on the top from ECMWF simulations. So they were actually able to run the simulation with 1.5 kilometers globally. So and on the right is And on the right is the verifying satellite data. So if we look at basically the clouds in some locations, we see that this is not perfectly represented. And we see similar situation if we run our regional or limited area models. So here this is of domain over Germany. So one hour forecast with a regional model and here is verifying radar reflectivity. Verifying radar reflectivity data, and again, we can find a lot of differences in the location intensity. Also in some cases, that the storms are present in observations but not present in the forecasts. So we have for the conductive scale data simulation very high resolution models. So the operational models which run The operational models which run on the limited area domain are of the order 2.8 kilometers. We expect in the next 10 years basically to have even global models come to a similar resolution. And for them, for the connected scale models, in addition to the dynamical variables which we would have in the global systems, we have as well as prognostic. As well as prognostic variables, the hydrometers. And here, this figure again as an illustration shows idealized actually convective storm moving to the domain. So, in terms of the atmospheric variables on the left and in terms of the hydrometeors on the right. So, these are ice, rain, liquid, snow, and graupel. As I mentioned already, these models are hard. These models are far from perfect. And one of the challenges in the convective scale data simulation is actually how to properly specify this background uncertainty, including the model error certainty. Particularly having in mind that much of these forecast errors are also the errors in the location as well as in the amplitude of the stops. Compared to the global systems, we have actually all the data which we go into the assimilation within the global system, but in addition, we have radar measurements. So radars within five to ten minutes actually managed to cover the large area of 250 kilometers in the horizontal and 15 kilometers in the vertical. And what they see Vertical, and what they see is basically a mixture of these hydrometeors. And with respect to the data simulation algorithms which are used in this field, they are both ensemble-based and variational-based. So there has been a recent review by Hustelson et al. twenty eighteen. And generally, prognostic hydrometers which I mentioned are operationally not updated. Operationally not updated. However, what has been done in recent years is that due to these fast changes of profession, we have increased the update frequency of the model variables. And so this is now done actually hourly in the convective scale applications. So much of the effort has been done on the background error covariances, small error representations also. Representations also within our own research. For example, this case where we have provective cells in the observations but not having them in the model, there are some methods where we actually generate artificially povective scale then cell in the model so that we can actually assimilate these observations. Also, there has been some work on the observations. Has been some work on the observational error covariances for the radar data. So, warner it all for the radial winds and zenging all for the radar reflectivities. So, there have been, as I mentioned, recent overviews of these challenges besides the Wusterson et al. paper. There's also the Banister and all paper, which nicely summarize these fields. And I will today And I will today focus on one particular problem because I also think that might be of relevance for the chemical data simulation. And I will illustrate it here. So basically we have on the left idealized simulation. I will show the results with this later in the talk. So we have a domain covered with With covered with radars, and we have generated a supercell here in the domain, which actually with time moves away or moves through the domain, splitting into two cells, and then also one of the cells having a significantly different structure than the second cell. So, if we use the ensemble council in this setting, so this is for data simulation in the OSI settings, Settings, we see that the relative change of total water, so total mass of all of the negrometers, actually increases with time. So this relative change can also go over 100% in this case. So this is one of the problems which I will be primarily focusing on, but of course there are many challenges and already mentioned some in the uncertainty quantification. In the uncertainty quantification, in the fact that we have this background state which has this prognostic dometer variables, which have to be positive in nature. And also, within a future years, so the modeling is moving towards including two-moment microphysics schemes. So each of these prognostic abilometers will be actually described now with two validators. Now, with two variables instead of one. So, we are generally going into the higher and higher-dimensional system, and in addition, we need this fast updating frequency basically not to miss some of the developments of the convection. Okay, so over the last 10 years, so we have worked on some of the possible Worked on some of the possible solutions which are applicable to this problem. And one of them is development of the algorithm which we call the QP ensemble. So this algorithm is somewhat based on the ensemble common filter in the sense that it actually has only the propagation steps, so they sit in the same. So we propagate the enamel. So we propagate the analysis with a full model. We calculate the covariances the same way as in the sample Common filter, but however does not use any of the equations of the Carmen filter for the analysis and instead for each of the ensemble member does the minimization but the minimization with a constraint and this constraint is or can be a constraint on the positive Constraint on the positivity, which is shown here, but the constraint can be in generally non-linear as well. So we have tested this algorithm on several different toy models. And one of these toy models is from Fortune Frag in 2014, which is the model which has been designed for conducted scale data simulation tests. Data simulation tests, and it's a one-dimensional model, has three variables: so, this is velocity, range, and height field. And it has a stochastic perturbation, basically in the velocity field, which generates the clouds. And if the cloud reaches a certain threshold, it starts to rain. So, the illustration of this output of this model are on the right. So, we see all So we see all three variables and we see these different colors, so different colored lines, the results of starting from exactly the same initial conditions and these differences we see after the 10 minutes already in the fields. So this model has been used for comparing the results of the Comparing the results of the QP ensemble and the ensemble AMU filter in a work of UI of V1 and MI 2018. And as we see here, that the QP ensemble in the red is actually able to outperform the ensemble count filter for all the ensemble sizes, including also the small ensemble sizes. Okay, but Okay, but the problem with this algorithm is that it's not very fast, so basically much slower than the ensemble Hammond filter. We have to do the minimization, cost train minimization on each of the ensembles. So in the last year we have tried to improve on this in several different ways and in particular for the constraints. Particular for the constraints which are on the mass and positivity which are disjoint. So, in our case, in the case of this modifier water model, for example, constraint on mass is the constraint on the height field, so on the total integral of the height field, and the positivity constraint is the constraint on the brain field. So, in this particular case, we were together Together with really an expert in the optimizations of deleted point, we were able to derive two algorithms which actually reduced the cost to one third of its original cost. So the first algorithm actually required all the KKP systems. This means that we have increased the optimization problem in the sense that we are also making Problem in the sense that we are also minimizing for the Lagrange multipliers here. And in the second algorithm, is actually a modification of a point-gate gradient method, and it uses the projection basically for this low-rank linear constraint on the mass. So we have tested this algorithm within the modify shell water model, and what we see are what is And what we see are what is illustrated here are in red the results of the QP ensemble and in the blue results of the ensemble column filter. And these two algorithms, so algorithm one with the KKP system and the algorithm two, which is the modification of gradients implementations, are actually not run to the convergence, so we actually The convergence. So we actually stop the first algorithm only after the first iteration, and we stop the second algorithm actually after a little bit more iterations. So to see how this algorithm would perform in situations which are similar to which we would find in practice. And basically what we see for the second algorithm is here relatively That we see a relatively worse even results than the ensemble-Palm filter for the velocity field, but relatively good results for the height at the rain field, while the orange line still seems to be very close to the European ensemble. So we have stopped data assimilation after 35 cycles here and then basically looked at the error growth of the Arrow road of the forecast. So the black line is the, so just to illustrate what is the predictability in this model. So the black line starts from the truth and basically we see how this diverges from basically due to this stochastic formulations. Okay, so this is one of the algorithms which actually is based or which is the modification. Or, which is a modification on already existing methods to include the mass and positivity constraints. And then we also had an idea basically how to try to use the machine learning within the settings to actually improve the speed of the QP ensemble or to get into the QP ensemble solutions. So here these are results where we have in blue ensemble filter, in red the QP ensemble, and in green the results of a neural network, where neural network is trained on QP ensemble results as a truth, right? And it's used during the data assimilation after in the top plots, after every five minutes, basically to correct. Minutes basically to correct for ensemble from total results. If this is done after every 10 minutes, so the neural network is going worse. Okay, so then Yvonne had the idea here to actually do during the training, when we have this 10 minutes updates, to actually include the penalty also during the training for machine learning on the mass. And there we see that. We see that the really denser neural network is able to pick up completely the QP ensemble solutions. Okay, so these were two ways where we try to improve on the algorithm where the mass and positivity have been implemented basically as strong constraints. And now I would like to I would like to go back to this idealized radar simulations where we actually modify the LEDKF and see what happens there. But here in the weak sense. As just a reminder, again, so we think that for the convective scale we should also update with the observations the The observations, the prognostic hydrometeors, because in that way we will actually get the most out of the radar data. Generally, these are not updated operationally, but this is what we would like to do. And in case you would get the negative values, what happens? One will usually put these negative values to zero, because they would be unphysical. And we have seen previously that it's important if we want to preserve positivity to... preserve positivity to actually also simultaneously try to conserve mass. And here in this algorithm, so we will first look basically how we can observe mass within the LEPTS settings. So to do this it's actually very simple. One starts with a cost function and an additional term here where m is a Where n is a vector whose elements are basically domain-wise integrals of the hydrometeors. So, this is a very small size, so this is size 6. So, we have, it's also for snow, brouble, ice, separate basically, vector quantities here. The S is our imitation of the observation operator, basically, which integrates domain-wise, multiplied with the density always, right? So, which integrates. Always, right? So it integrates and we try to constrain our analysis to actually preserve mass within a certain accuracy here, where this accuracy we do calculate similarly as we would be calculating PB. But here, note the M which I am extreme actually in this case is the true mass. The true mass. So I'm not assuming that I don't know the mass in this case. So if we do this, we have the very easy modifications of the LAPKF algorithms. We have to modify the analysis, error covariance. We have to modify the analysis itself. So the corrections for the weights are implemented locally. Are implemented locally so for the LATKF, but then basically after analysis, we are doing the calculations, so these calculations which are the collections for the mass for each ensemble members. So to deal with the positivity, however, we didn't do anything special. So this is something which has been already done in the work of Ashpet of 2009. We only realized that this is a reconstraint on the positivity. Weak constraints on the positivity. And so, what has been done is previously that because of the spurious connective cells that arise when we do a radar reflectivity data assimilation and all, ASHPI et al. actually assimilated the clear air reflectivity data. So, radar only sees somewhere if we have the hydromecuous present in the air, and this is the in addition one with the In addition, one would assimilate actually zero values, or basically the values which are fixed to a certain threshold. Okay, so of course, this is not at every grid point of the model, this is on the radar grid, so basically we do implement it and have a full radar operator within our test settings. And what this does, this is the in the weak sense, we are preserving non-negativity. Non-negative. But of course, this means that we will get negative values, and these negative values we will be again clipping to zero. So again, something about the setup, full radar operator here. We have a slightly larger, so we use 80 ensemble members, so this is used usually 40 in the operational settings. Operational settings, and these are the errors of the one meter per second in 5 dBZ for the radar reflectivity and radial length. And again, here we have starting at 3, ending at 7, we actually do from 3 to 5, we do the data simulation and the forecast is only basically two hours. So from pure forecast is only two hours per state. Is only two hours from till 7 p.m. Okay, so we let's look at the results here. So we have a control here, which is the radar reflectivity and wind assimilated, so no clear air reflectivity assimilated. We have the mass constraint, as I mentioned, so the mass constrah how the mass constraint is implemented. And we have the positivity constraint, which just means that we are, in addition to the reductivity data, assimilating. Data assimilating the clear error to activity data, and we have the both constraints. This is the experiment MP. So, if we look at some of the results on the bottom, so we looked at again this relative change compared to the truth in the total hydrometeors, so each of them separately and then all on the end. And compared to the control, To the control, inclusion of any constraint seems to be an improvement. So we have the mass constraint in blue and positivity constraint in green. So basically what is maybe surprising, but maybe not. So basically, if we have this positivity constraint, we actually improve as well on the mass simply because we don't have as many negative values, and so we are not clipping as much. We are not clipping as much to zero. But of course, if we have both constraints, we are getting the best results. So, this is just the number of negative values. But here, what is interesting perhaps, so from the physical point of view, we see that these two constraints actually do improve both divergence and the vorticity representation within our settings. Seconds. Okay, so to see this from the reflectivity plots, so these are control. This is the only mass constraint. So these are the spurious convection. In blue are the true cells, how they should be looking like. The sporious convection we don't lose with the mass constraint. We actually suppress it quite well with the positivity constraint only. Positivity constraint only, but we again suppress the best constraints. And if we look at the RMSCs, then basically the values are quite similar. So this is the values for the total domain, these are the values within storms. Only thing which we can notice is that maybe the positivity constraint affects negatively, the most negatively, the velocity here, so only positivity. So, only possibility constraint. And this, of course, transfers to us the accuracy of the short-term forecast. We have here only two hours forecast. This is the FSS score, which is the higher, this the better. And basically, and also to all boxes of the FSS scores. Alright. So, I would like to conclude already. So, we have, so we will be seeing that in the future. So we will be seeing that in the future even the global models will go to a very fine resolution. So we will have to deal also on this global level with this high temporal and spatial density of the data. And in our view, we have to improve on the methodology, on the data simulation methodology. What we have proposed is to try to include To try to include the conservation laws and the physical constraints to tackle some of the difficulties in the convective scale data simulation. And in particular, what we have seen is that by including the mass and positivity constraints, we can improve on prediction of the conductive events. So some of the methods which I talked about today actually require only minor changes. Only minor changes to the already existing implementation, particularly the valid F changes. Perhaps this is also of interest for the chemistry data situation. I don't know, but the reason why I was presenting this is that I thought maybe there is some overlap there. But however, for us as well, we have this problem. We have this problem that we still have to deal further with model error and observation error, even within the QP ensemble settings, which have not fully resolved this problem. Okay, and perhaps the bright future. Again, we have high-resolution models. We have 1.4 kilometers soon. We'll have new tools with machine learning, so new uncertainty contributions. We also have new data. So we have now polarimetric radars, which don't only give us the reflectivity, but actually give us some details, more details on the hydrometeors which are there present. So we hope that in the future we can actually even. Even make the rain prediction better than this kind. Okay, thank you. That was a really nice talk and your conclusion that mass conservation of hydrometeors and the negative filling, that's a really important thing for me to know because it causes me huge problems when I'm trying to use our weather model to do. When I'm trying to use our weather model to do carbon cycle modeling. But here's my question for you: Why do we need to add mass conservation and non-negativity as something we're going to put in our estimation when we could develop the physics routines in the first place to fully track the continuity equations of the hydrometeors and to not produce negatives? Because then we wouldn't have to use that as a constraint. To use that as a constraint, and we know it can be done, the climate modelers do it, and some of those carbon cycle modelers have managed to whip the physics into shape so that we can use it. So, I'm not familiar with the methods exactly. So, the question is, in this, what I have talked about today is really that the data simulation itself produces this. Cell is produces this artifact, and we are not doing posterior corrections. So, basically, after we do analysis, we don't do a correction to this analysis, but basically, we do try within a data simulation algorithm itself right away to impose these two physical, which we think are important physical constraints for the data simulation. Constraints for the data assimilation on them. But I've looked at the physics in our model, and you can see that the full continuity equations of all the hydrometeors are not tracked. And also that, you know, there's the clipping, as you mentioned. That doesn't conserve mass. And surely we could use some sort of negative filling in those schemes rather than clipping it. Yes, so basically the question is: so, if we are not certain about these mass values, and this is something which we did not. Mass values, and this is something which we did not do yet, and this is something which we are planning to do: is that we would actually estimate the mass together with the state. So we would increase the state with this mass values for each of the hydrogens. Let's just have a comment while there's three, and then that would be. Donna, it's Steve here. Just a quick question. So, are you running this in incremental form or form? Are you running this in incremental form or for field? Because if it's an incremental, then it's likely to create that correction or go beyond the lower bound because the DA system doesn't know that. So we are running it incrementally. So basically if you see the here, right, this is the constraint on the positivity. So this is how it looks like. So the implementation, for example, in Motifa Shell Water model is such that the mass does not change from the background. So the analysis mass is the same as the background mass. So let's move on 'cause we have to have the discussion so forth and then hopefully Kiana with you can join us for that some discussion session. Join us for the stamp discussion. So, thank you very much, and thank you, Kiana.