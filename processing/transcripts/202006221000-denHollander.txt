Welcome, everybody. This is week six of the Open Online Probability School. This week's course is given by Frank Denhollander. But first, a few quick announcements. Next week, there will be two courses given, one by Akash Jagannath on mean field spin glass, and one by Amin Kojo Glan on disorder. On disordered systems and random graphs. So we'll have four lectures this week, and the setup will be the same for all four lectures. We'll have two approximately 30-minute halves, and then a five- to 10-minute break in between where people can ask questions. You're encouraged both to ask and answer questions on the chat during the lecture. On the chat during the lecture. And we do ask that you try to use your real name for the chat. This helps foster a more intimate atmosphere. We're very fortunate that Elena Povarenti is here. You'll hear Elena's name mentioned several times as she's a co-author of many of the results being presented this week. She's at the University of She's at the University of Bonn, and she will be answering questions on the chat and visiting the breakout rooms after. If need be, we will interrupt Frank from time to time and ask him some of your questions as well. This lecture is being recorded and live streamed on YouTube and by Burrs. So if you do not So, if you do not want to be seen or heard during the lecture, you should make sure your video and microphone is turned off. At the end of the lecture, we will stop the recording and there'll be another question period. And towards the end of that, you'll be able to go to breakout rooms and discuss the lecture and ask questions in a more intimate atmosphere. There's also a platform. There's also a platform for discussing the course after the lecture on Zulip. So there's some exercises which you can find there. And you can discuss the exercises and the course there. So the access to Zulip is given on the OOPS webpage. There's links for the slides for today's lecture and exercises for today on the chat. And exercises for today on the chat. And Sarah, I may post those again so you can make sure everyone can see them. And so with that, I think we can introduce this week's lecture. Frank Don Hollander is giving a course on metastability and interacting particle systems. Frank's at the University of Leiden, and of course he's well known for his contributions to a His contributions to a very wide range of topics, statistical physics and probability. Frank is a member of the Royal Dutch Academy. He was an invited speaker at the 2010 ICM in Hyderabad. And even more importantly, he was one of the two long course speakers that was originally scheduled for the PIM CRM Probability Summer School, which Summer School, which would have been winding up this week. Frank and Ivan Corwin have both agreed to give their courses in two years' time in June of 2022 in Vancouver. And these week's lectures, I think, are really going to give you a glimpse of what those more comprehensive four-week courses will present. And today, I understand it's a casual introduction to beta stability. To better stability. And so, with that, I think, Frank, I'll turn it over to you. Sarah, I'll spotlight you and ask you to share your screen. Uh, let's see. Let's see. So, is everything now visible on the screen? Everything okay? Looks good. Yeah, okay. Then I'll start. So, thank you all for joining, and welcome to this lecture series on metastability for interacting particle systems. Systems. Elena and I are very happy that we can contribute to the Open Online Probability School by presenting this topic to you. The way we've put it together is that we have prepared four lectures and four sets of exercises, which are available online. In the four lectures, we try to give you a quick panorama of the topic. And today, we will start with an introduction in which With an introduction, in which we will talk about some background, some motivation, some key questions, and towards the end, also some mathematical tools and some mathematical notation and settings. And then in the three subsequent lectures, tomorrow and on Thursday and Friday, we're going to focus and zoom in on three examples, three very different examples of interacting particle systems. Systems and try to find out what we can do in terms of metastability for those and get into more detail about the mathematics. Along the way, several key notions and key computations are coming up also today. The exercises are meant to illustrate that, to support that, and they come with And they come with some hints. Some of them are easier than others, but it may help you to go a bit deeper into some of the things that I will not be talking about during the lectures. I will present the lectures and all the time, Erina is able to respond to your questions as you have them as we go along. So this way, you will have the best of both of us. And any interruptions are. And any interruptions are very welcome indeed. Okay, so let us start. I'm going to first talk about what metastability is before I even going to talk about any details. So, metastability is a phenomenon that appears in many different settings. You see it in physical systems, chemical systems. Systems, chemical systems, biological systems, sometimes in big economic networks or big social networks, where you see that the system is going through a sequence of what appears to be quasi-equilibrious states. In a physics language, you would call this phases. And it doesn't move much until suddenly it switches over to another phase or another state. Another state. So there are actually two time scales involved, and this is a general setting where metastability appears. And the challenge is to propose mathematical models that can capture these phenomena as we observe them experimentally. And we also observe experimentally a high degree of universality, which is, however, not easy to explain. To explain mathematically. And those are some of the problems that underpin metastability. Now, there are at this moment two monographs that are completely focused on metastability. There's a 15 years ago, Enzo Livieri and Maria Olalia Vares wrote a very nice book in which they combined metastability with large deviation theory. They were Deviation theory. They were following the dynamics of the system, try to describe the whole evolution of the system in path space, and then using large deviation theory in path space to understand behavior that these systems could do. In particular, these hanging around in quasi-equilibria and then on a much slower time scale actually moving between different phases. Different phases. Five years ago, I finished a book with Anton Bovier in which we were looking at similar problems, but there we completely took a different turn mathematically. We were building everything on top of potential theory. And I'll come back to some other approaches that have been developed over the years for metastability, but these have not yet found their ways into. Found their ways into monographs. And mathematically, it's a challenging and beautiful topic. I would say it's very much a topic also in mathematical physics that sometimes you really need to combine the best of worlds to make progress on the story. Okay. And paradigm pictures are what I wrote here. You can imagine that you have some kind of Can imagine that you have some kind of state space. Don't take this too literal, and your state space has certain compartments, and you will see that your system for a very long time sort of moves around stochastically in one compartment, and then it goes over some barrier to another compartment, and it spends a long time moving there, and this transition to go over the hill is typically very quick compared to the time that the system spends in the compartment. The system spends in the compartments. And if I would only look at the moments of transition, I can turn the bottom, the top picture that sees all the microscopic details into a kind of coarse grain picture where you say this crossover would correspond to a motion from one phase to another. Phase here is general, it's a kind of part of your configuration space. Part of your configuration space. And the dots here represent these compartments, and the arrows mean that you can cross over from one dot to another. And metastability is typically something where you say these motions are much slower than the motions inside these phases, and can we describe what the system is actually going to do on this kind of coarse grained level? Of coarse-grained level. So that is in a nutshell what it is that you're trying to do, and whatever happens precisely will very much depend on the model that you're dealing with. And we will see lots of examples as we go along. So this is sort of to set the stage. Now, since this is a metastability for interacting particle systems, we're going to zoom in and look at metastability within. At metastability within the narrower perspective of statistical physics, and you know that statistical physics is all about interacting particles and phase transitions and critical behavior. And we're going to look at what metastability does in that particular context of statistical physics. And the first thing to realize is that metastability is always connected to a first order phase transition. A first order phase transition. So this means you have more than one phase in your state space. Again, I will become much more precise about what that exactly mean later. And first order phase transition means you move from one phase to another, for instance, from liquid to gas or from liquid to a solid. And doing that comes with an energetic cost. You have to put energy in the system. To put energy in the system or pull energy out of the system in order to make that transition. And the fact that that has to be done means that there is a barrier. There's a barrier to overcome. And that barrier is what causes the mater stability, that these transitions are not so easy to do. They take time, and the system can linger around in one phase before it moves to the other. And a good example to think about is condensation. Condensation. So when you have a vapor, let's say water, and you cool it down, you're not going to, you come, let's say, from 101 degrees Celsius to 99. Well, it's not immediately going to become a liquid. You get what is called a metastable vapor. And before you get condensation, you have to wait until in this vapor the system. In this vapor, the system creates a little droplet of liquid. And if this droplet is big enough, then somehow the rest of the vapor is going to rain on top of it, and you see that you get the condensation. But when you're just at the boundary of around 100 degrees Celsius, this may take a very long time. And when that happens, we say that we are in a metastable situation. You do condensate, but you do it very slowly. Condensate, but you do it very slowly, and you do it under the influence of random fluctuations. And what you can also say here is that what is this super-cooled vapor doing when it's in its metastable state? It is many times unsuccessfully trying to create this critical droplet, which is hard to create because it has to be big enough in order to then capture all of the rest of the vapor. Capture all of the rest of the vapor, and it fails, it fails, it fails, and suddenly it succeeds. And so, when you go over the hill and you condensate, you do this after a random time. And we're trying to understand what the scale of this random time is. What is this critical droplet? How is the system actually managing to do that? And the answer to these questions depends. These questions depend on the particular model that you're looking at. So, here's a paradigm picture. There's a kind of state space which I've drawn in a one-dimensional line. This is, of course, ridiculous because it's just a caricature. State space is usually much more complicated. And every configuration has an energy. And then, very often, in a metastable situation, you have a picture that looks roughly like That looks roughly like this: a kind of double well. And there is a stable state which has the lowest energy, which is where the system will end up if you run it long enough. But if you start the system deep in a valley that is not the deepest valley, but still very deep, the system will linger around under the fluctuations here for a very long time until by chance it sort of manages to go over the top and then end up. Over the top, and then end up in a lower energy equilibrium. And the thing that it has to create in order to do that is some sort of saddle point, which would play the role of a critical droplet. And this is the situation that we are typically having. But again, this is a gross caricature because the state space is not linear. And also, these metastable states and critical droplets and stable states. And critical droplets and stable states, they can be quite complex. And we will have to deal with that as we go along. But this is a fair picture to keep in mind as a kind of guideline for what it is that we're trying to do when we do metastability. Okay, here are three pictures of metastable systems. You know, here's a cloud, it may start raining. Here's a cloud, it may start raining. This is a supersaturated vapor, and when things start to rain, this is very much like a metastable crossover. So it's a nice everyday example of metastability being around us. Here's another very nice example. You can see nice movies on the internet where you put Where you put supercooled water in your freezer and you take it out very, very quietly and maybe minus three or minus four degrees Celsius. And then you pour it in a glass. And the moment you pour it out of the glass, it freezes instantly. And that's because the shaking of the water when it goes out of the glass provides the fluctuations. Provides the fluctuations that make your system go from a supercold liquid to ice. And so here's another example. This is a bit more difficult to do in practice, but it works. And for the ski fans, well, an avalanche, a snow avalanche is another example of a matrix table crossover. You have the snow hanging there, it can fall any moment of time. It can fall any moment of time, and then somebody makes a big noise, and the snow starts rolling down. Not a bad example at all if you like skiing. Okay, now in statistical physics, we have been looking for more than 150 years at particle systems, trying to describe them, trying to particle systems with long and short. Particle systems with long and short range interactions. We have understood critical phenomena, critical behavior, phase transitions. And so we have 150 years of very successful theory trying to understand the cooperative behavior of interacting particle systems. And examples are spin-flip systems, so easing systems where the easing spins can go up and down. The easing spins can go up and down. You have lattice gases where particles can hop around. You have cellular automata. And you also have things like interacting disks in a plane. So in the last lecture, we're going to talk about a continuum model to deal with that. And there is a kind of splitting in the sense that we have been very successful over the years to describe discrete particle sites. Particle systems, but the moment you get into a continuum, life becomes much harder. And in fact, even today, there are only very few examples where we have been able to prove a phase transition and understand it in great detail in continuum models. So, continuum models are a challenge of their own in statistical physics and also in metastability. And we will see in the last lecture. In the last lecture, an example of that. Okay, so after having given you this soft introduction, I'd like to zoom in a little bit more into the history. I'm going to be very brief because meta stability has a long history, and in fact, nobody has ever written a good book or a good review paper about this. Or a good review paper about this. I think that would be quite interesting. One can say that metastability sort of started in the late 19th century with the work of two chemists, Valet Hoff and later Svante Arrhenius. And they were both interested in chemical reaction rates. They say, well, it appears that when you have chemical compounds reacting and turning into other Compounds reacting and turning into other chemical compounds, they have to overcome energy barriers. And they were trying to make computations with how fast chemical reactions would proceed, either slow or fast, trying to build some sort of theory of something must climb over some sort of barrier in order to make these chemical reactions happen. And they were quite successful. It was in the beginning very. Was in the beginning very phenomenological what they did, but they made great progress. And it was, but it took around until the 1940s, until Kamers started to look at metastability from a more mathematical angle. He was a mathematical physicist and he wanted to take. He wanted to take some of these questions to a higher mathematical level, which he did. And he was looking at Brownian motion, trying to escape from potential wells. And he wrote down some beautiful things in a very important paper that sort of started off the subject mathematically. And then there were a number of interesting developments over the years in the 60s and 70s. Oliver Penrose and Joe Lebowich were dealing with metastability for van der Waals in van der Waals model. So these are kind of mean field-like models in which they were trying to describe metastable behavior. At about the same time, Friedlin and Wenzel wrote a book about stochastic differential equations in which they were also looking at behavior of solutions. Behavior of solutions that can linger around for a long time in a certain part of the configuration space and then hop over to others. So, they developed a good theory more from the point of view of dynamical systems. In the early 80s, there was a wonderful paper by Cassandra Galfas, Olivieri, and Vares, who pulled these ideas into interacting particle systems and started to use. Particle systems and started to use large deviations in order to make progress on understanding metastable behavior for a number of interacting particle systems. And at around the same time, there was also a development around metastability by using spectral theory. So by trying to understand the dynamics by looking at spectra and finding largest eigenvalues. And finding largest eigenvalues, gaps between the largest eigenvalue and the second largest eigenvalue. And at that time, there were several theories being developed and gradually making progress on understanding metastable behavior for interacting particle systems, but also for dynamical systems and other things alike. The angle that that I want to take here is that of potential theory. And the idea is that potential theory is something that, in a sense, talks about electric networks, and that theory is very powerful in trying. Powerful in trying to understand metastable behavior. And this idea was put forward in a seminal paper by Bovier, Ekov, Gara and Klein in 2000. They were using potential theory and basic tools as we know them from potential theorem and applied them to metastable systems. And later you will appreciate. And later, you will appreciate why that is a very good idea, because it will turn out, and we will see that later, that potential theory is able to do something very powerful in understanding metastability. So they were working, they were trying to set up, borrowing from potential theory, a theory of metastability of Markov processes. Processes and they and what you have to realize is that there is a kind of dictionary that translates metastability properties of Markov processes into electric properties, into properties of electric networks. And the way this goes about is that you think of a state or a configuration. State or a configuration as a node in a network that you think of a transition going from one configuration to another configuration as an edge connecting two nodes. And then the rate of that transition from along an edge from one node to another, you would compare with a conductance that you associate with this edge. And then the And then the hitting time to go, let's say, from one configuration to the other when you do that, of your Markov process, you would like to link it with effective resistance or effective conductance between two nodes. So, if you want to say, I want to go from one state to another, what I think of, I attach a battery to these two nodes, and I'm trying. To these two nodes, and I'm trying to measure what the effective resistance is in the network between these nodes. So there is a kind of dictionary that allows you to go from the world of Markov processes on configuration spaces and transition rates to translate it into the world of electric networks and conductances and effective conductances. And that world is very suitable for. Very suitable for a description with potential theory. So that is why this translation allows you to actually benefit from classical potential theory in order to understand Markov processes and in particular metastability of Markov processes because we will see that the metastability will result in certain questions about these electric networks. Questions about these electric networks becoming very computable. Because if I give you an arbitrary electric network and arbitrary conductances, and I ask you to compute an effective resistance, this is typically not so easy. But when you try to model material of Markov processes, it turns out that something very beautiful is happening, which I will talk about in a second. And that will make this. And that will make this theory suddenly very manageable. And that is an important observation. And I think it's to the credit of Bovier Echo Figueroa and Klein that they actually saw that and applied it and made it work. Okay. Now we're going to have to be We're going to have to become a bit more mathematical. So, now, after this general introduction and having brought you to the point of saying, well, potential theory is a really important tool to start to think about stability of interacting particle systems. I think we are ready to start to become a bit more mathematical, talk about some notations, some. About some notations, some key objects that we'd like to talk about. And we'll be working our way up to a very important formula that sort of solidifies this link between the world of metastable Markov processes and potential theory. Now, I think this would be also a natural moment to have a little break because from this moment on I'm going to become more mathematical. Or become more mathematical. So, Ed, would it be fine to have a few minutes' break here? Yeah, maybe we'll first see if there's any questions from the high altitude viewpoint that we've had. See, you're much too clear, Frank. Everyone understands. I have a naive question. So, you've talked a lot about. You've talked a lot about the qualitative properties of these systems. Is there any examples of actual quantitative predictions where in this first lecture, I'm only setting up the stage, but once we get to lectures two, three, and four, it will all be about really making this quantitative for three. Quantitative for three different classes of examples and really getting to the details of this. And then also you will see certain universality features coming to the surface. But this is all about really head-on computing the quantitative properties of metastability. Sure. I was actually wondering about experiments. Ah, well, experiments. Ah, well, experiments more. Yeah, yeah, experiments. There are plenty of them. You know, it's difficult to simulate because you are often talking about very long time scales and it's hard to do a good simulation. You can do biosampling, of course. But in experiments, you see this. There are many examples in physical systems and chemical systems. Like I said, chemical reactions are already a clear example. A clear example of a metastable situation. Well, even a financial crisis often is something that has to do with a metastable crossover. But except that those systems are a bit harder than interacting particle systems. Yeah. The world does that. Are there any other questions for Frank? Sorry, can you turn? Can you let people unmute themselves if need be? That may be. If need be, that might be. Yes, they can unmute themselves. Okay. Okay. If not, let's take a three-minute break and have a bit of a stretch. Yeah. Okay. Very good. There is a question in the chat. Maybe I can say something in the meantime. If there is a there is a question about mean field, yeah, should I wait? Should I wait for Frank? There is a question about Minfield, but maybe we wait. Maybe people are not around during the break. Yeah, I think that's fine. Yeah. We wait for Frank, no? Or maybe we wait for three minutes, I don't know. Yeah, maybe we wait for three minutes. I don't know. There is a question about mid-field, but I think We will specify this in the third lecture. Yeah, then the third lecture will talk about global dynamics on random graphs and on the complete graph. But yeah, in a mean field setting, the interaction is easier because everybody is interacting with everybody equally strongly. And this means that you can project the system often down to a much lower dimensional system, which you Lower dimensional system, which you should then analyze, and this still poses challenges, but it's a way of simplifying a system. Yeah, it actually goes back to the very first set of lectures we've had in this series in Curry West. So it's a nice connection. Okay, so you tell me when I. When I start again, I think it's a good time to start now, Frank. Okay. So now we become a bit more question here. Yeah. Okay. Yeah, sure. So there is a question about phase transitions for the continuous for continuous systems. Well, I think that maybe I can answer to that. Yeah, please go ahead, Degrees. Essentially, no, only two models in the continuum for which phase transitions are. Continuum for which phase transitions have been proved rigorously. And one is the Widom-Rawlinson model. And we are going to discuss this in the last lecture. And one is a model introduced by Lebovitz, Mazel and Prezutti, but which is where the interactions are not decaying very fast. So apart from these two models, I actually don't know any other model. Yeah, where does, yeah, no, and that there's no full Yeah, no, and that there's no full description of the critical behavior. That's a big challenge. Yeah, there are many other questions. Okay, so question from Anthony Noriega. How is metastability related to retracing time in Gibbs theory? And could there be any relationship to electricity, as in your comments, to attain higher performance? I don't think, I don't see the I don't see the link in the second part with electricity. I'm not sure what the first part of the question actually refers to. Can you repeat that again, Ed? Well, I'm just reading. Anthony, do you want to unmute yourself and ask the question? Might be more effective. If not, how is misostability related to retracing time? Retracing time, space-time in Gibbs theory? No, no, because for metastability, you need really, okay, you have a dynamical system, but this dynamical system will usually have a Gibbs measure as a reversible equilibrium. So it's about crossing through and ending up in a certain equilibrium. I don't see a space-time picture here. There's no, I don't see a particular. I don't see a particular advantage of trying to draw a space-time picture to do that here. Maybe if you start to think, for instance, about vegetability of the contact process, maybe you then want to compare it with directed percolation. So there may be links there, but I don't certainly not within the realm of Markov theory is this sort of popping out in any prominent way. Okay. Okay. Okay. There's one quick question from Alex. He was asking about your statement that metastability is always a dynamical manifestation of first order phase transitions. Is this literally true or is this just a general feeling? This is close to literally true. I mean, I want to say that it's not a one-to-one statement, but metastability is always connected with having to. Connected with having to cross over a big barrier. You know, once you go to second-order phase transitions, there can be metastable-like behavior, but it's not of the type that we are discussing in these lectures. There's an interesting question from Peter Winkler we missed. Okay. Yeah, he's asking about jumping the other way. You know, if you can jump from metastable to stable, can you jump the other way out of the minimum to the meta-stable? Out of the minimum to the metastable stable. Yeah, absolutely. That is also a metastable transition. It's going to take even longer because you have to come from deeper. But the theory certainly allows you to understand that behavior too. But there are certain quirks that you have to deal with that may make the answer, well, will make the answer a bit different. But in principle, that all fits into the theory. It's going from one. It's going from one valley to another, and you can also have situations where you have a whole series of valleys behind each other, and you have to go through a whole sequence of metastable transitions, and the system only gradually goes down. This can be related to aging phenomena, for instance, that also happens. So, yes, it can go in both ways. Okay, so I'm going to try and be a bit. So, I'm going to try and be a bit formal now. And so, we're thinking of an interacting particle system where there's a set of configurations, which I'm going to denote by capital omega. For ease of exposition, we're first going to imagine that this is a finite set, but you can certainly extend that. And then we are going to think of a dynamic on this configuration space. It will be a Markov process that. Space, it will be a Markov process that comes with a generator. So I'm thinking here now of continuous time, but you can also do this in discrete time, of course. And this generator is such that it's acting on a very large class of test functions that it knows how to deal with that go from the configuration space to the real numbers. And so those are the settings. And we're going to look at various examples. We're going to look at various examples of such Markov generators. There are many choices, and we'll be looking at three examples in the coming three lectures. And in all cases, the dynamics is such that if you run it long enough, it will converge to an equilibrium and it will typically converge to a reversible equilibrium. And this equilibrium is given by a Gibbs measure, so a measure that lives on the configuration space. The configuration space, and it typically has a number of ingredients in it. There is going to be a Q, which plays the role of a reference measure. There's going to be a kind of Boltzmann weight given by a Hamiltonian and some parameter which plays the role of inverse temperature. And then you will have to normalize it in order to turn this into a probability measure. And so you will. You will have a dynamics that will have an object like this as its reversible equilibrium. And I listed here on the next slide again what these ingredients are. So h that appears in your Boltzmann factor is a function that assigns to every configuration a real number. And you think of this as the energy that you want to associate with a configuration eta. And again, we will see examples later on. Again, we will see examples later on. We'll talk about easing model and lattice gases and interacting disks. There is a strength parameter, beta, which is the inverse temperature in your Boltzmann vector. There is a reference measure. So this says if there is no interaction at all, what does the system look like? This is very often some kind of uniform distribution on your configuration space, but you will. On your configuration space, but you will have to choose it. It's what your system would do if there would be no interactions. And then there is a normalizing partition function that is also coming into this Gibbs measure, very important function that depends on all these parameters. And depending on what you want to model, you have to choose a configuration space, you have to choose an energy functional, you have to choose a temperature. Functional, you have to choose a temperature and you have to choose a reference measure. And one example where metastable behavior appears is when this parameter of the inverse temperature is going to become very large. So this is a kind of low temperature situation where you say, well, it's going to very much depend. This is going to concentrate around the local minima of this energy functional. That's where. Of this energy functional. That's where most of the Gibbs measure is going to be. So, for very large beta, this measure is going to live largely around the local minima. And therefore, these minima become very pronounced in describing what the system is going to do when it follows this dynamics that has this particular equilibrium. So, we will have to fill this in. This is in concrete cases, and that's what we're going to do. But this is sort of the setting mathematically that you have to deal with: configuration space, energy functional, inverse temperature, reference measure. Okay. And typically, this Hamiltonian has three important sets of configurations, which very much correspond to the paradigm picture. To the paradigm picture that I drew at the very beginning. It is there will be a global minimum, a state where you say they have the lowest possible energy. There will then be typically a local minimum that is not the global minima. There could be more than one, but very often we prefer to look at the situation where there's only one other deeper. Only one other deep state. So this would be a state that is at the bottom of the deepest value that is not containing the lowest energy value. And then there will be a saddle point connecting these stable and metastable states. And this is what we would refer to as the critical droplet. And this is a kind of ridge between the two valleys that contain the stable and the metastable. The stable and the metastable state. So, if I would turn this again into the picture, the paradigm picture that we had before, you say there is a configuration space. Again, as a caricature, I draw it in a one-dimensional fashion, but that's, of course, silly, but helpful. And then your energy functional typically has a kind of double well, and the stable state would be at the bottom. And the stable state would be at the bottom here. The metastable state would be the one at the next deepest value. And here would be something that would connect the two. And, you know, thinking about Peter Winkler's question, of course, there could be situations where there are three or four or five valleys. We do know how to deal with that. But let's focus on the simple setting where you just have to climb out of one valley and go. Of one valley and go to another valley. And as I said before, these states are not, they usually have a lot of internal structure. This is really a caricature picture, but there's lots of micro details going on when you really go into what this configuration space is. It's not a linear space at all. And some of the theory has to focus on what is happening. To focus on what is happening really here and what could be happening elsewhere in the state space in order to really understand what is going on. And we will be dealing with spin flip systems, we will be dealing with particle hop systems, and we'll be dealing with interacting disks in the continuum. And so in the case of spin-flit systems, omega would just be. Omega would just be, you know, all possible easing spin configurations on a finite piece of the lattice. If you are talking about a lattice gas, it could be 0, 1 to a finite set. So 0 meaning the site is empty and 1 meaning the site is occupied. If you're having disks, then you can talk about point processes and disks around the point. So the configuration space can mean different things. And the Hamiltonian will express. And the Hamiltonian will express how the various components in your particle system are interacting with each other. And as a result, every particle configuration comes with a certain energy that you have to specify. That's part of your model choice. And then you have to talk about a reference measure on the configurations that you want to start out with, maybe something like a Lebesgue measure or a counting measure. And then you have to start. And then you have to start to understand very much in detail what this curve looks like. That is the challenge when you want to go into the details of metastability. Okay, now I'm going to talk about a very important formula. This was a formula that in this seminal paper by Bovier, Ekov, Gerard, and And Klein in 2000, they were using this formula and made it work for them in a metastable context. So there's a beautiful formula that links on the left-hand side a quantity that is the expected time that the system needs to go from the metastable state to the stable state. So, how long does it take when you start here until you arrive there? And that's a key quantity in image stability. How big is this? And potential theory tells you that this is computable in terms of a number of objects. First, there is the harmonic function, and then there is something called capacity, and then also the Gibbs measure plays a role. And in the exercise that comes with this lecture, we are trying to make Are trying to make you derive this formula. And on the right-hand side are all objects from potential theory. Importantly, there's the harmonic function. So the harmonic function says, if I start in a configuration eta, anywhere in my configuration space, what is the probability that I will hit the meta stable state before I hit the stable state? Before I hit the stable state. So, this is some function that is between 0 and 1. And it tells you, depending on where you start, with what probability you will end up in one rather than the other. And once you know this harmonic function and you can apply your generator to it, then you have a sum here, and that is what is called the capacity. Capacity. And as it is written here, this capacity is quite a complicated object to deal with, but we're going to get some nice variational principles for that object that can really be made to work. And then if you know that harmonic function and you know the capacity, then all you need to do in the numerator is to take the average of your harmonic function with respect to your Gibbs measure and divide it by the capacity. And the wonderful thing of this formula is that on the left is the world of metastability and dynamics, and on the right is potential theory. And in a way, potential theory is to a large extent something that is also equilibrium-like, but not quite here. Of course, the generator sits in here. But we will see what we can do with that. And there is a schematic picture here that. Picture here that since the harmonic function is defined to be the probability that you end up in the metastable state rather than the stable state first, this harmonic function typically in your configuration space has a very simple global structure. In the valley around the metastable state, it's very close to one. In the valley around the stable state, it's very close to zero. And then somewhere on the ridge. And then, somewhere on the ridge, where there is this critical, yeah, this settle point, this critical configuration, which can be have a whole structure of its own, that's where the harmonic function starts to drop from one to zero. And therefore, a lot of the interesting behavior of what the harmonic function is doing depends on what it is doing in the neighborhood of the critical droplet. And we will see shortly that that is the clue to being able. To being able to make progress on understanding metastable questions, because this harmonic function on large parts of the configuration space is really trivial. It's either zero or one. And all the action sits close to the critical point. So now that idea is very important because it turns out that in major stable regimes, this sum here that is Sum here that was on the numerator actually simplifies a lot because it is entirely living on the metastable state. This function is one on the metastable state. It becomes tremendously small when you move away from the metastable state in such a way that actually this sum is essentially just coming from the metastable state alone. And that would mean that this simplifies to just the Gibbs measure. To just the Gibbs measure at the metastable state. And if I fill that in, I see that my average crossover time becomes a very simple function. It is just something related to the energy of the metastable state, which sometimes is zero. And you are left here with the capacity. And it was to the credit of Beauvier, Ekov, Gera, and Klein to realize that in Metastable situations. To realize that in metastable situations, this is almost always happening. You have to prove it, but it is almost always happening. And that, therefore, if you want to understand metastable crossover times, you just only have to compute capacities. There's a couple of questions on the chat. Okay, so one was about the key formula, whether or not that holds in for. Holds in for infinite state spaces, or is it restricted to finite state spaces? No, it also happens, you can use this in also in infinite state spaces, but then of course sums will become integrals. This formula is restricted to the case where the metastable state is a single configuration. If it is a richer set, this will have to be amended a little bit. And in the lecture, in the third lecture, I will say a little bit about that. So, this formula is more general than this particular case, but there are certain amendments that you have to do in more general settings, yes. And M can either be a single state or it could be a set of metastable states. But this formula will become more complicated when this is a set of single states, but for this formula. Single states, but for this formula, it doesn't matter whether the stable state is a single state or many states, it doesn't matter. Okay, so this is the simplest form of a formula that is true in greater generality. Okay, very good. Now, so the conclusion is if you understand capacities, you understand metastable crossover times. Made the stable crossover times. And now the wonderful thing is capacity has two dual variational principles that work together beautifully. And I'm just reminding something here from standard potential theory that there is something called a Dirichlet form. It's a quadratic form that depends on test functions on your so those are. On your so those are functions from your configuration space to the unit interval. And if you look at all possible transitions and you multiply the rate of the transition to go from one configuration to another, you multiply it by the Gibbs measure, you get a functional of your test function. And the capacity is given by the Dirichlet principle that says if you minimize this Dirichlet form over all functions. Over all functions that are arbitrary, but they are one at the metastable state and they're zero at the stable state, or at all points in the stable state, then this minimization problem gives you the capacity. And it is also known that the minimizer is obtained at the harmonic function here, so it's this harmonic function. function here. So it's this harmonic function that is the minimizing test function of that variational formula. And since you see that this harmonic function is trivial in most of the configuration space, it's either close to one or close to zero, you see that in most of the configuration space, this isn't doing anything. It's one minus one or zero minus zero or something that is close to zero minus something that's close to zero. So this is extremely small. So, this is extremely small when you're not close to where the harmonic function starts to change. And that was the big idea in the seminal paper of Bouvier, Ekovgering, and Klein that I mentioned, that they realized that this huge variational formula that can be living in a big configurational space actually reduces to a much smaller and lower dimensional variational formula. Lower dimensional variation formula near the critical droplet, near the barrier between these two values. And that was the big idea. There's also a dual variational principle, which is living in the space of flows. So flows going from one node to another along an edge. And between configurations, you can think of that as an edge. And then there Think of that as an edge. And then there is another functional that has these weights upside down compared to the Dirichlet form. And here's a square of a flow. And a flow is any flow through your network where in the metastable state, a unit flow is coming out. And in the stable state, the flow is coming in. And it's a unit flow. And everywhere else, Everywhere else, the total inflow and outflow is the same. And so there's a standard variational formula that says, also, in terms of flows, I can write my capacity as something that involves taking the supremum over a set of flows of a certain basic functional. And those two variational principles together are wonderful because. Are wonderful because the Dirichlet principle is an infimum and the Thomson principle is a supremum. And that is a wonderful idea because it means any test function you will pick and put it into your Dirichlet form will give you an upper bound on the capacity. Any unit flow that you pick and you put it into the dual of the Dirichlet principle will give you Principle will give you a lower bound on the capacity. And so the game is on to start to pick nice test functions and nice test flows in order to get good upper and lower bounds on your capacity. And what happens in metastable situations is that if you understand really what the system is doing physically, you can come up with You can come up with good choices of these test functions that make these upper and lower bounds be close to each other. And if you're very lucky, they're even going to be to leading order the same. And if you're lucky that you're able to do that, then you have really squeezed, you have sentwitched your capacity between two bounds that you can control. And once you have done that, because of our basic form. Because of our basic formula, you have really gotten a quantitative expression for your crossover. So I'm about to start to round off by making a few more very qualitative remarks to emphasize what it is that we have been doing. The key idea is that in metastable regimes, this high-dimensional This high-dimensional Dirichlet form and the dual Dirichlet form that comes in the Thomson principle, they are large variational problems, but they are largely controlled about what is happening with your harmonic function or what is called a harmonic flow near the set of critical droplets. And so, effectively, a very high-dimensional variational formula waters down. Waters down to a low-dimensional set of, you know, low-dimensional variational formula close to your critical droplet. And there are simple cases where you can just immediately solve this in a blow. And there are more complicated cases where you have to do a bit more work to deal with these problems. But that is a very powerful thing that comes with metastability. In metastable regimes, this is what is happening. This is what is happening. And again, this is an enormous, important observation in order to really go and quantify things. There's a guiding principle of this key formula linking metastable crossover times to capacities. And since we have good control of capacities through these variational principles, we have good control on many. Control on metastable crossover times. And so we are effectively able to link a non-equilibrium phenomenon like metastability to capacity properties which are a bit equilibrium-like. And the capacity plays the role of a kind of inverse effective resistance. Sorry. And it is the deal. And it is the Dirichlet principle and the Thompson principle that are at the heart of the technical computation because they allow you to get upper and lower bounds by doing test functions and test flows. And if you're clever and you have the right insight, and often you have to have physics inside to do that, you can make a choice and make them match asymptotically. And then, and if you can make that work, you're happy because you have really. Happy because you have really corrected the problem, and that is what metastability is about. I'm close to finishing. I have only given you a kind of framework. In the next three lectures, I'm going to give you three examples where we're going to make this work. And Aden and I have. And Aiden and I have also made exercises to make you help understand what you need to do in order to make that work. What I wrote down is to make things easy for a finite state space, but you can work in infinite state spaces, discrete or continuous. I was warning you before that when you go to continuous spaces, life gets harder. Also, statistical physics has more problems with continuous space. Physics has more problems with continuous state spaces, but in the last lecture we will deal with one of them. And I've been talking about this meta-stable state, critical droplets, stable state, as if they were single configurations, but in many cases, they're actually sets of configurations that have their own geometric structure. And part of the computation of metastasis. Of the computation of metastability, beta stability will be that you have to come to grips with what they are, and in particular, since all the action appears to be around the critical droplets, you have to get to terms with what is that. So what really is the best barrier for you to move through in order to go from the metastable to the stable state? And there are lots of interesting things mathematically which have to do with the state. Things mathematically which have to do with isoperometric inequalities and related stuff that I will come to talk about in the next three lectures and that also with Einena, we worked out that in detail in the exercises to make you get a feel for that. So, with that, I'm closing and telling you that the next lecture will be about Kawasaki dynamics. About Kavasaki dynamics on lattices. So, lattice gases, particles hopping around and interacting with each other in a nice geometric, you know, regular lattice structure. Then in lecture two, we will be talking about Glauber spin-flipper dynamics on random graphs. So, again, a completely different kind of setting where you are living not on a lattice but on a random graph. But on a random graph. And in the last lecture, as Elena already mentioned, we will be looking at the Willem-Raulusen dynamics on the continuum. So these are interacting disks that can disappear and reappear and they interact with each other. And so these three things, lattices, random graphs, and the continuum, will be three very different examples of how you can make that work. So I'm looking forward to you coming and listening to those. Listen to those lectures where we're really going to zoom in on specific models and make the general theory that I try to explain in rough terms here, make it precise and make it work and make it become quantitative. That's very important. Thank you. Okay. Thanks a lot, Frank. So, Sarah, I will now unmute people. And I'm hoping that you can join me. You can join me in thanking Frank for a very exciting lecture. And the lights can come up now. So before we turn to questions, just a reminder that tomorrow's lecture will be using the same link. And look forward to seeing you there. So if people want to turn, I think we're going to stop the recording now. Stop the recording now. It's my understanding. So, if people want to turn their videos on or turn the lights up, you can ask the questions while being seen and heard simultaneously.