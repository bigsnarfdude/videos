Thank you very much for joining us, and we're looking forward to your talk. Okay, thank you for the invitation. Again, for some of you who don't know me, my name is Yabin. I'm currently a postdoc at University of Michigan. And today I want to talk about some of the work that I started when I was a PhD student working with Adriana. Adriana, and we actually recently come back to this. We actually continue working on this more from the application side of point, but recently we actually come back to this and have a improved version of the solver available on our archive. So, and I'm very excited to share this with you. I'm feeling a little bit weird today. So, if I start coughing or sneeze, I'll just try to mute myself to make this more. Myself to make this more appropriate. But just want to apologize in advance if I start doing that. Okay, so the thing I'm going to talk about today is a fast-track solver for quasi-periodic structuring problems in 2D in layered media. So let's start with the motivation. So really, the major applications we are hoping We are hoping one day we could solve or tackle is this kind of design problems where domain experts actually solve this kind of wave problems. And the ultimate goal is actually to come up with this kind of nice optic devices that it can manipulate light. For example, one of these will be solar cells. And then also the bottom one I have here is a metal. I have here is a metal lens that can basically accomplish tasks that traditional lenses cannot. And so let's start with something actually way more simple than the very powerful metal lenses. So let me start by introducing what is quasi-periodic scattering in the, I would say, most simple setup. So this is a two-layer example. So what do you have? Layer example. So, what you have is a two-layer structure, and these are actually infinitely long. And their interfaces is some kind of periodic shape. And D here is the length of the period. And for each of the layers, they actually mathematically, they will have corresponds to a different wave number. And this really physically, this corresponds to different material you use to actually form the. You use to actually form these layers. And for the basic setup, we will actually indicate the interface by gamma. And again, as I mentioned, these two layers actually going to have, we usually have different wave number between them. Okay. And then in a very simple but pretty standard setting, you could have a Setting, you could have an incident wave in the top layer, all the bottom layer. The setup will be similar, but you have the incident wave in the top layer. And as a result, you will have scatter field. And the goal is to really find the total field, which is the instant field plus the scatter field. And the major goal for solving this kind of problem is to actually utilize the fact that each part of the field is quasi-periodic. Of the field is quasi-periodic. So, this basically means if you have the solution or the field value u at x plus d, d is the periodicity, comma y, this actually equals to alpha times u at xy. Well, alpha is the block phase, which depends on your incident angle. And also, if your incident wave is in the top layer, it also depends on the wave number in the top layer. Here. So our solver is based on the boundary integral formulation given out by Chow and Barnett, I think in 2015. So for a simple two-layer structure, what you have, the original problem is you first you have different Hamholes equation with different wave number in each of the layers, and then you need to actually enforce continuity in both the solution and also the flow. The solution and also the flux. And also, of course, you have the to enforce the outgoing radiation condition. So, to come up with a boundary integral formulation for this, the major idea is to actually represent solution as only a potential coming from the neighboring periods. Although this is infinitely long, you only have explicit terms coming from the neighboring periods, and also all. And also, all the contribution from the faraway peers is actually captured by this term called the far field. And the far field is actually written out in a very simple format. It's actually written as linear combination with some coefficients which is unknown, and we're going to solve it in the solver and multiply by some basis functions placed on some proxy circle. Placed on some proxy circle that kind of shows away the far-field copies from the ones that actually have explicit terms in your solution representation. And with this setup, we need to actually enforce something explicitly. One condition that we need to enforce is to make sure our solution actually satisfy the quasi-periodic. Satisfy the quasi-periodicity, which we know the solution should satisfy. And in the Child and Barnett formulation, this quasi-periodicity is actually enforced manually by placing these kind of walls structures. And what you do is actually you manually enforce that the solution measured at these walls, the discrepancy between them actually match the quasi-PRI. The quasi-periodicity of the solution. And also, you basically need to verify this for both the top layer and the bottom layer. And finally, there's one thing that I haven't talked about, which is the radiation condition. So in the Child and Burnett formulation, this is actually enforced by writing solutions very far away vertically from your center region. Center region as a really block expansion. And you add an actual condition which actually enforces the representation you have here to be equal to your expansion in the very further away vertically region. So, by all of these formulations set up, by the end of the day, after disclarization, you will have this linear system. So So the red block here, A, is the one that is usually the largest because the size actually depends on how many points you place on the interface, versus the others are some kind of the size really depends on parameters, which are usually much smaller than the number of points you place on the interface. Okay, so again, this is from the original paper. Again, this is from the original paper by Chaudh Bennett. So, right now, the rest of the talk will start talking about actually how do you actually solve this system. So, this is again, very standard, simple idea. So, I mean, to solve such kind of system, we can actually, first thing we do is actually follow the original paper and try to. Original paper and try to rewrite these blocks and unknowns in a slightly better way. What we did is actually very naive. What we did is actually put CA here, reorder them into, and start calling it X, and then you can actually kind of merge all of these blocks here into simple notation. So basically, these two linear systems are exactly the same, but with row and column rearranged. With row and column rearranged. Okay, so why do we want to rewrite this? Basically, once you rewrite this, you have a pretty version, pretty version of the sparsity of the linear system. And then it's just easy to, it's more friendly for the computational side. But once you do this rewriting, this is very simple, right? There's a two-way to, there's multiple ways. Way to there's multiple ways, but two most obvious ways for doing sharepoint complement for the system is first. Uh, for example, you solve first in the option first option here, you solve for sigma first, and then you plug in, actually sorry, in the first one here, you solve for x here first, and then you plug in to obtain sigma. Alternatively, you can do the second option for the short complement, which is you solve for sigma first, and then you plug in sigma. And then you plug in sigma to retrieve your X. And both in our, so based on our work, both options work. And if you look at what's the difference between these two options, they actually have very much similarity because they are both based on the idea of shared complement. For example, if you look at the first option here, you need to invert one matrix, which is presumably the largest matrix in this entire Largest matrix in this entire linear system. And then once you invert that matrix, you apply the inverse, you also need to deal with this kind of pseudo-inverse to retrieve your x value. So for the second option, you still have to invert a matrix, different the first one you invert in the first option, but same in size. And again, you also need to deal with another pseudo-inverse. So basically, there are two major steps in solving the linear system using either of these. The linear system using either of these options. One is the large inverse, one is the pseudo-inverse apply. So again, both approaches will work. And as we'll see later in the talk, even if you apply to problems more layers, these two approaches both work, but there's slightly a difference in their efficiency. And the difference in efficiency major actually happens. Efficiency major actually happens in the pseudo-inverse part rather than the inverse part. But one thing in common in both solvers, we actually do not construct the pseudo-inverse directly because it's not really stable. What we did is we use a truncated SUVD to actually apply the pseudo-inverse. Okay, so let's start with option one. This is option that we published a while ago. So for option one, So, for option one, recall the major step is first apply the inverse of A and then apply the pseudo inverse. For option one, there's nothing really fancy about pseudo-inverse. What do you do? You just construct the pseudo-inverse and then apply it via SYD. So, the major thing I'm going to talk about is actually how you construct the inverse of A. So, the inverse of A, the major takeaway is actually you can rewrite A into You can rewrite A into different parts, added together. So you can write A as A0 plus the block phase to the negative one power multiplied by A negative one, where A negative one is actually something like the interaction between the center period with the period on its immediate left. And then similarly, you have this additional term that is a black space alpha. alpha multiplied by a plus one, which is the interaction of the centerpiece with its immediate right neighbor. So the good thing for writing this as the addition of the three parts is actually we actually know how to handle each parts and also at the very end apply the inverse of A very efficiently using this kind of structure. So first we know A naught is actually a block diagram. A naught is actually a block diagonal, and each of the diagonal blocks is actually in this case it's not a black diagonal diagonal, it's just one dense block. It's actually basically very much similar to a discretized integral operator, boundary integral operator defined on some geometry. So we can use fast drug solver, for example, the HVA solver for constructing the inverse of A naught. And then the other two blocks, they two blocks um they because their nature is actually capturing interaction that are not self-interaction so they can approximate it by low rank factorizations and what's more interesting about these low rank factorizations you can actually construct as them independent of the alpha values and then when you need them you just put them together and multiply by the corresponding alpha values and with these two pieces of information you can actually apply the inverse of a Actually, apply the inverse of A while a Woodberry formula. Of course, you will use the HBS solver for E0 when you actually apply the inverse of A while Woodberry formula efficiently. Okay, so this is pretty much all you need to know about option one. What about option two? Okay, so if we go back to the short complement for option two, the matrix inverse here you need to apply to the right hand side. Here, you need to apply to the right-hand side is actually this matrix. So it's A minus something or plus something. So actually, you can kind of use the similar idea from the apply inverse part of option one, which is write A as A naught plus these two terms. And we know these two terms can already be approximated by a low-rank characterization. So what's more interesting is actually by the nature of these blocks, By the nature of these blocks, B, Q, and C, this extra term here can also be approximated by another low-rank factorization. And you can basically concatenate these two low-rank factorization together and rewrite A as A0 plus another low-rank factorization. That means the inverse of A can also be applied while Wooderbury's formula similar to the inverse of A. Sorry, the inverse of this new matrix can be applied. New matrix can be applied similarly to the inverse of A, just with a different Laurent factorization. Okay, so this is a very simple setup, which one layer. So when we actually move to more layers, which is the real structure that we want to model, the major difference is first, your solution is going to change a little bit. For example, if you look at your solution in the third layer here, it's going to have Third layer here is going to have explicit terms from the top interface and also the bottom interface plus the far field. Okay, and of course you will see more block structure for all of the blocks here. So let me just show you a more straightforward picture for what A look like. So for these, this is a five layer for interface geometry. For this structure, A will basically look like this. So it's going to be Basically, it looks like this. So, it's going to be a block tridiagonal. But the similar idea of writing A or the diagonal block as some kind of self-interaction plus low-rank factorization for the non-self-interaction, the same idea still holds. And also, if you look at the off-diagonal block here, for example, A12, this will correspond to the interaction between different interfaces. So, they are non-self-interaction as well. So, you can also So, you can also basically write these as low-rank characterization as well. Okay. So, by the end of the day, you still can write A as something like this. You have a block diagonal matrix, and each of these blocks will be like a boundary integral operator discretized, and then plus another matrix, which is block triagonal, but the entire block tried. Block tridiagonal matrix can be written as a bunch of low-rank factorization, and you can put these blockwise low-rank factorization together to form your final giant L and R. So basically, by the end of the day, A will still be something like A naught plus a low-rank factorization. And again, each of these block-wise low-rank factorizations, you don't have to carry the value for alpha when you're constructing low-rank, you just need to. Characterization, you just need to construct it for the alpha-free version and then scale the alpha-free version by its corresponding value for alpha. Okay, so finally, let's talk about, so now we know how to actually how they actually, now we know actually how the multilayer case linear system look like, let's actually talk about how to solve them. How to solve them. So, many of the key components of the solution technique is already covered in the one-layer case. For example, as I mentioned, you always want to write the A by A naught plus some low-rank factorization to make sure you can later apply A inverse efficiently by Udebury formula. So, there's one thing that makes this kind of a little bit technical when you apply the multilayer version of A universe to F. Multilayer version of A inverse to F, which is the matrix you need to invert in the Woodberg formula is going to be large, right? Because this depends on the number of layers you have. And if you have a couple more layers, this could be something large enough that you can't really invert very nicely while density linear algebra. But the thing is, you don't need to do that because with a multilayer structure, this matrix I plus R A naught inverse. matrix I plus R A naught inverse L actually also is block sparse. And basically you can invert this while like a block version of Thomas algorithm for inverting tridiagonal matrix. So it's actually quite efficient even if you have many number of layers, even if I wouldn't say many numbers, but moderately large number of layers, for example, 10 layers. Okay. Okay, so this is apply inverse part, and again, apply inverse of A minus BQ dagger C needed in option two can be done very similarly. So there's another part which is apply the pseudo-inverse part. This is actually the part that we figure out why we want to actually implement option two and make this as an improved version. Because in the pseudo-inverse part, actually, the pseudo-inverse you need. Part actually, the pseudo-inverse you need for option two is way more efficient to construct or easier to construct than the pseudo-inverse you need in option one. And the idea is very simple because Q dagger or Q is actually block diagonal, right? So when you actually construct a pseudo inverse for Q, you're just actually constructing pseudo-inverse for each of the diagonal block individually and then put them together. Individually and then put them together. You don't have to even put them together. You can just apply them in your code. So, this actually resulted in an improved version or option two, which we think is more efficient than option one, specifically for the case we have a large number of layers. Okay, so for the numerical experiments, what we did in the most recent archive manuscript is actually we Is actually we implement this idea in option two. And then let me show you some of the results. So we for reporting the cost or the time running times, we break into four parts really. So for pre-computation, there's one, two, three. One is a part where the computation is independent of the block phase and also some kind of fascinating algebra. Also, some kind of fascinating algebra. For example, the HBS solver construction for the diagonal blocks of A0. And then for pre-computation 2, this is still alpha independent, meaning that they can be reused for different block face values. But these are values, these are computations that are not fast, more like dense linear algebra. For example, how we actually evaluate the other blocks, those are done while linear algebra. Those are done while linear algebra. And for pre-computation three, these are computations that actually depend on the incident angle for the top layer, but only through its dependence on the block phase. So technically, due to the periodicity of cosine function, you could have two different incident angles by sharing the same block face. And those kinds of computations are actually. Of computations are actually included in pre-computation three. And finally, for the solve, these are parts of the combination that actually directly depends on the incident angle. Different incident angle results in different computations. So, for the scaling test, basically the goal for the scaling test is to test the solver's cost should scale linearly with respect. Scale linearly with respect to first the number of layers. Secondly, assuming the interface geometry is already fully resolved, it should also scale linearly with the number of points you placed on the interface geometry. So you can see kind of if you walk down along each column, along the rows, you will see the interface actually changes. It actually increases. You actually increase doubles each time for each of the parts. And then, if you work horizontally, you will see the number of points placed on each interface, they actually got doubled every time. So, for the scaling test, the interfaces are very simple sine and cosine waves. We just multiply by different amplitude and added a bunch of waves of different frequency together. So, the interface is actually very smooth. But if you look at the time. But if you look at the timings, it's actually pretty clear that this is a linear scaling in both the horizontal and the vertical direction for each of these parts. And you can see the error actually decreased pretty fast as you increase the number of points on the interface. So the other test we did is more motivated from an optimal design setting. Design setting, which is in optimal design setting for these optic devices. You domain experts, you really need to solve model these, solve these humost problems with many different incident angles. So we did a test with a pretty complicated structure with 287 different incident angles in the top layer, which is Angle in the top layer, which corresponds to 24 different instant block faces. And as we can see here, it's actually pretty fast for all these angles. It's actually way more fast, 135 times speed up, compared to if you try to build solver for each of the instant angles independently. Okay, so let's see. We also did with replacing. We also did with replacing the interface. The solver is written very modular in the sense you can actually just swap out the interface, and all the extra new costs you need to do for solving this new problem with new interface geometry is roughly the ratio about how many your interface you're changing with respect to the total number of interface here. So, here we have two interfaces, I'm only changing one of them. So, the new cost is about one-tenth of the original cost. Okay. Okay, so to summarize what I talk about today, I present you an improved version of a solver for quasi-periodic multilayer scattering. This solver will scale linearly with respect to both the number of points on each interface and also the number of layers or number of interfaces. And it's very efficient in an optimal design setting if you want to sell for many different incident angles or if you want to swap out. You want to swap out an interface or even wave number, you can save a lot with this solver. And for some future directions, we're going to work on actually using the solver to solve some real application problem with our collaborators. And one day, we're going to start talking about Maxwell problems based on what we have so far on the Hamholst problems. And that will be all. Thank you. Thank you very much. Thank you very much. We have a few minutes for questions. Alex is exhaling. So, Javi, great talk. So, can you extend? Great talk. So, can you extend the scheme to 3D? Yeah, so that's the ultimate goal, right? So, but there are a couple of things that need to be addressed before extending this to 3D. First, quadrature, and also we need to be very careful if we want to for memory management and how we actually, we just need to be careful with lots of stuff. To be careful with a lot of stuff if you want to extend this to a 3D problem modeling like a structure with a large number of layers, because it's very easy to run off of memory for this. Well, I mean, there is no fundamental difficulty in restricting the calculation to the say a unit cell. So fundamental difficulty in In well, I mean, right for 2D, you know, you're doing calculation in just one period, right? And you introduce the parameter alpha. So what happens to 3D? So 3D, what I'm thinking in my head is more like you will have a unit box. Unit box, and then you have a proxy sphere. Does that make sense? Yeah, well, I mean, that's what one would expect, I guess. Yeah, so I think the major idea here still works in terms of the farm. Still works in terms of the formulation, but to make the solver work efficiently, there are lots of stuff that needs to be addressed. So, Shudang, Min Yang's already extended this to 3D. So, the integral formulation extends directly. And in terms of making the solver efficient, we're running in 3D. We're running in 3D, you know, there's lots of work to distribute at least here, and not much communication. So we have that going for us. But yeah, the solver itself should extend as well. Just needs the appropriate, you know, upgrades with dimension. All right, fantastic. Thank you. Any other questions? Hi Yabin, very nice work. I'm just wondering, what's the limitation to running this on a large machine? Is it, I mean, the number of layers that you are choosing, you're only going to about 10 layers, but. But is it RAM? Is it storing all of the compressed A matrices? Is that the main problem? So, first, the first version of this code only go to about 10 layers. Sorry about the mailing. The new version of this actually can go way more than 10 layers. For example, the largest one weighted here is 64. So, even with this, the major bottleneck is actually Actually, if you treat each of the interface geometry as different interface, the bottleneck is actually how you store that geometric or discretization information on the computer. This is actually right now with the new version where the bottleneck is. Okay, so it's not the HBS compressed A matrices? No, it's not the solver. It's more like how much information. How much information you need to store to actually let the solver know you're describing what it needs to solve. Okay, well, if that's the limitation, then yes, no one's going to fix that. So, but okay, yeah, keep up the good work. I hope you do it in 3D. You can combine it with Bobby Wu's coadjutor schemes and work with Min Yang, and it'll be a killer combo. Great. So, I think it's time for the next. Great. So I think it's time for the next speaker. So thank you very much, Yavin. Very nice talk.