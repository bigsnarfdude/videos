Today, we'll tell you about the project where we use a Bayesian and parametric approach to model multiple network data. And we address the problem of clustering multiple network observations. So, I think it's important to clarify since the beginning that what we are trying to do is clustering similar network observations together, and we are not addressing the problem of clustering the nodes within a single network. The proposal that I'm going to The proposal that I'm going to tell you about is actually based on a pretty simple idea. We are going to use a well-established instrument from Bayesian parametrics, that is, a Dirichlet process mixture model. What is a little bit unusual is that this time, this model is defined on the space of networks. This is joint work with Francesco Barile, who deserves most of the credit for the work. Deserves most of the credit for the work. Francesco is a PhD student at the University of Milano-Bicoca and is about to complete his degree. And there is also a Mexican connection in this work because the other author is Sim√≥n Luna Gomez from Italy, Mexico City. Here is the framework of my presentation. I will start by intro, sorry, this is the outline. I will start by introducing the framework and the notation. And this will be useful for me to be able to define the domain model. Define the main model we are going to consider. The Diricle process mixture of SER kernels, where SER stands for centered Erdos Reni kernel. I will discuss some aspects related to posterior computation, and then I will move to illustrations. First, I will tell you about some simulation study, and by rephrasing the comment of Young, this is to show that at least in some situations the method works. And then I will move to the analysis of human brain network data. Now, when it comes to statistics. network data. Now, when it comes to statistical models for networks, a very relevant issue regards the larger dimensionality of the observations. For this reason, most of the recent proposals in the literature actually rely on some dimensionality reduction strategy. This was not, I would say, the main idea when we designed our model. We wanted to find the model that was flexible, that was interpretable, and that was reasonably easy to implement. Easy to implement. Nonetheless, in the last part of the presentation, I will tell you about a possible approach to exploit this model to handle situations when the number of nodes, this capital N, is large. Okay, so the main object of our work is multiple network observations. So I'm talking about network observations defined on the same set of nodes. Depending on the literature you look at, this type of network Depending on the literature you look at, this type of data might have also other names. They are known as multiplex and multilayer networks. But if we refer to this type of structure in our specific setting, we are not considering the presence of edges across different layers. For us, each layer is one observation, it is one network. Now, this type of data are more and more common, I would say, many fields of application. Let me just mention a couple. The first one comes from neuro. couple. The first one comes from neuroscience and is actually the type of data that I'm going to illustrate later on in the presentation. And in this field, brain scans can be typically represented as a network where different nodes refer to different brain regions with different regions of the brain. And we have an edge connecting two nodes whenever the two corresponding brain regions are connecting. In brain regions, are connected in some sense that could be either functional or structural, as it will be for the scans that we consider in the illustration. Another example that I would like to, I don't know, sorry, let me also say that if we have several scans, then we have several network observations where the thing that remains fixed is the nodes, because the idea is that the brain regions, especially if we are talking about one single individual. About one single individual, or maybe it's scans of the brain of different individuals. Nonetheless, we can consider the brain region as fixed. What changes is whether these regions are connected or not. Another example I wanted to mention is the third one, trade market networks. So I like this type of data, and we actually analyzed it, although I'm not going to tell you about this analysis today. And if you think of a specific product, you might represent the Product, you might represent that the market for that product worldwide as a network. So, in this case, the nodes are the different countries in the world, and you might have an edge connecting to two nodes when there is some sort of trades for that specific product between those two countries. So, if we consider several products, we end up having several network observations on the same set of nodes. Now, there is a, I would say, a huge There is a, I would say, a huge body of literature when it comes to statistical models for networks. The situation is a bit different if we consider multiple network data where the literature is actually rather limited. So here I'm listing, really this list is not exhaustive, but I'm listing a few relevant contributions that deal with multiple observations of nature. So let me just mention a few of them specifically. The first one by Durant Foggstein. By Durant, Foggstein, and Danson, as far as I know, is the first Bayesian parametric model for populations of networks. Another relevant contribution is the one by Simon Lunagome and his co-authors 2021, who defined a very general class of unimodal distributions for networks. And this will be the starting point of our construction. Let me also mention the paper by Jang and Kotor that I will consider for some comparison. Some comparison later on. This paper, I would say, in the same spirit as what we are going to do, consider a mixture model for multiple network data. Okay, so far I've been talking about network and I will keep on doing that. Sometimes I will also talk about graphs, but what I mean by this, with a slight abuse of notation, is simple undirected label graphs. I'm going to denote with I'm going to denote with V, the set of nodes, and with E, the set of edges. Now, capital N is the cardinality of the set V, that is the number of nodes. And we can represent a network by means of an adjacency matrix where we have an entry equal to one in position ij in case we have an edge between the nodes i and j, and zero, vice versa. Now, I'm going to call telegraphic g. calligraphic G with V, not very good, well, the space of all networks sharing the same set of nodes denoted by V. This space is clearly a finite space, but its cardinality grows very quickly with the number of nodes capital N. The starting point of our analysis will be the observation of a sample here denoted by G. Here denoted by G from one to n, that is a sample of networks sharing the same set of vertices. Okay, as I said, the starting point of our construction is something that we borrow from the paper of Simone and co-authors, 2021. And as I would say, the most tractable example of distribution for networks, part of the general class of distribution that they defined. Of the general class of distribution that they define is what they call centered Erdogani distribution. This is a distribution that builds upon the well-known Erdog generating process for networks and by introducing the notion of a representative graph that we denote by G with M. Now, the distribution, therefore, has two parameters, G with M, that is this representative graph, that is also, that turns out to be also the mode of the distribution, and a scale parameter. Distribution and a scale parameter, this alpha, ranging in the interval 0, 1/2. So, how is the distribution defined? Well, let's give a look to a specific pair of nodes, node I and node J. We want to essentially express the probability that a random graph with served distribution has an edge connecting the node i and j. To do that, we look at the representative graph GM and we check whether there is an edge connecting the node. check whether there is an edge connecting I and J in the representative draft. And then with probability alpha, we change that. So if there is a, we change the presence of an edge into the absence of an edge, or similarly the absence of an edge into the presence of an edge with the probability alpha. So the idea is very simple in the end. We need to toss several coins, several independent coins, all with the probability of success equal to alpha, in order to obtain a realization from this distribution. From this distribution. Now, if we want to think of the distribution globally, so not looking only at a pair of nodes, we can write the distribution in this very simple form that not surprisingly is reminiscent of the probability mass function of a binomial random variable. And it's expressed in terms of the Hamming distance between two graphs. In this case, it's the random graph G and the mode G M, where the Hamming distance between two graphs is nothing but the number of entries. But the number of entries of, let's say, the upper triangular part of the adjacency matrices that disagree. So, the use of this distance is actually very convenient, first of all, because it allows us to write this distribution without the need of computing complicated normalizing constants. And second, because we translate the problem of measuring the similarity between two networks into a problem of counting. And therefore, we're going to be able to use standard combinatorial arguments to. Combinatorial arguments to compute the quantities of interest. Okay, let's visualize this. So, here I would like to show you some examples of networks generated from a cell distribution. So, we are considering a cell distribution defined on a set of six nodes here labeled one to six. And in this case, I'm considering this mode, that is a simple network connecting only the nodes one, two, and three, and a small scale of variation, or at least scale variation equal to 0.05. Variation equal to 0.05. So, here is an example of network on the right generated from this distribution. We can see that, first of all, we can recognize somehow the same pattern that we had on the mode, a similar pattern, but there are clearly some differences. In this case, two edges were added, the one connecting the nodes one and four, and the one connecting the nodes five and six. Another example is this one, where again we can see there are some similarities, but we deleted one edge, the one connecting nodes one and three, and so on and so forth. And so on and so forth. Now, let's consider another cell distribution where we change the mode. This time is this network in blue. Well, not surprising if things work quite similarly. Now, let's move on. Let's consider a mixture of two cell distributions with equal weights. And in this case, I'm considering a mixture where the two components are characterized by these two modes, the one in red and the one in blue. If we sample from this mixture, sometimes we will be sampling from the red. Sometimes we will be sampling from the red component, other times from the blue component. So, the idea is very simple, but it seems like a very convenient idea if you want to address the possible presence of heterogeneity in the class, in the distribution in the population of networks. Okay, so this is the basic idea, the basis of our proposal. We just have to formalize it. So, what we're going to do is to consider a SER kernel function, and we're going to define a location scale. To define a location scale Diraclet process mixture of set-carnel functions, so this is a distribution. What we get is a random probability mass function defined on the space of networks with fixed set of nodes V. Now, in order to define the Dirichlet process mixture, we also need to specify the base measure of the Dirichlet process. And we do that by defining this truncated beta cell distribution. beta cell distribution on the parametric space that in this case is the space of networks Cartesian product the interval going from zero one half and we do that in this way a rather standard way I would say we start by modeling alpha with the truncated beta distribution on the interval zero one half and then conditionally on alpha we model the mode parameter G with M as a new set distribution centered at some G0. G0 therefore is a network defined on the user space. Defined on the usual space, always on the same space. And it can be interpreted as, I would say, the mode of the distribution around which our Dirichlet process is centered. And clearly, as for the scale of variation, we use the value with respect to which we are conditioning for L. Now, the same model can be written in a rather standard way in a hierarchical form, where I have introduced the notation theta with L to denote the set of The set of parameters specific to the statistical unit labeled with the index L. So here is a summary of the model and here is a summary also of the hyperparameters that are involved. A and B appear in the truncated beta component of the base measure of the Dirichlet process. C is the concentration parameter of the Dirichlet process. And G notice, let's say, the center of the Dirichlet process. Center of the Diraclet process that I have discussed, for which we adopted an empirical-based approach essentially by choosing a network that displays an edge connecting the nodes I and J. If this edge appears in at least 50% of the networks in the data set we are going to consider. Okay, in this slide I'm summarizing some model properties that we were able to study. That we were able to study. And they're actually quite appealing from, again, a modeling point of view. The first one is that the mixture model that we have defined enjoys the Kulberg-Leib Leibler property. That is, whatever is the element P star that we choose in the space of probability distributions defined on the space of networks G with V, then however small is Small is the value of epsilon, the prior distribution induced by the location scale DP mixture assigns positive probability to a coolback-Libert neighborhood of that distribution P star. So this is essentially telling us that with this model, we have full support in the sense that we are able to approximate as well as we want any probability distribution. probability distribution, we might be any probability distribution in this space with a set of distributions that our prior, to which our prior assigned positive probability. Now, proving this result is essentially made possible and not too difficult thanks to two properties of the model. The first one is that it is a location scale mixture model, so it is actually Mixture model, so it is actually pretty flexible. And the other thing that simplifies the, I would say, the whole theory is that in any case, although large, the space we are considering is a finite space. That is the space of networks with this specified set of nodes, V. A direct consequence of this result is that our model is strongly consistent. Now, let me also say that here I'm presenting Now, let me also say that here I'm presenting these results for the specification of the model that I described before. For example, we are not really using the specific form of the base measure, p node. The only thing we are using is that p node has full support on the parametric space. So, the same result holds even for other specification of p node. Okay, so as I said, what we are using is a rather well-known instrument, the Diricle Process Mixture Model. Process mixture model, for which there are really many possible algorithms available in the literature. And what we did was to adapt to this setting, to our setting, the marginal approach originally introduced by Escobar and West for the case of univariate Gaussian capitals. So the good thing is that, as I said, by using the Hamming distance, we end up dealing with With quantities that can be dealt with by means of standard combinatorial arguments. So, as a result, we are able to obtain all the full conditional distribution in closed form. And this makes clearly posterior sampling, let's say, efficient as much as it can be efficient for large-dimensional data such as these network data. Now, let me just give you an example. So, let's give a look to the update of the individual parameters tax. Of the individual parameters theta l from that full conditional distribution. This can be done by sampling from the generalized Polyeron scheme that essentially is telling us that theta L can either take values coinciding with other values of the same set of parameters already visited, let's say, by the other statistical units, or can take a new value. So let's give a closer look to this polyeral scheme. This polyeron scheme. So let's focus, for example, on the weights here highlighted in red. Now, the probability, I mean, these weights are known in closed forms, in closed form, and up to a proportionality constant are presented here. And the probability of sampling a new value is actually expressed in terms of ratios of incomplete beta functions, where the parameters of these incomplete beta functions critically depend on this quantity d. This quantity D that is nothing but the Hamming distance between the corresponding observation G with L and G naught, let's say the center of our prior Dirichlet process model. Or again, let's focus on the distribution from which we have to generate a new set of values for the parameter. Values for the parameters p ta l, p nu. Now, well, not surprisingly, we can write it by means of the chain rule in this way. And what we have is that the distribution for the scale of variation component alpha is a mixture of truncated beta distributions. And the conditional distribution of the mode parameter given alpha can be interpreted as a generalization of the cell distribution. In which terms is a generalization? Well, let's give a look to the Well, let's give a look to the nodes i and j, and let's consider the probability that a random graph with this distribution p niru displays an edge connecting the nodes i and j. So here is the expression we obtain, and we can see that it depends on two networks, G naught and G L. So the center of the model and the observation. And in particular, if both networks have an edge connecting I and J, then the probability that we will have an edge connecting I and J in the graph that we generate. In the graph that we generate from PNU, takes values larger than one half, here displayed with the yellow curve. On the contrary, if none of the two has an edge connecting INJ, then the probability of having one in the network that we are generating is smaller than one half. And if only one of the two values displays an edge connecting i and j, well, the probability is exactly equal to one half. The value of these probabilities also depends, not surprisingly, on the value. Not surprisingly, on the value taken by the other parameter, the scalar variation, alpha. Okay, so let me move to some illustration. First, let me tell you about the results of some simulation study that we ran. Here we are generating data essentially from a finite version of our model. So, this is a four-component mixture of SHER kernels. We are considering a rather small number of nodes and equal. Number of nodes, n equal to 20, and we are considering different values for the sample size, small n ranging from 4 to 200. And we decide to center the four components of the mixture at some networks with some very specific structure, some very distinctive structure. And this is done in the spirit of a similar simulation study considered in the paper by Daniele and Cotter. So specifically, we consider four. Typically, we consider four distinct structures for these four components: scale-free, small-world, stochastic block model, and Erdogan structures. What we're going to use for our analysis is the model that I've introduced. And when it comes to the problem of clustering networks, we resort to this variation of information criterion as implemented in this SALSO package. Okay, so I'm going to show you the results of our analysis where I'm comparing. The results of our analysis, where I'm comparing the performance of our model with the performance of the models of Danielle and Corters and Jangen Coaters. I'm going to stick to these three colors for the three models. And in these preliminary plots, posterior predictive check plots, we can see that in general, all the three models we consider do not highlight any problem of fitting to the data. Let's start by looking at the accuracy of the posterior estimate for the distribution. Here I'm displaying the Kurbat-Liber divergence between the estimated distribution and the true distribution. And I'm showing this for different values of the sample size. And okay, it is comforting to see that our model actually performs better than the other. And also that at least Better than the other, and also that at least for our model and the one in blue, things improve when the sample size becomes large. Okay, so let me say this is not surprising because, as a matter of fact, we are simulating from our model, the data, but I think it's nonetheless interesting because our model is by definition structure-free. So, I think it is interesting to see how it performs when actually components, sorry, sorry, when actually, yeah, the components of the mixture from which we generate the data instead. Range data instead displays a very clear structure. Okay, when it comes to the accuracy of the posterior, sorry, of the estimated partition, we consider these three measures of accuracy, the entropy, the purity, and the rand index. The smaller the entropy, the better. The larger the purity and the rand index, the better. And columns here refer to different simulation scenarios where what you can read low, medium, high, and so on, refer. Medium, high, and so on refers to the value taken by the scale of variation parameters alpha of the data generating process. So it is expected that if we have a small scale of variation when we generate the data, then identifying the correct partition is simple. Things become more difficult when instead we have more variability in the data generating process. So let's give a look, for example, through what happens on the last column where we have a mixed scenario with some components with small scale variation and other components with Variation and other components with larger scale variation, where we can see that always based on 100 data set, our method, the one in yellow, seems to perform according to the, I mean, based on these three measures better than the two alternatives that we have considered. Okay, another thing, I started by showing a picture of the type of structure that we imposed when we defined the data generating process. Define the data generating process for the four components. And here is kind of interesting to see what are the estimated modes once we condition on the estimated partition. Now, beside the details on how we estimate the modes, but let me just tell you that with only 40 observations, we are actually able to recover quite well the networks that display a clear, a rather clear structure similar to the one. Rather clear structure similar to the one, similar to the true ones characterizing the four components. Okay, second illustration. Here we analyzed human brain network data. We have a set of 266 brain scans. And these data have been prepared by using an ATLAS where the brain is divided into 48. Distinct regions. And the interesting thing, at least for our purpose of this data set, is that there exists some sort of natural partition. So clearly we are not dealing anymore with the simulated data. So we don't know what is the true partition. If it actually makes any sense to talk about the true partition, we should try to estimate. But it is interesting to compare the partition that we are going to estimate with our method with the one implied by the press. With the one implied by the presence of 30 subjects in the study. So, for each subject, we have actually several scans, up to a maximum of 10 brain scans per subject. So, I think here there are a few relevant questions. Is our model able to recover the partition into these 30 distinct clusters, which again is not a true partition, but is an interesting partition to refer to? Are clusters interpretable in terms of network features? Or again, maybe from a more Or again, maybe from a more interesting from a neuroscientific point of view. The scans that are not clustered with the other scans of the same individual, maybe pointing out some anomaly. Okay, also in this case, I'm comparing the results that we obtained with our model with the same two alternative options that we considered. We start from these posterior predictive checks, and in this case, we can see that the situation seems a little bit more challenging. Seems a little bit more challenging, although, at least for our model and the model of Daniel and co-authors, there do not seem to be major issues with model fitting. Here are the results of our analysis. Now, clearly, comparing different methods is always something that we should take with some care because results might depend on the specification of some parameters or setting for the other method. For the other methods. But in any case, let me say that based on this analysis, it seems that our model does better, at least in terms of comparing the estimated partition with the partition induced by the presence of 30 subjects in the study. Let me comment a little bit further on the result that we obtain with our model. We can see that we obtain an estimated number of clusters equal to 50. So k hat exceeds. So, K-hat exceeds actually the number of subjects in the study. But on the other hand, we can see that the estimated partition actually show a high degree of similarity with the natural partition. Indeed, only two clusters, only two estimated clusters contain networks from different individuals. And for four subjects, for only four subjects, I would say scans corresponding to the same subjects are actually distributed across multiple clusters. So, our results are reasonable. Cluster. So, our results are reasonably consistent with the natural partition of the data. We also wanted to give a look to the topological properties of the clusters we obtain. Now, remember, we have 266 observations, and only for the purpose of illustration, I focused on the three largest identified clusters. Clusters are colored, I mean, are determined based on the color, while letters refer to subjects. And in this case, we can see that these three clusters exactly coincide with three. Coincide with three with the scans of three individuals labeled A, B, and C. And it's also pretty clear that, at least by looking at this scatterblot that consider two summary measures that are known to be of neuroscientific significance, that is the clustering coefficient and the average path length. Well, by considering this catapult, we can see that the cluster actually do appear reasonably well separated. Okay, so in this Okay, so in this last part of my presentation, I would like just to say something about the dimensionality of the problem that so far I haven't really considered that much. Now, what we have so far, we have a model that seems to give promising results as it is flexible. And the fact that we can get post-formful conditional distribution actually is very convenient because it makes posterior computation. Because it makes posterior computation, I would say, reasonably efficient. Not surprisingly, though, the method suffers with large, or even, at least for our implementation, even with moderately large values of capital N, the number of nodes. So of course, one can try to improve the coding and implementation, but at some point, eventually you will have to deal with this problem. Now, I think that the Maybe the main message of our work should be that this model could be considered a good option to model multiple networks, and maybe could be the starting point also to include or to combine strategies to obtain dimensionality reduction. We are used to, I don't know, for example, to consider factor models within a normal Gaussian kernel. A normal Gaussian kernel. We could do something similar here by using a SER kernel or a mixture of SER kernels and maybe reduce the dimensionality of the parameters by means of some ideas from factor modeling. Now let's consider a problem where I'm going to illustrate a possible strategy to directly exploit the models that I've introduced to situation where the number of nodes is a bit larger. So let's consider the problem of clustering brain network data with the Problem of clustering brain network data with the final granularity. We're going to consider n equal to 200. In the previous analysis, n was equal to 48. The heuristic solution we're going to propose is something that we call consensus subgraph clustering. And the idea is, let's say, very intuitive in the spirit of, I would say, divide and conquer approaches. What we are going to do is to divide. To divide our set of nodes into smaller subsets of nodes. So, clearly, these divide and conquer approaches typically try to reduce the dimensionality by working on the sample size. Here, we try to apply the same type of idea on the number of nodes, capital N. So, the data set that we are going to analyze is actually the exact same data set that we have seen before. Simply, the scans are, let's say, The scans are, let's say, encoded into networks by using a different ATLA that leads, as I said, to networks with 200 nodes. The strategy we propose is the following. So we choose a smaller number of nodes that we call n sub, and possibly based on spatial information, as we did, we divide the set of n nodes into n. Into M sub mutually exclusive sets of nodes with cardinality equal to capital N sub. So here, for simplicity, I'm assuming that N can be divided by N sub, but if it is not, things do not change much. What we do is that we obtain M sub-independent data sets, each composed by N subgraphs with Subgraphs with n sub nodes. So, what doesn't change is the sample size. Simply, we are now dealing with many data sets consisting of n smaller graph. So, what we propose to do is to analyze each data set with independent directly process mixture of CERT kernels. By doing this, we obtain several samples from the posteriors of this model, and we pull the samples together into what we consider an approximate posterior sample. Consider an approximate posterior sample. So, if the goal is doing clustering, what we obtain in the end is a pool sample of partition sampled from different posterior distributions. So, we ignore essentially the fact that they come from different posterior distributions because each one refers to a specific subgraph. And we use the same approach that we used before when we didn't implement this idea. So, we use the variation of information. This idea. So we use the variation of information criterion to identify an optimum partition based on this approximate posterior sum. Now, as I said, nodes can be divided, for example, by resorting to some additional covariate information we might have on the nodes. This is the case for the illustration we are considering, where we know the location of the brain regions. So, what we did in this case was to use these. Use this strategy of balance clustering implemented in the anti-class R package to obtain subgraphs that are somehow close to each other in terms of location. So in this example, we can see a top-down projection of the atlas of these 200 nodes in the case where we consider n sub equal to 40. N sub equal to 4. Okay, how to choose N sub? So clearly, this is a critical question, and we need to keep in mind two quite intuitive aspects. First of all, the larger N sub is, intuitively, the better will be the approximation of the posterior. Of course, the smaller N sub is, the quicker will be the implementation of the method. So here we consider different values for this number of nodes. Number of nodes for the subgraphs ranging from 5 to 50. And I'm displaying the value of these summaries: entropy, purity, and random index, always comparing the estimated partition with what we call the natural partition, the one implied by the presence of 30 sub. So I think that the message that the plots in the first row pass is actually quite interesting. If this n sub is very small, pi. If this n sub is very small, five, the method clearly struggles. But as soon as it is large enough, and in this case large enough seems already 10, the method is actually able to estimate a very reasonable partition of the data. Now, since the goal is to improve, to reduce the computational time, it might be interesting to give a look to the same quantities, really for the entropy, for interpretability. Really, for the entropy, for interpretability purposes, we consider one minus the entropy divided by the computational time taken by implementing this idea. And based on these graphs, it seems reasonable to consider n sub equal to 10. Now, this is clearly, there is not any theory supporting these choices, but it seems pretty sensible. And that's what we did. We consider n sub equal to 10, we analyze the data. to 10, we analyze the data. And what we obtain in this case was, I would say, interestingly, a smaller number of estimated clusters. And in this case, it actually coincides with the true number of clusters, although the partition that we estimate is not coinciding with the natural partition. What matters, though, is that the results that we obtain are actually pretty similar to those that we obtain in the case when we started. In the case when we started from data with 48 nodes. Okay, well, let me skip this and let me just conclude with some comments. Now, this idea, consensus subgraph clustering, clearly it is quite brutal because we are ignoring some dependencies across subgraphs. But I mean, while clearly we are going to ignore some information that potentially could. To ignore some information that potentially could be useful for the purpose of clustering the networks, it must be said that this idea of ignoring dependencies is common to other dimensionality reduction techniques. We could think, for example, to variational methods in a different sense, but also in that case, you ignore some dependencies across dimensions. Another issue of this approach is that we need to choose and sub. Now, in this specific case, we at least had this. We at least had this natural partition we could compare our results with, but with other data sets, we might not have that. So we need to find another strategy to deal with this problem of choosing NSAP. On the other hand, I must say that based on this, the few analysis that we run on different data sets, it seems that you never really need to have a large value for NSAP. So sometimes the optimal value was 10, other times 15, and it never really went much farther away from these two. Away from these two values. Moreover, are there better ways to partition this set of nodes? For sure, there are. And I'm thinking, for example, of, I mean, this is clearly relating to the problem of sampling a graph. So I'm thinking, for example, of the presentation of the other Bernardo yesterday. But there is a good thing that I would like to mention is that we run the same type of analysis based on Of analysis based on a random partition of the nodes, so without exploiting the special information available, and the results that we obtained were not much worse than the ones that we obtained by using that piece of information. So, here I'm formalizing this by saying that there is a reasonable, at least from an empirical perspective, robustness to the way nodes are partitioned. So, some advantages. It is a very simple procedure. The subgraph data set. The subgraph data sets can be analyzed in parallel, and importantly, the overall results seem promising. Okay, so one last slide to maybe wrap up a bit what we discussed. So the reclaim process mixture of center Delta Sharnikernel seems like a flexible tool to model multiple network data. And I think it has the potential to be used as the starting point also to maybe define models that might extend this idea. Models that might extend this idea in other directions. For example, by incorporating some dimensionality reduction strategy, or maybe in order to define models for partially exchangeable data. Extensions to other non-parametric mixing measures is actually reasonably straightforward. I'm talking about the model, the properties, as well as the computational strategies we propose. Essentially, as long as the predicted distribution for the predicted distribution for the mixing measure that you choose is available, then the same strategy can actually be implemented. And finally, although I think this is interesting, I think, but maybe not that easy to achieve, it could be interesting to use the same modeling strategy by actually changing the distance that we consider in order to measure the similarity between networks. Measure the similarity between networks. And the Hamming distance is known to essentially focus on local differences and local similarity between networks, while other distances, such as the diffusion distance studied, for example, in the paper by Simone, Unagomez and Kotos 2021, is maybe more useful in assessing the global similarities between networks. And with this, I conclude my presentation. And with this, I conclude my presentation. Thank you. Thank you very much, Bernardo, for that. Very nice talk. Are there any questions? I was wondering about the data set, the 48 region, and then you scale it, right? So, was there any No, well, these type of data sets are available in many different granularities because I think they well that's something that is already made available in some neuroscientific repositories, so it's not something that we did, but I think that they start from the same collection of. They start from the same collection of brain scans, and then they use different brain athletes to translate the scans into networks. And you can choose how fine you want your network representation to be. So we started from the 48 one simply because let's say it was a size we were quite comfortable in dealing with. And then we realized that working with finite granularities could be an issue. And I think the And I think for this illustration, we like to stick to the exact same data set rather than changing it on our application because we might somehow exploit what we learned already in analyzing the data with 48 nodes when we actually assess the performance of our model in analyzing the data with 200 nodes. No, I would say not really. No, I would say not really at least. No, we only use it to compare the results of our analysis. I probably missed it at the why you have alpha and zero and alpha. Yes. And no, actually, the reason is different. So we need to go back to the cell distribution. That was that was here. So clearly, this is well defined for any alpha in 0, 1. But if you restrict alpha to this state, you end up with the situation where gm is actually the mode of the distribution. Otherwise, if alpha was larger than one half, you would have a bimodal distribution that is not actually, well, could be still sent to the GM, but. Well, it could be still centered at GM, but GM wouldn't be any longer the mode of the distribution. Kind of a random question. I noticed in your simulation example, you had like the simulation through. I think one was centered at the you said centered at the CR, centered at the stochastic block model, etc. Yeah, which assume centered at realizations and such fun. But that kind of But that kind of suggests the obvious idea that you could even have like cluster-specific different models. There's no reason to have like a mixture of CERs. You could have a mixture of interesting models. I mean, maybe it's excessive at overcase, but maybe it would be even better in Paris and right? Well, I think it would be more difficult to define and implement, no? So, you're talking about the data generating process or the model you fit to the data? Okay. It's a little bit of cheating, but you happen to use the narrative model that happens to match the data generated process better. Where I'm coming from is in clinical trial design and estimating a dose response curve, there's like popular methods that Obvious methods that mix over different alternative models and they are more robust because they can react to a display. Yeah, well, that's probably an interesting direction to investigate. I would say though that by the way the model we propose is defined, really the structure of the mode doesn't really matter. So it could take any shape. But I mean, since you already have a mixed kind of limit with it. Mixed kind of driven way to the mixture, like a finite parameter that indexes the different sides, right? I have one question. So, my question is: you're choosing DP mixers, right? And I think that the algorithm you're using and everything would be basically the same if you instead choose a bit manager. Yes. And I don't know if you have I don't know if you have noticed how sensitive is the number of clusters because Pitman Gerd tends to propose more clusters, and so maybe this will tell you how informative I are actually in this mode. Yes, so in general, I agree with you. The Pit Manure process could be a good alternative, and in some sense, more robust, for example, to the To how you specify your prior for the hyperparameters compared to the Directly process. I think that, I mean, that should be something that we could do, but we actually like to focus more, I would say, on the parametric part of the non-parametric model here. So we stick to the directed process because, I mean, it seems to be working reasonably well. But yeah, so we comment and say that extensions are actually reasonable. Expectations are actually reasonably easy to implement. You don't get by the parameters. I guess that parameters are probably it's not you don't get uh you might not get interesting structures within like I guess subnet so sorry what happens so as you assume that if you close so it's kind of more like a population of networks if you want like one you are mixture you can actually close the nodes so that you get like these sort of uh you know you create these like stochastic models that you notice but Well, I'm not sure I see exactly the point of your comment. So we're not working on clustering the nodes. So I would say that also in terms of sparsity, but it really depends on where you center your centered Erdogani distribution. If you centered around a very sparse network, frankly, the scale of variation is small. Of the reaction is small, you will generate a very sparse network. Otherwise, if you centered around the dense network, you're likely to generate very dense networks. So, yeah, so I don't know if I have much I could probably if I could generate like uh So, what I was kind of saying was more like, I mean, it's like another direction that we're going to consider. Like, you can use this type of, I guess, approach to try to, like, use the errors of any construction, which is like very, like it's conceptually, it's very, very nice. But if you kind of clustered like the notes or the echoes or whatever, you could be able to obtain like this sort of I guess nicer like. I think it goes a little bit in the direction of combining these ideas and using this model as, let's say, a container, as a bin to include other dimensionality reduction strategies, such as the one of first clustering the nodes into. First, clustering the nodes into a subset of nodes and working with the stochastic block model structure, essentially. No, not much. I think we have to move on. But if there are more questions, I think we can thank you, Bernardo.