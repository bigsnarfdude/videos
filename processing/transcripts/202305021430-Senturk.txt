And for not being able to be there in person. But today I'm going to talk about new modeling approaches for eye tracking data. And it's going to be a little bit of a different talk in the sense that I won't get into the mathematical details of the developments. You know, to stimulate more discussions in the workshop, I'm going to actually relate or try to relate the modeling. To relay the modeling ideas that we came up with or new ways of looking at the eye tracking data through some case studies because there isn't too much statistical work on modeling eye tracking data in the literature. So I'm going to begin with some motivation and explanations for eye tracking experiments. And I'm going to, as I was saying earlier, introduce the modeling approaches we came up with using two case studies on. On autism spectrum disorder. So, eye tracking experiments collect data through presentation of repeated presentation of stimuli referred to as trials, similar to Marina's talk when she was talking about the EEG having different trials. Here, the stimuli could be static or continuous images, and what's recorded is the continuous trajectory of a subject's gaze. Trajectory of a subject's gaze on a two-dimensional screen. In connection to the AST autism spectrum disorder, eye tracking is used to evaluate the extent to which visual attention is affected in this disorder. A little bit about the autism spectrum disorder, ASD. It's a neurodevelopmental disorder characterized by greater difficulties with social communication, more restricted and More restricted and repetitive pattern of behaviors, atypical response to sensory information, and it's highly heterogeneous. There's extensive variation in social, cognitive, regulatory, and attentional phenotypes in this disorder. And eye tracking is actually a very powerful, safe, and cost-effective platform for gaining extensive insights into attentional processes. It's actually one of the leading biomarkers for autism. Leading biomarkers for autism spectrum disorder. I will talk a little bit about the case studies, but the two case studies in today's talk are coming from this multi-site large NIH consortium called the Autism Biomarkers Consortium for Clinical Trials that I'm part of. And in the consortium, the biomarkers considered for ASD are eye tracking and EEG. And in many respects, eye tracking has a leg up EEG actually as a biomarker. Leg up EEG, actually, as a biomarker for ASD. But anyway, the goal of the consortium is to use these biomarkers for in treatment development for the ASD. And the consortium collected data on school-age children, 6 to 11 years old, with ASD and their typically developing peers at baseline, six weeks, 24 weeks, and four years follow-up after. Follow-up after baseline as well. And at each visit, participants complete a full battery of eye tracking and EEG measures. And in fact, collecting all these measures on the subject takes two days at each visit. So the first eye tracking experiment I'm going to talk about is called visual exploration task. And the participant is presented with an array of With an array of five images. One of them is a social image. It's a face with a direct gaze, and the eyes are blacked out here for privacy, but in fact, the participant can see this face with the direct gaze on, you know, without the blacking. And there's an outline of a face filled with a pattern. There's a bird, mobile phone, and a motor vehicle. And in here, the goal is to reveal ASD. Is to reveal AST-specific signatures of attention and visual array processing. The second eye tracking task I will talk about is called the activity monitoring task. Here it's a little bit more lifelike scenes shown to the participants. Eight of the stimuli are static images and eight of them are dynamic videos. But in fact, the common theme is that. The common theme is that in all of them, there are two actresses playing with children's toys. So it's kind of like a screen given on the left. And what we get out of, or the way they split the screen, the two-dimensional screen, is what's given on the right. So what's given in the red will be the faces. Green is the activity regions. So these are regions of interest basically. And everywhere else in blue is the background. In blue is the background, and there could even be a region of interest. You could consider it as a region of interest as non-valid. So, the non-valid would be if the participant is not staring at the two-dimensional screen. So, here we could consider four different regions of interest. And here, the goal is to figure out AST-specific signatures of social attention. So, a little bit about the traditional analysis and the need. Analysis and the need for developing new outcomes and new analysis procedures for eye-tracking data. Even though the continuous path of gaze is recorded during each trial, common analysis collapse data into simple summaries. So a very popular summary is percent of time looking into different regions of interest. Like in the slide, in the last slide, those four regions of interest: background, face, activity are. Face activity are non-valid. What we would do is, in a given trial, you could look at the percent of time the participant is looking at those regions, or it could be latency to looking at stimuli. For example, in the first array, what's the latency of the participant looking at the face with the direct gaze or the social image, basically? Or one could analyze the number of stimuli viewed, number of fixations, or fixation length. These summaries collapse information. These summaries collapse information across trials, but also in trial time. And typically, they analyze, for example, percent looking time from each ROI separately. So the data is very rich, and there is a lot of room to grow. Like there is a lot of information to be retained in terms of different from these traditional summaries. So I'm going to start with the first. So I'm gonna start with the first example and tell you what we did in modeling this data, visual exploration. There are five images that the participant is looking at. So before I go to our analysis, I just want to give you a little bit of the background on results from traditional analysis. Here, the most interesting outcome or the most important outcome for the collaborators were latency to the social object, so the face with the direct gaze. And if we look at this latency from each trial, or if we average it across trials, like we could do the analysis by a simple ANOVA or some linear mix effects model, but the result is the same that they were able to establish significantly longer latency in the AST group of looking at the social image. And there was also a shorter latency at ladder visits, implying that there's some practice effect. Implying that there's some practice effect, signs of practice effect, too, here. So, what we're going to do is we're going to try to retain some information in trial time. And in both case studies, we're going to propose a novel functional outcome, retaining different information about different aspects of the eye tracking experiment and analyzing. And analyze the functional outcome through some sort of functional principal components analysis, look at the mean and variation trends. So, here we first identify the target behavior. And the behavior, this is a behavior we're interested in in the eye tracking experiment. Here, we were particularly interested in when the array comes on the screen, is the initial gaze of the subject at the social object, so the face with the direct gaze. So, this indicator function in trial time. Indicator function in trial time is indicating that the social object was viewed first. So, for example, on this left plot, the initial gauge is at the social object and it's lasting about 400 milliseconds. Here, the initial gauge is not on the social object, so the indicator is zero. So, the way we formed our functional outcome is where we average these indicator functions over trials. And we're calling this out. And we're calling this outcome viewing profiles. And we're going to interpret it as the proportion of time this target behavior is observed across different trials. So in other words, it's the consistency of this target behavior across trials for that particular subject. And because we're averaging the indicator function across trials, the viewing profile will be between zero and one. And with this image, I want to And with this image, I want to give you the flowchart for traditional analysis on the left and the new approach on the right. So in the traditional analysis, presentation of each of these arrays gives rise to, for example, a single latency number to the latency to the social image. And we could be analyzing this trial-specific latencies or averaging it at the end to analyze the latency and comparison between groups, the AST. And comparison between groups, the AST, and typical developing. But in the current approach, what we're doing is that each presentation of the array is giving rise to an indicator function, and we're averaging these indicator functions in trial time to obtain our viewing profile, which is retaining information in trial time, which is lost on the left. So I'm going to jump directly to application. Jump directly to the application of this outcome in the visual exploration task and the analysis of it using functional principal components analysis. So as a summary, the overall mean of these viewing profiles across subjects showed that the maximum consistency of this target behavior happens around 500 milliseconds after start of the trial. So the subject is looking at the face. So, the subject is looking at the face with a direct gaze. And this is happening at 42% of the trials. So, indeed, this is a common behavior across study subjects and trials. And if we look at group-specific means, this is what's given on the right. TD group given in orange and AST group in blue. The consistency of looking at the social image first. Is actually 50% of the trials in the TD subjects given in orange, but it's only at 35% in the AST subjects. So we're actually gaining two information here. One information is that TD and AST subjects, in fact, are looking at the social object at similar trial time. It's 500 milliseconds. But the reason why we got longer latency for A. Longer latency for ASD in the traditional analysis is because they are not consistently doing the behavior across as many trials as the TD does. So TD is looking at the image 50% of the trials, ASD only 35%. So that's why they have a longer latency, but their trial time behavior is the same. So just briefly, you know, functional principal components can also give us directions of variation. Can also give us directions of variation. Here, the leading direction of variation was differences across subjects in consistency of this target behavior, followed by variation in duration of this initial gaze. But I think generally the way we interpret the result that we got actually connects to Marino's talk too, and it also highlights Highlights this body of evidence that's building up in the autism research that instead of a general failure to respond to certain tasks, there's greater intra-subject trial-to-trial variability across behavioral fMRI and EEG response for the AST participants. So, we're adding to this evidence base with another platform, which is the Another platform, which is the eye tracking experiment, but basically, we're showing that consistency is lower for the AST subject in showing this target behavior. So I'm going to move on to the second eye checking experiment and our modeling approach, describing our modeling approach there. So we have the activity monitoring here. Just to remind you briefly, there are eight images and eight videos, both containing two actresses playing with children's toys. Playing with children's toys. Images are 10 seconds, videos last 20 seconds. And the image is dissected into different regions of interest. Red here is the faces, green is the activity, blue is the background, and there's going to be non-valid as well that is considered. So the first thing we notice is that in order to retain information across trial types, Retain information across trial type and also jointly model looking percent from multiple ROI, we wanted to arrive at a new trial index, right? And this trial index given at the bottom figure is constructed by ordering the trial, different trial types. Remember, eight static and eight dynamic videos. And eight dynamic videos by the gross level of interest in the activity region of green is the activity, time spent in looking at the activity. And we're ordering these trials in the increasing order of looking at activity. So here the green is increasing, background, as the time on the activity is increasing, looking at the background is decreasing. And this is the new index that This is the new index that we will analyze the data in. So we're going to treat this as a functional outcome. This will be our functional index. We're going to have multivariate functional outcome because we're going to analyze percent looking time in all these four regions, including non-valid, which is given in purple. And there's going to be, similar to the first project, some constraint in this multivariate functional outcome. But the first thing we But the first thing we noticed before I go to the analysis, when we ordered the type of trials with increasing looking time at the activity, this ordered the different types of trials into the first eight being static images and the last eight being dynamic videos. In other words, our choice of the index determined that there was less looking at activity. Less looking at the activity region in the static images and dynamic videos, people, participants were looking at the activity more. In fact, there's some ranking within the static images and the dynamic videos. So building a tower of blocks gets more time looking at the activity than holding down a box, or crushing a play-doh seems to be more exciting in terms of looking at activity than finding a fish toy from a basket. So this order. From a basket. So, this ordering, in fact, made sense to our collaborators as well. So, when we consider the multivariate functional outcome as looking times at different regions of interest, green is the activity, blue is the background, red is the face, and purple is the invalid. One constraint that we have is at each trial, all these looking percentages in trial time has to add up to one. So, everywhere on this function. So, everywhere on this functional outcome, we have to add up to one as a constraint coming from this eye tracking experiment. So the functional outcome that we came up with to retain the information in this experiment retains the information across trial type. So, it didn't average across trials, but made the trial type into a new index. And we are jointly hoping to model. Hoping to model percept time looking at multiple regions of interest. And on the estimation side, rather than a typical multivariate functional principal components analysis, we're going to apply something called what we call is a constrained multivariate FPCA. I won't go into the details, but I'll tell you a little bit of the idea. But if we look at some results from the data application, we are seeing Data application, we are seeing that ASD and TD kids actually look at the activity, looking time at the activities comparable across different trial types, where AST subjects, as expected, look at faces significantly less than TV across all the trial types, consistently across all the trial types. And less time looking at faces is replaced by more time looking at the background and more non-valid trials. more non-valid trials which is also very uh well known in this field that AST has more non-valid it's not trials uh I want to correct what I said it's not it's just that the AST kids spend more time off screen off the two-dimensional screen basically during trial time so in order to incorporate the constraint in the multi-virt functional data that I was just mentioning what we did is Mentioning what we did is we identified one of these four regions of interest as the reference region. Here, the reference region is identified as the background because there's actually significant interest in percent of non-valid time during the trial as well. So, non-valid is retained. So, background is the reference group, but basically, the time spent in each region of interest is now transformed into a log odds with respect to the reference. Log odds with respect to the reference, which is the background. So that in these mean functions, anything positive is going to imply, for example, this looking at activity after trial seven, where it's positive, is going to imply more looking time at the activity than the background for the ASD. So if we look at the ASD and the TD diagrams, TD and ASD are spending, both groups are spending more time. Both groups are spending more time looking at the activity than the background, starting from around trial seven. But it's similar time looking at the activity. In ASD, they're looking at faces less. The TD is higher here. In fact, more than the background for the dynamic videos. And the non-valid is higher in AST, like we saw in the original data here. TV is lower. So in variation, So in variation, in this constrained multi-veritive PCA, the most of the variation among AST subjects was their time spent on faces. Whereas in the TD group, the most of the variation among subjects is time spent non-valid in the non-valid region. And in the second component, this reverses between the groups. AST subjects, the non-valid picks up the variation in the It picks up the variation in the TD, it's the faces. And the third component in both groups is picked up by variation in looking time at the activity given in green. So just briefly with this last slide, this is showing what we gained by changing the estimation from a multivariate FPCA to a constrained multivariate FPCA, the new estimation approach. What we gained by incorporating the constraint is there are some gains in. Is there are some gains in prediction of this multivariate functional response. In particular, in the non-valid region, the constraint multivariate FPCA given in blue has smaller prediction error. This is probably to be expected because the non-valid is the harder to predict. And by putting in the constraint that all the looking times have to sum up to one, we are gaining and predicting for the non-valid with this approach. But just to wrap up now, Just to wrap up now, we've looked at new approaches to modeling the eye tracking data using two case studies. There were new functional outcomes defined to highlight task-specific trends. Sometimes we wanted to retain information in trial time, sometimes across trial type. And we incorporated in the last example, also modeling of non-valid data. And what was common between the two projects is usually eye-tracking data, there's some sort of a constraint in these experiments when we look at looking time. These experiments, when we look at looking time or different approaches. And in the future, one direction we want to go in is actually modeling transition probabilities from region of interest to another region of interest and how that could change across trial type and how that compares across AST and TD. Just to acknowledge our funding source, we have this NIMH grant and then the U19 is for the Autism Biomarkers Consortium. Biomarkers consortium. This is joint work with my PhD student, Minfei Dong, my postdoc at UCLA, Brian Donatello, and Catherine. And the consortium has collaborators from multiple sites at UCLA, Harvard Medical School, Yale, Washington, and Duke Department of Psychiatry. Thank you. Thank you very much, Danla. So do we have any questions? So, do we have any questions? Yeah, okay. Hello? Hi, Damla. This is Armin Schwartzmann. Oh, hi, Armin. Too bad you didn't come. Oh, yes, I know. I know. Well, very interesting problem you presented in about the first type of trial. Yes. That you try to treat this as functional data, which is. This is functional data, which is great, but I'm hesitant about being able to separate the vertical variability versus the horizontal variability. So, what do I mean by this? So, suppose you have all the kids are either they look or don't look at exactly the same time, and then you can get 30% this way, right? Or it could be that they are all looking, but they're at different times, right? And if you slide the this data signs right or left, you could also get 30 percent. So, this summary they producing, I don't know if you. So, this summary they produce, I don't know if you can separately between those two, the horizontal and vertical variability. It seems like you do, but I'm not sure how. Yeah, yeah. So, Armin, actually, the first thing we established while analyzing the data is actually this is a common behavior. Like, more than like most of the subjects in most of the trials, they are looking at the social subject around 500 milliseconds when, you know, because the Milliseconds, when you know, because the experiment starts with a cross, right, at the beginning. And then by the time they can move their eyes to the social, the face with the direct gaze, it's about 500 milliseconds. But the duration of how long they hold that gaze could be quite different, right? And that could also change like with trial and trial and things like that. So that's where I think the variation across the trials are also. Trials and are also sorry, across the subjects is also coming up, you know what I mean? But that probably did not exactly answer your question. I try to think about it as I talk out loud, you know. Is there any other question? Hi, Carly. I'm not Carly. Don't Mark Diakis. I got a question regarding, I think, what you called, was it non-valid region? Those are non-valid region. So, I'm thinking, like, when I talk to Michele, he doesn't look at my face, he looks at his own feet. So, you know, thinking about like, where are they looking at, right? I mean, they might be looking at other parts of the kid's bodies, their feet, right? But not necessarily the face. So you should have, because it's eye tracking, so you know exactly what looking at. You know, I don't know if there's any way to generalize that the definition or the non-valid a little bit more. The non-valid, a little bit more specific to account for near their face, but not really the face. Mark, actually, the non-valid here refers to the like, so they, you know, they calibrate the eye tracking so that they can detect where your eyes are at at the two-dimensional screen, right? Are you like at the left top of the screen, right top over right bottom? Where is your eyes exactly? So they calibrate the machine, but non-valid refers to their eyes are not on the two-dimensional screen. Are not on the two-dimensional screen. Oh, okay, okay. Got it. Thank you. Yeah. Uh-huh. Say something about Mark now. Yeah. First, I don't look at my feet. Second, so I was trying to connect the two talks, right? The one by Marina and the one that you just said. So very nice. So, very nice talk, both of them. So, I was wondering if there is any feature of a data of a functional shapes that you see that you can actually take to leverage also to this to study more what Armin was talking about, the duration, latency, and so on. And how would you incorporate that in case? Right. And Michela, is your question about maybe, I mean, I think there was a link between Marida and my talk, right? In terms of trial-to-trial variability, is your question more about functional data analysis and trying maybe to, if there is any feature of the data, of a function or data that you see, at some point you have some, but you can try to leverage it to try to learn about the difference. About the different ways in which the subjects look at the pictures. So I think like you can, because remember, we get the path of gaze on the screen, right? So at each experiment, I think you can choose to model different aspects of Model different aspects of that gaze path, right? I mean, one simple way is to look at the percent time spent in each region of interest. Um, but you it's sort of like you model whichever outcome you're interested in in that experiment. And depending on what's your outcome, it's going to have a shape, right? So it's not like EEG, where we have the characteristic components, and you know in that experiment which characteristic components should show up. Which characteristic components should show up? Is it an N1? Is it a P200? Or so in here, you have the two-dimensional path of gaze. And like, what do you want to model? What's important for you? You know what I mean? And what's challenging is stuff showing up on the screen. If it's a dynamic video, for example, at each moment of the video, the screen is changing also, right? So if your path, if your gaze is on the your path if your gaze is on the left uh left upper corner it's that doesn't mean the same thing as the video goes on does that does that make sense any last question either for marina or for dambla no if not let's just thank again all of them thank you very much marina dambla Damn that. Thank you.