question of T cell activation, where we are kind of wondering why we need such a complex machinery, right? So this is a cartoon from Guy Verland and Ed Palmer, where they are really documenting the events associated with T cell engagement and activation. And what's very striking is that you see a lot of phosphorus, a lot of happening. There is a lot of events. Lot of happening. There is a lot of events happening when you get engaged with peptidemichi in terms of phosphoration and recruitment, so on. And we are wondering how we do we build a model of that? Do we need everything? Do we need to care about all the details? Can we remove stuff to really go to the core of it? And then vice versa, why does the biological system need so many chains and so on? And so we're going to tell you a little bit or catch on that. So I'm going to do a little bit quick review of what we know from the background. Quick review of what we know from the biology side about why we need such a complex TCR signalosome to really discriminate peptide. Then, Paul will tell us about what we have done in terms of modeling to really account for the way T cells discriminate ligands. And then we'll show you a new robotic platform and some machine learning that we did to really go at building a model of T cell discrimination. And typically, when you open T cell papers, right, you know, people purify T cells, purify antigen presenting cells. Purify antigen presenting cells, mix them together, and you can build these assets where you're going to see the dose response according to different ligands. So let me set up the laser pointer. And no, you can case in the case of the OT1 system, we have a hierarchy of peptides which are singly immutated. Now, Omer spoke about this different peptide image. In this particular assay, we are looking at phymocyte development in vitro, and they saw that you had extreme sensitivity. And they saw that you had extreme sensitivity simply by being one changing by one mutation, you could really change by a factor of almost 100,000 the potency of this peptide. And this is a world characterized system. But then when you look at functionally, what it means, and that these different continuous ranks of the antigens translate into a very digital decision, where if you have a peptide, which is very important to drive negative selections, and these thymocytes die in the dish, if you have peptide in the right position. If you have peptide with the right potency intermediate, you're going to get positive selections and no negative selections. You can recover a lot of single CD8 OT1 phymocytes. And then if you have a very weak peptide like BSV or E1, you die by neglect because you have absolutely no engagement of the TCR. So in a way, if you look at the field, typically people would think of antigens as existing in three classes: strong antigens, which drive negative selection in the fimus, partial agonies, which Fimus, partial agonists which are going to drive positive selections, non-negative selection, and fimus, and then all the cells which are not peptide for the T cells. So, maybe three classes of antigen. So, then why do you need so many tyrosines? Why do you need such a complex signal machinery if at the end of the day you are doing mostly discriminating between three types of peptides? And indeed, when people have mutated some of these tyrosines in the associated change of the TCR, they Associated change of the TCR, they found that you need all of them. If you start to mock around with the TCR-associated chains, you end up with like a mouse which tends to get automated disorders. And as you titrate the number of phosphorylation on the x-axis here, you see that you really accelerate the time of the onset of the disease. So you really need all these tyrosines, you really need all these signals inside the system to really drive very sharp. Um, a very sharp discrimination and the sharp boundary between the different antigens. So, this comes to some issues which are coming back to the field, which is when people start to do single-celler analyst and look at the TCR repertoire, they find out that when you look at individual clones, so on the bottom axis here, on this graph, you are looking at the individual clones which have been tagged, and you look at the state of differentiation of the CDAT. The state of differentiation of the CDAT cells inside the lung of tumor patients, what you see is that each individual clone seems to differentiate into very different orders. So in this particular paper from the amigo NI group at Curie, they found that you could define maybe six classes of clones, which most likely control, which relate to six different classes of antigens, so much more than what people have seen from Famic development. And we wanted to have a method to really be able to characterize that more quantitatively by understanding. More quantitatively by understanding what T cells do in terms of quantitation discrimination. So that goes back again to the issue of understanding where we need the signaling machinery. And we have a slew of papers with Paul where we tried to do some models to reconcile the speed, the sensitivity, and the species of T cells. And we learned of that from doing these models. And I'm going to pass it to Paul for the next step. Oh, sorry. Yeah. I have to share my screen now. To share my screen now, yes, it's gonna be a tricky uh back and forth. It's fine. Can you see my screen? Yeah, okay. So, I'm there. So, I'm just going to describe you first our first theoretical ideas on this process, how you have this very, very sensitive and specific discrimination. So, this comes from years of discussion with Gregoire, starting with his masterpiece in two thousand five in the PLOS Biog paper. 2005, in the PLOS Biog paper. And we finally got an idea of a class of models that we called adaptive kinetic proofreading. And I'm just going to tell you a few words about this that will be relevant for later. So what is adaptive kinetic proofreading? So we heard about kinetic proofreading, classical kinetic proofreading, starting from Matt Keysan in 1995. And Omer talked about this. We also talked about this earlier this week. And so this is the idea that you're going to have several. Here, that you're going to have several phosphoration steps induced by the antigen ligand receptor interactions. And so, what do we add to this to account for some very puzzling experimental data starting again from Gregor's paper in 2005, including antagonism? We add basically a negative feedback. And so, our idea is, for instance, that this negative feedback could be mediated by. Negative feedback could be mediated by a non-specific phosphatase, like SHIP1. And so let's assume this SHIP1 phosphatase is activated by an early step in the QT proofreading cascade and essentially move the proofing steps backwards toward the left. So then in this class of model, you can easily show that there are very interesting things that happen. So for instance, you could get a situation like this. So if you look at the output of the network, let's say this complex. Of the network, let's say this complex Cm, and then you vary the ligand concentration and you look at different binding times, you see that you can really have a very, very sharp. If you imagine you have a threshold here of activation, you can really have a very, very sharp threshold of activation where ligands that are, you know, agonist ligands, binding time, say, higher than 10 seconds, will really trigger response with a very, very few number of ligands. But then, if you decrease a little bit the binding time, then this class of ligands will absolutely not enable. Will absolutely not enever trigger response. And then you will have all intermediate cases where some ligand could act as partial agonists. And so to conceptualize this, so just a small equation to conceptualize this, the way you should think about this is basically like not only one kinetic proofloading cascade, but something like the ratio of two kinetic profiding cascade. And so if you do that, basically imagine you present a pure mixture of ligands first, then you're going to have a kind of rescaling that is going to happen. A kind of rescaling that is going to happen so that you're going to be only sensitive to the binding time of the ligand, kind of irrespective, almost irrespective of the concentration. So, then if you threshold on this, you could easily detect then with a very, very good specificity a ligands above a threshold. So, then the interesting thing is that when you start studying this class of model, you have all kinds of interesting phenomena coming with it. And one phenomenon that we're really interested in. That we're really interested in in particular is antagonism. So, sorry, there was something? No? Okay, good. So, antagonism is going to be especially if you consider a mixture of ligands, because of course, in the real world, the T cell is going to be exposed to several kinds of ligands at the same time. And then imagine you make a mixture of ligands that are just above sexual detection with ligands that are below sexual detection. Then, you can show that this mixture of ligands will yield a response that is. Of ligands will yield a response that is lower than the maximum response that you could get. And so that's antagonism. And in fact, you can show that this class of phenomena, you always have antagonisms. It's kind of a theorem. We call that the spendrel theorem because this is a bit like a spendrel evolution, where if you have this class of very specific and sensitive discrimination, we call that absolute discrimination, then necessarily you will have antagonism. And to get a bit more into the details of why you have antagonism, I just Of why you have antagonism. I just want to describe very briefly on a kind of a more realistic model how this works. And so we're going to focus on this kind. So we have these kind of planes here, like we look at the response as a function of ligand concentration and ligand binding time. So those numbers are risky as they do not really matter. But you know, let's say we're looking at some kind of weak agonist or weak to strong agonists. So then what you can do, and you can kind of What you can do, and you can kind of understand from the network what is going to happen and why you can have a very sharp discrimination. So, I'm going to show you in a very sketchy way what will be the response of this network as you increase the number of ligands for a quality of ligand just above threshold. So, then what is going to happen that for low construction of ligands, it's kind of like classical KT proof reading. The total response is just going to increase monotonously. So, something linear in NOx scale, say. So, then what will happen? NOx scale say. So then what will happen as you increase the ligand concentration, you're going to start turning on the negative feedback. And so if the negative feedback is strong enough, you're actually going to have something counterintuitive. That is, you're going to have a decrease in the response because it just kicks in the negative feedback, you know, and if this feedback is really strong enough, you're really even going to decrease, going to win against the positive activation. And then at some point, what will happen is that the feedback is going to maybe like saturate or they could. Maybe like saturate, or there could also be some positive feedback. For even higher ligand concentration, you're going to start again. And so, this kind of combination of a positive activation with a negative feedback gives you this kind of non-monotonous behavior, and this is kind of flat. So, because of that, now you can look at what happens for stronger and weaker ligands. And for the weaker ligands, basically, you only activate the negative feedback pretty late so that you never get to this blue curve here. To this blue curve here. And for very strong ligands, what will happen is that the negative feedback will kind of be irrelevant and will always be above these blue curves. And because of that, let's say you make a threshold of activation here, you're going to see that only the blue and red ligands activate the response. And importantly, one interesting thing that you see, and I will come back to that later, is that you have some kind of non-monotonic behavior. And in fact, this non-monotonic, where you first increase the response and potentially decrease. Just increase the response and potentially decrease the response locally. And in fact, we see that experimentally. And so there was this very nice experiment by Gregoire's lab in 2008, where they look at the cell cell variability, and in particular, the variability of three proteins, CD8, ERC, and SHIP1. And so what you can do, you can look at the dose response curve for various ligands and define EC50, like the concentration for which you have 50% of response. And then you can also define. And then you can also define the amplitude of response, and you can show, you can look at how those different parameters vary with the concentration of the various proteins. And so what is interesting is that, so if you look at EC50, for instance, you see that ERC and SHIP1, so sheep 1 is our negative feedback. They basically do not change EC50. ERC1 is going to be implicated in later processing at the very end of the process. But CD8, you get some kind of analog. Of analog response, you can modify the threshold with CD8. So that's kind of what you would expect for a normal model, like say kinetic proofing-like model, that you would have some kind of continuous response of CD8. But what was really striking in this paper is what happens for the amplitude as you vary CD8 occurrence on sheep 1, and in particular sheep 1, what you see is that as you increase sheep 1, there is a very, very sharp drop of response when sheep 1 is basically much. When sheep one is basically multiplied by a factor three, which means that this thing here is essentially collapsing as soon as you increase sheep one. And if you think about ship one as a negative feedback, this kind of makes sense because you increase the negative feedback. And then if you increase it so much, boom, you're going to completely collapse response. And in fact, our model explained that very, very well. You can just make Gillespie simulation of the model. And as you increase CP1, what you will see, you see exactly a phenomenon like this. Will see, you see exactly a phenomenon like this. Like, as you increase C point, you're going to collapse the response, and you're going to start collapsing the response at high ligand concentration because of the non-monotolyity of the feedback. The feedback is for strongest at high concentration of ligand. And this is really seen experimentally as well. This is what happens when you increase chip one. You lose the response first at high concentration of ligands. So, this is kind of our model kind of recapitulates this very contraintuitive phenomena by just a simple non-monotonic feedback, negative. Monotonic feedback, negative feedback, also explaining at the same time specificity and sensitivity. And this led us to all kinds of exploration. And in particular, since there was a few discussion about machine learning and biology, I wanted to point out something that we have studied more specifically because this kind of networks where you would have, you know, steps of phosphorylation related to a feedforward or feedback interaction, it's kind of reminiscent of, say, Reminiscent of, say, a neural network, and one of the questions I asked myself is that so, if it's a neural network, and given that there is antagonism in the system, because I know very well antagonism in this model, and I can really prove a lot of things on this antagonism of the system, is it something that you could see on your network as well? And then, after some exploration, we realize that indeed this is something you see. And so, this is something you might have heard about. This is called something adversarial perturbation in your network. So, it's just So it's just a machine learning parenthesis, a small game. So you can, let's imagine you train a neural network to recognize digits, say three and seven. Imagine you do it very naively. So your neural network is basically printing, oh, this is a three and this is a seven. And so it's well known in the machine learning community that depending on the training, what you get, you could get what are called adversal examples, which are those kinds of perturbations here. So you basically engineer perturbation, and here you Perturbation, and here you engineer the perturbation D. So here you do three minus epsilon D. So this is kind of a small background change. And here you do seven plus epsilon D. So it's kind of the reverse thing. You have this small background change. And then it's what happens is that you completely fool the machine learning algorithm. This is detected as a seven, and this is detected as a three. And that's a big problem in recognition of tons of things in machine learning because machine learning people are aware that you can. People are aware that you can engineer very small perturbations like this that are fooling you, and it's even worse than that, it's universal. So, for instance, if I take a picture of this picture and then I give it to another machine learning algorithm, it'll be fooled the other way, the same way. So, that's something that is very, very universal, very, very common in machine learning, and you need to really engineer different to get that. And so, the reason why I mentioned this is because that reminded me of antagonism in immune recognition, where if you put a little Recognition: where if you put a little bit of some spurious ligands with agonists, you might basically decrease the thing so that you lose response. And in fact, this analogy is actually more than an analogy because we showed that mathematically, this perturbation that you see here is called a fast gradient sign method perturbation. It exactly corresponds to antagonism by weak self-ligands in this context. And so, we could show that the proofreading mechanism implicated. Mechanism implicated in this network are akin to a defense against this adversarial perturbation. So, the point I want to make is that there's really like a deep connection between this immune recognition and cell decision networks and machine learning. And I like what Gregoire says. It's like, you know, the immune system is a bit like a liquid brain. And I think there's a lot of analogy to explore between a neural network and immunology. And immunology. So, this is the end of the intro. Oh, no, there's one last thing I want to say. So, now, if you think about this machine learning thing, analogy, again, what is interesting that in machine learning, you're going to have a lot of intermediate nodes that will process something. And so, maybe an idea that you might have is that maybe these nodes in a real system, in a real immunological system, will correspond to some different proteins on different cytokines. And so, we have kind of this idea that. And so we have kind of this idea that if you look at this adaptive KTP fooding network, maybe different complexes in this relatively simple phenotypical model might control different cytokines. And so this is really helpful to think about this this way. And now this is the end of the intro. We're going to tell you how we can combine those ideas to do universal antigen encoding. So I'm going to stop sharing and I'm going the relay back to Gregor. The relay back to Gregoire. We are really trying pushing the envelope into a vietro, but yes, thanks for Paul. So taking it there, we developed a robotic platform to go after the kind of data set that Paul is describing. We wanted to understand if when we measure lots of nodes in the system, maybe we could rebuild the way the T cells see the world and really understand what they see inside the peptide MHC. What they see inside the peptide MHC and kind of build a model of T-cell activation that way. So, again, we are trying to address a very fundamental issue of T-cell activation. We want to distinguish between different quality of ligands and different quantity. In particular, we want to do something that we describe as absolute discrimination, which says the T cells will be able to discriminate between different antigens completely independently of the quantity of ligands from one to ten to five. We could be distinguished anagonist. Five, we could distinguish an agonist from non-anagonist. So, again, classical assays, we can measure how much antigens are being presented and how much T-cell activation you can get by looking at different markers at the level of single cells. But this is one time, one readout, and things get saturated, and things can be very hard to go from one system, one TCR to another. So it was not very, very satisfying. And most particularly, like, no, I'm going to skip that part because we're a bit behind, but basically, That part because we're a bit behind, but basically, what we want to state, and again, that's really from the discussion we've put going back and forth over so many years, is that we really can map this issue of the way the TCR reads different ligands and different spectrum of ligands into kind of a signaling bottleneck, right? The TCR is taking all these hits and it's trying to activate all these different nodes, you know, map kinase activation, calcium, and fat, and JAXAT, and so on and so forth, and then decide in the response in terms of cytokines or in terms of. In terms of cytokines or in terms of cell cycle, in terms of king, and so on and so forth, with, of course, some feedbacks because these cytokines are going secretive. But again, at some level, we wanted to map it into a first step, which is really about antigen recognition. A second step, which would be some kind of a bottleneck where we do some antigen encoding. So we are kind of looking for a model which would reduce the dimension of this very high-dimensional problem of many ligands, many different signals to something manageable for the cells. And then we can always. For the cells, and then we can always expand and map it into different response in this case in terms of cytokines separations. So, the jokes that we are totally prepared for the pandemic. So, we have been building this robotic platform which enables us to do experiments without coming to the lab. So, no, at some level, we thought we are being very smart at the beginning of pandemic, but we didn't anticipate that to win this robot, we need a lot of tips, and we are running out of tips because nobody's making them. But this is what it looks like in the lab. So, I'm gonna let it run. So, I'm gonna let it run while we are running. I mean, it's exactly what Jaron was presenting for his project as well. We have something which enables us to really multiplex with plate measurements such a way that we can really acquire big data sets. So practically, we take the classical assay that we get into an immune system, mixing T cells and gentlemen cells, run it into the robot. The robot goes and assemble the system, collects cytokines, collect the cells. We can analyze all the The cells, we can analyze all that by flow cytometry, and we end up with a very large data set. We measure, you know, typically today, you know, 40 cytokines or 40 markers, because it's a robot, we can measure a lot of time points, even in the middle of the night, and we can really get larger assets very quickly. And if you just do like dimensional reductions or very quick clusterings, you see nothing, right? There is no obvious pattern. If you don't do any smart processing, you are not going to get an easy way to really. Going to get an easy way to really collapse this data and deal with the complexity of the system, so we had to do something a bit smarter. And I stop sharing and pass it back to Paul. Okay, so my turn again. So now I will tell you a bit about the data processing and modeling part. And so the first thing we did was to find a way to really analyze the data and reprioritize the data to. And reparamilize the data to get a sense of what happens. It also relates to things that Alexander Hoffman presented earlier this week. So, the first thing is to find, you know, how do you analyze those data? And so, the analogy we use is that, so, for instance, imagine if you look at planetary motions and you have a geocentric model, that's beautiful, you know, and that's predictive. You know, in fact, the ancient Greeks could predict the location of planets, but that doesn't sound like the right theory because we have a much better theory, which is heliocentrism. And so we want to. And so we want to use a bit this analogy to tell you, to basically find what kind of good features in the data are relevant for the analysis. And then combine this with machine learning. So there are a few examples, recent examples where people use neural networks to try to rediscover things like Newton's law and essentially, you know, rediscovering the physics. The physics basically, but to rediscover this, you need to have the right set of variables. You need to know that this is the distance to the sun and the speed around the sun that matters. And that's the first question. How can you get these good variables? And so to make a long story short, we found a good way, in our opinion, to freeze those data. And I'm not going to tell you how we found it. I could give you more details. But essentially, let's look at two kinds of two different At two kinds of two different ways to look at the data. So we can look at time course, cytokine, time course. And so this is the first way. You're really looking, for instance, here at the log of concentration of cytokines. So there are three cytokines. And then we look at different antigens. And so you can see that, I mean, obviously there is some structure. But, you know, if you want to say do classification based on the ligand quality, the separation is not that great. But then, Is not that great. But then, a very simple trick that you can do to somehow get a better sense of those data is to integrate in time those cytokine concentration, and then you get this. And so when you, so this is really like going, say, from, you know, the energy again, going from geocentric to heliocentric, you change your frame of reference, and then suddenly your data are much easier to analyze. So I don't know, the movie is not okay. Suddenly, the data are more easy to analyze, and then you see that the antigen. That the antigen are much, much better separated. And so, once you have this, let's say this is your feature in the data that you want to deal with, what you can do is to then train a machine learning algorithm. And so, we're interested in deconvolving quantity from quality. So let's do quality first. And so, you have all your data. So, you know, all the cytokine trajectories for different antigen strikes and different antigen quantities. Tracks and different antigen quantity. So we take the integral, we feed that to the neural network. And so, and the way we did it, we did it to be able to somehow analyze the data. We did not feed it to any neural network. We gave it to a very simple network, but where there is a two-node button. Just for a second, Paul, sorry. So this is in single antigen conditions or in mixture? No, this is single. This is single. You see later, we have more. We have more to say. We have more to say about this, but we train on single antigen, but we have, of course, we have many, many antigen quality. So we train this. And so we train this neural network to basically predict the antigen quality. But importantly, we add what we call a bottleneck so that sometimes somewhere in the middle of this neural network, you're going to have two nodes. And the idea was we would like to analyze the dynamics of these two nodes and maybe to learn a bit more about this process by just. About this process by just focalizing on these two nodes. And so that's what we did. And so then we do that. We get, so we can look now at all data essentially project them on this two node space, like the 2D latent space. And then as you can see here, so the colors are different antigen, and we see like we have essentially a perfect separation, almost perfect separation of antigen. So that means that our data has been treated, compressed, so that we could really infer from Uh, infer from this latent space the quality. Like, we know we know that if you have a trajectory here, if the data are here, we know that this is N4, essentially. That's what that means. And we know that we are here, we are essentially a T4. Okay, so and then we can do a lot of things. So, for instance, one thing you can do, we can actually reconstruct, importantly, we can reconstruct the data from this 2D space by, you know, everything is very transparent, so it's really something we can do easily. And so from this. We can do easily, and so from this 2D projection, we can reconstruct our data. In fact, we can do even better, we can even generate synthetic data. But for now, let's just assume we know let's just focus on reconstruction. So that means that this 2D thing is basically capturing entirely the complexity of the immune response. And so this is great for theories of physicists because you can just focus on what happens in the 2D latent space and model it, understand it. You don't really have to look at anything else. Don't really have to look at anything else to understand the dynamics of the system. And then, yeah, it's just to show you the quality of the reconstruction in various conditions, various time points, various eye tokens, we get perfect correlation between the reconstruction from latent space and the actual regional data. So that means that we capture everything there. Okay, so now what we can do, we can model it. And so I'm a physicist and then I like rockets. And so when I see trajectories like this, you know, So, when I see trajectories like this, you know, boom and then coming down, it's very natural to just say, let's model this as ballistic equations. And so, we define a few parameters: the speed of the ballistic thing, there is the angle, and then what happens here, you see that they're going going up sometimes, then they go down. So there's a kind of push phase, and we could parameterize this push phase by a parameter T0. So, simple equations, you write simple equations for this, and then you feed the data. And basically, And it basically works very, very well. You can feed those things very precisely. You can really, really parametrize all your data in a very nice way. So, I mean, we're just showing you one data set, but actually this works for many, many data sets. It's very reproducible. We did a lot of things. Gregoire will show you more about this. And the interesting thing that we see is that now, if you look at the parameter structure of the trajectories, so here I show you three parameters, V0, T0, and theta. V0, T0, theta, they are essentially perfectly correlated. Like they're just on a 1D manifold, essentially. So all those parameters are correlated, which means that it's very interesting because we're kind of rediscovering that there is only one parameter governing this trajectory. So it's essentially one quality that you can continuously change. And as you continuously change it, you'll be able to essentially generate all those trajectory in latent space. That means all those title calculations. And space that means all those titokine dynamics that you saw. So, our machine learning algorithm is essentially so we are just training it on a few qualities, but training it from a few qualities, you can essentially rediscover that those there is this continuity on those qualities, on those strengths, and we can reconstruct it, infer it, parametrize it. So, our model generalizes. This is called generalization. We're not only recognizing antigen, we are able to generalize to a new antigen. Okay, and so. Okay, and so another few things we can do, and so we could take maybe an information theoretical hat, you know, Alab Dialek, as a physicist like that. And so, for instance, what thing we can do, we can try to infer, given some noise in the data, how many categories you can distinguish, how many ligand strengths are possibly encoded in these cytokines trajectories. So, we did a bit of information theory. It's very gory. They say that's kind of technical, we have to do a lot of things. Nicole, we have to do a lot of things, and but it works very well. And so, in the end, what we can do, we can show that we can at least distinguish five categories in these trajectories. And the five typical trajectories are shown you with this with these different colors. 6.5 now, 6.5. Okay, so we improved it. Okay, good. Sorry. Yeah, we improved it a bit. So we get many categories. We can show that you have really many method separation in this. You have really many separation in this kind of continuous space, and the important thing is that it's more than three. Like Gregoire was telling you, you know, maybe there are three categories of antigen, but we have definitely much more than three. And we're probably still limited by some exponential noise and stuff like that. So we've very, in fact, the way to think about this, it's really like a continuous encoding of ligand strengths that we see in this cytokine time process. And so the way we visualize this now, and I'm Utilize this now, and that we find very useful. And to connect back to what I told you before, is that we could look at, for instance, typically this trajectory latent space at t equals 36 hours. So I'm just looking at the position in this latent space at t equals 36 hours. Then we're going to plot the position of this coordinate one and the coordinate two. And one very striking thing we see that for the coordinate one, it increases monotonically as a function of the integing quality. Function of the integing quality, while for this latent space two, we have a kind of non-monotonic behavior where you first go down and then go up again. And then the analogy is very similar to what we showed you before with our SHIP-1 model, where we had a non-monotonic behavior. We also see this, and basically, there is a competition between a positive part of the network and a negative non-monotonic part of the network. Our latent space projection seems to really. Our latent space projection seems to recapitulate this. We have this non-monotonic part, which is kind of like the activation part of the network, and then this non-monotonic part, which is a bit like our negative feedback, or we believe it's a bit like our negative feedback, and we'll tell you more about that in a second. So we kind of reconstruct this kind of trajectories. And in fact, there are theoretical models, including by Dielek's recent paper, where they suggest that you need this kind of convolution between a monotonic variable and non-monotic variable to be able non-monotic variable to be able to encode many, many categories, many biological categories. So we cover that from our data. And so now I'm going to finish here for my part and again give it back to Gregoire who is going to tell you how we apply all of that. And I'm going to move to the room back for the questions. Sorry, can I interrupt for a second? Yes, of course. Yeah, I didn't quite understand the thing about the normal elasticity in L1 and L2, because I tended to think of L1 and N2 as. I tended to think of L1 and N2 as some hidden space in which you could do patients. Yeah, no, that's true. This is what you see in the latent space. This is absolutely what you see in the latest space. I'm thinking of the latent space as a space in which you can do rotations free. Yeah, that's true. But the way, yeah, I see what you mean. If you do rotation, then things that are non-monotonous become monotonous again. No, but we. Again, no, but we do have, yeah, it's more like a sign on a cosine, if you want. Yeah, it's you're doing one cut in the latent space, right? You're just in a way, it's like a section of the latent space at one given time, which reveal this stuff. But yeah, you're right. It's one parameter of but so you could be worried that this statement would depend on exactly what rotation you choose. So I think what you're saying is that the thing is turning, right? Yeah, yeah, yeah, that's basically what we say. That's basically what we say. We this is turning. This is more like a sign on a cosine, you know, like the sign, you know, for an angle, it's monotonic, and the cosine will be, you know, non-motonic. This is kind of the idea. But yeah. Hang on to this interpretation because we're going to come back when we repeat a bit in the experiments. So, I mean, I hope you appreciate our continuous or back and forth. We even keep a French accent, right? It's very impressive. Anyway, so this is some. Anyway, so this is some continuing experiments to try to test how we can use our latent space. Again, I'm going to skip the last part, which is really all the parameters that we determined in our calibration data sets with the OT1, just showing that it's really a low-dimensional space where all these parameters are qualities. We can really parametrize all the cytokin trajectory with a single parameter, and we're going to use this V0 out of convenience. And here we are plotting. And here we are plotting this V0, which is kind of the immune speed. So the speed at which you get out of the starting block, the speed at which you start accumulating the cytokines as a function of the antigen quality measured on this OT1 system. So again, from the very weak peptide like E1 to the very sorry, very strong peptide like N4 to the weak peptide like E1 and NSERF. And then we can train on a bunch of peptides. And you see, it's again, it's a very low-dimensional space. Low dimensional space. I will emphasize that this is very striking because V0 is measured because we know the time dynamics, right? We know nothing about the quantity of peptide. We do it with just the T cells which are given to us from the mouse. We are not titrating the amount of peptide. We are not testing different number of T cells. We are not testing different inflammation. And yet we can measure something which is very intrinsic of these different antigens, something which is really about the level of antigenicity or strongly they can. Level of antigenicity, or strongly they can activate the T cells. And this is again something which is very, very useful when we move towards clinical applications because it tells us: give us some T cells, give us the tissue you are trying to respond to. We're going to measure the time dynamics, and we're going to be able to tell you our antigenics or how strong of an immune response you're going to be able to get, even if we don't know the peptide activating with T cells, even if we don't know the frequency of T cells, even if we don't know the quantity of otagen that we have in the system. So we can do it in different contexts, and we're going to We can do it in different contexts. I'm going to skip this part because it's going to be in the paper, but like, no, different macrophage, genetic cells with different levels of inflammation. Every time we get new data, I mean, it's really becoming the stuff when we look at the data with Paul's rook, we always look at the latent space. It's really the way to compress this 40-dimensional space of 40 different cytokines. It gets projected into a 2D latent space. It's all we need to know to really look at our different conditions. And again, this model which And this model, which was learned with B cells and splenocytes, generalizes well with dynamitic cells, maybe not so well with macrophages. We have to do a little bit more experiments to understand the background activation that we see for the macrophages. But again, we have been able to work this latent space across different settings with tumors, with the dendritic cell, different inflammation, and so on. This is really our go-to compression because it really teaches us in the way these T cells get activated in all these different contexts. These different contexts. And not only normal T cells, but we can apply it to all kinds of different T cells. So we have done it with human T cells, which have very different patterns of cytokines and such. But again, once you project it into the latent space, it compresses very nicely. We can still rank antigens very nicely. And this is even more striking because the collaboration with Naomi Taylor's lab at Near Alicia, where we're looking at more SCAR T cells. So, sorry, I missed. CAR T cells. So, sorry, I missed the cartoon for most CAR T cells. The CAR T cells are chimeric antigen receptor cells. They are endowed with receptors which contain an antibody, which recognizes the tumor, and then it's linked to a signaling domain which looks like a T cells in some ways. So it's completely different in terms of signaling machinery, completely different in terms of the range of parameters at which we get activated because it's an antibody binding to CD19 on the surface of B-cell tumors. And yet, when we look at the litan specification, And yet, when you look at the lithon space, again, in Dash, you're going to have all the peptides which are activating the OT1. And then in continuous line, you're going to have a mix of T cells activated with a CAR T cell recognizing CD19, that's a black line here, or CAR T cells corresponding with OT1 TCI with different peptides. And again, we can look at the data in latent space and organize them very nicely. In particular, we can measure or Can measure or go to parameters, you know, these immune velocity parameters that we derive from the latent space. We calibrate the system with the TCR only, and that's the system with the TCR only. You get the NASCRT for the different antigens for VOT1. Then, when you look at the CD19 CAR T receptors interacting with CD19 on the tumors, you get this V0, which is like a very weak, very partial antigen, which is already a big deal for the carefield. For the Car field, because we expected these things to be more like an N4 because the binding affinity is very high, and not at all. We find something which, in fact, very weak in terms of T cell activation. And then when you mix the two signals, you see something which again is very important for our model, which is antagonism, meaning that suddenly when you mix CD19 with E1, you can really go down in terms of isymmocity and really decrease the overall activation. And we see also synergism in different contexts. Suddenly, by looking at the latent space, we can really. Looking at the return space, we can really calibrate how these signals are adding in a nice way, and there are a lot of surprises in what we are doing. And then the last bit, which is so, I'm going to skip that part for the sake of time, but we can use some drug inhibitors and again look at the latent space. The important stuff is that every time you perturb the T cells, here I'm showing you the variability in terms of our V0 parameters in our latent space, you have calibration with the pure pay. You have calibration with the pure peptide. And when you add the drugs, what you are mostly doing is recalibrating the ontogenesity of the T cells. So, as soon as you start perturbing mat kinase, perturbing calcium, and so on and so forth, what you are doing is mostly perturbing the apparent ontogenicity of a given tissue. So, you are moving in a kind of a boring manner, kind of deprecating the tissue activation. Except for a few drugs, like these two blue drugs, which are JAC inhibitors, or Recyquimode, which is a TLR agonist. Which is a TLR agonist. Suddenly, you derive a new class of T cells, something which can be fitted in the latent space, but again, in terms of parameters, looks very different compared to what we are used to in terms of T cell activation. It means that we can define new classes of T cell activation. We can derive new type of T cells, and we're exploring that further to understand what we can do with these T cells. In particular, we expect that this Recycling mode is really a nice way to boost the activation of the T cells in a way which makes them much more passive. A way which makes them much more persistent and much better at fighting tumors. And again, having been able to really fit all these parameters and look at them in a return space enables us to really find these perturbations, which move us out of the ordinary and these have interesting perturbations that we want to further test. And the last bit, and I don't know if Elene Rothenberg is still here, but this is really something that she suggested just before we shut down for the pandemic, and she's to contact Paul Love at the mill. Paul Love at building six at the NH who is working on some genetic variants. So, Paul has some mice, and I can't tell you at this stage what the mutations are and how this mice were perturbed. I'm sure Elena knows what it is about, but we're still in the process of finaling this manuscript. When we look at these gene variants, so we start with OT1, so again the monotonous latent space one versus the more complicated latent space two, when we look at the cross-section at thirty-six hours. A cross-section at 36 hours according to the different stress advantages. So, this is really our adaptive kinetic profiling scheme as we understand it from the modeling point of view. And now, when we move to the signaling impair.t1, we see that things are kind of slightly changed. So it's not very striking when you look at the raw data. It looks like you can always compensate one side of canvas to another. But when we look at the latent space, we see something very nice in terms of interpretation, which is that latent space one has been shifted towards the left. So, in a way, this node is. Left. So, in a way, this node is responding much more strongly than the wild type. But, more importantly, a latent space 2 is completely missing this negative part, this kind of antagonism that Paul was describing in the original model. So, simply by doing some mutation in the way the TCR is organized, you're going to get these new patterns of response in the latent space. And what that means is that from a system which is very good at discriminating many antigens, so this is just looking at the cytokines or interflow. Looking at the cytokines, you know, interferon gamma versus L2 for different antigens across different cells. You see that you have a lot of classes, and maybe again six different classes of antigens you can define in that space. And when you move to these mice from Polov with experiments with Guillaume Go, we get into a regime where suddenly it's completely digital. We have lost the nice non-monotonicity in the latent space, and this makes you kind of limit the number of classes that this T cell can respond to and make these T cells kind of very. To and make these T cells kind of very limiting in the ability to discriminate different ligands. And I think, again, the analysis from the latent space really gives us a chance to understand what's happening and we are mapping it back to key signaling events, which make sense at some point. So I'm going to finish here and make a conclusion for Paul and I. So I hope we give you a sense that, obviously, getting big data sets these days is kind of a piece of cake. We think that with our robotic platform, we can really get time diligent. Can really get time dynamics, which is very important to infer causality and to be able to deconfold different aspects of diesel activation as it unfolds in time. So, again, a robotic platform is kind of a stunt in terms of being able to scale up the experiments and being able to acquire time series, but it's really becoming a go-to method because we can really acquire big data sets very cheaply and in a very scalable manner. And we can look at cytokines. We are starting to do a lot of single-cell. We are starting to do a lot of single-cell profiling with high-dimensional spectral cytometry. Obviously, we're going to get some very nice space there. And again, a bit of machine learning will be necessary to tackle that, but we hope to really be able to understand that in very high-dimensional space. And again, from the experimental point of view, because we are using only primary cells and we can really take them off directly from the mouse on the patients, we can work with a limited amount of products and really do these measurements in a nice manner. But then, from the theoretical side, we having this big data. Side, having these big data sets and being able to try to work around it to really compress them in a nice way, we think we can really compress the immune complexity to very low dimensional space where all the decisions are being made. And now we always go back to this latent space because everything is happening there. And it's really the way we want to learn the different nodes and the way this pathway is wired. We are a bit biased because we had our model that we think is very powerful at predicting a lot of stuff, but we really Predicting a lot of stuff, but we rediscovered it from scratch simply by looking at a little space, and we can really use it to understand why TCRs are very good at discriminating independently of quantity, independently of number of T cells, and so on. And we think we are going to be able to scale it up to really learn models of immune decision making. So, these are the people who did the work. So, very huge effort and very collaborative. So, we are really meeting very bi-weekly almost with Pulse groups. So, on my side, Suraj, Ashar has done a lot of development. Ashar has done a lot of development of robotic platform and a lot of the experiments in the pipeline to handle the data. And on the theoretical side, from the Paul's group, Francois Boasa and Thomas Rademaker, and again, we are really hand in hand. They tell us what mice we need to kill. We tell them what kind of model they need to do, and it's working very well, and some collaborators to four regions and the different aspects. Thank you. And I hope the back and forth went okay. And looking forward to your questions. Okay, so I don't know whose hand was first, but I'll start with Sid. Oh, it's a great talk. I guess I have a question which starts from what Thierry was asking in some ways. If you, so imagining, right, this is the perfect picture, right? So you have these vectors, right? In the end, you have lots of, you can look at this data, you can just have these vectors. And if you, I guess if I'm interpreting. I guess if I'm interpreting correctly, you have some kind of variational autoencoder, right? You sort of are finding these latent space vectors, which can sort of generate. And of course, in some limit, your autoencoders become PCA, right? If you take out the nonlinearity. So my question was, if you just did PCA on these vectors, how much? I mean, looking at, yeah, the question was, how does it look compared to that? I guess you guys. You're absolutely right. I mean, sorry, Paul, maybe you want to. absolutely right i mean sorry paul maybe you want to no no go ahead go ahead right you're absolutely right that the pca doesn't do so bad right so we the way we and at the end of the day our neuronal our model is very simple this is kind of a two pcas one to go to the latent space and one to reproject back to the different quality of antigens so what we did was to to quantify the the information content we could derive from this latent space derived from pca from an onto encoder of From an anto-encoder from our model. And we do a little bit better in terms of information content. We do much better in terms of being able to classify the antigens. It's not black and white, right? It's just quantitatively better rather than qualitatively better. You're absolutely right. The PCA was already quite good at capturing most of the diversity of the response. It's pretty cool. So non-linearity helps a bit, basically. You're saying, okay, that's pretty cool. Bit basically, you're saying okay, that's pretty cool. Okay, yeah, and also if I can add, like, for when when we reconstruct data, we like non-inergy really helps. So, uh, so that's that's also an important thing. So, so the encoding part you can capture nicely with, I mean, relatively simply, but the decoding part, you need a bit more work to get really important tricks. I see. So, the generation was hard with the linear model. You're saying the generation, I see. Okay, that's pretty cool. And, yeah, I don't know, maybe let's. And yeah, I don't know, maybe let's actually ask this question, but okay, fine. Thank you very much. I'm so sorry because I have to leave in eight minutes, but I love the talk. Thank you so much, both of you. Could I just ask?