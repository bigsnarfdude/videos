But I do work with integration of biological data, specifically networks. So I'm hoping that there's some overlap here between my project and what's been presented so far. Okay, so I work on biological network integration. So these are, I'll give you some quick background about why we care about that problem. I think you'll all, this group will certainly understand. This group will certainly understand why it's important. So, in systems biology, we really care about characterizing relationships between genes and genes themselves, basically trying to figure out where genes sit in a sort of global functional map, right? We care about just characterizing genes, right? And for some model systems, we have a good sense of what a lot of the genes already do. Of what a lot of the genes already do, but for other systems, we don't necessarily. So, humans, for example, there are still many uncharacterized genes, there's still a lot of stuff we don't know. So, you know, it'd be great if we could figure that stuff out. And a great way to do this is by using biological networks. So, these are networks that connect genes or proteins. And these are, you know, some examples would be protein-protein interaction networks, coexpression networks. So, you know, networks that are derived from. You know, networks that are derived from gene expression profiles and genetic interaction networks. So these would be networks that correlate the genetic interaction profiles of various genes. And so all of these networks have useful information in them. They're all different in the kind of information that they show. So protein-protein interactions, for example, obviously show physical binding events between proteins. Events between proteins where genetic interactions are a little bit higher level, and they look at what happens. You know, if you remove two protein or two genes, is the effect on the cell worse than expected? And if that's true, then that indicates that there's some functional correspondence between those genes, but it doesn't tell you exactly what. So, you know, all of these networks give you some sense of functional linkages between genes, but they But they can only determine sort of certain types of functional linkages. And they all have some sort of bias or noise associated with them in general. So bias would be sort of what I just mentioned, as well as instances where certain areas of the functional spectrum are maybe overrepresented or underrepresented. So co-expression networks, it's sort of well known that co-expression networks. Known that co-expression networks can capture like the ribosome and ribosome biogenesis really effectively. But other biological processes, maybe not so much. So those are examples of biases. And of course, there's noise. So these are things like false positive relationships between genes because these networks are produced using high-throughput technologies, which are very Throughput technologies, which are very prone to noise. So, yeah, so all of these networks, they all have some sort of issues with them. So, using them alone will get you, depending on the quality of the network, will get you somewhere close to characterizing the genes you're interested in. But for a holistic picture, you want to sort of avoid these kind of biases and reduce the amount of noise, right? And reduce the amount of noise, right, to get a better picture of how genes function together and do that functional characterization. So we like to try network integration, right? So yeah, and the idea here is just on aggregate, you can come up with a representation for your genes or proteins that sort of, I don't want to say averages over, but controls for the bias. Over, but controls for the biases and the noise that are present in these input networks. Right, so there have been many methods produced that do this, but for the sake of time, I'm just going to talk about one that we've developed, which is called Bionic, stands for biological network integration using convolutions. It's a deep learning approach which basically takes advantage of this new. Advantage of this new architecture that's been developed in the deep learning field called the graph convolutional network and it uses that to effectively encode networks. So I'll describe how Bionic works. So the idea is you have your input biological networks over here. Here we have two input networks, but in reality, Bionic can scale to potentially hundreds. I don't know what the upper limit is. Hundreds. I don't know what the upper limit is. I haven't tested that. But in experiments, we've done up to 68 networks. So you can have a lot of networks that you're integrating. Of course, these networks are traditionally represented as adjacency matrices, which you see here. And one thing we're doing, and this is probably relevant to this particular single-cell proteomics section, is that we have networks that don't necessarily overlap. That don't necessarily overlap in their gene sets, right? Or only partially overlap, right? So, for example, these networks, you know, they share a certain number of genes, but they also have these other genes that aren't actually present. So, what we do is we add in these genes to the network without any edges. So, it's basically in the adjacency matrix, you just get this, right? It's basically a vector of zeros. And this is a problem, and I'll explain. This is a problem, and I'll explain. I'll give some examples of what happens when you treat the extension of networks this way, when you extend networks this way kind of naively by just adding some zeros, it causes problems. So we actually had to take this into account in our algorithm design, and we found a way to sort of avoid corresponding bash effects that you get. But I'll explain that in the next Explain that in the next slide, I think. So you have these networks which can be represented as adjacency matrices, and these are fed into these graph convolutional networks. And I won't get into how these work, but essentially what they do is they take in a network and then they output features for all of the genes in this network. So and they're sort of a low-dimensional feature space. So essentially, what they're doing is Space. So essentially, what they're doing is they're encoding the topological information from a given network into a low-dimensional feature space. And when you have a low-dimensional feature space, you can, if you encode this effectively, you can actually reduce the amount of noise that gets encoded because you're doing a sort of lossy compression. And the information you're losing is, the information you want to lose in this case is noise. So that's kind of an analogy. It's not exactly compression. It's not exactly compression. I don't know what the mathematical arguments for that would be, but I haven't investigated that. But it is very much like compression, like a sort of lossy compression approach. So anyway, you get these network-specific features. So for all of your nodes in your individual networks, you get features from the GCN. And you get that in a network-specific fashion. So each of these networks is passed through its own independent encoder or GCN encoder. And then these And then these network-specific features are then basically just summed to get your integrated features. There's a little bit more that goes on here. There's a sort of a stochastic masking procedure. I won't get into the details of that, but if anyone's interested, I can talk about it. It does help improve the resulting feature set. But you get these integrated features, and this is what we're after. This is what we care about because this is the information from multiple networks encoded in a single feature. Encoded in a single feature space for all of the genes across all of the networks. This is like your sort of compact representation of your network topologies. And the idea here is that you have genes that are sort of in a similar area in this space should be functionally similar because across all of your networks, there's an indication there that those genes interact or share interacting partners many times. Interacting partners, you know, many times across multiple networks. So, this is how we sort of derive functional relationships between genes based on proximity in this feature space. Now, that's great, but the issue with this is that we have no way of optimizing the underlying parameters or the weights of this algorithm. So, we actually need to specify an objective, right? So, what we do is we then take these features and we And we decode them by simply the dot product of the feature matrix with its transpose. And then we get what is a sort of a crude representation of a network out the end of this algorithm. And then to specify the objective, it's just a mean squared error loss between this network representation. This network representation and the input networks. So we minimize a difference between this and our input networks. And by minimizing that difference, we can learn the weights of this algorithm, or the algorithm can learn its weights to more effectively encode these integrated features. There's a lot more I could talk about here, about specific choices in this algorithm design, but I probably don't have the time for that, so I will move on. If anyone's interested in talking further about this, I'll be. interested in talking further about this, I'll be happy to. Okay, so probably more relevant to this particular section of this hackathon is some of the problems that we encountered. And I'll talk about one main one that I think is relevant here. And that is this issue of overlap, right? So here I mentioned we have these networks and this network on its own, this one up here, Network on its own. This one up here only has, say, four genes, and then we extend it to have this gene, which is present in this other network. And this network here only has these four genes, but we extend it to have this gene, which is present in this network, right? So we're basically extending all of our networks with the union of nodes, the union of genes. And of course, the reason we do that is to sort of get an effective So, we can have these GCNs all be the same size, and there's other technical reasons why we do that, and other algorithms do this too. But the issue with that is when you give the values of these extended nodes just zero, so you basically say there's no edges between any of these extended nodes or genes. What you get is. What you get is, you're basically telling the algorithm that there aren't any edges there, when in fact, the reality is you just don't know if there are no edges there, right? It wasn't measured in that particular data set. So right here, I have what I did was I took two networks that had some overlapping genes and some non-overlapping genes between the networks. And I put them through this algorithm called MASHUP, which is a fairly well-performed. Which is a fairly well-performing integration algorithm. It learns features based on the topology computer input networks. I'm running out of time. Okay, so basically what's happening here is that mashup actually will embed your genes in different parts of the space depending on whether that gene is either in one network solely or in the other network or in the intersection. So you see, if a gene is in the intersection of these two networks, it gets. The intersection of these two networks, it gets basically embedded in its own space. If it's in network one, but not network two, it ends up in its own space. And if it's in network two, but not network one, it ends up in its own space. And this is a problem, right? This is kind of like analogous to a batch effect. I don't think it's really a batch effect, but it sort of feels that way, right? So you'd expect, you know, these green genes over here should all be one cluster. They should not be split into three separate clusters, right? And these are sort of genes of the same class. This is just Same class. This is just a toy data set. It's not actual networks. But yeah, so when we designed our algorithm, we identified this pretty early on as an issue that we needed to sort of figure out because networks, as a general rule, don't perfectly overlap. A lot of network integration algorithms, they just integrate this intersection, which is a big problem because you're throwing out all of the other genes. Throwing out all of the other genes that exist in the sort of non-intersection or the different parts of this penn diagram, right? So, how do we actually handle this? It's actually fairly simple. So, I mentioned that we try and reconstruct these networks. So, basically, we have our, you know, in that diagram I showed on the second slide or whatever, we get our network representation that we produce at the end. That we produce at the end of bionic, and then we try and minimize the difference with the input networks. But what we do is we just mask out any genes, any rows and columns of this adjacency matrix that are from extended genes. So basically, if there's a gene that wasn't originally in the network, we don't try and reconstruct it. So we're not going to punish the algorithm if it puts some If it puts some other values in there, and then we get this kind of nice, nice plot where everything's put together. Okay, so I'll finish up here. We also ran into issues of evaluation techniques, and I'll be happy to talk about this in the brainstorming session. How do we construct the benchmarks? You know, how do we compare input networks to the outputs of our algorithms when the nodes? The outputs of our algorithms when the node sets are different, that's always an issue. And then, how do we ensure that our algorithm isn't just biasing towards certain functional categories? Because that would kind of defeat the purpose of a network integration algorithm. And then we sort of came up with these three evaluation approaches, gene-gene interaction, functional module detection, and supervised gene function prediction, all using the features that are produced from Bionic. Okay, that's me done. So I'd, yeah, just like to thank Gary, who I think. Thank Gary, who I think is in the audience, and Charlie and my collaborator, Bo. Yeah, be happy to take some questions if we have time. Fabulous. Thank you. Thank you. Emirates would like to know, is it possible to extend Bionic to a supervised framework by adding a softmax layer? Yeah, so that's something that we've been thinking about potentially doing in the future. Potentially doing in the future, it'd be very cool to basically do gene function prediction directly in the algorithm. So you could take that embedding layer and then basically tack on a softmax layer or another feed forward layer with a softmax, something like that, and predict gene function directly in the algorithm. And it would all be online, and you could update the algorithm weights based on its performance on that. So it's totally possible, yes. Okay, Taylor, please. Okay. Taylor Farabay is asking what is the benefit of your GCN methodology versus other network inter integration techniques, such as using graphical models or similarity fusion? Yeah, that's a good question. So we have, there's many different limitations for a lot of these integration algorithms. And it's kind of, you know, some integration algorithms have, you know, Algorithms have, you know, they're really good in just about every area, but they have some limitations where others share a whole bunch of these limitations. So, some examples would be scale. A lot of these network integration algorithms don't scale past many networks, like Mashup, for example, which learns really good features, but it doesn't scale. So, you know, we want to create a scalable algorithm. Many networks don't handle noise effectively, so the features they produce are just not that great, or the output networks that they produce. Or the output networks that they produce are not that great. Other integration approaches have this issue that I showed with MashUp as well, which is this sort of when you have these partially disjoint node sets, you get this disjoint embedding space, which is kind of a, well, it's a big issue, right? So there's a lot of these sort of, yeah, there's a diverse set of limitations. And we kind of designed Bionic to address all of these as best possible. All of these as best as possible. Cool, and I think we really only have time for one more question because we'll have the photographs in a few minutes. But Joshua Welsh is asking, how do you envision applying this to the protein expression data sets from this session? That is a great question. I have not figured that out yet. And well, actually. And well, actually, I've probably got time for Vera's question. Vera Pankaldi is asking: how does this compare to using a multiplex network with different layers? I don't have a good answer to that question. I've not really worked with multiplex networks. The advantage of our approach, I'd say the advantage of our approach is the fact that we learn network-specific encodings. So with multiplex networks, you're just sort of multiplex networks you're just sort of you know smushing everything together and there's topologies in each one of the so if you if you you know you blow that up each one of those networks that you're putting together has its own sort of unique topological characteristics and the advantage of our approach is that the the gcn encoder learns that topology independently for each network um so it can sort of better handle that kind of thing yeah that'd be my answer Yeah, that'd be my answer. Cool. And with that, I think we'll finish up on questions. I really want to thank all of the speakers.