About a bunch of different projects that we've been working on using the fact that you can compute fast, flow-ranked approximations to matrices to accelerate all sorts of different optimization outcomes. This work is being led mostly by my PhD student, Zach Carnjella, who moved with me from Cornell to Stanford, and who will be on the job market this year for graduation next August. So he's fantastic and someone to watch out for. And someone to watch out for. Kratik Rathor is also working with us at Stanford on this project. She, who was a student at Cornell, is now at Amazon working on this. Theo has done a lot on implementations, particularly in Julia. And then Martolomeo and Volp have collaborated on various parts of this. So I guess that tells you I'm going to tell you about a lot. We'll see what we get through. Okay, so I don't think I need to convince this audience that ill-conditioning is a major. Ill conditioning is a make your problem. We've been looking at it mostly in machine machine learning context, where you should kind of expect ill-conditioning because if you're trying to collect a data set in order to predict something, then ideally you're collecting a lot of features that are all pointing in the same direction as the thing that you're trying to predict. And as you collect more and more features that are informative, the conditioning of your matrix gets worse. And so these are pretty. And so these are pretty typical spectra that you see in machine learning. And I think the key aspect that the things in this talk are going to be explaining is the fact that there's really fast spectral location at the beginning. Even towards the end, it flattens off. And this continues. We're only going to be in the top 100 singular values here. But this is really extreme, right? You can go down within even three, maybe five. Three, maybe five singular values, but you can go by many orders of magnitude, right? And if you can capture the information in that top subspace, which really isn't that much information, you can reduce the condition number of these matrices by preconditioning by exactly that much. That's a huge effect and can really speed up convergence at not a large cost. Okay, and the fact that these matrices are really badly conditioned really. Intrigues are really badly conditioned, really slows down machine learning. So, here, I mean, it's almost a ridiculous plot. Okay, so here's the spectra of this data set E2006, which is a standard machine learning test data set. And then we train all these state-of-the-art stochastic optimization algorithms on this, stochastic gradient descent, and the various accelerator approved or stabilized versions of these, like SVRG, Saga, and Tatayusha, which is accelerated. And actually, you can't even see that they are making. Like, you can't even see that they are making progress. It looks like they are making no progress. But if you do the right kind of preconditioning, which is still sort of compatible with the stochastic mini-batch model with common in machine learning, you can actually get convergence. You actually get fast convergence. We can prove fast convergence to the optimum at a linear rate for algorithms that are similar, which I'll get to at the end. We'll talk that to it at the end. That's a sort of preview of the kind of thing we'll be able to do. But here's the big thesis: is that randomized numerical linear algebra, the way that I like to think of it, is it allows you to compute approximately the inverse of an n by n matrix in order n time. You form a sketch of the matrix, use that to compute a low-rank approximation of the matrix, you can invert that low-rank approximation using Schumann Morrison loop, as long as you have a little bit of regularization. Once you know how to do that, you can improve conditioning for all sorts. That you can improve conditioning for all sorts of optimization problems. So, I'll tell you about a method that we used to use this, we used this idea to solve just linear systems using a method we call NISTROM precondition conjugate gradients. Next, I'll tell you how to use that as a primitive to speed up composite optimization. Problems of the form minimize usually some loss function that depends on a data matrix A, plus a regular. Data matrix A plus a regularized matrix. And then we're using the alternating directions method of multipliers, where we're noticing that the most expensive step can be sped up by approximating it as a linear system and solving it using this memory condition. The third thing that I want to tell you about is this method, Sketchy SGD, that allows you to add approximate and fast preconditioning to And fast preconditioning to stochastic optimization methods using a lowering approximation for the Newton system. And it can improve robustness and speed compared to first-order methods and other plastic methods. So, randomized from pre-pro. So, I would love for you to, you know, if people, you know, I'll watch people's faces and see what you're understanding, whether it's faster or slower. I'm guessing everyone's going to be very comfortable with the. Comfortable with the sort of linear system stuff, and I'm happy to explain more or even cut out entire sections of the machine learning or optimization parts. Depending on interesting. May I ask your question? Yes, of course. Previous question. Yes. So here I see that you, in icon two, you have f of a times x. Yes. And in icon 1 is AX. So you strongly rely on sketching on A. Strong rely on sketching on A. That's the key. The key idea is sketching on A, yes. And what about I? Oh, and in here, the relevant A is the mini-batch Hessian. So mini-batch, you take, instead of the whole sum, the sum of some of the terms, compute the Hessian of that, and sketch that Hessian. Then invert three. Okay, and that's your preconditioner. So anywhere you have an A. So anywhere you have an A, then you can have. Anywhere you have data, you can apply a sketch. Yes, I mean you can make things faster. Well, A, because if you do POD, for instance, you have data, but you don't do sketch. Yeah, yeah. Yeah, yeah, yeah. No, there's something special. I mean, there's something special about the structure of these matrices, right? These methods, the methods I'm describing, work really well when you have really fast spectral decay at the beginning. And they're also, they're more important when you have dense data than when you have sparse data. Than when you have sparse data. Because the sketching that I'm showing you destroys sparsity. And so if your data is extremely sparse, that might not be worth it. Okay, so what's the basic tool that we're going to be using to sketch all of these various things? We're going to suppose that we're given a symmetric positive definite n by n matrix, and we're looking for a good rank S approximation. And here is Approximation. And here is a randomized method for computing this. I want you to draw a Gaussian matrix omega with S columns. There are other sketches that can be used here, but for what I'm talking about, Gaussian works just fine. You get a randomized linear sketch, which is just A times omega. And then form the NISTRM approximation. And the NISTRAM approximation, I guess what's nice about it is that you don't need to revisit the data matrix. So in streaming applications, Matrix. So, in streaming applications, that can be extremely important. So, you can form this approximation only using the sketch y and the random matrix omega. You don't need to touch the matrix again. So, that's important if the matrix, for example, can't fit in memory. Not so important for this talk. Now, in practice, you don't actually compute the pseudo-inverse, right? But you compute this in a stable way. What are the properties of this approximation? You compute it only. You compute it only using matrix vector products with A. The total computation is just S matrix vector products plus order ns squared computation to do the QR and the small SV. The total storage is order N times S. And this approximation is positive semi-definite as long as A is. Its rank is less than or equal to S, usually equal to S. And it is a lower approximation of A in the positive semi-nuclear word. Approximation of A and the positive severity order. Okay, so what do we get out of this? You know, here's, you can prove a variety of different guarantees for this. In practice, it works much better than the guarantee, but this is the kind of theorem you would prove about it. So if you define the stable rank in this way, which is a sort of smooth version of the rank matrix, then you can show that the x Then you can show that the expected error in this approximation in the spectral norm is bounded by some constant that depends on the stable rank and lambda p. I guess the interesting thing here is that the p that appears on the right-hand side is only half as large as your sketch size. And there are different ways you can control this. You can improve this down. But this is kind of what you get, like by going up a little bit on sketch size, you can. You can compute this approximation without having to read the sphere matrix, and the cost that you pay is that depends on something sort of earlier. The quality looks like something earlier in the spectrum. So, you know, the best approximation, you'd have, if this is ranked S, this would also be led by S. We can't get that, but it's pretty easy. Okay, so let's see how to use this to solve the least squares problem faster. So, imagine we've got this problem, we want So imagine we've got this problem. We want to find x such that a plus mu i. Yes, please. So it's an expectation. This is an expectation. Yeah, so we also have probability bounds. I prefer to show the bounded expectation. You can easily work out a probability bound using Markov's inequality. I'm surprised by the number of times the reviewers come back with exactly this. Okay, so yeah, so I guess the fact that it's an expectation doesn't bother me, and I find very good results in practice. Okay, so we're solving this regularized linear system. Why would we want to do such a thing? I'll bet all of you can think about applications from your own work why you'd want to do such a thing, which I'll get to on the same slides. But just to set some notation, the eigenvalues of A will be lambda 1, 3 eigen. The eigenvalues of A will be lambda 13n lambda n in decreasing order. The condition number is going to be kappa. I also want to write the regularized matrix A sub mu, which has an improved condition number compared to A. And I'll write mat dec of A to save the time given the matrix that requires. Why would you solve this? Here are some examples. Iteratively related new squares, kernel rate regression, a regularized Newtonian system, like the one we saw in Petrus's talk. Like the one we saw in Petrus's talk earlier today. Gaching processes comes up. I've seen it in an approximate cross-validation. So if you want to figure out what the sort of what would the classifier look like if you left out one data point, you want to evaluate that quickly. You can do that by approximating that problem as a linear system and solving it. Influence functions, hyperparameter optimization. So, actually, some sort of surprising applications of machine learning that are just this regularized linear system itself. Now, how do you solve a linear system? So, again, I don't need to tell this audience, right? You can use direct methods like Chalesky, you can use indirect methods like conjugate gradient, and conjugate gradient, of course, depends on the square root of the condition number. So that's a problem because, as you saw, these condition numbers are very large. Surprisingly, in the machine learning context, I've seen a shocking number of people solve a linear, a regularized linear. Solve a linear, a regularized linear inverse problem using a series expansion for the inverse, which is a really terrible, provably worse than Punjabi gradient and often unstable. But you can do it. What do they do on the theoretical side, or do they actually do numerical? I mean, some of these papers have numerics. Like Kohen Liang. Koen Liang has numerics. They might. I think that they're not. I don't remember if they have numerics. They might not. But I still think that a lot of his entrepreneurs. But I still think a lot is on Trenton. Anyway, oversight. It's fine. Ryan is not confused, I thought, but then Brian is a particular person. Okay, you can talk to him about using C chi. Yeah. Okay. But I actually think there may be something to the fact that these matrices are so ill-conditioned that sometimes it's easier for people to think of. Sometimes it's easier for people to think of how to improve the conditioning when they're looking at the series expansion rather than, you know, I mean, if you pass it to CG, it doesn't work, it's true, because it's badly conditioned. Okay. Okay. So how do we solve this? So let's imagine that we have our rank-es-no approximation. Now, one thing we might do is just plug in our isomo approximation, A hat, and solve this approximate linear system instead of the original linear system, right? That's a reasonable idea, first thing to try. The nice thing about thing to try. The nice thing about this is it's really fast to solve this system, right? Because you have an exact factorization of a hat, and that gives you a factorization of this whole thing. So this is the inverse. You can write it down and then you can apply it to a vector. That's great. On the other hand, so it works actually quite well as long as V is in the span of V, right, the subspace on. The subspace on which you've really approximated A. On the other hand, you've got no guarantees of that, right? Depending on the application, you may find that B is in that span or it's not. Actually, in a lot of these machine learning applications, B is in the span, it's provably in the span or close to the span. You should expect that if the data that you gathered is very, is the right data to predict the thing that you're trying to predict. If B is what you're trying to predict, you know, most of your data, your You know, most of your data, your homing, most of your features are kind of pointing in the v direction, right? It's in the top subspace. And then this works quite well immediately. Okay, but if not, in the worst case, to get high accuracy, you need to send a sketch size all the way to end, so it's not a useful one. Okay, so if you want to actually solve a linear system and prove that you solve it, you need to actually use conjugate gradient. So when we remind you about free condition conjugate gradient, which again hardly conditions on to get gradient, which again hardly need to with this audience. If you've got ax equals b, that's the same as putting a p on the left-hand side of both sides. Okay? And we'll do a change of variables in order to make this matrix symmetric again. And so now I see that I've got a new linear system in terms of a different matrix that has a different spectrum. Yeah, so this works well, as long as the condition number of this new matrix is much better than the previous condition number. And common ways to precondition are things like Jacobi preconditioning with the diagonal or incomplete Chules D. As far as sort of preconditioners, these are sort of the typical ones that you'd see in optimization. So sketch and precondition, right, is sort of the next step. And this is related to what you saw in Pekos' talk. And this is exactly what you saw here, which is that there's still this order. You saw here, which is that there's still this order. So, number one, you need to know a factorization of your matrix. You have to start with a factorization of your matrix, okay? And the complexity is going to be cubic in the inner dimension, or faster if you believe in fast matrix multiplication. Okay. So, but those are some conditions that aren't generally satisfied. So, you can't sort of do this with a sort of oracle axis or matrix, where you only have matrix vector products. Okay, so let's think about what would the best Lorentz preconditioner, and this is the picture I always have in mind. If this is the spectrum, and if I have a good approximation of the top S subspace of my matrix, then what I want to do is I just want to flatten that top S subspace. And suddenly, my condition number goes down from this big thing divided by this small thing to this much less. Small thing to this much less big thing divided by that small thing. And for the spectro that you saw at the beginning, this is going to be a huge improvement. So you can design a preconditioner that uses that information. And it's not too great of an intellectual leap to then, instead of plugging in the exact best rank S approximation, to plug in an approximate one that we get by sketching. So given a NISTRA approximation, I'm going to call this the NISTRA preconditioner, which is exactly the same as that off-grid preconditioner, except with my approximate. Preconditioner, except with my approximate top s of space. Okay. Again, you can apply the inverse of this mister and preconditioner efficiently so it functions easily as a precondition. Your precondition is going to depend on mu? It absolutely does depend on mu. It tends to work better the larger the regularization is. But we've taken regularization down to like 10 what do you mean it's 10. But if you want to solve the problem for new potentially you want to cross-validate or something, then the problem of gap is required. Yes. Well, so actually, a lot of the work, right, most of the work is forming the disturbance approximation. Okay. And then, yeah. So changing mu is no problem. Yeah, yeah. On the other hand, we're in the context of conjugate gradient, where we are going to revisit the matrix many, many times, right, at every iteration. So this looks in a way except for the new looks like uh something like the spectrum recognition. The spectral preconditioning. Yes. So if this is a partial spectrum. Yes, I believe it's a very classical idea. Yeah. Okay, so if you apply this to, you know, let's say a large machine learning problem, this is one that we've made even larger by using a random features transformation. We take the features, we compute random combinations of them, those turn into the sort of big features. Into the sort of pictures. This also creates a dense matrix, which is the setting where these methods are particularly useful. So if you look at this, if you just did a direct method QR, you'd get down to machine precision, but it would take a long time. If you did normal conjugate gradient, right, you could break it down linearly. The COBI preconditioning actually makes things worse. But in this case, preconditioning, it takes a little time to compute the sketch, and then much ahead. Um quite too many. Okay. Um so you can prove that um this preconditioner does in fact control the condition number. Um so I guess that that might be a little bit new compared to the classic stuff. The bounds on the quality of the randomized preconditioner might be a little bit new compared to the classical results. So you can show the condition under depends on things you might expect, like and this is nice that these are all Might expect, like, and this is nice that these are all actually easy to measure and tell on posteriori how well you're doing. So, the error in your approximation, which is actually easy to get access to using just a sketch that's one larger, the regularizer, and then the estimated last S icon. So, you can sort of stare at this and say, well, as long as you have a large enough sketch size, you can get. Large enough sketch size, you can get that last eigenvalue down below the strength of the regularizer, and you can control the error. And so you can make sure that this condition number, precondition condition number is less than 30. Okay, so find you a great thing to converge extremely. How do you get that? In practice, we often choose a fixed sketch size, which works really well for these most machine learning problems. In this paper, with In this paper with Zach Carring Della, Vienna Chopp, we have an adaptive method as well, where we increase the sketch size until the estimated error is small enough. When we try this in the optimization algorithms, it doesn't help. It's not worth it. The fixed sketch size is already enough. And we can also get an operatory bound. We need to make sure this depends on the effective dimension. Okay, so how well does this work? So, you know, on So on the right data sets, on data sets with the right properties, it can outperform these other sort of sketch and precondition methods by a factor of 10 or more. I'm not going to say that much more about that. Any questions here? Can I move on? Let me just show you one more time so we can focus on that. Yes. So, how do you know your eigenvalues? You have a decay that is. Well, you don't know in advance. I feel like I've gotten pretty used to which matrices will and which matrices won't. And basically, when I see a dense matrix from machine learning, I expect it will. The ones that really get me and really annoy me are the ones that come from large-scale mixed intercurlinear programs. Mixed into your linear programs. So those matrices don't necessarily pass back over K because the matrices come from constraints and the constraints are relating disparate sets of variables. If you could get rid of them, then probably you would have gotten rid of them at the pre-solve set. And so those matrices, among the matrices that I hang out with, those are the ones that bother me the most. Do you cause those matrices? Do you process those matrices at all? I mean those behave very differently, for example, you should like you mean you mean centering. You mean centering those mean centering is just a ring function, so it's not going to make a big difference. That might remove the link. So in some of the data sets that you saw, the pop single variant is extremely large, mean center. Yeah, then it helps a little bit. Yeah, that's a good way to do it. I'm just wondering if you these are off the shuffle, your local name. Yeah, these are off the shuffle. Yeah. Yeah. Yeah, yeah, if they correspond to the smaller singular values, so this still helps because it sort of tunes down the effect. Of tunes down the effect of the top singular subspace, right? So it's sort of easier to resolve what's going on there. But of course, it takes more iteration after get down to information that's in the bottom of the matrix. I have a question related to this, actually. I was wondering whether you have observed the convergence history of your recognition CG in terms of residual norm, like to say. See how the residual decays as number of iterations grow, which should. I know we looked at that, but we looked at that a long time ago. Because I'm afraid, that's why I'm asking. I'm afraid, I wonder how this goes, whether you have a rapid decay, because you are capturing the important part of the spectrum with the spectrum of computers. You have a rapid decay at the beginning and then you slow down. Yes. And then you slow down. Yes. Because you have no information on the retrospective. So you have to gain it through the iteration. So if your stopping criteria is the stopping tolerance, it's large enough. It's above that. Yeah, that's large enough that you won't see this drag. Yeah, so here the tolerance, so on some of these, it's 20 minus 10. So I think that's still... Oh, I missed 10 minus 3 now. Yeah, yeah, yeah, yeah. On the largest data sets, yeah, we're using a pretty moderate data. That's yeah, we're using a pretty moderate tolerance, which is typical of machine learning. I mean, obviously, it won't apply in numerical analysis. Yeah, and these things are images, but images, if it's fashionable, it's not the numbers, right? This is the extended MNIST. This is like with I think rotations and colors and stuff. Okay, yeah. So probably it's nastier than the usual. Yeah, it's not fashion though. Yeah, it's not fashion though. Okay, okay, I see. A little bit faster than fashion. Oh, even faster, okay. Just because the rank for the. Yeah, but I mean, of course, you can't do magic, right? But you can do magic when what we're slowing people down is the top subspace, right? Which frequently that is the problem. But yeah, you can. Okay, thank you. Okay. But I really think this is a sort of a think this is a um sort of a big difference between sort of what sort of what happens in sort of most of machine learning um you know versus what you see in um uh you know in in physical systems and in sort of um you know discretizations of continuous problems like the where the structure of the matrices looks very different but also I think many of the machine learning problems are actually just easier but people aren't even bothering okay Okay. But the issue of tolerance is certainly something that is not a problem. So here's the standard response rather than a physical phenomenon. Yeah, the standard response in machine learning is that you've got statistical error to deal with, right? You've got sampling error to deal with. And as soon as the optimization precision is lower than the noise that comes from statistical sampling, then there's no point optimizing further. And for most of these problems, you expect that that statistic. Of these problems, you expect that that statistical rate is sort of at the level of tolerance programmes. Yeah, particularly in finance, they often seem interested in optimizing further, but there's no point. Interesting issue with that. Yeah, the point is that they're different. You know, like for a given data set, how do you quantify that? How do you know what the statistical voice floor is? That's challenging, right? Voice floors, that's a challenge, right? You know, it's easy to assume in theory, but access to our practice is true. Okay. No, I love the conversation. This is what makes it fun, right? Okay, composite optimization problems. Okay, so here we're minimizing a loss function that depends on some data matrix plus a regularizer. I'm assuming my loss is smooth. My regularizer has a prox that's Regularizer has a prox that's easy to compute. So, a closed-form solution to minimize r of x plus one-half x minus y squared. So, for example, for the L1 norm, prox is the soft regression operator. Great. Here's an example. The lasso problem, minimize a least squares loss plus one normal health. And this gamma ran through the regularization. Regularized logistic regression and support vector machine also fit into the same framework, and a bunch of other problems like portfolio optimization control problems, you can cast the same. Okay, so my base algorithm is going to be the alternating direction method multipliers, which has been super popular over the last 10 years, largely because it's sort of, you know, it's sort of easy to design from problems. But, you know, it's not what anyone would say is. You know, it's not what anyone would say is like the state-of-the-art solver for any particular problem. But here's how it goes: you say, you write down your augmented Lagrangian for your problem. So this is the augmented Lagrangian term. And you minimize the augmented Lagrangian with respect to the variable x. And then you've introduced this sort of other variable that goes into the regularizer of them. They're connected via the augmented Lagrangian. You minimize the augmented Lagrangian with respect to this other variable z. This is what gives you a problem that you solve. You have a problem that you solve using prox. So, this step is a prox, but this step involves the data matrix A. That's the hard step. That's the step that takes the most time. Okay, so what do we do about this? So, here's one idea. We approximate the X minimization using a linear system. And we solve that to moderate tolerance with NIST and PCTA level. And there's And there's so if you just use sort of approximate sub-problem solves, there's existing theory for that by Extend and Bertzikis from ICNU2. But if you first approximated the problem as a linear system, there was an existing theory, so we've got Bertis theory in a paper that was called GAM. So here's what that looks like. You approximate your loss function using a quadratic model. Using a quadratic model. The quadratic model involves the data matrix comes out, and then you've got the Hessian of the loss. With this approximation, solving, minimizing this approximation, plus the quadratic regularizer becomes a linear system. And so now you see this regularized linear system in here, which has this nice factorization. So you can use the Neespin preconditioner, or you can use the Sketching Free Conditional methods. The sketch and frequential methods. Okay, plug that into the algorithm, we get our new algorithm, then we can say, how well does it work? Well, let's compare it to a bunch of state-of-the-art methods for these particular problems, for solving lasso, for solving distant progression, for solving a sport vector machine. And we're solving them to, we're using the same stopping criteria and parameter settings as the standard theoretical for each of these problems. We're using a constant sketch size, so that's Constant sketch size, so nothing adaptive, just sketch size 30. And we are using a random feature map, so we're generating more features, making our problem less dense and harder. It also makes them have better solutions, more features. So on LASA, what happens, you can see NICE ADMM is, so as a function of time, how much progress have you made on the stopping criterion? And for a pretty sort of For a pretty sort of long amount of time, I see any members that are a lead. At a certain point, SSNAL, which is a second-order semi-smoothing monoclement with the bronze method, catches up. But usually people stop around, oh, actually, only minus one or only minus two. So, yeah, so it's sort of not worth going to the high order. Okay, so yes, if you stop at a not very young If you stop at a not very impressive precision, then the sketch sketch method goes much faster. Okay. Similar results for LN regularized logistic regression. And here, we started to be, I mean, we were actually kind of surprised, because ADMM is not a statement here at solver. And yet you plug in this thing that speeds up the X minimization, and suddenly you are competitive with statement here at solvers for this problem. Saga is a stochastic optimization method that is supposed to scale really well if you're That is supposed to scale really well for very large problems, but it doesn't do any preconditioning. And that's a big problem. And here's results for support vector machines. LibSVM is a library written in C that's really designed to exploit the structure of SVM problems. And Mexican is able to feed it on these dense problems. Okay. We have 11 minutes left. We say, as an optimizer, right, a talk, what are you trying to do? You're trying to maximize how much the audience learns, subject to the constraint of stopping at the right time. No stop at the right time, infeasible. No points. So, sketchy as she, let me tell you a little bit about the stochastic optimization of this. Okay, so, new problem class. Before we were in the setting where we could access the whole data matrix, say we could sketch the data matrix, we could do precognition conjugate gradient, right? We were going to revisit the whole matrix many times. And now we're going to imagine we're in much larger problem data settings where it is not reasonable to complete a full task to even compute this loss. So instead, our main primitive is going to be sub-sampling the loss. Loss. Okay, so what's the most standard method here? This is probably, okay, a small variant of this sum, atom, is probably, you know, using, you know, anyway, a sizable fraction of all the compute that's being used in the world right now is going to Adam and STD. So it's an important place where if you could speed things up, you could save a lot of greenhouse gases. So what does So, what does the stochastic gradient method look like? You compute a thing, g, that looks kind of like the gradient, and it is the gradient in expectation. And then you go in the direction of negative g. Choosing a step size eta is always very difficult, and often it's sort of monitored by hand, and you look for when your loss starts to diverge, and then you say, oh, that was too large, I'll turn it down, you go back to a checkpoint, you look for when your loss is sort of not making. You look for when your loss is sort of not making much progress. You say, okay, I better turn down ADA. But it's actually often done by hand or multiple runs because sometimes for non-convex problems, you get to different level minima. Depending on how you set ADA, your schedule for ADA, it's actually a huge pain. Okay, so what would we do as people who know about preconditioning, we'd want to precondition the system, right? So a stochastic quasi-mutant method would look like this. A stochastic quasi-Newton method would look like this, right? You just put an H inverse in front of the G, where H is somehow an approximation for your Hedgehog, right? It's like a very natural thing to do. You would expect faster convergence. You would expect it to be more robust and converge better for real-position problems. You expect it to be much easier to use hyperparameters like the learning rate. And the only con is that this Hessian is really expensive to compute and really expensive to apply. What did we do? Okay. Oh, first I say, Okay, oh, first question: what do we not do? Causing muting methods like LBFGS. So you might expect that you could do something like this, and the answer is if you try doing it with stochastic radiance, it fails pretty badly. Because the secant approximation that it's using to build its approximation to the curvature is just wacky if you've got noise in the gradients. So it really doesn't work well. There is a stochastic variant LGFPS. Fantastic variant LGFPS by Michael Jordan's group, but I'll show you my paraccines to do later. Okay, so how do you approximate the Hessian? So here are some principles, right? It better be from a data subsample because you're not allowed to capture whole data. It better be from stale data because approximating a Hessian is a little bit expensive and you don't want to do it all the time. You know, you don't want to do it at every stochastic rating filteration. You could use a secant condition. I told you why that's a bad idea if you're doing. Condition, I told you why that's a bad idea if you're doing stochastic. You could use a diagonal approximation, which is a pretty common strategic of the atom, which is you know, maybe the main solvent that's used for deep learning is using a diagonal approximation. You could use a block diagonal coniker approximation. This has gotten some popularity in deep learning, though I don't think it's sort of the main method used anywhere. And you could do it by low-rank approximation, which is what I want to tell you about today, and that's sort of a method. And that sort of method that we called sketchy SPD and then some others. So, sub-sampling the Hessian, right? So, here's the sub-sampled loss. So, instead of the average over all the data points, I take a random subset S and I compute the sub, I write down the sub-sampled loss. The Hessian of the subsampled loss, I can write down like that, but I better not compute it like that. A sub-sampled Newton method does this. Sample Newton method does this. It uses the inverse of this regularized Hessian, right? So the inverse of regularized Hessian applied to a vector. That's the structure of the computation that we need to be able to do, and we need to be able to be fast. And it's not so bad to add a little regularization here. The original stochastic gradient method is basically taking Is basically taking this to zero and the regularization is everything. So, certainly a little bit of regularization will hurt us. So, how do you access this Hessian? Here are two ideas. One is, if you know the objective is a generalized linear model, like logistic regression or least squares, then you can actually write down that the Hessian of this subsampled loss has. That the Hessian of this subsampled loss has this factored form. And this is sort of similar to what you saw in Pegasus's talk. So you can factor this regularized inverse session in time that's sort of cubic in the number of points in your subsample, which is usually on the order of 500 or 2,000, something like this. So it's doable, it's not too bad. And you can apply it in order s squared time. S squared 10. Let's find. This is a good idea if A is sparse because it preserves sparsity. If A is not sparse, or if you want to apply this to losses that you don't have explicit access to, maybe you only have access through automatic differentiation, which is like the primitive of deep learning. You could still get a preconditioner. So I think that's actually maybe the most exciting idea here. So here's how, how do you compare? Here's how. How do you compute a Hessian vector product? Right? In order to get a sketch, in order to form this preconditioner, in order to form the regularized inverse using Woodbury Farm. So I think the only part that you guys don't already, some of you don't already understand, is how do you get a hashing vector product using automatic differentiation. So this is a cool idea. So here's my gradient of my mini-backed loss. I'm going to write that as. I'm going to write that as tilde g of double. That's the gradient of the mini vector loss. I can define the Hessian vector product. This is the Hessian of the mini batchlocks applied to a vector. That's the same as the gradient of the mini-batch loss dotted with that same vector. So look, I have now written down my Hessian vector product in terms of the gradient of some function, which I can compute using automatic differentiation. Which I can compute using automatic differentiation. This is called Perlmutter's trick, though I did not note the name until a year or two after I was talking about it. The cost of this is just two passes of automatic differentiation, which is the same as four function evaluations. So it's kind of cool. You can get hash and vector products. That is acceptable. And there are even some papers by a team at Google that's sort of working on making this work on hard. On making this work on hardware at scale, and they got it working so that it's actually dual. Okay, so you can use that to develop preconditioners for just about any stochastic optimization method. So we're working on this paper that's, I think, going to be posted on archive sometime this week, that does this for a bunch of different algorithms. So, SGD, we already have a paper up on a precondition version of SGD. SGD. But there are sort of ones that are better, especially for convex optimization, that get variance reduction, so they actually converge to the optimum, unlike SQD with a constant step size, and they get acceleration. So that's getting calculation. Of course, if you accelerate, you get less stable, and you're going to see that in the numerical instance. So there are a bunch of other algorithms that are trying to address the same setting. Trying to address the same setting compared to them. We're the only ones that we know of that have theory for the fact that we're allowed to use a preconditioner that we estimated at another point and have it work even as we move farther away from that point. So we're estimating this preconditioner maybe once every epoch, but like once every pass through the entire data set. We're just taking a little bit of time, 2,000 samples, estimating the preconditioner, and then continuing on. Whereas most of the theory for the other ones, Whereas most of the theory for the other ones assumes that you're estimating precondition every iteration, which increases the computation by order 2,000, which is not practical. Okay. Okay, I've got like two minutes left, so I don't know. I think I'm just going to show you some figures. So in the figures, what I want you to see is the original algorithms, these base algorithms, SBRG, Saga, and Capuusha. SBRG, Saga, and Capucia are the dashed lines. The stuff that we've been working on is the solid lines, and we've got the two different preconditioners I told you about, and three different methods, Saga, SBRG, and Capucha. And what you'll see is generally the dashed lines are not reaching the same text accuracy as the solid ones. So we're getting better forms, or when we're minimizing loss, the solid lines are doing better, the dashed lines are doing worse. This is when the competitors are not too increasingly competitive. The competitors are not tuned. We're using their default parameters. If you tune the parameters, of course, tuning the parameters is really expensive. It often requires 10 runs, 20 runs. So it slows you down tremendously. But if you tune the parameters, you do get more similar performance, but you're seeing the fact that the precondition methods are more robust. You're not getting these excursions down to very low values of the afters. Yeah, that's the main difference. And they're still doing a little bit better early on. It's sometimes even better, you know, all the way. Tune versions of these left algorithms. Yes, this is the URL data set, which is detecting malicious URLs. Yeah, Yelp and the ACS loop. Okay. So we can also apply this in a streaming setting where it's not feeding. Apply this in a streaming setting where it's not feasible to compute a fold rate. So that immediately means that we can't use SVRG and we can't use Cartier Shell. So SAGDA and SPD remain. And so you can look, and so you can find data sets that are badly conditioned. And when the data set is badly conditioned, then using the spreading dictionary allows you to get to a much higher accuracy than that. And you can do a more thorough comparison on a bunch of, you know, on 17 data sets. On 17 data sets, it showed that you solve a higher fraction of the data sets faster using these methods than using the baselines. I wondered if I would... Yes, yes, it's time. These are performance robots, yes. Yes. These are performance reflex. Okay, so I believe that's time, which means I don't get to tell you about the theory. So, but basically the theory, anyway. The theory, anyway, the theory says Newton's method works really well, and if you view Newton's method in the right way by sort of looking at this quadratic lower and upper and bound, instead of in the Hessian norm, look at it in the preconditioned norm, then what you lose is just a factor of the spectral error in your prediction. So, all of this would be very familiar to you guys, and I have the action on the slides if you want to look at it more. Okay, so it says you get linear convergence and the rate depends on what we're calling quadratic regularity ratio, which is sort of how much does the curvature of your function change from place to place? And the spectral error here, preconditional. Okay, so I'm going to end there. I'm going to tell you low-rate structure is everywhere in machine learning. It can be used to accelerate all sorts of different. Machine learning, you can use it to accelerate all sorts of different optimization problems. And here's all the papers that this talk is going to be. Thank you. Thank you. I have questions. Is the Jordan uh modified LDF the same one that I heard uh One that I heard Jorge Nosadal talk about where you're the standard difference and you just have to use the batch for the gradient at each end of the step before moving on to the next batch. I think that's correct. More questions? If anyone has a question on Zoom, you can unmute yourself and ask. If not, then let's go and have some conversations. Then let's go and have some coffee. We'll start back again at half past three. So thanks very much again. 