It's basically on functional data analysis and most of them methodology models. So I'm not going to give mathematical details rather than just explain a little bit what is the main ideas that I have developed. So I'm going to start with the definition of functional data for those who are not familiar with this context. Context. So basically, what happens here is that we are assuming that instead of having, for example, the usual time series where we have a sequence of real numbers, so we will have continuous functions. So we'll have a sequence of continuous functions like this one that is shown on the right side. And in fact, the assumption of this functional data analysis is a bit more general, and the assumption is that. And the assumption is that the data set, the observations are objects that have this characteristic that varies along a continuum. So this could be surface, this could be a continuous function, or this can be a more complex object, right? So for example, we have here this sequence of continuous functions that represents log mortality rates over the years. And also here we see on the middle, which is the Middle, which is the results of after pre-processing radar data, which basically are like spectrums, and also on the right hand, we have this satellite image, right? So, all of this can be considered as functional data in the sense that we can think that there's something that is continuous and various across the domain, in somehow, if you define the domain properly. Yes. Yes, oh right, so yes, yes. So the domain usually is going to be assumed uh compact. So the the this uh the data is defined in a dimensional space, Shilbert space, for example. Yeah, yeah, thank you. Um, right, uh, but uh, of course, uh, similarly, as in the classical statistical methodologies, uh, we We have like the data are observed across time, so we can have temporal depends, right? So we're going to have like a sequence of functions or sequence of images, like in this example. So at time t, we can have one image or one continuous function. Then at another time point, we have another versions and so on. So we're going to that's temporal dependent. So then my research focus. My research focused mostly on this context where we have temporal dependence, but assuming that at each time where we have that continuous function or that images that can be considered as a single object. Right. So, the simplest model in this context is the similar that we have in the time series model, which is Have in the time series model, which is the functional AR1 model. So, in this case, this model is defined like this one. So, we have that at this time point, this depends on the previous one, but in a way that this dependency is described through this b-variate function, right? So the and then we have this kind of functional white nose, which is useful like Gaussian process or let's say Brownian motion, things like that, that are independent across times, right? Times right, uh, so the idea here is that, uh, for example, if we want to, let's say, predict this point, so we can take into account all the information that is contained on the previous function by using all the values on the domain where that function is defined in a way that we have, let's say, the different coefficients that affects the future observation in this case, which is xn. In this case, which is xn, but it is leveraged or it is described through this b-variate function, right? So the main point here is estimate that b-variate function, right? And this is okay, so this is kind of the theoretical definition, but of course, in practice, we're not going to we won't be able to observe the continuous function, right? So what we will observe is like if the functional data is an image, so that means that we are going to observe like the metrics, right? Observe like the matrix, right? With the values of the pixels, and if it is a continuous function, so that means that we are going to observe a vector of that function. So, what will happen in practice is that usually this to be able to recover the continuous function that we want to estimate, that we want to analyze, we will use a basis representation. For example, the BS plane basis representation. So, this is done before doing the analysis of. For doing the analysis of the data, right? And there are many ways to do this. So, this can be assumed to be, like, for example, this coefficient can be assumed to be Gaussian distributed. One can use like BS plane basis function, Fourier basis functions, and included more complex basis function. But that will depend on the problem that is being addressed, right? So, I'm going to assume that in this talk, I'm going to assume that I have that continuous version of that. That I have that continuous version of that function, either using basis representation or other kinds of methods, but I am assuming that I have that continuous version of the data. Right. And one of the advantages of using the continuous version of instead of the discrete observation is that we can, in fact, it is not required to observe the data at different equal, let's say, Equal, let's say, grid points. Let's say we have the domain defined, so we could observe the data at the different time points. But because we are using that continuous version, we are estimating that observation. So, it doesn't really matter where at what point in the domain that function is observed. So, somehow it is described through that continuous function. So, the projects that I would like somehow to Somehow, to explain here in a brief way, it's this one. One is related with trend estimation and this functional time series. I'm going to call it functional time series. Another one is functional factor models. And also tell you and also somehow explain or show that functional data can be applied in a more complex when you say you're representing it as a continuous function of free data. function with free data. Are you interpolating the data with this function or are you fitting a yeah it's it's it's not an interpolation. I mean it could be an interpolation if that is what matters but that means that you have to consider for example as many basis as many data points you have but usually you will fix the number of basis functions across all time points so it's not really an interpolation I mean it could be an interpolation I mean, it could be an interpolation by assuming that you have also like noise around the function, but that depends. It's not like something that you should do it. Okay. Thanks. Right. So the first one, trend estimation. So I have decided to use this illustration because I think we can see it clearly that we have somehow a deterministic trend. So this is the log mortality rate in France. Rates in France. So we have that. So this data is over the years. So for each year, we have a continuous curve where the domain is the age. So we have from 0 to 100, for example. And it's not hard to believe that we have a deterministic trend over the years, right? So the model, for example, can be written like this one. So we have the data, the continuous curves. So then we have that the mean that changes over the year. The mean that changes over the years, right, which is the deterministic trend. And then we have this residual, which is going to be a functional time series. So it's going to be temporal dependence. So the way that we can do that, or the way that we can estimate deterministic trend is, again, we can use, for example, basis function. But in this case, if we consider that, if we think this data set like, let's say, like a surface, so we could. Let's say, like a surface, so we could fit a two-dimensional basis function, right? One on the dimension of the time and one on the direction of the domain, right? So we could somehow see it as a surface, and then the problem then is try to estimate that surface by assuming that we have correlated residuals over the years and also on the direction of the domain. So that's the challenge here. So, this is a very simple idea. So, we can fit a B-dimensional BS pline basis function, for example. So, it's the same idea. So, if we define a tensor product where in one direction is BS pline, in another direction is another BS plan. So, basically, we are saying that we are approximating that surface with polynomials on each rectangles in a way that we can have a smoothness property on the surface, right? And then we can then estimate the coefficients of this. Estimate the coefficients of this tensor product, for example, by using a penalized list square estimator where the penalty term can be the one that describes the smoothness property. So the specific penalty that we used on that paper is this one that controls the smoothness parameters individually on each direction over the time and on the direction of the domain, right? Because it is expected to have different. It is expected to have different properties of smoothness across data directions, right? So it's given, in fact, given the penalty, right? So if we have this component, so then the estimation is very easy. It's very straight away. So it's just a least square estimator, right? But the problem here is that we have dependent residuals, to say it in somehow. To say it in somehow, so we have dependence across time, and also because this is our continuous function, so also we have this dependency on the direction of the domain. So, we need to somehow carefully choose this smoothness parameters that will allow us to recover the true deterministic trend without affecting the dependency of the data. And the way that we did it was motivated by this paper because. This paper because dependency can be very complex, you will never know what kind of dependency you have on that data. So instead of trying kind of to work with a specific dependency structure, one strategy is to propose to estimate these parameters in a robust way that can take into consideration different dependency structures. So one way to do it is because we are using BS blinds to estimate the trend, so that can be seen as a mixed effect model. So then can Effect model so then can we can estimate the disk parameters in a use and restricted maximum likelihood estimator which is known that is robust on the different complex dependencies on the time series. So that is the main point here and by doing that and using that ADR we can we can somehow see and use that and to estimate the trend on the on this data and this is the result so this is the trend that we estimate and this is That we estimate, and this is the residual. And also, by doing that, we can also kind of robust on the outliers. In this case, we have the second war event, right? So we have a theoretical results on the paper. We can show that the way that we on doing this way, the estimation is consistency. Okay. So that's one approach. And I think this idea could be also. This idea could be also extended to this when the trend are. So instead of doing voxel by voxel, we could kind of somehow use this idea to kind of estimate the trend on the data in the context of the neuroscience. Now, another idea that I have worked on is functional. Now, let's forget about the trend estimation or the trend. Estimation or the trend component. Now we have only, let's say, the mean is zero. Now we have only the functional time series component, right? So, and we are going to assume that that functional time series component is driven by some finite factor components, right? Again, we have this continuous function, so we have that the data is observed over the time, so we have time dependence. So that means that this is going to be common for. That this is going to be common for all functions over the time, and what is going to change is the coefficient. So, the coefficient is going to kind of describe the dynamic over time, right? Now, the model that we studied in this case is that we're going to consider that in many scenarios, you will not have stationarity on the data. You will have non-stationarity as well. Or in some cases, you will have both components, but because Components, but because you will have this non-stationary component, that means that you will see that this data is going to be non-stationary, even though some of the components can be stationary, right? So, the idea was here to propose some methods to estimate the factor these functions in a way that we can have this both S scenarios, in a way that we can estimate the parameters in this complicated S scenario. In this complicated scenario, where we have both non-stationary component and also stationary component, right? And it is well known that this, when it comes to factor models, it's not when we try to estimate this latent functions, it's not unique. But the space that defines, if we assume that this function are orthogonal, for example, so that that function can be seen as a basis function. Be seen as a basis function. So the space that defines these functions are uniquely defined. So instead of estimating the functions, one could estimate that subspace that is defined by the stationarity component and also that is defined by the non-stationarity component. And once you have estimated that subspace, so then you could just define any basis function and then, for example, do a bary max rotation. Barimax rotation to be able to propose or to be able to have one possible estimator of that functions. And once you have that, so you can also given under 30 conditions, you can also recover the coefficient, which is the main key here if you want to do predictions or if you want to understand what is the dynamic over the time, for example, right? So that is the idea here: estimate is super space, and the way that we do it is. And the way that we do it is that with a little bit of algebra and math, it turns out that if we define an operator something like this that is that basically contains the covariance information at different lags, so then turns out that the kernel of this thing is related with the sub spaces that I am looking for. So it basically means that I just need to estimate this object and then This object and then find the eigenfunctions of that object of that objects to be able to find the superspace that I am looking for. Okay, so by doing that, for example, well, this project was motivated by from the environmental side. So it's very important for them to kind of know in this case, we have particle number size distributions. So at each time point, basically, what we have is a density function that represents. Density function that represents different sizes of the particle. So, and then we have data for several years. And the idea is that trying to know what are the main sources that contribute the total particle size distribution and how that source is codes across time. So, by doing the and by applying methodology, it was very interesting to find that there are like main four main sources that contribute. Four main sources that contribute in different ways across to the total particle size distribution. And this was very nice in the sense that instead of aggregating the data, which is usually done in the literature of the environmental size, so there is no need to aggregate the data, let's say the PM10. So you can just use the whole data by considering all different particle sizes by using the functional data approach and then recover the sources. Recover the sources and also estimate the trajectory of the contribution of sources. And lastly, I just wanted to mention also briefly that the idea of using the continuous approach, I really like it. I think it can model complex and large data sets and this can be used in a different way. So for example, I had this project with Rebecca. I had this project with Rebecca a couple of years ago, and the idea here was to kind of detect change points. So, the change point idea here is that we have data that measures activities of people. So, there are sensors installed in the houses. And when the, for example, the front door is open, so that activated, the back door is open, so then this. Back door is open, so then it is also activated. Or if people use the kitchen, they are, I don't know, doing some activities in the kitchen, so also that is activated. So it's a zero-one time series if you want to see it, but at random times, right? And the idea was to kind of detect changes across the activities across this. But of course, this is a little bit complicated because if you think about it, you have changes within days because morning involves different activities than It is than uh dinner time, lunch time, or things like that, or just reading a book, right? So, you will have also that changes within day, but that is something that is part of the routine. We don't want to detect that. Instead, we want to detect the changes that is across days, across that routines, right? So, the idea was instead of using like the usual approach, so we decided to use like thinking like each day will be. Like each day will be considered as a non-homogeneous Poisson process. So at the end, that means that the data is going to be a sequence of non-homogeneous Poisson processes. So that means that the problem now is to detect change points on the sequence of non-homogeneous Poisson processes. And the way that once the problem is formulated like this, so then what develops its extended usual ideas from change. Usual ideas from change points, right? Instead of having the data has real values, so now we have that the data is the whole non-homogeneous Poisson process at each time point, right? So then we are dealing with that kind of complicated processes. And then, of course, that this for each non-homogeneous Poisson process is described with the intensity function. So basically, we have the functional data can be intensity function can be considered as a functional data. function can be considered as a functional data, functional time series, if you want. So we have one intensity function for each time point, right? So then we can then detect changes based on these intensity functions. So then this is what I said, basically what we want to do is find a segmentation of the sequence of non-homogeneous Poisson processes. And the results are this one. For example, we have this is our data. Example, we have this is our data. So we found that we have three segments, and each segment corresponds to this intensity function that represents different activity or changes of the activities. Some of them shift like from more activities during the morning to more activities during the night, which can mean something related to the health conditions, for example. So I think that's what I wanted to show. Thank you. Thank you, Drell. Questions? It seems to address so many problems that let me just concentrate on the first slide. In the first question, have you looked at the residuals of the fit of the model? It's a very practical question. The reason I'm asking that question is my student is using the same data set, and it's a very interesting data set because you had sense, you know, two of them that are important for your data will be the world wars. So the thing is that the residuals there will be very different, like they should be, the model shouldn't. They should be the model, shouldn't be fitting well unless you truly take into account the different mortality caused by the world wars. Yeah, so the short answer is no. The focus here was more like trying to record the truth, deterministic trends, and that worked very well based on many simulations, complex scenarios. And we didn't really focus on And we didn't really focus on the data analysis like in this one. But yes, you are right. So we have some isolated, I would say, isolated outliers which should be taken into account. But I also think this method is not 100% designed for kind of outliers, but I would say it's robust somehow. But that means that I think other methods that are specifically designed for our should be work better. Yeah. Yeah, yeah, yeah, yeah. That's why I'm saying it's somehow also robust when we have. When when we have our liars, but um I don't know, it's not working, yeah, this one because basically what we are doing is that we are smalling and when we are smoothing, we are finding the optimal, let's say the optimal parameter that defines. That defines the smoothness across time and also across the domain. So, but we didn't really try under that different SMRs when we have our lives. Yeah, but it seems it also works. Because, I mean, it's not that difficult, I would say, to say it works because basically we are doing smoothly in a smart way. So the couple isolated. The couple isolated outliers should be a problem if it is isolated outliers. But if it is a different outlier, so of course you will have more, it's not, will be more difficult. I don't remember exactly. So there are, I remember there are like different events that is well known. One is the second war. Well known. One is the second war, one is the first one. Right, right, right. Yes, yes. Yes. Thanks for my start. I'm thinking it's easy to quite respond to what it is. I mean, there are different ways to do it. One way There are different ways to do it. One way is to do it on the coefficients, and yes, it's very easy to do it. Just use multivariate methods, for example, on the coefficients. Yeah, let me um yeah so I am assuming that we have a deterministic trend here. Of course, in practice, you will know what kind of determining what kind of trend you're going to have. You could have a stochastic trend, for example. A stochastic trend, for example, but here I am assuming I have that. So, in practice, you will be doing a hypothesis test to see if you have this kind of trends. Yeah. Yes, I would, yeah. I mean, it's it's just that. It's just for me, it's a deterministic function that is there, that is not. Yeah, it's the mean of the process that changes across time. 