A rare event. I'm coming over here. I like to have more space to express myself. So thank you to the organizers for putting together really a fantastic meeting. Thank you to all the participants. It's been a really stimulating and occasionally inspiring set of talks that I've seen so far. And this is actually my first time talking about this stuff, so I'm really eager to hear some feedback on your thoughts on all of this. Thoughts on all this. Okay, so let's get started. So, collaborations, acknowledgments, right at the start. So, the work I'm going to tell you about today was primarily done by Zhaolong Jiang, who is a grad student at Caltech, working under the supervision of Matt Thompson, a longtime collaborator of mine. The work I'm going to tell you about is up on the archive. Time willing, I'll tell you a brief vignette related about work done by an undergrad with Matt. Work done by an undergrad with Matt Audrey Huang, as well as undergrads working with me: Ben Sheldon, Brittany Liu, and Chun Yun Chen. And then funding sources are here on the bottom. So a broad goal that many of us have in sort of engineering technology is we want to intervene in systems that we're interested in. We want to drive them to some new desired state. And so, an example we've seen a couple times already in this. In this forum, we might have some embryonic stem cells that, in their natural state, are maintaining themselves in some relatively pluripotent capabilities, but we might want to drive them to actually differentiate into some neural progenitor cells. So we have some desired state we want to push some system to, and driving it away from perhaps its ordinary everyday operation. And so we've seen examples of this throughout. And so we've seen examples of this throughout this series so far. In order to figure out how we want to drive a system to some new desired state, we really want to be able to learn the interactions that govern that system so we have some insight into how to productively push on it. And it's okay that this is blurry because I don't actually want to dwell too much on the system in particular. But Elizabeth Reed gave some examples of this, I believe, a very essentially this regulatory system previously. Previously, but the basic idea is that we're faced with an inverse problem, right? We want to observe the behavior of the system that we're interested in, and from that we want to solve the inverse problem. We want to learn the interactions that produced in the forward direction that actually produced this behavior. So natural statistics may not always be very illuminating when we're trying to do this kind of exercise. So as we've heard, transitions that we're very interested in may be very rare events that are very hard. Maybe very rare events that are very hard to observe on the time scales we want. We may want to actually push systems of interest to states that were just not accessible in any meaningful sense on very, very long time scales. And so just observing systems in their natural state may not be as informative as we want. So a general strategy that many people employ is to perturb a system to learn about its behavior in ways that might be more efficient than just its natural statistics. Efficient than just its natural statistics. So, one thing that Matt and I have worked on together is using light-inducible activation of particular players in a regulatory network to actually dial up or down that disturbance, that perturbation, to learn about how systems can respond differently to different intensities of this perturbation, different durations, leading to different behaviors from that system. From that system. Michael Asoff, this is really very much in the spirit of what Michael was talking about earlier. So, what I'd like to talk about in this talk is to really think about a theoretical framework for how we go about devising perturbations that might be well suited to learn about a system's behavior. What can we say about perturbations that best improve our inference? How do they go about doing that? This is related to ideas from a field known as optimal experience. From a field known as optimal experimental design, and how can we combine essentially repeated rounds of inferring about our system in some imperfect way, designing perturbations to then push the system in ways that can be informative about learning in more detail about those systems' interactions, and repeating this cycle. So as a physicist, we're big fans of spherical cows in our research because these are systems we can actually do calculations for. Do calculations for, and so this time I will not be doing a spherical cow, but I'll be doing a spin circuit model instead to try and think about this general problem of designing perturbations. So let's get into this basic idea. So we've got the problem of inferring a spin network. So I have a system of spins, it has some set of connections, and I want to observe its behavior and learn about its properties. So the basic problem, right, is that. So, the basic problem, right, is that states of this spin network, spin configurations, are going to be occurring according to some probability and exponential of the energy of a given spin configuration. And we'll be looking at, in Ilya's language, the second-order Ising model for these configurations. So, we have the states of the spins are binary variables. They can be either Variables. They can be either up or down in the context of more biological networks. This could be a gene that's being expressed or is off. This could be a neuron that's firing or is silent. So we have a term that looks at the interactions of these different spins with each other. The parameter j here is parametrizing the interactions between those different spins. A positive j means the spins like to align. A negative j means they prefer to anti-align. And then we have And then we have a second term here, which is due to an applied field. In the work that I'm going to tell you about, we're going to assume that this applied field is known. It's something that we're applying. We're perturbing the system by inducing this field. Examples of this you can think of, maybe we can knock down particular genes, maybe we can optigenetically manipulate the activity of a neuron, for example, up or down. So we can, in principle, apply fields to push systems in different directions, and then the problem that we're facing is: how do we learn? We're facing is how do we learn these J's, which are the interactions governing the behavior of this system? Okay, so the basic problem: we have some system, we observe a succession of states of that system, so we observe different configurations of all the spins in this system, we make a set of the observations, and then our goal is we want to now infer a model of these interactions. The color might represent the sign, the width might represent the The width might represent the intensity of these interactions. And then our goal is: what can we say about fields that we can apply that will make this problem easier to do? Okay. And T is a measure of noise here? T is a measure of noise. That's right. So sorry. So T corresponds, beta is inverse temperature, right? So the higher this T, the greater spread over your different states. The greater spread over your different states you'll observe, a lower t, you'll be more focused in the ground states. So, this problem has a formal name, it's known as the inverse Ising problem. We're trying to learn these couplings, these interactions of the different components of our system from observations of the states of the system. Yeah. We're only trying to learn J, not the H. We're trying to learn J. We assume in this part of the talk that H is known. Okay. It's something we've chosen, and then we are trying to learn. Okay, so how are we going to structure how we go about thinking about what makes for a good J? So this is related to something that's come up in a couple of other talks already. I want to kind of break it down, hopefully, in a way that kind of gets across the idea. So one question you can ask is you can say, so essentially the question of inferring the interactions that govern your system comes down to how distinguishable are the probability distributions. The probability distributions over states of your system for different values of the parameters of your system. In this case, the different values of your interactions. So, one way to think about this is: what's the relative entropy or a KL divergence? These are identical concepts. How distinguishable are two probability distributions, one of which is parametrized by a set of parameters, theta, and one of which is parametrized by a set of parameters that differs by some small amount, delta theta? So that relative entropy. So that relative entropy, it's just defined as a sum over all the possible system states x, the probability under one distribution, and then the log ratio of the probability under the two different distributions. So you can write this as an expansion in this difference between your two different sets of parameters. You get a quadratic term and then you get higher order terms. This quadratic term, which again has come up now in several of our talks, is known as the Fisher information. Is known as the Fisher information. It's basically the expectation, that's what these angle brackets are over all the possible different spin configurations, and then the second derivative of the log of your probability, i.e. the curvature of your log likelihood. Okay, so the basic idea is, right, you're looking for the set of parameters that best explains your data in the sense of a likelihood. The curvature around that best value is going to tell you how sensitive. How sensitive the distributions are to changes in these parameters. So, when you have a large Fischer information, that's telling you that when you change the parameters of your model, it changes the distribution a lot. That means that your distributions are relatively distinguishable as you change your parameters, and therefore that makes inference relatively easy. You can think of this Fisher information. Why do we call it an information? It's basically telling you how informative is each of your measurements about the actual model parameters that you're trying to learn. That you're trying to learn. Okay, so what are the implications for inference? So there's something in statistics known as the Kramer-Rau bound. This tells you that if you have an unbiased estimator, an estimator that at least asymptotically does not have any biases in terms of the parameters that it infers, if you look at the sum of squared error over all your different parameters or the L2 error, we know that you need of order You need of order epsilon to the minus one, lambda to the minus one samples in order to estimate a particular eigenvector of your Fisher information matrix. In the spirit of Thomas's talk from yesterday, I'll call this an eigen coupling, right? It is a linear combination of all your different parameters, all your different coupling interactions. So if this eigenvector has an eigenvalue lambda, then you need a number of samples that scales inversely with that lambda to actually learn it with a given prediction. To actually learn it with a given precision. And we can also say that if I want to look at the sum of squared error over all the parameters in my model, I'm going to need something that goes as the trace of the inverse of that Fisher information matrix. And so on the basis of this, we're going to use trace of the inverse of our Fisher information matrix as one measure of the quality of how good a particular field is for making our inference easier. Okay, so to kind of summarize this up, it's easier to learn when you have a large Fischer information, when you have eigenvectors that have large eigenvalues, these correspond to what are called stiff modes as opposed to sloppy modes, or when you have a small trace of the inverse of your Fischer information matrix. Okay, for a spin network, bless you, the values of the components. The values of your components of your matrix are actually relatively simple in terms of moments of your distributions. And in the case where you're looking at the diagonal elements of your Fisher information matrix, they simplify even further just in terms of 1 minus the square of the correlations for each pair of spins. Okay, so this is giving you the first indication that inference is easier when your correlations are small in your system. Correlations make inference hard. Make inference hard. Okay, so let's look at the simplest system possible: a two-spin network. There's exactly one coupling coefficient we're trying to learn here. We have some value of j that's going to then generate these observations, and we can look at the Fisher information as a function of the fields that we apply to these two spins. Okay, and so again, with this intuition that correlation. So again, with this intuition that correlations make something, make it difficult to infer, if you think about the behavior of this spin network, when J is relatively large compared to the temperature, what you're going to see is you're going to see both spins up or you're going to see both spins down. Okay, so you know what the ground states are, but it's very hard to know exactly how strong that interaction is because you never see the excited states that actually tell you how unlikely are the excited states. So the intuition is that So the intuition is that if you go along the direction where your fields are in the same direction along this diagonal, you're just reinforcing those correlations. So that's going to be bad. That's not going to help you learn the interactions. If you apply opposite fields to the two different spins, you're essentially breaking apart those correlations, and you're making it easier to learn what the strength of that J actually is. Okay, so it turns out that the optimal field to apply is exactly this. Opposite spin, or sorry, This, opposite spin, or sorry, opposite fields to each of your two spins. And then you can plot along this different, along this contour here, you can actually look at what does this fissure information look like and what do the correlations look like. So with no field, right, you have relatively high correlations, relatively low Fissure information, and as you push to the optimal value of the intensity of these anti-aligned fields, you get Aligned fields, you get to a value of zero of your correlations, you just look like an ideal gas, and you maximize your Fischer information. Okay? Yeah. I guess this conclusion doesn't depend on the temperature. So it's going to depend on the ratio of J to your temperature, right? So in this case, T equals 1 and J equals 1, and that tells you this. But if you have a different ratio of those two, then the value of H where this band occurs. With H where this band occurs will change. But the argument about the correlations being zero is where the H star is. That's that will hold independently of the temperature, but exactly what is the magnitude of that H, at what field strength does that occur will change. Yeah. Okay. Okay. So one thing you can also look at is you can look at the trace of the inverse of the Fisher information. In this case, it's just a single coefficient. In this case, it's just a single coefficient, so it's just the inverse of that value. And you can see that without perturbing the system, when h equals zero, you see that the difficulty of learning that j component grows exponentially with the strength of this interaction. And that may be a little intuitive once you think about the fact that as J gets larger and larger, the correlations get stronger and stronger. It's harder to see any but the ground states. So it's hard to learn about how. To learn about how excited are those excited states. When you apply this perturbation, you basically make the difficulty of learning this interaction independent of the strength of the interaction. So you get a really nice scaling with J. Okay, so again, sort of the takeaway from the cartoon of a cartoon, the two-spin network, is that strong correlations mean you never observe excited states, which makes it You never observe excited states, which makes inference impossible. So, your goal is to disrupt correlations, basically, to make learning better. Okay, let's look at a slightly more complicated example. We've got a spin ring, right? So, we've got a set of spins that interact with nearest neighbors on either side, and you've got some periodic boundary conditions that lead to a geometry of a ring. In this case, let's assume all of the interactions are activating, are positive j, so all of the spins want to align. So again, your ground states are going to be everything up or everything down. Are going to be everything up or everything down, and in general, for relatively high values of j, it's going to be hard to observe any other states. Oops, wrong way. Okay, so the single best perturbation, it turns out, to learn the interactions of this system is the thing that I've drawn right here. You have pairs of up fields alternating with pairs of down fields, and what this is going to do is it's going to break up the correlation. Going to do is it's going to break up the correlations of the entire chain into little blocks of correlated spins. So here's the plot of the correlation, each of the spins against each of the other spins, and you see, whereas before, I haven't shown it, but we had very strong correlations across the whole system, we now have local correlations and relatively less correlations apart from those local pairs. Okay, now because the field affects the sampling. Affects the sampling in a nonlinear way, right? Because it goes into this energy, which then goes into the exponential of the probability. There's actually an advantage to applying a sequence of perturbations rather than just a single perturbation. So if you then ask, in addition to this single best perturbation, what is the next best perturbation I can add that in concert with this will improve my inference further? It is one where you have alternating signs of your fields all the way around your. Way around your room. So, why is the alternating not the best? I don't know, actually. That's a great question, and we've kind of thought about that a bit. For me, intuitively, you would think that would be best. But one thing I can say is that if you apply this alone, it doesn't break up all the correlations because you still have correlations between the alternating spins. The correlation structure is quite different than what I've shown for this example, but it turns out, again, that's something I've kind of pushed around a lot. Kind of pushed around a lot, and we've yet to come up with a really satisfactory thing. But if you have any thoughts, I would love to hear it. Because the first three times you showed it to me, I said, That's got to be mid. That can't be right. That's induced by the periodic boundary condition, you're saying. Because you didn't have that problem without the periodic boundary conditions. Well, without you, you're saying in the case of the pair of spins, or you're saying if I just have a chain without any periodic pair conditions, I would not have that problem. Conditions, I would not have that problem. But didn't you show in the previous example with a lattice that you don't have this problem? Well, but that was a, so I mean, that was just a case of two spins. I have exactly one coupling between them. I see. Right, so there's one parameter in that case. So two things have changed since that one. I've gotten more spins and I've got this periodic boundary condition. I think this problem still holds even without the mission of the spin. Do you know about the connection? Are you assuming it's? Like the connection? Are you assuming zero connection between those that aren't next to each other? In this case, I think in this case, in this example, I think he is assuming that he knows the topology and is just trying to learn the interactions. We've done also the calculation where you just have no clue and you're just trying to learn all of them. And you, it turns out it can be very difficult to realize that those cross terms are zero when you have these long range correlations. Well we'll get to an example that I think highlights that pretty well in a second. I think highlights that pretty well in a second. So, when you now put the two of these, is it the optimal perturbation to put yourself at the temperature where the correlation disappeared? We will talk about that time-wheeling at the end. So right now, we're assuming that you don't have a choice about the temperature. You have a choice about you can apply a set of fields to your individual spins that can vary between the spins. We'll talk when temperature is a perturbation as well. Okay, so when you now apply these two perturbations, So when you now apply these two perturbations, you now get something where again, I mean the correlation has to be one on the diagonal, but other than that, you've disrupted a lot of these correlations. Okay, so your intuition should be that this is going to be now more efficient in learning the interactions of your system. So is it just the sum of the two or is it the sum one half the other? So the idea is that I collect samples with this guy, I then collect samples with the other guy, and then I, because the fourth one, And then I, because the form of my Fischer information is in terms of the log probability, I can add those two data sets to give me the Fisher information of the composite. If I put them together, I would just have one perturbation that would be the sum of them, and it turns out that's not as good. With the same weight for both? In this case, we're assuming we have equal sampling times for each of them, and then we put them together. Okay, so you put them together, you much further disrupt. You much further disrupt the correlations on this chain. So, we can then use some of our ways of quantifying how well we are doing. This is just kind of the anecdote because we know that correlations are related to learning efficiency. So, when we look at the eigenvalues of the Fisher-information matrix, so again, this is telling you each eigenvector is going to represent some combination of your coupling coefficients. And this is telling you how easy it is to learn all eight of those, right? So, harder. Of those, right? So harder is down at the bottom, easier is at the top. We've rank-ordered them from the hardest to learn to the easiest. And what you can see that with no perturbation, it's relatively hard to learn any of these interactions. As I make this first perturbation, I then make several directions relatively easy to learn, but several are still relatively hard. And the combination of the two makes it so that all of my directions, i.e., all of the parameters of my model, become relatively accessible. Okay, and again, Accessible. Okay, and again, the intuition being that a given eigenvalue, you should think I need sort of order one over that number of observations to actually learn that particular eigenvector. Okay, another way to look at this is just to look at, so again, this is looking at how easy is each direction in parameter space to learn. This is now saying as a whole, how easy is it to learn all the parameters of my model. And what you see is that as a function of the interaction strength, as the strength gets larger, you again As the strength gets larger, you again see this characteristic exponential growth of the difficulty of doing this. And as we apply these perturbations, we bring down that difficulty significantly. Okay, so in particular, we make the difficulty of learning this relatively independent of this coupling string. Okay, so now let's look at something even more complicated. Let's now look at a quote-unquote modular network. So I have a bunch of positive interactions, activating interactions. Positive interactions, activating interactions within each of these modules, and then I have somewhat sparser but still not totally non-existent repression between the different modules. Okay, so we can now look at the eigenvalues. So this is now a much larger system, right? So the number of interactions, right, is going to go as the square of the number of different nodes in the system. So I've now got significantly more parameters to learn. Parameters to learn. With no perturbations, I have a very wide spread. This is reminiscent of what Ilya was showing last time in terms of the very large spread in terms of those eigenvalues. So, you know, one direction, one combination of my parameters might be easy to learn, but then it quickly drops off. And as I apply the two best perturbations in sequence to learn about this, or as I apply five perturbations, I can really bring this curve significantly up. Bring this curve significantly up such that all the directions become much more accessible. And again, five is already sounding like, well, that's a lot more than one, but if you think about all the different degrees of freedom in the system, it's still a relatively small subset in terms of ways that we're pushing on it. I have another confusion. To calculate the beneficiary information, don't you need to know the distribution? So right now, this is small enough, and we're sort of assuming that you know that for purposes. Assuming that you know that for purposes, I'll show you an example where you're estimating it just based on your empirical samples so far. But you're right. In order to calculate the friction information, you need to know what distribution am I averaging the curvature rule. Which means you know the jij already. So what I will show you is that you can essentially bootstrap your way up from a relatively imperfect knowledge of what that distribution is to basically find perturbations that will take you in a direction. Perturbations that will take you in a direction that will significantly improve how you're doing. But you're right, it's totally, you could say, well, wait a second, in order to calculate this at all, I would need to already have solved the problem. But it turns out you can get a reasonable enough estimate of what that information looks like to push you in good directions. Okay. Yeah? Okay. Good. Okay. So we see that. Let's look at the correlation structure that results from these different perturbations. So when you have no perturbations, so this is a frustrated system, right? So, this is a frustrated system, right? You have sort of three mutually repelling modules, right? So, there's no way to satisfy all of those repressions. So, I guess the simple story is that there's sort of relatively less repression between these two than this one. So, the easiest thing to do is to have these guys aligned and this guy anti-aligned. And so, you can see within a given module, you have very strong correlations, and then you have very strong anti-correlations. And so, you can imagine from this, this makes it very hard to resolve. Wait a second. This makes it very hard to resolve. Well, wait a second, there aren't actually positive interactions among all of these guys, right? Exactly, where are the positive interactions that are making these guys strongly correlated? That turns out to be the thing that's quite difficult to do. And then, likewise, from here, you can imagine it's very difficult to know exactly which ones repel each other versus which ones are just along for the ride because their buddies in the module are repelling. Okay, and so as we apply perturbations, so we apply two perturbations, and you can see here that we're now beginning to distinguish. Here, that we're now beginning to distinguish between what are the actual interactions and what are the ones that are just correlative. And then finally, when we get five perturbations, you can see we still haven't dispelled all of the correlation, but we've at least gotten it to where we've significantly damped it down and we're beginning to see a little more of the structure of these interactions. Okay? How do you find these perturbations? How do you find them? Okay, see, I haven't talked about that. So, I mean, there is. About that. So, I mean, there is a method, it's not fancy, in this case, the person doing some approximate gradient descent on this landscape. I guess our attitude is there's going to be some method for finding these things, and what we're trying to figure out is can we improve that through a smart choice of the field? You may have views on like a better or worse way to actually do that inference at the end of the day, but we're just trying to think about how can we this this element in this inference problem, how can we improve that? How can we improve that? Is this a a framework for online? Is it kind of an online method where you after you do an experiment, you analyze it? Everything I've done so far has not been, but in a slide or two I will show you an online method that is exactly that. So you collect some data, you design a perturbation, you collect more data, you design a perturbation, you collect more data. Yeah. But so far this has all been... You decided what I was doing. You just decide what it is, and then I'm just showing you that, you know, if you could pick very good perturbations. If you could pick very good perturbations, what kind of improvements could you get? And then I'll show you that you can, in fact, find good perturbations. Okay, so a couple other measures of this. So one is sort of a qualitative measure of how well are we doing. This is what we're calling edge prediction. So it's basically saying I do some inference of what all these interactions are, and then I look at what are the k largest magnitude interactions that I infer. Largest magnitude interactions that I infer, and I ask how do those align with the actual K edges in the network? Okay, so I'm seeing how many of those do I accurately predict are the strongest or the most repelling or the most activating. And so what you see here is that with no perturbations, you saturate very quickly at something like a 60% success in estimating the strongest edges. And with these successive perturbations, you very quickly get to where you're getting all the right edges in terms of the strongest ones. Edges in terms of the strongest ones. Another more quantitative measure is: are you getting the interaction strength right? So, what is actually the mean error in the coupling coefficients that you are measuring? And you can see here, here it's less of a case where you just immediately jump to the final answer, but you progressively get better and better at your estimates as the number of samples that you draw increases. And you can see that for the no perturbation case, it's a very slow learning. Perturbation case, it's a very slow learning process, and for as you add these perturbations, you greatly increase how quickly you improve in your estimates of these interactions. Okay, and then just to show you the actual values that are inferred, so this is the ground truth, this is just the matrix form of all of these interactions here. And as you, with no perturbations, again, the intuition is, I know that there are these modules, but it's much harder to know exactly. But it's much harder to know exactly what are the interactions that are driving those modules to align with each other. And as I progressively add perturbations, you can see that I begin to resolve the structure within each of these modules. And by the end, which I've conveniently put right below the ground truth to bias your visual perception, you can see that I'm essentially reproducing exactly the ground truth. Okay, so as a couple questions have pointed out, okay, so there's one. Okay, so there's one more example. That was one network. You can do this with lots of random modular networks, and the basic conclusions hold. You can look at 8-node, 12-node, 16-node networks with no perturbations, with a few, and with lots of perturbations, and you can see these trends are very consistent. Over here, you can look at this edge prediction again, quite consistent, and you can look at the mean error that you get as you go from five samples up to five million samples. 5 million samples. Again, a consistent picture. There's some interesting blips right here in terms of why this is doing worse at very, very few samples. I don't have a good answer for that. So one takeaway from all of this is that applying perturbations is much more effective than just sampling for longer. If you sort of look at, if I have a mean error of this and I now want to sample for a million times longer, I'm going to get to here. But if I can do a smart perturbation, then I can really quickly improve my inference. Improve my inference. Okay, so now let's go to the case that a couple people have been asking about. So I now have a case where I just start, I just, I observe a network, I have no clue what the topology is, I have no idea about what are the perturbations I should be designing for this. So we've done 50 different random networks, just essentially the Sherrington-Kirkpatrick model made a bit sparse. So we just pick random networks. So, we just pick random values for all of our coupling coefficients, and then we prune the ones that are relatively small. So, we just have not every single interaction in place. And then we look at, we start collecting data from some initial sampling of data. We say, aha, like let's try and figure out a good direction to push the system, then collect more data, then do another round of inference, et cetera, and keep on iterating this. Okay, so what you can see is that the mean. Okay, so what you can see is that the mean error in our prediction goes as follows. So if I do one round of sampling, then I have a similar error between the two different methods. But as I start designing perturbations and then sampling using this pulled-on system, if you will, we can significantly decrease the mean error in the inference that we're doing. If you look at this edge prediction, how well do we actually How well do we actually identify which are the strongest edges that are driving the behavior of the system? Again, with no perturbation, you get a very, very slight perhaps increase over time, but you have a much greater increase if you're doing this in a smart way. And you are, at each step, designing a perturbation that you think, given your imperfect knowledge of what the actual distribution is, should make it better for you. How many steps per perturbation? In this case, In this case, I think it's like a couple thousand or something like that samples. So it's not nothing, but it's also not, you know, trillions or so far. Is that the next step? Are you using the maximum likelihood or a posterior from the previous step? So we are using a posterior from the previous step. Yeah, that's right. And there's a prior, if you will, in that we're regularizing this, that we don't want to get crazy large values, right? We've got some L2 norms that we'll just. Maybe that's an LTO that irregularized, but yeah. Minimizing the inverse feature information. Expected value of the inverse visual information. Yes, that's right, that's right. Using all the accumulated data up to that point, plus our estimate of what we'll get from that extra. One thing that I had no particular expectations on, but an interesting thing that we found in the data, if you look at as a function of what is the initial minimum eigenvalue. Initial minimal eigenvalue, so in other words, what is the single direction that's hardest to learn in the entire system? What you see is that if you don't do any perturbations, you see that over time, or sorry, sorry, it's relatively independent of that difficulty of the hardest degree of freedom of your system. It doesn't really change very much. But in fact, what you see is that with this perturbation method, in some sense, for problems whose hardest degree of freedom is relatively easy, that's where you can. Is relatively easy, and that's where you can really make a lot of advance on this. So that I don't know exactly what to make of that just yet, but it was an interesting trend that we interesting to us that we picked up. Is that moving around over each step now? That is a good question. I don't know the answer to that. So, in other words, does your perturbation sort of move? Perturbation, you sort of mop up that one, and then you're left with kind of the one that you didn't deal with. I would suspect there's not going to be a general trend for that, would just be my guess. That sort of because you're trying to, the thing that we're explicitly trying to optimize is this trace of the inverse of the fission. So we're trying to optimize the sum of squared errors on everything. We're not sort of saying, but you could imagine a different application where that's what you care about. Or you could imagine an application that says, all I care about is the best eigenvector, and I really want to nail that one down. And so that would lead to. I really want to nail that one down, and so that would lead to a different optimization. The original value spread was so big that the trace is like super dominated by that large one. That's a fair point. That's a fair point. Yeah. Yeah. So I guess, yeah, you could maybe perhaps when you indeed have this spread and there's one that's well separated, you could imagine that's a basically push and that's a good point. Three minutes? Okay, good. Okay, so I'll tell you very briefly about this. About this second piece, what happens if you perturb temperature instead? It's a little harder to know exactly how you might experimentally do this, though the general idea of injecting noise into a system is not totally foreign to us at this series. So let's look at one simple example. Again, a modular network. I have three spins that like to be next to each other or align. I have three spins that like to align, and then they have one repulsion between them. And what you can do is you can look. And what you can do is you can look at this is just to visualize this idea of this curvature of the likelihood, right? So when I have a very low temperature and I look at the likelihood of three different of these coefficients, so this is the one that says what is the interaction between the two modules. This is someone that says, you know, how happy are these guys? And then two to five is saying, what's the interaction between these guys that actually don't interact? And what you can see is that at a very low temperature, all you know is you know that there's Know is you know that there's something repelling between these. You have no clue how strong it is, right? Because at low temperature, all you see is these two modules anti-aligned. And likewise, for these other two parameters, you know kind of the direction of that coefficient, but you have no clue about the sign. If you go to a very high temperature, you're too hot, basically, right? Because entropy wins. You just have no clue what the actual interactions are governing your system. And so, of course, if you have something that's too hot, you have something that's frozen out. If you have something that's too hot, you have something that's frozen out all the degrees of freedom, then you've got your Goldilocks state in the middle where you actually have significant curvature at your optimum. That actually shows you just graphically why you should expect this inference to be relatively efficient. One example, the 20-spin ring, right? So I now have 20 spins on a ring, and I can dial up or down the temperature at which I sample in order to do inference. I do this estimation of the Fisher information matrix and its inverse of its trace. Inverse of its trace, sorry, trace of the inverse. I find the maximal value and I estimate that's going to be the best place to do inference. And when I look at the error, I get indeed that's where that inference is done the best. And then again, one online example. I have some horrifically, to my eyes, complicated network, though, again, there's definitely some structure there. This is some estimated network from some brain circuit. I couldn't. Marine circuit. I couldn't tell you more than that at the moment. And so, what I do again is I do some sampling at different temperatures to estimate what this Fisher information matrix or Fisher information looks like. I then can do sampling at either of these two temperatures, and what I find is that my inference error is much, much, much lower at something that's a factor of 10 higher in temperature than what I might, without any further thought, do. And your inference in terms of the structure is much improved, obviously. Okay. Obviously. Okay, so caveats, which can also be rephrased as future directions. So we've talked about this idea of manipulating temperature. In practice, you probably have more limited control than what we have in mind, though it seems like every year brings significantly enhanced capabilities in that area. So what if the strength of the fields are constrained? What if you can only apply a sparse number of fields? What if you have imprecise control over exactly what those fields are? Here we've assumed, even in the online learning cases, Even in the online learning cases, that we had the model that actually generated the data, right? We didn't know what those coefficients were, but we had the right model. What happens if your model is wrong? And then, of course, everything here is very much an equilibrium model. What happens when dynamics comes in? So, my more immediate future directions this afternoon I'm going hiking and I'm throwing the gauntlet open to anyone else who wants to come. Sundance Canyon should be, I mean, where the sun dances. What more do you need to know? And then, or the alternative is to go up Sulphur Mountain, so come talk to me after the session if you are interested. To me after the session, if you are interested. And then finally, can I get out of here? Yes, conclusions. Optimal perturbations can reduce the sampling complexity, the number of samples you need for a given precision significantly. Helpful perturbations, they function by disrupting correlations in your system. Active learning can, online, help you find productive directions to push your system. And finally, injecting noise can actually improve inference. Okay, thank you very much for your time. Thanks. So, um what I'm thinking is it if it's so hard to learn some of those parameters, right? So you have to really invent very, very creative derivations to actually learn those parameters. Is it worth it, right? So, fundamentally, the network is going to be described by its state space, and there's going to be a bunch of attractors, a bunch of attract There's going to be a bunch of attractors, a bunch of local points in the landscape, right? And so do I really need to know all of these interactions to know the landscape, right? And maybe philosophically, why is this a goal? So I think it depends on the application you have in mind. And so, I mean, one thing we are thinking about is we would like... So generally, we're interested in can we enhance transitions which maybe are already observed, like pushing a Maybe are already observed, like pushing a stem cell into a particular differentiation pathway. But more pie in the sky, we'd like to be able to push stem cells to make different kinds of cells that they've never made before. So the idea is that we want to know about actually the interactions which govern very rare states in principle, which may lead to your concern from the last talk, perhaps. But the idea is you could imagine guiding this process by saying, here are the desired end states I would like to be able to achieve, and I want to know. Able to achieve, and I want to know how to actually, like, you know, I want to derive a new set of Yamanaka factors that's going to achieve some new outcome. How do I push on a system in the learning phase? How do I sort of in a reinforcement learning context think about how do I manipulate my system so that I can learn how well to get it to that state? Here we're thinking in terms of: I want to be able to characterize everything about the system, and you're right, you might not care about that. If you want to push it, maybe the question should be: which parameter should I find out? It's complete derivation should. Should they find out what prohibition should they apply in order to know whether it's pushable to that state or not, whether that's probations to do that? Yeah, but if it's in sort of crazy land where you've never seen the system ever sample, there's going to be some way to think about how do I intelligently enhance the sampling towards that state just to even get in that neighborhood in the first place, I guess is my thought. But I agree, the application is going to determine how you guide this kind of search. That's just a different not the output. That's just a different optimality criteria. Instead of doing that trace optimality criteria, you get the serious prediction error on some new condition. Optimize optimality. Agreed. I guess I'm wondering if, do you think that, let's say that the perturbations were difficult to achieve, exactly those perturbations are optimal perturbations experimentally, but do you think there would be a way to map these perturbations onto something that might be similarly achieved naturally? Like, I don't know, maybe like interpretation. Naturally, like I don't know, maybe like in different tissue types or something, somehow the natural system can to some degree mimic perturbation. That I mean, so in the sense of constrained options for how you can go about driving a system, that's something we're thinking about right now. But I welcome any ideas about how you would think to relatively simply approach that. I mean, it seems like that's a compelling problem, but I don't know how to kind of do that in a simple, general way right now. But yeah, absolutely. Away right now. But yeah, absolutely. You can only push on some subset of the nodes, or perhaps, you know, you can't just give a field of a thousand, you can only give fields of two, or you can only do fields of a thousand because you can only knock out or you can only knock in particular functions. You could imagine all kinds of those constraints going into your problem. To me, that's more like that's going to influence some of the nitty-gritty that I said nothing about in terms of how you actually do this optimization, but I suspect the general ideas of one. The general ideas of wanting to disrupt correlations and advantages to having successive rounds of different kinds of perturbations will still hold. Would do, I guess. All right, last question. So, I guess I have a remark. So, it feels to me that this inverse kind of Ising model inference is sort of philosophically misunderstanding what MaxEnt is about. MaxEnt is about that you make certain observations on a system and then you write down the Write down the least biased, least assuming models that can reproduce those to predict sort of the least structured model, to make other predictions about the system, to learn more about the system in this way. But the actual JIJs that occur in your model are never presumed to reflect any real reality that is underlying there. They're just intermediate calculating entities for making. Entities for making in that they could have predictive power, but they might not correspond to anything mechanistic? They're just used to represent your state of information about system. In this case, this Ising model is what you get if you have all the pairwise correlations. You can spread all that, then you get this. And then you could use that to predict something else. I mean, the general approach, I agree with. You want to know, can I? I agree with you. You want to know can I push these cells to this state, that state, and want to know what are the best perturbations? To formulate it is trying to learn what the MaxEnt model is, I think is misunderstanding what MaxEnt is doing. MaxEnt is just a calculating tool for turning a state of information into predictions. It's not presuming that you're capturing the microscopic underlying interaction. I think I would agree with that. I think our goal here is we want to come up with a description which can allow us to make predictions about how do we achieve this kind of state. And we may not attribute any sort of physical reality to any of the structure that we develop that will be helpful in predicting that. But if it's helpful in predicting it, then I think we're okay with that. Is that would you agree with that or no? You think that's still fundamentally changed the problem that you It will fundamentally change the problem that you would like to solve. I'd love to hear how to do it in a more aligned way. We have to hopefully. Well, let's thank David again. And the next talk will be given by Laura, and he will be talking about building key networks from the bottom of the lab. Good morning, everyone. I think I met most of you already, but my name is Laurent. I studied my lab about a year and a half ago. About a year and a half ago at Concordia University in Montreal. And I think I'm the last one standing between you and the mountains and the coffee and food. So I'll try to go relatively quickly. So as a quantitative biologist, right, sorry? Quantitative biologists, right, who like to understand, have a quantitative understanding of the cell. So we'd like to be able to look at these complicated networks of genes. Networks of genes and be able to make predictions about how the cell will make decisions to differentiate or how they will respond to the environment. Right, here we're in the gene network conference, so I don't think I have to really convince you about this. And then we could, kind of in the longer term, use this knowledge to engineer cells for useful purposes. And what we are particularly interested in my group is. Interested in my group is about dynamical signals, right? How cells can both generate and utilize these dynamics inside the cells. So we saw yesterday a nice talk about oscillations, which I'll come back in a few minutes. But cells also the remarkable ability to be able to use encode information in the dynamic of signals. And there's many examples of this. I'm just going to mention one. Mentioned one, which is the Bacillus cellulus stress response, shown by a group of Michael Elowitz, where they shown that they could encode the amplitude of the stress of the environment into the frequency of the pulse of the sigma factor. So, here when there's no stress, we have low frequency of pulsing, and when there's a stress, you have a higher frequency of pulsing. And the approach is Pulsing. And the approach, and there's many other examples which I'm not going to go into. But the approach that we use in the lab is kind of a bottom-up approach to building a small circuit to build this knowledge about dynamical circuits. So there's kind of these two approaches for why we would like to engineer biology. There's one for direct applications, such as producing valuable chemicals or for cellular therapies. But what we're particularly interested in. But what we're particularly interested in is this kind of bottom-up biology, right? Building these small circuits that will help us understand constraints for natural circuits. And this is a little bit how synthetic biology was started about 20 years ago with the publication of two iconic circuits, the bi-stable toggle switch and the repressillator, which is a synthetic oscillator. So it's a very simple design, right? We have three, I'm sure you're all well familiar with it, three classic repressors that inhibit the production of each other in a single feedback loop. And while everyone was amazed that this circuit designed on a piece of paper could oscillate, right? This was a circuit that topology that was not found in nature, designed on a piece of paper, and then you could actually see oscillations in single cells. Oscillations in single cells. So here, those are three single cell trace. The red one looks like it's oscillating, the blue one is not clear, and the green one is clear.