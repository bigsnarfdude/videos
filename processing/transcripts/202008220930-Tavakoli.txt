Thank you very much, Egypt. So since Winfrey talked about the simulation and practical mathematics. Mathematics rather than theoretical mathematics. Before I start, I would like to talk about a little bit, just one minute, about a mathematician, theory mathematician, and practic mathematician. So in theory, mathematicians, they don't care about complexity, to be honest with you. As far as I know. You. As far as I know, for example, myself, we just want to find a formula, a very complicated, sometimes formula. We don't care. We just want to find a recipe. But this recipe, how practically this recipe is good or bad, we don't care. On the other hand, practitioners like Winfried, they would like to They would like to see how complexity involves in the efficiency of a method. So they look at those methods. They compare methods by method for certain problems to see which one has less complexity. Less complexity, and they choose that one. I think that this is not that easy to do that. And one of them is my talk that we have several method to find the line length in GIG1 or any kind of Any kind of queuing system. And then complexity is the main issue in here. This is what I meant by all of these, that practically we want to see how much less time computer takes to make this program, to make a program efficient and Efficient and less time. So, in here, actually, I introduce two methods which we believe both methods, especially the first method, distribution little loss or distributional little loss and roots method. Roots method. And as far as we know, these are the methods that make complexity very low compared with the other methods. This method finds a line of Q line in this G I G one. So the outline is going to be like that in this. Is going to be like that. In this talk, we look at the following model: G I G 1 Q. Inter-arrivals assume the values from 1 to G plus 1, maximum. So we have a bound for arrival and for service also we have a bound. As you see, the problem is to find Pm. The probability the line length is M. So So, to find PM already has been done for many, several others before. This is not new to finding PM. But the method that we pro present here is kind of new, it's brand new. And this method makes this more efficient. Is more efficient and time complexity less. So we are going to talk about two methods. We call method L, it stands for liter, distributional law of liter or abbreviation DLL and Based on this DLL from the waiting time distribution, abbreviated WTP. And the other method is based on characteristic rules. We call it R. Now, for the method L, our approach is based on Approach is based on convolutions. Whereas Chadley used characteristic routes and these characteristics, they have their own problem. But if you use simple manipulation, that might be very appropriated. Appropriated correct result using this method. Grassman and Jane calculate the number of flops to find, so we start with weighting time distribution. So we are talking about GIG1 and the discipline is F. Is FIFO. So this is, you have to emphasize that. So they calculate this and they got the complexity GH per iteration. So this paper, this talk is all about complexity, mostly about complexity. We work on that method and then we compare the complexity, how complexity. Complexity, how complexity would be affected. Now they use this formula, easy formula relation, and where W is the waiting time and S is service time and A is inter-arrival time. So we later on we We remove the subscript because mostly we are dealing with equilibrium situation. Now, this is how they did. Or we can explain here that we create a sequence. We call it UI. This sequence is the probability of the difference between service time. Difference between service time and interarway time, which can be negative or positive. And then out of this sequence, they create two other sequences. They call it BI and CJ. And to get BI and CJ, they define this relationship. They define this relation for Bi and number four for CJ. And these are equation in terms of Bi and Cj. And then if you solve this equation in terms of Bi and Cj, then you can get weighting at this solution. Making that distribution very nicely. So if you call Wi to be probability of W equal to I, then as soon as you get W0, which is related to C, and we assume that you already find C from those equations above, so then Wi can be calculated this. This way, and the upper bound of sigma is meaning that the minimum value of H and I. Then immediately you get something interesting, which is Bj, and Bj is not only give you CK to get the waiting time distribution, but it's also. Distribution, but it's also interesting because it also gives you the ideal ideal time distribution. And it's easy to see why it is that. So we have BJ gives us ideal time and CJ's gives us waiting time. Now we go to another approach of getting these. Of getting this waiting time distribution. And this approach comes from reference number one, which is chat. So they start an expression with the coefficient UI. So we know that we have already UI, we got a sequence UI, we create. We create some kind of generating function, but it's not really a generating function. And we set up, actually they set up this relation. And for mathematician point of view, this is very interesting idea that you all of the sudden in the middle of nowhere, you get some relation. You get some relation like that. And it's very interesting to me how this comes to their mind that they start with this kind of semi-polynomial. So then they got H root. This polynomial has more than H roots, but H root. H-roots, but H-root outside the unit circle. This is, I am talking about a characteristic root actually. So I should mention this before. So we have Z1, Z2, ZH. And then they have these two relations in terms of Zi. Z i so w zero get this and w i can get from this equation where this d and d k are fixed number, dj change by k and d is fixed, and this is the product of all of these roots minus. roots minus one one minus the root and dk is the reciprocal of the difference between roots with z the roots z and so-called characteristic roots of the waiting time distribution so by characteristic route we mean By characteristic route, we mean these the roots of these equation. Now, let's go back to complexity. This is what we are aiming to see which method is really good method in terms of complexity. In terms of complexity, the time complexity of characteristic routes, those roots that I mentioned for WTD, is this, per iteration. Now, if you put h equal to g, then this is going to be quadratic. And to convert from WTD to length distribution is another quadratic. Distribution is another quadratic. Now, if you compare this complexity with matrix iterative methods, the best is cubic. So, this is lots of saving. So, up to this point, I just want to make sure. I just want to make sure that for WTD, even if you go further and get the length distribution, still you have safe time compared with matrix iterative. Now we are going to discuss the method L. Based on distributional law of this TLL is important and is very useful. And here we present here because of usefulness of L methodology. Of L, method L or D L L. Computationally, one of the most efficient method with quadratic rather than cubic. So first, if you are interested in complexity, so this method is much better than the other methods, that they end up, you end up with cubics. Especially for high values of the bound G and H. So, but on the other hand, DLL is less obvious mathematically, and the proofs in literature are often confusing. However, we don't care about that. We want to find a method that gives us. To find a method that gives us complexity less. This is the difference between practice and theory. However, the resulting mathematical forms are simple. Well, I said it's complicated, but the result is simple and is easier to program. This is the point, but when I say. When I say that practically is good and complexity is good in both, so this is what I mean by easy to program. So we apply DLL to find Pm. The probability of the number in Q, not in the number in the system, we call it XQ. This idea used by Chatty, the first reference. We don't use for this, we don't use correctly this routes that I said before. And this is more efficient, conceptually easy. Conceptually easy. We use a theorem in renewal theory to formulate this discrete time renewal process. This is a theorem. This is the basic theorem that we use. And in the equilibrium environment, renewal. Renewal process, the time between this is inter arrival time and the probability of arrival time i equal to a i. Select a value t, any t you wish, and then according to t, find a subscript k. subscript k such that t occurs between t k and t j plus one the arrival of cj and arrival between arrival of cj and cj plus one choose a time there or first choose t and then accordingly find the index and then call a star to be t minus t k which is TK, which is the time between T and arrival, CK arrival. Then we get this probability and this holds for all t because we are in a session. So this A star is a new. This A star is a new variable, and the toorem says that if CJ is the last customer arriving before T, then a star is the difference between TJ and T, or T and T K. So the interval arrival interval the interval between arrival Between arrival and the time that you choose to be there. And this has this distribution. So A star has this distribution. And this is the I guess this is the major part of this method L. Now, the customer CJ lives at TJ. Leaves at TJ plus WJ because his waiting time and arrival times all together is the time that he leaves. Now, if T that you choose is less than or equal to TK plus WJ, this indicates that CJ still is in the system immediately before T. Before t and that means that xq at the time t is at least one, which includes ck. Hence, probability of xqt bigger than one depends on this CK and this is exactly what we have, and this is why. And this is why we introduce a new random variable, a star. Okay, I don't have time. So I have to finish it. So to find X, so this is bigger than 1, then we have - bigger than 1. Now for bigger than 2, we have also the same argument. Also, the same argument, almost the same argument, and then we end up with this, and then we get a probability of h cube bigger than 2. So far, so good, easy, not very complicated. And now a little bit A little bit cheeky here to find the probability that bigger than or equal to m for any m and we end up with this formula recursively so to get this probability you need to introduce You need to introduce a new mark of chain. We call it V tilde K and use empty as the absorbing state. So this is the definition of the new Markov chain. As you see, it's recursive definition and Absorption only occurs when this or that is certainly negative. Now we call the new again sequence, what I call sequence when we mu tilde is i and then we use the fact that before we had for initiative. For initiative waiting time distribution, W zero, and then according to that, we get all of these sequences values. I don't have time to go through all of these. And then we end up with very nice expression for probability q equal to n. This is the final answer for that. And so it's a little bit more to get xq equal to zero. So we have for bigger than or equal to one, bigger than or equal to two, bigger than over equal to m, and now we end up with. equal to m and now we end up with x q to be zero then we have this and finally we have the final form this approach we call it method l for litter's law let me skip this this because i really don't have and these are easy to follow and let's go to two minutes or three minutes to To two minutes or three minutes to method R for roots. So, this is very interesting relation between characteristic root, which is Zn and eigenvalues of QBD. This is nice. So, we have a relation between QBD and characteristic rules. So still we have to find eigenvectors and programming is really maybe you don't get very good complexity. In fact, it's cubic. However, Haslinger use a technique that choose only those eigenvectors and eigenvalues. vectors and eigenvalues that is useful. So he removed all of eigenvectors which are not really useful and then choose the only those that are useful. And in this case, then he end up with quadratics. So he removed most of the eigenvectors. And this is the formula for a probability of number in Q. Number in Q and using characteristic root, and this is why we call it root. So, both of them they have quadratic complexity. And I guess I should stop here for questions. For questions. And these are my references: Chadri, Winfried Grassmann and Jane, Grassmann and me, and Haslinger that I mentioned. Thank you very much. Thank you, Java. Any questions?