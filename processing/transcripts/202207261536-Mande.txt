Which appeared in CCC 2020. And this work itself appears in Stacks this year. All right. And both of these works were done when Manasvi was a PhD student at ISI Kolkata, and now he's a postdoc at Ahouse. All right. So I'll start off with some introduction about the query and communication models that we care about for the purpose of this talk. So since most of you probably know all this, I'll rush through the introduction a bit and immediately I'll be able. And immediately, I'll be able to state our two main theorems. I'll give a somewhat detailed sketch of one of them and a less detailed sketch of the other, and I'll end with a summary. And if time permits, I'll get into some more details of the sketch of theorem too. Okay, so without further ado, let's get into the query model of interest. So, as usual, there's this non-Boolean function f. So, throughout this talk, I'm going to deal with minus. So throughout this talk, I'm going to deal with minus one, one as bits instead of zero, one for convenience. So there's a known Boolean function f, an unknown input x, and your goal is to compute f of x. What are you allowed to do? You're allowed to query an index i, and an oracle tells you the value of xi. So a query algorithm is also representable in the classical setting as a decision tree, where internal nodes are labeled by variables, edges are labeled by one or minus one, and leaves are labeled. one and leaves are labeled by values in one comma minus one and an internal node corresponding to the value xi basically corresponds to the corresponding query algorithm querying the variable xi if the value is minus one you go down the right subtree if the value is one you go down the left subtree and continue so on until you hit a leaf at which point you output the value you see at the leaf okay so the query complexity of the corresponding algorithm is the number Corresponding algorithm is the number of queries you make in the worst case. And that's nothing but the depth of the corresponding decision tree. So the relevant measures that we care about for the purpose of this talk is deterministic decision tree complexity of F, which are also called deterministic query complexity of F, which is the smallest depth of a deterministic decision tree that computes F. And in the randomized case, you can think of it. And in the randomized case, you can think of it as you're allowing the decision tree to have internal coin flip nodes, which are free. And the correctness requirement just becomes for all x and minus 1, 1 to the n, probability that the randomized decision tree computes f of x correctly is at least two-thirds. All right, so that's the classical decision tree or classical query complexity model that we care about. Now, let me tell you. About. Now, let me tell you a little bit about the quantum query model that we care about. Unfortunately, there's no clean representation as decision trees as in the classical case. However, there is a reasonably clean representation. So one of the main generalizations or quantizations that's there in the quantum model, it's not there in the classical model, is you can make queries in superposition. So here, this I represents an index. So I corresponds to an index register, if you may. Index register, if you may. This B corresponds to the output of a query. So this register contains outputs of queries, and Z corresponds to workspace. So how does a quantum query algorithm look? You start with a predefined start state, which is given to you for free. And then you're allowed to apply several unitaries interleaved with these oracle operations. Well, these UIs are input independent unitaries. At the end of this algorithm, you're going to measure. At the end of this algorithm, you're going to measure a designated output qubit and output the value that you observed there. And the correctness requirement, just as in the randomized classical case, is that for all inputs x, you require that your quantum circuit outputs the correct answer with probability at least two-thirds. And the cost of this algorithm is the number of times you invoke this oracle. And again, you want to minimize this overall quantum algorithms that query algorithms that compute. Algorithms that query algorithms that compute F, compute F in the sense given here. Okay, so that's it about query complexity. Now, let's look at the basic model of communication. In this party, in this setting, there's a known Boolean function f to two parties, Alice and Bob, and an unknown input is split between them. Alice has X, she doesn't know Y. Bob has Y, and he doesn't know X. And they wish to compute f of X component. And they wish to compute f of x, y via a communication protocol. So Alice's message m1 consists of bits in the classical case. It depends only on x and the function f. Bob replies with a message m2 that depends only on y and m1 and the function f and so on until they agree upon an answer, pi of x, y. They output an answer. And in the quantum case, we allow Alice and Bob. Allow Alice and Bob to share an arbitrary amount of prior entanglement in this talk, unless I mention otherwise. And the messages are over a quantum channel. So the messages are qubits. And the correctness requirement, again, is that for all inputs x, y, probability that the protocol outputs the correct answer is at least two-thirds. So I just want to be clear here. There's no input distributions. The communication channel is completely error-free. There's no eavesdropper. It's just a worst-case requirement. It's just a worst-case requirement that we consider for the rest of this talk. Good. So, having said that, let's look at compose Boolean functions. In general, here's a way to convert a query problem into a communication problem. Hopefully, it was already clear from the previous talk, but I'll just go through an example in any case. So, let's say you have a Boolean function on n input bits. Here's a way to convert it into a, so you can think of it as a query problem. Here's a way to convert it to a Here's the way to convert it to a communication problem. Replace each input bit by a small gadget, in this case XOR, on two input bits. Give one of those inputs to Alice, give the other to Bob. So now this is now a communication problem, F composed with XOR, where Alice has all the X bits, Bob has all the Y bits, and their task is to compute F composed with XOR of X, Y. So just in there are several examples that are composed Boolean. Examples that are composed Boolean functions. Just as an example, here is arguably one of the most fundamental questions in problems in communication complexity: the equality function, where Alice and Bob have two n-bit strings, and they wish to know whether these n-bit strings are equal or not. So equality is nothing but NOR of XR. Another example of a composed function is set disjointness, which is NOR composed with AND. Yeah. And inner product is parity composed with AND, and so on. And so on. So, here's a simple simulation of how do you get a communication protocol for a composed function given a query protocol for the initial base function that you start. So let's say you have a query protocol for F. Let's just focus on the classical case for now. A query protocol is nothing but a decision tree. And the query complexity of this decision tree is just the depth of the decision tree. So here's a way to So, here's a way to first convert this into a decision tree for f composed with XOR. Replace each variable by the corresponding XOR. Straightforward. So whenever you see Z1, for example, replace that node by X1, XOR, Y1. Apologies for the zeros and ones, treat them as ones and minus ones. So this is a decision tree for F composed with XOR, except now the queries. Compose with XOR, except now the queries are no longer variables, but the queries are XORs on two bits, parities on two input bits. So now, if Alice and Bob wish to simulate, use this decision tree to construct a communication protocol, it's fairly simple. So they jointly simulate this decision tree here. At each node, they exchange two bits of communication, and then they both know which edge they're supposed to take. So Alice sends Bob X1, say in the first round, Bob sends Alice. First round, Bob sends Alice Y1. Now they both know the value of X1, X, or Y1, so they both know which edge to follow. And then they just traverse the tree until they hit a leaf. And it's easy to see that if your original query algorithm had cost K, had depth K, then the resultant communication protocol has cost at most 2K. This holds in general if this gadget was any gadget on two input bits, XOR and OR. XOR and OR as examples. And it's not hard to see that a similar statement also holds in the randomized case. What about quantum? So unfortunately, like I said, there's no clean representation of quantum query algorithms as decision trees. However, there's a similar simulation that can be done. And Bohrman, Cleave, and Vigderson showed in the late 90s that a similar simulation indeed can be done, except this overhead of two. Except this overhead of two gets replaced by an overhead of order log n. Now, there's this whole research area of lifting theorems that Robert, for example, motivated really well in the last talk, is trying to construct gadgets square such that these inequalities are actually tight. And what would this enable us to do is it would enable us to prove lower bounds on communication complexity, which is a richer. Communication complexity, which is a richer model than query complexity, by proving lower bounds in the query complexity regime in the corresponding query complexity model. Yeah. But this is not the focus of this talk. So you've heard about lifting theorems in the previous talk. I think the next talk is also devoted towards lifting theorems. The focus of this talk is going to be whether this log n overhead in the quantum setting is necessary or not. Necessary or not. See, usually, when you hear about quantum models, the common belief is that whatever sort of statements you have in the classical models, you have the same things in the quantum models. So this log n overhead being present is somewhat strange. I mean, if a classical query to communication simulation algorithms admit an overhead of a constant, that's two, why do quantum query to communication simulation algorithms? Simulation algorithms yield an overhead of log n? Is this avoidable? So, that's the question we're going to address in the remaining part of this talk. It turns out, for example, for the disjointness problem, which is, well, what's represented here is actually the set intersection problem. The negation of this is set disjointness. Yes, this log n overhead can be avoided. So it's known that the query complexity of R is theta. query complexity of R is theta, the quantum query complexity of R is theta of square root of n R on n input pairs. So by this result of Bohrman, Cleave and Wiggleson, you get that the communication complexity of set disjointness is order of square root of n times log n. So Hoyer and DeWolf were able to get this overhead down from log n to a constant to the log star n. And then finally, Ahrens and Ambinus, a couple of years. Aren'ts an ambinas a couple of years later, they were able to get rid of this overhead completely, get it down to a constant overhead via slightly complicated procedure. So now you may ask, if disjointness does not admit this overhead, is it's feasible that for all Boolean functions f, this log n overhead is actually avoidable. So, and we could actually get a constant. That brings me to our main theorems. When Main theorems, when, if at all, is the log n overhead necessary. So we showed in 2020 that this log n overhead is actually necessary. And how we did this is we exhibited a total Boolean function f such that the quantum communication complexity of f composed with XR is requires this log n overhead. And I should point out that this result, this function, this statement. This function, this statement really only holds when the inner gadget is XOR, or our argument only works when the inner gadget is XOR. And now in this current work that I'm talking about, we show that if your outer function is symmetric, then you can avoid this log n overhead altogether. You can just get a constant overhead. And this, for example, recovers the set disjointness order square root n communication. Order square root n communication complexity and via an arguably simpler protocol than Aarons and MBN. And I hope to sketch that protocol, in fact, soon enough. Our second main theorem is that if we just relax this notion of symmetry to transitive symmetry, I'm going to define both of these notions on the next slide. Then there is a Boolean function for which this log n overhead is necessary. Is necessary. Not only that, it's even necessary if the communication model you're looking at is the unbounded error model of communication. And so that's what I want you to interpret this as. The quantum unbounded error model of communication is the same as classical unbounded error communication complexity up to constant factors. And one important thing worth noting is that in the unbounded error model of communication, you do not allow for shared entanglement or public randomness. Entanglement or public randomness because otherwise every function is easy to compute. Okay, so I hope the statements are clear. Modulo, the definition of symmetric and transitive. Great. So what are symmetric functions? A function is called symmetric if it only if the function value only depends on the number of minus ones in there. So several natural functions are examples. Several natural functions are examples of symmetric functions. The OR function, AND, parity, majority, these are all examples of symmetric functions. Here's an alternate definition. Now, transitive symmetry only requires for any pair of indices, there must exist a permutation of all your input variables such that i gets mapped to j and f of x equals f of sigma x for all x. x equals f of sigma x for all x. So just as an example, let's look at an example of a transitive symmetric function. Majority composed with majority is a transitive symmetric function. So for example, if you take one index here to the second majority and one index here to the last majority, what would this permutation be? So first, what you can do is you can swap this whole majority block with this whole majority block and then permute variables inside this block itself. Variables inside this block itself. So eventually, what you're doing is you're mapping this second variable to the required variable in the last block. And you can see that the function value is preserved. Okay. All right. So let me tell you a proof of a sketch of the proof of theorem one. Remember that theorem one says if your outer function is symmetric, if you have a constant size inner gadget, then Gadget, then this log n over it is not necessary. In other words, the quantum communication complexity of f composed with a constant size gadget is order quantum query complexity of f. So here's another representation of a symmetric function. I've just plotted out how the function behaves on different Hamming weights. When I say Hamming weight, I mean number of minus ones in the input string. So this is an example of a symmetric function when the Hamming weight of the function when the hamming weight of the input is zero then you output this value so think of this value as minus one and this value as one say when the hamming weight is one output value one and so on so what's the query complexity of a quantum symmetric function this is known from the work of deals burman cleave mosque and devolve uh that if you take hamming weight n by 2 and plot out the largest interval around that in which f Around that, in which f is a constant, and look at the lengths of the remaining sides, that's t0 and t1. Look at the maximum of those lengths. Then the quantum query complexity of f is theta of square root of n times the max of those two. So just as an example, let's look at the majority function, for example. P0 is going to be n by 2. So the quantum query complex is theta n, which is true. is theta n which is true for the or function t 0 is going to be 1 and sorry for the or function yeah t 0 is going to be 1 and t 1 is going to be 0 so the quantum query complexity theta square root n okay and just to give you a sneak peek as to what's coming a way of witnessing this upper bound is doing the following so you're going to run a quantum query algorithm query algorithm that in step one, it decides whether the Hamming weight of the input is larger than equal to. So for the rest of this talk, let's assume that T1 is equal to 0. Let's only consider the case where we're dealing with a possibly non-constant function over here and everything on the right of n by 2 is constant. So step one of the algorithm is going to determine whether or not the Hamming weight of the input is larger than equal to two times. input is larger than equal to 2 times 2 t 0 or whether it's less than or equal to t 0 so suppose the hamming weight of the input is larger than equal to 2 times t 0 then you already know that the function is a constant so you know the answer but if step one outputs the hamming weight of the input is less than equal to t 0 then what you're going to do is you're just going to keep on iteratively find indices i such that x i is equal to minus 1 so So, in other words, again, in short, you're either going to determine that the Hamming weight is greater or equal to a certain quantity, or you're going to find all the minus ones. So you'll know the Hamming weight exactly. And at that point, you know the function time. So this is how our communication protocol is also going to work, but we have to take a lot of care because our goal is to avoid this log n overhead. And that comes in the knife simulation. All right. So here's a sketch. So, here's a sketch of amplitude amplification. If you've seen Grover search, then this is the same picture that appears. So, this is another formulation of it. So, in this setting, think of ket psi as a uniform superposition over all indices. Ket G, which I want to call the good state, is a uniform superposition over all indices i, such that xi is equal to minus 1. To minus one and ket B is a uniform superposition over all bad indices, so it's a uniform superposition over all ket i where xi is equal to one. And our task is to find an i for which xi is equal to minus one. So first note that if all xi's are equal to one, then ket psi and ket B are the same. So how does this work? How does this procedure work if you want to find this? We have two operations that we're allowed to do. One is a reflection around ket B, which in the query setting just takes one query to implement. And another is a reflection around ket psi, which in the query setting takes zero reflections to implement because this reflection is a unitary that's independent of the input. So this is exactly what one iteration of amplitude amplification does. And then, how do you get closer to ket G? So, eventually, what the algorithm is going to do is it's going to keep doing these reflections. And then, eventually, you measure your state and you hope to observe an I for which xi is equal to minus one. And this is easily checkable. So, after a reflection, after one iteration of amplitude amplification, this angle theta became an angle of three theta. After another iteration, it's going to become five theta, and then so on. become 5 theta, and then so on. 5 theta, 7 theta, it keeps increasing in additive increments of 2 theta. But we're going to take a slightly different view. Suppose you've done J iterations of amplitude amplification. Now what I'm going to do, instead of reflecting around ket B and ket psi, I'm going to reflect around ket B, which is the same as before, and use this iteration. And use this iteration, the j minus one iteration itself to reflect around ket psi j minus one. So this OG times OG simply reflects around ket B and you can check that Hi minus 1 R psi Aj minus 1 inverse reflects around the J minus ket psi J minus 1. So the cost of Aj in this in this view is just three times cost of Aj minus. Just three times cost of AJ minus one because we've used it three times, and plus one because OG costs one, and R psi is free in the query setting. So, this I'm simply talking about things in the query model right now. R psi is free because we're reflecting around a state that's input independent of the input. Now, the main problem in the communication model: if you want to simulate this algorithm, so this is an example. This is an example run of Grover's algorithm. So, if you want to simulate this algorithm, we cannot reflect around ket psi for free in the communication model. Why? Because ket psi, the analog in the communication setting, is now an entangled state. So if Alice and Bob wish to jointly reflect around this state, they'll have to, in the naive way, they'll have to exchange log n qubits of communication. And this is precisely what we're trying to avoid. Avoid so the setting now is ket psi is this uniform superposition over ket i ket i. This first register is held by Alice, second register by Bob. The good states is a uniform superposition over ket i ket i where xi is equal to yi equal to minus 1. Again and the goal is to find an i for which xi equal to yi equal to minus 1. This is precisely the set intersection problem. Right. Right. So, how do they do this without incurring this log n overhead? So, the first thing to note is this ket psi is nothing but log n EPR pairs. And we've assumed that EPR pairs are given to the parties for free in the communication model to consider. Just as in the query model, it turns out that this OG reflection around the bad state can be implemented perfectly using a small amount of communication. But this reflection around But this reflection around ket psi can be done efficiently, but noisily. So sorry, I didn't mention the noise on this slide. So if we allow for noisy reflections around ket psi j minus one, then yeah, this is how we're going to run our protocol finally. We're going to efficiently Protocol finally. We're going to efficiently reflect around cat psi psi j minus one in a noisy fashion. Yeah. And then we carefully choose these errors, this noise, so that Alice and Bob succeed with high probability in the end. So there's a lot of analysis and calculation that I'm sweeping under the rug here, but set intersection is doable efficiently without incurring this log n overhead. And this is just. Head. And this is just in my view, a simple quantum communication protocol for set disjointness of cost order square root n. Now, how do we generalize this to symmetric functions? Well, like I pointed out when I was defining symmetric functions, what we're going to do is we're going to use a subroutine that tells us either the Hamming weight of mod Z, so mod Z is just the bitwise AND of X. is just the bitwise and of x comma y and it's going to tell us that the hamming weight of this string is either greater than t or it's going to tell us mod z exactly and efficiently and the proof idea is um let's first run uh this subroutine that asks is mod z greater equal to 2t versus is mod z less equal to t note that mod z actually might That mod Z actually might lie in the interval in between, but this is not really a problem for us. So, this subroutine itself can be run in order square root n over t qubits of communication. And then we're going to invoke the noisy distributed amplitude amplification from the previous slide to find in t by 2 iterations t by 2 many solutions. What's the solution? What's the solution? Solution is just an index i for which xi equal to yi equal to minus 1. And this we're going to be able to do with using order of square root of tn communication. And then we just reduce t to t by 2, we repeat, and the total communication cost is order of square root of tn. Again, there's some error analysis and all that I'm really sweeping under the rug that needs to be taken care of, but it all works out. And we just get away without. We just get away without any overhead at all. So, the quantum query complexity of f to begin with was order of square theta of square root of tn, and the communication protocol we obtain in the end is order of square root of t. All right, so this was theorem one. If you have a symmetric outer function, then you don't need any overhead when you simulate a query algorithm through communication. If you simulate it cleverly. If you simulate it cleverly. All right. Now, for the second theorem, we show a transitive symmetric function for which even the unbounded error communication complexity of f composed with any gadget you want of small size requires this log n overhead. So omega query bounded error query complexity of f times log n. And how do we do this? We exhibit a function whose query complexity is theta m. Complexity theta m and the lower bound satisfies exactly what we need. And the function is constructed in a clever way, in my opinion. And it's heavily inspired from this result of Ambinis and DeWolf from 14 and also our earlier work from 2020. And for the lower bound, for the upper bound, it's a combination of the Bernstein-Wazzirani algorithm and Grover search algorithm to get a And Grover search algorithm to get a theta m order m upper bound. And for the lower bound, it's simply setting variables in the input so that the restricted function becomes an inner product on m log m bits distributed between Alice and Bob. And we know from Forster's theorem that the unbounded error communication complexity of inner product m log m is omega m log m. Okay, just to summarize quickly, we showed that for symmetric functions f, this This log n overhead in the naive quantum query to communication simulation is avoidable. You only get a constant overhead. On the other hand, there exists a transitive symmetric function for which you require the log n overhead, even when the communication model is as strong as the unbounded error communication model. We have some other results in this paper that generalize our older paper in some setting. And an interesting open question, I think, is that our quantum query. I'm sorry, this should be communication here. So, our quantum communication upper bound for the proof of theorem one, it uses some amount of shared entanglement. And for almost all symmetric functions, this cost is essentially absorbable in the communication complexity itself. But there are few symmetric functions for which this is not. So, for example, For which this is not. So, for example, when, yeah. So, can we avoid this in the general case? And I'm not sure if I'm out of time. Yeah, I think you are. So, maybe we'll have another time in the next few. Well, let me ask you something like this login factor that appears. Is there a short intuitive explanation for where it comes from besides just you see? Yeah, so that's a good question. Thanks. Right. So, in some sense, it comes from the fact that queries in the classical world are queries to Classical world are queries to single bits, and queries to single bits when composed with a gadget are just functions on two bits. The problem in the quantum model is queries are not queries to single bits, they can be queries to a superposition of indices, log n indices, and this is precisely where, sorry, log n qubits, and this is precisely where the log n overhead comes from. I'm being a bit hand wavy, but just the fact that you can query. But just the fact that you can query superpositions of variables is, I think, essentially where the login overhead comes from. Thanks, Kevin.