So, this is joint work with Valent. And well, if you have any questions and you type them in the chat and I don't see them, I'm sure Valent will be able to help with answering them. Okay, so we'll just start with the permutations, something like, I don't know. Something like, I don't know, three, six, four, five, two, one, seven. Actually, let me change the order of some of these. Okay. And an increasing sub-sequence in this permutation is just going to be, well, it's a sub-sequence where the numbers are in increasing order. And the longest increasing sub-sequence is just. Just going to be an increasing subsequence of maximal length. Okay, so here there's a longest increasing subsequence of the length is three, and you could have another option as well. You could do say four, five, seven. There are other possibilities. Okay, so then there's a question of Ulam, which is going to be asked in the early 60s. We asked in the early 60s, which is basically try and understand Ln. So the length of the longest increasing subsequence in a uniform n element permutation. And so there was kind of lots of interesting math to do with this problem and the definitive and definitive answer it was really this like dived johanson theorem like dived johansen and what they showed is that so ln there's the leading term of ln is two root n so this was kind of known before shown by vershik and kirov and logan and chup and they found the fluctuations so there's an order n to the one-sixth fluctuation an order n to the one-sixth fluctuation and this fluctuation term this yn converges in distribution to a tracy widthum to you random variable okay so this is kind of you know this really answers you know you get the full limiting picture for this ln um but you could try and think there's there's really a richer structure here so ln doesn't tell you everything about Tell you everything about the longest increasing subsequence. So you could ask about this subsequence itself. So right now, Richard's structure. So I can think of the increasing subsequence as a function i n from 1 to ln into the numbers 1 through n. So for example, for this For this red one, red subsequence that I drew, I would have I n of one equals one, i n of two equals four, and i n of three equals seven. So just indicating the location in the permutation. Okay, and you can ask this i n as a function, this should have an interesting scaling limit. And so this is what I'm going to talk about today. Today this is a theorem. Okay, so I'll let I n be any longest increasing subsequence in a uniform n-element permutation Figure M So then I n of 2 root nx minus n x, sort of after rescaling by 2n to the 5 sixth, this converges to a limit pi. So here pi Pi from 0, 1 to r is a random continuous function. So this convergence distribution is joint in all x between 0 and 1 and say, you know, you can say in the uniform uncompact topology, or because i n is going to be, because i n has discontinuities, you could work with like a score code topology if you want. If you want. Okay. And so this you can, I'll just point out. So where we're centering at 2 root n times x. So we know the length of the size of the domain is ln. And because ln is roughly 2 root n, if we look at the location 2 root n times x and let x vary between 0 and 1, we're going to see this whole sub-sequence. And if you want, you could replace this 2 root n term by term by, you could replace the two root n term by the length ln itself. It's not going to change the statement of the theorem. And the other thing is, you know, in our example up here, there are multiple longest increasing sub-sequences. And for this theorem, it doesn't matter which one you choose. And in fact, any two distinct longest increasing sub-sequences, if you look at their difference, then this is going to be of lower order. So I'll write this as more over. I'll write this as moreover. So if Jn is another longest increasing subsequence, then sort of the uniform difference between IM and JM, so the subscript U meaning uniform norm here, if you divide by n to the 5/6, which is the scale at which we're converging, then this is going to converge in probability to zero. Convergent probability to zero. Okay, and in fact, something much stronger should be true, but this pops out fairly easily. Okay, so the next thing you can ask is, well, what is this limiting function pi? And so pi is going to be a kind of a bridge-like object. So pi of zero equals pi of one equals zero. Equals zero and we call pi the directed geodesic for reasons that you know will become clear later on. And so you might think, well, other, what's another bridge-like object? You know, it is a Brownian bridge, would be maybe something you might expect to see here. So pi is in kind of much different from a Brownian bridge. So in particular, Different from a Brownian bridge. So, in particular, it's holder two-thirds minus rather than holder one-half minus. So, it's more regular than a Brownian bridge. And with a Brownian bridge, you have a lot of independent, you essentially have independent increments, okay, once you sort of subtract the bridge part. And pi has a more complicated dependent structure, which we'll see. Okay, so this is the main theorem, and then the next thing. So maybe going back to the Spike-Definition-Hansen theorem, and all the work on this longest increasing substances sort of the first interesting work started with Hammersley, and he had this idea of reinterpreting the longest increasing subsequence as a geometric problem involving this model called Poisson. Involving this model called Poisson last passage percolation. So this is the next thing to talk about is Poisson last passage percolation. So I'm going to let P be a Poisson process on R2. So you can sort of just draw it a bunch of points. Okay. And now, if I take points u and v in R2, which where u1 is less than or equal to v1 and u2 is less than or equal to v2, then I can define sort of a distance-like object here. So, this is sometimes called the last passage value, or not sometimes it is called. Value, or not, sometimes it is called the last passage value. So dp of uv, this is going to be a maximum over all paths pi from u to v that go up and to the right of what I'll call the length of pi with respect to the Poisson process p. So this length of pi with respect to p, this is just the number of Poisson points. In pi. So you can kind of give an example. So if I set u to be my origin over here and v up there, okay, then I could draw an upright path that looks something like this. And I pick up one, two, three points. So this is an upright path which gets length three and Unless I'm missing something, but I think this is the longest one. Okay. And so this DP, you should, it's supposed to evoke the idea that this is a distance-like structure. So really, it's metric-like. And we call it a directed metric structure. So, what are some differences between this object and a regular metric? Well, one is that in a regular metric, you can imagine taking the minimal length path. Here, we're going to take the maximal length path. And so what that means is the triangle inequality is going to go backwards. So dp from u to v is greater or equal. V is greater or equal dp from u to w plus dp from w to v. So we have a reverse triangle inequality, but that doesn't really change very much. And then another thing is we're only allowing, we're only going to assign distances between points in a particular order. So dp of uv is defined, but dp of vu is not defined. So one thing you could do there is, you know, to keep consistent with the triangle inequality, is you could call dp of vu equal to negative infinity. So it has this negative infinity so it has this directionality so it no longer we no longer have dpuv equals dpvu and this is like a directionality of the metric okay but really you know for all intents and purposes we should think of this as a metric like object and so what's the connection to the longest increasing subsequence so if i let tn be the first time First time when we see endpoints in this box 0t squared. So maybe I can try and draw an example on here. So maybe this, okay, I didn't quite draw it correctly, but this could be something like. This could be something like the point T4. Okay. Then the last passage value from the origin to the point Tn Tm is equal in distribution to the length of the longest increasing subsequence in an n-element permutation. So nothing particularly complicated is happening here. That is happening here. Basically, if you look at the set of points inside this box and you look at the relative ordering of their y-coordinates versus their x-coordinates, because we were just looking at a Poisson process, this is a uniform permutation. And looking at upright paths that collect the maximal number of points precisely corresponds to looking at longest increasing sub-sequences. Okay, so that's the connection with Ln. And then the connection with this. And then the connection with this, the subsequence itself, this ion, is a bit more complicated. But I'll just say that, and I won't write it down precisely. I mean, you can work it out if you want to, but I'll just say that IN is closely related to the optimizing path. Because we're thinking of a symmetric, we'll call this a geodesic pi. Going from the origin to the point Tn Tn. Okay, so this is the connection. And what this tells us is that, you know, in order to understand the limit of just the length ln. Ln what we really need is the limit of just the last passage value from the origin to the point n. So we just need one distance. But if you want to understand the limit of the optimizing path, the easiest way to do that is actually going to be basically take a limit of the entire Basically, take a limit of the entire metric. Okay, if you take a limit, we take a limit of this entire directed metric and then look at geodesics in that limiting metric. And that's going to be the best way to understand the limit of IM. So to understand, I'll just write this down. Limit of IM. We need The limit of the entire directed metric. And this is kind of going to be the key. So let me say this as a theorem. We'll introduce some scaling first, so I'll just let x, s sub m. comma s sub n be s n plus two x n to the two thirds comma sn and I'll define the function h of x y which is x plus y and so now we can define our rescaled version of this metric using these so I'll let ln going Going from u to v, be n to the minus one-third dp going from u n to v n. And then you have to subtract off this mean term coming from this h. So minus h of v n minus h of un okay. So the idea here is with this rescaling. With this rescaling, maybe I'll just include a few points. So I maybe going, I'll draw this origin in this scaling still stays as the origin. And this point nn is now 0, 1 in this three scaling. And I'm really taking a scaling limit around this diagonal strip. So I have distances which are order n. Distances which are order m in this direction. So this is going to be my time direction. Yeah. What are u n and v n? U n and v n. So u, this is like, so here, yeah, I should, so here u is going to be, u and v are points in R2. And so you can think of u as u equals xs. And so u n is going. Un is going to be XS sub n. Yeah, this is. Does this clear things? Yes. Okay, great. Okay, so this, as I was saying, you have distances which are order n in time, and then their order n to the two-thirds in sort of this transversal direction. So order n to the two-thirds in this direction, I'm going to call this direction space. Going to call this direction space. Okay, so the two directions are distinguished. Okay, and then the theorem, which we should keep the beginning of it up, is that Ln converges in distribution to a limiting object L. And this limiting object L, this is a limiting directed metric. And it's called the directed landscape. Okay, so basically, so the next thing is we want to go from, so now we've created our limiting metric. In order to understand the limit of the longest increasing subsequence, we want to understand the limit of these optimizing paths. The limit of these optimizing paths. So we need some way of constructing optimizing paths in this limiting metric as well. So it's good to think a little bit about what this L looks like. So L is going to assign distances. So maybe I'll put the distances in quotation marks here. So it's going to assign distances, which are now real numbers. So we're no longer going to have positive distances necessarily because we've Distances necessarily because we've subtracted off this large mean term. So it assigns distances, which are real numbers, to pairs X, S, Y, T with S less than T. So this ordering in Poisson-last passage percolation that we saw, where the points needed to be ordered so that. The points needed to be ordered so the second one was above and to the right of the first one. In the limiting picture, this manifests itself just in terms of the time direction. So if I keep this picture up from here, this in the limit, you can think of it like this. I'm going to have the point zero zero, and I'm going to the point zero one. And what were upright paths? What were upright paths, so I had these things which were upright paths. Now these are just going to be paths with the restriction that they can never turn back in time. So that's going to be our idea. Okay, but so L assigns distances to pairs of points in the plane. And now that we have an object which is assigning distances, we can use this to assign lengths to pads. And once we have assigned lengths to pads, then we can talk about geodesics. To paths, and then we can talk about geodesics. So let's build path lengths. So this is going to be my candidate path pi. So we can think of our paths because they're always moving forward in time. It's going to be nice to just think of them as functions rather than actual paths in the plane. So we're just going to think of them as functions from a time interval. Functions from a time interval into the real numbers. And so these are continuous. And I'll define the length of pi in L in the following way. So it's going to be the in FEMA over all partitions, so S equal to R naught less than R1 less than. less than Rk equal to T of sort of an approximate length defined through this partition. So sum i equals 1 to k of L going from high bar of Ri minus 1 to high bar of Ri, where I'm just using this shorthand high bar of r is equal to Pi bar of r is equal to pi of r comma r. So, what's the idea behind this definition? This is, you should think of this, this is analogous to defining the length of a curve in, say, r2 in terms of a polygonal approximation of that curve. And so, when you take finer and finer polygonal approximations, you should converge to the length of the curve. So, here, you know, we say I had a partition of Say, I had a partition of this here as a function pi from 0, 1 to r. I could partition into, say, one, two, three pieces. And I can approximate the L length of pi by just saying, okay, what's the L distance from here to here? The L distance from here to here, L distance from here to here, and the L distance from here to here. Okay, and you end up taking the infimum over all such partitions. Over all such partitions. So, usually, if you're defining length, you're going to take a supremum. But remember, the triangle inequality is going backwards, so we're going to take an infimum. So now you have a notion of lengths of paths. And so you can say that pi is a geodesic if the length of pi in L is just the distance between its endpoints. So it's just going to be L for L. As that points, so it's just going to be L from pi bar of s to pi bar of t. So here I'm keeping my convention that you know pi is a function from s t to r. Okay, so there's a fact that there is almost surely a unique Unique L Geodesic pi with let me give a domain for this pi before I go on L geodesic pi from 0 1 to r with pi of 0 equal to 0 and pi of 1 equal to 0. Okay, and the Okay, and that so really this is the geodesic between these two points in the plane, and my choice of points in the plane doesn't matter. So, L has you know various invariance properties, which mean that you can send any pair of points to any other. So, this pi here is the directed geodesic that we saw before, and this is the scaling limit. And this is the scaling limit of the longest increasing subsequence. And now you can kind of see the dependent structure. So, L, like one way of thinking about L is it's there's some background noise, which is the limit of your Poisson process, and L is telling you what the optimal path through this background noise is. So, the dependence structure on this directed geodesic is coming from the fact that you're doing some sort of optimization. Coming from the fact that you're doing some sort of optimization problem. But the difficulty is that when we, in the pre-limit, we had a Poisson process there. But when you pass the limit, there's no real notion of this noise. So all you have, all you can recover are these lengths. And that's why we define things in this slightly different way. Okay. So now the next question really is: well, how do you get to this theorem? And I should say that this is directed landscape is not coming out of this new work of myself and Valant, but previously together with Janis Ortman, we showed that this sort of theorem, this Lm converges to L. To L for a related model called Brownian last passage percolation, which has some similarities with Poisson-last passage percolation. And the key inputs that you need for understanding convergence of Brownian-last passage percolation and Poisson-last passage percolation are quite similar. So, they sort of have a core integrable structure which allows you to do this convergence. And one of the Convergence. And one of the, aside from you know, the idea of this project being just to take the limit of Poisson-last passage percolation, the idea was to develop a general theory for convergence to the directed landscape, which isolates the integral input. So this is, you know, now we're most of the way through the talk. I can tell you the sort of secondary goal. So the goal of the project was to develop Of the project was to develop a general theory of convergence to L. With the idea being you want to isolate, you want a sort of minimal integrable input. And with the hope that this minimal input, eventually you can remove this, the sort of science fiction dream is eventually you can remove this integrability assumption, but at least now you know what. Integrability assumption, but at least now you know what the important input is. Okay. So for talking about the general theory, I'm just going to change the I'm going to change the reference from Poisson-last passage percolation a little bit. So instead of my p being a Poisson process, now I'm going to look at a function p from the lattice z2. From the lattice Z2 to zero infinity. And I can still assign path lengths, but now this is going to be the sum over points in points in pi that are on the lattice of the weight of those points. So this is kind of best to draw a picture again. So previously I had this Poisson process, which is just a bunch of scattered. Process, which is just a bunch of scattered points, all of which got weight one. Now I'm going to have a lattice picture, and I'll have paths that maybe go through some lattice points like this. Okay, and they pick up. Okay, and they pick up all of these weights along the way. Okay, and you can imagine Poisson last passage percolation is a limit of these sort of lattice last passage percolations. You make the lattice finer and finer and make most of the weight zero. So, this does path length, I need to again, so dp. Again, so dp of uv defined in a similar way. So this is going to be the maximum over pi from u to v, which goes up and to the right of the length of pi with respect to p. And in order to state this theorem, we're going to need sort of these multi-path statistics as well. So we'll let dp k of uv be the maximum of our paths pi one up to pi k which go from u to v and are disjoint of the Of the sum of the weights on these paths. And I said, these are going to be disjoint other than at endpoints. Okay. So this, maybe let me try and draw this. So say dp2 is going to be the maximum weight of. Is going to be the maximum weight of two disjoint paths between u and v. So gp2 might look something like, you know, this. Okay. And let me define, oh, change color. Oh, green is fine for now. I'll call dp delta k uv. This is the gain that you get from adding that kth pattern. So this. Kth path. So this is going to be dpk of uv minus dp k minus 1 of uv. So you expect by adding each extra disjoint path, you're going to get diminishing returns. And so these dp delta k's are actually deterministically decreasing essentially. Okay. And maybe, you know, if you're familiar with the Robinson-Shenstead correspondence. Robinson-Shenstead correspondence, you can see that these dp k's and dp delta k's, these show up in terms of row lengths in the Young table. And so this is, you know, in terms of the integral structure, it's coming from this correspondence. And so this is why, you know, our assumptions are going to have, you know, something to do with things that you can analyze in terms of these DQKs. Okay. For now, let me state this theorem. So I'll let P is going to go from Z2 to zero infinity. This is an IID array. Okay. And we're going to have scaling relations. So I need to define a rescale. So, I need to define a rescaling of this last passage metric. So, I'll have scaling relations XTN, which will be Um and a mean shift H going from R two to R. So this X T N, I'm I kind of need to go around here. This will, you know, it's again going to look something like, you know, constant times n to the two-thirds x plus nt comma nt. So these are, you know, we're trying to set up the same framework that we had in the Poisson case. And now rather. Now, rather than just defining Ln, I'm going to define all of these multipoint statistics, all these rescale multi-point statistics, so these Ln K U V. So this will be C1 times n to the minus one-third dp delta k unvm minus the mean term h of v n minus h of un. Okay. Okay, so we've set up the rescaling. So now suppose that Ln K of 0, 0, Y1 converges to a limiting object alpha k of y. Again, jointly in the parameters that I've actually. Parameters that I've actually written in so k and y where l k of y is this script l k of y is a parabolic area line ensemble. So this is something that you can get from integrable methods. And there are formulas for proving the sort of convergence. We need in terms of the We need in terms of the strength of the convergence here, we need uniform convergence on compact subsets of the parameter space. So that's all you need in order to prove a convergence theorem. So then Ln1, so if we just restricted the actual statistics of the directed metric rather than looking at these multipath statistics, this converges to L, the directed landscape. The directed landscape. Okay, and then there are okay, so this is the essence of the theorem. There's a question of, you know, what type of convergence do you have here? And these assumptions are somehow a little bit too weak to get the strongest type of convergence possible, but you can get something very good and But you can get something very good, and it's very good in the sense that it's very good in the sense that you can still get convergence of your geodesic. So, let me say what we have is a type of hypograph convergence here, but I'll describe this in terms of a coupling. So, more precisely, there exists a coupling. Of Ln and L so again a new coupling of the environments because the Ln's are already sort of naturally coupled, but we're going to get a new coupling such that one ln converges to L such that almost surely not same one Ln converges to L on rationals two Two the positive part of ln minus l converges to zero. Yes, I see the notes about five minutes. It will be, that's, I don't know if I'm supposed to respond or just acknowledge and finish after five minutes, but it will be done. Okay, and then the second one: ln converges to l, or ln minus l, the positive part converges to zero uniformly. To zero uniformly on compact sets in this parameter space, which is the set of X, S, Y, T, and R4 with S less than T and three for any U V in this parameter. UV in this parameter space, which I'll call directed R4. So for any pair UV, which sits in this parameter space, and any sequence of L N geodesics pi n going from U to V If there exists a unique L geodesic pi from U to V then pi n converges to pi uniformly. Uniformly same. So, I mean, of course, with these geodesics, it depends how you set them up. But if you set them up as functions from S T, from the interval S T to R, then you can talk about this as uniform convergence. And this condition that if there exists a unique L geodesic, so for any fixed UV, this holds almost surely. So this is sort of a, it's not possible to get this simultaneously. It's not possible to get this simultaneously for all pairs of points u and v, but you can get that this holds almost surely for every fixed point. I'll mention about this second condition. So this first one says that, okay, they're close on rationals, and you want to upgrade that to something stronger. So this second thing says that ln is converging to l uniformly, except possibly ln has some holes. Okay, and with these, and it's not really possible to get rid of. And it's not really possible to get rid of the holes in this framework. That's kind of a difficult thing to do because your metric is maximizing, is choosing maximal paths, maximal distance paths. And so holes somehow aren't seen by points nearby. They're only seen, large spikes are seen nearby, but holes don't get seen nearby. And so that's why it's harder to get rid of that condition. And I want to any time to really say anything about the proof. Anytime to really say anything about the proof, but I'll say you can apply this theorem to exponential last passage percolation, TASEC, geometric last passage percolation, some version of it applies to Poisson last passage percolation and Brownian last passage percolation, where you can check this integrable Airy Line ensemble convergence assumption. Okay, so I'll stop there. There are any questions? So perhaps we can all unmute and thank Duncan. And we have time for questions for a few minutes. Can I ask one? Yes? Yes. So two questions actually. First, did you try to look at the same problem in pattern avoiding permutations? I saw that recently they look at So that recently they look at this longest increasing sequence on some pattern avoiding promotation. Maybe there is something interesting also there? Yeah, my guess is it wouldn't like the pattern avoidance is going to introduce. So I think they have cases where the fluctuations are still Gaucher and where other cases where they are not. So I think it's really model dependent. Yeah, so I mean depending on the type of pattern you want to go. Depending on the type of pattern you want to avoid, so I mean, you'd need, are there cases where it's Tracy Wittam fluctuations? I think so. Yeah, I mean, yeah, I think we haven't looked at it. Again, it's going to be, I can certainly, like for certain patterns, it's definitely not going to hold, but there might be some, but I don't know if I could say that. Yeah, I would expect that for some pattern, you have a different function in the limit. Yeah, for some patterns, you definitely have a. Yeah. Some patterns you definitely have. Yeah, like and another more simple question: do you have simulations of the function? Balance, do you have simulations available? At some point, people have simulations. Exactly. I have simulations, but maybe not what you think. No, no, of the limiting function? No, I don't have any simulation. No, I don't have any simulation of the function. I have simulations of the distance function. And do you have a rough idea how it would look like the function? This is the picture that I draw. And this is my Brownian bridge. So maybe you think it looks a bit smoother than it does for the Brownian bridge. I mean, you know, it's like a rough function between two points. Like a rough function between two points. Okay. There are projects in process about investigating various properties of it. Yeah, yeah. I imagine that there is much more to know about this function, right? Okay. Yeah. Very nice, by the way. Thank you. Thanks. As you know, if you take Poisson points in a higher dimension, you get an obvious generalization of the longest increasing subsequence problem. And something is known about the distribution of L in that case, although the actual mean, the constant in front of n to the one over. In front of n to the one over n to the one over d is not known. I don't know whether your methods would help with that, but. So I would say in terms of methods, like there's there's obviously this huge integrable input that we're using as a starting point. There are you know some things that we're, you know, we have like this directed metric framework, which is more general and doesn't require any planarity, but you know, in order to But in order to have any sort of strong theorem here, I mean, you could come up with, in order to have any sort of strong theorem, you're going to need, we rely pretty heavily on planarity and integrable inputs. Yeah. Yeah. But it was very nice. What you have, Dad, is really very nice. I like it. So, may I ask a question? Question? Yeah. Yeah. Okay. So, can you say anything about the distribution of the value of this bridge at a single point, for example, at one half? Yeah, so the distribution, I mean, you can this distribution at one half. So what you see is at one half, what you're doing is Is at one half, what you're doing is you have like distances from this point zero to the center, and distances from the point one on the other side to the center. And so, really, you have a rough function here and a rough function here, and you're looking at the maximum of the sum of these two functions. But both of these are just ARIE2 processes. Okay, so these are two independent ARI2 processes after a parabola has been subtracted. And so what you're looking at is the argmax of, you know, these, you're looking at the argmax of the sum of these two processes. And so while you don't have exact formulas for it, you can certainly, like, for example, you can get precise tailbounds on the probability that the point deviates far away, things like that. DV is far away, things like that. And this is sort of how you end up with this older two-thirds estimate is by doing tailbounds like this.