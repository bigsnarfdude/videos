It's on on the side of the that's in front. No, no, behind your other arm. Look at your other arm. There you go. Are people going to sleep? Okay. Good. So welcome to the last session of the day. So we've been talking a lot about likelihoods and so on, how we built them. We kind of washed over the fact that in particular physics and in the other fields of physics, we actually Physics, we actually don't have a clue what the likelihood is and we cannot calculate it. And so, this leads to this field of likelihood-free inference, or we will sometimes call simulation-based inference. If you have an effective simulator can generate data. And so there has been a lot of activity recently with the lights of machine learning because we can particularly nicely combine machine learning with trying to solve a situation where you don't have a luggage. And yeah, so I'll hand over to Anne. Most of the electric frequency literature is Bayesian, and so I was very happy when I found out that there was also a frequency version of that. And so yeah, take it away. Thanks so much. Alright, so I'll talk about our group's work on what we call likelihood frequentist inference. We're creating a general inference machinery. And it's the last session, so to be on good terms. On good terms. I'm going to skip the first two slides and just go back to what likelihood-free inference is. So, as you know, the likelihood relates your observed data to the internal parameters of your theoretical model. And if you can evaluate the likelihood, then you can do frequencies, do Baylor inference, or you can use analytical or numerical methods like MCMC. Methods like MCMC. So, in some settings, the likelihood cannot be evaluated. So, it's intractable. But it's implicitly encoded by your simulator. So, it's not really likelihood-free, it's just that it's implicitly encoded. And what you have, you can, under different parameter settings, nuisance and main parameters, to simulate observed data. So, you basically Observe data. So you basically can create these samples or pairs of data maps. So inference in that setting is referred to as likelihood-free inference. So for a long time, likelihood-free inference has often meant the same as approximate vision computation or ABC for short. So just like a quick, I won't talk much about this. But the sort of simplest algorithm puts a prior on the parameters. Prior on the parameter space, and then you simulate that prior, and then you compare your similar data to your observed data. And if they're sort of close enough, usually you look at summary statistics, then you retain those parameter values. And often it's iterative, you end up with a sample of parameters that's an approximation of your posterior. Okay, so more recently. Okay, so more recently, researchers have been using machine learning algorithms to directly estimate key inferential quantities from simulated samples. So you might be estimating posteriors, likelihood, or likelihood ratios. So the advantage of these sort of training-based approaches are that you can leverage different machine learning statistical methods to handle more complex, high-development. To handle more complex high-dimensional data without a prior dimensional adoption. So we might not reduce it to certain features. And the other is that this provides amortized inference, meaning that once you've trained your model, you don't need to repeat your training again at the inference stage. So we don't have this iterative scheme. Okay, so that's all good. So what's missing then? Missing them. So often the goal is that if you haven't observed data, we'd like to constrain parameters of interest using your assumed theoretical models. You theoretical means your simulation model, and you might see different, this is Kicking Fuckel's models, you might see different kind of region estimates like this. But then the question is: does your region estimate of your parameters? Can you say with certain probabilities, certain confidence, that the true parameters are included? True parameters included in those sets with, for example, confidence 1 minus alpha, no matter what your true parameters are. Because you want conditional coverage everywhere of your entire parameter space. And there's actually a shortage of practical inferential diagnostic tools with final sample guarantees of conditional coverage. So, most approaches actually rely on They actually rely on asymptotic assumptions like Wilkes' theorem. They may not assess validity across the entire parameter space. Sometimes they might rely on costly multi-colour simulation, for example fixed parameter settings and so great. So they don't scale very well. So what we've been doing is to build an inference machine or a unified inference machine. Machinery, unifying inference machinery that provides valid inference, so confidence sets and tests with final sample guarantees, practical diagnostics that checks your coverage over your time parameter space, and our goal are modular and efficient procedures that can leverage generative predictive procedural algorithm and that's compatible with any test statistic you want and any priors. The results should be independent of priors. You can use your prior. Independent of priors. You can use your prior to get higher power, but you should always get validity in the public. And yeah, so we have, okay, I'll let you fast. But so the code is public and you're welcome to contribute. It was a GitHub link there. And we also posted a series of papers in the archive. So this is one of them that's called Likelihood Free Frequency Inference, or LF2I for short. For short. And the main idea is super simple. It's basically based on the equivalence of test and confidence sets. Old result by Newman from 1930, that constructing one of my alpha confidence set is equivalent to testing hypothesis tests for your parameter theta, for your entire parameter space at level alpha. So you have to control the type of level alpha. And what you need, the ingredients. And what you need, the ingredients are: you need data from your theoretical model, so you can simulate that. That's the assumption. You need to define a test statistic, and you also need to know the critical values where you can control your type 1 error for testing. Okay, so just a quick reminder of how the Norman construction of confidences work. So, for a fixed data, you basically find the critical value that controls the type 1 error. So this is the probability rejection can be no larger than alpha. And this critical value, it depends on theta. And this flow of the solvent line basically shows you the acceptance region. Region. And in theory, for the Neumann construction, you need to repeat this for every theta in your parameter space. So to the right, the solid lines basically show the acceptance regions for different theta. It's just a schematic. Suppose now you observe your data and you compute your test statistic here and the key idea of the normal confidence. The key idea of the normal confidence set is basically that the confidence set is defined by the set of parameter values where your test statistic is an acceptance region. Okay, so that sounds like a lot of work because you have to go through your own time parameter space. And I think that's the reason why normal construction hasn't really been used that much in practice. Used that much in practice. So there are two challenges. One is the normal construction itself. As Roui Lyon writes in Open Statistical Issues in Particle Physics, in practice, it's very hard to use the normal frequencies construction when more than two or three parameters are involved. Software to perform a normal construction efficient in several dimensions will be most welcome. The second challenge is the validation of reproduction. Challenge is the validation of frequency coverage. As Bob Cousins writes in Lectures on Statistics in Theory, Prelude to Statistics in Practice: A complete rigorous check of coverage considers a multidimensional grid of all parameters and for each multi-dimensional point in the grid generates an ensemble of toy month to call suit experiments or maybe parametric boost travel as you as you call it. I call it, runs the full analysis procedure, finds the fraction intervals covering, the parameter of interest, and so on. But the ideal of fine groups is usually practical. So people were thinking, like, how do we turn the Norman construction and validation to practical procedures? So the thing to realize is that the Norman construction requires one to test these hypotheses. Hypothesis for all parameters in your parameter space. But the key insight is that test statistics, your critical values, and also if you want to check your coverage of your constructed set, they're even conditional distributions. So they're functions of the unknown parameters. And in many applications, they vary smoothly across the parameter space. So basically, what we're doing is that Doing is that rather than running a batch or Monte Carlo or bootstrap for basically every raw hypothesis on a finite grid, so why don't we just simulate pairs of theta x over everywhere? And then we just interpolate across the parameter space by leveraging training-based machine learning algorithms. So we basically use prediction algorithm and posterior algorithm to do inference. To do inference. I mean, that's the point. You can use preceding prediction to do classical inference. And this is basically our inference machinery. It's completely modular. So we have different branches. So one branch, the middle one, generates a sample. And then from there, we estimate test statistic and the different choices that might give different results. And another branch estimates basically the critical values or P. Estimates basically the critical values or p-values for controlling the type 1 error over the entire parameter space. So that gives us a normal confidence at. And if you want, you can also independently generate the third sample, use a held out, simulated sample to do diagnostics, basically checking your conditional coverage over the parameter space. So, and it's modular, so you can just switch and pick what you want. Switch and pick what you want. So, when it comes to test statistics, we work with different ones. They're all leveraged machine learning or AI classification prediction algorithm. So, today, I'm not checking time. We'll talk about two test statistics that are based on classification of odds, A Core and BFF. We also recently worked on what we call Waldo, that instead uses prediction of That uses prediction or posterior estimation to construct three points testing or frequency confidence. So that's a separate paper. Okay, so let's look at the center branch. So here we estimate test statistic by estimating, by classification, by estimating how. So we used a sort of density ratio trick that you might be familiar with via probabilistic classification. Via probabilistic classification. So we basically simulate the sample from your theoretical model. Then we simulate another sample from reference distribution that dominates F data everywhere. So it could, for example, be the marginal distribution for the feature space. And we call one sample class one and the other from G. The reference is made class the. From G, the reference is made class 0. Then we define the odds. That depends on theta, so it's just one parameter, it's not two parameters, like in call. We define the odds as the chance that your data was generated from your theoretical model rather than your reference distribution. So this is a very fast computation. Depending on your probabilistic classifier, you can work with potentially very high. Work with potentially very high-dimensional data, like images or sequences of images. And then we define the test statistic. So suppose we want to test that null hypothesis that theta belongs to parameters space big theta zero that can be composite or simple versus that total, which is a complement here. So for observed data, we formed Data, we form the product of the estimated odds that we get from the probability classification. And then we here define two different test statistic. One is the A-core by maximization. So it's the log of the ratio of the soup of the alt product for the null versus the alternative hypothesis. And the RSVFF, which is the Which is the ratio of the integrated Alts product for the low versus the atomic. And these expressions after today's talk might kind of look familiar to you. So we can show that basically if your probabilistic classification is well estimated, and you have some idea of that, but Then the A coal statistic is just the likelihood ratio test statistic. So the log of the ratio of the soup of your likelihood. And the BFF, well, what's the Bayesian frequency factor would be an approximation of the Beiges factor. So it's the ratio of the integrated likelihood. And note here that the Beige factor is heat used as a frequency tester this day. This day. But yeah, we're going to elaborate on some of the advanced use of the vision. Of course, for the Lohmann construction, we only consider, only need to test theta is equal to theta zero. And if you don't have any nuisance parameters, it's basically testing a simple hypothesis versus composite. So the test statistics, they simplify. And you just have a supernova integration in the denominator. Integration in the denominator. Okay, so that's how we estimate test statistic by estimating odds through probabilistic classification. Now we need to, you know, actually make sure that we have valid confidence or valid tests. So we need to estimate the critical value. So we do that from another second simulator sample, a health out sample. Okay, so suppose the right figure is. The right figure is the null distribution of our test statistic for theta zero. So we want to find the cutoff so that the tail probability in blue here is no larger than alpha. So basically, this critical value, as I said, it's basically the alpha quanta, and we can estimate that by a quanta. Estimating that by a quantile regression of your test statistic versus your parameter space. And here again, you can leverage the tools that you might tell. And the test statistic is from the first branch. So, and here's kind of like the main thing. It's all analyzed. So if you estimate your test statistic across the entire parameter space, you estimate your critical values by quantile regression of your entire parameter space, then you Entire parameter space, then you get the normal confidence edge by just comparing that. So once you've done the estimation, then you get this automatically. So there's no iterative process. Okay, so and you can show, maybe not surprisingly, that if your quantile regression is consistent and if your signal to sample for the quantite regression is large enough, then you can basically construct confidence that. You can basically construct confidence that's guaranteed nominal coverage everywhere, and that's regardless, I mean, regardless of the observed sample size, and this is regardless of your test statistic. So even if you don't estimate your test statistic well, you get frequent discoverage everywhere if you do your content reversion well. But power depends on how you define your test statistic, how well you estimate it. So it's two different things. Well, it's not different, but validity depends on. Validity depends on your calibration. And the last thing here is: okay, so we want to see, I mean, this is what we guarantee in theory, but how do you actually assess your empirical coverage everywhere, right, without doing the amount of cover bootstrap, which is computational, not feasible, high-parameter dimension. So, your coverage, I mean, by definition, I mean, by definition, is your probability that theta belongs to your confidence set as a function of theta. And this is just the same as the expectation of the indicator function that I think that belongs to your constructive confidence F. So we can estimate that from a regression, right? It's just a conditional mean. And that's what we do. So we basically create the So we basically create the third set for validation. We sample things everywhere. We get the data. We construct the conference set. We've already done that, so it's amortized very fast. And then we basically see, we compute the indicator function. Does data belong to this constructed confidence set? That's my indicator function, that's my variable, Z. And then I regress Z on data, and that gives me an estimate on purely. That gives me an estimate on periodic cover, and we can visualize that. So here's just an example. So the gas and mixture model is a classical example where the limiting distribution, the micro ratio statistic, is intractable. So here we're just similar data one dimension from a mixture of two normal distributions. Theta is unknown, it's separation. And in one case, the left And in one case, the left plot, so here we are covered as a function of data. Here we use the Leclerc statistic. We thousand on the call of simulations at, it's very expensive, at each data on a fine group, but in one dimension we can do that. And we achieve the nominal coverage. In the center, we just use the Lagov ratio statistic. We assume chi-squared, or Wilkes theorem. That doesn't hold here. So theta zero, we get the nominal coverage. We get the nominal coverage, but if any separation on there, we don't. And it doesn't matter how large your sample size is, we would never cover it. The third one we use our contact regression with the light probation statistic. And we have a thousand simulations total instead of a thousand at each grid point. And it really makes a difference in how you parameter dimensions and which you normally cover there. And here's some examples. Here are some examples of constraint confidences. This is multi-bay Gaussian. We just wanted to see in different dimensions of parameter and feature space that the eight-core BFF confidence sets for order 5,000 simulations that they're close, I mean they have to be valid, right coverage, but also they have to be close in size to the exact likelihood ratio confidence. And yeah, they were. They were. And we've seen that we're still improving the computational aspects. But I think L5I scales well for up to 10 parameters with reasonable similar sample sizes. So I won't talk about the details about this, probably because I don't know this so well. But this is an astronomy application where we follow the galaxy parameters from spectra energy distributions. And we actually And we actually use a different test statistic. We'll basically leverage a neural posterior estimate that gives credible regions, and we've constructed LF2I confidence sets instead. So it's a different test statistic that's based not on classification, but based on your experience. So we created our LF2I confidence sets, and I just want to show that we could look at the coverage across the entire parameter space, so the phi. And in this case, this is for the credible sets with neural posterior. This is for the credible sets with neuroposterior. It looked fine when they looked at the marginal, but if you look across, you can see that you severely undercover in a large proportion of the product space. And this is for the wilder L2I conferences. We do pretty well. And we also worked on different ways of visualizing it because five parameters is hard to see, and you can also see where the neuroposterior parameter regions, where the undercover, and also like. Undercover, and also like how we chief down the cover with wall. Okay, so business parameters. So this is Henrik's slide. So we do pretty well with up to 10 parameters. But as Henrik says, that one issue is that the theory space is not the only thing affecting the data. Every step comes with its own. Every step comes with its own parameters, nuisance parameters. So, how do we handle that? And so, one is like nuisance parameters L of to I, which can handle, I think, like five to ten pretty well. So, in this case, when we form, for example, normal confidence sets, you have to remember the null is really a composite hypothesis, right? For each main parameter, you have to consider all the nuisance parameters. Consider all the nuisance parameters. And yes, I'll wrap up. So the challenge is that you need to compare your test statistics to the cutoffs and the inf of your critical value with all your nuisance parameters. So you need to control the type 1 error at each main parameter for all possible values of nuisance parameters. And the two sort of the ways that Two sort of the ways that have been talked about is one, you do profiling, so you basically reduce your nuisance parameter space to one, right? You just look at it for every main parameter, look at the nuisance parameters that maximize the likelihood, or you marginalize over it. And the million dollar question is for small sample sizes, I'll just say Bob Cousins, 2018, so hopefully he hasn't changed his mind. There's no theorem as to whether profiling and marginalization would give better frequency. Marginalization would give better frequencies cover. But our diagnostics tool can sort of tell us that. So, this is the on-off problem with two channels. Signal strength is the parameter of interest, and then distance parameters were two. So, we can check the conditional coverage everywhere. So, we just like showed where you undercover, correct, covered, overcover. So, in this particular case, EFF hybrid with average or nuisance parameters performs the best in terms of having. This performs the best in terms of having the largest proportion of the parameter space with correct covered versus undercover. But then you can also look in main parameter users parameters where, like the smaller proportion, where you're undercover. But interesting is that where you're undercover, you actually do much worse than profiling for this particular example. So you get diagnostics that can tell you how it works. So last to just wrap up. Last, to just wrap up, we'll basically build the general inference machinery that's modular. So you can plug in any test that you want. It doesn't have to be Waldo BFF and ACO. You can kind of calibrate to get frequencies conference sets. Or you can just, some people, they don't want this at all. They just want credible regions and check coverage. That's fine. Then you can use the dynamo sticks to look at your credible regions and see how well they actually do. Takeaway validity, I think, is actually not as challenging. So we get, you know, conditional coverage everywhere. It's power that's a hard question. So it depends on how you define your test statistic, what prior you use. You can lose quite a lot of power. So I think picking good prior will give you higher power. This is primary diagnostics. Who knows what's best? But we can assess coverage in primal space. In the privacy space. So, kind of projects, just to bleed into Alex. So, I'm working two systematics projects. One is how to construct test statistics, the invariant to nuisance parameters. I'm working with Luigi and Raphael. So, the idea is that if you have invariance, then when we calibrate, we don't need to care about the nuisance parameters. The second is what Alex is going to talk about. It's basically a nuisance parameter LF2I. We works with about five. With about five peramps for atmospheric cosmic ray showers. This is a collaboration with Tomato Dorigo. And I will. Should I pull up your slides? Does it go immediately after? Yeah, no, I think it makes sense to maybe do the talk immediately after, and then we have the long discussion sessions. Does that work? But I need to stop the recording and restart. Is that right? Yeah. 