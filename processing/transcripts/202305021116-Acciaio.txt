So I will connect two kinds of uh bridges. One, they come from equilibrium problem, where we have insider with some additional information, and bridges from Schrodinger. So I will just give quickly, I know that probably most of you know both things, but I will just give a couple of slides on the first kind of equilibria, just to recall one easy example, a couple of slides on the Schrodinger plugins, and then we put them Blue juice and then we pull them together. So, about equilibrium, I really want to just recall the easiest kind, kind pack, which probably most of you have seen already. So, we have one traded asset. We look at the time horizon T1, so we are just interested in what happened between time 0 and time 1. We have a bunch of noise traders, so we can look at their cumulative holdings as being Bronier motion B. And then there is an insider. And then there is an insider who knows, so there will be some release of information at time one that tells at what price the asset would be traded after that time. So we think that the insider already has this information at time zero, meaning that he knows the time one value of the asset S1. And this is independent of what the noise traders do. So we have this S1 knowledge for the insider. He's a risk-neutral guy and he will decide on the holdings in order to. Decide on the holdings in order to maximize just expected value of the terminal. Well, so that's his objective. And the third actor in this market is market maker. So also this neutral, they observe just the total demand, so I cannot distinguish what the two kind of traders do, but it just observes this process Y, which is the noisy holding and the holding on the insider. So it just observes Y. The insider, so it just observes why, and they will have to set the price and sell or buy in order to clear the market. So, what do we have as a tiny assumption? So, let's assume that S1 has no atoms, because we have then easily a strictly increasing function such that we can see the value of S1 as an increasing function of a normal 01. Okay, so we will become clear while we have this assumption. So, the idea is that now the market makers have The idea is that now the market makers that set up the price observe just y. So with the assumption that then the asset priority at y is a sufficient statistic to set up the price at time t, so assuming that we have this function h that just depends on the time and on the total orders, in equilibrium what we'll be left is that this function h satisfies the ether condition, right at time one. It will have to give me the price of the asset at time one, and very nicely. And very nicely, we have a very simple bridge, right? So we have the total holding, right, in equilibrium will be the noisy holding, and this second guy here, the second guy are the holdings of the insider. And the holding of the insider is a very nice bridge rather than time one pushes y one equal to n. So this is a very quick recall of what happened in this Kalbeck model. And Kuipeck model, and you know, just to look at what happened in the insider filtration. So, we can consider this n function, so this n variable, right, so this guy here, which is a normal 0, 1, of course we can see it as the time 1 value of a Brownian motion, right? So, for me, this n now becomes z1, where z is a Bronyan motion. So, we have no problem with that. And we have that because S1, right, which is a strict increase in function. Strict increasing function of n was independent of B, of the ordering of the noisy trailers, we can take that this Branier motion Z is independent of B. So now I have these two independent Branium motions, Z and B. So Z is the signal, right, of the insider that already knows Z1 at time 0, and we are the whole thing of the noisy trailers. Now, in principle, if I look at, if I want to tell you, okay, what is the filtration of the insider, right? I would just have to say that it is Fb plus. It is Fb plus the information of Z at time one, but actually it doesn't change anything to give the insider also the information of this signal that we built, right? So this Z is the fake thing that we built. So also including the filtration of Z doesn't change anything in the optimization of the insider, so we can put it in, right? So for us, this would be the filtration of the insider. And as we saw before, right, we have bridges, which means that now in the filtration of the insider, this is how the two processes This is how the two processes look, right? The signal process and the total voltage. Okay, so we have this super nice, very easy bridge. And now, let me just recall in a couple of slides about the Schrodinger bridge. And then I want to see how these guys can be seen as a solution or approximated by a solution of Schrodinger bridges. So then bear with me for a couple of slides about Schrodinger. We already saw yesterday morning, we already had. Saw yesterday morning. We already had a nice discussion after Camillo's talk. So let me quickly recall the setting. So we will look at omega being the space of continuous function, right? So this is my time horizon 0, 1. In general, we can take the dimensional processes. I call X the canonical process. And in order to set up the Schrodinger problem, I need a reference measure and target distribution. So reference measure would be a measure R on the path. Would be a measure R on the path space, right? So a probability on the path space, which usually is a D-dimensional. So here we are in RP, granular motion, possibly starting at some random distribution. And this is just notation. You all know what is the relative entropy. I will look at the problem of minimizing the relative entropy of some other probability on the path space with respect to R, which of course is finite if and only if P is absolutely continuous with respect to R. With respect to R, and I will have a constraint, right? I want to minimize this guy, but I want the constraint on P, right? So I want that at time 0 the marginal of P is mu 0, and at time 1 is mu1. Meaning that this is the problem we are looking at. So Schrodinger problem, or let me call it dynamic, because in a moment I will give the static one. So the Schrodinger problem tells me that I want to minimize this entropy, right, over the probability here, continuous function 0, 1, with this. Function 0, 1 with these two prescribed marginals. So now we see, okay, we are a path space, I want to minimize the entropy part of this law on the path space with respect to, let's say, D-dimensional Brownian motion, but I'm always only imposing condition at time 0 and 1. And then you say, well, maybe we can just restrict to look at what happened at time 0 and 1, right? And if this is your intuition, you are right. So let me introduce, we will see this in the next slide. So that's why I need what is called static shift. Why I need what is called a static Schrodinger. So now, here when I put the number 0, 1, I mean that I'm looking at the, so R is a distribution on the path space, but I'm projecting and just looking at the distribution time 0 and time 1. Okay, so this is my R01. So we call the static Schrodinger problem. The minimization now of the entropy, so here I'm not looking anymore at the path space, but at lowest rhythm low at what happened at time 0 and time 1. 0 and time 1. So this is the joint law right at time 0 and 1 of the reference process. And this is this now is a law just that I'm take d equal to 1. So just a probability on R times R, right, times 0 and time 1, which matches the two marginals. And now I'm reduced to this minimization problem just at 2 times, see the entropy minimization at 2 times. Why did I introduce this? Because actually, after actually after one after well let's let's put those part we will have a theorem that tells us that we can go from one to the other and this is the only thing that we need to use so we need to use the following here whenever i have a measurable function f that acts on probabilities in the path space i can decompose the entropy of any probability on path space with respect to r as entropy of the push forward under this f of the respective measure and some mixture under the push right of the Some mixture under the push, right of the conditioning. So, this would be the general force. Let's just concentrate on this one, which is simple, it's the only thing that we use. So, if as a measure on the path, I just take that I project on the initial and final point, right? And if I denote by qxy the bridge of a probability q, right, starting at x and ending up at y of this path. So, using these things above, I have the following. So, I have the entropy of, again, The entropy of again in R of V on the task space with respect to R, which is again on the task space, can be decomposed in entropy of just the coupling of initial and final, right? Plus, now here what do I have? I have P, so the probability at just a final initial time, and here I have the entropy of the bridges of P given the bridges of R. Okay, so I'm looking at the path that start at X and end up at Y, right, under R. And then I'm at Y under R or under P. I'm taking the bridges. So, why do we look at this? Because now we have this nice result by Firmer that tells us that actually, looking at the solution of dynamic or static Schrodinger, right, it is a one-to-one problem, because they both admit at most one solution. They are strictly convex, so either they don't admit solution or it's unique. But in the moment that one of the two admits a unique solution, also the other one does, and we really know how to pass from. Pass, and we really know how to pass from one to the other. So if I have a solution of the dynamic Schrodinger that's going to be head, then I find the solution of the static Schrodinger just taking the projection at time 0 and 1. Vice versa, if I have a solution of the static Schrodinger, which is a coupling between 0 and 1, how do I build the dynamic one? I take this solution and I concatenate with the bridges of the reference measure. So, just to say there is a really easy way to go to one to the other, right? So, if I want to solve this minimization of the entropy on the path space, actually all I need to do is finding, solving the static case, which is, if you want to transport, right, of mu 0 into mu 1, minimizing the entropy. Okay, very good. So, one last slide on Schrodinger, and then we connect it with the equilibrium in the insider case. So, this last slide. Case. So this last slide is to show how things look, how the solution looks like, because we will find this now in the rest of the talk. So how do the solution look like in the static and dynamic Schrodinger? We will have that under minimal conditions, we can write the solution, so this is pi hecta the solution to the static Schrödinger, in terms of the density with respect to the reference measure, in this product form, right? So I would have some function f that just So I would have some function f that just depends on the coordinate at time zero, a function g that just depends on the coordinate at time one, and the optimal solution is a product of the two. And this transfers to the solution of the dynamic string, and we'll have the same form. So this x, right, is a canonical process. So this is x, a process from 0 to 1. So in principle, of course, the density of this measure of redist depends on the whole path. But actually the solution will be a function of the initial state. A function of the initial state times the function of the final state. But now let's write it like this because this will like it more and we know how to do a change of measure, regular summary, and so on. So we know that, right, for a change of measure, we like to look, so let's forget from alpha x0, we like to look at some stochastic exponential. And the nice thing that you already saw comment a bit yesterday on, so what is now here the drift? So this intrinsic drift of Intrinsically of p-hat is the gradient over the logarithm of the H function, right? So this P hat will be this H transform in the sense of loop, right? So if you want what you can do here to find the drift, you look at this function g, so the density here at time one, right? And then I take if you want the conditional expectation under R. So there is a very easy way to construct this h function, meaning that I have a very function, meaning that we have a very easy way to build the drift. So this is the rule that tells us how to find the optimum p-hat solution of the dynamic Schrodinger region. And this means of course then we also know thanks to what we have seen before, we know how to write the entropy, the optimal entropy. It's a curse of. We know how to find the optimal entropy and this is like Entropy and this is right just entropy of the initial measure with respect to the reference initial and then we have this expectation of the square of the tweet. Okay, so now this wants to be a quick recap. I'm going to have been quick enough. So now we have a quick recap of this Schrodinger problem. So I have this path space. I want to minimize the entropy of some measure. I'm fixing initial and final. On the other hand, at the very beginning, I said what I want to do is to I want to do is to find a way to write a solution to this Keilbeck model. So let's go to the Keilbeck model. So if I want to see it on the path space, right, so trying to put it in the setting of Schrodinger, I would say, okay, now my path space in between 0 and 1 are 2, right? Because now what do I have? I have two processes. I have the process Z and the process Y. So what I want to do here, I want to look at those, at the joint law of Z and Y, which gives me the solution of the final. Which gives me the solution of the child back y, because I have the insider signal and the total bolding y. So I want to look at those two, remember that y is a bridge with respect to z, at this couple, as a solution or almost a solution of a Schrodinger problem. So let's then try to understand how we can do that, right? So this two-dimensional process, this coupled, I know what is the initial and final law, right? Because at the beginning, that loss was just. The beginning, that loss of generality, we take that they both start at zero. At the end, I know that they are the same and they are a normal zero, one, right? They are each a Branium motion in their own filtration. So I know what is the joint flow of these guys at time zero and time one. Okay, so zero, zero, and again, this guy is just, they are both normal, zero, one, and they coincide. Then you can think, okay, maybe we can try to see if, you know, to find this guy, right, to compare him. This guide, right, to compare it with some reference measure, because now I have the target measure and try to see what is the minimal entropy with this constraint. Now the problem, and this we come back to the discussion we had yesterday, is that this mu1 is singular with respect to the law of the reference measure, the two-dimensional branding motion at time one, right? Because this guy is like on the diagonal, right? So the two are the same, and the normal zero one. And the normal 0, 1. And this R1 is a two-dimensional burn motion. So this is a normal 0, 1, right, independent normal 0, 1. So meaning that the distribution that we have at time 1, fixed, is singular with respect to the reference measure. Meaning that with this target, right, distribution mu 0 mu 1, the Schrodinger problem is ill-posed, right? Because I don't have absolute continuity. So if I look at this entropy, this is infinity. So what we are going to do, right, we are going to say, well, Do, right? We are going to say, well, I know I want to target this guy, but let's add a bit of noise so that now I diffuse it and it is absolutely continuous with respect to this thing. We solve it approximating problem, and then we take the ring. Three, five minutes. Okay, so this is again just to give you the idea. So, what do we do? So, now we have this x1, x2, canonical process. I'm going to build this x new 1xi. Be this x new one epsilon, right? Absolutely continuous, so it doesn't matter now how much, but this is like how explicitly. But the meaning is what I say before. So we add some noise, right, in a way that now this mu1 epsilon is absolutely continuous with respect to the reference measure and weakly converges to what we want. Okay? So that's it, we say, okay, then that's called excellent Schrodinger, the minimization of the entropy when I'm now targeting this fog. Targeting this fogged final distribution. And with the classical technique that you would use to solve a Schrodinger problem, you can prove that now that this is like the two-dimensional SD that comes out of our problem, meaning that now the law, the distribution of this guy, is exactly the unique minimizer of the Epsom-Schrödinger problem, right? And you have that exactly here the density, right? The density of the optimizer is exactly this H transform. And in terms of time, I'm just going quickly. So, one remark: we can prove this, give another proof, an alternative proof of the solution of Schrodinger problem by relying on progressive enlargement of it fraction. So, we can rely on either or standard argument or trying to understand the relation between z and y and the conditional probability in order to find the solution. In order to find the solution, but I'm going quicker because I was of time. So, this is the point that I wanted to come. So, now I have the solution. I write, I put some extra noise in this new one target distribution. I know that we can solve them. And now we just let epsilon goes to zero. And then we have the distribution that we have seen before, right? This epsilon. So here, right, we solve this approximating. Approximating Salmon-Schrodinger problem, we have this distribution, you can prove that they are tight and that they converge, and they converge exactly to the distribution of what is the joint law of Z and Y in the Keybeck model. So meaning that you have now a series of epsilon Schrodinger problems that you can solve, and you know that the solution of this distribution converges to what is the solution of my equilibrium problem with the insider. equilibrium problem with the insider. Of course here we have done the Babby case of random motion. Instead of Z being the random motion we can take any possibility kilt right at some level L here. Time in homogeneous diffusion. The only thing, all the messages are like the same, but they're just much more technicalities. So I wanted to put it here, but we can keep in mind that we can deal with many more general equilibrium cases. And on the last slide, Cases and on the last slide, what we are doing now, we are looking at the case where the insider instead of getting at time zero the price of the asset at time one, right, which was this Z1, right? So now the insider just gets some on the way, right, some signal, Zt of some process that will have at the terminal time the same value of the asset, right? So S1 will still, Y1 will still be equal to Z1, but I don't know this in advance. To Z1, but I don't know this in advance. I get some signal on the way. And here, the nice thing is that mathematically, when I try now to say how is this related to Schrodinger, now I have a constraint on the drift that the insider can put, right, in its filtration to Z. So it cannot be the good just before. So mathematically this, we are now looking at PD methods. So there are other ways, but we definitely cannot use the classical Schrodinger. So either we use some constraint Schrodinger or look. Schrodinger or look at PDE methods. And then, of course, okay, there are still many things that we want to explore because now we have connected those two things, and because the Schrodinger problem can be recasted as an entronic optimal transport or a weak transport, all things that now are developed, we can try to use this mathematics developed here in order to tackle equilibrium problems. And we can connect to many other paper, beautiful papers that they developed this entropy. Was that they developed this entropy and entropy?