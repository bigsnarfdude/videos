Thank you very much. And thanks for the invitation. So what I'll be talking about is asymptotics of one of the many parameters in our models that is called overflows. This is based on joint work with Raul Goed, who is at the University of Chile, and Jacek Vesowowski, who is at Warsaw University of Technology. Warsaw University of Technology. And so, what we are, I'll describe it in more formal and more detailed way in a second, but we'll have a collection of finitely many or infinitely many containers and we'll be putting balls into them. Assumption is that each of the containers has a limited capacity, say R. Analytic capacities, they are. And if a ball that arrives, it is to be placed in one of the containers, chosen at random, but according to maybe non-uniform distribution between the balls. If that container already contains R balls, then the ball is placed into an extra basket that is called an overflow. Okay, and so we'll be interested in the size of the overflow in general. The overview in general. To deflect Jim's questions, we are not driven by any particular application, but this thing has been looked at. So in particular, in volume 3 of Knoop's book, there is a section on external searching, and it discusses the size of the overflow in the context of economic algorithms. When r equals one, so this is particularly sort of I'm sorry. Sort of, I'm sorry, this is particularly frequently encountered case. So when r is equal to one, this is the number of balls that falls into already occupied orbs. And this is often referred to as the number of collisions, and it has been used in, again, for example, in some algorithms, testing for how good the random number generative algorithm is. This is discussed in KNU, but it's also been used in statistics and some other things. Statistics and some other things. So, in particular, Ramakrishna and Monahan, so these two are statisticians, they gave two independent calculations on probability that there is no overflow under the uniform assumption. So, most of those things are done under uniform assumption. Uniform assumption means you have a finite number of boxes, and each box is equally likely to be chosen for the box. Chosen for the ball. But this is not our requirement. We'll be talking about general distribution. Some boxes will be more likely than the others. And on the other hand, Dupois, Newsman, and Whiting, they sort of estimated the opposite extreme of probability of having unusually large overflow. And finally, what I want to mention is that Huang and Svanteans. That Huang and Svante Jansson had a paper in which they gave sufficient condition in the case when R is one for the limit of this overflow to be Poissonian. I want to say that the paper was not about that. The paper was about something different. It was about proving the central limit theorem for the number of occupied boxes in the general. Boxes in the general OR model with as weak assumption as possible, which is that the variance goes to infinity. So they prove that as long as the variance of the number of orthographic boxes goes to infinity, the distribution is normal. And as a side remark, they gain a Poisson limit for this in the case r equals one, and that's sort of what got us started, sort of. Okay, so more formally, so what we'll have, we'll have So what we'll have, we'll have n experiments. So for every n, we'll have n IID random variables. And we'll have a collection, as I said, finite or infinite of boxes. And P and M is the distribution is the probability, they are IID. So this is the probability of putting a ball into M's box. So what we want is, so the want is so so that now here are the definitions so we let n k m to be the number so this is some of the indicators that x and j from j up to k minus 1 went in box m so this is the number of balls landing in the nth number of balls landing in the nth box among the first k minus 1 okay and as I said r is the capacity and then y in case is is the indicator that Case is the indicator that the cave ball goes into overflow. So this is then represented the sum over all boxes, indicator that you want to place the cave ball in the n box, but the n box is already full. And then the size of the overflow is just the sum of those indicators. And we want to start. We want to understand the behavior of this random. Want to understand the behavior of this random problem. Alright, so okay, so in particular, what we'll do, I'll give this try two regimes. One under one, the limit is Poisson, and under the other one, the limit will be Gaussian. And those regimes, so there are two parameters that drive the situation. So one is, so P and star is the maximum probability, okay, so this is the most likely box. The most likely box. And so one of the parameters is n times p and star. And the other one is this quantity n to r plus one times sum over all boxes, probabilities of getting into that box to the power r plus one. Okay, so for notational difference, for notational convenience, this sum will be known as expectation of pn to r, okay, which is, I mean, this is expected value of the random variable that takes from the takes value p. Takes value Pn M to R with probability P and M. So that's what it is. Okay, so all right. And okay, so here's the regime for Poissonian behavior. So if this quantity has a positive limit, finite positive limit, then of course you can always write the limit in that form as r plus one factorial over some positive mu. one factorial over some positive mu. And if n times p n star goes to zero as n goes to infinity, then the size of the overflow is Poisson of parameter mu. All right, so that's for the Poisson limit. I won't be talking much about proofs, but what I so okay, so before I go to that, so here are examples. So for example, if you have So here are examples. So for example, if you have a uniform case, so if you have Mn, M sub N boxes, and you uniformly distribute among those boxes, then if say M and M are related like this, Mn is roughly A times fixed A times N to R plus 1 over R. So you have that condition. This NPN star goes to 0 and this quantity goes to 1 over A to R. So the conclusion is that the resulting random variable is asymptotic. Variable is asymptotically Poisson with that parameter. If, on the other hand, you consider a geometric case, so probability of landing in the J box is distributed like geometric random variable with, say, parameter with parameter Pn that can change from n to n. Okay, and again, if you relate n and p by by this formula, that p n is a times n to r plus one over r. A times n to r plus 1 over r, then again, in the case of geometric, of course, pn 0 is the largest one. This goes to 0, and this goes again to a r over r plus 1. So again, you get Loisson limit this time with this parameter. So the way we formulate it, it's very natural to consider conditioning, to consider sort of martinger-like scenario. We look what happens. Like scenario, we look what happens to the first k minus one ball, then what happens to the k. So, if you consider what happens to the k minus one, it's sort of you can get your hands on upon this conditioning. So, so this is, you know, very, our approach is very well suited for, you know, conditioning and marking it types of arguments. Arguments. What I want to point out here is one of the, along those lines, criterion for the convergence to Poisson, which is due to Beshka, Popotovsky, and Swominsky. This is now 40 years ago. So what it says is if you have a rectangular array of non-negative random variables, which is adapted to an increasing sequence of sigma fields, okay, and if you have those three conditions, Those three conditions that expected the maximum along the row of conditional expectations of y and k given f and k minus 1, this goes in probability to 0. And if the sum of the conditional expectations goes in probability to some positive eta, and you have another condition, namely that for every epsilon expected value, the conditional expectation of that, but Expectation of that, but on the set where y and k minus 1 is exceeds, the difference between them exceeds epsilon, this goes in probability to 0, then this, then the sum of y and k goes to a sum with parameter eta. And I'm sorry, I just realized that there is a collision of notation. y and k are not the y and k I described before. These are just generic random variables. This is a sequence of random variables. One, so this is sort of like So, this is sort of like the conditional version of Lindeberg's central limit theorem: that you have a lot of conditional expectations and conditional variances and things like that, except that it is suited for the convergence of Poisson and normal. One comment I said about this that I think is interesting, that most of the criteria for the convergence to Poisson random variables, they require that the random variables are actually indicator functions. So, most of those conditions. So, most of those conditions are for zero, one-valued random variables, for sums of the indicators. This one formally is not. Okay, of course, if you look at this, this condition is hidden here because what this condition says, okay, this conditional expectation sum converges to positive eta. If you introduce that indicator, then it has to be vanishing. And of course, if you have an indicator, then this product is identically zero. Okay, because either zero. Okay, because either if this indicator is one, that means your random variable is zero, so that is zero. And if it is zero, then that means that this is zero. And so this is one. So for indicators, this is identically zero, so it's hidden there, but it's not formally required. So yeah, so I think this is an interesting criteria that I said is quite old, but it doesn't need to have been used. All, but it doesn't seem to have been used for quite extensively. So the second thing is the convergence to normal. So this, so I will give you, so okay, so maybe convergence to normal. So the assumption is that now I have a lime soup of this n p and star. This is just finite, okay, can be positive. And if n to r plus 1 times this expectation. n to r plus one times this expectations goes to infinity, then when I take my overflow and normalize it in the usual way for central limit theorem, so I subtract the expectation and divide by the square root of variance, then there is a convergence to start at normal level. So now a thing that I'll come to later on that we actually, so I have an idea. So, why haven't I written some specific expression instead of this expected value? Because we actually don't know what this, we don't know what this expectation is. So, all we have is we have upper and lower bounds. So, this expectation is of order n to the r expected p n to the r. So, so this is sort of expected value, as is the balance. But we only have upper and lower bounds for those ratios. Bounds for those ratios. And as I said, I'll come back to this. Let me just say that if it happens that lambda is zero, then so this lim if contains a factor that is incomplete gamma integral. If lambda is zero, then this integral actually evaluates to now one over r plus one. So when lambda is zero and this n that's the slim soup is zero, That the slim soap is zero, or limb is zero, then we actually do have a limiting behavior of the expected value, but not in general. And for the variance is even worse, even if lambda is zero, then we are off by a factor of r plus one. So one r plus one smaller than the alpha. Okay? I come back to it. And again, the proof here is just the sort of classic. Just the sort of classical Lindeberg central limit theorem for marking indifferences. So you take this, so it's the standard, you take this thing, write it as a telescoping sum of martingale differences, and you just check that the Lindenberg condition for the central limit for Martingers are satisfied. Generally, to get those estimates, so this requires. To get those estimates, so this requires proving some convergence in probability and things like that for certain quantities. To do it, we generally use the negative type of association of number of balls between the various ohms. So this is well known that these quantities are negatively associated or I mean, they come under different names, but but that that helps with with with this sort of bounding the the the differences and things like that, okay. Differences and things like that. Okay, so, all right. So, one thing that, for the lack of a better term, we call phase transition is the following. And it is that if I assume that npn star actually goes to zero, and there is r such that this quantity goes to a positive limit, which I said I can write like this. So, there is for some r, I have a. So, there is for some R, I have a Twissonian type of behavior, okay, for some capacity. If I decrease capacity even by one, then I get a normal limit theorem. And if I increase the capacity even just by one, then I get a degenerative limit. So that's, as I said, this is not really, this is just an observation. It follows from that, it's always written here that if you have this quantity, you can pull up any possible. This quantity, you can pull up any positive power of p and star from here, get an upper bound, and now you have okay, if this goes to a finite limit and this goes to zero, then this has to go to zero, so that's you get degeneracy, and vice versa. If this goes to infinity, now if this if this goes to a finite limit, this goes to zero, so for that, you have this, this has to go to infinity. Okay, so basically, that's so, as I said, this is just an observation about theory. But but I mean and I guess so this part is maybe this part I guess is easy to believe. You had just Poisson bolts outside, you increased capacity, so they'll feed. This find is maybe a little bit more surprising, but it's okay. All right, so for the rest of the talk, what I want I want to say a little bit about this. About the asymptotics of the expected value. So, in order to do this, I'm going to give a condition under which this limit exists. And in order to do this, I need to define new random variables. So, as I said, when limp soup is zero, we know the limiting behavior of the expected value. So, let's assume it is positive. And let's normalize this. Let's normalize these probabilities of falling into various boxes. And I'm going to look at the number of distinct values among those. So how many different probabilities you have among those. And now my new random variable Tn will be sort of, so it will take one of those values with probability that is Pnk to the R plus 1 normalized, and it is sum over all. And it is sum over all. So this is multiplicity. How many, so this is this value times how many times in my probability distribution this value appears. So this sum is just multiplicity of that value in those terms. And now the definition is that I say that the sequence xn is in class TR if this sequence of random variables converges on distribution. Converges on distribution. Right? Okay, and then what turns out, so the theorem says that if my sequence is tau r and this limit exists, then actually this ratio has a limit. And more precisely, okay, here's the full statement. So again, this goes to lambda that is positive, sequence is in tau r and this limit. R and this limit turns out to be 1 over r plus 1 factorial times integral of a, you know, this is one of those special functions, generalized hypergeometric functions. So here's the definition. Okay, you take, has two parameters, pq and those numbers, and you just take rising factorials of those and divide by rising factorials of the second set of parameters and zk over k factorial. Okay, so this limit in the case is just integral of that. In the case, it is just the integral of that function with parameter 1, 1, RR plus 1, lambda mu over, and you integrate with respect to the limiting distribution of the sequence TA. All right? So maybe two examples. So again, if we go back to the example of uniform distribution, so uniform distribution, as I said, P and J are all the same. So this random variables Tn are degenerate, but it's only one value among. One value among the different probabilities. So this integral is just evaluation at one. So you have, so in that case, this is the limit. If, on the other hand, you take a geometric distribution, then again, you can doesn't take much to find out the limiting measure, the limiting distribution of the sequence TN. This is that measure. And when you integrate, Integrate, you get that bit limit is another generalized hypergeometric function with different parameters. So, what it shows in particular that this limit depends on more than lambda. It depends really on the full distribution among the boxes, not just lambda. You can have the same lambda, but very different distribution among boxes. And what's more, this limit doesn't exist because you can interspace two different sequences as your PLs, and so there is no hold. PNs and so there is no hope for just going farther than that. It's just that maybe the iterative case lambda equal to 0? Well, with lambda is equal to 0, we already know that there is a limit. Okay, so we don't we don't need it, okay? Right. Okay, and so I guess we we we I guess I'll say I'm not sure how frequent it is that this limit TN sequence wouldn't have a limit. I mean, I can always artificially, as I said, take two different sequences and sort of mingle them together so I won't have a limit. But among natural, so to speak, distributions among boxes, how frequent it is, we worked out one more example of curiosity. That's what we call the Riemann distribution. So P and J is sort of related. Of related to the Riemann function, okay, and so j to negative alpha n divided by, so normalized by Riemann and alpha n, and regime you take n alpha n minus 1 goes to 1, so those alpha n's get close to 0. And again, you can figure out the limiting distribution of those pn's. Okay, this is the limiting distribution as a discrete random verbal, and you can integrate whatever we have in the statement with respect to the distribution. I mean, the statement with respect to the distribution, and what it turns out at the end that the expression is this, that it can be represented as the integral of that function over this is another function. So I guess for many natural things, it seems to, the limit seems to be exist and everything is okay. But as I said, in general, there is no way of going farther because I said a limit you can always force not to do it. All right, so I think all that's all I wanted to say, so thank you very much. So there is some heuristics I never understood. So I noticed that for many analytical college problems, it's always like in a special case you have how you can distribute. Have Gaussian distribution, and for all the other cases, you have some other general distribution. There are reasons for it. If I agree with that? No, no, no. Is there any heuristics behind that? I know that. Well, I mean, like, if variance doesn't go to infinity, you won't have Gaussian distribution, right? I'm not sure I understand the question. I agree. Sorry. So I noticed that in many Sorry. So I noticed that in many uh analytic combinatorics problem, like all the parameters, there is always like one special parameter where you have Gaussian the menu. And for all the other parameters, it's another distribution for all the other parameters. There are reasons for that. So as I mean, I think there are many reasons for that in the sense that as I said, if if those varying If those parameters cause variance not to go to infinity, then you may end up with, you will end up with discrete distribution, for example, things like that. So, like what is here, right? This n to the r plus 1, pn to expected value pn to the r, this is actually a variance, okay? This is order of the variance. So, if it doesn't go to infinity, you do not get continuous log, you get discrete distribution, which under this mild assumption npn star goes to zero is a Poisson distribution, right? It's a it's a Poisson distribution, right? And if this goes to infinity, then you get a normal law. But I guess in general, many problems you can have. I guess depending what those parameters are and what setting them to some values versus our is, I think that's. Okay, so it depends on like when the variance, whether it's well, not only, but one of the things is yeah. But I mean you know in general guiding principle if if if the variance is the same if the variance is asymptotically equal to the expected value and finite you expect your random verbal to put Poisson. Okay, that sort of statisticians base some tests on they develop some tests based on that and for normal I guess you know rule of the time is if you have expected well the time is if you have expected value going to infinity and variance with roughly the same size as expected value, often you expect normal distribution, but of course there are exceptions from that sort of thing. Okay, let's thank Paul. We have one more touch in the morning session. 