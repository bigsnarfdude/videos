Good morning everyone and uh thanks to Professor Tanya and Yeuan for the invitation and I really enjoyed this workshop. And our last talk today I want to share my project I recently did with my advisor Dr. Bing Li about the non-parametric test for the UP distribution based on the kernel embedding of the probabilities. And we will use, this is outside of our This is outside of my talk, and we'll first talk about some backgrounds and outside the program. And we will talk about kernel embedding and how to construct a test statistic with asymptotic distribution and the simple level implement. And we'll also talk about some simulation results and applications later. So, in the title, we want to give a test on input distribution. So, what is input distribution? Distribution. So, what is the input distribution? We know that input distribution is like, geometrically, it is like its control is ellipse. And when we want to formally define it, it is defined as we give a center mu and a shape parameter lambda that is d by d matrix. And the definition of limit distribution is that this density is simply a function. Is simply a function of this. This thing looks not that intuitive, but if we replace this lambda by sigma, we will find that this is exactly the normalized virtual x. So, in some sense, it is like after normalizing x, the distribution of density only depends on its norm or its length, but norm depends on its direction. But it looks depends on its direction. Well, this function, there is a function h, and this condition is just a normalizing condition to make this density equal to 1. And here, this denotes the determinant of lambda. And if the lambda is the identity matrix, we call it a spherical distribution because it controls our sphere. And one thing we want to note is that the shape parameter lambda. That the shape parameter lambda can be specified after a constant. That is, replacing lambda by C lambda for some constant C will not change the distribution because the lambda is here and also it comes out here. So after a constant, it is okay. Then, one thing we can do is we can reparametries of x via the length and the direction. Because clearly, here this is the length, right? But except length, another thing we can use is the direction vector, which means we can direction characterize x by its length in the direction. So what we want to do is we use the this is just a statement that we can normalize the x to get w, which has a spherical. To get W, which has a spherical distribution. That is, for any liquid distribution, after this kind of normalizing, we can make it a spherical distribution zero. This is the same as the standardization of the multimarried normal distribution. And if the components of x are square integrable, what we can see is the mu, that is the location parameter, is exactly the expectation of x. But the lambda, since it can be specified after a constant, Since it can be specified after a constant, we can see that it's related to the variance covariance of x. But after a constant, the constant is exactly E. Oh, sorry, sorry, I mean the type of E. So here, this should be Y. This should be Y. Poss here, this is Y. This is E. This is a constant, but it only requires the second moment. And in this way, what we can do is. way what we can do is we can reparamatch x by u v or u theta where the uv or u theta can be defined in this way what is this we see the u here it is in this form we replace lambda by sigma and we take a square root so it is like the norm of the standardized version right so we can say that the u can be viewed as the length of this standardized version The length of this standardized version of x. And the b is this one divided by u. So b can be viewed as this direction vector. So it is it divided by this length. So it's only characterized as the direction. So it's a direction vector. And we here we set theta equal to g v, where the g is the polar coordinate transformation of the unit sphere. So we represent the v into the polar coordinate system. Into the polar coordinate system. This is to avoid, we need to do a lot of analysis on the unit sphere. But when we do it in the polar coordinate system, we can do it in a cube. That will be more convenient in the later analysis. So here, U can be viewed as the length of this, and the V can be viewed as its direction vector, and theta can be viewed as another representation of this v. And for convenience, we will mainly use the representation of the. And for convenience, we will mainly use the reparametrization of U theta in this project. So, what are the equivalent conditions if we do this reparameterization? Actually, we can know that x has an elliptical distribution with center mu and shape parameter sigma. But since the sigma and lambda, we know that they are different only after a constant. So, we can use anyone to be shape parameter. So, here we can directly say it is sigma. Directly say it is single. This, if and only if, for u and theta defined as above, we need these two conditions. The first condition is u and theta should be independent. This is because the density, the direction never appears in the density. So u and theta should be independent. And also, since theta doesn't, or the direction does not appear in the density, it means it should be uniformly distributed on the unit sphere. On the unit sphere, which induces that the theta should have a distribution with this density. This density looks weird, and the support is in this rectangle. Although it is weird, actually it is induced by the fact that V is uniformly distributed in the unit sphere. And actually, if you see this. And actually if you see uh these these a lot of terms, a lot of cosine, they are just the Jacobi when we transform from the uh condition condition system to polar coordinate system. So all these are just the Jacobi and C is a normalizing constant. So theta has a distribution with this density. And the condition one is equivalent to u and the v are independent. That is length independent of direction. Condition two is the equivalent to direction. The equivalent to the direction should be uniformly distributed. So let's offline our problem. Actually, visual distribution has a lot of applications. For example, in the sufficient dimension reduction, it is a lot of assumptions on moment-based methods. For example, like this last inverse progression, polynomial least squares, or iterative Hessian transformation. And also, in this TISO graphic model, the unique distribution. The unit distribution is also the requirement for a class of methods like graphical assault or the translated graphical models. But before applying such methods, we will need to decide whether the data are even that illegal distributed. So, this arises, this contributes to our question: how to construct a test for whether the data follow illegal distribution. Actually, there are a lot of There are a lot of existing methods, and we went over the literature and we found that the existing methods can be roughly classified them into three types. The first type is sometimes for spherical distribution. We have these four papers they are talking about for spherical distribution. The spherical is like they restrict the Restrict the shape parameter to be identity. So they cannot estimate it. So this is just testing a smaller hypothesis. Some of them even restrict the location parameter. Some not. So these are ways to test for the swift distribution. And there are some methods testing elliptical distribution. I found these two papers, but these two papers. Two papers, but these two papers, when they consider the illegal distribution, general illegal distribution, they will need to use bootstrap. I remember this paper also talks about some asymptotics, but their asymptotic is derived based on multivariate normal distribution. So, under the elliptical distribution, their asymptotic does not hold. And they also mentioned that in their paper. And the third type is they test for individual distribution under some specified alternatives. For example, these two papers, they are testing this paper is testing whether the standardized direction vector is uniformly distributed on unit sphere. That is, they om they could only violate they alternative could only violate the input distribution for this condition, the uniform distribution condition, but not for the independence condition. But not for the independent external. And also for this paper, it talks about the optimal tests. Again, some generalized skill elliptical alternatives. That is, they also restrict it into the skill elliptical as the alternative, as those general alternatives. So we want to seek something that can solve this or can give a general test. A general test for your distribution. They assume the sigma to be known. Okay, they just this sigma or the shape parameter. That's gonna be normal. Yeah. So you have no associate sorry to be gone. Yeah, yeah, yeah, yeah, they are they are they are they are almost. They're almost equivalent. They assume that this sigma is a known matrix. But here we want to estimate. Some of them even assume this mu should be known. Some just say that test whether it is any or zero. Then our idea is based on doing some kind of goodness of these tests. But the difference is The difference is about the setting of the goodness of fit, that is in the distribution setting. And the main tool we involve is about the kernel embedding of the probabilities. When talking about kernel embedding, I will start from the reproducing kernel Huber spaces, short for RKHS. And we know that the definition of RKHS is that we guess is the Hibbert sphere. That we get is the Hilbert space of the real value functions to find all this domain, I mean, just a simple space. And a function we call it reproducing kernel if it is in the Hilbert space and it has a reproducing property, that is, we consider this function, kappa dot C, and we inner product with this F in the Hilbert space, it will reproduce the F evaluated as C for all the Z in the field. For all the z in omega f in the Hilbert space. And this is called the reproducing property. And we say that if the H had a reproducing kernel, it should say it should be a reproducing kernel Hilbert space. And this reproducing property seems a bit abstract, but one remark is: we define that, or we see that H is an RKHS. If the value H functionals, there's a Z, which is defined as. z, which is defined as delta zf is the directly sketch the evaluation of f at the point z. And this function, if it is bounded, then we can say that h is an RKHS. And then this kappa dot C will be the representer of the delta C. That is, we just use this reproducing property that can reproduce the F every evaluation of F. Evaluation of F. In this way, what can we see is if we can write something else just based on F, for example, we want to combine the different evaluations of this F, we can make it the inner products, and the inner products are the inner. So we can directly characterize it in this hibern space. In our work, we will use the Gaussian radio basis paradigm. Will use the Gaussian radio basis kernel. That's the or short for Gaussian kernel in this way. And one interpretation of using the RKHS is we may view this map from the Z to the kappa dot Z as the feature map from omega to H. The feature map, actually, it is a terminology coming from SBM. SBM, we always map something from a lower space to a higher, a lower space. To a higher, a lower dimensional space to a higher dimension space, and to try to separate them. And here, the comma adult state is just to replace the higher dimension space to an infinite dimensional hilbert space. So we can interpret it as a feature map in this case. And behind the HX, we will have the mean element. The mean element, we just suppose x is the random element, and hx is. X is the random element, and H is this corresponding arcane H is the kernel. And we suppose the kernel, kappa X, et sigma X and X will be of expectation, and the mean element will be defined as an element of HX, such that if we X or do inner products with any function f, we can reproduce or we can get expectation of. We can get the expectation of f. And we call this mux as a mean element. So in this case, μx is in fact a representer of this bounded linear functional that is from f to e of fx. We know that this linear function is bounded, so then we can directly find its representer in the Hilbert space. And we make it known as mu x. We made the note mu x as expectation of kappa x dot capital x. This is just a notation and when we act on any f, we will move this e out. That is, we put this as a kappa f to be inner product with f and then we get the e out. You can see that this is the same as this because Same as this because with the reproducing property f and kappa s to the inner product, we can represent fx, a reproducing property. So we have the mean element. And with the definition of the mean element, we can further define the one thing that is very important for us. It's called characteristic kernel. Characteristic kernel is that we want x, y, and y to be random elements. And y to be random elements taking value omega and have the probability p and q. What we want to see is if this kappa is a reproducing kernel and we call kappa a characteristic kernel, if this, the mean element induced by x will be the same as mean element induced by y. This will be equivalent to p equal to q. That is, P equals to Q. That is, these two probabilities are the same. Or the two distributions are the same. So, that is, we know that if the two probability distributions are the same, the right to the left is very trivial. But for the left to the right, if the kappa, we say that kappa is characteristic, then we can go from left to right to make it equivalent. Equivalent. And we also call this math, that is here, perimbed. That is, for x has distribution of p, the map from the probability measure to Hilbert space, that is from p to e p, dot x. We call this map a kernel embedding. That is, we embed the probability distribution. In fact, the probability distribution to this RK address. Yeah, yeah, yeah. The name just comes from the characteristic function. Yeah. Here, I write a remark here, the kernel kata is characteristic if the kernel embedding is injective. And that means the kernel embedding characterizes the probability distribution. Actually, the characteristic function got its name because if two characteristic functions If two characteristic functions are the same, it means the probability distributions are the same. Observing. And here, this is a generalization of the characteristic function. So we call it characteristic kernel. And it says that if the kernel embedding of two probability distributions are the same, it will lead to that the two probability distributions would be the same. Probability distributions would be the same. So, which means they characterize the probability distribution. And this is very important in our later derivations because we will need to categorize the distribution in the good space-like text. Distance of that karpa is guaranteed or is assumed? Actually, we can give a specific compound, we can directly verify its characteristic. Yeah, as long as the key is not very supported in a very real set, it should be okay. For example, in most cases, the dosing kernel would be characteristic, and the plus kernel will also be characteristic. And there is another type of Characteristic. And there is another type of kernel called a distance-induced kernel. That for some distances, it is also characteristic. And actually, if we view it as a feature map, at least it will be, the feature map, if we expand it in some basis, it will be, at least it will be infinite-dimensional. So, a general, for example, a general feature map induce a General feature map induces a kernel or not. If we just map it to a finite dimension case, it must not be characteristic. But unique. It's not unique. We can apply any characteristic kernels in this case. But in general, in most cases, Gaussian kernels are characteristic. So that's why we apply Gaussian kernels here. So we can also apply a lot of different kernels. Like Gaussian kernel has is Like Aussie Internal has a unique advantage in our case, because it can be product type, which is useful in our test. So let's come to the construction of the test statistic. First, we talk about the goal of this. We want a test statistic that solves the previous three issues mentioned in the existing literature. First, Literatures. First, it accommodates the issue that the mean vector mu and the coherence matrix sigma need to be estimated. We cannot say that we know mu and sigma. That is not that general. And the second one is it has some asymptotic properties, so that's the bootstrap step can be avoided. And the third one is we want it to simultaneously test the two conditions. One is UN Defi independent, the second one is Zefa independent. The second one is D5 has no distribution, just the distribution presented before. And in general, let's first consider Go3. Go3 is the most interesting one. This can be tackled on the construction of the test statistic via the kernel embedding to involve the both conditions. And this is what we will do. We had some failure attempts, and I will show immediately. And this is at the final stage, why we This is at the final stage why we arrive at this kind of kernel embedding thing. And for the goal one, if we want to estimate this mu and sigma, we just use applying estimators mu and sigma. This is a standard way to do so. And the goal too, we want to specify some asymptotic properties. We just use some asymptotic tools and we will derive it later. But we need to notice that the estimating process of this mu and sigma also μ and sigma also matters in the asymptotic distribution. So let's talk about the goal 3 first. We have some better tests. One is we only test whether u is independent of theta. And when we search for independence tests, we found that there is something called the cross-covariance operator. And what this operator did is, for example, we directly say. We directly see or consider a kernel kappa U tensor product Kappa theta. That is in the tensor product here a space of HU tensor product H theta and then we do kernel embedding So this is the kernel embedding in the tensor product space and in the later term is we do the kernel embedding in their own spaces and we tensor product them together. them together. And then we see that if u and zeta are independent, first this tensor product can be separate out. So this sigma u theta would be zero. So in this case, if we know that the kappa u and kappa theta are characteristic kernels, that is, if they will uniquely characterize the distribution. So in that case, the sigma u theta case, the sigma u zeta would be zero if and only if u and zeta are independent. But this is the operation, right? This operation, how to categorize is zero, when we construct a test, we can use the Hilbert-Schmidt norm. Hilbert-Schmidt norm is just like the infinite dimensional L2 norm. Yeah, they are almost equivalent to infinite dimensional L2 norm, of course. This really the equality statement. So we're not assuming normality. Yeah, not. So it's like kind of. Oh, okay. This is only equivalent to the UNTI independent. We have no distribution assumptions. Highlight again the difference between the two terms. This one? This is, the first term is we first do the tensor process. The first term is we first do the tester product and then we take kernel embedding. The second one is do kernel embedding and task product. This is like EXY minus EXEY. So the first one, so you are giving the sort of joint of the U and the theta distribution E. Yeah. It so th that that tensor product gives you a a kernel on the U theta space. OU theta space? Yeah, th this space is in HU tensor product H theta. Okay. Yeah, this is this is an element in the tensor product space. And also also this is also an element in the tensor product space. This is in HU, this is in H theta, and it will tensor product them together. First one is the joint, sort of the joint PDF of U and Zeta. The second one is the product, the two margins. Right, right. Yeah, so it's Right, right. Yeah, so it's the same as just seeing that the joint PDF is a product of the marginals. And this is valid when you're dealing with characteristic kernels. So here you can use a Gaussian kernel. Yeah, we can use a Gaussian here. And is it problem-dependent, like what type of characteristic kernel you want to use? kernel you want to use or what you assume? In general, Gaussian kernel should be okay. As long as the distribution is not too weird. But at least when it's supported on the whole in the whole RD, it should be okay. But there is some moment uh moment conditions, just like Ex to the power something. Power something. I remember it's ex to the power fourth. Fourth is smaller, uh, smaller than infinity. That is, that's a requirement, but there are not very specific requirements. And then here we use the is here by Schmidt norm to categorize if they are independent. And here we can get the tester statistics to test whether they are independent. Test the statistics to test whether they are independent. And here, the sigma use if a hat is computed by replacing all the e's by the en, that's an example average. So, we can construct statistics in this way. And actually, if we specify the kernel as a distance induced kernel, this will be equivalent to the distance covariance in some literature. There is some equivalence between them. And if we do the here-respect. And if we do the here by space C L T based on root n sigma u theta has over sigma u theta, because since we just replace e by e n, so all of them are moment estimators, C L T will hold. We can derive that important distribution for this T n in this case. But this only has the condition one, which is independent of the condition. This is far from enough. For example, if we see this contrary example, if the first and third quadrant The first and third quadrant are very dense, second and third are very sparse. We can see that this is actually a actual example, but this only testing for independence will not say anything. Before about the norm, the Hilbert Schmidt norm, this is a norm on the operator. Yeah, operator. How is it defined? It is defined as the operator. Defined the operator at on basis. For example, if E1 to Em as a basis, so HA is length. Sigma is the operator, EI is the basis. Everybody is defined this way. X is on the basis of the Hubble space. The basis. Sigma is operator. Sigma is operator. So you don't have to work with the supreme law of the sigma. So we end them together. So this is, we need this to be a Hilbert-Schmidt operator. That requires some moment conditions. Conditions. And that input sum is computable, so all is computable, actually. Yeah, yeah, this is computable. And in practice, we will do a transition from the first n terms. And actually, this is the same as the, in the matrix case. In the matrix case, it will be the summation of the eigenvalue squared. But yeah, like like uh like for s same uh same as for Venus Norman in the in the measure. Yeah. We talked about these contra examples that testing only testing condition one is not enough. Another way is we can if we only test condition two, we're testing condition two there is also a kind of goodness fit method because condition two is just to see whether the distribution of Just to see whether the distribution of theta is uniform. The distribution of theta is there's no distribution. So, in this case, there is a terminology called maximum mean disparity, MMP, that is exactly finding the difference between the current value of this P and Q. So, if P and Q are the same, the current values are the same, so it will. Is unsame, so it will be zero, and take a normal ethereal space. And given that the kama theta is a characteristic kernel, this is zero only when P and Q are the same. So, which means this MMD is zero if and only if P and Q. And if we use the MMD squared as a statistic, where the P theta has empiric distribution of theta, and the P theta is the known distribution. And the P theta is the known distribution of theta, and we can directly see how they are, how much their distance is. And for this, we can also use CLT to get this asymptotic distribution. Because here, this is EN, this is the true distribution. So the CLT also holds, because this is the moment estimator, this is node. Clearly, this also has some examples. This is the concrete example we see. This is the complete example. We see clearly the direction and the um length are very related, right? Of course, we we even generate in this way. But in this way, if we directly test the direction, we can say nothing. We can say it is very elliptical. So testing either one is not enough. So our idea is So, our idea is. Oh, no. First of all, talking about another goal first. This is about estimating UN sigma. Because this is very fast. Here, when we need to estimate the mu and sigma, previously we define u, v, and sigma in this way. When we need to estimate the mu and sigma, what we do is we replace this mu and sigma in order. and sigma in all these terms by mu hat and sigma hat. And we do it in this way. This is a standard approach. And one thing to notice, the multivariate C L T value minus mu hat minus mu is n to the next half rate. And sigma hat minus sigma is also of the same rate. So this rate will mean that the estimating mu and sigma will be to very Estimating view and sigma will lead to various inflation in the asymptotic analysis. We will talk about in the asymptotic analysis later. And here comes to our test construction. And here, we give a closer look at these two test statistics. First one is HSIC code, but just here, but we know this. This measures the sample level, it measures distance between operating embeddings of P hat U theta and the Zeta and the product of pHU can be p hat theta. So it measures distance in this part. And the MMD for condition 2 measures the distance between p hat theta and the true distribution of p theta. So we aim to measure the distance that combines these two conditions together. That is, we want to measure the distance between the kernel invariance of this p hat u theta and has used theta and this term. This term you see here, this is comes from this product measure, but we replace this P theta hat by this true version. So in this case, we measure the kind of distance between kind of embeddings of these two distributions, and when we define the tensor statistic, we first of all do the kind of embedding of the p hat new theta that's rejoined. The p hat u theta as the joint empirical distribution. But in the second term, the u part is the empirical distribution, empirical marginal distribution of u, but the theta part, we use the true distribution of theta. Here the e zero represents the exponential under the true distribution of t theta, and our test statistic will be constructed by tn equals to n times the sigma u theta breath. The here should be known. With the Hilbert Schmidt norm square. This is very similar to the cross-correlative operator, but the main difference is here. We need to apply the true distribution. Then it comes to the asymptotic distribution. We introduce a tool about momentum expansion. That is, if X1, SLI desamples from a distribution. From a distribution, and here F0 is the true distribution, and Fn is the imperial distribution, and we write the statistic as a statistical functional of Fn. So, if we can write it in this way, we can know that we have this asymptotic normal distribution, where this gamma that is on the various place is Place is the linear operator that is constructed by the influence function. Star is an influence function of t. The f naught has to be a specified distribution, so it has to say, it has to have all the parameters completely specified now. F0. Or F0. It's the true distribution. It's a true distribution. Yeah, it's the true distribution. Yeah, it's the true distribution. So it's not just a wavelength. It has to be weightable with alpha equal 3 and beta equals 4. Right, right, right. Right, F0 is the true distribution. Totally true. This is exactly the same as T. And the T F 0 should not have any randomness. This is like this is like expired, this is like new, something like this. Yeah. And this is a very gen uh general This is a very general picture. And our sketch of deriving the asymptotic distribution of this sigma u theta brief will be just we write the statistical functional related to sigma u theta f and derive the inverse function and use this to use asymptotic distribution. So this part, actually, this might be a boring part because it is more of computation. We just write down Of computation. We just write down all the statistical functionals. First step, we need to write the mu f and the sigma f. That is, we need to estimate, because we need to estimate the mu and sigma. And after estimating the mu and the sigma, we need to plug it into u and v, or u and theta. And we use two intermediate steps, V is W, except to define it this way. That is normalized as U is its norm, V is its direction, theta. normal, V is this direction, theta is this polar coordinate representation, and we write the sigma u theta f finally in this way. And here, here we need to notice that the last term, this term, is using the true distribution. And our statutory function of interest is then defining this n times this sigma u theta f here norm squared. And then it's just continuous. And then it's just computation. We try to find the influence function. We find the influence function u star, sigma star, and sigma ingratio star, and sigma negative heart star, something like that. And we find the influence function of ux, vx, and d5x. And they are kind of tedious. And here, one thing I want to mention is here, the influence function of all these terms will combine something times mu. combine something times mu star sigma star and this will exactly categorize what's the variance inflation in the estimating the mu hat in sigma single sigma hat so every every time it will contribute to the variance inflation and over the a1 a2 b1 b2 c1 c2 we have this form and also finally we And also, uh finally we get input function of this sigma u theta based on this form. And here we need a derivative of some kappa function, kind of function. And the derivative will be taken with respect to the same argument here. That's why we consider this kappa dot of derivative of the same argument. And also we define the kappa tilde theta as the centralized version of the Kamal function. Centralized version of the Kamal function. And here, the terrible thing is we have the integrals here. And we want you to deal with the integral in under the true distribution. One proposition to mention is we can know that if kappa u, kappa, theta are both C2 function and this f to the this sigma u theta free is uh free differentiable, then in this case we can know that the inverse function is a member of this tangent product Qbert space. We take the product hub of space. So that's why we can directly connect all our analysis in this hub space. And the silver term coming from the estimation of the medium and sigma is it. Otherwise, we just have the first. Yeah, otherwise only have first. Yeah, the second and third term. You start until the hatchway appears there. And then the integration is always going to be. Oh, it's for the first time. So even if you. Yeah, this one also has integration. Right, right. Yeah, yeah. Also, this term is. Yeah, yeah. First time we have because you estimate you have this difficulty. Right, right, right. And here, this proposition means that the kappa u and kappa theta, when they are good, we can kind of all our analysis in equivalent space. For example, the Gaussian kernel. For example, the Gaussian kernel must be very smooth, so we can do everything one. What and H0? The expansion is 3. Is this what and H0 or is it general result? General result. This is general result. In H0, we can remove this term. In H0, it says this term is zero. So look, will you? So you can you can do the integration term, this is expectation of the curve. This should be our true distribution. Truth is whatever the data is. So so this part we cannot estimate. We will need to compute under the truth. It's all But it all computed under data high. Yeah, yeah, yeah. Under the estimated push. Yeah, yeah, yeah. When computing a test statistic, that part should be computed under truth. Truth of H0 or truth whatever the data is. H0, H0. Because when we construct test statistics, this should be computed under the truth. But when we approximate the SV. But when we approximate the asymptotic distribution, that can be estimated. And the truth is easy because you know the truth. Yeah, but we will need to compute the integral. But the integral of the dimension of the theta is t minus one. So the integral is not that easy, actually. But for the Gaussian kernel, the Gaussian kernel, the advantage is it has, it's his product type kernel. So we compute dimension by dimension, one by one. One by one. So we only need to do the one-dimensional thing executable. So it is easy to do. Okay, so we have Gaussian influence functions. And then we can write asymptotic normality of this sigma-breath as this asymptotic normal distribution, where the gamma is just this expectation of the influence function, the History itself. And if we take here, we make norm on both sides. Both sides. And under now, this is zero. So we can guess this asymptotic null distribution as the weighted sum of k squared one. So this is the asymptotic null distribution we get. For the sample level implement, I will mention it briefly because it j just requires coordinate mapping. Collimate mapping. Just requires collimity mapping, which means that the kappa, when we consider two bases, the basis will depend on the data. We just consider both n basis in the HU and H determined. And when we compute this Gibbersbate norm, we just represent it under this basis and take a trace. And take a trace of this, and it's a joint operator, times a joint operator, which is using based on this coordinate mapping. So it is exactly computing the trace of a matrix, because the basis is finite. So after the coordinate matrix, it will be a matrix. So we can compute it directly. But one thing is, here, after we compute, it requires one thing that says time. requires one thing that is k theta tilde. But the k theta tuna is the inner product of the kappa theta tilde. The tuna involves an integral under the two distribution. So that integral, since we will need to take into account, we will need to compute that integral. So what we want or what we did was to use numerical integrals using this two distribution. This is better than Monte Carlo because This is better than multi-carlo because the data might be very high-dimension. So, the multi- if we use multi-carload, we will need a lot of samples. But numerical integration, at least for Gaussian current, we can do it dimension by dimension, so this is okay. Yeah, yeah, yeah. Yeah, yeah, yeah, that will also be okay, yeah. Right. So, this is how to compute the test statistic, and also when computing. And also, when computing the ethnic positive null distribution, that is, we will need to estimate those lambda that are the eigenvalues of just this lambda here. So we will need to compute something about this gamma. And we use the same trick to consider basis as a product space. And what we compute is it should be in this form. And this form, we can write it as m hat, m hat transpose. M hat, m pat transpose, where m hat is in this form, well it is like this. It is looks overwhelming and actually this term is n square by n square matrix. So it is too large. And what acceleration is, since we only need these negative values, so the m times m h transpose is n squared by n squared. But inside each of them, it is n squared by n. So we By m. So we change the order, with which the order, we confuse m hat, transpose m hat. So this matrix will be m by n. But they give the same angle values. So in practice, what we do is we just use this m by n matrix and compute the angle values. So it's much faster in computation. Let me go over the simulation results very, very quickly. This is under the Results very quickly. This is under the null distribution. We use the very normal distribution and generate sigma, just generate positive damage metrics. And we see most of them are greater than 0.05. P-values are greater than 0.05. And under the alternative distribution, we replace one-third of the indices by a chi-square distribution. So to add the skewness, if we use chi-square 2, we see that. Chi-square 2, we see that when n is 500 and 1,000, most of them are pivotal are very small, indicating it's not really difficult. And also, if we reduce some skewness, we use chi-square 4, that is more like normal. So we see that when, oh, still most of them are, p-values are smaller than 0.1. But we use 0.1 as this line, but we see that when the dimension is very. But we see that when the dimension is very high, the performance might be affected. When the dimension is high, the distribution looks more like an illiterate distribution. And we also applied to a standard data set. This is a data set on studying the efficacy of most of homeoid infection control. And we plotted the data and did a box transformation. And before the And before the box hooks transformation, the p-value is very small, indicating it's not illegal. And after doing the box-hook transformation, the p-value is this, which is much larger than 0.05. So it's verified that box-cox transformation will make the distribution more easy. This is what we tried. Just for finally, for the conclusions, we propose a test.