Computing power and then having something that was really working. So these are the features of BFO, I mean, of the first release. And so it is a direct search algorithm, simple direct search algorithm for solving bound constraint problems. So it is thought for solving problems of small dimensions, no convexity, regularity assumptions. It is sort of Assumptions. It is a sort of black box, F. And a feature of the algorithm was that at each iteration, the set of search directions are random orthonormal directions. So this is, so we have some randomization in the procedure. We handle bounds, mixed integers, continuous variables, and multi-level problems, min-max problems. And this was min-max problems and this was required by some formulation for parameter tuning for example but one of i think the the the the most peculiar feature of the code is that it included self-tuning facilities because it's true that we when when we want to uh deliver a code we want to provide default values which hopefully will give a code that behaves better That behaves in a good way on a large class of problems. But if you are in the shoes of a practitioner, I mean, a user that downloads your code, he would like to tune the parameters on his or her class of problems, doesn't care of your tuning. Okay. So BFO includes the self-tuning facilities. So you could self-tune his parameters, its parameters, or I don't know, girl, I don't know. It's a new girl, and anyway, on a certain class of problems. And it was meant really for being used by practitioners. So it includes facilities as, for example, flexible termination rules, incomplete function evaluation. This, for example, was crucial for training. For example, if your function evaluate function value takes into account the sum of function values. Of function values, for example, you can decide to stop the calculation of this summation when you know that, for example, the value is above the current best function value, for example. So in this way, you can save a lot of time. It can also handle undefined function values, for example. I mean, all these tricks that really made the code usable for practical things. So, before So before it was and it still is a MATLAB, a single MATLAB file, 7,000 lines, and well documented. And as Philip says, every time you can read it as a romance, because it included a lot of comments. It's very easy to use. Easy, I remember. Yes. And so what's new in the And so, what's new in the BFO 2.0 release? So, now the lines are many more. So, now it's a heavy romance. So, if you really want to read it, you need to be patient. But anyway, so what's new? So, these are the news of the new release. So, first of all, it can handle categorical variables. But I have to admit that we leave the hard job. We leave the hard job to the user because the user has to provide the neighborhoods of the categorical variables, that is the hard part. And it can do it here or she in two ways. So using static sets, so in order to define the neighborhoods, but also dynamic thing. So in principle, one can redefine the so-called optimization context at each iteration. That means, for example, Each iteration. That means, for example, one can define also the problem dimension can vary at each evaluation. But the user has to provide this. So this machinery is inside BFO, but I admit we didn't test it except on toy problems invented by Philippe on how to make a nice mealshake. So we are here. I mean, we are, if you have, I mean, I would. If you have, I mean, I would like really to complete this part and to test the way BFO behaves on these problems, real problems. Moreover, in the new release, we have two new training strategies. In fact, I was a bit frustrated when I wanted to tune BFO using the standard techniques because I trained the parameters, but in the end, when I compared the End when I compare the performance profile using, I mean, reasonable parameters or those tuned. I mean, I didn't get an improvement or some, I mean, I mean, there is no correspondence, I mean, direct correspondence in these things. So I said, okay, let's define a training strategies to improve the data profile so that at least I move in that direction. And so, I mean, now in the new release, you also have these ways of computing the Computing the to evaluate the algorithmic performance. So, the other two features are the fact that now BFO can exploit some available coordinate partially separable structure in the problem. And I will discuss a little bit more these later on. And moreover, BFO is equipped with another very long matchup file. Another very long MATLAB file that is called BFOS, that is a library of model-based search steps. So, this library is compatible with BFO, and you can compute a standard interpolation-based search step using, I mean, standard technique as those that Anna described this morning, exactly in that mood. And in particular, And in particular, these BFOs can be adapted also to exploit the CPS structure, I mean the coordinate partially separable structure. We will discuss this later on. So these are the four main new things in BFO. So let's focus on the first thing here. So the idea is that we know that in general, I mean, at least until a few years ago, I mean, at least until a few years ago, using DFO methods for solving large problems and structured problems was prohibitive, I would say, both because using building models, quadratic models, for example, requires a lot of function evaluation. And on the other hand, also using sampling techniques, I mean, implies that the cost may explode. Coast may explode. So in the last recently, there were a few contributions in order to make the DFO approaches scalable with dimension. And what we do, in fact, is trying to solve a large problem using derivative-free approaches, but getting rid of the idea of a really black box problem, but using a gray box optimization problem. That is, we exploit. Problem. That is, we exploit some underlying structure. So, this is the structure that we exploit. So, we assume that the objective function is the sum of Q element function f i's. And if x is the variable and it is in Rn, we assume that each element function depends only on a few elements of the variable vector, mirror. Of the variable vector, a few variables, let's say. And what is interesting is that this number doesn't depend, it's not related to the full dimension. So, for example, you might have a problem of 10,000 variables, but each element function depends on 10, 5, 3 variables. And these kind of structures arise many situations, and in particular, arise when you have a problem that comes from discretization of a continuous problem. Of a continuous problem. This is very common. And in that context, it is common to know the connectivity pattern of the problem. I mean, in this framework, one should know for each element function which are the variables involved in that function. This is the connectivity structure in some sense. And this is some in some situations, this is an information that one knows and can an information that. And can information that we can exploit in the algorithm. Okay, so this is something we assume to know. So let's make an example and I try to make two key observations that will motivate the adaptation to be a thought to this specific structured problem. So here we have a function that depends on five variables and we have a five element function. So first element function. So first observe that if we move along if we move along E1 then since X1 appears only here and here, I mean if I have to evaluate the function f, I only need to evaluate f1 and f3. I don't need to evaluate all the element function. On the other hand, if I move along E3, that is here, okay, I can do, I can Okay, I can do, I can make this step without affecting the computation that I made along E1. And how can we translate this property? We say that X1 and X3 belongs to this joint set of variables. So steps along those variables can be made independently, one from the other. Moreover, since X1 and X3 appears only in these three. Appears only in these three element functions. If I want to evaluate a step of this form, in practice I move along this direction with keeping these fixed, then I only need to compute these three elements functions. So in principle, the cost of this evaluation is much lower than the full evaluation of f. Okay, these two things are crucial to. I mean, are crucial to understand how we do exploit structure. So, because the idea is this one: that we don't need to compute all functions, all the element functions, if we choose, I mean, where to move, if we choose coordinate span subspaces in a proper way. And so, the idea is that is this one. So, I have the connectivity pattern of my problem. I somehow will see how completely. Somehow, I will see how compute the set of independent variables. Then I can compute, make computation in these subspaces. And then when I compute the function value, I only evaluate a fraction of the full function. Okay, so we will exploit this in the poll step, the poll step of BFO, but this could be adapted to any other direct search method. And the idea is to compute The idea is to compute the pull step in each subspaces, subspace of independent variables. And this means that I can handle the mesh update in each subspaces and then recover the full function value without evaluating completely. So the main step here is how to compute this set of independent variables. Variables. Okay, because this is the starting point. So I will show you how we do it in the MBFO. This is done automatically. So the user only has to provide this information. I mean, for each element function, he has to say which are the variables involved in the function fi. Okay, so for example, here we have this set. So x1 means that f1 depends on x1 and x2. This is the only things the user This is the only thing the user has to provide. Then we build these sets, EJ, which are set of indices of element functions. So not of variables, but element function. And we say that I belong to Ej if and only if J is in Xi. That means that, for example, E one contains one and three, that means that X one appears in F one and F three. Appears in F1 and F3. Okay, so these are set of indices of functions. Then this is an important step. So we eliminate repeated list of element functions because what we want to find here is the structure that takes into account the dependency of variables and functions. Okay, so we defined this. We define these sets, these are sets ik, these are the set of independent variables in this way. Two, j1 and j2 belong to the same set if and only if the corresponding set of element function function is the same. So this means that those functions are the same in terms of dependency. So for example, here, for example, we only have this case. We only have this case here because we have that since these two sets here are the same, variable four and five goes in the same in the same set. The other are distinguished. So again, these are variables, indices. These are the corresponding functions. And here, the same, but the indices are the same for these two variables you see. Okay, so variables four and five appears both in F4 and F5. In F4 and F5. So now we construct a collection of independent sets. Okay, so this means that two sets of variables are in the same collection if and only if the intersection between the element functions indices is empty. So what does it mean? I take I1 here, I take Y1, I check if the intersection between Y1 The intersection between y1 and y2 is empty. No, so okay, I keep I2. So I now consider I1 and I3. I consider y3 and y1 and check the intersection. The intersection is empty, so they are in the same collection. And so on. These are empty. So these are three groups of sets of independent variables. Variables C1, C2, and C3. And these are the corresponding functional values. This means that if I move in this group here, I only use these function element function values. If I move in these two other groups, I think this. In any case, I mean, I only evaluate a fraction of the full function evaluation. Well, of course, this is a toy problem. It's very small, but taking into account a large problem, in any case, you have a number of groups. You have a number of groups, it doesn't depend on the dimension, but depends on the connectivity properties. No, we just use this greedy approach, but yes, I mean, this is just a choice, of course, one. I think it's a nice red to find language. You want to find a minimum number of color number of coloring things. And Paul Conan has a great keeper about this. Yes, I mean, this is not something new. I mean, something is our way of doing this. Oh, it's cool, Margarita. Yeah, cool. No, no, I mean, it's not just to say that this is what we do. This is what we do. Okay, so once we have this collection, so we define these sets as k, and so the idea. So for each set of independent variable, so for each set in one of the group, we perform a pull step in the subspaces. Okay, and the interesting things is that we check the sufficient decrease in the function f, the full f, only evaluating the element function. Evaluating the element functions which are in the corresponding set. Okay, so we do the minimizations in sense in the subspaces, but we recover information on the full function. So in this example, we have already said that. Okay, so that's the idea. So this is how now BFO 2.0 works. So at the beginning, So at the beginning, we check the input. So if the user provided some CPS structures, we move to this preliminary phase where we detect this set of independent variables. If not, we go on with the old VFO. So in this case, we can do a search step and then we do the structured pull step. So for each set of independent variables, we do a standard pull step. So with the standard mesh update, but in a lower dimension. But in a lower dimension, so considering a random basis of orthonormal vectors in the reduced space. And so, and here, after we made this whole step for all groups in the collection, we check the decrease in the function value, the full function value. If we had a sufficient decrease, then we have to enter the so-called second. And we have to enter the so-called second pass because here we cannot conclude that we have convergence because we only have made steps in the subspaces. So, in principle, here we should make a full step in the full dimension. So, in principle, one, I mean, here we should lose what we gain here. But fortunately, this is not the case. I mean, this is just a double termination step. I mean, this step, in fact, is not so expensive. In fact, it's not so expensive, but in principle, here one should do the pull step in dimension n. And in practice, we don't do the pull step in for n checking n direction, but checking maybe two or three. Okay, and it's interesting is that if in this loop we find a better point, then we go back to the structured pull step and repeat the thing again. Here, if we don't get a sufficient reduction in the function, A sufficient reduction in the function f, we go back and we proceed. I mean, the structured pull step with the in each subspace is taking the old, I mean, the current mesh size. We have mesh sizes for each subspace. So, this is an example with the same structure as before. And so, if you And so, here this is BFO without using structure, and this is without structure. So, as you can see, I mean, the reduction in terms of full function evaluation is really notable. And what you can see here is that at this point here, we have an unsuccessful step. I mean, this means that we couldn't find a reduction in this in the structure step. So, we go back to the Step so we go back to the standard fault step, we find a better form, so we go back to the structured fault step and we share convergence. So as you can see, even if we take some function evaluation here for the second part, I mean the green was so evident, but it's not important, fortunately. Okay. So here. So, here you have more experiments comparing FO with and without the structured fault set. And as you can see, the structured fault set is extremely efficient in reducing the number of functional evaluation. The number of functional evaluation is independent more or less, I mean, it's might be dependent on the dimension. And what is interesting, I think, is that. And what is interesting, I think, is that exploiting the CPS structure is useful not only for large problems, but also for small problems. Okay, so when you have it, you should exploit. So regarding the FOS, so we just adapt a model interpolation-based restriction strategy to the partial. Sorry, standard to the partial separable k. And in fact, we built a model for each element function. So, working in a reduced dimension, but in the end, we reconstructed the model for the full function and we minimize this to find the search step. So, what is the effect of mixing this the structured full step, the model interpolation search step? Search step. So these are the results. So these are results for a small size problem. So first, you can see here that the unstructured whole step, I mean the structured therefore is the worst. Including models helps, but what makes the difference is including the partially separable information in the poll step. We have an improvement if we use in combination with the post step model. With the full step models. Okay. But what happens if I take larger problems? So these are smallish problems and we still have here I cannot show you the comparison with the method without using structure because we cannot do it. Problem is already too large. And so here we still have a gain using the models, but if the pigment means But if the medium problems, I mean, we lose all the gain through the models, it is worse. So the best behavior is using false step without models. And so the question is why this happened. So this is what we could conclude by these experiments is that when we use the CPS structure, we are using sort of global information on the problem. Global information on the problem, and so using the pull step improves performance by orders of magnitude, really, no way. If we use local information, so using, for example, perpolition models, then we have an improvement for small, moderate size problem. But when the problem is large, we had a lot of failures during the computing time. This is what Jeff was saying, maybe at the same time. It's true that we don't compute. The same time. It's true that we don't compute extra function evaluation because we recycle points. We do a lot of sophisticated techniques that a lot of tricks to build the poised set of points without evaluating fractions and so on. But the linear algebra that is hidden there becomes heavy when the problem becomes larger because you have to compute SVD and SVD and a lot of things. I mean, and so that's the problem. We lose, I mean, it's true in terms of times, function evaluation doesn't change a lot, but we have failure. Here we have a maximum time of eight hours. That's the point I had to quit. And so this is what for Philippe I could stay forever waiting, but I decided. But I decided at some point that we had to conclude this. So, let's see. So, here you have some readings and the code is on GitHub. And if I have an extra five minutes, I want to describe a new project I'm involved in. I mean, I'm the responsible of this project, I will admit. And this is. And this is a PhD project in collaboration with Beckett Youth. Bekel Youth is the new name for the former General Electric and it has to say, it has a building, sorry, a plant in Florence. So that's why I'm in contact with them. And so they work on gas turbines. Bias and so this is I just want to tell you which is the mechanical aspect we work on. So I try to explain. I mean this is not my comfort zone of course. But anyway, this is how it works. So we have some air that goes inside. Here we have a compressor that compresses the air that exits from here with that pressure. Here we have the conduction with gas and from here very hot gas exits. Here, very hot gas exit from here, it's right. Exit from here, and here you have some rotating blades and some nozzles that I discovered, have I heard about them? There are sort of fixed blades, I mean something that drives the flow towards the rotating blades. And I mean, the point is that one would like to do this combustion at high temperature because higher temperature means Higher temperature means more efficiency, but the point is that here the pieces of this part mess, I mean, we have damages due to the high temperature. So there is a part of the mechanical engineers that works on strategies for cooling down the temperature here while keeping the fishing. So we work on this aspect, so and in particular on a special technique. Special technique of cooling that is called impeachment. So, this is the nozzle. And the idea, this is the profile. So, the idea is to include inside a sort of box, an insert, where you put some air, fresh, not so hot at least, and making many holes around like this. And so that the airs exit from here, exchange. Exit from here, exchange heat with the walls and could down the walls. So here the assumption are that we know that the pressure and the temperatures of these cooling hair and the outlet pressure. And we also know the temperatures and the heat transfer coefficient of the hot gas that is provided. Hot gas that is provided by RTV analysis that is considered fixed for the moment. So it is given. We don't use that as a back box for the moment. That's given. And so here the design variables are the positions of the centers of the holes, the metals to be dip, that means these distance. This distance here. The diameters of the holes, the number of the holes, and maybe a categorical diaboles appear, I hope. And that says something about the layout of these holes. So this is the problem. So far we have just had a few discussions with the engineers. This is what we could extract because you know the communication is not easy. The communication is not easy, especially in the beginning. So, this is the way we formulate the problem. So, the idea is that they are interested in maximize the heat transfer coefficients of the cooling air. Okay, let's see. And this is evaluated using a one-network solver, our black box, that returns together with this the external wall temperature and the intensity. World temperature and the internal wall temperatures because what they want is that the difference between these two temperatures is always below some quantity. So the gradient of temperatures is bounded. Of course, we also have constraints on the design variables, as we expect. So this is how we formulate our black box, but this is the one way that we could, we understood there are many. We understood there are many others. This is the simplest version. There are one can define other instances, multi-objective formulations, but this is where we start, I think, to reminisce. And so the problem is now, and which is the best formulation or the effort approach. I mean, I'm open to suggesting. I mean, it's the first time and this is a real, real application. Be the real, real application. And so that's and thank you, Brett. On color blind, I'm pretty sure the Canadian flag is not black. So thank you for interviewing the reintroduction. I mean, that's. I mean that's really but um just to show remotely that you have to use a performance in data profiles so okay it's cool that you know to just say to yourself okay just take a look at the data profile it's tell me what is the data value on your school that you can copy into a real framework it's so questionable how will it be so thanks for the question some slides on this uh because Because I know I was a bit fast. So, these were some performance strategies that were available in the literature: summing the value function values or minimizing the worst case scenario. This is what we do, in fact. So, we maximize the area under this graph. So, each of each and also for the top of phi. Also, for a data profile. And so, this is what we do. So, we compute these, and so this is our function, objective function, and try to maximize this. Is it okay? Did I see one of the slides that you used allow? Yes, yes, yes. This was our first. We also compared. them we also we compared maybe i should have these are all slides but should yes we compare the vfo using uh vision all the parameters profile robust optimization and profile i have to admit that usually uh usually in this at least in this framework the the the parameters that you set by your experience are very close to the best but you never know the best You never know depends on the algorithm, of course, but these are quite already tuned. So, does it play nicely with 4 PM? Sorry? The 4 PM file problem set it has a partial separable structure, right? So, is it easy to link with 2 PM problems? The new collection, you mean? Ah, sorry. Yes, but the OPM are just a subset of cutest problems, right? Do you mean the new facilities with partially separable? I mean, not yet, because let me think. I don't know if that facilities is already exploited until the end. But anyway, not yet. Not yet. Not yet. I saw it, but I still have to talk with Philippe. But the idea is that I mean, I would say the problems that we had were problem that Philippe had in his withdrawal and probably are those which have been then recorded in OPM. So the circle is there. The secret call is that.