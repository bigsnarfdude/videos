Lithening to a double dipping. And so, before I start the talk, I should probably tell you what I mean by double dipping in a statistical context. So, as many of you know, classical statistical methods assume that we're only ever testing pre-specified hypotheses about pre-specified models. But this isn't really how data analysis goes in practice. In practice, we explore our data, we fit a bunch of models, we select our favorite, and only at the end of that whole process do we test some hypotheses. So, in this talk, I'm going to let double dipping refer to the practice of using the same data for. To the practice of using the same data for multiple steps along this exploratory pipeline. So, some examples are using the same data to generate and test a null hypothesis or using the same data to fit and evaluate a model. And the reason that I care about double dipping is because if I take something like a t-test, which is designed to test a pre-specified hypothesis, but I apply it in a setting where my hypothesis is data-driven, I'm not going to get standard guarantees like type 1 error control. However, I also don't want to tell people that they're never allowed to explore their data and they're never allowed to do model selection. So, in my research, Model selection. So, in my research, I'm really focused on giving scientists tools for avoiding double dipping in their exploratory analyses. Okay, so all of you are probably very familiar with one common approach to avoid double dipping. So if this is my data set with four observations and two features, the simplest thing I can do is I can split my sample. So I can put half my observations into a training set and half my observations into a test set. And if my observations are independent, then my training and test sets are independent. And so I can. Independent. And so I can select a hypothesis on the training set and test it on the test set. And I've avoided double dipping because I've used two independent pieces of data for the two tasks. Or I can fit a model on the training set and evaluate the model on the test set. And I'm sure that many of you have used this in your work. Sample splitting is like a super common tool and it's great. However, it's not really the end of the story. I can't just end my talk now and say, always avoid double dipping through sample splitting. And so to motivate the rest of the talk, I'm going to. And so, to motivate the rest of the talk, I'm going to talk about some settings where sample splitting actually doesn't work. And then I'm going to get into my work, which is an alternative to sample splitting called thinning. Okay, so my motivation comes from the analysis of single-cell RNA sequencing data. So here is an extremely small single-cell RNA sequencing data set. So my rows are cells, and my columns are genes, and my entries are non-negative integers, which represent gene expression. And so this type of data wasn't available to us a decade ago, and so it's really exciting that we're. And so, it's really exciting that we're able to see gene expression at the level of the individual cell. And so, the type of question that this has unlocked is questions such as which genes are differentially expressed across cell type? Because now that I can actually see gene expression at the level of the individual cell, I can ask, like, oh, are there actual real differences between this 31 and this 6, or is that just sequencing noise? But this question is hard because cell type is not observed, and so we usually estimate it via clustering. Usually estimate it via clustering. And so a pretty typical single-cell RNA sequencing paper looks something like this. So here their goal was to build a human-liver cell atlas. So they sequenced a ton of cells from the human liver. And then the first thing they did was they did clustering to come up with estimated cell types. So their clusters are shown in UMAP space here. And then after they cluster, they have this heat map where the cells are on the x-axis and they're ordered by estimated cluster and the genes are on the y-axis. And they're really interested in the pattern of which genes are. Interested in the pattern of which genes are differentially expressed across clusters. And if you just naively test for a difference in beans across estimated clusters, you end up double dipping. So here's my small data set with two genes and 100 cells. And so the idea is that if the first thing I do is cluster my observations to come up with estimated cell types, which here are red and blue. And then I generate a hypothesis, which is, okay, I think gene 2 is differentially expressed between the red and the blue cells. And then I test this with a t-test. And then I test this with a t-test. Here I end up with a p-value of less than 10 to the negative 10. And in this case, this is bad because I generated this data. All of this data comes from the same distribution. It's completely homogenous. The fact that the red cells are on the top and the blue cells are on the bottom is just random noise. There's no actual true cell types here. And so what happened is that I took my t-test and I applied it to a data-driven hypothesis. And so it's not going to control the type 1 error. However, even though maybe if you think about this for a while, it's obvious that. You think about this for a while, it's obvious that there's a problem here. This is what people actually do in practice when they're analyzing this data. Oh, sorry, I got ahead of myself. First, I'll tell you why sample splitting can't be used. Okay, so what would happen if I tried to avoid double dipping via sample splitting here? So here's the same data that all comes from one true cluster, and I have my 100 cells. So what sample splitting would say to do is it would say, okay, first split your observations into a training set and a test set. So I put 50 points in the training set and 50 in the test set. And then it would. Set. And then it would say to cluster the training set. So now I have red cells and blue cells in the training set. And then it would say to test for a difference in means using the test set. But I'm stuck here because my clustering algorithm doesn't actually tell me which cells are red and which cells are blue in the test set. And it also doesn't provide a simple formula to figure out which cells are red and which cells are blue in the test set. And so in order to make sample splitting work here, I have to add something like a step 2.5, where I somehow assign labels to the observations in the test set. Labels to the observations in the test set. And so here I did that with three nearest neighbors classification. But this actually doesn't solve our problem. So if I now test for a difference in means across the clusters in the test set, I still get a p-value of less than 10 to the negative 6. And the problem here is that I still double dipped. I still used my test set twice. I've just done it in step 2.5 and step 3 instead of step 2 and step 3. And so what happened here is just that clustering is an unsupervised problem and sample splitting is really tricky in all unsupervised problems. And you could try to use something And you could try to use something besides three nearest neighbors, but you're not going to solve this problem. Okay, however, people actually do this in practice, the naive method where they ignore double dipping. And so here's a 2020 paper called 11 Grand Challenges in Single Cell Data Science. And they cite this double use of data, where the two uses are first clustering and then testing for differential expression, as one of the really big open problems in the field. And furthermore, really popular single-cell RNA-seq software packages like Surat. seek software packages like Serat. So this function find markers, what it's designed to do is it's designed to return genes that are differentially expressed across estimated clusters. And they have this note in their documentation that says p-values should be interpreted cautiously as the genes used for clustering are the same genes tested for differential expression. And so they're aware that these are not valid p-values, but they don't really have a good solution. And so they just return these invalid p-values anyway. Okay, and so that's going to motivate my first part of my talk on Poisson. My first part of my talk on Poissontheny, which is going to be a way around this problem. Okay, so as a reminder, sample splitting is not going to work for our motivating example. And the reason it's not going to work is that if I estimate clusters on my training set, which only has cell one and cell two, I don't get any clusters for my test set. And so I'm kind of stuck and there's no way to do the analysis that I want to do. So the alternative is going to be Poissonthening. So I'm going to take my data set X and I'm going to turn it into X1 and X2. And I'm going to turn it into X1 and X2, which I can use as my training set in my test set. But I'm not going to split the cells and I'm not going to split the genes. I'm going to actually take each count within my data set and I'm going to split that. And so I'm not just doing this arbitrarily. The way I'm doing this is that for every element in my matrix, I'm going to draw xij1, given xij, from a binomial distribution with parameter xij and epsilon. Epsilon's a tuning parameter that I get to pick, and then this whole top I'm using 0.5. And then I'm going to let xij2 be xij2. And then I'm going to let xij2 be xij minus xij1. And so, why did I do this? Because a very well-known fact about the Poisson distribution tells me that if I'm willing to believe my original counts are Poisson, which is pretty reasonable for single-cell RNA sequencing data, then when I do this operation, marginally, xij1 is Poisson, but its mean has been scaled by epsilon. Xij2 is also Poisson, but its mean has been scaled by 1 minus epsilon. And Xij2 and Xij1 are independent. So what I've done is I've made So, what I've done is I've made two copies of my data that both look like the original data. All I've done is I've scaled my mean by epsilon. However, they're independent. So, the reason we can assume these are, we can make it easy independent because we will assume the assumption is a Poisson distribution. Yes, yeah, there are. At least the dynamic binomials can be required. Yes, that's coming later. Yes, yeah. So, if the data is actually negative binomial, you won't have independence. Yeah. So, the next couple of slides assume Poisson. Yeah. Okay, so what this means. Yeah. Okay, so what this means is that if we believe the Poisson assumption, we can estimate our clusters on x1 and then test for a difference in means on x2. And because of the independence, we've avoided double dipping, and this should work. And so we can visualize this and see why it avoids the pit-fall of sample splitting. So here are my same 100 data points from before. So this is one true cluster. There's nothing interesting here. And so what I'm going to do first is I'm going to apply training. And so what you'll notice is that both my training set and my test set still have 100 points in them. Still have 100 points in them, but what's happened is that the counts themselves have gotten shrunken closer to zero. And so every data point in the original data set is still represented in the training set and the test set, and I know exactly which point it is. If this is the first row in my data set, I know exactly where it finds in the training set and the test set, and so on for all the other points. Oh yeah. Okay, but now what's really cool is that I can cluster the training set and get my red points and my blue points. And in this case, the blue points. And in this case, the blue points are only on the top half because of random nodes, because all of these points were drawn from exactly the same Poisson distribution. So when I apply these labels to the test set, I just get kind of this random jumbled mix, which means that when I test for a difference in means on the test set, I get a p-value of 0.8. And I'm really happy about this because there was no, like my clusters are just random noise. There is no signal in my data. And so if I repeat this over many data sets, the p-values are in fact uniformly distributed. So we control the type 1 error grade. Distributed, so we control the type 1 error grade. And I'll just really quickly point out that there's also other tasks we can accomplish with this. So, suppose I wanted to know how many clusters were in this data set. So, I tried out one cluster, two cluster, three cluster, four cluster, and computed a loss function like within cluster mean squared error. So, in this case, there's only one true cluster. And what I see is that if I do that process, Poisson thinning gets a loss function curve that's minimized at one true cluster, which is the truth, whereas the naive method and sample splitting due to overfitting. Method and sample splitting due to overfitting both have monotonically decreasing loss function curves. Okay, and I should probably also convince you that it works on data that has more than one true cluster, that it can actually preserve signal when there is signal in the data. So here I generated data with two clusters where the clusters are defined by differences in the second feature. So now I can thin just like I did before. But now if I cluster the training set, these blue points aren't just high up due to random noise. They're high up because they actually had a higher need. High up because they actually had a higher mean. And that mean structure is preserved in the test set. And so, if I do my test for difference in means over here, I'm back to a p-value of less than 10 to the negative 10. And so, this is good because it means I still have power under the alternative. And similarly, if I make my loss function curve here, something simple like within cluster mean squared error, Poissoning is able to tell that two true clusters is the correct number of clusters. Okay, yeah. Question. So, in the previous formulation, you used the if someone for the covering. Yeah. For the property. Is it any reason to use other 0.5? Yeah, so it actually controls the information allocation between the training set and the test set. So if you were doing sample splitting and you wanted 90% for train and 10% for test, you should use 0.9. Yeah. And another question is the test here, you add just user t-test or? Here I am using t-test. In our paper, we use like Poisson GLMs. Either is fine, I think. Yeah. When you're calculating the loss for the Poisson thinning, are you just using the X1? The X1? No, so we estimate the clusters here, but then we actually compute the within cluster mean squared error on X2 using the colors estimated from X1. Yeah. Okay, so hopefully this was a whirlwind tour that Poisson thinning kind of works for our motivating application. And we have a paper in biostatistics that's completely focused on Poisson thinning and applications to single-cell RNA-seq data. And we talk in this paper some about the role of epsilon and what happens if your data is actually negative binomial and things like that. Binomial and things like that. Okay, but there's a lot of reasons to want to extend this. And so the first reason to want to extend this has already been hinted at. If we apply Poisson thinning to data that is not actually Poisson, we don't have independence between training set and test set. And those nice results I just showed you will not hold. And as was maybe preempted by a question, there is a lot of evidence that actually for single-cell RNA-seq data, it's better to use a negative binomial distribution. So even if all I wanted to do was analyze single-cell RNA-seq data, I would want to extend Poissonthening. I would want to extend Poisson thinning beyond the Poisson distribution. However, there's also a lot of other reasons to want to extend Poisson thinning to other distributions. So, first of all, it wasn't unique to this differential expression testing problem that sample splitting was hard for clustering. Sample splitting is always hard for clustering. You can't cross-validate clusters. And so, a thinning type approach is going to be really useful for any unsupervised setting where sample splitting is not an option. But it can also be really useful in supervised settings where sample splitting might be unsatisfied. Settings where sample splitting might be unsatisfying for some reason. So, anytime you have non-IID data or where your individual observations have specific meanings, it might be really good to use spinning instead of sample splitting. So, an example is like if my observational units are the 50 states and I want to like draw inference on Texas, well, if Texas isn't the training set, I'm out of luck. And so, that's a case where thinning might be preferable. And then, also, anytime you have outliers, like it might be kind of uncomfortable to use sample splitting in this setting, because whether or not this point is in my training set. Whether or not this point is in my training set or my test set is going to really impact my results. But the idea of thinning is that we split every single observation, and so the contribution of that outlier will be evenly split between the training set and the test set. Okay, so yeah. Question. Yeah. So I understand for this particular multivariate example that you can use Poisson to model the distribution. Yeah. Maybe making phenomena is a bit better. You can do exactly the same thing for Gaussian data. Yeah, yeah. For Gaussian data. Yeah, yeah, yeah. Yeah, that's on the next one. But I really like the Poisson one because the mean and variance are the same. When you scale it down, the signal to noise ratio doesn't change. Yeah. When you scale down the intensity. But if you use Gaussian thinning, the signal to noise ratio gets corrupted because the variance is the constant one. But the signal is different. Yeah, okay. I'm going to have Gaussian thinning in a couple of slides actually. And our recipe is like it's similar to other ones in the literature up to a scaling by epsilon and 1 minus epsilon. Epsilon and one minus epsilon. I might have to think more about that because for the Poisson, if you instead do mean over standard deviation, it's the same, right? Then you get the square root of all things. Yeah, yeah, because I think, because like there's no free lunch, right? So like in sample splitting, you only get half your information in the training set. Even for Poisson splitting, that should be true here. It is also. Because the Fisher information about Lambda in the training set is like epsilon times the original Fisher information. Yeah. But you can use some notion of variance stabilizing testing. Of various stabilizing transformations, other transformations from that one distribution, approximately to some other distributor. Do I have any advice on what type of thinning techniques? Oh, that's a good question. So in the next part of the talk, I'll talk about how it extends to other distributions. But then, like, of course, in practice. So you're saying in practice, if I have data and I don't know what distribution it's from, how do I know what learning to apply? I don't know. Okay, cool. Okay, I will move to the next part of the talk, which is an Okay, I will move to the next part of the talk, which is about how to extend Poisson thinning to other distributions. Okay, so what did we like about Poisson thinning? We liked that we were able to take every single observation, x, and split it so that x1 and x2 have the same distribution of x up to a parameter scaling, and x1 and x2 are independent. And the really important part for us was that it was every single observation, instead of splitting like half the observations in the train and half the observations in the test. And so the idea is: just can we achieve the same properties when x is not Poisson? Not Poisson. And so to introduce our first the meaning recipe, I'm just going to need this definition of convolution closed distribution. So what I mean by convolution closed is if I have two independent Poisson random variables and I add them, the sum is still Poisson. But that we also have that property for a lot of other random variables. So if I have two independent normals and I add them, the sum is still normal, and a bunch of other distributions. And so it turns out that for any convolution closed distribution, we can apply the same thinning recipe. And the thinning recipe is as follows. is as follows. So if I observe some x and I know that x belongs, or I assume that x belongs to some convolution closed family, the idea is that just because the family is convolution closed, I know that x could have arisen as the sum of two independent random variables who come from distributions whose parameter has just been scaled. And the idea is that if I could have only observed those two random variables, I would have accomplished my goal of data thinning. And so can I work backwards to recover x prime and x double prime? And x double prime. And so the answer is yes. And the way we're going to do this is we're going to let g of epsilon x be the conditional distribution of x prime given x. And this is just something that we can write down for whatever family we're in. We can work it out with algebra. So then our recipe says, okay, draw x1 from this conditional distribution. Let x2 be x minus x1. And then the theorem says that we've gotten random variables with the desired distribution. And so the reason this recipe is nice is that as we'll see on the next slide, this Is that as we'll see on the next slide, this is exactly the recipe that we used for the Poisson distribution, but writing it down in the more general way makes us see how to extend it to lots of different distributions. The idea is that if we see x and its Poisson, we know that x could have arisen as the sum of two independent Poissons. And so we want to work backwards. And if we had never heard of Poisson thinning, like Poisson thinning is kind of a property that's taught in like probability classes. But if we'd never heard of it, all we would have to do is sit down with pencil and paper and work out the conditional distribution of x prime given x. Distribution of x prime given x. And we would see that it was binomial. And then we would say, okay, we know how to thin the Poisson. And so you've already seen this. And again, this is a really well-known operation that has appeared in like lots of different papers. But the cool thing is that by recognizing that all that was going on here was like sums and conditional distributions, we can immediately see how to do it for the negative binomial. So the negative binomial is also convolution closed. And so to figure out how to work backwards, all I have to do is work out the conditional distribution. And it turns out to be a beta binomial distribution. And it turns out to be a beta binomial distribution. And so, all of a sudden, I know how to thin the negative binomial into two independent pieces. And this is a brand new result. So, when we were working on the single-cell RNA-seq paper, we kept like banging our heads against the wall, like, oh, is there any way to extend this for negative binomial? No, like it's not in any existing papers. But it turns out to be really simple if you just kind of know that what you're looking for is the conditional distribution in this setup. And so, we can actually do this for lots of different distributions, whether or not Different distributions, whether or not that conditional distribution has a simple form, but it's kind of cool and neat that the conditional distribution often does have a simple form. So, the Poisson and binomial, as mentioned, is really well known. And also, there have been a whole slew of recent papers using this in the context of avoiding double dipping. So, for things like inference after variable selection in Poisson regression. Similarly, for the normal, and maybe you were hinting at this, so this exact formulation with the conditional distribution, we haven't really seen before, but up to a couple of steps. Seen before, but up to a couple of scale, like multiplications by epsilon, it's equivalent to this recipe of adding and subtracting random noise that has been used. Like, actually, even this morning, I almost updated this list because there are even more recent papers. This idea of adding and subtracting random noise to get independent copies of your data set has been an important tool in recent, like, selective inference papers. But then, everything else in this table is new, which kind of shows why this general convolution close recipe is very powerful. There gotta be something here where you. There can't be something here right where you're saying, I mean, it's only convolution, if you take the negative binomial, it's only a convolution to also be a dispersive parameter, it's right. For a negative binomial, it's actually not that the dispersion parameter is fixed, but you do need to know it in order to thin, which is the next slide. Because when you add two negative binomials, you add both parameters. Yeah, yeah, but there was one other one in the table. Like for binomial, for binomial, the probability is fixed. So you add the size parameter, but you. So you like add the size parameter, but you fix the probability parameter. Yeah. So to apply this, you need to estimate all this parameter ahead of time. Yes. Which can be a problem for high dimension. Yeah, yeah, for sure. Yeah. So like what if we get a nuisance parameter wrong? So you might have noticed that like in order to apply negative binomial thinning, I have to actually plug in this value v, which is the true over dispersion parameter for the data. And this is kind of a bummer because I probably don't know this. And so the idea is like, how bad do we do if we replace it with some bad do we do if we replace it with some estimate or guess v tilde? And so we of course don't get our three nice properties. We instead get three other properties. So the first two say that we still have the correct mean structure. So our x1 and our x2 will still kind of look like x, like they'll still have the same clusters. But for avoiding double dipping, we really wanted independence. And now instead we have non-zero covariance whose magnitude depends on the magnitude, by the difference between B and B tilde. So yeah, this is definitely like a limitation in practice is that In practice, is that if you don't know the nuisance parameter and you plug in a bad estimate, you could have pretty strong correlation between your training and test sets, which is going to invalidate type 1 error control. Okay, and like I use the negative binomial as an example, but similar for the Gaussian, you have to know sigma squared, but you don't have to know the mean. Why does net formula tree depend on how good your estimate is? Wait, this does depend on how good your estimate is. Is that what you asked? Oh, you have a tilted up. Yeah, sorry, it's like the ratio of the ambient. Yeah, it's like the ratio of B and B tilde. That's another question. Yeah. So if we, in order to estimate the parameter, we have to use the data. So then will there be another issue continuing as well? Yes, okay. So this assumes b tilde is not a function of x. It assumes you got it from elsewhere, like just to work out this nice formula. Of course, that isn't actually true. So the nice formula will not hold if you estimate it from your data. But we have a preprint about negative binomial thinning for single-cell RNA-seq, and we estimated the value. Cell RNA-seq, and we estimated the values with like the SV transform package. And in simulation, everything seemed like fine. But yeah, I came like, you're right, there's double dipping, yeah, from estimating it. Okay, cool. Okay, so all of this work is in a paper that's also on archive. Okay, so I'll really quickly go through a real data application. So I want to save a little bit for the end, because some of this stuff, like misconceptions parameter, and more distributions, is covered at the end. Okay, so in this real data. So, in this real data example, our goal is just going to be to validate the results of a clustering. And so, this is like a high-profile paper where their goal was to build an atlas of fetal gene expression. And so, what they did is for every organ, they took their cells and they first clustered the cells to identify the main cell types. So, these are the kidney cells, and this is shown in UMAP space. So, they estimated these main cell types, which are really well separated in UMAP space, and which they were able to recognize. They were able to look at other reference data sets and say, I know for sure these are. Data sets and say, like, I know for sure these are the metanephric cells. So then they went a step further and they said, I wonder if there's cell subtypes within the metanephric cells. And so they took just this cluster and they ran another round of clustering to come up with 10 metanephric cell subtypes. And then faced with this, they had to convince the reader that these cell subtypes were actually like stable and reproducible. And so they introduced a method that they call intra-data set cross-validation. And so what they did is they first clustered the cells, and then they treated those clusters as. And then they treated those clusters as the truth and did five-fold cross-validation with a support vector machine. And their intuition was: well, if my cluster label can be predicted when my own data was not used to train the SVM, well, my cluster must be reproducible. Great. So then they compared the clusters to the SVM predictions. And so in their paper, they have these two heat maps that are very diagonal. And they say, look, our heat maps are so diagonal, our clusters are so reproducible. But hopefully you guys see a problem with this, which is that we already used our data. Which is that we already used our data in step one to cluster the cells. So it really doesn't matter that we do cross-validation here, like every cell's data has already been used. And this is really easy to see in a toy example. So here's a data set, total random noise, one true cluster, but I estimate five clusters with all the data. And as soon as I've drawn this here, it's pretty obvious that a support vector machine is going to be able to figure out that the pink is on the top and the blue is on the bottom, even though that was originally just driven by random noise. And so indeed, if I do their And so, indeed, if I do their procedure, I get 96% accuracy, and I say, yay, my clusters are stable. And so, luckily, data thinning provides a really simple alternative. So, here I use negative binomial data thinning, actually with estimated over-dispersion parameters, yeah, because this is real data. So, here's x1 and x2. And so, if I, the idea is just that I'll run my k-means with k equals 5 on x1, and then I'll do it again separately on x2. And in this case, like these two cells belong to the same cluster in x1. Cells belong to the same cluster in X1, but different clusters in X2. And so that's an immediate strike against reproducibility. And so I can summarize that with the adjusted RAM index here at 0.01, because these are completely independent clusterings. And so data thinning provided a much more reliable way to see that there were no true clusters in this data. And so here I reproduce the analysis from the human fetal cell atlas, where I used their flawed interdata set cross-validation procedure. And then I tried again with data thinning. Tried again with data thinning, and for the main cell types, the ones that were really well separated and recognizable in UMAP space, I still get an adjusted randomness of 0.87, which is quite high. So, like the main cell types were like definitely reproducible. However, when I do the cell subtypes, things drop kind of a lot to 0.43. So, this is a lot better than the zero from my toy example. Like, there does seem to be some sort of mildly reproducible structure within the metanephric cells, but definitely not quite as much as I feel their picture. Quite as much as I feel their picture claimed. Okay, great. So I'll just spend like one minute on ongoing work. Oh, I have five minutes. Okay, great. Well, I want to save time for questions. Okay, cool. So to summarize, like the way I see it, there's kind of three ways to avoid double dipping in analyses. So the first, which I did not talk about today, but I also work on, is developing specialized methods such as selective inference. And so the main strike against these is just that we have to develop like a new statistical framework for every new problem at hand. So it's not super flexible. Problem at hand. So it's not super flexible. And the reason that everybody loves sample splitting is because sample splitting is so flexible. I don't have to come up with a new sample splitting framework for a new scientific problem. As long as my data are IID, I can just use sample splitting and people understand it. However, as we saw in this talk, it's not an option in unsupervised settings and it's pretty unsatisfying in some other settings. So that's like the outlier example or like the switch and states example. Okay, so data thinning, like sample splitting, there's no bespoke solutions needed, and it works. Bespoke solutions needed, and it works in both supervised and unsupervised settings. But there's definitely some major drawbacks. So, like, I have to assume a distribution, I have to know or estimate the nuisance parameters, and also, like, why is it limited to convolution closed distributions? So, these are kind of areas of active work for me and some collaborators at the University of Washington. But this last one has actually kind of already been solved, so I'll just go through that in a minute. So, why were we working with the convolution closed assumption before? Nothing. Closed assumption before. Nothing in our goal really implied that we had to work with convolution-closed distributions, but it just happened to be the case that we were also restricting ourselves to x equals x1 plus x2. And that's kind of what led us to this convolution-closed family. That's not really necessary. It's nice that x1 plus x2 is x because it means that we haven't lost any total information about our parameters, but we would be just as happy if that was replaced with any deterministic function t. And then similarly, And then similarly, this top one was nice, but I would say that as long as we understand the distribution of x1 and x2 and they relate to our parameter of interest, they don't necessarily have to be the same as x. And so this leads to generalized data thinning with non-additive decompositions. So the idea is that if I observe some x, and I'm willing to assume it belongs to a family p theta, and I just happen to know something about p theta. I know some fun fact from probability class, which says that x could have arisen as some function. Could have arisen as some function of two independent random variables, where the two independent random variables relate to theta in some way, but they don't have to come from the same family. Well, it turns out that our method for working backwards is going to be exactly the same as before. We're just going to write down the conditional distribution of these random variables given x. And this tells us that we know how to actually recover these two independent copies. And this generalized framework, the cool thing about it, is that formalizing this notation helped us realize a key point, which is that we Point, which is that we're obviously going to be kind of bummed if we have to know our parameter of interest, theta, in order to be the thinning. Like in negative binomial, at least it was the nuisance parameter. But if we had to know the mean and the nuisance parameter, there would be no point. And so it turns out that if you want to guarantee you don't need to know theta, it needs to be the case that x equals t x prime x double prime is a sufficient statistic for theta, which is kind of cool because it's actually just the definition of sufficient statistic, but it's a nice way to like characterize the classes of distributions that we can and can't fit. Classes of distributions that we can and can't thin. And so, with these non-additive things, we can thin a lot more than we could before. So, this top part is all the stuff from the earlier part of the talk, but now instead of convolution closed, I've written natural exponential family. Because if you're more consistent about which parameter you have to know, these are just natural exponential families. But we can also do new non-convolution closed distributions. Yeah, and this is also on archive. And I'd like to thank all my collaborators. Yeah, great. Yeah. All right. I guess we have to thank you, Dr. McConnell, although we have like a few seconds. Yeah. So I've been thinking about saying, what if I've got something multivariate that doesn't fall, where I've got some nice properties of the margins, but I need to worry about something, do something like the calculator for the joint? Is there anything that I can? Oh, that's a good question. Yes. Oh, that's a good question. Yeah, so basically, if you ignore the fact that there's correlation between the entries and within each entry separately, you have independence between the individual entries, but you have weird cross-correlations. No, we have not worked that out yet. That's a good question. Yeah. I'm kind of unconvinced about the negative by normal exam, right? You're unconvinced? Oh. Yeah, so have you tried to figure out? I mean, so in single-cell data, right? I mean, you have very little information. I mean, you have very little information about this phase programmatic. So, is that something that helps you have you can you simulate data by break the data level? There gotta be something. It cannot always be. Oh, yeah, no, no, that's a good question. Okay, so like there's like this common assumption made, I think, or at least it's the assumption made by like, I think, like DESEQ and like SC transform, that there's like some relationship between like the over-dispersion parameters and the means that's like smooth across all the genes. So, if we generate data under that assumption, So, if we generate data under that assumption and then use SC transform, things work pretty well. But if you generate data where every gene is sparse and just has a random overdispersion, you get really bad estimates. And yes, you can break it with bad over dispersion estimates. That'll be something about how much information, how well you have, I mean, yeah. So I'm a bit confused about the correlation. Fields about the correlation, how the joint structure could happen once you do the DD. Like, you know, if I have p-dimensional data, you do this, and the margin will have no problem. The joint, is it still preserving the joint structure? Oh, oh, that's a good question. So, like, if you have, if you have like data that's normal, and is it here? Yeah, yeah. Okay, so if you have data that's normal, but like P features, and you want, like, you can't just apply univariate, and they're not independent, you can't just apply univariate. Independent, you can't just apply univariate thinning to every entry. Like the correct thing to do would be to apply multivariate thinning where you have to know the whole matrix sigma. Or it works. Yeah, so you'd have to treat it as like a vector. Yeah, so like in what I was assuming for the Poisson and the negative binomial, I was assuming my entries are independent. Yeah. Majestic for mean and the sigma cell thing. What is majestic for the dispersion? What if you're dispersing correctly or your error or what you're at? Oh, yeah. Oh, yeah. So, yeah, so I think one reason that, like, the okay, so we have this result. Sorry, I know I'm out of time. Oh, gosh, it's really far back. Okay, so we had that result with the B tilde, which basically says that, like, you'll, okay, also when you're not in a Gaussian setting, like, covariance is not really a perfect way to measure independent. So, we have a couple results about like distance covariance, but I always think the covariance works pretty well to get a sense of how dependent your data are. So, you have B and B tilde, but that calculation assumes. Be in B tilde, but that calculation assumes B tilde is not a function of your data. I think that because B tilde is not your object of inference, whatever peculiarities of your data led you to a specific B tilde don't really affect anything downstream. But I think if you were actually estimating the thing that you wanted to do inference on, plugging it in, and then doing some, like that seems a lot worse to me. Like the fact that the beat, the detailed is like orthogonal from the mu, I think means like mistakes in the estimates of detailed. Like basically like you get noisier train and test. Basically, you get noisier train and test if you estimate it badly, but I don't think it affects your downstream estimates of meat very badly. There must be something about the kind of stuff you do. I mean, so your resource says you're violating independence like this. And then you're saying it doesn't matter too much, but it must be, that's what you're saying, must have something to do with what you are going to do with your split data. Yeah, yeah, exactly. Yeah, but I never wrote that down formally. I was like, oh, maybe you can prove that since they're orthogonal parameters, it doesn't matter. But we didn't get that far yet. Matter. But we didn't get that far yet. Yeah. Yeah, very nice talk. But I just have a question on the supervised problem applying these kind of ideas. So to follow up about infant's question, so basically I'm just interested in estimating your y equals x beta. Do a linear question. I'm interested in the spada. But I understand that if you're a simple joint of Myoji, then we can just follow your recipe and do the thinning and do the SPD spada. But yeah, that seems to me like overkill, right? Because if I just want to write linear revision, I don't need any of the pneumatics. Linear regression, I don't need any of the normality assumptions. I can still organize problems with radio. So, is there any way to do data thinning to assimilate thinning? Yeah, so like when we've done data thinning for regression, we don't thin x, we only thin y. And so, right, because like the x can just be left alone and then thin y. So, you keep the same x's in both. And like, you would have, like, if you, if you think your y is normal, you should apply normal thinning, but like, if you think your y is plus one, you could also do that and then still do, yeah. Be bad and then still do, yeah. And it's you need some sort of parametric assumptions, functional assumptions on y. Yeah, yeah. If you wanted to thin y, you'd have to use parametric assumptions. And so there's kind of a trade-off because you actually get narrower confidence intervals than under sample splitting if you do this. There's like an information inequality that I can like show you later if you want. You get narrower confidence intervals, but you had to only under the correct assumption, and you had to make that assumption. Yeah. Yeah. Oh, but I feel like I'm taking up too much time. Yeah, yeah. But your clockwork example was something a little bit. Your clustering example was something a little bit different. Which seemed like does this provide the solution to estimating how many clusters you have on the data set? I mean, that's a little bit what we were hinting at. We were not saying that that was what we were hinting at. Is this how we're going to do this in the future? I think that needs to be another paper because on real data, this within cluster mean squared error loss function does not seem to work very well, which is not surprising. And I think it could be, and then on my real data example, you'll notice I told you the clusters in the paper weren't reproducible. The clusters in the paper weren't reproducible. I did not make you a loss function curve telling you how many subclusters they should have estimated. I think this definitely, I like want to use this for estimating the number of clusters, but so far I have not found for single cell data a good loss function that makes this seem to work nicely in practice. I really like the stability interpretation that I did at the end of just like cluster x1, cluster x2, how similar were they? But you can't use like negative rand index as your loss function because if the clusters are like nested. Clusters are like nested, it's like higher at smaller numbers anyway. Yeah, I don't know if that answer made sense. I do want to write that paper, but I'm not done yet. Okay, I should go.