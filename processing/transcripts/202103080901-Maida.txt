Uh but it seems that let's try. Oh, yeah, okay. Okay, it's okay. Great. So I'll just once again welcome everyone here to Banff via Zoom. We're glad you could all join us for this conference on geometry and stochastics, or stochastics and geometry. I guess it's commutative in this case. We have a great lineup of We have a great lineup of great speakers for you, not too many. Two today and three each of the next day and the day after that. We will have lots of opportunity for socialization and mathematical discussion. As you have seen in the emails, we have two math discussion rooms and a lounge and a lobby where we can go to chat in between the talks. And there will be some informal sessions later in the week, which you'll hear more about when we get there. In a moment, I'll introduce our first. Get there. In a moment, I'll introduce our first speaker, but I just want to remind everyone of a few things. First, please mute yourself during the talk. And if you have a question during the talk, please post that question in the chat. And I or one of the other organizers will keep the question or decide whether to interrupt the talk and ask the speaker to address the question. Mostly, we will ask you to save questions until the end, and at the end, you Is until the end, and at the end, you can post your questions in the chat the same way, or you can raise your hand on your video or raise the raise hand icon, and the organizers will call on you in turn to ask questions. If and when you want to ask a question, we ask that you please turn on your camera. And we ask everyone, if they're comfortable, to turn on their camera as much as possible so we can all feel like we're actually here as a group. And with that, I will now. With that, I will now turn the floor over to our first speaker, Milaine Maida. Milain is a mathematician at the Lille, University of Lille, in France. She was a student of Alice Guionnet many years ago, not too many years ago. Same time I was a student, so hopefully not too many years ago. She works in probability and mathematical physics, random matrices, determinantal point process, and all sorts of very interesting things. Sorts of very interesting things. She's going to give us a mathematical introduction today to two-dimensional Yang-Mills theory as that relates to many of the topics of this conference. And I'm excited to hear it. I hope we will all have lots of good questions for Milan in the next 15 minutes. Milan, please go ahead. Thank you. Thanks a lot, Todd, for the introduction. Thanks to the organizers for this nice opportunity. So I would like to thank you and also to say I would like to thank you and also to say that you gave me a kind of a challenge because I'm so a probabilist basically coming from random matrices and I'm not at all a specialist of Youngmines theory and there are a lot of specialists in the audience so I'm a bit honored and also impressed so I'll try to okay so to give you more like a mathematical tour let's say Tour, let's say, to give you a little bit the landscape of all the nice mathematics that are involved in this two-dimensional Young-Mill theory. So I'll start with the outline of the talk and from the beginning, I would say that this is kind of an unreasonable outline. Reasonable outline for a talk because, okay, I'll try to cover quite a lot of material. And the goal of this talk is more to give you a landscape and like a flavor of the mathematics that are involved. But okay, I would need, or one would need maybe two hours at least to introduce each of the topics that I would try to just answer. To just uncover in this talk. So, I apologize in advance. And so, I'll try to first give you like physical motivations, then explain you how one can give a rigorous construction of a young male's measure in the two-dimensional case. And then, in the third and fourth part, give you some mathematical. Some mathematical results in this framework. So, first on partition functions and then on what is called the master field, which is some kind of limit when the Jau group is large. Okay, and if you want to go deeper in this subject, so there are a lot of nice references. Of nice references, but I would like to maybe emphasize a very nice work, which unfortunately is written in French. But if you can read mathematics in French, I strongly advise you to read at the first chapter of the PhD thesis of T-Bollen1, which is so defended recently under the supervision of TA Levi. So let's go first. Go first to this description of the physics that is involved in the theory. So in this part particularly, I will be really, I mean, I won't try to give precise definition, but like just give you the motivation. So, for example, if you look at so in quantum physics, Physics, a particle can be represented by its wave function, but small than the wave function, what is important is the probability density is given by the wave function, and the probability density is given by the modulus square of the wave function. It means that you can define the wave function up to a phase, okay? But this has to be done. But this has to be done not in any way. It means that once you have chosen a phase, then it has to evolve smoothly on time. Okay, and so this is the kind of, so you have an invariance, but you have also constraint. And this is this kind of concepts, of idea, of models, but Young and Mills tried to Try to formally implement in a very general model. So the ingredients, there are two main ingredients, which is... I apologize for interrupting. Is there any way to get rid of the left bar? It's obscuring your slides partly. Ah, okay. Okay. So I thought that you couldn't. Okay, so let me. So let me try. So here, like that. That's great. That was better. That's better. Okay, sorry. So thanks. Sorry. So I'm coming back on this slide. Thanks. Thanks a lot. So, okay, sorry. So you have a space-time which is just a manifold, let's say a Riemannian manifold, and the invariance is represented by a group, which is A group which is called a gauge group. So it's in the theory, it's a compact Lie group with its Lie algebra. So I wrote the definition, but let's forget about the definition. You can just have in mind a group of matrices, like unitary matrices or special unitary matrices. And you can also use invertible matrices, which is obviously not compact, but it can be put in the same framework. Can be put in the same framework. And they have to talk together. And how do they have to talk together? They talk together through what is called a bundle, a G-principle bundle. It means that above each point of the manifold, you have a copy, you have a fiber, which is a copy of the group. So in the case of the phase, the wave function with the phase is just a circle above each point. Above each point. And so J is acting, the group is acting on the precipitate bundle, so P is just the union of the fibers. And so locally, you can define a section which is an inverse of your projection. So it means that locally above each point, for each point, you can choose a representant of the A representant of the in the fiber. And the fibers have to talk together. So you have something called a connection, which tells you how to perform parallel transport from one fiber to another. And from there, you can so you can define, so I wrote a bit formally, but you can define something like the coverture of a connection. Curvature of the connection. And the energy of the Hamiltonian of the model is called the Young-Mills action, and it's something like the integral of the curvature of the connection. Okay, so and so the Young-Mills measure, it's a probability measure, which is something like a Gaussian measure. So the Young-Mills actually. The Young-Mills action is quadratic, so you can see dÎ¼ Ym like a Gaussian measure, and it has the density exponential minus one over two Tsym omega with respect to something that I wrote d omega, but I wrote it in red because this is the obstacle, the main mathematical obstacle, because what Obstacle because what you would like to define here is something like a Lebesgue measure, a translation invariant measure on the space of connections. But in fact, this space is really nasty. It's not locally compact. So there's no way to make directly sense of this measure. But still the physicists work with this measure. With this measure, and they can say interesting things, but it's not possible to directly make sense of this measure. So I just wrote some examples in physics. So in the physical setting, so M is the space-time, so it will be four-dimensional, three-dimensional space, one of time, and the invariance. And the invariance of the Gauch group will depend on the theory that you are looking at. And the nice thing is that a lot of models, a lot of theories are like fitting in this broad framework. So you want to define this d omega. So one One way to think is that, okay, maybe the space of connection is very nasty, but maybe I can look at connections up to some invariance and get maybe like compactify a little bit my space and get something better. So, the invariants you have to look at are called a gauge transformation. So, they are preserving the fiber. So, they are preserving the fibers, and the action of the Gauss transformation commutes with the action of G. So, I forgot to say that G is acting by right multiplication or left multiplication or conjugation, things like that. And so I can look at the connections up to Gauch transformations. So, this is a better space, but it's not enough. But it's not enough. It's not enough to be able to define a Lebesgue measure on this space. And to go further, a key concept is the concept of holonomy. So what it means is that if you draw a curve on M, you can lift this curve in the bundle by the section. So you take the images by sigma and you get a curve in your bundle. A curve in your bundle, and you can get another curve starting from the same point, starting from sigma of c and performing the parallel transport given by the connection. You get another curve, and the gap between the two endpoints is an element of the group, which is here H, H1, and this is called the. One and this is called the holonomy of the connection along the curve, and it has very nice properties. It's invariant if you just reparametrize properly the curve. If you concatenate two curves, the holonomy is multiplicative. If you go the other way on the curve, you just get the inverse. And also, if you apply a Gauss transformation to A gauge transformation to your connection, then the holonomy, you apply kind of a conjugation to the holonomy. And the conclusion of this part is that you cannot define the Young-Mills measure on the set of connection even up to a gauch transformation. But if you look at the image of this measure that does not exist by the Measure that does not exist by the holonomy along a curve, then you get a measure on G, which is conjugation invariance. And this measure makes sense, and this is what will be mathematically constructed. Okay, so if you did not understand anything to this first part, you can just remind, keep in mind the concept of holonomy because this will go back in the other. Go back and the other things will be, I mean, will start from crash in the mathematical construction. So now let's go to the I'll just take the check the time because I don't see the time on my screen now. Okay, sorry. So okay, so let's go to the mathematical construction of the Mathematical construction of Young-Mass measure. So first, I will say one word: in a huge mathematical theory and huge literature. So I will stick to two-dimensional manifolds. So this is kind of one would say a toy model because the true physical models are four-dimensional, but you'll see that still this model. But you'll see that still these models are very interesting mathematically, even if toy models from the physical point of view. So there's a huge literature in the M equals 4 case, but I won't tell about it and stick to the two-dimensional case. So I told you that the introduction of the models was in the 50s by Jung and Mills, and I'll give you some steps. So, a discrete version was developed by Migdal in the 70s. Then, in the end of the 80s, Gross and Driver gave the first rigorous construction in the case when M is just the plane, is the plane, and they made the link with the heat kernel that will appear later. Colonel that will appear later, and I think that Bruce is in the audience. Oh, very honored. Then Witten studied further the discrete case. And then came other mathematical construction, so general in the general case. So one was by Singupta, and the other was by By Levy, by Thierry Levy, and I will explain Levy's construction just because I'm more familiar with it. And Levy's construction is related to the discrete models of Migdal and Vita. So let's go to this construction. So I will stick to two-dimensional Two-dimensional, real, differentiable, compact, connected, orientable, closed manifold. So, when I will say M is a surface, this is what I mean. So, you know that there's a nice classification. So, you have the sphere, the torus, the torus with two holes, etc. And the planar case can be seen, for example, as the limit when the torus of the sphere. The torus of the sphere, the area will go to infinity. So the planar case is also included. And also, the fact that the surface is orientable is not crucial, but it's a bit easier in the orientable case. So I just remind you something that I will use later, which is that the search surface can be. Such a surface can be represented by a polygon with four J edges where G is the genus. So, for example, the torus, you take a square, you identify the two sides to get a cylinder, and then you identify the other sides to get a torus. Okay, so this is an apartheid. So, let's go now. So let's go now to the steps of the construction. So the third step is to construct this discrete version, so to explicit Young-Mills measure for graphs. So what is a graph? A graph, I will consider a graph embedded on the surface. So a graph has vertices that are points on M. The on M edges, so edges are path between vertices. And the edges are non-oriented. I mean, it means that you can consider both orientations. So if E is in the set of edges, then E in the other way is also in the set of edges, and they meet also only at They meet also only at vertices. And so, if you cut the surface along the edges, you get faces, and the faces are supposed to be, all of them are supposed to be homeomorphic to a disk. Okay, so this is called a graph embedded on M. And then, once I have such a graph, I can choose an orientation of the edges. Okay, so I have all the, I am, I have. have all the I am I have I mean all the edges in E I have both orientations so for each edge I will choose one orientation so if you if you make a picture you just draw a rows an arrow on each on each edge okay and now I will define a configuration so So, it means that to each oriented edge I will associate an element of the group. Okay, so now I had the surface, now I introduce my compactly group and to each edge. So, the space of configuration is the space, the set of application from E plus. From E plus to J. Okay, so to each edge you associate an element of the group. And the goal is to study random configurations. So to define this randomness, to put a measure on this set of configurations, I need a concept of holonomy. So here the concept of holonomy is the following. If I choose a path If I choose a path on the oriented edges, so I choose a path. So here epsilon, epsilon, one, epsilon n is just one or minus one according to which orientation I choose. So I choose a path. So if I have a configuration which is a collection of elements of the group and a path, then I get Then I get an element of the group by multiplying the corresponding elements along the path. Okay, so I go through the path and when I meet an edge, I multiply by the corresponding, by the element of the group which is attached to the edge. And it's called holonomy, so this application, this map is called. Application, this map is called olonomy because it has the same kind of properties that we saw about the olonomy of the path in the first part. And then, so the Young-Mass measure on the graph J is a probability measure on this space of configuration. So it gives you a random configuration. Random configuration. And it's constructed as follows. So the reference measure is just the product of our measures on the group. So if you have here a random configuration which is GE, then the density with respect to this project of our measure is given by Is given by the product of the faces of the graph of the heat kernel. So the heat kernel is, so I'll explain what is the heat kernel. The heat kernel, it's indexed by time, and the time will be the area of the face, okay, the size of the face that you consider. And so you need And so you need an area measure on M. Okay, so you have the area of the face and you apply this heat kernel to partial F of J. So partial F of J is just the path given by the border of the face. So you you Of a face. So you look at a face, you go along the border, it gives you a path. This path gives you an allonomy. The allonomy of a configuration gives you an element of the group. And this, so PF of this element of the group is the heat kernel at time size of the phase applied to this element of the group. And you have a normalizing. And you have a normalizing constant. Okay? And so this definition also already appeared in the works of driver and Sengupta. And so it's usually, it's known as the driver-Sengupta formula. So now let me say what is the heat kernel. So it's like a Gaussian measure. So here is the definition. If you have a compactly group, If you have a compact Lie group, if you have a scalar product, any scalar product on its Lie algebra, you can, so through the adjunct representation, you can construct an invariant scalar product. So you can forget about the construction and consider that you directly have an invariant scalar product. And once you have the scalar product, you can define Product, you can define an autonomal basis. And when you have this autonomous basis, you can define a Laplacian, so a Laplace-Beltrami operator that takes a function, which is C2, and gives you a continuous function, which is given by, so here you take the sum of a significant. The sum of a second derivative in each direction given by the basis. And here you have to define the second derivative in a proper way. So in the Lie group. And this is, I wrote here the proper way to do it. But it's really like a usual, let's say, Laplacian. And so when you have a Laplacian, you can And so, when you have a Laplacian, you can look at the it equation, and the it kernel is just the solution of the it equation. And so we will go back. So, in terms, if you think what it means, so here, J, you can think of J as a group of matrices. Okay, so it means that if you are a probabilist, you think of Are you a probabilist? You think of Pt as the law of the Brownian motion on this group of matrices, and this is the law at time t. Okay, and so you have your discrete Young-Mills measure on graphs, and you want to construct the Young-Mills measure on the surface. So you have your surface with an area measure. You have your surface with an area measure, total area big T, GULA group, and you want to construct your measure and the group. So you have to work, here I'm going very fast, but you have to work to think about compatibility of graphs, convergence of graphs, etc. So you have to make, so for each graph, you can define a Young-Mills measure, but you have to care about. Measure, but you have to care about the compatibility of all these measures. So you want a measure which can be defined for any path and which is in a consistent way with respect to your discrete construction. This can be done, and this is the theorem here. So you can. So you can define a process which is called which is the holonomy field. So this is HC indexed by the path on M such that when you restrict HC to a graph, well, the marginal, let's say, has the law which is given by the drivers in Gupta formula. And this behaves well with respect to convergence. So if Cn converges uniformly and the length converges, then you have convergence in probability of the holonomy of the process. But okay, this tells you about compatibility. Okay, so this is an abstract construction. So what does it tell you about tells you about um about the the the the behavior um sorry for interrupting there was a question in the chat i thought yeah we have yeah um all paths or close paths that was on the previous slide i'm great uh not necessarily they they can be but i'll i'll address this question just right now thanks so um so uh so so let's look at the So let's look at the consequences of the some consequences of the construction. So this is the autonomy field. If you look at a transformation, a gauge transformation, so the construction should be invariant by this gauge transformation. So if I rewrite the properties about the holonomy, it means that if you But if you conjugate, so if you look at j of c1 minus 1 with the starting and ending point of the path, then it has to be equal in low to the initial holonomy field. So, what it means is that if your path is not closed, if it's open, so if C0 is not C1, is different from C1. Then, playing with all the Gauch transformation, in fact, this equality in law has to be true for every G. So, in fact, you don't have a choice. HC has to be uniform. In fact, it has to be R-distributed. So, that's why, in fact, not closed path, open path are not very interesting. You can read of them because. You can read of them because it's just uniform random variables, which you can just forget about it. And then, if the path is closed, then this equality just tells you that HC out to be a conjugation invariant. And you have also other invariants. Okay, so here I wrote a little example about how How for an example of a path, so dc d minus one, how the law of h of this path looks like. So this is a closed path, how it looks like. Well, so I am going a bit fast here because I will get out of time soon, but so in Soon, but um, so in terms if you are a probabilist, well, uh, the law of the holonomy here is related to the law of a Brownan bridge between starting from identity and which endpoint the commutator of two R distributed variables. Okay, so it's okay, it's it's you can. Okay, you can characterize it, but it's not very easy to study. Okay, so if t is the total area here of the surface, when t goes to infinity, well, it's easy to check that pt goes to one. And so here the law just goes to the Brownian motion. The Brownan function. And so t going to infinity corresponds to the Planar case. Sorry. Okay, so in the Planor case, you get some families of Brownian motions. So now I'll try to tell you a little bit about some mathematical results. Some mathematical results. Mathematical results of the theory. And so, when you have physical models, the first object, I mean, one of the first objects that the physicists try to look at is just a partition function. So the partition function is just the normalizing constant is the Z, Z T G, the normalizing constant in the in the in the in the in the In the definition of a probability. Okay, so let's look now at the example when J is the unitary group. Its Lie algebra is the space of Q emission matrices. So we know that, so here we go into more like representation theory or harmonic analysis. Theory or harmonic analysis. Okay, to compute, to give a representation, a Fourier expansion of the heat kernel. So it goes like that. So you know that the irreducible representation are indexed by Jung tableaus, Jung diagrams, that is decreasing, non-increasing sequences of length n in of integers. Of integers, of integers. So, if you have a representation index by lambda, you have nice formulas for the dimension of a representation. And also, the eigenfunctions for the Laplacian are the so-called SU polynomials, and the corresponding eigenvalues are called the Called the Casimir of the representation, and all of them have nice formulas. And so you can have the heat kernel as a nice Fourier expansion. So in the basis of show polynomials, and you have here coefficients that you can write in terms of the dimension and the Casimir. Okay, and so from here you can get And so, from here, you can get an expression of the partition function, so in the orientable case, so according to the genus, and it's the sum of exponential minus c2 lambda t over 2 d lambda to the power 2 minus 2g. And this is obtained using the fact. Using the fact that, in fact, the partition function does not depend on the choice of the graph. So you choose a very simple graph, which is just, in the case of the sphere, you just take the equator. And if in IOGNUS, you just take a fundamental domain, as I showed you in the square in the case of a torus. So you have a nice formula. You have other formulas in the non-oriented. Formulas for in the non-orientable case, and so you can ask about the asymptotic behavior of a partition function in the case when n goes to infinity. And so with a few years ago with Thierry Levy, we showed that, I mean, we give a mathematical proof of a result by Douglas and Kazakh, showing that in the case of a sphere, you have all the three. You have all the free phase transition. It means that the free energy of the model has a discontinuity. I mean, the first derivative of the free energy is discontinuity at the time at the area p squared. Okay, and this is very common in random metric theory to see third-order phase transition. I don't really. Order phase transition. I don't really know what is the physical interpretation, but okay, physicists were motivated to look at this phase transition. And very recently, in the PhD thesis of TBOLEM1, it could address the Iogen use case. And in this case, there's no phase transition. And in fact, you don't need to re-normalize to look at the free energy, but directly the partition function as a As a limit. And it's given by Jacobi in terms of Jacobi theta function. So let's now go to what is called the master field. So sorry, I'll try to be quick because I don't have much time. But so, what is the idea of a master field? The idea of a master field is that if you look at the case If you look at the case when the gauge group is of dimension n with large n, you can look at the limit of your Young-Mills measure and you can hope that the limit will be easier to understand than what happens at fixed n. So the idea that it should be simpler was first appeared in papers of Tuft in the 50s, then In the 50s, then there were work of Gross Singer and Sengupta. So Levy on one side and Kelevich and Sengupta could construct the master feed on the plane. Then Daltvist and Norris quite recently could construct this limiting object on the sphere, and it's not yet achieved in It's not yet achieved for general surfaces, but there are many partial results. For example, Werner's paper by driver Gabrielle Olenkamp. I think three of them are in the audience. And also in the thesis of T-Bolum 1, there are partial results about the mass of fields on general surfaces. General surfaces. And those results are related to the behavior, of course, of brain in motion on group of matrices. And there are many results on this asymptotic behavior. Some are related to Youngman's theory, some are not. So asymptotics was first studied by Philippe Jan, then with Thierry, we got Then, with Thierry, we got some results on the fluctuations. Collins, Dalfist, and Kemp got some results about strong convergence in the behavior of the edges. There are also a nice paper by Driver Hall and Kemp on the Brownian motion on GLN of C, which is more, I mean, which is harder to understand. And so let's Uh, and so let's uh let's talk a little bit about uh this asymptotic behavior of brain and motion. So, this brain and motion on unitary group, for example, can be viewed, you can have different, at least three point of views. You can define it as the Markov process with generator one half of the Laplacian. You can look at you can you can define it so. Define it so as the so I told you that the tangent space of UN at identity is a space of skewermission matrices. So it's easy to construct a Brownian motion on skewer mission matrices. You just you can define it entry by entry, let's say. And then you want, in a sense, to exponentiate. Exponentiate this Brownian motion. And the good way to do it is to define it as the solution of a stochastic differential equation, which is written here. And also you can view it as a process with independent increments. So the stationary and independent increments. And the increments are of course multiplicative. Of course, multiplicative. And so they are stationary, and the law is the law given by the driver single-tar formula. So what can we say about the asymptotics of the about the asymptotics of the Brownian motion? Well, let's look at the The convergence of the moments. So you see, trace of the matrix to the power k is the sum of the eigenvalues to the power k. So this is the kth moment of the distribution of the eigenvalues. And so what this result say is that the kth moments of the eigenvalues of moment of eigenvalues of a distribution of a distribution of eigenvalues sorry converges to a quantity which is the kth moment of some distribution on the unit circle okay this distribution is denoted by nu t uh and we know a lot of things about nu t although its density is not explicit but it can be characterized through some transforms. Through some transforms. Okay, and then you can say more. You want to look at, so you are looking at holonomy fields, so you are looking at processes. You want to say something about joint laws of the process. So you want to look at the convergence as a process. And the good framework for this is free probability theory. So again, it's Theory. So again, it's hard to give you a crash introduction in less than five minutes to free probability theory. But let's say that Un of T, when N is large, are large matrices. So it's natural to expect that, in a sense, they would converge to operator. And the convergence also, of course, has to be. Of course, has to be specified, and you have to define what is the sense of S-convergence. So, the sense of S-convergence is written at the bottom of the slide. If you apply a non-commutative polynomial P to un of T1, UF of TK, okay, so you want to look at the joint distribution of your Brandon motion at different times, okay, then it will converge. Then it will converge to something. And the quantity on the right is defined on an operator algebra. Okay, UN of T is going in a sense to operators. They are living in an operator algebra and toe is a tracial state. So it's something behaving like exponential of a trace. Okay, so you. Okay, so you have a framework for defining the limit, and in this space, you can define a process of unitary elements such that for each t the law of ut is your measure nu t. Okay, so the distribution means somehow something like the spectral measure of ut the multiplication. The multiplicative increments are stationary and the multiplicative increments are free. So free is means that by our, I mean it's a notion replacing the independence. So it's kind of so these properties characterize something that is called unit free unitary or free multiplicative brain and motion. And Bian could show And Bian could show that UN of T as a process converges in non-commutative distribution to a free multiplicative brain and motion. And in fact, free probability theory is the good framework to define this master field, to define the limit of the holonomy field in the In the limit when n goes to infinity. So I'm just giving the definition, the formal definition. So a master field on your surface is a probability space, a free probability space. So an operator algebra with a tracial state and a non-commutative stochastic process indexed by loops because you can forget about open path. Can forget about open path with some natural invariances. And this non-commutative process is the limit of the holonomy path in the sense that I wrote in the last line of the slides. Okay, so the holonomy field that we define is. Field that we define is converging in non-commutative distribution to this non-commutative stochastic process. So each time you have a surface or you have an all-onomy path and you have to study this convergence and if you can't find this non-commutative process at the limit, then you have your master field. So now I'll go back to just the first lines of the slide. So in the Planor case, if you look at just at simple loops, we saw that the law of the process associated, I mean, of the holonomy of just one simple loop entering an area T, it's just a unitary brown and It's just a unitary Brownian motion. So it will go to a unitary, a free unitary. So if you look at a family of loops just with T growing, you'll get at the limit a free unitary Brownian motion. And so now what you have to do to construct your master field is to Field is to extend this convergence to more complicated loops. And I just say one word because I think I'm out of time now. So the key ingredients to make this extension are called Macken-Comigdahl equations. And so it's a kind of rule. It's a kind of rule, rules that allow you to decompose a path. Each time you have a self-intersection of your path, you can decompose it in simpler path. And this gives you equations on the moments of your holonomy. So you can, with these rules, you can reduce the study of The study of, let's say, regular path, closed path to more simple, to simple loops. And then you have a lot of complicated probability tools to control the variance, to get concentration results, to go to the convergent syndrome. So, okay, so this was very quick, I'm sorry, but I hope that I I hope that I gave you the general landscape and I convinced you that there are a lot of very nice mathematics to do in the field and where it involves harmonic analysis, probability theory, combinatorics and okay, so it's a very nice subject and thank you for your attention. Great, let's all thank Milen for her wonderful. Great, let's all thank Milen for her wonderful talk. Does anyone have any quick questions right now? It's probably best if you post them in chat or raise your hand in the chat.