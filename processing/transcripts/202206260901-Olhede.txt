So, hopefully, you can see my screen. Yes. Excellent. That's what I want to hear. So, I've taken the bold move of deciding to have a question for my title. So, we'll see if we have an answer by the end of the talk or not. Although I cheated, I already know what the end is. So, thank you very much for inviting me. Of course, I'm sad at missing the beauty of the local area, but you know. Area, but you know, well, we do what we can, so and thank you also for scheduling me this early because otherwise it would have been hard. So, I'm going to talk about something that's slightly different from what we heard yesterday. So, before I went to bed, at least, we mainly stuck to time series and a little bit of random fields, maybe, but I'm going to talk about the Fourier transform of a spatial point process. Spatial point process. And I should thank my funders, the ERC. We can forget about them now. So, one of the data sets that I take inspiration from is the Barrow, Colorado Island Forest Survey. So, basically, what people do, they take their grad students and they force them every five years to go out in the rainforest and to take a census of all trees and plants and record their. And record their location. And what you get then is a marked point pattern. So a point pattern is just a set of random locations where they have found a tree. And this is not the Bar of Colorado Island data set. These are generated data sets, but you can see the types of patterns you might get. You might have something quite regular, so a completely periodic regular pattern would be very irregular. You can have something like a You can have something like a Poisson process in space, where it's completely random within the area you're sampling. But often with trees, you'll have more clustered or even anisotropic processes, which are not isotropic in space. And one of the questions I've been asking myself since we started to analyze this data is that, well, first you might ask, why do we care? Well, the reason we care is basically one of trying to understand. One of trying to understand the changes in the ecosystem. So, my collaborator, who's a plant ecologist, is interested in the difference between having like plants and trees which have niches as a predefined role in the ecosystem versus complete randomness. So, we want to try to infer which of these two hypotheses are more most likely when we study. When we study exactly observations that are locations in space. And I like to Fourier transform things. So one of the first things we asked ourselves when we started to study this data set a bit further is what would we Fourier transform and what would we get out of it? So we started to ask ourselves, what is the Fourier transform of a point pattern? So the first problem with the point pattern is that many of the things we're using Is that many of the things we're used to from time series and other stochastic processes? Is that we cannot, for instance, do linear transformations in the same way we're used to. We can't think of doing linear filtering the way we're used to, because if we apply those to the amplitude of one when a pattern is present at the location, we don't get back a valid pattern at another point. So we have certain non-Euclidean properties that you need to receive. Properties that you need to respect. So, the first thing you really need to ask yourself: even if the process is homogeneous in space, if it's stationary, there's no particular areas where you have different behavior. And in the rainforest, you could due to elevation or chemical components in the soil or other matters. But when we do the analysis, we picked out a set of species which looks roughly homogeneous. Now, if we're trying to study a point pattern, Now, if we're trying to study a point pattern, we of course need to understand what are the basic properties that we're trying to understand. So, the first is the intensity, which is basically answering how many dots do we see per area. And the second that we aspect we're trying to find out is do we have a clustered or do we have a repulsive process? And that corresponds to having some sort of second-order measurement, like a covariance for a time series or a random field. Series or a random field, and what you look at naturally is something called a second-order product density, which is basically trying to understand if you're at one particular point, how likely are you to have other points at a certain distance apart. And if you have a stationary process, it means that this descriptor of the second order structure only has one argument, the distance apart of the two. Part of the two between the two different points. So, for a normal stochastic process like the types we heard yesterday, do I think after I went to bed that was a point process, but in time, we would want to define the spectrum as the Fourier transform of the covariance. And of course, the covariance, if you had a homogeneous process, only depends on shifts and not on two locations. So, you might say, oh, if I want to approach this process. If I want to approach this process and answer my title question, you might say, Oh, I will just Fourier transform the covariance because that's what we do in other settings. And in fact, what you see pretty easily once you start to look at the covariance is that the covariance is defined in terms of these other properties: the intensity that we've already talked about, the average number of points per unit area, and also this rho to the point. Rho to the product density. Covariance is very easily discussed for time series and random fields, but if you go to point pattern people, they instead like to discuss things either in terms of going at a particular location and counting all points within a disk around it or in an annulus at a certain distance, or we can talk about this nth order product density. And we like to look at the second one because it's related. Look at the second one because it's related to the covariance. And from an intuitive point of view, the nth order product density is just the probability that you get a certain number of points in infinitesimally small disjoint regions centered at some location. And if it's stationary, it only depends on the distance apart. And I'm trying to avoid gratuitously introducing lots of horrible measure theory, which I don't like myself. So, what people realized in the 1960s, when first people started to realize how wonderful the Fourier transform is in earnest, they said, well, let's just define the spectrum as normal. And there was work due to Bartlett in 1963 and 64. And he said, let's just take the covariance function that we've defined, Fourier transform it. And see, I've lost out an e to the minus 2i. lossed out an e to the minus 2i pi w dot z on the first line there but let's just Fourier transform and then study the particular patterns you get and if we do this using our description of the covariance that we already saw before as being linked to the intensity as well as the product density what we end up calculating a Fourier transform of well firstly the delta function at zero meaning we get a constant contribution across all Constant contribution across all frequencies, and secondly, we Fourier transform something which is like this product density, but lambda squared subtracted. And if you have complete spatial randomness, if you have a Poisson process, then the second part is exactly zero everywhere, and you just get a constant term for all frequencies. So, in a sense, this is our intuitive equivalence to white noise. So, the spectral density is constant plus. Is constant plus deviations away from constant. And the constant term is present at all frequencies. There is no Nyquist frequency because the points can take any value in the plane. Of course, you don't have an upper frequency. You can't assume bound limited really. And the spectrum shouldn't stop at any particular frequency. So Bartlett in the early 60s discussed this spectral representation. Discussed this spectral representation of pattern, and not so much was done. We'll meet some work of someone from the same country as me. So Peter Diggle looked at some applications. But until several decades had passed, nothing practical was really done until the 1990s. So multi-tapering will appear, don't worry, but it's going to come in. Worry, but it's going to come in a little bit later in our story. So, the first thing you might ask yourself is: what would the spectrum tell you in this setting? So, point process people likes to look at the pair correlation function, which is basically trying to figure out if you're at one point the relationship with other points. And they like to think of the scale as where you'd get a likelihood of having more points, probably as a reference. More points, probably as a reference to a regular process where it's like a checkerboard and you've got everything equally spaced. Of course, if you Fourier transform something like this, the scale you get out from frequency is about variation of your second order description. It's not just telling you that at a certain distance there is or there isn't correlation present, but it's telling you about the whole set of variability that occurs in the covariance. That occurs in the covariance or equivalently the pair correlation function. But we could say, yeah, we might have some trouble explaining this to the application people, but let's just start to ask ourselves, what happened if we get a sample? What can we estimate? What do we do with this? So, what Bartlett already did in the 1960s was he defined an estimator of the periodogram and The periodogram. And we already talked about the fast Fourier transform yesterday. We won't have a fast Fourier transform here. It's impossible because the locations of the points are all at any point in the plane. So there is no way we're going to have regularly spaced data. So we're not going to have fast or computationally efficient estimator. We've thrown that out through the window. But the easiest way of doing the estimation is. Of doing the estimation is just to start out by defining just a sum where we now have the locations in the argument of the complex exponential. And the weighting is just one because the points are absent or present, and they're present on the locations where you find them. So you can calculate the periodogram and you can get your first notion of a frequency description of the spatial point pattern that you're studying. Spatial point pattern that you're studying. And the first thing again to realize is: you know, you're not going to have a Nyquist frequency. You're still going to have to make digital decisions on implementation. What W do you calculate this as? And how do you do the implementation? I'm not going to focus too much on that, but this is clearly something you have to resolve when you do the estimation. So we see immediately. So we see immediately, I saw Don was here, so referring back to Percival and Walden, this is a direct spectral estimator. It's not linear in the data, right? Because you have the locations, which are the data coming as the argument, but it is a direct spectral estimate. And normally you would have something sesquilinear in the observations. We see this is not the case, but. Not the case, but you know, what are you going to do? This is the type of data you have, and you still have to understand the patterns you see in it. So, the issue with just implementing this is, well, there are at least four problems. The first is what frequency grid do you do the evaluation at? What's the highest frequency you look at? And we're going to see that there are even problems before reaching that. Problems before reaching that point. So let's just start as naive undergraduates would do by just defining what you could do and see what happens. So the simplest thing you could do is to just do a direct Fourier transform. Let's taper. Why do we want to taper? Well, we still know that we have this issue of ringing if we decide to go on off in the spatial domain. So we still want to taper the observation so we don't get a horrible Fourier transform. Horrible Fourier transform. So, what is the Fourier transform now? Well, we still have the complex exponentials. We still have the locations in there. The 2 pi or not is like a personal choice you want to do. I've stuck them all in here because I don't like to remembering them later on when I normalize things. And I've got a paper that I've evaluated exactly at the spatial locations. What happens if we do that? Happens if we do that? Well, first, let's just compare what would happen in a situation where we're more used to doing it. So, if we have a regularly sampled standard time series XT, where we have n observations and we use the fast Fourier transform, which is fast, then we would just multiply by the taper. The observed time series would take values for each time point. And I'm now swap to K instead of W, just so you see that. Instead of W, just so you see that we're in a different dimension, but we'd get an easily written out DFT. So we have the taper. We get a little bit of very narrow bind bias from the width of the taper. We get rid of broadband bias, as you all know. You can come up with asymptotic, so this is a good idea. And you have the grid of frequencies to do with the bandwidth where you can get independent estimates. And it removes. And it removes issues that would come on with good old-fashioned asymptotics for time series, where you have to assume that the process has a very smooth spectrum. And that's some things which probably Adam, my collaborator, will talk a little bit about the talk after mine. Having done that, it's standard to get the raw spectral estimator by just doing the module squared. And asymptotically, as we get a longer and longer time series, we get an unbiased estimate. We get an unbiased estimate, but the variance doesn't die down. Okay, so what happens now that we have a process which just corresponds to locations in the plane? Well, we can still argue that the Fourier transform as we've defined it converges to a complex Gaussian observation. We can, for instance, refer to Bissio and Bon-Petersen, and I'll reference my paper where we show. I reference my paper where we show that you satisfy the arguments they need and you end up with a variance corresponding not quite to but nearly to the spectrum. And depending on which frequencies you're looking at, you might get a little bit of relation as well, which means that the expectation of j squared need not be exactly zero. And the first issue we see here is that compared to our experience of time series, To our experience of time series, we don't have a zero-mean process. Why don't we have a zero-mean process? In time series, that's easy to get, you can just subtract the mean. Well, the rationale here why we don't get something zero-mean is that a point process of locations where you have waiting one is something fundamentally and intrinsically positive. So, we really need to do something to it in order to get rid of this mean. So, previous. So, previous authors that studied the original estimators introduced by Bartlett suggested that you shouldn't calculate at low frequencies. So if you used a window or taper function h, you would just stay out of its bandwidth and not go down near frequency zero, and then you wouldn't have to worry about this mean. Others suggested that you should go down to a certain low frequency and then assume. Frequency and then assume that what happened at lower values, you would just take the same value as your estimate as the lowest frequency you decided to go to. So there were various kind of ad hoc reasons for finding out ways to get rid of this non-zero mean. The non-zero mean is actually quite annoying when you're trying to do something practice. So what we decided to do, my co-authors and I, is you can easily estimate lambda. Is you can easily estimate lambda by just counting up the number of points dividing by the area. And we know what the Fourier transform of the taper is. So we just subtracted it off. And then you get a mean corrected Fourier transform. And we get a spectral estimator, which just takes the modulus square after we've done that. As the region we're studying, the process gets bigger. The estimator becomes unbiased, but Biased, but we still, just like for time series, need to figure out how to average or find some way of reducing the variance. And lastly, my collaborators coming from the application, they really didn't want to get out 2D images. The reason they didn't want to see those is they understand things in terms of scale, and so they only wanted to make radial summaries. So just So, just to show you what you get if you're trying to Fourier transform something like this, so you get a little bit of intuition. In the top row, you got three generated point patterns. You got regular sampling, a Poisson process, a clustered like a Thomas process, and then a clustered anisotropic process, iatacid direction. If you just start out by calculating the Fourier transform and doing modulus squared, And doing modulus squared, you can see that the Poisson process doesn't have any particular structure because we've chosen to de-bias this blob that would otherwise appear in the center. But as you start to have clustered or anisotropic processes, you see more patterns in the raw periodogram. But this, of course, is just like other settings. If you don't smooth, you might as well be trying to interpret tea leaves. Interpret tea leaves. So, if I don't mess up my timing too bad, we're going to say that it's better to use tapers. It's better because some of the mathematical properties become more convenient. But of course, you can use multi-tapers and you can reduce variance by averaging. And that, giving the story we're leading up to away, we see in the bottom row that as we start to use multi-tapers, we see a reduction in variance in both the We see a reduction in variance and both the clustering and the anisotropic nature become clear. So, in fact, what my co-authors and I showed was: if you just used the periodogram as people used to, there's a lot of math, blah, blah, blah. All it's saying is if you have a square or a box domain and you let the minimum side grow, then if you just take the periodogram as Bartlett's periodogram to begin with, As Bartlett's periodogram to begin with, you end up with the quantity you want as the region gets bigger and bigger in expectation, but you get a bias to do with the taper. It would be replaced by just the modulus squared of the Fourier transform of the boxcar if you didn't use a taper. So, this is not far away, kind of from what people knew. And it's been linked to chemistry theory. There's some old chemistry books from the There's some old chemistry books from the 70s and 80s which discuss point patterns in chemistry where similar results were intuitively derived. How does de-biasing help? Where you can make up the same story, you can make the same mathematical assumptions. You have to assume you have a twice differentiable spectrum apart from a frequency zero, where because of this, you're going to get a delta. You're going to get a delta function because of this, one of the contributions in the covariance. But once you subtract the estimated bias, you asymptotically get what you expect. This is not surprising. It was not known and that's perhaps like worked out properly and no one had figured out how to do the de-biasing, or at least they didn't talk about it. But we mainly see these two results as justifying how we want to do. Justifying how we want to do spectral analysis. And as I said, Digglegates, et cetera, just suggested you shouldn't go to frequencies that were too small. Okay, so now we want to make a multi-tapered estimator. So the first thing to point out is because we want to evaluate the taper at arbitrary location in R2, we need genuinely continuous functions. Because if we just have discrete tapers, We just have discrete tapers, and we want to evaluate the tapers at the locations in R2. We're going to have a problem, and we really don't want to interpolate or do something funny because that's going to throw away data. And the most convenient paper I found, because my collaborators liked coast form expressions, is I went back to a paper of Ridel and Sidorenko. I think one of the authors was at least there yesterday, and they proposed. And they proposed continuous time minimum bias tapers. And that's what we ended up using so that we could get to multi-taper estimates. For obtaining a radial estimator, which the ecologists really wanted, my co-authors and I hand-crafted a taper. I almost feel ashamed to admit it. Based on using Hermite functions and putting them together in a sort of semi-cunning way. Semi-cunning way. So, previous authors doing point process and time, and I'm a bit cautious of saying something because I think one of the authors were here yesterday as well. If I understood their paper correctly, they had averaged and interpolated and used discrete taper. We could, of course, have used the continuous spheroidal wave functions as well. Now, I'm just going to add a quick little note about how we do this in multiple dimensions. Dimensions. So, my ecologist co-authors were quite snippy about not wanting to do it in 1D because they didn't see any use of it at all. So, I'm just going to explain simply how we go to multi-tapering in 2D. So, just to get the principle across, if we go back to Ridel and Sidorenko and steal two of their continuous orthogonal tapers, let's take G1 and G2, then as we make two. Then, as we make 2D tapers, we basically multiply them together to end up with four orthogonal tapers. So, you just keep on going if you want to use more 1D tapers. If you want to use P of them in 2D, you just end up with P squared tapers. And then, with these choice of tapers, and we now form them by doing multiplication across the two domains, but we're going to assume we have capital P tapers once we've done this function. Papers, once we've done this funny combination of them, then to reduce variance, we then end up with a non-parametric estimator. So, yesterday someone talked about eigenvalue weighting or not. We were just lazy and we just did the same weighting of each spectral estimate with each taper. You have to be a little bit more careful as you start to argue about the covariance between each estimate. Between each estimate that you get for a point pattern, and it gets really ugly because you have to use something known as Campbell's theorem. You know, you don't have issues and lots of other nice things simply because a point pattern is not fully specified by its second order structure. So when you go to covariances of squares, you have to do all of the horrible calculations. And if you want to see them, they're in the preprint. So just as a sort of So, just as a sort of easy check, you can just ask yourself: does multi-tapering work? I mean, if you come in as a naive ingenue and all you've seen before are estimators which are inconsistent, your first question is, does multi-tapering actually improve what's going on? So, I'm going to show you, and I know I have three minutes left, so I'm going to not show you for too long. I'm going to show you what happens for different sets of processes. So, we're going to look at matern processes. At Matern processes for point patterns at the Thomas process, and we're going to have a simple Poisson, which is just a little bit too easy. So I'm going to basically point out that all the funny symbols and squiggles here, and I'm going to go to a zoomed in version in one second, are different sample sizes. So just to show you for two different matern processes and the Poisson. And the Poisson. Of course, if you look at the legends, the little round black thing has the biggest sample size, which we expect to be the best. The yellow thing and the green things have small sample sizes, we expect to be the worst. And what we're basically seeing here is the integrated variance, the squared integrated bias, and the mean square. So just look at the bottom row, which is the mean square error. And we're comparing as we start to do multiple. As we start to do multi-tapering on a logarithm scale, how much better we do from multi-tapering. And we could also smooth the periodogram, which is what we see at the end here. And of course, as none of you would deem surprising, multi-tapering is better. It reduces variance quite significantly. We can either use, we just use three or six tapers here, just as an example. Papers here, just as an example. And even if we just smooth, if we just use like a kernel smoothing locally on the periodogram, we also see a reduction in variance. But tapering is, of course, a lot easier to do than to figure out all the details for smoothing. More complicated processes, the same story still holds. And here are basically sort of plots which shows what. Sort of plots which show what happens when you start to rotationally average because most of the ecologists didn't want too much anisotropic information. They just wanted to know what scales things were going on. And again, these are too much information. There are five different processes, different sample sizes going from 25 to 800 absorbed points, but it behaves as you expect in that performance improves with sample size just like you would. With sample size, just like you would in any other statistics problem. The bandwidth factor at the bottom is just telling you how much you're averaging in order to end up with isotropically averaged process. And I'm not going to go into the details of that. So I'm probably supposed to stop now. Point processes are different from other stochastic processes. They take the form of the presence of an object at a given random location. As the location is random, we can't use the fast Fourier transform. We can't use the fast Fourier transform. However, because it's hard to come up with good spectral estimators, characterizing the second-order structure for a generic point process when you're not able to cut things off conveniently and frequency is hard. There are more details in the archive pre-promprint. And using multi-tapers we saw from the simulations worked. It works from the theory as well. And spectral estimation is much improved by averaging. By averaging. So I'm going to stop there and take questions. And I apologize for not showing more.