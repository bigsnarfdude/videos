Thanks to Valera for introducing quantum chemistry because that will save me some slides, some time. So what I'm going to be talking about is essentially how to try to use quantum computing for doing quantum chemistry. And the idea is to eventually bring the quantum vantage. What this means is to build a quantum on a quantum computer, build some classical computing methods in the chat. Computing methods in challenging systems. And what's the challenge with that? And what's the hope? Let's start with the hope maybe. So why do digital quantum computers provide a new hope? As Valera was explaining, the usual setup, it's the eigenvalue problem that we need to deal with here. But the idea is very simple. The solution is really a Solution is really a linear combination of Slater determinants in this space of one particle functions that are constructed into the anti-symmetric products. And there's an exponential number of coefficients you need to find in order to solve this problem if you don't account for any symmetries. And so that seems like a hard problem, but what it easily is mappable to is Q. To is a qubit problem because this slater determinants they could be essentially enumerated by the binary sequences of zeros and ones, ones on the occupied orbitals and zeros on unoccupied, right? And that nicely maps to qubits, which could be an up or down orientation or any linear combination of those. So now if we add to the this simple kind of encoding, Simple in kind of encoding of zeros and ones, also the power of entanglement, which allows us to encode these coefficients in the coefficients of the products. So we can, with n qubits, store 2 to the n complex numbers, essentially. And that's what gives us hope that the quantum computer essentially provides us exponentially large piece of paper where we can write down a solution if we want to. But this is just to write down the solution. But this is just to write down the solution. Someone needs to find the solution first, and this is the hard part. Now, this brings us to quantum algorithms and our friend Richard Feynman who said that essentially the quantum systems that we are trying to study, electrons here, they are quantum by nature and we can try to use quantum machines to study them, right? Essentially, it's paraphrasing his In his phrase in 1982. The hard part is starting when you try to create the answer to your problem on a quantum computer. So very simplistically, you can say that you're putting some information, question, or input data, right, on a quantum computer is some state and the quantum computer, in order to avoid losing information, needs to do unitary transformation of the question into the answer, right? Question into the answer, right? Now, this is all looks very simple, but the problem is that quantum computer may not know how to do this unitary transformation that you may know, right? And quantum computer knows only just how to do a few simple operations that we can control, right? And then decomposing the needed unitary into the ones that the quantum computer can do is really a quantum compiling problem that is. Problem that is hard makes things not so straightforward. And we hopefully will learn more from Falivia's talk, maybe this afternoon more about this problem. But this is the root of a difficulty in creating quantum algorithms. And that's what one can see in the last part of the Feynman statement: that it's a wonderful problem, but it doesn't look so easy. Essentially, I like to think about. Easy. Essentially, I like to think about this problem of decomposing unitary as a prime factorization with an extra complication that the prime factors don't really commute, right? So the unit is generally don't commute and you need to decompose the one that you're interested in constructing. Alright, so one problem. Now, what do we have right now? What quantum computers can actually do? And well, there are these big names. These big names that provided recently roughly more, even more than 100 qubits. But the short answer really is not much we can do with this, especially if we care about doing something that classical computers cannot do. And the difficulty here is that we cannot do these operations without error corrections because the errors accumulate, and essentially, even if you know how to err. Essentially, even if you know how to decompose your algorithm into a sequence of steps, well, those steps generate errors, and you start, but your quantum computer starts generating curvature, right? And the only possible solution to that is we need to create the error correction algorithms. For that, we need more qubits, and we to have one logical qubit, we need roughly from thousand to Need roughly from thousand to ten thousand physical qubits, and that brings the number of physical qubits to do something with error correction to close to one billion. Of course, these big companies have roadmaps to reach 1 million qubits in about seven years probably, 2030 roughly, right? And so, hopefully, that will happen. But right now, expectation is that hardware will come and we here to develop algorithms. Algorithms, and another possible opportunity here is to maybe think about algorithms to solve quantum chemistry problems and get inspired and go back to classical computers that actually work right now and develop something that works better than previous classical algorithms. This is the one reference that I'm not going to be discussing too much today, but this is where we did exactly that: created a quantum inspire method for. Quantum inspired method for organic light emitting diet calculations. Alright, with that, the goal of my group, I guess, is to reach the quantum advantage in quantum chemistry. And in order to do it sooner than later, I believe that it's good to find the good applications. And by good applications, I mean the ones that are not too demanding but showing the advantage, right? And the way I see it is. And the way I see it is like the intersection of these three areas: high utility, classically hard, and quantum easy. What I mean by these abstract notions is that high utility is generally coming from our chemical intuition. We, those who are in chemistry and engineering, maybe, we know what sort of chemical compounds provide utility, like two examples here: catalysts, organic light diets, know some multi-computer. Diets, those are multi-billion dollar sort of industries. And then among those, we would like to further curve the systems which are difficult for classical heuristics, like DFT or too expensive for DMRG or multi-reference CI to run, because we don't want to bother a quantum computer, which is just finding its way with the systems that are kind of easy to treat with the classical methods. With the classical methods, and turns out today I'm not going to be talking about this problem, but it's an interesting problem to figure out the protocol that, if someone gives you a Hamiltonian of the system, how to say whether this system will be hard for most of the classical methods, because methods are so inhomogeneous, it's not that easy to formulate one simple criteria how to do it, or how to essentially one can think of it as a how to determine very precisely what stromi-related systems. Precisely, what strongly correlated systems are. And then, what I'm mostly going to be focusing is quantum algorithms and what makes things quantum easy. Because there are multiple algorithms, in different algorithms, you may have different parts, different approximations that affect cost. And of course, you could judge the system by how difficult it is to run a certain control, how many T gates, or how many. T-gates or how many operations you need to do eventually. Alright, so that's the plan. And when it comes to algorithms that we believe will be in future useful for quantum chemistry, very simply one can focus on a quantum phase estimation as a very simple scheme that potentially provides the advantage. And in the quantum computing literature, this Computing literature, this algorithm described the circuits and I think very complicated manner for physical chemists or physicists. It's much easier to think about it is just simply we have a Hamiltonian with the eigenstates and eigenvalues. We want to find some state and some energy corresponding. Then for that, we could just construct the wave function that contains among the other states the state of interest. And if you do State of interest, and if you do this object, calculate the object like autocorrelation function here, essentially the expectation value of the evolution operator, and do the Fourier transformation, that gives you a spectrum of energy spectrum, essentially. And this is what the quantum phase estimation does just inside the quantum computer, because this operation, the exponentiation of Hamiltonian, is unitary and the Fourier transformation is unitary as well. Is uniquely as well. So, no need for circuits to understand this simple idea. Now, there are two questions related to this simple idea is how to prepare initial state, right? Because if you don't have a right state inside your initial state, you are not going to generate the corresponding energy value. And how to propagate? Because this operator is one of those cases which I was showing before, even though you Showing before, even though unitary, you know, but in order to break it down to the steps that quantum computer can do, it's not so simple as it turns out, right? Because you cannot ask a quantum computer just to exponentiate me some commit point. It doesn't know how to do it. Alright, so the outline of my talk, I will discuss the near-term operational quantum mind and software in some details, mostly focusing on state preparation because this is effectively the way to prepare. This is effectively the way to prepare the state for the propagation. There is a measurement problem also, which I'm not going to discuss, but Michael did quite a bit on that topic. We can talk more coffee break if you're interested. And then the last part will be mostly focusing on quantum phase estimation, different ways of providing this capability of doing the unitary evolution with Hannibal, right? Two approaches to media. Approaches parallelization, meaning a combination of units. Alright, so but just to stage the scene, we have a Hamiltonian and we usually start with second quantized form. The one electron and two electron integrals are sort of obtained on a classical computer because that's a polynomial kind of cost. And these are creation and relation operators. Now we can map them using Jordan-Lignal transformation or any other. Transformation or any other ways to qubit operations. Single Pauline matrices are usually represented X, Y, Z, right? And the subscript is pointing to the qubits, qubit number. And then by doing these standard operations, we get to the qubit Hamiltonian that is isospectral to our initial electronic Hamiltonian. So if we can solve now either qubit Hamiltonian eigenvalue problem or electronic Hamiltonian eigenvalue problem or electronic eigenvalue problem. Either way, we will get the same answer. Alright, so now in variational quantum eigensolver, well everyone probably here familiar more or less with this simple idea, we use variational approach and on the quantum computer we create the initial state and then we do the measurement and then classical computer just tries to find a better alternative to initial trial wave function and there is a feedback. Our wave function, and there is a feedback loop that goes between a classical quantum computer. Now, there are two main issues with this scheme: one is how to prepare a state, because even though you have all the unitaries potentially that you can run on the multiple qubits, then there are lots of degrees of freedom. So, you don't want to explore all this exponentially large space, you want to find some ideas how to build the units that. Ideas how to build the unities that will bring you closer to the identity state you are looking for. And then, how do you do measurement? Because this large box really is not that simple. Again, you cannot ask the quantum computer, just measure me any operator that you want. But I'm not going to go into this problem. We've done quite a bit on that, and this is one of our recent works on that subject. I will mostly focus on the state preparation because it's related. On the state preparation because it's related more to the quantum phase estimation. And in the future, we believe the probability measurement will be less of the importance. Rather, we will prepare the state in the circuit and then run quantum phase estimation once we have an error curve. Alright, so how do we prepare states? Very quickly, you could use either fermionic intuition or qubit intuition, but eventually you will end up with a sequence of gates. Some people start with hardware efficient others. With hardware efficient answers, but I don't recommend that because simply it's at this point there's not much physical intuition you can use. And in those two qubit of fermions, involved your unitary is just a product of exponential functions of some generators, which are anti-Hermitian operators. That's a typical parameterization of unitaries. And here the main question is which generators to use. And each generator is, you can think of it like an axis of rotation. With like an axis of rotation, right? So, which axis to choose in this exponential large space and in what order. Now, it turns out that most of the algorithms that we currently have, you could think of them as the following simple scheme, is that you take the Hamiltonian and consider some simple part of it. And the simple part is simple because it's usually a linear combination of operators that don't commute, but they form so-called Lie algebra. They close with respect. The algebra, they closed with respect to this commutator relation. And then it turns out that with all the normal algebras, you can effectively find this linear combination of the A operators that are analog of raising and lowering operators of thermodynamic. Using this raising and lowering operators, you could formulate the generators that I need to be anti-Hermitian. That's why you take the raising minus lowering, say, or products of raising minus products of lower. Raise and minus products and flower. And then, in order to find which raises which these generators are better, you could calculate the energy gradients effectively around which axis it's better to rotate to lower the energy, essentially. That's the idea. And it's really pictorially can be seen in the 2D sphere. Let's say we are going in sort of finding the directions on the manifold, which meet the Which meter group is really. And in order to do that, we are using local quantities. It's almost like if you are on the surface of the Earth and you analog of energy would be the height with respect to the sea level, you could use local gradients to see which directions are the best to raise your height with respect to sea level or lower your same idea. And now, if you compare this fermionic or qubit algebras, Call qubit algebras because you could think of X operators in fermionic space or qubit space because there is Jordan-Wigner mapping to go between them, but you could work in either case. So in fermionic algebras, you can start with Hartifock and these generators could be excitation operators. That's what gave rise to the unitary couple cluster theory, adapt techniques or generalized unitary couple cluster theory. To need to take up a construct theory. We have a whole review actually on various methods. And my group started working in the qubit algebra space where you can formulate generators as a products of Power operators. And the same ideas could be applied that I was explaining with Lie algebras, it's just in the qubit space. And we formulated this qubit couple cluster theory that was the first adaptive technique, and then the qubit adapt and adapt PQE kind. The QB adapt and adapt BQE kind of adapted that idea later. But if you compare the two hierarchies, the fermionic ones they have symmetry conservation usually, at least the number of particles is conserved initially. And the qubit ones don't conserve the number of particles. But that's okay because in what what I mean by conservation is really just uh conservation at the level of individual transformation. Level of individual transformations. But since there are many transformations, you may break the synergies at every individual step, but you will restore it by a variational principle because it drives the system to eigenstate of the Hamiltonian that has the right symmetries, right? And so that's why we are sort of winning by doing the cubic algebra, breaking symmetries in the middle. We are reducing the gate cost usually because there is a price you're paying for symmetry conservation and a You've paid for symmetry conservation and every step. And here is the illustration of that. If you look at how many two qubit gates we need in the qubit coupled cluster theory or unitary coupled cluster theory, red is unitary coupled cluster theory, qubit one in blue, for small molecules to reach the chemical accuracy in the small basis. So it's more like chemical precision rather than accuracy. So then you could see that the qubit coupled cluster here is systematized. qubit coupled cluster here is systematically better uh and uh I'm looking at two qubit gates because these are the uh the most uh kind of uh expensive uh in terms of the overall fidelity in the near-term devices. All right, but going back to now quantum phase estimation, the question uh is still how do we uh run once we obtain the initial states and using the additional algorithm, how do we do this uh propagation of the Propagation of the Hamiltonian. And how do we do the Fourier transformation? That's a standard algorithm. Some people even don't do a Fourier transformation on the quantum computer. They rather obtain altoculation function and do the Fourier transformation on a classical computer. But this operation is important and it's done or thought about really currently in two ways. One is trailerization. Proterization, you Factorization. Factorization you're probably all familiar with is an approach to substitute the exponent of the exponential function of sum as a product of exponential functions. And then you change the amplitude in front so that you reduce the error from that approximation. Now, the natural question is which fragments should you use in this approximation? Because we never can do k equals We never can do k equals to infinity. We usually find k equals kind of one or just a finite number, right? So then the question is which Hamiltonians we should pick. Because we need to differentiate them, we usually take the Hamiltons that are easy to diagonalize. And what this means, well, first of all, let's say the diagonal part is now called large C here. We could exprentiate. Here, we could expreate them. And what diagonal means in these representations, really, in the qubit algebra case, is just a polynomial of the power D Z operators, because in the computational basis, this becomes diagonal matrix. And in the fermionic space, it's just occupation numbers, right? So because this and occupation number operators. Now, with qubit to fermion mapping, these two forms kind of equivalent because you can go from qubits to fermion. Because you can go from kubits to fermions and get one from another. Alright, but then the next question, of course, is which fragments can you easily diagonalize in the operators that we deal with? So which ones we can contain easily? And the answer will depend on in which space you work in. So in the qubit algebra case, what uh makes sense is to group uh some of the commuting Pauli properties. Some of the commuting Pauli properties. So, if they commute, it turns out that you can easily find the unitary transformation that diagonalize such groups. And then you can break the Hamiltonian into the groups of fully commute fragments. There is a nice way to do that because if you take the Hamiltonian and represent every Pauli product, the qubit Hamiltonian as a point of the graph and connect only those that commute, then this decomposition of the graph to This decomposition of the graph to fully connected subgraphs is a standard problem, and it can be done with polynomial heuristics. Now, it turns out you can go beyond commutativity and study in fermionic space the linear combination of non-commuting fragments or fragments that can contain non-commuting operators. We call them Harti-Fox-solvable fragments, and the way to understand how we build them from the point. We build them from the quantum radio quantum chemistry. Everyone knows that one electron operators in the frequency space, they can be always rotated to the diagonal form by orbital rotations, is used. And then, but this is one electron terms, right? So we want to decompose the two electron commit orders. In order to do that, all you need in this form is to substitute linear combination of occupation numbers by quadratic function. That's it. So then, when you do orbital. So, when you do orbital rotation back, these fragments become just a normal two electron operators. And if I show you these operators, you will not be able to recognize that they are actually hydrophobic solvable. Hydrophobic provides the exact answer, and all eigenstates of these fragments are slated determinants. But we can decompose our Hamiltonian in this such fragments and then do the exponentiation of those fragments. How do we find this hard to focus? This hard to folk solve all fragments by non-linear optimization. So, because we can sum some of the linear combination of those fragments and they should match the initial electronic coming torment. So, in each fragment, we have orbital rotations and quadratic forms in front of the occupation numbers, and that can help us to do this continuum optimization. Alright, so now the question that Now the question that sort of we almost started with from was that okay, which fragments are better really for Trotter approximation? We use practical choice of the first order, where we just substitute the sum in its point by a product. And we could just simply look at the error of this approximation and see which fragments are providing a smaller error. But before we do In a small error. But before we do that, the error of the charter, really, one thing to kind of realize about it, it's related to the sum of the commutator norms because the more non-commutative your fragments are, the higher usually is the total error. And it turns out you could, well, this expression is not very conducive for analysis because who could tell what's the norm of the commutators, even though Of the commutators, even though fragments could be simple, but the norm of the commutator is not that simple quantity to calculate. But it turns out, using triangle inequality, you could massage the expression and make something simpler to analyze, this expression, that contains total spectral range. Why we call it total spectral range? Because these delta E n's are essentially the difference between largest and lowest eigenvalue of the fragment. And then there is this. There is this kind of we call it entropic part because the weights here is just the weighted spectral ranges of each fragment, and they are indeed linearized entropies in informational sense. So why I'm going in all these details is that clearly the fragmentation that makes the smallest spread of eigenvalues is the best. And the additional kind of advantage of that analysis is advantage of that analysis, it tells us that if you want to reduce the error, you want the entropic part to be the smallest. And we know from the thermodynamics entropy is the largest for uniform distributions. And the less uniform your distribution of fragments is, the better, essentially. And what gives the non-uniform distributions are greedy algorithms. The ones that first find the largest part and then the in what is remaining the smaller and smaller parts. Remaining smaller and smaller points. This is just a graphical illustration. And so this is really kind of the case where actually nature plays in our favor because the 3D algorithms are easy to do. They are relatively straightforward. And that's what we see painted the Trotter error. Now time to compare the Trotterization error for again a few small systems where we can exactly calculate what the norms of the computators are. The computators are. And for methods, here the qubit algebra methods in blue. If we use a non-greedy approach, that maybe reduces the number of fragments. And the greedy approach in light blue, you can see that the greedy constant is winning. And in the red and orange, the fermionic algebra approaches. Again, greedy is winning. And also, overall, you can see that fermionic fragments are winning. And the reason for that is actually quite simple because. Is actually quite simple because fermionic fragments they have molecular synergies built in, and if you calculate the kind of the norm of the commutator, it's usually the highest eigenvalue. That eigenvalue could be reduced if you consider the right symmetry subspace. And since the qubit fragments they don't have, they don't share symmetries with the total Hamiltonian, we cannot do that very easily. But with Moleca, with the fermionic The fermionic fragments we can. And compared to a very naive alcoholic decomposition, what we can do here is that our methods actually are 10 times better in reducing the error of the short decomposition. So it is worth decomposing Hamiltonians in a more sophisticated way rather than just exponentiating individual powers. Okay, so very quickly, linear. Okay, so very quickly uh linear combination of unitaries is another uh way to encode the Hamiltonian in quantum computer. And the uh idea here is that we need to represent Hermitian operator as linear combination of unitaries. Why will anyone do that? There are algorithms which I'm not going to go into, but they can use this decomposition. And because I'm not going to go into the algorithms, the only thing you need to know here is that the game Need to know here is that the game is to reduce one norm of the coefficient because it turns out that the cost of all these algorithms scales linearly with one norm, which is sum over absolute values of the coefficients, right? And time that you need to propagate. It's a linear scaling rather than quadratic, like in throttle case. So that's why this considered approach, throttle algorithms, and they are considered more efficient. But you need to reduce. But you need to reduce potentially this one more in these decompositions. And before we talk how do we reduce, how do we choose the unit theories, it turns out that it's easy to find the lower bound. No matter how you choose the unit theories, this lower bound shows that there is an expression that you cannot go lower no matter how you choose the unit there is. And that expression really can be obtained from the Can be obtained from the just triangle inequalities, which are very easy. And the idea here is that let me just skip the details and say that the one norm is always less than or equal than essentially the difference between again the largest eigenvalue of the Hamiltonian minus the lowest eigenvalue of the Hamiltonian divided by two, we call it spectral range. And so that's the absolute hard limit. Absolute hard limit. You cannot meet it no matter what unit risk you choose. The one norm will always be there. Of course, you can understand that since we're working in the second quantized formalism, these guys may come from the completely unphysical sectors of too many electrons or too few electrons. They may not be really physical energies that we're interested in. And that could be one of the issues that That one can fix, potentially. And this is one way out of this sort of theory that if you want to improve the sort of your one more, then maybe it's before you start thinking about the unit theories, you may start thinking how to change the Hamiltonian in order to reduce the spectral range. And this is what we did in our recent work. This is what we did in our recent work. My student came up with this acronym. Essentially, the idea is quite simple. What we want to do is, as I was saying, let's say we're interested in a particular number of electrons, which chemists usually do, right? And in that particular sector in blue, the number of electrons and E, right, so we have largest and lowest eigenvalue, but in the fox space, the entire folk space, we have, of course, larger eigenvalues and lower eigenvalues. And lower eigenvalues. And what we do with this manipulation with the Hamiltonian, we create the operator, that K operator, that what it does, it essentially changes the spectrum of all the states that have a wrong number of electrons, which we're not interested in. And it moves them somewhere. And by doing that movement, essentially we use the spectral range of the modified Hamiltonian. We can do that. Hamiltonian, right? We can do that. So, because we're interested in the number of electrons, say, there is this operator that we can create, k, that doesn't affect the states of the right number of electrons. That's why it acts like a killer for the right state of the electron operator. And what we do, we modify some, like there is a form that is relatively complicated, maybe looking, but the idea is very simple. You have a prefactors that kill anything that has the right number of electrons. Anything that has the right number of electrons, so the k doesn't affect the states of the right number of electrons. And we optimize the k so that the one norm of the h minus k is lower than the one norm of h in the decomposition using just the normal Pauli problems. Of course, k is not a symmetry operator, it's just using symmetries, and we use the number of electrons, but other symmetries can be used as well. It's just they didn't show as much improvement as Much improvement as with the number of electrons in this case. In order to see how successful this technique is, you could define the success essentially on looking at the spectral range of modified H minus K operator and spectral range of H that is projected to the right state of electrons. Projector is a hard operator to do in the second quantization, but on the model systems you can do it. And so effectively, Can do it. And so, effectively, what we want to do with this modification is to tack the wrong number of electrostates inside the spectrum of the right number of electrostates, right? And so that the spectral range will be within the same physical range. And as you can see, we do it very well. H minus K numbers for spectral range versus PHP. It's a projected version. It's within 1.5%. It's within 1.5%, so we really are close to putting all the wrong eigen states of the wrong number of electrons inside the range of the right number of electrons. So that's quite encouraging. And just the last point here, I'm not going to go over all the algebra, but there are also ways to improve the unitaries that you use for this linear combination of. For this linear combination of unitary decompositions. The one, a simple, of course, unitary decomposition would be just the Pauli products themselves. They are kind of reflections, they are Hermitian unitaries. But you can do better by turns out to combining certain combination of power products. And I was talking before about combining commuting products. Here we combine anti commuting products. Here we combine anti-commuting products because it turns out that if you sum anti-commuting Pauli products, then it turns out that what you get is again equivalent unitarily to the Pauli product. And you can use this as a new more complex unitary. And by doing simple algebra, you can show that one norm of what you obtain will be always less or equal to the previous one norm. So you won't do any harm by doing. You won't do any harm by doing the grouping, and more than that, if you do any non-trivial grouping, your one norm always will go down. There is a way to do fermionic reflections out of the activation numbers, and I'm not going to go too much into these details. They are in this paper recently published, well, submitted to our track. This one is actually already published. But then, the final results of L C V decomposition. I just want to say that we could. I just want to say that we could improve roughly three times the kind of the one more of the decomposition. And this is pretty close to what you can potentially expect. And the way we usually gauge ourselves is by looking at, alright, so what is one norm of our method versus one norm of a simple Pauli product? So you could for all the molecules For all the molecules, put as lines, where the 45-degree line will be power itself, it's slope one. And if you take the slope of delta E divided by two, spectral range is ideal slope, that's the absolute minimum you can get. And so you can see that every other method will be somewhere in between. But with these grouping techniques, we're coming close to the theoretical mean. This is only for the small molecule. Only for the small molecules. We are currently trying to generalize this for the larger systems and see whether we will be able to come close to the theoretical mean there. With that, just a last point that we were testing also for small molecules where we stretch a lot of bonds, whether, let's say, if you stretch bonds, the system becomes usually classically hard. And what we want to do is. And what we wanted to see whether this classically hard regime correlates well with the quantum hard. And by quantum hard, I mean what's the Trotter error? The larger the Trotter error, the more quantum complicated system is. Or one norm of LCU, that also could be a characterization how hard the larger one norm is for the particular geometry, then the harder it will be for LCU algorithm, or the number of measurements it will be, or number of unitaries in the perfection of. Eateries in the construction of the initial state. All that, those criteria of quantum hardness. And the nice thing about this experiment is that none of these, like say Trotter error or LCU cost, none of them were correlated with the classical hardness which we put here, say, in the H4 example. It's the difference between the full CI and say CISD function energy. Alright, so when you stretch the H4 linearly, you get For linearly, you get into the regime where the CISD classical algorithm kind of has the largest error, and it turns out the quantum algorithms don't worry about that. So, the quantum algorithms are fine. The only quantum algorithm that is sensitive to the classical hardness is state preparation. So, essentially, you need more unitaries to prepare state that is overlapping well with the full CI answer, but it's one thing to But it's one thing to create the exact answer in terms of energies, that's hard, but it's much easier to create the state that has a good overlap, but not necessarily chemical accuracy in terms of energy. So that's why we're quite optimistic about these results, that the quantum hardness and classical hardness are sort of separated, and we could potentially use in the future quantum computers to solve classically hard systems. Astomic related Atomic related. So, to summarize, quantum chemistry is still attractive to demonstrate quantum advantage because it's a relatively compact problem in terms of the Hamiltonian that you need to translate to a quantum computer. It's not really so much in the number of qubits, but in the number of gates that current limitations are. And we hope that with error corrections, we could use VT in the future as well. Use VKE in the future as a state preparation and use QP. Lots of algebraic techniques, as you've seen, could be used and studied more. Now, the great news is there is no relation between quantum and classical hardness, and that gives a hope for quantum algorithms. And more you could find about our works on my YouTube channel. It's one result of pandemics, I guess. And we have some of this. And we have some of these codes implemented in Tequila, which is developed together with Group of Allowance Proguesik. And these are the people who actually did the work. I have a post-doctoral opening. This is funding. And thank you all for your attention. Thank you. You mentioned that the limitation is however it meets. So if you go back to your However, it needs. So if you go back to your QCC versus QCS. So what I'm wondering is, you sort of seem to be losing gain as you go to larger and larger systems. And do you know, is there a point where you're going to get the equivalent? Or is there a scaling relationship that I don't see in the data that you have? Okay, so roughly the scaling could be that the fermionic. That fermionic operators that say, well, if we go like in the orders of excitations, single, double, triple, and so on. So say double excitation minus the excitation requires eight power products and then the next one requires, I think, either 16 or 24. So some like exponentially larger number of power products, right? So it keeps growing. So even if Keep growing, so even if we, but if we stay on the double level, I guess it's at least a factor of eight difference. Usually, for representing one thermionic, we could potentially use one Pauli product out of eight. Because they all have the same gradients in energy, if we define the gradients like this, like this expression, so as an expectation. So, as an expectation value of the generator commutator with Hamiltonian, all the products that originate from the same excitation, the excitation actually have the same gradient. That's why we can select one of them out of eight and proceed that that doesn't seem to reduce the quality of answers. Of symmetry breaking. I talked a lot about the number conservation. Is there some kind of hierarchy? Which of them is nasty if you don't control a problem? Number of electrons, yeah. Number of electrons. It's not as bad as spin contamination. Eventually you kind of probably, as I was saying, because variational principle drives you to eigenstate of the Hamiltonian, which has a right symmetry. So you break in them at the end. Breaking them at the end. If you got to the state with a good energy, then usually that state is already the symmetry deviations from the right symmetry state will be small because you cannot have good energy with completely wrong symmetry unless you in some sort of a degenerate state. Sort of consistent with the classical quantum chemistry world. Right. So know how to handle expensive. Right, right. When we solve to be QB by using the cubic coupled cluster or unitary coupled cluster, the order of the quota data is also important, right? Right, but in adaptive algorithms you sort of add while you go in and you sort of obtain usually the gradient when. When you include it already some states with some generators, then you just calculate the gradient after, and you select the next generator so that the gradient is the largest. So the order sort of builds up naturally if you do adaptive scheme. But if you try to use a technique like the unit recouple cluster single and double, here where you include Here, where you include all the single and doubles. There, the order will matter. We have a paper about that. Actually, there are some issues with this approach because the order matters, and also it turns out that if you keep building this, even if you have an electron system up to n electron excitation, the excitation, it doesn't really provide the exact answer in some cases. And the reason for that. Some cases. And the reason for that is that the algebra of generators is not closed if you just include kind of excitation, de-excitation. But if you close the algebra by including everything that comes out of the commutators, then it's exact answer if you go up to n electron operators. So you mean that the facts approach is like adapt or the method you mentioned? Right. So when you compare between plastical and quantum hardness, do you see any difference in what convergence you need? Like when the molecule have multi-reference signature when you are stretching it, you are just looking at the microphone. Right. So in those cases, actually preparing the initial state becomes harder, meaning that more generators will be needed. But there is But there is one point that I want to emphasize: that it's still easier to. We did the experiments like to reach chemical accuracy by constructing unit reands. You need, let's say, 50 generators. But to reach, let's say, 10-20% overlap between the exact eigenstate and the constructed one, you need five generators, for example, right? So, therefore, there is still, I feel, like, a There is still, I feel, like a big hope that we can construct with the relatively shallow circuits states that overlapping quite well and then do quantum phase estimation. There is no probably point of doing VQE once we can do QPE with the error corrected calls. Let's thank our 