Thank you very much, Zasha. So I very much hope that you had a nice time there. So thank you very much for the introduction and for the invitation for sure. I'm sorry that I cannot be there, but you know it's a long way and stuff. So in case you don't see the top line of my slides here, because there is the bar that's often there, let me shortly go back for Me shortly go back from the full screen and just to explain what I'm going to do. So, of course, I want to give some introduction first on the topic I'm giving here. It's the first talk in the morning, so I try to be slow. Then I introduce the results we have, and then I want to speak mainly about the history of the result. There is a quite recent and active history on that, and explain the proofs, at least the most important steps. Important steps. I think it's almost self-contained regarding some standard details. So let's see how far I can come here. And afterwards, I will talk a little bit about my favorite example and some remarks and extensions of our results. Okay, so let me go back to full screen here. So what is the general theme of my research, so to say? And I put the slides on my talks for On my talks for quite some while now, so probably many of you have seen it already. So, what we are going to do is we want to compute, whatever this precisely means, an approximation of a function in a class big f with a finite and hopefully small number of informations. I will define what this all means here. And the reasons are twofold. So, first of all, one reason could be we just don't know f, we can take some measurements and we want to find it, right? That's one of the typical things people have in mind. Typical things people have in mind, especially also in approximation theory. What is, I think, more important nowadays, even if even if we know F precisely, there is just no way to work with it on a computer or to save it even. And so you need to compress it because of computing issues, right? And compression means you want to make a small error. If the error should be as small as possible. So let me say that this F in F, many people are. That is f in f, many people are afraid of making assumptions of this kind, but it just means that's your model of some a priori knowledge you can give on your function f. Could be just some differentiability, but could also be some more structural. It is permutation invariant or whatever, right? You have some assumptions, because without any assumption, you can just make any computation. And the question I usually have is: what information is allowed, and how important is this choice? Okay, and so let me be a bit more detailed to what I mean with all that. So, the basic notation here is we always have a measure space, rather arbitrary. We consider L2, the space of square integrable functions with respect to mu, and we have a separable metric space F, which is compactly embedded into L2. Okay, so our F, so our a priori knowledge somehow A priori knowledge somehow should be modeled in the case of a metric space. But let me add, a metric space is not really a structure to have here. And I will explain why we need that. So a typical example that you could have in mind is you just consider some arbitrary D, some arbitrary measure, and F is the unit ball of an arbitrary separable norm space. And the second one could be you take a finite measure on a compact set, and F is just compactly embedded. And f is just compactly embedded into the continuous functions. So both are, say, big groups of examples where these assumptions are met. And we will see that our results can basically hold for many of these classes. What do I mean by an information? And an information of an object, of a function f in our class is given by a linear functional. So we fix some functional. So we fix some functional, c, some linear functional, and one information of a function f is just c of f. And it's not a surprise here. The thing is, in general, in applications in particular, we do not have access to arbitrary linear functionals. So what we have in our application is we have some what I call, or we call, admissible information. But this is a class lambda. It's just a subset of all linear functionals on a set. Of all linear functionals on a set F. And this could be some expectations of F, it could be coefficients with respect to a given basis, it could be derivative values, Fourier coefficients, whatever you have in mind. For this talk, just for the purpose of this talk, I'm talking about function values. So many of the results, and that's the part I will talk in the extension part here, can be generalized to other classes of information. And as I said, my main question usually is, what does good information mean? Is what is good information? So, not like I fix a class of information and want to check for which class of functions this may work. It's some kind of the vice versa question. You have a fixed class of functions in an abstract sense, and you want to know which information give you optimal information. In the sense I will discuss soon. Okay, so since I'm only considering function evaluation, let me define directly the two concepts. Define directly the two concepts that we want to compare. So, what we want to study are the minimal worst-case errors. So, something, so to say, that's the best we can do based on a specific information. So, first concept are the sampling numbers, to be precise, the linear sampling numbers. So, what we do is we take f, we take an approximation of f with respect to a linear combination that is given by some functions phi, and our function. functions phi and our function values. And now we just take infimum over all sampling points xi and all these functions phi i here. Supremum is inside, so we are looking for fixed points and fixed functions such that for all f in our class big f, this one is small. So as I said, that's the minimal error that can be achieved with function values. And since we make general theory, we need a kind of benchmark. General theory, we need a kind of benchmark, and what we use here is Ardakon-Gorov with, which are in the specific case of L2 approximation. I'm considering during this talk. Sorry if I didn't say that so far, I'm considering L2 approximation as in my title. And then the Kolmogorov widths are just the same as the approximation numbers and defined like that. So it's the same kind of object like here. You take F, and so you take the error that you make with your algorithm here based on some. Here, based on some informations. So, you take the f minus this algorithm, take the worst case of all that, and then you are just looking for the best functionals and the best phi. So, this guy here is a minimal error that can be achieved with arbitrary information. And we just want to know: is g n the same as dn? Are the sampling numbers of the same order or is the same as the Of the same order, or is there a class where Gn is very bad and Bn is very good, and so on and so forth. So clearly, we have this kind of inequality. So we always have that the sampling numbers, the minimal error with function values, is larger, the minimal error that can be achieved with arbitrary information, at least if point evaluation is a continuous linear functional on this class. By the way, maybe you already see here. By the way, maybe you already see here I'm talking about continuous linear functionals. And the word continuous means that you need some kind of a metric. It's the reason why our class F needs to be a metric space. Otherwise, of course, could be replaced by other concepts, but this is where the condition as a metric space comes in. Okay, so the Gn are always larger. The question is, how large is the difference between these two? Large is the difference between these two guys. And there are quite some early results by now, so they are about 15 years old, a bit over 10 years old. And these results show that you cannot always have a good comparison. So let us start with the, what I say, negative result proven by Eich-Henriz, Erich Novak, and Jan Wibiral in 2008. So if you have a sequence of numbers that are not in L2, Not in L2, think of 1 over square root of n, then you can find a Hilbert space which has precisely these Kolmogorov numbers, Kolmogorov width here, for all n, but the g n to almost not decay to zero. So the g n are some kind of lower bounded by one over log log n. So they are very large, although dn might be something like 1 over square root of n. So this result already shows that if you want to have Already shows that if you want to have a general comparison between Gn and Dn for all classes F, then you need to assume some decay of the Mogorovwitz. And this decay, one might guess, is this L2 summability. A bit later, it was proved by Francis Kuhl, Greg Wasikovsky, and Henrik Wozniakovsky that if you have a unit ball of a Hilbert space, or if you have some additional structure to just some decay on the Komogorov numbers, you also need that. Kolmogorov numbers, you also need that it's a unit ball of a Hilbert space, which fulfills all the conditions I just presented. And you have a decay larger than one-half, which is something like this is in L2. It's really one is in L2 here. Then you also get that the sampling numbers can be upper bounded by something decaying to zero at a polynomial rate. And this rate was approximately, if you look here, approximately approximately like something something like alpha over two. Something like alpha over 2. So you see there is some convergence, still there was a relatively big gap. And then there were quite some things going on in the last few years. So let me report on the final outcome of this newly new story. So as I showed on the last slide, that Dn is in L2 is a necessary assumption for comparison. But what if Dn is in L2? Would it be true that Dn? Could it be true that DN and GN are just the same? And the answer to that is no. The answer is, and this was only found recently by Eich Henrik Stavit, Krieg, Rejnovak, and Jan Wiebereil last year, they found the first example of a unit ball of a Hilbert space where g n is really of larger order than dn. So g n goes slower to zero than dn does. So how does this result look like? This will result like? So, for any sequence in L2, there is a unit ball of a reproducing kernel-Hilbert space, which has exactly these Kolmogorov numbers, but the sampling numbers can be lower bounded by this square sum here. So, this is the quantity that will appear quite often during my talk, but I will give some corollary soon, and then you can maybe understand better what this guy is going to do here. Okay, so for infinitely many n. Okay, so for infinitely many n, you have this lower bound. What this means if you have some sequence dn which is only in L2, but it is not in Lp for any P smaller than 2, so it's really on the border of L2, so to say, right? Then it can happen that this guy really is much larger than this guy here, than DN, so to say. So in particular, this result shows that there is a Hilbert space such that the sampling numbers are larger than the Numbers are larger than the Komogogov numbers times square root of log n. So they are really larger. It's not a big gap, but it is a gap here. Yeah, by the way, the Hilbert space they construct, maybe I could show that, that's just the univariate periodic Sobolev space of a certain smoothness. So that's really a very classical space where this result holds. I just wrote it down as an existence result, but it is a. As an existence result, but it is a quite constructive example. Okay, so what about upper bounds? Is it the best we can do? And the answer to that is yes. Essentially, yes. So there were quite a few active years since 2019. And the final result, I will come back to earlier results in some minutes. The final result is the following, proven by Mathio Delbo, David Crick, and myself beginning of this year. So F is a separate If f is a separable metric space of functions on D such that point evaluation is continuous on f, again that's not really an assumption, it's just needed such that all the concepts I introduced make sense. Then, for every p between 0 and 2, there is a constant, c depending only on this p here, such that we can bound the sampling numbers with constant times n points just by this quantity here with a p. Quantity here with a p. And if you don't like the p, you can use some additional structure here. So if f is the unit ball of a Hilbert space, then you can also just put p equal to 2 here. But if f is not, if you don't know that this is the case, you cannot put p equal to 2 here. Let me state that as corollaries, just to see the more um I don't know, I found the corollaries even more more impressing here. More impressing here. So, in particular, what we have: assume you have a unit ball of a Hilbert space, such that the Komogorov numbers decay at the rate n to the minus alpha, log to the minus beta. Then, sampling numbers are upper bounded as follows. Assume alpha is larger than one half, right? Which means it is really inside of L2, the sequence here. So, dn is really inside L2, then g n and dn, gn is just upper-bounded constant. Just upper bounded constant times dn. So sampling is just of the same order in the Kolmogorov numbers. The sampling numbers are of the same order than the Kolmogorov numbers if the decay is large enough. Moreover, in the boundary case, alpha equals one half and beta larger than one half here. You need the second condition such that this is in L2. Then you get the same bound here, but with an additional. Get the same bound here, but with an additional log n. And as we already have seen, both bounds here are sharp. First lower bound here is trivial. The second lower bound was proven by theorem I presented two slides ago. So, as I said, we don't want additional structure. So, let us state that for the most general classes we can consider. So, just f in the theorem, it's just a separable metric space, function evaluation. Space, function evaluation is continuous. Again, assume that the Komogorov numbers are n to the minus alpha, log to the minus beta. Then what we get, we can count the sampling numbers. Again, alpha is larger than one half, and we get precisely the same result like in the Hilbert space setting. So there is no condition at all here, and no additional factor at all. So the sampling numbers are upper bounded by the Komakorov numbers. In the boundary case, alpha equals one half. case alpha equals one half and theta now needs to be larger than one here, not one half. I don't go into these details here. Then we get an additional factor log n and clearly we have the upper bound constant, upper bound one, in all other cases. And our paper also contains lower bounds and these lower bounds show, so we construct classes F that show that all these bounds here are sharp. So this corollary just states that if If you only know, the only thing you know of your class F, except that it's a separable metric space and so on, is that you know some decay on the Kolmogorov numbers and you know precisely what is the worst that can happen for the sampling numbers here. I think that's quite interesting. Okay, so what was the motivation? And let me say something to the history also later then. Okay, so our motivation. So, our motivation in the beginning was not to obtain good error bounds. So, what we wanted to do is we wanted somehow to study IID random information in a deterministic worst case setting. And the reason for that is that's just machine learning. It's just we want to know if, if you get IID random information, so far you haven't seen random information, this will come on the next slides. We wanted to study if IID random information can be good. And for some reason, I thought it's not. Thought it's not because I don't know, I was just not believing that getting IID information is good in every setting. I thought there must be a case, some class of functions, where this just does not work. Yeah, at the end it turned out I was wrong, at least in the sense that I presented already. And the starting point of all that was a seemingly oversimplified problem, and here together with And here together with Eike Henriks, Josha Prochno, Avid Krieg, and Erich Novak, we considered the radius of random sections of high-dimensional ellipsoids. So kind of classical problem in asymptotic geometry, just as a simplified problem for random information and how good this can be. And surprisingly, we obtained some sharp bounds with very high probability. And we even realized that it was not necessary to consider To consider all sections, so to say. So it was okay to also work with special sections. And this, at the end, worked also to transfer all the techniques to function evaluations, just to give a high-level story on that. Okay, so moreover, in this paper, we obtained to some extent an explicit algorithm. So what do I mean by that? I mean, the algorithm that turned out to be optimal in this setting was just a weighted lease. In this setting, it was just a weighted least scratch method. So, what is it? Consider an orthonormal basis of L2, assume this to be given and fixed. And Vn is just the span of the first n of these basis elements here, basis functions. And then this weighted least squares method here is just to take the arc min of this discrete sum. We have seen that many times already, and we will see it many times during this workshop. That's a big theme, finding good points or. Points, or yeah, at least showing that there exists points such that quantities like that can be small, or how to bound discrete L2 sums by continuous ones, and so on and so forth. So this is the algorithm we used. We just want to minimize this discrete sum, but here is some weight inside, and this weight somehow includes all the basis vectors. So the first one is well known to many people. Is well known to many people is the Christoffel function, often called the inverse of Christoffel function. The second one was kind of new, I guess. I usually say that this kind of density seems to be first used by David and myself. And again, as in many talks earlier, I say I just tell you that because I'm waiting for some comment that I'm wrong, because this is too easy to have been overseen for so many years. I will show you the proofs in some seconds and then you see where this comes from. Seconds, and then you see where this comes from. Okay, so you have some weights here, and this weight function is surprisingly, or maybe not, also a density, probability density, and we will sample with respect to this density later. Okay. Yeah, this is again some, we have to fix something here. So I want to show some kind of the evolution of the proofs, but for convenience, I cannot do that for general classes. I cannot do that for general classes F. There are some technicalities involved. So let me do that only for Hilbert spaces. General result can be then, yeah, we can get that by some embedding, or you could prove it directly like that. But for Hilbert spaces, it's much easier. Why? Because assuming that your class is a Hilbert space, a unit ball of a Hilbert space, then what we can do is we can choose functions, we can choose an orthonormal basis here, which is also an orthogonal basis. Which is also an orthogonal basis in H, and such that the L2 norm of the functions, of the basis function, is the same like some constants times the norm and h of these functions. And these constants are precisely Konogorov width. So they are just supremum, the best error you can do if you take the projection onto your class VK. BK. Okay, yeah, what is the basis? The basis comes from a singular value decomposition, and there you also get sigma k's and so on. And since you want to use singular value decomposition, you need to study those bases. And these sigma k here are at the end also the guys in our density, so the guys that appear here. Okay, let me go a little bit into the proofs. As I said, I want to give you the important steps because, as we now polished all these. As we know, polished all these things, there are many papers on the subject. I think there is nice to see, and I think it's also quite self-contained. So, the first important insight is well known. The algorithm can just be written by kind of a matrix multiplication. So, algorithm A n of f is just this linear combination. Here are the basis vectors bk, and these guys here, these are just a vector of length n. Of length n, where n is just the vector n of f, is just the vector that contains the function evaluations divided by the rates. That's what is needed here for formal reasons. So that's the weighted information mapping. And g plus is just the Moore-Penrose inverse of the matrix that contains the first n basis vectors and again grades here. Okay, so you can write this as matrix multiplication and so it This is matrix multiplication, and so it's already clear what to do. You have to study some kind of singular values, but let's see what is the first step here. The first step is we split the error, some kind of splitting of the main part. So we know just from the form we see here, algorithm is exact if f is one is an element of vn. So if g is an element, if f is an element of vn, then you can just choose g. And you can just choose g equals to f here. So the algorithm is exact, assuming that function evaluations are not too correlated. Okay, so the algorithm is exact on the space vn. So for every element of the class, called g, we see that error, the L2 error of f, so of the algorithm an at the function f, so f minus an f, can be upper bounded by f minus g in L2 norm. By f minus g in L2 norm and this g minus a and f. Now we use that the algorithm is exact and can be written by this matrix multiplication. So we see this is just the same as g plus n applied to f minus g. Now we just use bounds on the norm of this guy. So we take norm of g plus between the respective spaces times times this n applied to f minus g in the respective space. And just for simplicity, I rewrite that a little bit, just to have a little bit more of a feeling what this is here. So we upper bounded error, the L2 error, L2 error I'm doing with an arbitrary function g in my class Vn. Plus, you need the smallest singular value of the matrix G, which is just the operator norm here. You have the smallest singular value of this You have the smallest singular value of this finite matrix times this discrete guy here. So you see the error you are doing can be upper bounded by some L2 error and some discrete error, some weighted discrete error here. And the point, what we need to do still here is choose a good G. So you could take the infimum over all G on the right-hand side. This would be the error bound you have. But it was not really necessary for our purposes, right? So we just choose, there's an omissing, we just choose G to be the projection on the class Vn, so G is just equal to Pn of F, and what we then get is the error for such a, not only for one function, but the error for all functions here, for all f in our class capital F can be upper bounded by the Kolmogorov. Can be upper bounded by the Kolmogorov numbers here. So they are just F minus P and F. Just the Kolmogorov numbers. We denoted them by sigma n. Thus, you have the smallest singular value here to the minus 1, and you need the supremum over all f over this discrete norm here. So let me repeat. Worst case error can be bounded by that. Here is still the supremum involved. Is still the supremum involved. What we need to do is we need to study concentration inequalities for random matrices because what we are doing now is we choose these xi here just randomly with respect to the density rho of x with density rho. We choose the points xi, and then we need to check what's going on with the smallest singular values here. And what we show, what we will show, at least as I will skip the At least, I will skip the details, but what we will show is that the smallest singular value of such a matrix G is just upper bounded by some square root of capital N, where capital N is n log n. So we choose capital N times is the same like something like n log n points, and then we get that the smallest singular value of this guy is lower bounded by square root of n. And this guy, which is just a discrete Which is just a discrete norm, is upper bounded by the same square root of capital N times the quantity we want to have in our upper bound. So the second part is clearly the technical and more difficult one. The first one is very classical. And yeah, the main tool here, I will skip the details afterwards. But the main tool here is a result we use of Oliveira or Mendelson-Pajor from two thousand six, which just gives you bounds. Which just gives you bounds on the operator norms of this difference here. So you take rank one matrices, just take the average, and you want to know if this is close to its expectation or not. And so these bounds are well known. The important point for us was this bound is just dimension free. So the k so if you take a random vector of infinite dimension, everything here still works. And this was important for us because we needed to work with infinite dimensional vector vectors here. With infinite dimensional vectors here. Okay, so let me skip these details. So we prove the smallest singular value is lower bounded and that this discrete norm is upper bounded. If you choose the points IID, that's the result we have here. So if you take IID function values with respect to rho, capital N of them, and this is something like n log n, then you get that the worst case error of the rated Of the rated least squares algorithm can be upper bounded by precisely this quantity here. So it's the same quantity we have in our general and sharp upper bound I presented in the beginning. Now we can do it for random points with the density we, at least we know, it's not clear how to handle it, but we know it. So we can sample these points and we get the same upper bound, but we need some logarithmic oversampling here. Let me add, the very starting point of all this was probably the paper of David and myself from 2019, where we proved exactly this result with p equal to 2 if f is the unit ball of a Hilbert space. So this general result was done later. Okay, so what would you do if you see you have you have this result you have, you have the upper bound that you might conjecture that it's optimal, and that you have n log n points. To have n log n points. Of course, the idea would be you remove the oversampling by choosing good points. But the question is how to do that. And what we are going to do is we want to choose n good points out of n log n points. And I think this was in a setting very close to ours. This was first done by Limonovan-Temlyakov, say, in 2020, and applied precisely in the way I will show you now by Nagel Schiff von Ulich a little bit later. A little bit later, or probably at the same time. I'm not quite sure. Ask Teal. So, that is for a subset of one of n, we have capital n points, we want for a subset call it j, we only want to consider the matrices gj and vectors nj of f, right? Are just shorter. We just remove some of the points. You only take points that are included. points that are included sorry okay so you only take points that are included where the indices are included in this j here right so and by just removing uh just using all the proof ideas we already had you get that the algorithm a j which is based on the subset of points you have the same upper bound but you clearly now need the smallest singular value of gj and the norm of this nj here so the first insight is So the first insight is norm nj you see here. So here you have norm of discrete norm of a subset of the points. Each entry of this guy is positive and each entry of this guy is also positive. So if you just add some more points, you get this trivial inequality. So every upper bound on Nf, also on the norm of NF is also an upper bound on the norm of NJ of F. The norm of nj of f, right? Which means we can just use precisely the same bound we already proved. There is no subsampling involved and nothing. This is just trivial. The improvement then was that you now need to find a subset j, which is of say optimal size, such that the smallest singular value of gj is still lower bounded by this. And this is, as many of you probably know by now. No, by now, based on the solution of the Cadiz and Singer problem, it is called Reavers. It's not Reaver's theorem. Someone told me that's not correct. It was close to Weaver's conjecture and Feistinger conjecture and many things in this area, but it was finally proved by Marko Sprimman Srivastava and in this form used by Nitsa, Nolevsky, and Ulanovsky in 2016. And what they do is they just say they sub-sample frames, downsample frames, which means. Down sample frames, which means if you have a frame, so you have vectors ui with the norm condition, such that their average, so the average of these inner products with w gives you almost the norm of w, right. And there is also a subset of optimal size, and the optimal size is given by the dimension of the space, such that you can do the same. So, this is roughly speaking. Speaking, the major result in this area are what we need here. And so that's the downsampling thing here. And as I explained, that's everything you need to do. So in the case p equals to 2, as I just explained it, this is the result of Nagel-Schaefer and Tino-Ulrich from 2020. And a little bit later, we also proved that for general classes F with the same technique we already had before. Technique we already had before. So, again, you have a general class, then for all p between 0 and 2, you get this, and you see there is still a logarithmic factor here. But that's not the end of the story. Problem with this approach somehow was, I'm just seeing that I'm running a little bit out of time, I guess. So let me be a bit faster here. So the problem was to remove the final log. To make it short, the theory. The theorem we just used for downsampling only worked for finite-dimensional frames, so for a finite-dimensional sub-problem, so to say, and was only applied here for a finite-dimensional sub-problem. So, this is some kind of the infinite dimensional part of the whole problem here and needs to be treated a little bit differently. And what we did together with Mathieu and David is we extended this down sampling idea to the sum of infinite-dimensional rank one matrices. Infinite dimensional rank-1 matrices. And this basically looked like that. So assuming you have these infinite vectors, again, you have a norm condition, but this n here has nothing to do with any dimension. There is no dimension, just a number. And you know that these, for some capital N might be extremely huge, I don't care, such that this average of this rank one matrices is close to, say, diagonal matrix. Diagonal matrix identity in the first n components, so that's an n times n matrix here, and just some matrix with norm smaller than one in all these infinite dimensional entries. So if this norm is smaller than one half, then you can sub-sample in the same way I just explained. You find a set of optimal size. This size is just necessary, except for the constant here, such that we have that this guy here has a lower. This guy here has a lower bound in the Leuveno order just on the first n entries of the matrix. So you take the first n times n block matrix out of this infinite dimensional matrix, and you get a lower bound on all singular values here. And moreover, you have an upper bound, but now on the whole infinite dimensional space, and not only on the finite-dimensional one. So that's the infinite-dimensional equivalent of the whole. The whole of the thing we have just seen before. Okay, yeah, and using that, and again, some standard finding the right vector, bounding probabilities, using rank one, this concentration of rank one matrices, and so on, we finally came to our result that we have. So, also the last log disappears here. Okay, so what was the next? So, let me, yeah, so let me finally. Yeah, so let me finally. I think I have to skip the extensions, but that's good. That's the reason why it's called extensions. Let me go come to my final example here. So one of the things that we found interesting when we started with all this is that for sampling numbers, it is still not completely known what happens if you consider the tensor product of spaces. Although, if you so let me say what I mean here: assume you have an What I mean here. Assume you have a Hilbert space, think of a univariate Hilbert space on a set D, and F is its unispole, right? Then, of course, you can just consider the default tensor product of these Hilbert spaces, which is again reproducing a Hilbert space on the domain D to the D, think of the unit cube or something. Again, take F D to be this unis ball. And one might guess there should be some easy way to get the sampling numbers of F D just from the sampling numbers of F because that's. Of f because that's just true for the Komogorov numbers or approximation numbers. That's just easy to compute. But for the sampling number, somehow it was not, and it is still not. Maybe there will be a better formula at some point later. So if what was known before is, if the sampling numbers are something like n to the minus alpha for some alpha larger than zero, then the famous sparse grid method, so sometimes called Smolyak's algorithm, invented in 1963 by Smolyak. 1963 by Smolyak himself. This gives the estimate that sampling numbers of the tensor product is then n to the minus alpha, like in univariate case, times a logarithmic correction. This logarithmic correction was alpha plus 1 times d minus 1. So the exponent here. And our result now finally shows that in this general situation here, you can just omit the plus 1. So you just get this, and this is precisely. Just get this, and this is precisely the same as the Kolmogorov numbers in this case. However, we still need the condition alpha is larger than one-half, although the result of the Smolyak result here works for all alpha larger than zero. Okay, so there is still a gap where we don't precisely know what the sampling numbers are in this case. Okay, one example here are the Sovolf spaces of mixed smoothness. And I think you all know them. Maybe I don't need to explain them. We take just periodic functions on the torus, and we want this norm to be smaller than one, and this norm is just the L2 sum, or sum sum, of all derivatives up to mixed order s. So you are allowed to take the derivative in each direction s times. And what was there? There was quite some known results. So it was known from Smolyak already. Was known from Smollhag already, as I explained, there is this s plus one here. And there was later some improvement of Infrit Zickel and Tino Ulrich who proved that there is some one-half here. And I'm very sure Volodya should be on the slide somewhere, but maybe he should tell me where. I worked too much on these slides and somehow had to delete some names. Here it was some kind of a prevalent conjecture that this letter bound was sharp in the specific case here. That turned out this is also. Here. But turned out this is also not the case. So, in this case, here, in the last one, we can just omit the one-half as already explained slides ago. And we now know that the sampling numbers for these classical spaces are just of the same order than the Kolmogorov numbers. If s is larger than one-half, which is needed. Yes. So, what is the random result in this case? So, in this case, just to say how constructive Just to say how constructive to some extent this is. Assume that you take these basis WS2, and this orthonormal basis I needed in the construction is just the Fourier basis, just the trigonometric system. And since this basis is uniformly bounded, it is not hard to see that in all our proofs, and it's very easy to see from the very beginning, that we can just use the density rho equals to one. So just uniform distribution. So there is no co. Distribution. So there is no complicated density and also no complicated weight in our squares algorithm. It's just the normal unrated least squares based on uniformly distributed points on the torus. And what we get is just take n points independently in the unit cube. And then the error that you get here is already small with large probability. It's smaller than that. And this bound is asymptotically already better than Smolyak. Already better than Smolyak. It's not quite the best bound, but it's already better than Smolyak depending on smoothness and dimension. And this shows that sparse grids are clearly not optimal. So random points might be better. And I think it's an open problem to find some explicit construction that is a good point set for this case. So which point set is better for approximation in WS2 than sparse grids? Than sparse grids. And I think that's a very interesting open problem. The reason for that is just sparse grids are so famous in the world now. There was really a sparse grid error for 30 years. And I just think that finding a new algorithm is probably at the end a beautiful algorithm and could start the next kind of sparse grid error in high-dimensional approximation. So I would be very happy to see that. And I think I'm skipping the extensions, and that's all I almost all I want. And that's almost all I want to say. So let me just give the final remarks here. Of course, there are some more powerful results in the randomized setting, like the results of Albert Coron and Mathieu Dabeau, something about absolute constants. So later, we will also see that the sub-sampling I presented can be done efficiently in some cases. So what is precisely meant by efficiently, you need to ask Tino later because he will give a talk on that this evening. And maybe we see that everything. Maybe we see that everything I explained here will be done efficiently at some point. We see. The other point is often no complicated sampling density and rates are needed, not only in the case I just presented, also in other cases. I think that's very interesting stuff and will be discussed in a talk by Ben Edcock on Thursday. And the final remark is LP approximation is rightly open in the same context I discuss here. I'm not sure if Igo is discussing that on Thursday, but at least That on Thursday, but at least talking about LP discretization, which is a closely related problem. So that's all I want to say. Thank you very much. Okay, thank you very much. So any questions? Should I speak into this one? I know. So, hi, Mayo. Thanks for the interesting talk. Maybe you should. Maybe you should mention that the sparse grids give this rate which you put on your slide, but they cannot do better. So one might ask, is one too stupid to prove a better bound using the sparse grids, but it one cannot improve on this result. So, in that sense, sparse grids are really not optimal for this setting. And a second remark, when you Remark when you ask for the good points in this situation, what do you precisely mean? Um, is randomness allowed, or do you want a deterministic construction, or what do you have in mind? Yeah, I really don't know. So, what of course we have a random construction, and then we can sub-sample that's not constructive by now, but even if we assume that's constructive, I don't, I'm not sure if this is really a I'm not sure if this is really a satisfying answer. I would be happy to see some kind of really deterministic construction, some combination of Smolyak and other lattices and then whatever. I don't know. Of course, a random construction would be already great. I would be happy. And practitioners are anyhow using random numbers. So what is the problem there? But I think we just miss some very basic structure of these spaces if we are not able to write. If we are not able to write down a point set that can achieve that, I'm just, I think that's a lack of knowledge. And in five years, at the next conference where we meet, hopefully it's not the next conference where we meet in person, but say in five years, most likely this is solved. I'm very optimistic on that. So, if you are fine with the square root of log, which you removed in your analysis, then I think we can give you some points at. So, I will maybe. Inset. So I will maybe come to that in my talk. But without that square root of log, I don't know so far. So this is really interesting question. Yes. So that's again, that's the sub-sampling strategy. And usually, if I would have the time, I would make exactly this advertisement. There is no need for finding an explicit deterministic construction, like many of us have done for many years, right? There is no need for that anymore because you can just take random points in many specific. Take random points in many specific settings and use some sub-sampling and whatever, finding smart data out of big data. That's what I'm usually saying if I'm invited for an interview for a position. And that works. You some kind of prove that it works. You have simulations on that. So why do we need to look for points? And I say, from a practitioner's point of view, most likely there is no reason for that. But from a mathematician's point of view, I think if we find the construction, I think if we find the construction, I would almost bet money on it that this is a very beautiful one. Because it's a beautiful problem, it's a classical problem, everything here is natural, only natural geometric structures are involved. There must be a beautiful construction. I am quite optimistic. Okay, thank you. More questions? Probably from Zoom auditory? Okay, if not, let's thanks, Mario.