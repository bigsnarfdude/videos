Must always be exposed. And the way to reconcile this thought experiment with the second law was to recognize that the demon is actually exploiting his knowledge about the microscopic process. It's exploiting some knowledge, some information about the microscopic process. And so information must be a thermodynamic quantity that enters the second loop and plays the role of an input, so that the resulting dissipation is a positive quantity. And from this intuitive understanding of the role of information, our understanding evolved in time together with the further developments of thermodynamics itself. And from the equilibrium theory that was developed in the 19th century, we have nowadays a full non-equilibrium theory which is able to characterize the energetics of microscopic processes that occur arbitrarily far from equilibrium. Prototypical examples are molecular motors or quantum. Molecular motors or quantum dots. And this theory is nowadays known as stochastic thermodynamics. And in this framework, I think that the role of information becomes particularly clear. And the reason for that is that stochastic thermodynamics basically builds a thermodynamic description on top of a dynamical system in terms of a stochastic dynamics, here represented by a master equation. And the correspondence between the thermodynamics and the dynamics. Between the thermodynamics and the dynamics is established with the so-called local balance assumption, which relates the rates of the transitions between two states, let's say Z prime and Z, to the variation of the thermodynamic quantities here expressed in terms of the variation of atomic potential phi and the work performed by a reservoir on the system. But independently of the mathematical expression of this look at the balance, what it physically represents is that the dynamical system. Is that the dynamical system is tracking down all the degrees of freedom that are out of being, all the degrees of freedom that are dissipating energy. And in this way, one can formulate a non-equilibrium theory of thermodynamics and the second law can be, for example, expressed as a balance equation for the entropy S, so that the variation of the entropy are given by the entropy reduction plus the entropy flow representing the entropy that is irreversibly exchanged with the environment. Reversibly exchanged with the environment. Like if dealing with a thermal machine, this entropy flow would represent heat flux with the environment. And the reason why this setup is particularly useful to establish a correspondence with the role of information is that both stochastic aerodynamics and information theory basically share the same kind of mathematical structure. They are both based on a probability theory. And furthermore, in And furthermore, in stochastic thermodynamics, the thermometric entropy is a shaman entropy, which has already appeared in formation meaning. And by combining these two theory, people were able to understand, let's say, how, let's say we have a global system which is bipartite and composed of two subsystems, how the dynamics and the energetics of the two subsystems are correlated. Is correlated. Basically, stochastic thermodynamics would provide the characterization of the energetics at the global level, but by combining the two, one can split the global second law into two local second laws for each subsystem, here a demon and an engine, if you want. And the local second laws resemble the global one. So in each second law, we have the variation of the local entropy for each subsystem. The entropy flow, we have the reservoirs. Entropy flow with the reservoirs each subsystem is coupled to, but then there's an additional term accounting for the information flow exchanged between each subsystem because one can use the modal information to quantify the correlations between the two subsystems. And while this has been understood more or less ten years ago, and it works pretty well for microscopic systems whose dynamics is well described in terms of stochastic processes. Describe in terms of stochastic processes. There are processes both in biosystem but also in synthetic chemistry which operate with such a large abundance of molecules that their dynamics is in practice described in terms of deterministic rate equations. For example, here I just do a molecular motor, a light-driven molecular motor, which is composed of a molecular axle with a bike group on this right-hand side. Right hand side, and by binding a microcycle and harvesting energy from a light source in such a way that the molecular axo can change its conformation, the macrocycle is released from the right-hand side. In this way, the molecular axo creates, produces a directional motion of the ring from the left-hand side of the axle to the right. Side of the axle to the right hand side. But the emergence of this directional motion is in a way related to the synchronization of the conformational change. In a way that the conformational change must happen just after the binding. And in a way, one can think about this as the moul√© part. Is there a question or has Or has I had one part of it? I think he was looking at it like this or something. He was trying to pin this one. Oh, sorry. You can mute your comment. So you can think about this as the molecular axle must know whether or not the ring is bound before changing its conformation. So in a way, it must gather information about the position. Must gather information about the position of the macrocycle before changing the conformation. And the interesting thing is that experimentalists which designed and synthesized these kinds of motors already think about them as two subsystems, one of the two, in this case representing the conformational changes of the molecular axon, which is directly coupled to the power source, the light source in this case, and the other subsystem representing the System representing the target of this molecular motor, in this case, the motion of the ring, is maintained out of equilibrium, not because it is directly coupled to the power source, but because the other subsystem provides free energy in the form of what they call information and energy asset means. And the interesting thing is that until, I mean, while at this level of description with deterministic chemical processes, there is a hymnal theory to characterize the Hermann theory to characterize energetics at the global level, a correspondence with information theory was totally absent because the two theories don't share the same kind of mathematical background, right? One is based on deterministic dynamics and the other is based on fluvial theory. And what we have done has been to fill this gap. So, as I said, afronomy theory at the global level is already well established and it can be applied. And it can be applied to the network of reactions for the molecular axon. Here are just all the reactions that are involved in this process with the microcycle and the molecular axon. And the thermodynamic description is again built on top of a dynamical description here in terms of deterministic dynamics for the concentration Z of all the chemical species. And the correspondence is again established with a local delta balance assumption, in this case relating In this case, relating the deterministic reaction currents to the delta G of each reaction, plus F. F here is atomic force created by the external reservoirs that maintain the system out of a view. In this case, we have just the light source. But the interesting thing is that, and this is maybe a detail, but it's important when dealing with autochemical reactions, is that when we work with deterministic chemical reactions, Deterministic chemical reaction, we don't actually need to rely on the localized advanced assumption as we develop thermodynamic post-screening procedures which allow us to basically track down the full energetics even if the dynamical system doesn't track down all the degrees of freedom that are out of equilibrium. And this is particularly important when dealing with photochemical reactions, because typically the photochemical currents don't track down all the intermediates of photochemical reactions. Intermediates of optochemic variation. Okay, this is a detail, but I wanted to mention it because I will further discuss this model where there are optochemic variations. But so, the problem again is to go from a global thermodynamic description to a local one for each subsystem. And the expectation is to obtain expressions like this, because these are the local expressions of the second law that people obtain in the framework of stochastic thermodynamics. And the way we proceed to derive these local second laws is the following. We basically recast some of the strategies people have been using at the level of stochastic thermodynamics, but we adapt them to this framework of the thermistical process. So, the idea was to consider that in our system there are two kinds of species, what we call bipartite species, meaning species that are univocally. That are univocally identified by a double state. For the case of the light-driven motor, if you consider the species involving the axle, they can be univocally identified by specifying whether the axle is binding the ring or not, and that this would be our X state in this representation. And the other state would basically have the conformation of the axle, right? Conformation of zero, conformation of. Then we consider there might be other species we call these ancillary species and these are the species that are not biperactile. In this case, the free ring which reacts with the free axle is an ancillary species. And then we further assume that the chemical reactions have a byproduct structure, meaning that they can change at most one state of the byproduct species, which results in having only horizontal or vertical transitions in this representation. Vertical transitions in this representation. The horizontal reactions are those changing the X state, whether the bipartisan species are binding or not real, and the vertical transitions are those changing the conformation. And just with this bunch of assumptions, one can already uniquely identify dissipation and entropy flow for each subsystem. The problem here was to identify, was to split the entropy of the bipartisan species. Of the bipartisan species in the entropy of each subsystem plus what people would expect to be like a mutual information contribution. And the reason why this is not obvious here is that this expression, this equation that one would find in the first pages of a standard information theory book, they rely on the idea that the entropies here are Shannon entropies and they rely on the fact of dealing with. Their line we're dealing with probabilities, while here we are dealing with concentrations. And the way we solved this problem was, first of all, by introducing a meaningful notion of multi-informations in terms of concentrations. And we did so by introducing the probability of observing the bipartite species in a specific state xy by normalizing basically the corresponding concentration by the total concentration. Concentration by the total concentration of the bipartisan species B. And from this probability, one can derive the probability of observing a bipartisan species in a specific state X, like the probability of observing the free axon independently of its conformation, or the probability of observing the axon binding the ring independently of its conformation. And in the same way, one can define the probability of observing the bipartal species in a specific state. Species in a specific state y. And by combining these probabilities that can be expressed in terms of concentrations with the definition of the neutral information, one can express the neutral information in terms of concentrations. But this was not enough as the entropy of the bipartisan species is not a Shannon entropy. It's not a Shannon entropy in terms of the probability that we just introduced to define a neutral information. Introduced to define a neutral information in terms of concentration. And furthermore, when dealing with molecules, molecules have a sort of internal entropy, which appears in the expression of the entropy of the vibrant species as the standard contribution. So it was not obvious how to split this term in terms of local entropies for each subsystem. But the solution was to recognize that this part of the expression is a channel-like entropy, meaning that. A Shannon-like entropy, meaning that it resembles a Shannon entropy. In a Shannon entropy, we have a log of probability times a probability. Here, we have a low concentration times the concentration. And just by exploiting this similarity, one can split this Shanghai entropy into a Shanghai entropy for each subsystem, the multi-information we just introduced in terms of concentrations, and then we have an additional contribution that emerges because we are not dealing with raw bits. Because we are not dealing with raw beads, but with concentrations, and so we have to account for the total concentration of these bipartite species. But so, putting together all these ingredients, the bipartite structure of the network, which is the only assumption we make, the expression of the nuclear information in terms of concentrations, and then the splitting of this shannon-like entropy for each subsystem, one obtains this loga-second laws for each subsystem plus a third-second law. System, plus a third second law accounting for the dissipation of all those processes that might not involve the bipolar species, if there are any. These expressions look more complicated than those that I showed you before and that were derived in the framework of stochastic thermodynamics, but each term here has a clear physical meaning that I'm going to explain. So, the first three terms are the same as in the framework of stochastic thermodynamics. We have the time derivative of the log. The time derivative of the local entropy, the entropy flow with the reservoirs the subsystem is coupled to, and then the information flow between the two subsystems. The next term here accounts for what I was saying before, that molecules have an internal entropy. And so we have to account that when we have chemical reactions, they interconvert molecules and different molecules might have different entropies. This term was not present in the derivation. Was not present in the derivation done at the level of stochastic thermodynamics. But this is just because people didn't consider that stochastic states can have an internal entropy. If they assumed an internal entropy also for the stochastic states, they would obtain something very similar to this term here, also at the level of stochastic analysis. The last term in this local second law emerges because chemical reactions are in general not minimolecular reactions, and the bipartite species might react with the Bipartite species might react with the other species, the ancillary species that I was mentioning before, like the free ring in the case of the light-driven motor. And so this term accounts for the entropy contribution of these ancillary species. And then the last, the third-second law that we have is just because in the network of reactions, there might be reactions that don't change the Weberhead species and just involve these ancillary species. So, I would like to conclude just showing you the kind of understanding we can gain by applying this theory, these expressions, to the light-driven motor I was talking about until now. Here, I will discuss only about one condition in which this motor can operate. In the paper, we actually study another condition, and we also study a driven self-assembly process. So, oh, sorry, as regards the Oh, sorry. As regards this light-driven motor, as I was saying before, experimentalists already think about it as two subsystems that are exchanging free energy via energy and information ratchet mechanism. The energy ratchet, according to the experimentalists, emerges because the binding of the macrocycle is always energetically favored, but it's more energetically favored when the axle is not bent. Axole is not bad. The information mechanism emerges because there's a kinetic asymmetry between the photochemical reactions involving the free axle and the photochemical reactions involving the axo binding the ring. And these two mechanisms together basically establish a steady state current which flows anti clockwise in this network of reactions, creating the directional motion of the ring from the left to the side of the microcycle. From the left to the side of the microsign. And when we specialized our equations, our logose second dose for this setup, we can see that for the subsystem representing the conformational changes of the axle, there are three contributions balancing the dissipation. One is the work performed by the light source. The other two contributions are: one is an information flow, the same information flow we had in the general equation. Flow we had in the general equation. The other can be recognized as an energy flow, and these two terms are the only one appearing in the local second law for the other subsystem, the one representing directional motion. So they are the only source of free energy that can maintain this second subsystem out of equilibrium. And the interesting thing is that the subsystem accounting for the conformational changes of the axle in a way it operates like harvest standard, harvest standard. It harvests energy from the light source and it transfers energy to the other subsystem. So it operates like an energy transduction mechanism, and one can introduce also a notion of efficiency at this level of the resolution, so between the two subsystems, not at the global level. And I would like to conclude just showing you how these quantities change for different values of determinant force established by the light source on the system. So this is the force maintained. System. So this is the force maintaining the system out of the blue. For large value of the force, the entropy production here in green for the subsystem representing the directional motion of the axle is balanced by a positive energy flow in orange and a positive information flow in blue. So both energy flow and information flow, according to our expressions, play the role of a free energy source for the subsystem representing the directional. For the subsystem representing the directional motion of the microcycle. And maybe here it's not clear, but there's a vertical line here where specifying the experimental conditions. So in the experimental condition, we verify that both the energy flow and information flow play the role of a free energy source. And the role of information here is the one I was saying before. Basically, the subsystem representing the conformation that changes. Representing the conformation that chases the axle creates correlations with the motion of the ring, which are exploited by the ring to stay out of equilibrium. However, for a small value of the force, the information flow becomes actually negative, meaning that it is the microcycle that creates correlation with the other subsystem. And so it's the other subsystem that exploits this correlation. So the notion of energy. So, the notion of energy and information ratcheting that the experimentalists used to explain this kind of mechanism has not a one-to-map correspondence with this information and energy flow that we can define on a thermodynamic T. So, to conclude, what we have done was basically to reformulate information and thermodynamics for bipartite chemical reaction processes when their dynamics is described with deterministic rate equations. Deterministic rate equations. In this case, the loga-second laws are more, I mean, there are more terms of getting the loga-second laws for these deterministic chemical processes compared to the case of stochastic thermodynamics. And we use this theoretical approach to compare the notion of information energy flow we can define in a thermodynamic theory with the notion of information energy velocity. That's it. Thank you for listening. Thank you for the organization. Thank you for listening. Thank you for the organizing, for letting me speak here. And these are the references I used to prepare the presentation, and this last one is the one that we'll discuss here. Thank you again. We have time for some questions. I have maybe a technical question. In the middle of the talk, you were relating concentrations and probabilities. Yes. So, concentrations that are somewhat. So concentrations and are thermodynamics through the the chemical potential. Yes. So is there a connection between the chemical potential and the entropies that you like? Sure, sure. I mean okay there's no relation between so these probabilities were just introduced like to obtain an expression of the neutral information. To obtain an expression of the neutral information in terms of concentrations. But there's a relation between the entropy of the bivariate species and the chemical potential, of course. If you want, one can express the entropy as the derivative of the Gibbs free energy in terms of chemical potentials compared to the temperature. And that's, I mean, in this non-equilibrium theory, all the relation between the thermodynamic quantities that one has in equilibrium thermodynamics. As in a group of AMS, it can be a covert host in this framework. So it's the same red issue. I also have a question. So you framed everything here in terms of this assumption of bipartite systems. But in principle, my guess is that everything that you've shown is also generalized. Systems are non-bipartite. They can be more than bipartite, meaning that you can have more than once than two states specifying the molecules. Specifying the molecules, the specific species. But if you add like transitions changing both states, like reaction, binding the ring and changing the conformation all together, you break down everything. Break down everything. Because you cannot identify univocally the dissipation and the entropy flow of each subsystem. We still have a global psychology. Yeah, yeah, yeah. Global structure. Yeah, yeah, yeah, no, no, no. Everything at the global level holds. The problem is that if you don't have the bipolar structure, you cannot define the two subsystems. There's no way to distinguish one and the other. That's only the reason why you need a bipolar structure. So you lose basically the local segment. Yes, actually. So I guess going to be a follow-up on that. Is your answer equivalent the same as the WordPress system? It's a hyperview or a partition product with a background. Or a partition product with back to graphs. Yeah, I mean, this is something we expect, but we didn't check it. It's more an expectation. But as long as you, okay, let's say that you have n states, but transitions, the reactions can change one state at a time at most, I would expect this to work. Sure. Any other questions? Thank you, Speaker again. Thank you, Speaker, again. See everyone tomorrow for our dinner. Do you want to carry that or do you want to talk about this? That's okay. I think you've got a great system. I just think it's tough when you've got to deal with individual variants. Yeah, clinically. We're just going to just deal with our terminology or what we call leaky leaky. These are classified, so you're not fine. I just think of the batch again, and the category of the store of your wire is actually our channel. So your wireless taper. Well, I can be a meetup with me. So we all have to do it. I just think that we already made that new character. Well, what is the meaning of it? There's no analogy. You just need to simulate the creature changes. But it's going to be two different creatures and you see the same function with the function. Or you start with one might say the rest of the. Because they are exposed to the same information. Because there's something that is different about their internal state, they are filtering the information out there. It's almost like a metaphor for police. If you support Trump, if people support Trump, I'm really worried about, I don't know, guns and crime. Guns and crime. You don't have to support why they're really worried about education, access to medical care. They listen to the same and they have different newscasts. They could listen to the same newscast and they would come away hearing different things. It's the same information you'll hear, but they are filtering information based on something by so in that sense it is subjective thing that makes the difference is something. Number of more subject motivation that master treaties. This is the poorest, simplest process towards morning is that the syntactic structure, the structure capture equation. I mean, the analogy of simple gas initially what that's been done has some angling is for some data more complicated. So consider very good from the beginning of the DC to VC course. And we think that the approach, in this case, is just history for knowing this core test. Just a little bit of a teacher. But people talking to each other like hosting Twitch. It's hard to say anything about. This is why I like Gindos 4. Gindos 4 is trying to be very parsimonious about the information. It gives up lots of information channels. Because it's only asking the wrong thing. It's like how it has to choose what is the most important thing. Whereas in the vegetative scheme, Whereas in the educators thing, it can afford to listen to many different channels at once. Because it has many opportunities for rebuild products. The way you describe it as depending on the cyclical state of the organism is very similar to the first two talks. So the corporate shows that there's an internal state, but the internal state is fixed. It's not the name of the varying with the whole history of all the operations. So yes, what was the first talk? Oh, Andrew, so Andrea would be weird. He had their guide versus the non-regardic. They're actually quite different features. I'm hoping we'll summarize that. That was precise. We don't have to take notes that I can't. I think I got something. Cool. So you definitely had something to say after our top of complication. In this case, I'm going to exactly a process. So yeah, it it yeah. That was going to play. I mean, we try to just play it. And it's going to be synergies through it. So that's important to know. That's a very long time. They are kind of digital. So we're not allowed to, you know, it's our results. And which I've I wouldn't mind. So if you add cost into your model, in principle you would actually even have a stronger standard. Like a costless center. So does it cost to me? Does it cost a center? Does it cost a cents? Well, cost of cents, right? If you put a cost of cents, then in some sense, you need to be disappointed. Well, now we have a spread. So we fixed the total concept. And we allow the cell to the city. We reallocate receptors between the different types without cells. On real cells, there would be a cost of converting receptors from one type to another. Because you have to convigate it, break it down, resynthesize it, and go through it. So if you add a realistic cost to it, using the flexibility, this whole result is not sure if it's worked. That's why you wouldn't strengthen the result that would actually. I don't know. Because the receptive, the. Because the receptor, the creature that always has exactly 200 spaces, exactly 200 receptors and 2004 receptors, doesn't have to spend any energy moving it back and forth. But what about the first one of these shows? Wouldn't the cost catch just definitely not? So the sensing cost there: all the different creatures we compared had the same sense. Had the same sense of the cost, well, which was zero. But so you would just take that expected curve of the three crossing lines, that whole axis could go up or down. So you could impose a sensing cost, which would be the same as a metabolic cost, and you just, well, sorry. You could also have a cell that moves randomly. The referees made us do this calculation also. You could have a cell that... What is that? You could have a cell that what was that? Um there's a cell that we have a chance of getting through and all the other ones are better than there's also uh someone said that we should we looked at the strategy where instead of maximizing the information minimizing the entropy, maximizing the chance of landing on a target or playing the three most of strategies. Playing between most of strategies, we try to minimize the expected squared distance between the creature and the source on the next time step. We ended up being able to calculate that. The referee, it was great. This referee was like, well, obviously the solution is to minimize the distance when you guys didn't even consider that. So then we proved that that strategy always dies and is always worse than all three of the strategies that we did consider. So you can introduce a cost per movement, which would be realistic. So the cells, they can teleport anywhere in that simple model. You could either make them only move wider labs, or you could penalize them for how far they move. That would totally change. I mean, it might strengthen the result, but the analysis we do where we reduce it to a financing market process with like. To a financing market process with like three states or two states, all exact stations are made. I don't know whether that works. I don't know. And you can even take it in the whole thing. Well, it might still work because the strategies will be the same, basically the same, and then you can calculate the expected work. You you mentioned the line counting. Um so Robin Snyder wants to work. So, Robin Snyder once worked through for me all these kind of really interesting ecological mathematics of actually predator-prey, realistic, quote-unquote, realistic predator-prey type things, where you also have the down period, which is like, you know, the lion actually needs to rest after he's eaten zebra. And that actually, in terms of like the amount of time the lion spends hunting and searching for food, you have to back out of it. And that creates kind of the calcimenton type of relational choice. Yeah, log alternative. Yeah, um, Locke Voltaire is a lousy model. I mean, it's a lousy model of anything. I don't know any empirical model of anything like that. But it's a quadratic impression, so it's the simplest. I'm sending you the link they just sent me to the video, or maybe you already got it. No, yeah, yeah, and I mentioned their name on the video page, not the arrow. 