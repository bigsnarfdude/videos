Thank you, Tatiana. Thank you for inviting me to this workshop. And I thank all the organizers for giving me this opportunity to share my work. Well, okay. So today my talk is combining deep learning with EIT, and this is done for mainly for the purpose of classifying stroke. To begin with, I don't think this audience needs an introduction into EIT. However, Into EIT. However, just to make this talk complete, EIT is the inverse Calderon problem where we want to recover the interior conductivity from the boundary electrical measurements. So say, for example, if we apply boundary voltage F on the boundary of the domain that we are interested in imaging and measure the current density along the boundary, then from the map that connects the boundary voltage to the current density, Boundary voltage to the current density, we wish to recover the interior conductivity sigma, which is governed by the inverse Laplace equation. There are many applications for the EIT, such as monitoring cardiac activity, lung function, pulmonary perfusion, and so on and so forth. What we were interested in this particular project is applying EIT for the classification of stroke. The classification of stroke. There are two different kinds of stroke. One that is ischemic stroke, where a part of the brain is cut off from the blood supply. And because blood is conductive, and since the part is cut off from the blood supply, it results in low conductivity region. And the other is the hemorrhagic stroke, where a blood vessel ruptures and blood is accumulated in certain. Accumulated in certain parts of the brain, resulting in high conductivity region. So, since there is a contrast in the conductivity, this problem is in some sense ideal for EIT. However, there are difficulties of imaging brain using EIT. And one of the most prominent one is that the skull is highly conductive, the skull which happens to be on the boundary. Which happens to be on the boundary is highly, sorry, highly resistive, which makes the penetration of the current into the interior part of the brain very difficult. And because there is no penetration of current into the interior part, this makes the EIT directory constructions, such as D-bar or any other method, quite very difficult to image. Then the idea. Then the idea is that we do not want to do a full reconstruction. Rather, we would like to do the classification just by without having to do the full reconstruction. How can we do that? We made use of this inverse scattering methods that transform the EIT into so-called quote-unquote linear X-ray tomography problem. Linear x-ray tomography problem. This is done by using virtual hybrid edge detection functions, which Professor Greenleaf explained on Monday in his lecture. Without going into the details that are explained in this seminal paper by Alan Greenleaf, Mathila Sas, Mateo, Samuli, and Gunther Ulman. The propagation and reflection. The propagation and reflection of the singularities along the leaves were used for detecting inclusions. How can we make use of this idea in order to detect the inclusions? This can be done using Beltrami-type CGO solutions. Again, Professor Greenleaf explained all of this quite very well. Explained all of this quite very well. So I'm not going to go into the detail, just to give a recap. Suppose if we know the interior conductivity, interior conductivity in the region of interest, then the conductivity can be changed into another variable mu through this change of variable. And then the CGO solutions are the solutions of the Beltrami equation, which are which by Astala Paivarinta. By Asthala Pai Varinta type are of the form e to the ikz plus one times of reminder term, where the remainder term omega plus and omega minus goes to zero as z goes to infinity. These CGO solutions and the conductivity equations are related by this if and only if condition. And actually, in the DBAR method, this is made use of in order to compute the reconstruction. The reconstruction. However, in this case, we will compute the reminder term. We will compute the reminder term, and this can be done using many methods. But one of the ones that is widely used is the Hutan and Peramaki formulation, where the reminder term that is defined in the CGO solution satisfies this particular. Satisfies this particular equation where μ and alpha are given by these relations, which relates alpha and μ to the change of variable mu, and k and z are the complex parameters. Then, if we define a function u bar as minus d bar of this remainder term omega. Reminder term omega, where the function u is defined in LP space, then omega is given by the Cauchy transform, and the derivative of omega is given by the Buerling transform, and all of these derivatives are taken with respect to Z. The Cauchy transform, which is the inverse of the D bar operator, and the Buerling transform, which is the derivative of the inverse of the D bar, are given by the standard. Are given by the standard integrals. Then substituting the function u into the equation and simplifying leads to a real linear equation of this sort, where A and rho are given by, are related to Cauchy and Bürling transforms, and rho is the complex conjugation operator defined by this particular. This particular defined in this particular way. So, summarizing all of this, suppose if we solve for V from the equation, which is supported in omega, then one can calculate U from the knowledge of A and rho, and then omega can be computed. The remainder term omega can be computed by taking the Gaucher transform of u bar. What all of this means. What all of this means. Once this is done, then consider the reminder term. Then let's write the complex parameter as tau e to the i phi. Then the reminder term omega of z, k can be expressed as z tau and e to the i phi. Then one can take the Fourier transform of Fourier transform in the variable tau resulting. tau resulting in a function called as omega hat plus. This omega hat plus on the boundary can be integrated to give a averaging operator T plus minus. What all of this means is that suppose if we know the interior conductivity distribution, then the profile of omega hat plus computed at z equal to one, which is Equal to one, which is which is on the boundary of the real axis with the phase e to the i phi given by one results in a profile such as this. And one can see that wherever there is a conductivity jump, the profile also has changes along that. The only slight issue over here is that omega. Here is that omega hat plus results in this anomaly, which can be removed by taking the difference of omega hat plus and omega hat minus. And one can see that this profile very much gives the change in the conductivity, the jumps in the conductivity. And here is the t plus minus operator defined for e to the e phi. While this is all While this is all great and nice, and it actually gives the idea that the stroke can be classified through these profiles, since these profiles embed the knowledge of the change in the conductivity profile. This is not practical because one does not know the interior conductivity distribution in practice. So, how do we compute these VHAD profiles in practice? Files in practice. What happens in practice is that we do not apply voltage on the boundary since that is not practical. We apply a current pattern. And in this case, we have applied a trigonometric current pattern on the boundary. Then, with this trigonometric current pattern, we solve the Neumann problem using the finite element methods to recover the Methods to recover the boundary voltages. Then, what we get in the discrete term is an ND matrix, which is the inner product of the boundary voltages with the current pattern applied on the boundary. From this ND matrix, one can invert this ND matrix to obtain the DN matrix. So, what we have in practice is the DN matrix, and the CGO solutions are to be computed from this DN matrix. DN matrix. How does this computed? It makes use of the relation that if there is a complex function that satisfies the Beltrami equation with the condition that the imaginary part of this complex function averages to zero on the boundary, then the imaginary part and the real part are related through this Mu-Hilbert transform. This mu Hilbert transform. Then, for a real-valued function g, the tangential derivative of that mu-Hilbert transform is related to the DN map. This is what we make use of in order to compute the CGO solutions. Now, in the discrete version, the H mu acting on real-valued zero mean functions, which are expanded on the functions which are expanded on the trigonometric basis is given is discretized by the discrete by this equation where L sigma is the discretization of this DN map and DT inverse is the discretization of the inverse of this tangential derivative in the boundary. CGO traces do not have a zero mean so we zero mean. So we append this basis function phi zero to the mu Hilbert transform. So the hμ in the discrete version is given by this matrix and h minus mu is given by this matrix. Now the operator h mu is not complex linear. So we define a real linear operator using that h mu and an averaging operator L through this particular this particular relation with for a complex function g then this real linear making use of this real linear operator one can solve the boundary integral equation to get the cgo solutions so this is how cgo solutions are computed in practice from the from the dn matrix from the nd matrix which From the ND matrix, which in turn leads to DN matrix. And this is how we get the CGO solutions. Once the CGO solutions are obtained, the same trick that was used earlier that Alan explained in his talk on Monday, we make use of the polar representation for the variable k as tau e to the i phi. Then omega can omega of z comma k can be expressed in terms of. k can be expressed in terms of z, in terms of the three variables z, tau, and e to the i phi. And one can take the one-dimensional Fourier transform in terms of the variable tau, resulting in the pseudo-time variable t, which results in what we call what we refer as VHLD profiles. VHLD profiles are essentially omega hat plus minus omega hat minus that is computed. hat minus that is computed in a in this particular way now now that this vhed profile encodes the geometric information about the change in the conductivity one wants to make use of these profiles to classify the different kind of two different kinds of strokes how is this How is this done? This is done using neural networks. The question arises: why neural networks? In practice, let us say that we compute these VHAD profiles over a, one cannot compute this from minus infinity to infinity. Let's say we make use of a window in order to compute these profiles. Now, let us say that these in these Say that these in these two examples, blue represents the ischemic stroke, and red profiles represent the hemorrhagic stroke. And one can see that this is not easy to classify by just eyeballing them. Even though we do know that these represents the change in the jump in the conductivity profile, it's not easy to classify them. Thus, we make use of the Thus, we make use of the neural networks. In order to train the neural networks and to use the neural network, we simulate 10,000 examples of unit disc representing the cross-section of the brain. The skull is represented by an elliptic shape and it has a conductivity in the range of 0.1 to 0.2. Then there is 0.2. Then there is one circular inclusion to train. The radii of this inclusion varies between 0.1 and 0.5. The ischemic stroke has conductivity in the range of 0.7 to 0.8, and which is low, and hemorrhagic stroke has the conductivity in the range of 3 to 4, and the background conductivity is 1. One, the ischemic stroke is labeled as zero, and hemorrhagic stroke is labeled as one in order to do the classification. The neural networks were trained with two pairs. One is the DN matrix pair, that is, the input was the DN matrix, and the output was the labels zero for ischemic and one for the hemorrhagic. Hemorrhagic and the other training was done with the VHAD training pair, where the input was the VHAD profiles computed along the boundary and the output was the two different labels of the stroke. We made use of fully connected and convolutional neural networks to train this. Fully connected neural networks are shallow networks with only three With only three layers, one for the input layer, one for the hidden layer, and one for the output layer. And the convolutional for the CNNs or convolutional neural networks, two such networks were trained. This one for the DN matrix training pair and a little bit elaborated one for the VHAD training pair. The way the input can be viewed. The input can be visualized for the CNN is that the VHED functions were computed along the boundary for along 64 points. Then the real part of the VHAD function computed along the 64 points was concatenated with the imaginary part of the VHAD functions, VHAD functions, which results in Which results in an image of this kind. So, the input layer makes use of this image-ish kind of VHLD profiles, and the output is the labels 0 and 1. Once these networks were trained, they were tested on slightly different data sets, that is, they were tested on. They were tested on elliptic inclusions and they were tested with different kinds of data. That means to say that for the DN matrix, noise was added to the DN matrix training pair. And corresponding to these noisy DN matrix, the VHAD profiles were computed. And one can see that the fully connected or the shallow network accuracy for the VHAD. Accuracy for the VHAD training pair is better than that of the DN matrix training pair. For the CNN, again, one can see that the VHAD training pair is better than that of the DN matrix training pair. And one can also observe that as the noise level increases with the DN matrix, the accuracy. metrics, the accuracy of the classification actually is lesser. However, with the fully connected neural network, it's not much different from that of the no noise added to the computation of the DN matrix pair and the corresponding VHAD functions. However, we also noticed that the CNNs, which are actually deep neural networks, did not perform as well as that of the shallow neural networks with these kind of inclusions. Then, this was also tested on the irregular inclusions. Again, one can see that VHED training pair outperforms that of the DN matrix. That of the DN matrix training pair in the shallow network. And same in the case of the CNN as well. However, here too, we notice that the deep network does not do that good of a job as that of the shallow network over here. It is quite very prominent, particularly in case of DN matrix training pair, where DN metrics training pair where the shallow deep network really did not do as well as that of the shallow network. Then it was also tested on multiple inclusions. And with the multiple inclusions, one immediately sees that the DN matrix pair does not do its job at all. However, with the VHED training pair, it actually does the classic. It actually does the classification better. Here are the results for the CNN. And here comes the problem that the deep network actually did not perform well at all in case where the testing data is far different from that of the training data pair. This was a little bit concerning because. Was a little bit concerning because this is simulated data and the DN matrix was computed along using the point electrode model and so on and so forth. And then the difficulties actually increase if we include the complete electrode model and so on and so forth. So, why does a deep neural network not perform as well as that of the shallow neural network was a concerning problem. Was a concerning problem. So, thinking more about it, more about it led to experiments with the recurrent neural networks with long-short-term memory. So, why recurrent neural networks is that instead of thinking of the VHD profile as an image input, one can think of VHD profile. Think of VHVD profile computed along the 64 points on the boundary as a sequence instead of thinking of it as an image. If one thinks of this as a sequence, then this is a perfect example for the recurrent neural network training. So we made use of the recurrent network, neural network with long-short-term memory. The architecture is given by The architecture is given by this, where there is an unrolling of the neural network in this long-short-term memory part of the neural network. Now, making use of this recurrent neural network, one sees that the RNN accuracy for the VHID training pair starts getting better than that of the CNN accuracy. And this is for the elliptic inclusions. Inclusions. Here are the results for the irregular inclusions, and one can see that RNN accuracy is almost one as opposed to that of the CNN accuracy. And particularly with the multiple inclusions where the testing data set differed much from that of the training data set, the CNN accuracy, which CNN accuracy, which was concerning, actually changed and it became almost one over here. So this is how the neural networks were used for the classification of the stroke and classification of the two different kinds of stroke. And here is how the ideas. The ideas that were discussed, particularly in this paper. Sorry, particularly in this paper was brought into, was made use of in order to classify the two different kinds of stroke. So coming to the conclusions, we learned that the learned classifications are much better with the VHED functions as opposed to the boundary electrical measurements, that is the DN matrix. DN matrix. Now the ongoing work is to make this DN matrix from the actual measurements, that is the complete electrode model, which is much closer to the practical measurements. And then to test this data on and then to test this on the simulated complete electrode. Complete electrode model and the complete electrode model, and also to test this networks on the 3D models that were trained on the 2D data. That is the future work that is ongoing. Thank you for your attention. Thank you, Rashmi, for this very interesting. Rashmi for this very interesting talk. Do we have questions from the audience? Giovanni. Hi, can you hear me? Yes. Okay. Hi, it's Giovanni Alberti here. So my question is on the CNNs. And if I got it right, somehow the inputs of the CNNs are not really images. Could it write? Correct me if I'm wrong. I mean, these are like. Correct me if I'm wrong. I mean, these are like, you know, inputs of the, you know, the, those, how do you call them? Whatever. So my question is whether the fact that those inputs are not actual images may be the reason why the convolutional structures is not well adapted to those inputs. Yeah. That's actually a good question, and indeed, it could be. I also suspect that, well, I don't have theoretical results for this, as with many of the neural network things, but I suspect that the VHAD profiles, while computing the VHAD profiles, there is already some kind of a convolution that comes with the That comes with the computation of the one-dimensional Fourier transform. So, adding more convolutions, I think, makes it blurry. So, there is no feature that can be extracted. So, maybe that could also be the reason. But I believe that these not being the actual images might be the reason that. They didn't do well. Okay. More questions? I just had a small curiosity, a very silly one. When you do the training, do you have also noisy data or do you only test on noisy data? Sorry, you mean? Yeah, when you have the input pairs here, so the data you give are. Data you give are noisy, some of these profiles like corrupted in any way, or you the noise amplitude that you see here is just for the testing. Noise amplitude for testing. Yes, I'm not sure I understand that part. Okay, so okay, let me let me just say, and then maybe I'll come back to your question again. I'll come back to your question again because I didn't quite get it. So, we compute the voltages along the boundary from the Neumann problem over here. And sorry, here. So, we compute the boundary voltages by solving this Neumann problem from the finite element method. And because it is a finite element method, there is Finite element method, there is a certain amount of approximation. However, that's the closest that we can get to without noise data. So this 10 to the minus pi represents that no noise added to the DN matrix that is computed from the inner product of the voltages and the trigonometric current pattern. Then to the DN matrix noisy. The DN matrix noise is added. So, this is like a very, how do you say, very artificial setting of noise added to that. Because I go on to the current pairs and the voltage pair, which actually should happen, it's adding on to the DN matrix pair. If that makes sense. No, no, no, no. You answered my question in like this. Okay. Thank you. I guess we have no more questions for the audience. No, maybe we do have one. Okay. Hey, thanks for the talk. This is Jeff Fessler. So you have a, you're not trying to do this in real time, right? Are you is this real-time reconstruction online? Time reconstruction online? No, there is no reconstruction happening at all. Well, okay, you're not trying to find the right process in real time because you mentioned that you could take these waveforms and treat them as a sequence and use a recurrent network. And to me, that makes sense if you have data arriving in time that you want to process in time. But if you have all of the data already in your computer, what's the point of processing it? What's the point of processing it left to right in a sequence versus right to left? You have it all. So, I'm could you just say a little bit more about the motivation for using a recurrent neural network for this data? Okay, I see your point. Well, while thinking about it, because the CNNs didn't perform well, and then I was just trying to graph these VHAD functions, then in some sense, even though this is not coming in real time, one can think of that tau variable which gets transformed to T in the Fourier transform, which is not actually. Fourier transform, which is not actually time. If one, I mean, visualizing that as some kind of time, time series, then each of these VHAD functions could be thought of as sequences, which is why I thought of using the recurrent neural network. If that makes sense. That makes sense. Okay. Thank you. Okay. I guess we can thank Rashmi again for the talk. Thank you. But this will be incomplete without mentioning my collaborators. I would like to thank Juan Pablo, Ainur, Alan Greenleaf, Matila Sas, Mateo, and Samuli, who were my collaborators for this particular project. Collaborators for this particular project. This talk would be incomplete without thanking them for all their help. Great. Thank you again. And let's thank the speakers of this morning.