  I seem to set up my Wikipedia. I think if you share screen, just play your flag. That recording is so the Zoom can be. Oh, yes, you need to do mirror. But I did. I wonder if one of these cameras. This is not over. Okay, yeah, I see different from the mirror. Oh, yeah, it's here. Mirror just. Okay, no, no, we see. I don't see all those. That's what you see on the screen. No, no, no, we less wait because we should see the same thing here on that side. Otherwise the unlocked people don't see the slides. Let's wait for this to set up. Maybe because the I think you can see in like in the if that says on this window, it's just whatever it is that I show. Oh, okay. Yeah, we do see it. I mean, there's something we discuss if you want. Are we pretty much good to go? There's just something for Zoom. It'll just be a minute or that. Sorry, I need to do it down there. Are we waiting for something down here? Waiting for he needs to do something downstairs for me, but I'm not sure it is for you, what is the local people? Yes, that's human. Oh, yeah. Can anyone on Zoom see the multi-pie right now, or do you just see live screen? My screen. I'm also trying to see the screen. Yeah, so I think the problem is that one camera that you turn on. Oh. Okay. The camera that looks at the audience and the camera's on somehow. Very strange to see that. Accident is being recorded, so that's it. Accident's being recorded, so we have the request. Maybe I can see it first. Thanks to the organizers. Did you tell CJ? No, he's missing. Yeah, I guess he forgot. We're not still in the perfect. Alright, so let's start. Next speaker is Huang Kai-jiao from Duke, and he's gonna tell us how much value we can squeeze from single PDs of the psychology. Okay, first thank the organizer for the invitation. Yeah, I think it's a little bit not used to yet, but for this kind of in-person meeting, but I For this kind of in-person meeting, but I really feel it's much better than virtual. Okay, so thank the organizers, and this is the joint work with my former host, Yi Ming Zhong, who's supposed to be here, but due to visa issue, he cannot join the meeting. Okay, so let me emphasize in this talk, I am trying to more emphasize what we can learn. Emphasize what can we learn for a single solution trajectory. So see heat equation, wave equation, you have time, you observe one solution from some initial data, and you see, suppose you can record all the snapshots of the solution. So the question is: what is the data space? How much can you learn from this data space? So, the emphasis of a single solution trajectory is: if you think in practice, we do weather forecast or climate change, the only thing you can have is a past. So one solution trajectory up to now. You cannot repeat the experiment. Like people want to do PD learning, they simulate many different solution trajectory according to different initial data. But that's usually not, you don't have the lack. That's usually not, you don't have the lecture in practice. The question is: what can you do? What can you learn? How much data space you have? So that's basically why. And we'll see, depends on the dynamics, depends on solution type. I don't know why this. Oh, this one. Ah, okay. It jumps on it. Okay. So it's very important when you do data-driven approach, learning approach. Important thing is how large the data space is, because you can only learn your operator in that data space. You cannot learn something from nothing. So that's one thing you have to know. The other thing is what would you recover? How rich, if you want to learn something, whether this phenomenon is dynamics, it's very rich. And that means you need more data. That means you need more data. If it's not very rich, maybe you can do comfort query with limited data, yeah, less data. So we use a simple example, which is the kind of parabolic type, but this L doesn't have to be. You can think of now it's just Laplace, but you know, can any high order as long as this is elliptic and several joints that Self-adjoint, that technical reason, then you can easily see all the eigenvalues. The eigenvalues grow like this. The beta, maybe I should say what it is, beta. Oh, beta is, you can think it's if this is of order m in d dimension, this beta is m over d. Okay, so this is the very important. The growth of this really tells you half. Really tells you half how fast the high mold damps and how compressed your solution is. So we'll see a balance between this of this one, which is a balance between the dumping, the speed of dumping and the dimension of your problem. Okay, in earlier work, we have shown that for this PDE, This PDE for a single solution trajectory, you have infinite family of snapshots, right? Infinite many snapshots. So you think it's infinite family of functions you can use, but that family of functions parametrized by t only stays to a very low-dimensional space, a linear space of epsilon squared, log, epsilon squared. Squared, log epsilon squared, this dimension, if you want epsilon to be the accuracy. So basically, see all the trajectory, all the snapshot stay near a linear space of this dimension if you want this tolerance. So it's very small dimensions. Data is very small. And remember, this is from t equal to zero. Start from t equals zero. You can also show the data space of all solutions. The data space of all solution trajectory, okay, you collect all solutions, no matter what initial data you start from. But if up to some time delay, because high mode has been dumped, and it's not hard to show that this, how much time delay is here, okay? But the whole solution trajectory, if we say we're doing weather forecast, you want to do something further in the future. Further in the future, then the solution space is not large either. So I remember during Stan's first day conference, Beyond, we discussed whether they said something about it's so hard to predict weather, which is sometimes, yeah, in some sense, yes, because the data space is small. You learn from history, that is very small. You only learn the operator in this linear space, basically, of this dimension. Linear space basically of this dimension, that's what you can do. But now I realize also after that discussion, the whole space is not big either. If I don't want to know exactly what happened at t equal to 0, we only want to know sometime in the future. The space is not large either. But there is this factor, and this is also dm, which is the beta sum of the, yeah, beta k. Okay, so the question I'm going to talk about. So, the question I'm going to talk about today is: can the space spanned by a single solution trajectory as reach as the solution of all trajectory after some time? Or can we, more mathematically speaking, they say, can we use a superposition? So we know all the history, all the weather pattern we have seen before, can we use a superposition of that to predict some hurricanes in the future? Hurricanes in the future. Okay, so that's basically the question. And more mathematically, it's this formulation. Say this is the sample solution, okay, some solution for OT of 1. And then you have a new solution corresponding to some new different initial data, arbitrary initial data. Can this new solution at certain time tau, of course the tau is bigger than zero, okay, can you use this? Okay, can you use this, find a weight function, linearly superpose all the snapshots of your sample solution to recreate this? Okay, that is the question. Any questions? So that's basically it. Whether I using one solution trajectory, I can use superposition of its snapshots to reproduce or forecast an arbitrary solution in the future. In the future. So, is there an orphan bound for tau? Excuse me. Is there an orchard bound for tau? No upper bound, but we always see there's a lower bound, but tau cannot be close to zero. If it's very close to zero, you can do it, but it's very ill-posed. Okay. Yes. What is the rational? What is the intuition to have such a system? Excuse me? What is the intuition to have? Oh, intuition. Okay, if you want the data-driven approach, then I just have to observe. Approach, then I just have to observe one sample thing. Then I can use the space spent by this to do dimension reduction. Then I know my new solution is a linear combination of this. So another concrete example in the weather forecast, I only have to measure a few places and using the matching of those local sensors, I can predict other places. How do I use superposition or paths to predict that place in a different time? Place in a different type. So I was working from a kind of ergodic system or something like that? Oh, a God system, of course, you can do that, but this is not necessary to have to be a goddess system. It's just that can I, from past, can I know the future? Well, data-driven approach is basically this. You know everything before you, can predict something in the future. Of course, you can say I don't need to have to linear superposition. But then the question is, is the original problem? Because that's a map. Hard are the original problem because that's a map, right? That's a map basically the same as the original problem. So, linear superposition, I think, is most of the time we can computationally do efficiently and also relatively stable. Non-linear map, then the question is, you know, again, you have to say how twisted or how complicated that large map is almost as the same as saying how complicated the dynamics is. So, let me make it a more mathematically different way of doing this, but this is give us exactly what we are trying to study in order to address this question. So, this is the eigenvalue, eigenfunction. Your given sample solution is of this form. Of course, if you miss some mode, you're never going to be able to predict something happening in that direction. So, assume, but we will see actually this coefficient Cn can be. See, actually, this coefficient Cn can be extremely small, but you have to have it. An arbitrary solution is of this. This Fn is different, the tau, the different, but the mu n is the same. So equivalent, the thing, this question is equivalent to say, can I find a weight function? It's inner product with e to the minus mu and t, give you this required value, which is this, okay, determined by this. Which is this, okay, determined by this and this. That's equivalent to that. So it's something kind of moment problem. Can you find some function, it's this exponential moment because it's an exponential, but this is e to the minus t to the new one power. It's moment problem. Can you really find a weight function, a function that has this moment? So you're talking only the homogeneous problem without any sourcing term? Yeah, if you have a source, the solution is unique, right? Solution is unique, right? Any solution you can. So the training just is self-enclosed, so we can think of that. Yeah, otherwise it's definitely not doable. Okay, so there are already well known results from the PD control community saying that. Saying that if this thing grows fast enough, so that means the high mode damps fast enough. Okay, so you can signal that way. Because mu n is growing, okay? The smaller this is, is the fastest decay. But if this is finite, then you can find a by orthogonal series, basically. This VK only sees this mode. No, this This mode, this moment, not other things. Okay, so it's delta here. But this is like this orthogonalization process, QR process. It's pretty bad that these things grow. Well, you have an estimate of this. I haven't shown you how fast it grows. But this norm of this has to grow, has this bound. So now the question is if question is if this is converges then this is what you need okay this is the moment condition you want and this is like orthogonal basis you're done your combination superposition of this you're done the key thing is how fast these things grows okay definitely this is the condition you have this bound if this is not finite basically say this sequence of function This sequence of functions, if this is true, this sequence of functions spans a proper subspace of L2 function, and each of this is not in a space spanned by the other function. So you can create this by auto-onization, something like that. And then you estimate how fast it grows. We can show this mu n grows, if mu n grows like this. Okay, well that's for elliptic equation. Well, that's for elliptic equation with order m and dimension d is this. And it is bigger than 1, and there is some separation. Well, the separation could be constant, doesn't have to grow, but this is something when we have to estimate this, otherwise this is 0. If you have these two assumptions, you can show this growth rate of this norm is like e to the n because 1 over beta of mu n, mu n grows like that, 1 over beta. Grows like that, one of beta. So this thing grows like exponential to the nth power. So it grows very fast, but if your solution decay also fast enough in high mode, so there's competition, then, okay, so basically if this mu n, mn can provide some decay, compensate the growth of this, and then you have absolute convergence of this. And then you have absolute convergence of this, then you can have that. Okay. Basically, say you need this, that's why you need to for you can only forecast weather at a later time because you have further decay than what is now. So you have this. But basically, this says you can do it, but you know, when you try to satisfy those moment equations, it becomes very ill-conditioned, and in order to be able In order to be able to make this series absolute convergence, you need some fast convergence in this. Where does that fast convergence come from? It's come from different time. So basically, we can show, you know, the coefficient has to be non-zero, but you can see its high-frequency mode can be really small. It can go to e to the minus pn alpha. So this is, as long as you have any new solution. Have any new solution corresponding? Oh, so this is your sample solution with initial data like this, and this is the new solution, okay. Then at any later time, okay, tau has to be bigger than the t0. Then this tau minus t0, well, your high frequency mode, one is decayed like e to the minus mu n tau. This is e to the minus mu and t zero. And T0, that difference can compensate the growth of this bioorthogonal basis. And the one thing you should notice, there are some factors here. The most important factor is this. These are quite mild. What the L2 norm of this wave function grows like this, and the important signature factor, it's basically The signature factor is basically exponential large in terms of 1 over tau minus t k. So it's when you have the history from now to 12 and you want to predict the weather at 12.01, that's pretty bad. If you want to predict at 1 p.m., that's bad. Okay, you can do better. That's also natural. Intuitively. Okay, so a few remarks. First of all, this really means even you want to do this superposition, it's very ill-posed because this norm grows like when a QR, the diagonal component really grows fast. So, also, this is what I said. These things depend on the difference model, but this T1 minus T0 is modeled, but on the tau. But on the tau minus t0, okay, it's very significant. So you cannot predict something immediately after you have started recording. For the sample solution, condition needs to contain all item modes. That's also. Otherwise, you will miss that mode. You cannot predict. Okay, that was with no multiplicity. With multiplicity, that's fine. You just have a finite multiplicity that's A finite multiplicity, you just have to have a few sample solutions. Each sample solution starts with a different initial data, and that initial data in that general eigenspace, they kind of are independent. You can use superposition to get something in there. One trajectory is not enough, but finite distance is good. So, non-existent, so if the m is less than or equal to The M is less than or equal to 2, then D, so basically the order is less or equal than the dimension. So the diversity due to the dimension is more than the dumping speed of your operator. So for example, unfortunately, even heat equation 2D, it's marginal, which is like the borderline. So I think if you want to predict weather, the things you can do is if you predict California weather, or sunshine. Weather, old sunshine, little breeze. Yes, you can do it. And barely because it's 2D already, okay? But anything more than that, you have strong convection or hyperbolic things going on, then you have no way to do that. You can do it, but for boring solution. Well, no free lunch. Okay, so some numerical tests. This is the 1D heat equation plus. The 1D heat equation plus a convection. And this with periodic boundary conditions, so you sine cosine are all eigenfunctions. So its eigenvalue have multiplicity 2. And we have two different sample solutions. And you try to test the solution, you try to superpose an arbitrary solution. The arbitrary solution is some kind of different mode, and the cake could be. mode and the k could to be 8. So first of all this mode, these things, the eigenfunction, it is nothing to do with eigenfunction. It has something to do with eigenfunction, but definitely not the top 52 eigenmode. It's using the more but it's compressed in some way to do the S we do S V D to extract the really what's the first, the dominant dimension. The dominant dimension, the direction of this linear space, the optimal linear space, to approximate the whole solution to some tolerance. That's what it looks like. It's definitely not sine, cosine. And you see the error is machine-based, machine accuracy. But when you do 2D, you see the error. But, you know, if 10 days, 12 days weather forecast, this is probably good enough. Okay, but the data has noise, or there's noise. Okay. You are ahead of me. I will show you. Of course, it's difficult to deal with noise. Yeah. But just say the theory is literally right. The thing is, fourth order, then everything is fine into D. Yeah, it should be minus time. Yeah, it should minus, yes. Yeah, I should minus, yes, you're upset. I have a minus L. Sorry, I didn't put minus there. Yeah. This, well, this is give you, if it is true, then this is basically usually what you can do in data-driven approach. You just measure the past, and okay, using that. So basically, say if you can extract the leader space V. Extract a linear space v from one solution trajectory, and that v is good enough to approximate any other solution. Okay, so that's basically you can see dimension reduction or other things. And we already know if it's of this, we know the dimension is also very low. Okay, this is what earlier work showed. And then in practice, okay, you don't have to have full solution, okay, because the linear superposition is at each. The linear superposition is at each location. So, where you want to, you only have to do this S V D on the measurement in those local measurements. That's it. So, times a snapshot at different locations, you form this matrix, you do it. And then you only need a few measurements comparable to the dimension to find the parameters, superposition, coefficient, and then you can pretend all other places have the same coefficient. Okay, so this is good example. This is with noise. It's pretty bad. You know, there's no magic because your system is so ill-conditioned. Here we have to record a window and the number of the things in the window is also goes to larger and larger if you want smaller and smaller things. And then still, when you When you do the inversion, which has a bad condition number, you cannot do very well. So, this is basically even very, very small delta t. So, then allow me to have many different snapshots to the average. But you see the error basically reduced to minus 6. Otherwise, it's 10 to minus 2. I think, you know, if you're machine learning or anything like that, 10 to minus 2 usually is some kind of easy. Is some Chinese. I don't know whether there's coincidence or not. But basically, as you ask, and the answer is no. It's not very stable with it. Okay, so that's it. Any questions or comments? Yeah, I mean I haven't learned that you want the classmates about model. So uh it So uh i it for me it makes perfect sense if you want to predict the room temperature. At least you push the bottom right field enough. If you want to really predict the weather outside, then we have to do that never set non-linear yeah let's say not even not go to non-linear. So for example there's a strong convection, right? The strong wind bound, right? But you know it's very interesting. If you you like calm weather, but the calm weather usually the sample solution The sample solution also has smaller data space. But this, if it's a heat equation like that, the difference is a multiple constant. It's not much more. But if you have observation of hurricanes or whatever, tornadoes, that provides richer data space. But if you, that means the future tornado, future hurricane could be even more richer than the history can predict. Yeah, it's a delivery. Well, it gives you more if you want to say. More, if you want to say, create templates, you definitely don't want to record calm weather, you should record all these different harsh weather, and that gives you a more library, more template. But on the other hand, that usually from this is not enough to predict another hurricane, another topology. But it helps. It's better to see a hurricane before you can predict hurricane. Yeah, so my second part helps. My second part is that. My second question is that in my opinion, have you done anything about a hyperbolic wheel pushing? We don't like wheel coaching, the solution is very rich. We don't even have diagonal experience. It's just using a dimension argument because there's no compression, right? So if you look at all your snapshots, is basically although all the snapshots are in 2D, 3D, whatever D dimension, but the 3D, whatever D dimension, but the trajectory is 1D. So you can just like look at a big matrix, your rows and columns could be very different, but you look at the smaller dimension, the time dimension. So which means you can at most approximate all kinds of solutions with certain regularity in 1D, basically. But when you have 2 or 3D, then you're talking about your solution, there's no smoothing, so any future time step will still stay in the same. Time step, you still stay in the same smoothness class, but in two-dimensional, three-dimensional, there's not enough movement. My third one is back to the heat equation. So, have you thought about like a stochastic and heated quote as HE and you know join the sources to my like a white noise or like HES? So, I can see that. That's an interesting question. So, basically, you pop into the mode and the kind of your Or you can think of the Hammond principle, you can always start a new different initial data, and you can see. I think so. I definitely think so, but then the question is, yeah, if in practice, if you can do that, that probably give you, if you collect the data, that's definitely the data space would be larger than you just start from initial. Yeah, I think so, yeah. Yeah, yeah, I think so. Yeah, I think so. Any other questions? Question for the elliptical operator. What if it has some continuous spectra? Oh, then that's a very different volume I consider. So this is based on the discrete. Well, it's probably the same if you as long as you have, I think that if you are willing to wait. If you are willing to wait long enough, as long as your smallest, but that's it's very interesting, but that definitely is not from the moment equation, you can solve it. Moment equation. So the question is, my question would be infinite dimensional. So is there a finite dimensional space you can approximate? Well, that I don't know. Um that I don't know if you have a spectrum, a continuous spectrum right here I have um uh basically do the dump the the later one is not the only first few one and you can see there's a you cannot do the decomposition happening using this In terms of the stability should be the same. Of the stability should be the same. Stability is the same, but computation also be very different. So, how do you extract the five-dimensional space out of this? Yeah, and you measured in discrete time, but because the mode can oscillate as fast as we want. So, yeah, so all these things is uh based off this discrete spectrum. The initial condition, I mean, might not have that much more to start with. Have that much more to start with. That is true. So, but on the other hand, we're trying to predict any new solution for any initial data. Even though the initial context may have a few modes, but as time goes on, it depends on really on the operator. But we are talking about some dumping operators. So the question, the next question is like what Leo said, right? It's ergodic. Eventually, then maybe you can see, wait long enough, you can predict it. You can predict it. You cannot predict transition, but you can predict near the steady state or something. No, let's thank Hong Kai again. So we are reconvening after the break at 10:30. I think we should actually make more controls to  You have a promising role. So if you number your weights, more weights are. Thank you for watching. Yeah, yeah, so it's for my phone. Oh, yeah, it's good. So it's not this where you want to suggest. Oh, yeah, if you have the projection. And I'm saying he's just to do it. Yeah, but you still can show it. It's not the difference. Oh, wait, logging on. Yes, yes, yes. Light up all the third people. So if you have a visionable initial convention that's like for nothing, well, more like to give prompt. Here you want it, is what I love. To partner is boring, everything is possible, but then there doesn't really touch. But I think it's not. But I think after a little bit this very small team, everything is almost like your decay is like minus 80 minutes and the payload which is even faster. Fast money. I mean, that's not every Talmud. All of these things are not. So if the new THC is very small, many of these are the same. My pressures are not this high, so there is some kind of a data. Definitely. I think only the current sample data should measure. At a later time, that's the space of the. So then in principle fiction work becomes different basis of the high level work. Have you tried the production? So these actually should get a personal company. If you take a elliptic idea, you know, which is perhaps you can see that actually I did something for the ancient. Actually, I did some calculations that I created. The other is that you don't have to. Then you create this case a network if you have the scope of the network or say more. Is there any quite specific