All right, okay. Um conservation laws with noise. This is what this topic is about. And then compactness analysis: we have a sequence of approximate solutions and we want to pass to the limit with just the compactness properties. I'm going to do that by using some. Using some micro-local defect measures and velocity averaging for non-homogeneous problems. Okay, so the equations that we're interested in are conservation laws. They depend on X and T, and that's the main feature here. And they can be very rough, discontinuous, for example, but even worse than that. And this is going to be perturbed by noise, and there are many different ways to add noise to this equation. So, what we're going to do here is to consider a mean process that is white noise in, so take the time derivative, so we've got white noise in time, and then we have a sigma of u, which is a multiplicative noise function and non-linear noise function. So, you can think of this as single. Single Vienna process, but also a finite sum of independent Vienna processes, or even an infinite sum. And in the latter case, you have to view this as a cylindrical Wiener process evolving over some Hilbert space. And there is always some stochastic basis under everything so that each time you have a stochastic integral, the relevant functions are going to be appropriate. Are going to be appropriately measurable with respect to the filtration in this basis. Okay, so what about mathematical frameworks in which to deal with this kind of stochastic conservation with stochastic forcing? Essentially, two frameworks. The one is an adoption of Kirshko's work. So, this is basically what you do: you ask for LP integrability for any finite. Integrability for any finite p and then starting from the assumption that you have bounded in initial data. And then you impose initial data, you impose entropy inequalities. So these are appropriate adaptions of those that you know from the deterministic case. If you're interested in existence, uniqueness, and in particular this L1 contraction property, you need to assume some additional conditions. Additional conditions on top of these entropy inequalities. The reason is a technical one: that you end up, if you want to do the Khrushchev method over proving contraction, you end up with stochastic integrals containing non-adapted integrands, and that causes problems. Anyhow, the first one that did this is Feng and One that did this is Feng Annual Law, and then after that, there was a long string of papers that dealt with stochastic conservation within this framework. Alternatively, you can look at this from the kinetic perspective. And so, what you do then is that you have general conservation law, you take a solution new to the macroscopic conservation law, and then you look at the sub-level set. You look at the sub-level set of that function. Let's say that we call that function row. And then this row is going to satisfy this transport type stochastic TDE that you see on the bottom of this slide. So, different from the deterministic case or this macroscopic equation, what happens here is that the noise, which is a source term in the macroscopic equation, Macroscopic equation becomes a transport type noise in the velocity direction at the macroscopic equation, but you also introduce a second-order parabolic operator in the velocity variable. So, using this equation, you can prove existence uniqueness and contraction properties. And this was first done by the Busch and Vaugewell. The Busch and Vaugel, and after that, this generated a lot of interest, and many papers were produced following this one. But I'm not really interested in the well-positance analysis in this talk, so I will leave the details. So, what we are going to look at are instead the compactness analysis of sequences of solutions to these stochastic conservations. So, the classic So, the classical way to do that is, at least in the homogeneous deterministic case, is to use velocity averaging theory. So, if you have a transport operator on the left-hand side of the kinetic equation, and then you have some singular source term on the right-hand side, if you velocity average or integrate in the velocity variable, what you get are quantities that have more irregularity than what you started with. What you started with. So, typically, what you get are quantitative compactness estimates saying that the velocity average belongs to a fractional soblow space or a Bessel space. So, this is very well established framework with many different results. There have also been some work extending this to the case where there is noise, for example, a stochastic forcing term in the kinetic equation. Forcing term in the kinetic equation. But everything here is a circle around this assumption that the drift F, this capital F, does not depend on the spatial variable or the temporal variable. And this is because the technique is based on Fourier transformation, so to essentially separate variables. And that doesn't work if the rift is non-homogeneous. In a non-homogeneous case, when there is no noise, Case when there is no noise, what you typically have to do is to turn to micro-local defect measures, for example, the H measure, to get compactness. You don't get quantitative compactness estimates, but you get a strong convergence in, let's say, L1 or L2 of the velocity averages. And this is what we are trying now to, or we have tried to do, is to see if we can take some of these compactness results for velocity averages for non-homogeneous. For non-homogeneous kinetic equation to those where we add some kind of noise. Okay, so let's go to illustrate this on an equation that comes from Pud√®re's MediaFlow. So this is a so-called dynamic capillarity equation. It contains a noise. It contains a second-order parabolic operator which models diffusion effects. And it also contains this term with a delta. This term with a delta Laplace U, which is the capillarity term in the equation. So, this equation is supposed to be a better model for capillary pressure effects in, let's say, of flow of water and oil in an oil reservoir. So, what you want then is to have water display displays the oil. So, what we're interested in. So, what we interest in this equation is to take these two parameters, epsilon, think of this as the diffusion parameter, and delta as the capillarity parameter, and send those two to zero. We don't have any strong estimates, so what we're going to do instead is to assume that the flux function is going to depend badly on x, but it's going to be. But it's going to be non-linear, and the question is whether we can use that non-linearity to actually do a compactness analysis or try to be able to pass to the limit when epsilon and delta tend to zero. Okay, so the first thing here is that, of course, you need to make sure that this equation is well posed, and you can do that in the ordinary sense of weak solutions. Solutions. A weak here means that you have a point-wise formulation in time and weak in X, and you can look for solutions in H1 and a process that is going to be continuous in time with values in L2, for example. But these solutions are going to be more regular than that. And what we're going to use later is that at least that the solution is H2. But there is well posedness for each fixed value of epsilon and delta. Fixed value of epsilon and delta, you can prove that there is a unique, weak solution to this equation. All right. So what about optimum with estimates? Start from HQ initial data. What we can get out, this is the only estimate that do not depend on the diffusion and the capillarity parameters, is the LQ estimate. Parameters is the LQ estimate, and that gives you basically weak compactness when epsilon and delta tend to zero. The remaining estimates are going to blow up. You have a standard dissipation estimate saying that the gradient in L2 in space and L2 in time is going to blow up as 1 over epsilon. But because of the capillarity effect in the equation, you can also have H1 estimate in space with L infinity in time. Space with L infinity in time, but that is also going to blow up as one over the capillarity parameter delta. And you can move this up to H2, the same kind of estimates for H2, but they're going to blow up faster. The only important thing here is that you have a precise estimate for how fast this blow up of the upload estimates occur. So we're going to use this to control some of the We're going to use this to control some of the terms that later show up in the kinetic formulation of these SPDs. So we have very few operated estimates only the L2 estimate that is independent of the small parameters. So we turn out to the kinetic formulation in order to be able to make use of the non-linear flux function. Non-linear flux function. So there are different ways of doing this, but what we do here is let h be the sine function of your solution u, and then minus lambda, where you can think of lambda now as the velocity variable. And after some work, one can derive a kinetic equation for this H and this is. This is on the left-hand side, and this kinetic equation is a transport operator, as usual. On the right-hand side, you have some stochastic integrals, and then you have other terms that are partial derivatives of objects that are it's not so important what these terms actually are, but these are partial derivatives of objects that are either converging in L2 or stays bound. Or stay bounded in L2, or they stay bounded in the sense of measures, at least in the statistical sense. So, these are in some sense the information that we need for a compactness analysis, not the precise form of these terms that you see on the right-hand side of this equation. And let me also emphasize that in order to establish these bounds, we need to make sure that there is a balance between the diffusion of. Balance between the diffusion effects and the dynamic capillarity effects. So basically, the capillarity parameter should go faster to zero than the diffusion parameter. Okay, so what we want to do now is to use this kinetic equation to show that the velocity averages of this H converges strongly in L2 when epsilon and delta goes. When epsilon and delta goes to zero. This is the goal. And if you manage to do that, the direct consequence is that there is L1 convergence of the macroscopic variable, the U variable, towards a weak solution of the underlying stochastic conservation of this. Let me also mention here that these velocity averaging results. That these velocity averaging results that are available for stochastic conservation loss do not apply here because of the non-homogeneous context of the equation. We're going to derive, use some stochastic variant of the H measure to get compactness, but we're only going to use the H measure in the X variable, in the spatial variable, and then we're going to treat the temporal variable and the probability variable. Variable and the probability variable differently. And everything here now happens under that assumption that the flux function, it can be badly behaved in the X variable, but it's genuinely non-linear in the sense that you see here. This is a standard, it's a general non-linearity condition, but it's a standard condition in the deterministic velocity averaging theory. Theory appropriately modified to account for X dependency in the flux. All right, okay, so how do we do this? Well, we start with a sequence of kinetic or solutions of the kinetic equations, H that converges, let's say, to zero, which is R in L infinity in all variables, probability, time, X, and the velocity variable. So now we're going to apply. So now we're going to apply this H measure technique, but we would like to avoid the H measure in time, and the reason for that is that the H measure involves Fourier multiplier operators based on the Fourier transform, and we don't want to do the Fourier transform of the stochastic interval. So, the assumption that we start from now, and then we have to come back to later, how to establish this assumption. We established this assumption: is that H is going to converge to zero in this particular space that mixes strong topology in time and weak topology in X and velocity lambda. So L2T, L2X lambda, that is a weak. And we're going to assume that it converges then strongly in time, weakly in X and Lambda, and almost surely in the. That and almost surely in the probability variable. This is the starting point for applying this H-measure technique. All right, so what we do is that we want to find or use a micro-local object mu to represent the limit of h squared. So technically, we do not really look at h squared, but we double the number of variables. So we evaluate h. Number of variables, so we evaluate h at velocity p and then we evaluate h at the velocity q, and then we multiply these two things together. And we're also going to localize at different points in x. And it's this product that we seek a micro local object to represent the weak limit of. So one can prove that there exists a functional mu that satisfies the following. Satisfies the following, and namely, that there is you can represent the limit of the Spatel-Fourier transform of this localized kinetic functions H at P and H at Q. Phi one is localization in X, Phi two is a localization in another X. And then you can look at the limit of the product of these two Fourier transforms. Of the product of these two Fourier transforms. So, this seems very general, this object that we pick up here, but you can prove something more. So, if you integrate away the temporal variable, at least on the Fourier side, what you can show is that these objects, they are going to be square integrable as functions of the two velocity variables p and q, and they take value. And they take values in the space of measures. And these objects are going to be the weekly star measurable. And this is what we need because the next step now is to actually derive a localization principle for this functional mu. The goal being to show that it actually vanishes because of the non-linearity assumption on the flux function. So, what we do, I mean, you can focus on So what we do, I mean, you can focus on the I don't have a pointer here, but you can focus on the last equation. So we have the product basically of h times h, but evaluated at different x and different velocities. And then we do the Fourier transform. And then we want to use stochastic calculus to derive an equation for the product of these two objects. And this is something that one can do, and the result is the equation on the bottom. Equation on the bottom of the slide, and the specific terms sitting inside dt and dw are complicated, but it's their precise form not so important. So then you start from that in a stochastic differential equation for the product. And what you do is that you pick up and multiply appropriate multiply capital psi. Capital Psi, you localize this multiplier at high frequencies and then you integrate over the spatial Fourier variable. And the result of doing that is that you make certain sell differentiable differentiable operators or multiply operators appear. And then you compute the expectation. And the order here is important because we want basically to kill off the stochastic integral before we do the Fourier. integral before we do the Fourier transformation in the time variable. So after a number of computations, when you let these two parameters epsilon delta go to zero and the localization parameter m to infinity and you use this H measure representation of the essentially of the weak limit of H squared, what you end up with is a localization principle for this micro local object mu. Object mu, which tells you because of the nonlinear flux that it must vanish, and that immediately translates into a strong L2 convergence of the velocity average. This proof, although technical, it doesn't involve much advanced analysis. The only two things that we use here basically is that the Fourier multiplier. Basically, is that the Fourier multiplier operator linked to your symbol psi is bounded on LP. And the second thing is that these potentials take weakly convergent sequences and maps them to strongly convergent sequences. These are the two essential things that we use in order to show that the localization principle of the microlocal functional leads to the vanishing vanishing of mu. Vanishing all uh mu all right, okay. So let's now go back to this assumption that we started with because we assumed that H, and this is not how we would actually do it in the deterministic setting, but we assume that H were converging already strongly in the probability variable, at least in the almost everywhere sense, and strongly in LQ in the temporal variable. And then the assumption was weak. The assumption was weak convergence in LQ in the X and velocity variable. That was the starting point. And from that, we got compactness, strong compactness of the velocity averages. So how do we establish this assumption? Well, we're going to use this mixed, strong, weak space as a part space for H, for the probability laws of H. And this is a non-standard space in the context of. And that space in the context of tightness analysis. It is not a polished space, it's a non-metric space. So we need some condition to ensure that the probability loss OH is going to be tight. And this lemma here is in some sense the tightness condition. So you remember that we have a kinetic equation. We have an equation SBDE for H where we have some. For H, where we have some rather singular terms on the right-hand side of this equation. So these are partial derivatives of objects that are either bounded in L2, goes to zero in L2, or stays bounded as measures in the statistical sense, like this. So from these bounds, what you can show is that you have some sort of translation estimate in L1 in time. In time, but then you measure a translation or time continuity in h minus n, where you take this n to be sufficiently large. And this lemma will provide you with the tightness in this non-polish space, this mixed strong, weak space that H is converging in or assumed to converge in. All right, so let's now go back to the kinetic. Let's now go back to the kinetic equation. This H measures or any micro-local defect measure is very flexible because it allows you to treat a non-homogeneous equation, but there is also a price to pay here. And you can see this clearly at this point in the presentation, because if you were in the classical velocity averaging theory, a homogeneous drift, what you would do is to take the What you would do is to take the quantitative compactness estimate that you get, the fractional sublow estimate that you get in space, and then you go back to the macroscopic equation. And then you look at the macroscopic conservation law, and you see that you have time continuity with, let's say, values in some big negative sublow space. And then you can conclude by proving tightness on a standard polish space, and you have finished. This doesn't really work here. Doesn't really work here. So now we have to prove or pass to the limit on each of the objects that you see sitting inside the derivatives of this kinetic equation, and you have to assign non-polished spaces to each of these objects in order to be able to pass the limit. Okay, so let's focus on H, the solution itself, and this variable D. This variable d and see how we can pass to the limit in these two variables. Okay, so we view now h as a random variable that takes values in this path space or mixed strong weak topology. And you have D that is a random variable taking values in the space of measures, radon measures. And we equipped that space with the weakstar topology. So, what we have to do now is to establish that the laws of H and the laws of this D are tight in these respective spaces. And you can do the tightness for H using this previous translation estimate. The tightness for D is more or less straightforward. Straightforward. The issue here now is that these spaces that we are looking at, these are not polished spaces. So you are not in the setting where weak compactness of probability laws is enough. So, what you do instead is that you turn to something called the quasi-polish spaces. Quasi-polish spaces and this Ilakubowski version of the Skoyod representation theorem. So, a quasi-polish space, this is a Hausdorff space for which you can find an injection that will take you into a polished space. So then it turns out that if you take a separable Banach space and you equip that space with a weak topology, then you get a non-metric space. So it's not polished, but it turns out. Basically, it's not Polish, but it turns out to be quasi-Polish, but then you can apply this Yakubowski version of the square-off representation theorem to get almost sure convergence after you have changed the probability space and changed your random variables. Let me just summarize this. So, what you do is that you collect all these variables that you have sitting inside the That you have sitting inside the kinetic equation. You assign path spaces to each of these variables. Some of these path spaces are going to be non-polish or more precisely quasi-polish. And then you do the product, and then you get a resulting space that still is quasi-polish so that you can apply the Jakubowski version of the score of the representation term. So, what you get out is that you shift the Get out is that you shift the probability space, you redistribute the values of your random variables. That was called the resulting variables by the same names, but you put a tilde at top. Then this Jeremyakubowski and Skoyo tells you that you will have convergence in the topology of your path space. But most importantly, it will converge almost surely in the probability variable. The probability variable. And then you are in a situation where you can go back and you can apply this micro-local defect measure to get compactness in the remaining variable, namely x. Yeah, I think I stop there. This is another topic. Thank you.