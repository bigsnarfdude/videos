Morning, everyone. Can you hear me? I think it's very early for some of you. Okay, today I'm going to talk about the hiding solutions in Mode RB, and I will illustrate that the forced instances with the two hidden solutions are almost as hard as the unforced ones. So I will begin with. So, I will begin with some background of this topic. Planting solutions to random cultural satisfactions is an efficient way to generate hard but satisfiable instances, where instances are built by first selecting a random solution and then a bench of constraints will be chosen to satisfy. Chosen to satisfy this solution. And in ground theories, there have been works of planting clicks and planting the partitions and planted colorings. From the construction of first instances, we know that sometimes planting a solution may change the properties of the ensemble, and therefore it will make things harder to understand when and why they can provide. And why they can provide hard instances. For example, if we plant a solution to random case sites, then the problem tends to be easy because the hidden assignment will attract assignments that is close to it. Therefore, the problems can be solved efficiently. However, if we plan to complementary assignments to Assignments to three sets, which will balance the constructions, will therefore the problem will appear as hard as random three side formulas. Well, as we know, the problem with determining the size of the largest click in a random graph is MP-complete. And there has been a sequence of results showing that it's hard to even find appropriate Find an appropriate approximate solution. In 2000, Juice and Pinado proved that large hating clicks are as hard to find in UU random graphs, where they consider a random graph G. And from this G, they construct another graph, GK pry, by choosing K random nodes and then Nodes and then with these nodes, they create a click on them that is to see that is to see to complete the inducer subgraph. And they obtain the following important result here. This result shows that given such parameters here, then if there exists a polynomial time algorithm A, which finds a kick lag in graphs. Which finds a k-click in graphs joined from GKP with significant probability, then there exists a polynomial two, such that this algorithm A can find a click-click in graphs drawn from J with significant probability two. Also from this result, we can see that there is a polynomial difference between the cost of solving J and Of solving J and JK parameter. Well, in this talk, I will have the solutions in concerning the satisfaction model named model RB, where it consists of two parts. First is a set of variables here. There are n variables. Each variable takes value from its domain di. And for simplicity that we're calculation, Simplist double calculation, they usually set the dominant size to be the same, d, where d is exponential with n, that is n equals n to the alpha, where alpha is positive constant here. Also, there is a set of m constraints here, and it is different from the UUR constraint satisfaction problem to in here that the number of constraints is superlinear with an. The superlinear with n. There are times n login constraints. And each constraint will involve with k distinct variables, which are chosen uniformly at random without repetition from the verbal set x. And it also defines. This ri is a permit set of tuples of values, which are chosen uniformly without repetition. Without repetition. And the size of this relation, in fact, this P here, it is a constant, it mirrors the tightness of each constraint. And of the model RB, there has been a lot of results. And the most famous one would be: first, the existence of asymptotic physical additions can be guaranteed while applying. Guaranteed while applying a limited restriction on dominant size and on constraint tightness, where it is in fact this R is constant here. If R is lower than value RC, then it is a satisfiable. If R is greater than RC, it's a critical value, then it is unsatisfiable. So this exact physical position has been fixed. And also a threshold point can be precise. The point can be precisely located, and all instances have the guarantee to be hard at the threshold. That is to say, to have exponential true resolution complexity. The interesting one is the third one. It is possible to generate forced status variable instances with one hidden solution whose hardness is similar to unforced one. That we can. That we can, that is to see if we had one solution, then both the number of the solutions and the distribution of these solutions will stay the same, which means this space may be uniformly distributed. And one of the experimental results they have obtained is this one. It lists 15 instances where the mean search cost of solving unforced instances and the forced instances. instances and the fourth instances will be almost the same in the status variable base on this side here. Well, based on model RB, it is possible to generate a special candidal for random graph. It is by first generate n distinct disjoint clicks. Each click contains n to the average vertices and each click And each click here is in fact corresponds to each variable in the model RB. And the size of this click is in fact the dummy size of each variable in model RB. Then after we define the clicks here, we randomly select two distinct clicks, which corresponds to model RB where k equals 2 and generate p times n to the two alpha random edges between the Edges between the two clakes with repetition. And we will repeat step two for another R n log n minus one times. This corresponds to the number of constraints here. And since model RB, we can find, we have introduced that if we had one solution in model RB, then the hardness and many properties does not change. Not cheap. So it's verified that it is possible to generate a random graph with a hidden independent set and the exact side is in the following steps. And this step is almost the same with the previous step, except there are some more restrictions, step two and step three. Let's look at it. First, generate. First, generate n disjoint clicks and each contains into the alpha vortices. Secondly, select from each clake a vertex at random to form an independent setup. Well, the third step is the same with the previous one, only when we construct edges, we need to notice that no edge is allowed to violate the maximum. Violate the maximum independence in step two. Or the last step would be run step three for another R times n log minus one times. Well, from the construction, it is easy to see that for the graph model we have introduced, the size of the maximum independent size is in fact at most n. And whether Most n and whether this upper bound n can be reached or not. And this problem is equivalent to determining the satisfiability of the corresponding RB instance problem. So there is a one-to-one correspondence between the solutions of the two problems. To have any dependent side of the instances of this graph model, instances of this graph model, we can first use this fourth step here, first stack verbal at random from each disjoint click to form an independent set of size and then in the above process of generating random edges, we need to avoid violating the maximum independent side. And from the And from the previous studies of model RB, we have the reason to believe that the solution-based structure of model RB is uniformly distributed. So that means the hidden assignments are not likely to affect, are not likely to attract more satisfying assignments. So we consider hiding more solutions in RB problem. In RB problem. But the analyzing will be a little complicated because if we had more solutions, then we need more complicated calculations. Because the most frequent tool we use is moment measure. So that means we need a high order of moment estimation, which makes things very complicated. So in this talk, we only had two solutions here. And the main reduction And the main result is a satisfiable phase. The expected number of solutions for forced satisfiable instances is simply topically equal to the expected number of solutions in unforced ones. In other words, the strategy of hiding two solutions has almost no effect on the number of On the number of solutions. And that's not due to a bare sampling of instances with many solutions. And in fact, not only is the number of solutions will not change, we also show that the distribution does not change too in this category. So we also talk about problems in the satisfiable phase and the T. Fibral phase and t is a positive integer. Then the distribution of RB instances with t solutions in instance, the three instance spaces are asymptotically the same as n tends to infinity, where the three instance spaces are S, which is the general solution space of random RB model, and S star would be hiding one solution. And as a double And as a double star, we'll be had two random solutions. So the distributions are in fact the same. Well, another interesting result is this one in the also in the satisfiable phase. If n represents the number of solutions of a random enforced RB instance, then the solid moment of So the moment of n is asymptotically the same with the qar for e. And from this result, we have seen that the number, both the number and the distributions of solutions are because the number and distribution of solutions are the most important properties factors determining the cost of solving. Determine the cost of solving satisfiable instances. Therefore, from these results, we can conclude that the hardness of solving forced instances should be similar with that of solving unforced satisfiable ones, which is the last theorem here is the satisfiable phase of model RB with all parameters being fixed. And algorithm can solve randomly generated. Randomly generated the first RB instance with one or two arbitrary hidden solutions with probability P, if and only if this algorithm can solve a randomly generated unforced RB instance with the same probability of P. So this is a little different with the previous clicks because there is in the CLEGS problem there is a gap of polynomial and here the probability should be the same. Here, the probability should be the same. Well, of the proof, there are two key steps. First step is quite natural, that is to prove this result shows is quite natural, that is to see the number of variable in enforced instances spaces tend to be greater than unforced ones. So the most difficult one. So, the most difficult one is the second one is to prove the opposite inequality hold here. So, to prove this one, we suppose we are hiding assignment sigma and tau here and assume that the similarity number equals S0, which means sigma and tau assign S0 variables the same variables, same values. Sim values, and we consider a random one, omega. This omega will have similarity numbers with sigma and which will produce three more parameters, S2, which is the common one with sigma, and S1, that is the common one with a toll and the common verbals assigned by sigma, total, and omega. These four These four parameters here will affect the defines the relations between the three assignments here. As for the calculations, the expected number of solutions in forced up instances can be written as the sum over these probabilities here. Here, where sigma and two are previously defined assignments, and only one is a random one, a random assignment. We need to estimate this probability. Then it goes to this one, and the numerator here has been worked out by other researchers here, and the denominator can be estimated by two parts. First is By two parts. First is the similarity similarity numbers of the common variables, which is this practice is defined by four parameters here and multiplies the probability that sigma tau and omega satisfy a random constraint and because there are Rn login constraints here. So after we get this expression, After we get this expression, we reduce the problem to estimate this sum here, this sum here. And this sum is a little complicated here. And we can find global maximum of this expression and then partition the different parameters into many small intervals. Many small intervals, and then calculate the contributions of these intervals. And then some of them finally prove that this result is in fact less than one. So which will which will prove the update the crucial one, this equality hopes too. So with all the results, we can all the results we can repeat the previous problem that is to generate generate generate two heading independent variables sorry generated two heading independent size of that and the the overall processes are the same except the second one we The same except the second one we can select to click at random to form two independent sites, and all the other problems are outside are exactly the same. So because the work involves a lot of calculation of proving this equality, and I think it's not necessary to expand all the calculations here. So, yeah. So, yeah, I think I can stop here.