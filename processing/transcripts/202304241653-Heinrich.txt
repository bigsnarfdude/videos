typically when quantile regression doesn't perform very well if you're far off. So I was wondering how if you're using some special type of quantile regression to perform? Well we could yes for before we did like also estimate the p-values and then we have an issue that the tail probability is very small. Quantum regression we're trying to find the critical value right so we do quanta So we do quanta regression of the test statistic versus parameter space. So I don't think that's as sensitive to the problem you're mentioning. Okay. But if I say p-values, then we, yeah. Although Herson Prospos said he would like work better for his application for some reason. So in that is a way to account for the uncertainty associated to that estimate? Like in your variance, like it could be biased potential. Right. Yeah, so when we do that. Yeah, so when we do the quanta regression, that's a regression we can, I think, we minimized the pin boss loss, and we ended up using neural networks because it gives you smooth estimates. We tried other things like boosting increase. So we also, I guess, some uncertainty, depending what algorithm, uncertainty on the estimates. So that's what actually we showed, but that's a different problem. When we estimate But that's a different problem. When we estimate the imperial coverage, right? I mean, we want to control a type one error, a level alpha. But when we estimate that we do the regression, there's an uncertainty in that. And that's how we got talked about undercoverage, overcoverage, across coverage. Yes, I didn't show that. Yes, I wasn't sure if you were accounting for both values and various. Yeah. Yeah, when you were not supposed to. Yeah. Yeah, when you're on Netflix it's a little bit um complicated because very good consultant mister Maybe it's a related question, but you had mentioned the sort of difficulty of like power in increasing the power of the test, which I mean, is this eventually related to the same problem in the sense that like you may not have much control over how good of an approximation of the likelihood ratio you're getting, in which case the power can be all over the place. The power can be all over the place if the likelihood of approximators. First of all, there's nothing magical about the likelihood statistic if it's not like similar simple. So how much so basically how whether you have validity or conditional validity? That's just how you calibrate. But then we've seen that the power is the whole thing. It's like how do you define your test statistic and how well you estimate that. So the likely ratio might not be the. So the likely ratio might not be the. I kind of believe in the Beige factor now. Because you see that if we use the Beige factor and if you have a well-specified prior, then you gain a lot of power. I guess what I mean in some sense, like you have untoward regression, which allows you to calibrate whatever's coming out of the neural network. Whether it's because even if you compute the base vector, it may, the O-hat may be way off- It doesn't matter. So validity depends on your calibration. Depends on your calibration, doesn't, you know, any test statistic. But power depends on, you know, the important thing is that we have, since we use frequency methods, validity and the prior, the independent of each other. So first you have to control type one error. Once you control that, then you can talk about power and how to define a test statistic, how to simulate more efficiently. Otherwise, you have to correct for this. If I simulate differently, To correct for this, if I simulate differently, then I no longer have, yeah. So now we disentangle, then we can talk about how to do things. Yeah. It's sort of an easier discussion than sort of turning around and be like, well, what's the, how do I quantify uncertainty on the quality of my ratio estimate? That's a really hard question. But sort of calibrating some tests. Yeah. So validity is basically, you know, we can look at it. And power would be for confidence at how small, how well we can constrain parameters. How well we can constrain parameters, and that's usually what people look at. But if they're not valid, then why do you look at the size of it? So now we cannot disentangle that. Now you can look at how big constraint you have. Any other questions in Perisho? We have a question on Zoom. From Jim. In yours? So somehow And somehow, so you've talked about a lot, a lot of mission movement. And I'm on one of these gamma-ray experiments, which we have been working at gamma-hedron rejection for some number of years. But I'm not understanding what is distinctive about your machinery. We've been trying to use information gathered from simulations and from background events. This is really Monte Carlo to measure efficiencies and your injection hadrons quite some time. I'm not understanding what does your machine do that's different than the kind of thing we do variables. We evaluate fraction of them of ones that pass and so on. And so on. So, when does different company change? Okay, so first, because of my ignorance, I don't know what exactly you did, so I can't answer that question. So, it was an example that Alex just lost in general. So, we basically just talked about how to do classification that's independent of how you simulate and what your nuisance parameters are. So, that was an example of how to do nuisance parameterized classification to solve this. You know, to solve this testing problem. So I don't know what you did, but it's without using asymptotic assumptions. And the second part was how to get confidence sets that are valid without using asymptotics. I haven't seen that much on that, so I don't know what, again, we're doing. Can I make a point? I think the question is basically similar to what we did in Thomas's experiment. So if I heard correctly, like the person is asking. If I heard correctly, like the person is asking that they're using some kind of handpicked features correctly for estimating these properties of the astro particles. And I think here the difference is that you would not be using these hand-picked features. You basically use the raw data as it comes out from the simulator without having to do this kind of explicit damage reduction by picking some features. Right. Yeah, so the simulators, I mean, it might have physics in there, but it is. I mean, it might have physics in it, but it's kind of a map from your underlying parameters to absorb data. So we don't do an explicit compression necessarily. So for the hypothesis test, we, for example, average a classifier that can deal with high-dimensional data from all the different arrays without summer statistics. How is this different than saying with the function? We use the nine problem legal performance. But I'm still not understanding what in theory is. Maybe one thing I would suggest, we have also the Slack channel, so for kind of detailed, I mean, I'm not sure if it's easy to kind of compare detailed views. Yeah. And maybe one question. So, if you do the functional regression, you need to generate. Regression, you need to generate training data of the tested testing distribution and the parameter. And so, one of the things that's difficult with Nyman construction is that basically, you have to have it everywhere. But in order to generate the training data, you also generate it from a prior, basically. So, you pick some kind of... But it doesn't affect your results. But it has to cover your results. It's all about parametric bootstrap, and it's all about where do you trust your It's all about kind of where do you trust your critical region, or where do you trust that you know how your critical region is behaving. And so it's basically tied to the choice of prior. And parametric boot service is basically saying, okay, that's the New Jersey parameter, I will just extrapolate the critical region from that point to everywhere else in the New Jersey parameter space. And here you use some kind of prior. So the the pri it's not really prior, it's just like how do I sample, so it's more like a reference distribution. So it's more like a reference distribution. I mean, we can change it. Primary, you cannot change. The other is like when you say a primary bootstrap, don't you create like a batch at each grid point, right? Like you want the column. Also, if you basically say, okay, I kind of estimate the sampling distribution at some parameter of interest and their corresponding best Newton's parameter, and I just kind of assume that that critical region holds across all Newton's parameters. Now I don't need to check those. Parameters. I don't need to check those dimensions. No, no, no, we do. No, no, no. So, so what Alex is doing, we don't, it's not hybrid. We sample the entire space. So the emissions parameters are just like. So I guess that's what I'm saying. You can't really sample all of the parameters. You need to sample the entire space. You choose some kind of density over the parameters. Yeah, it has to cover it, and then we do contact regression. So it's not primarily bootstrap. It's not a batch. So you do it over your entire parameter. Over your entire parameter space, if five, that's the results are shown. So five you can do. But if you have thousands, then exactly. So I think that this is where part of the success. Like a lot of the free inference, they basically always involve choosing some kind of prior private space. And if that becomes large, you essentially become really sparse. Right. Right. But you cannot have an idea what's very simple follow-up on that, because I I want to make sure I understand a very basic point. A very basic point. The simulated data that you use generates an energy and a direction for your positive brain, correct? Is that right? Yes. And there will be a distribution for the photons and a distribution for the protons. Are they the same? In the simulated data, do you store it? No. So no, I think in the simulation you specify those parameters. So you can limit like energy and direction. Energy and direction. I'm talking about the distribution of those particles. They're different, right? Yes. Well, so I know they're different in nature. But your simulations, your theoretical model, right? So they're built into your simulator. And whatever error you have there, well, we just use that. What I'm trying to understand is that you wanted to get out of this algorithm whether the cosmic ray was a proton or a photon, and you want to estimate its... And you want to estimate its energy and direction. I've got that right. But what I want to understand is that in nature, I'm guessing that the energy distribution between the protons and the photons is very different. Yes, so they're different. And so already, if you then just look, I want to know how you avoid the following problem. Let's suppose that the energy distributions of those two populations were very different. Suppose you then generate one of the energies, which happens to be more characteristic, say, of protons. Would your algorithm Of protons. Would your algorithm therefore not somehow identify it as a proton on the basis of that energy? Now, what the energy distribution is, you say, is a polling. As Dan says, it's different, but it's not so different. It's not that best to that you can put in the classification as an additional feature or exploit it. In principle, you could. C principally cool. Then it'll do some prior and then you can't. Okay, so that was really my question is whether or not you're exploiting differences in the distributions, telling which was a photon and which was a proton. But the answer is you're not. I mean, in many cases, we are not doing it, although we could. So for instance, we could improve the particle flow algorithm by looking at our fragmentation functions and no parameter I deal with that on a probabilistic basis. With that on a probabilistic basis. We are not doing anything because it's on the edge of our current comfort zone, right? We would do, for example, in particle physics, we could use something like a multiple regression to try to determine the energy of a particle, one of our detectors, on the basis of these directions. But in fact, we know that particles that come off at low angle. Particles that come off at low angles relative to the beam pipe usually have higher energies than angles that come off perpendicular to the beam pipe. So, just on the basis of that direction, I could already get a pretty good guess as to who's the high energy and who is the low energy. But that would be cheating. So, we don't do it that way. And that's what I wanted to see if you were doing it. So, just to be clear, so we do not do classification or regression to get internal parameters because that will not be correct. That would not be correct. So, the assumption is that your theory is your likelihood model. If you do classification to classify the two, if you change your energy distribution, change how you simulate, you'll get different results. I took out those slides. So, the main thing is that we leverage prediction, procedure, whatever you want. That's machine learning is good to do inference. So, do not do classification for that. You just need to have a good model for different energies for what would happen. Different energies for what would happen. You need to have a good forward model, but do not do classification to do any intense problem. So I have a question for Alex. I think you said that when you were trying to distinguish photons from protons, photons are much rarer, and that there was a question of whether you should use equal mixtures for training or maximum Ink for mixtures. I mean, one of the, okay. So which do you use? I think, well, in practice, we're probably going to use an equal, but that issue only comes up when you're trying to do, like, just directly predicting from the observed data. And you get all these problems with label shifts. But I think the good thing with our method is that. Method is that basically, no matter how you simulate or what fire you use, you can always get Python car naturally. Because if you have a very rare process, or maybe a non-existent process that you're looking for, you don't want to use natural mixtures with natural cosmos. It doesn't really matter in terms of getting validity. That's what we're saying, because it's an inference problem. When you do prediction, then you're sensitive to prior probability shift. We're not sensitive to that. But in terms of power, But in terms of power, it might matter how you simulate and how it is. That's the key kind of key message. Because if you have a very small fraction of signal in the training data, you just classify them all as factor. Yeah, but we're not doing that. We're not customized. Very simple question, but also to do Lucas, you're also working or not in this backwards appearance. I mean, of course, you have somehow identified the speed of this simulation program you're using, right? I mean, in your case, it's Causica. And for a detector simulation, LHC, like this clear detects by 20 seconds will even. Right? And so I don't know. I mean, from what you're saying, it looks to me that for your case, you are not in the problem. But there are some limitations to that. That's not clear to me at the moment what these limitations are. I mean what these limitations are roughly. I mean like you I guess it was also question about the nuisance parameters, how many you could take care of it. Yeah, I mean the the two sometimes here, right? One is that it's a high fidelity simulator, right? That is your model. So any systematics have to be in as users' parameters and that's what we'll work on. The other is that they're fast. But I think once you know like you can you know have conditional validity you know how to assess power then you can talk Or how to assess power, then you can talk about how you simulate maybe more efficiently if you want to take costs. Yeah, but they still have to be relatively cheap. I don't think they need to be super. I mean, it's the same thing, it's like you amortized to generate a huge amount of simulation as we do right now. I mean, we already do simulation-based inferences decades, but it's just like a different way to use the simulated Monte Carlo events that you have anyway, so you and they're embarrassingly parallel, so you just throw a lot of view. Then you have them based on these samples. So you don't need Them based on these samples. So you don't need to run the simulator on the fly as you're training the thing. So I think it's not that's big of a chill. Question for Alex. I mean, you very nicely explained about the method. Does it work? Yeah, I just started. Okay. So don't know if it works. Yeah. Okay. Hope it works. Yeah. It works on that. Yeah, it worked on other problems. Not for the easier. Any pathway to scale to the larger numbers of problems? To larger number of parameters. So you have a model with like, I don't know, 20, 30, 100. Yeah, I mean, I think we can deal with reasonables simulation sizes, 5 to 10. If you go beyond that, the 2. Beyond that, two things. One is use hybrid methods, which doesn't guarantee you conditional coverage, but maybe we'll come okay. The other is what I wrote that I would maybe have installed. It's basically having defined test statistics that are robust to nuisance parameters. So if you have this, you know, it's inspired by quantum physics, you have massive variance, right? So if it's robust to nuisance parameters, then you don't need to look at it. I think ironically it might be that the way to deal with high parameter counts is just to go back as we've always done. I mean, so kind of there's an equivalence between the simulation-based inference and just modeling the output of the machine learning thing as the observable that you were studying. And then you build like a Poisson model around this density. And for that, we've always been used to having like many, many parameters. Yeah, but then there are other. But then I think there's also an issue in how well you train it and then maybe just follow up to that thing, because you just said you're earlier there. It wasn't super clear to me how I understand that you can calibrate whatever test it is you end up. My very first step is learning, say, the odds function. And then how sensitive you are, or where if you don't do a good job there, when you If you don't do a good job, then where does that show up in the skills? Is it just you lose power? Yeah, you lose power. So if you caliber well, I mean it doesn't matter how you what test the disc you have and it's just the quantum regression. But you lose a lot of power. Actually the most power we lose is not how well you estimate the odds. It's actually when we define a test statistic, there's an integration suit. That's where it's a numerical issue. We lose a lot of power out pictures. We lose a lot of power and picture that. We don't know how to integrate the higher dimensions basically. I mean, a lot of the kind of examples we've seen so far until Alex finishes, he's kind of on toy studies where you have something to compare to because you have a lot of it doesn't look far or a real kind of that's the kind of worry I have in my head. I cannot cut basically fifty slides imagine. So but I have a full GMM. But I have it for GMM, how much where we know the truth. And not GMM for the Gaussian model, where we know what the truth is. So we can see in 10 dimension, 10 parameter space from what we use power and what we've done in what we why. So it was actually not because we estimated all it's not so good. It was actually because we didn't do integration properly or the super. So it's a numerical issue. Yeah, so so regarding the super, so uh is this how it So you estimate with the neural network to the odds ratio and then you run some kind of minimization on top of that ratio. Or integration. Or integration. And that's where we lose the power. I should estimate the classification without some power. But then that's maybe different. Yeah, yours is. We don't know how to implement. Any other questions? You can just add the names previously. So, we do actually have a very realistic case study for this, which is for Tomaster's problem of inferring neon energies from hydrometers. And it works extremely well in there. So, I don't know, do you have any of those? Because I mean, I think Tomas. I do, but I don't know if the audience can't take it. But it does work, like it works extremely well. So, basically, you assume you have a fine parameter with about 60,000 output channels. Your task is to use. Channels. Your task is to use that calorimeter information, the infrared, the energy of the inframion. And this method was used there and it works extremely well. It's well calibrated. If you use this high-dynamosophy image, basically the raw parameter information, you get smaller intervals than you would get. Yeah, yeah, but it's just one. So basically this is okay, you can just cut me off, but whenever I'm going to be able to do it. Well, I think we have like a little bit of a time. So dinner starts at 5.30. So dinner starts at 5.30, so maybe we break in five minutes so people can drop their bags and change themselves. So this is a different audience, but what you have to show is that when you send the microphones and you interest the data and if it's low dimension, you X, which is very high dimensional, a lot of people would just use a CNR and predict that. But this was like two muscles uh weight problem that you have a neuron that goes into. That you have a neuron that goes into kilometer, these are the energy deposits, and we basically want to estimate the neuron energy from this high-dimensional data. I don't remember, 30 times the time story cells. So if you try to predict, then if you look at the true energy and the predictor one, you get this prediction bias. And it's foundational, right? It's not like you, it's basically because when you predict, you estimate the unconditional mean or the prediction. In conditional me and the posterior me, and it's not the same. It's an inference problem. So, works here means the expectation value of what you get out is basically aligned with the true value. Yeah, I actually don't have the slides here. Yeah, I'm sorry, but this is posterior, not with it. You use neuro posterior, you create posterior, and whether your coverage is good or not depends on what your prior is. So, this shows, so in this case, the prior was close to the true value. Close to the true value. So if you look at the conditional colour, you do well there. Elsewhere, it's terrible. But it's also like you have a posterior. So what we do is we use this for wildness, for make as a. So we call it wilder because it's kind of like the wall task statistic, you usually have a. It's a it's a converging, right? The posture mean to the MLE and the yeah yeah yeah well done yeah. Yeah, well done, yeah. So if you have a posterior estimate of prediction, it will give you the posterior mean and you can estimate the posterior variance and if you have a neural posterior you can compute that. So this is another test that is there based on posterior prediction algorithm and we use calibration and then you can get, yeah, I didn't show that. And then you can see how big they are, right? So that's power. And what Nika says is for the muon, we could see that. We could see that they're well calibrated everywhere for all energies, and then you could see the intervals actually tell you, for example, high granularity versus using the entire energy slab, you get more constraining power of smaller intervals or smaller prediction sets. So, when you're comparing the coverage between the Bayesian and your procedure estimation and score, so is that actually a surprising? I mean, Bayes don't care about coverage. About coverage. I think that you're right, and they want to talk about bias, isn't that? In some sense, you want the true parameters to be right, but I mean, like in the kind of orthodox fashion thing, yeah, yeah, sure. I mean, so is that like a surprising result? I mean, it made people very angry when we do this. Yeah, so we rejected the paper. Yeah, yeah, I I think it's not surprising, but it's it's like if you change your prior, right, then it's it's prior independent. But if you have a better prior, But if you have a better prior, then you get more constraining power. Okay. So, following up on what you just said, a big source of confusion here is that there's kind of two different types of coverage that people look at. So, here, Anne and others are looking at conditional coverage, which is conditional to parameter. But a lot of these people who do like the pays and stuff here, they look at the unconditional coverage. So, they just march and look at how things are covered. How things are covered, and then you basically re-sample the parameters, and then everything works fine. And the basin procedure almost by definition has correct, unconditional coverage, but conditionally has this behavior. So, like, when people say that it has good coverage, like the basic area, just be very careful which type of coverage. I was kind of saying the opposite. The description, you might not even want to make sense, but I've got coverage. You're also talking about the coverage of your posterior, right? And we're also looking at that, but that's a different thing. Okay, we'll discuss how we did. But that's a different thing. Okay, we'll discuss over there. Okay, thank you, everybody.