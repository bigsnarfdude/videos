And this is going to uh be based on two papers with subset of this group of collaborators. Elia Bruet now in Milano, Occoli, Maria Colombo in Lozan, Camilo de Levisimi, Princeton at the IAS, and Massimo Sorella, a graduate student of Maria Colombo. So let me start by introducing the main questions, but I will not make the main comments for this audience. So we work with incompressible oil. Incompressible Euler equations, so the homogeneous incompressible Euler equation is the velocity is the pressure. On RB or on the torus, I don't look at the case with boundaries, that's too complicated. So we model an homogeneous incompressible implicit fluid. And of course, we have the well-known formal, at least estimate for the kinetic energy that we get by scalar multiplying with the velocity V itself, integrating and Integrating and partially integrating here and exploiting in the vergence-pray condition, and that leads formally to the conservation of the kinetic energy. So, this quantity, so categorical E as a function of the time, is conserved, so E of P equal to E at the initial time for every T. Then, if we consider the case with viscosity, so if we move to the Nagierstokes equation, so mu is now viscosity, so we add this Laplace. Quantity. So we have this Laplacian term on the right-hand side. We are now considering the case of a viscous fluid. And the formal estimate for the kinetic energy now gets a little different. So we have this minus integral of gravity squared. And we don't have conservation, we have no dissipation of the energy. So that's something that you know very well, probably better than me. And as a summary, energy is conserved. Energy is conserved in the inviscid case, with caveat, but energy is dissipated in the viscous case, and the rate, as I was illustrating here, so the rate of dissipation depends on the viscosity and on irregularity of the solution itself, okay, for its gradient. So, all this is formal. That is, I made computations, assuming I can do all the computations. Le Real solutions to Navier's thoughts are weak solutions which satisfy a form of the energy balance in terms in the form of an inequality, a non-pass inequality. So it's very important this formally. So I am assuming until now that I can do all this computation and I get the equalities that I use. Nevertheless, already in 1949, Osavier wrote my paper in which I In my paper, in which at some point it was sentence that the turbulent dissipation could take place just as really without the final assistance of viscosity because the velocity field does not remain differentiable. So this means the following, so Onsaker was already pointing out out of some formal reasons that these computations are really formal and you might have some regularity setups in which, well, you are not able to justify them as qualities, on the contrary. On the contrary, you should expect dissipation. And this is the content of the celebrated Kolmogorov K-41 theory of turbulence. So one of the tenets of this theory of turbulence is the so-called zero, which is so basic to our understanding of turbulence that has been made in this way, that the dissipation of the kinetic energy is uniformly positive, so it's uniformly Positive, so it's uniformly discositive lower bounded by a quantity which is strictly positive, which calls it. And in Kolmoboros theory, so these brackets denote something which is never clearly and rigorously defined. So it's some kind of average, space average, time average, ensemble average. If you have some variant rate or spacewalk solution, which is clear that you have, but okay, so that was. That you have, but okay, so that was more like a holistic derivation of this type of statement. Of course, there is a difference, which I will not stress too much, between 3D and 3D. So in 3D, you have an inverse cascade, so you have formation of large patterns, so 3D3 case for this empirical theory at least. And the phenomenon is exactly the following. So you have the convective non-linearity in your equation. So this u dot gradical. So, this u dot graphical U, and this implies a cascade to small scales, high frequencies, where the Laplacian becomes more and more effective. So, it has a stronger effect. So, let me stress that both experimentally and numerically, so Fugo and Spur is very much validated for a very large extent, although there are some corrections, some deviations from it, but so say that there is a strong numerical and experimental support of And experimental support of a large amount of statements from this theory. Nevertheless, there is not a large mathematical theory for this. So if we formally manipulate the nonlinear term, so in the kinetic energy estimate, so we add the v dot grad v is color product of v. So you see that we have like three v's and one gradient. So I distribute, you know, one third, I give one third of the gradient of. I give one third of the gradient of HP, and then I get something like this, which is formal, you might think of just defining this with some paradigmial calculus, but okay, let's just come to the realities of this level. And this gave the, basically, the scaling is one way of seeing the scaling in non-Sagger well conjecture, like 70 years ago, here and by now. So Euler solutions will see alpha regularities, so the increments scale like Increment scale like the increment in the space variable T alpha. So, are expected to conserve the kinetic energy if alpha is larger than one-third, and this has been proved by several others, King and concentrating reading with errors, but are also expected to potentially dissipate the kinetic energy if the regularities in C alpha, alpha less or one further. And this has been proven as the cool. And this has been proven as the culmination of ten years of complex integration efforts by IZ and by Buchmaster DeLellenis Select Nicole via complex integration. And this dissipation of the energy also brought together with itself non-uniqueness. And so they could at the same time prove many of solutions for Euler. So one thing about performing convex integration. About performing convex integration at the level of the Euler equation is that the proof by itself, so the strategy, possibly gives an increase of the kinetic energy. Non-conservation, not necessarily in the form of a dissipation, so the kinetic energy can grow locally, or globally, which you might deem as non-physical from what I was saying before. Nevertheless, non-physical. So the colonography. So the Kolmogorov theory of turbulence says something which is stronger than dissipation for the limit in this equation. In particular, it postulates uniformly in diminishing viscosity, so dissipation of the kinetic energy, which is not just for the illnessic problem, but it's uniform in viscosity, for small viscosity. And this is why we open largely. And this is widely open, largely open, for the Navy stocks to wider limit. So something has been done in an important paper by Bookmaster and Peekle for weak but non-relayoff solutions, again with complex integration. And in particular, so Kolmogorov theory is a sort of statistical theory, so this predicts sort of universal behaviours, while convex integration basically constructs simple examples. That constructs simple examples. So it's not a statement of the type the typical velocity field solution of the fluid equation has this type of signally popular. And here there is a full lack of a mathematical setup, so it's unclear how to formulate this statistical state. And also, so in all these problems, so complex integration brings with itself extreme non-uniqueness. And then another question. Uniqueness. And then another question, which, well, if you take a point of view of conservation laws, is very natural. Can we restore in some way uniqueness via some selection principles? For instance, so if you consider the Burgers equation, so scalar conservation law, you know that weak solutions are not unique, but there is a unique entropy solution which is also selected by Lenichi discosi. So you have a mechanism which tells you So you have a mechanism which tells you that although you have multiple weak solutions, there is one nicer solution and approximation algorithms will converge to that solution. So in a sense it's a way of restoring things. So a very legitimate question is to which extent can you think of doing anything like this selection in this type of fluid evaluations. So we don't address that. So there will be something in the second part of the talk about the knowledge. The second part of the talk about the non-linear equations, but let me take one step back and go to the linear case, so to the so-called theory of scalar turbulence. So think physically to the following situations. We have a given velocity field view, which is irregular, which is turbulent in some sense, and some passive scalar, so the temperature or a concentration of some dye in the fluid, is just passively advective. So there is no. Advected. So there is no feedback of data on the velocity. It's either passively advected or advective and diffused with the diffusivity zone kappa. Now I change in a notation I call it diffusivity, but that's not very relevant. So I consider now this linear equation and I might start doing the same treatment as I presented earlier for Euler and Lavis Toads. So I do a formal L2 estimate using again. Formula to estimate using again the divergence free condition for the velocity, and I get the following. So the L2 norm squared of my passive scalar at time t is, so it equals the initial L2 norm squared minus the dissipation of the L2 norm, which again depends on the diffusivity and on the gradient of solution. Same computation as before, and again I'm doing the same formal treatment. Treatment for the nonlinear term. So in the L2 estimate, it comes from u dot graph theta product with theta. And then again, I have one gradient, one u, two data, and then I get the following. So I give alpha gradients to u and one minus alpha divided by two gradients to each of the loop data. I get this thing here. And then again I get uh the criticality condition on the uh on the regularities of the solution. Regularities of this function, which goes under the name of Jebelon's relation. So if u as alpha derivatives, theta has beta derivatives, the criticality now is alpha plus 2 beta equal to 1. If I have more regularity, if I am better than critical, it is subcritical. If it's less than 1, I am in the supercritical. So this, in a certain sense, is the equivalent of the one-further criticality for the lawyer. The Euler for the other equations, which you formally recover if you said you want to pick up the equation, whatever this means now. Very nice. So, this is not our invention, so the work is to apply mathematicians, physicists from the late 30s, 40s, beginning of the 50s, so all of courses of Russia and COVID in Russia and of course in the US. And well, they did some predictions, so basically by analogy, so by having some arguments similar to those by Khoroth. So they predicted that in the sub-critical regime, so regularity alpha plus two beta greater, so large with that one, they predict the uniqueness. So this does not show up for the other equation because that's For the other equation, because that's non-linear. So now, due to the linearity of the equation, so in the sub-critical regime, you can expect uniqueness. Uniqueness is subject to this regularity assumption. Then existence. Other story, why should C beta solutions exist? So this uniqueness for the equation. And conservation of the L2 norm for linear advection in the cellular. While they While they also predicted that in the supercritical regime, the Maya might have uniqueness and dissipation of the L2 norm, like lack of conservation of the energy for the other equations. And more specifically, so dissipation of the L2 norm coming from anomalous dissipation. So not only the linear transport equation in the limit as dissipation of the L2 norm, Dissipation of the electron or for the advection diffusion equation, you have a uniform in diffusivity dissipation type or solutions which are uniformly in dissipativity in the regularity class, in the supercritical regularity class. So, kappa uniformly in kappa in the space variable. Variable. So this is their prediction, which has been done in analogy to the Alacomodorov's prediction for the other conservation. So the sub-critical cases, so uniqueness and conservation is sort of easy. So again, it's a concept in ADD-like argument with the computer person that works basically in the same way. So for the supercritical case, of course, the first question is what are we expecting? The first question is: What are we exactly asking? This was very formal. So, are we looking for one velocity field and one initial data so that this happens? Or one velocity field so that it makes this treatment to all initial data. So, for every initial data, we have this form of this patient. Or something which is even stronger: should we expect a statistical statement? So, for Statistical statement. So, for a large class, for almost every instant sense, velocity field, and for almost every solution. That's again a part which is very valuable clear. Okay, so what I would like to tell you is what we did in this paper with the first one with Mario Colombo and Massimo Sorella. So, what we did is we addressed the supercritical regime, so alpha plus to data plus 1, and we should put And we could show anomalous dissipation for one velocity field and one initial derivative. A velocity field and one initial derivative. So that for every kappa, if you consider theta kappa, the solution to the attraction diffusion equation with that diffusivity, and the family of those solutions satisfy this property. They are also uniform. They are also uniformly bounded in step. Then, so we have kind of two bonus statements, so by slight modification of our proofs, so it was mentioning before this connection to Kurger's equation. You might wonder whether in a regime in which you have a lot of non-unique solutions, you might expect selection. So, you have some good selection procedures, some good approximation. Selection procedure, some good approximation procedure, so that the limit of this approximation procedure has a best weak solution for your problem with multiple weak solutions. The answer is no. So what we could prove for the linear transport equation, lack of selection under vanishing fusility and lack of selection under convolution of the velocity field, which are better what these statements really, really mean. Really, really mean. And what we use, there is a sort of a Lagrangian approach, but not a stochastic component. So I will tell you now in a few words what this thing is. So you know the Lagrangian approach, so I repeat it too. So if you have the linear transport equation, daily theta plus 2 dot graph theta, well you have a deterministic flow. So you find integral lines of the velocity, the velocity is given. Velocity is given. And then you have like a handle representation. So theta is implicitly determined by the initial data. So you go backward with the goal with the flow, and then you look at the value, the initial data length of the point, and you want to manage that realistic curve. So when you have diffusivity, so you have a stochastic approach, which means well you don't work with the new. Work with an OE, you work with an SE, which is a rather special one because it's quite different in a sense. So you only have the suburban motion, you have the Levant side, and you have a stochastic Levant representation of this form. So, I mean, if you are like me, coming from analysis and not strictly from stochastics, so you can also see this equation here, like a parametrized family of Parametrized family of ODs, depending on a parameter which is the representative of the Bernie motion, once you fix the element of the probability space. For every given element of the probability space, you solve this equation here, and then you have the stochastic Lagrangian representation, which is now cuts, which does the following. So for each of the uh of this uh parametrized equation, Parametrized equation, you find the corresponding flow, you find the corresponding push forward of the initial deduct, and then you average with respect to the probability dimension. And that's final guts, and then you have this representation of this. So it's a sort of extension of what you have in the MIT case. Okay, technical is much more complicated, but we hide the whole MISD. Okay, so this W, as I was mentioning, as Brownian. W, as I was mentioning, is Brownian motion, and for what we need is, you know, you can think of this very simple picture. So you have a probabilistically parametrized family of trajectories, which are quite irregular. And we only use basically two properties, so properties on the increments, so you have the Aussian increments, so these trajectories, and so you know the distribution, so the distribution of the increments. So, the distribution of the increments of the trajectories. And the other property that the Sampoon will use is an isotropy of the Downian motion. So, it's not more likely to go in one direction rather than in another direction. This comes from the fact that here we have a full applaution, okay, so which is also a concentration. So, okay, so the basic phenomenon is based on the theory of uh mixing. The theory of mixing for the linear equation. And this works in the following way. So, what we do basically is we apply the following strategy. So, first we describe the different stages, so the steps, the solutions, so the payment solution undergoes. There will be some parameters in that some of them will describe, some others will be hidden. I will not really mention them. Much from the mind. And then we construct a velocity field that realizes that evolution happens, such that this evolution goes through the stages that I described. So we do the following. So we pick a sequence of times, TQ, converging to zero, and the sequence of space scales, AQ, converging to zero as well. Then I do the following. So Then I do the following. So I prescribe that my solution takes the values plus and minus 1 on a chessboard of side AQ at times 1 minus TQ. So you have the initial time, you have time 1, and then you have a sequence of times, dq, which are done in such a way that 1 minus dq converges to 1, which is the Q converges to 1, which is the singular time, which basically you are fully mixing, you are completely homogenizing your solution data. And I am prescribing, so I am picking the sequences in such a way that at this time my solution has a scale which is a q. I use the q because I was working with people doing complex integration, they like a q for the index. For the index. But we don't use convex integration. So nothing against it, but so the velocity field, well, we need to construct it. So here I prescribe the solution. The cool thing is that you can really make this evolution of your scalar happen by using shear flows at suitable space scales, which are related to this. Which are related to these, and we don't go into the details. So, think of a dyadic evolution. So, think that in time 2 to the minus Q, you refine your scale of a factor of 2 for the solution as a kind of way of getting a picture of what we are doing. But that's not really the thing that we need to do, so technically the proof, so we need a super exponential in order. Super exponential and not an exponential sequence for two reasons. So we need to separate scales much better than what you do with an exponential sequence. And second, at some point, you know, we want to optimize visibility. Okay, so here we have an open C beta. We will need some regularizations, so we need a little bit of space to optimize vectority for the concern. And okay, And okay, at n t equal to 1, you have perfect mixing, so your solution converges here to something which has average 0, which is identically 0. Then you might think, okay, so I just revert everything. So I time mirror everything. So for t between 1 and 2, we just reconstruct our G S model. Then we go back to the initial dimension. So that we have defined now the evolution for time between uh C and two. C then 2. Okay, so that's the basic choice we said. Now we start from the third theorem from our paper because it's the one which is easier to visualize. So what do I mean by selection by convolution? I mean the following. So I have multiple solutions in principle of this equation. But then I take a convolution. But then I take a convolution kernel, so I replace u by u, I call it epsilon, okay, so I convolve my velocity here, so I have epsilon, and then correspondingly I find theta epsilon. Okay, so theta epsilon is not the convolution of your solution, it's the solution of the same equation but with a regularized velocity. For phi epsilon, a standard convolution. Standard confusion curve. Keep it so you decide this curve. Okay, so what I want to convince you is that we have an example of a velocity field and of an initial deduction so that the sequence derives epsilon as multiple different points. It's convolution in space time? It's convolution in space. That's convolution in space. So we do this by the following. We do this by the following then. So this proves the question or the conjecture of the selection under regularization of the velocity theorem. So that's for what I will tell you later about the fusibility. So think that you're doing the following. So what is convolution doing basically? So convolution filters shear flows which has a scale below epsilon. So convolution is an average, not something which has Not something which has a size epsilon. If at some point in my evolution the shear flow has a scale below epsilon, that's killed. More or less, small arrows, but it's filtered. So which means the following. So you create a sort of a, I don't know, a barrier here, you know, in such a way that you have your evolution with your refining of the scales and in a certain time. Until a certain time. And then, so for a given epsilon, you block your evolution here because the shear flows after the time are finer than epsilon. You filter them, and then your solution theta epsilon does not get mixed at time is equal to 1. It's possible. It rather stays fixed at a scale, which again is of the order of epsilon. It goes through this singular time, and here the solution. Time, and here the solution finds again a velocity field which wants to reconstruct large scales. So basically, you're doing nothing, so you're doing the evolution without any change, then your solution does not change in this interval of time, and then you reconstruct the large scales. So you just go through the singularity. But then you need a tilt, so why multiple limit solutions? So for t greater than one, T greater than one, okay, so you don't just do the evolution as I was describing before, so you introduce a new move, a peach step in which you reconstruct the large scales, which we call the swap. So the swap does the following. So it just swaps the black and the white tiles in your chestboard solution. So here at each step you refine the scale, and here at each step you make a coarser scale. Make coarser scale and swap them. Coarser scale and swap them and so on. So then what happens at the end of the day is the following. So depending on the parity of epsilon. Just to make sure I understand, you're always describing the V right now, the U right now. I am describing what the U is doing to the data. What the U is doing to the data. Correct. So the swapper, so the swapper is another share flow which does this to the data. So that's a picture of the swap. So depending on the parity of epsilon, so depending on the fact that here you capture an odd or an even number of steps, and correspondingly after the singularity, so in the limit you will select the solution which is odd or even. So you will converge to a solution which underwent an odd or even amount of swaps, and therefore you will converge to two different And therefore, you will converge to two different solutions. The basic principle for the lack of selection under convergence. So there were previous results, so one by myself with Stefano Spirit and Gio Nelo Champa, and another paper by Camingo Delellis and Victor Miri on this topic, but in our case we do not do velocity fusing short. Not two velocity fields which were at infinity, so you have the velocities in L infinity. And in both cases, so the regularization of the velocity field was not under convolution. Both papers were concerned with the question whether some regularization will select the unique solution. They were disproven the statement by constructing a dog and very anisotropic regularization. But not canonical ones. But not canonical ones convolutions. So now there is a slide with kind of parallel. So to which extent can you apply the same line of thinking to the case in which the regularization is not by convolution and rather by make a parallel? So the convolution axon development. So the convolution acts on the velocity field and it has a sort of range of action which is epsilon, while as I was describing by equilibrating motion, so the diffusivity has more trajectories and the range depends on product diffusivity times the time on which it does. So if we have diffusivity for the time to, then the action then the action, so the effect is proportional to the square. In particular, if I decide in some interval of time that I have no velocity, my velocity is prescribed to be equal to zero, because I want to do that some stage, then so convolution is doing nothing, but the um diffusivity is doing something, so everything is convoluted. Everything is convoluted with the material. So, for our effects, so that's something which, if you take that literally is quite wrong, but you know, for the way we use it in our proof, so convolution has infinite propagation speed. So, at each time I modify, so if I have a, if I look at the velocity at a certain point, it gets average, right? So, the velocity at a point. So the velocity at the point is also influenced by the diagnosis velocity at my point. While in a certain sense, if you just consider 99% of the trajectories of the Brownian motion, they will be bounded. And then in a certain sense, with that probability, you have a solo at finite provocations. It's literally wrong, but it's a principle, it's an empirical principle. And well this works in the following way. So in order to prove the no selection, we work under diffusivity, we work in the following way. So the advection again refines the chessboard at this step. So for time between 0 and 1, it's exactly the same velocity as it was before. So we refine the chessboard. So we made one. So we made one modification. So in each stage we have a large interval in which the velocity is equal to zero. We have just a pure heat equation, only diffusion. I call it the heat equation stage. Then, so since we had separation of scales, so since super exponential, so we find the critical time depending on the diffusivity, so that converges. So that converges to 1 when the feasibility converges to 0. As before, I am pushing, in a sense, my criticality to equal to 1. So that basically there are two scenarios. So before that critical time, diffusion is just a small perturbation. But, so if you look at the heat equation stage for T around this critical time, diffusion is the dominant. And then the following happens. And then the following happens. So, I mean, there you are expected to fuse a lot, okay, so to trash a lot of your solution, in that sense. But of course, one difficulty is that, well, this heat equation stage is very short in time, because we are squeezing everything around time equal to one. And then we have this kappa times tau phase, so it should become smaller. But we prepared our solution appropriately in the following sense. In the following sense. So the solution has very high frequency when we are at a certain time very close to one, which you need to quantify because we had a movie, you know, we had a cartoon for the case without this positive, so without diffusility. And here you have to, you know, check that here also that you have to have the solution. And then what happens is that, well, you have enough diffusion. So you prepare the solution. Diffusion. So you prepare the solution with a very high frequency, then you have small diffusivity, but they combine. Then you still get a dissipation or a fixed amount of your normal. So enhance diffusion again in a cartoon example is the following property. So if you have a solution with a space frequency one, then you have a decay at this time to the minus T. I got the key equation. But if you increase the frequency of your solution, you get a much better Solution, you get a much better principle in two baby formulas for enough distribution. So, in a sense, the small time is balanced by high frequency of a solution, and then what we get is a dissipation of a fixed amount of conditions. That's the kind of guideline inside our work. And what I completely hid to this presentation are the following So there are technical things, so I didn't describe at all the lack of selection under vanishing inclusivity. So what I described, of course, is the convergence with the symbolic solution. The point is there is another sub-sequence of infusibilities such that the corresponding solutions will converge to a conservative solution. Solution, this is the lack of solution in the unit. And this is used, this is due to the isotope, stochastic analysis. I hit completely the space regularity, so how you make the velocity field and the solution regular and the corresponding time integrativity. The regularity of the solution you want it uniformly in viscosity. You want it uniformly in viscosity, and that's based on the regularity of the stochastic flow. Yeah? And then, okay, there is this issue which you may like or not. So this final loss of the flow happens only at the final time, meaning the following. Every time you fix a small delta, and then you look at the evolution between time 0 and 1 minus delta, for small diffusivity, For small diffusivity, everything here is completely regular. So every time we look at dissipation, you know, one minus delta, a two norm of this type of Kappa for two. Yes, this goes to zero, as kappa goes to zero for a delta positive. In this sense, you have all the dissipation of the final time. Well, which you might be Well, which you might give us non-physical, so trouble lensing a sense should be isotropic, so you should see the same thing at each time. So in another paper, so Massimo Sorelda with another student of Maria, so Carla Johansson, they prove some continuous in-time form of the dissipation, but at the cost of making the concentrated space and are getting the same type of space. Are they in the same type of space? Of course, you know, the big question we had around this intermittency, so how many corrections in the form of automation C, and all this mechanism. And there are also now recently results on the dimensional dissipation set by the BOSR bus and pressing today. So of course we were not the first one to tackle this question, so To tackle this question, so Calvin and collaborators. So give us indeed talk and jet lagged. So, what they could do, and that's where we started, you know, posing ourselves this question. posing ourselves this question the case of a balmy solution. So of the regulatory solution. For recently there have been a lot of very nice progress on this question, so very nice papers by Scotanus, Proke and Praticol, in which they could prove, again, construct examples of velocity fields by fractal homogenization that would get you anomalous. Would get you anomalous continuously in time. So, on every sub-interval of time at the two-normal impassive scale, it decreases. This is, you know, it's a very interesting approach. It's also much more complicated than ours. Another recent paper by Uisman and TT, so they showed that in the vanishing dispositive unit, again for the question, the energy can also increase and not just. can also increase it and not just decrease it. And a few days ago, maybe a couple of weeks ago, there was another nice preference by Tara and Windy and they could cook up an example of basal autonomic shears or even very expensive, but they could also cook an example in which you have a universal Okay, so I believe we have just a couple of minutes, so I will just say what is the principle of our application to be forced in Aristotle's equation. In that simple aberration again, we have maximum, but also we can do it here, so we join forced, building on the previous paper by meaning. So, what we construct is also a two and a half dimensional solution of Navier-Stop's fodder equation. In the following sense, so you solve a 2D equation not involving theta, and theta is advertising and diffusible. So the 3D solution of Maybe Stock Sauler takes this form, V equal to U theta absolute plus V, but in a certain sense the third component is only a vector. So that's why this is called is called map tune for that. Okay, so what we do in order to construct this solution is we get new data from the previous paper and we notice a few things. First, since we have shear flows, this vanishing is not in a problem. Then for the pressure, then we have a solution to many stokes that will be forcing, provided that we define the force as Define the force as the amount by which the loss in the wing already constructed does not solve. It's like a perfect trick, but I have to put that in my hand at the moment. So then what we can prove is by again setting V, what we do there, and by considering this 3D force. So our 2D force coming from this error in the first component. So we construct an example of anomalous dissipation and lack of selection for the two and a half dimensional dissipation with 54 C. And this we can do in the full on-site supercritical range. Remember that from the previous paper we have this altoplastic data, equal to one, that is the critical one. This reflects in the criticality for this concept of work in So, of course, you know, this might seem a trick, the fact of introducing a forcing. For certain MS90T is a trick, but there are some haves. The dissipation is in theta. Is that correct? The dissipation is in theta. The dissipation is not equal. That's not new. So the first remark is we can arrange f. First remark is we can arrange F mu and the solutions, so the force to be smooth for every mu, so the force is also dependent, and V mu to be the unique smooth solution, mandatory, for every fixed or unique calculation. Of course, that's the criticism, you can always say that the velocity fixes of whatever else you want, so if it would. But the point is that we retain some physical meaning, so we have. Meaning. So we have bounds on forcing, which are uniform in viscosity, in some spaces which prevent linear boundaries. So this means the following. So if we only had bounds in a infinity on the force, on the forcing, we could have a number of dissipation just for the integration. Nonlinearity would not play any role. But in fact, so with our construction, we are So, with our construction, we have better balance, so we have a little bit of regularity on the forcing, which implies that we are in a setup in which the heat equation does not have anomalous dissipation, implying that the anomalous dissipation really comes from the gains of the additional result with the same bit, provision's lack of selection under mentioned is positive. Of course, here while the Of course, here, well, there are many open questions. So, can we remove the forcing is to look up an example in which the forcing is not physically independent. Or something which to me is a very appealing question, which is still super open, and so what happens in the 2D case? So, is it possible that under vorticity bounds maybe? Maybe you have selection other conditions quite different but also better. That contrast, that last part, the last part seems very similar to like what Sam Punch and Smith and Alex Sam Punch and Smith and Alex and Jacob Jossi, right? I mean in the sense that they are looking at a transport equation forced by Nader-Stokes. There's stochastic forcing in Nager-Stokes, which is also rough in the same sense that it can reproduce things like which, right? But that's stochastic. That's right. So that's so I just have a letter that it's stochastic. But um I'm just assuming the opposite. But uh Methodated the opposite. But more than just saying one stochastic, I would guess. I mean, in their case, they could prove it for every initial data, right? In this, this is a year. This depends on the initial data or? In our case, yeah, it depends on the initial data. So we really produce one example, so one gross reference and one initial data don't matter. So it's very possible that if now you take uh so direct example, So directs example and then put them, you know, I think they make a remarkable paper on that. So probably get four components we prescribe that, but we're in the first component of the velocity. Right, but I mean they're one also, they prove a whole family. They construct a whole family of deterministic. No, sorry. They only prove enhanced dissipation. They do not get a long dissipation. They don't get longest. They only get enhanced. Okay, that's how I was trying to remember. That's right. Thank you for watching. This guy against that. Right, right. Gonna get stuck. Right, right. So they only get one. But I thought they get in the classic scalar, they got anonymous dissipation. I think there is anomalous dissipation. I think they're wrong. 2023. The 2021 paper is only enhanced. It's not anonymous. If there was a more recent one, I don't know about it. It's like a couple of weeks ago, a few weeks ago. So then I don't know. Sorry. No, no, no. He's talking about Jacob's paper. He's talking about Jacob's paper. You're talking about Thomas. He's talking about Jacob's paper. You're talking about Tarak's paper. Uh I'm talking about Tarek's paper now. Jacob's paper from 2021 or 2022, that only has a nouns, doesn't have a nonprofit. They prove, okay, we can talk about this. They prove the convo, and they prove the batch of spectrum, which relies on anomalous dispatch batch, right? I missed something too. I missed something, so this is only for 3D? You mean not for 2D? I'm sorry, because you say that the 2D, but for case is open. 2D case is open, so we are not able to. So the underlying transport equation is 2D, but when you move to Euler Megastox, it becomes twenty-half dimensional because the dissipation is in the third component. So not able to do the same But not able to do the same uh in non-linear level too. What regularity is U for this example of yours? Uh C one third minus. So in all the range below, below the super side L1 in time with C13 in space? Or C13rd minus? Probably we also have some additional. Uh probably we also have some additional integrity in China so that I guess every China integral speed up here. So let's have a break. So we'll have a break. We'll come back and start it.