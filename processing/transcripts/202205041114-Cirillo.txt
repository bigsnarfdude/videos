He is one of the most prestigious grants in Spain. His area of expertise is data analysis and predictive modelling for precision medicine using network biology and national. His research in precision medicine includes rare diseases and anti-active cancers as well as ethics and affinity experience. Justo the viola panel was moderated by him. Today we expect a missing talk on explainable artificial intelligence, applications. Intelligence applications for where it is. Okay, thank you so much, Carlos, for the presentation. Okay, let's start. So, this talk is about interpretability in rare diseases. And before jumping onto this topic, I would like to just advertise a little bit the Passover Cooking Center just because. Computing Center, just because before we were talking about computational resources and computational efficiency, so this is just to let you know that actually you can apply for computational hours to those two programmes. One is a European programme called PRAICE, and you can ask for many, many hours of computation in the supercomputer. The other one is national, it's a Spanish programme. It's a Spanish program, less computational hours, but still the possibility of applying. Like, if you have a project and you want to use the Marinostrum, the supercomputer, you can for free send this small proposal. It's very easy to get this. So it's just to let you know that there is this possibility. So okay, yes, we have this Mario Smoke 4 and and this is not word. And this is not working, maybe. Okay, and we are installing the myostom 5, which is even more powerful. And so, I mean, yes, just invite you to check out RAISE and RAS. Okay, so rare diseases, specifically pediatric cancers. And this refers, like all the examples that I'm going to show you, refers to results of a European project coordinated by Maria called IPC Individualized Pediatric Cure, which is a very big. Which is a very big consortium. It consists of 21 partners from three different continents. And the objectives of this European project are to create a cloud-based platform to share standardized and organized data on pediatric cancers, create predictive models that we eventually verify through clinical trials and preclinical models, and also to identify some personalized treatment recommendations. Some personalized treatment recommendations for those children. Okay, so pediatric cancers, pediatric tumors are part of the big family of rare diseases, which in Europe are defined as those conditions with an incidence of one in 2,000 people. So as you can see, eighty percent of the of those rare diseases have a genetic um cause uh and Cause and indeed half of them manifest in children. But it's quite worrying that ninety percent of those rare diseases still lack treatment. So there is really a need for investigating more in rare diseases. In general, it has been like 7,000 rare diseases have been described, and even if each condition is rare, together they are really like they are really like posing big challenges in terms also of medical cost, consider that in the United States there are 30 million people affected with rare disease. Okay, now machine learning for rare diseases. This is really funny because if you go to the web page of PsychicLearn, you find this quite silly diagram that's basically explaining you what kind of decisions you have to make. Of decisions you have to make when you want to apply some machine learning algorithm. And as you can see, rare disease researcher in rare diseases are kick off immediately, like not even entering this. So if you have less than 50 samples, you basically cannot apply machine learning, which of course makes sense, is totally reasonable. You have a lot of problems with generalization and overfitting. I mean, it totally makes sense. Fitting. I mean, it totally makes sense, but we have tend to find like solutions. And one of the most intuitive solutions when you work with small data is basically to take this small data and put it in a bigger context so that you can basically increase the information that you can get about the data you are working with. And this is the main idea that let us work with a particular framework, a graph-based framework that. A graph-based framework that is called multilayer networks. So, a multilayer network is basically a network of networks which are called layers. And those networks are characterized, like all those layers are independent networks, which are characterized by nodes connected with intra-layer edges. And on top of this, those nodes are also connected through the different. Connected through the different layers by interlayer edges. The way you build a multi-layer network really depends on what you want to study. So the easiest way, for instance, of creating those interlayer edges is to connect entities that are exactly the same. So for instance, imagine that the first layer represents gene co-expression and the second layer represents gene mediation. The nodes will be gene. The nodes will be genes. So, if one gene is exactly the same and it's present in those two layers, you can connect with an interlayer edge, which will represent the identity of this entity. But again, if you have other reasons to connect other nodes in other ways, you can always do it. It's like trade-off beauty of this type of construct that you can really flexible depending on your hypothesis and your questions. So, if you do this again. And your questions. So, if you do this exercise for all the other nodes, you end up with an object like this, and this is a multi-layer network. Now, as you can see, some nodes will only present in some layers, and this is actually really good because essentially, by doing this, by integrating all this information, you are not excluding anything, like you are still connecting this information with the rest through the neighbors, essentially, or other nodes. Nodes. So, well, from a formal point of view, you can describe a multilayer network with a quadruplet in which you have the set of layers that compose your multilayer network, the set of nodes, and then like a matrix that associates a node to a layer, and then a matrix that associated a pair of nodes and layers to another pair of nodes and layers. Like, for instance, here, node V1 in layer 1 is connected. In layer one is connected with node V1 in layer two. It's basically this connection here. Okay, so the way you build a multi-layer network, so essentially like those layers, where do you take those layers? It really depends. There are two different ways. So either you have data, maybe in your lab, or you have omics information, like here it's mRNA expression on the DNA network. On the DNA net healation, and first of all, you have to convert those tables into a network representation. This operation of going from omics information to a network representation is called network inference, and it's something that is absolutely not trivial. Indeed, there are many, many, many ways of doing this. But once you have your relational representation of the ohmic information, you can basically stack all those layer one. Basically, stack all those layers one on top of the other, create your multi-layer network, and then do some kind of network analysis on top of this. An example of a method for network inference is COSIFER that has been implemented by, developed by IBM in Maria's lab. And basically, this works in this way. You have this expression data matrix, for instance. COSIFAR applies a series of metrics for finding similarities between For finding similarities between those genes, and then it finds a consensus, and this is going to be like the network representation that is the leader, and that you can use as a layer in your multi-layer network. So, this, if you have omics data, you want a multilayer network representing this omic data. Another way of doing this, which is the one that I use most, is to take this network information from public databases. Public databases, so external information that you can retrieve from databases that contain biological information that you might be interested in studying, in using. Why I'm doing this? Because there is much more information here. So remember the previous slide, we have small data, we want to put this data in a wider context of biomedical knowledge. Also, another advantage is that Also, another advantage is that many times those databases already report the information in a relational way. So, it's very straightforward to just take this information and have it represented as a network. And then the exercise is the same. You stack the networks and you do some kind of analysis of the format. Another way of building those layers is to create networks that contain information that you extracted from an From unstructured data, from texts. And so, again, in collaboration with IBM, we created a pipeline that is essentially able to identify mentions of bioentities like genes, drugs, from texts and represent this using word embeddings in a network in the form of a network. And so, again, you can then take this network and use it as an additional layer in your multi-layered network. In this case, it's actually. Multi-layer network. This case is actually a pediatric liver tumor that is called hepatoplastoma. It's very rare. Okay, something that we introduced with a publication last year is a way of analyzing this multilayer network in order to extract functional information. And it's this new concept that we introduced, which is called multilayer community trajectories, that seems to work very well for studying rare diseases as I go. Studying rare diseases, as I hope that we're going to illustrate now. So, basically, we have this multi-layer network that we constructed with those databases. Now, something that you can do with this is to find communities. This is something very typical in network analysis. A community is defined as a region of the network in which you have a higher density of connections compared to the rest of the network. Of the network. So, to find a community, it's basically an optimization problem in which you want to maximize a property Q that is called modularity. This is just the name of having a lot of connections, of the number of connections that you find among those nodes. So, you want to maximize the modularity in a specific fraction of a network X. If you have many layers G, what you want to do is to Layers G, what you want to do is to maximize this modularity at the same time in all the layers. So basically, a multilayer community is not like a region of a network, it's like an entire region of all this object in which you have a higher density of connections. As you can see, this modularity is parameterized by gamma. This is called the modularity resolution, which basically is a parameter that governs the Governs the size of the communities that you are going to find. And actually, the relation between the community size and the number of communities while you are increasing the resolution is of this kind. So, essentially, it means that if you start at resolution zero, you are considering the entire multi-layer network as a big community. Then, if you increase this resolution, you are finding more communities. And then if you increase More communities. And then, if you increase, you find more communities, and so on and so forth. All those resolution, all the community structures that you are finding at different levels of resolution, they are all valid. They all exist, coexist in the multilayer network. There is not one that is better than the other. Those are just like when you look into a microscope and you adjust the resolution of the lens. It's basically what Of the lens is basically what we are doing here, but looking inside this multi-layer network structure. Now, some properties that we observed, and that is very interesting, is that some nodes tend to be always in the same communities no matter the value of the modularity resolution. And thinking that those communities come from a multi-layer network in which each layer represents different aspects of biology, the fact that those Biology, the fact that those nodes, imagine if those nodes are genes, always stay together, these are a very strong evidence of the relatedness, some functional relatedness between them. But then, of course, like, I mean, some genes always stay together, then there are other cases in which you have a gene that stays together with others until a certain point, then it escapes and then it goes back. So each gene will have its own trajectory through this journey. Through this journey throughout the different communities. And so we said, okay, let's compute all those trajectories. And this is basically the method that we are proposing here for finding multi-layer community trajectories. So the way it works is the following. We are screening this modularity resolution and we are finding different community structures. So the first step is to label all those communities. Then what you do is to compare pairwise all the nodes, for instance in All the nodes, for instance, in this case, those are genes. Okay, here there is an image that is not showing, but basically you are finding distances, parallel distances between all those nodes. So you end up with a distance matrix that you can represent with a demogram like this, which is really nice because you can see, for instance, that I don't know, gene two and gene three at the end of this journey, they always stay together until the end. And indeed, they are here, no. Indeed, they are here, no. But gene 1, it escapes at a certain point, and indeed it's here. Now, if you apply this to a real multilayer network, the type of dendrogram that you obtain is something much bigger, but it's also a really interesting object. And you can do a lot of things. Once you have this, you can, for instance, look for the distance of the genes in the tree, the composition of the different communities at each level of resolution, and so on and so forth. And so, we apply And so, we apply this approach for studying a pediatric tumor that is called medulloplastoma. Medulloplastoma is known to be classified in four subtypes. The name of those subtypes comes from the pathways that are known to be altered in the different subgroups. Group 3 and group 4 are less, let's say, it's Less let's say it's less clear the molecular characterization. So even if, for instance, group three is actually the one that has the poorest prognosis, so it's the most aggressive. So four subgroups are known for metal oblast. And what we did was to what we did was to basically use a Use proteogenomic data, so multi-omics data of Medaroblastoma and see which are for all those omics information the genes that are altered and that characterize the four subgroups. It end up that if you want to characterize the four subgroups using all this onics information, you have a set of 40,000 genes. So for each person, like you will For each person, like you will have a union of 14,000 genes that are altered for some reason. So some are overexpressed, others have alteration in manipulation, others are some phosphoriomics characteristics. So you can define those four groups using this proteogomic data using those 14,000 genes, which is actually like a lot of genes. It's not really very manageable. Manageable to not really informative, no? So the objective here was basically to find the minimum number of altered genes based on this ONIX information that was able to recover these four subgroups. So it's basically a dimensionality deduction approach for which we want also the interpretability, the functional interpretation, and this is why we Interpretation, and this is why we use our multilayer network approach. So, first of all, we created a multi-layer network of five layers using different databases, as I said before. And we created our demogram of all those communities that are present in this multi-layer network. And then we use this object, we project those 14,000 genes onto this object, and through an optimization process that now will show you how it works. Will show you how it works. We found the minimal set of genes that were still able to recover the four known subtypes. So in terms of reconstruction accuracy, we had a very high reconstruction accuracy with a very high also dimensionality reduction. We go from 14,000 to 1800, which is something a bit a bit gathering. Okay, so how did we do that? How did we do that on this dendrogram? So, essentially, if you have a dendogram like this one, and you have all those genes, and then all the genes that are present in the multi-layer network, some of them will be the alternate genes that you identified. So, those are the ones. Now, those in red are those 14,000 genes that you identified. Now, we have two parameters: theta. Parameters, theta and lambda. Theta is the distance in the tree, and lambda is the maximum number of alternate genes that you allow in a community. So let me explain. For instance, if we have theta equals one and lambda equals two, this means that the distance in the tree is one, so you are up here, and this corresponds to two communities, one composed of those four genes. They split after that, you see, but we are. After that, you see, but we are here analysing theta was one, so we are at a certain level of the three. No, so they are the four of them are together, the others are all together already. And then you say, okay, now I want like a maximum number of two genes in my community. This is the maximum that I can allow. Consider that the objective here is to reduce the number of genes. So, in this case, of the two communities of all those altered genes that I can genes that I want to keep the ones that correspond to theta equals 1, lambda equals 2 are those ones, G1 and G4. So if you do this for all the combination of theta and lambda, you end up with something like this. So on the left you can see all the combinations of those two parameters and the one that is giving the highest reconstruction accuracy, which is zero, which means that the genes that you are retrieving are actually. The genes that you are retrieving are actually the ones that stay always together, which is actually interesting, per se. And six, which is the maximum number of genes in one community that you are allowed. So you can have six, five, fourth, and so on. And yeah, this is a bit like the way we optimize this reduction in the number of genes. And then we also tested how robust is this. The robust is this combination, this optimal combination of parameters. We shuffle the labels of the patients and we obtained basically a random accuracy compared to the original one, which is up here in 1995. And then we also did an interesting test, which we call it sequential exclusion test. Basically, so our one So, our 1800 genes that we identified, what we did is to take those genes and remove it from the analysis and repeat everything from the start. And so, we identified like a new set of theta lambda parameters and a new set of genes that have a high accuracy. And then we take those genes and then we remove it and start again. And as you can see, the more sets of genes that we remove, Genes that we remove, the less good we are in reconstructing those four subtypes. So, which really means that the genes that we found in the first place were actually the ones, no, well at least the, I mean, yes, the first ones, are the ones that characterize best this subgrouping. Okay, so using those one thousand eight hundred genes that we 1800 genes that we identified, you can cluster those patients and you can see that actually like where this 95% accuracy comes from. So the boxes represent the real labels, so the real clusters where those patients are. And so you can see that we basically are able to reconstruct those clusters very well. The only mistake is that Let's say mistake is about those three patients that belong to a cluster by themselves that we call G3G4. But those by inspecting the multiomics data of those three patients, they have actually missing information compared to all the other patients, which means that we have a set of mutiomics experiments and those three patients, some of them they don't have them, so they have missing information. Have that, so they have missing information, and so we could use less information, and they seem to cluster all together. So, the approach is actually like able to spot something that is different from the rest, which is quite interesting. Okay, so why, you know, this is a dimensional high reduction approach. Why bothering with all those networks and with all those things? Like, where is the interpretability gain of using a multi-layer network? Well, the interpretability gain is that when you finish and you have like You finish and you have those genes, you can go back to the multi-layer network and analyze layer by layer what those genes are doing, like in all those layers. So, we computed enrichments for those genes in the different layers. For instance, here you have the pathways in which those are involved in the different subgroups. And you can see that the ones that we found to be enriched are actually also supported by lead. Supported by literature about this particular paniatric cancer. We did this for all the layers, of course, and now we have a set, not only a set of genes, but also a set of explanations why those genes are important in the characterization of these patient subgroups. We apply exactly the same approach to other rare diseases, like for instance, this is the case of a congenital Nastenic syndrome, this is a group of rare diseases that are all Rare diseases that are all caused by a mutation in a receptor for acetylcholine. This is amiasidinia, so it's a problem with the neuromuscular function. And the characteristic of this cohort is that all those individuals all bear the same mutation, but they differ in the manifestation, in the phenotype. So there are individuals that are So, there are individuals that are more severe and individuals that are less severe. And if you analyze the demographic information of those patients, the clinical information, even if you analyze the omics information, so whole genome sequences and RNA-seq alone, you cannot distinguish between severe and non-severe cases. We tried everything. It was no way. And so it's where we thought: okay, maybe we can discriminate. Okay, maybe we can discriminate those looking at more functional, let's say, implications. And this is exactly the analysis, the multilayer analysis that we are proposing. And indeed, we could identify some communities that are specific of the severe cases and other communities that are specific of the non-severe cases. An interesting thing. Well, this is how we basically did this. So you have some altered genes, those ones in purple. Those ones in purple, and you are screening all these resolution parameters, and you're finding a module that contains all the genes that you know are altered in those patients. So, it's just a more graphical visualization of the process. Now, the real interesting thing is that this module is, for instance, an example of a community of genes that is specific of the severe cases, but each individual in But each individual, each severe individual, will have personalized mutations. Like, for instance, this individual here has a mutation in a gene called Agrin that is mediating the clustering of this receptor for acetylcoid. While this other individual here has a problem in other genes that are implicated in functions of the synaptic plat. So, as you can see, like looking for functional modules. Functional modules. It's also interesting for a personalized medicine point of view because they are all severe cases. They have mutations in different genes, but all those genes are implicated in the same functions, so that are the ones that are affected in this metastatic symbol. So we found also a module for the non-severe cases. We characterized everything, and we even We even validated experimentally using a Zebba fish. This is a model organism that is used for this kind of studies. We validated experimentally that one of our candidate genes that was not annotated to be implicated in neuromuscular function seems to be actually implicated. Like you can see here, when we mutate this gene, for instance, we have a decrease in the muscle fibre. instance we have a decrease in the muscle phyloticness, we have a decrease in the number of clusters of receptors that are clubbed together and a series of other evidences. So it's a really nice approach. Okay, so if I have time first, yeah, it should be started. Okay, as you want to. Okay, as you want. I have a couple of slides of synthetic derivation, otherwise myself. And this is actually an application that we implemented in the lab and applied actually to Megaloplastoma. You can see here again those four subtypes. For some types. So, a variational encoder is basically like encoding the real data that, in our case, is transcriptomic data. It's microarray, but the same can be done for RNA-seq or even other type of data. You learn basically to reconstruct the original data, and while learning this, you are also learning a latent space of your data. And the trick is that you can use this latent space. Is that you can use this Latin space in order to sample instances close to the ones that are real, so that those represent basically synthetic data. And with some, let's say, metrics of reliability, you can basically increase the volume of data by generating those synthetic expression profiles in our case. Now, we identify. Now we identifi I will show you in the next slide how we did that, but we identified some important genes that actually have different signatures in the four subtypes. And this is interesting because we identified those genes by taking advantage of this process of synthetic data generation. And this is where the interpretability comes into play. We also use the ability of generating synthetic instances in Generating synthetic instances in order to study things that before were not possible to study because of the small sample size. So, here, like in this humat, you have all the training data, so each one of those points is a patient. And in the literature, as you can see, there is an overlap between the group G3 and G4. This is what I said before. Now, G3 and G4 are less characterized, it's not very clear. And in the literature, a possible new subgroup. A possible new subgroup in between the two was postulated, but never really demonstrated. So now that we can increase the number of cases and specifically in that region, and this is what we did, we could actually study those cases, like the molecular signatures, the molecular characteristics of that region of the lattice phase. And this is what we did. And we found out that actually there are points there that have a very specific signature. Very specific signature that is related to synaptic signaling, nervous system, things that make sense in the context of the disease. But it's interesting because it's something that was not possible to do before, and we are exploiting the synthetic data generation in order to study something very peculiar. So, before I said that, we identified some important genes by using interplickability, and this is exactly what we did with these sets. So, we have Did with these sets. So we have our variational encoder, and in order to explain those latent variables, we have two steps. So, first of all, we train an XGBoost to classify the four subtypes using the latent variables. Once this classifier works well, it's trained, we can explain this using the Shattree explainer. So we can find which are the latent variables that are important in the definition of the force of types. And now there is the The four subtypes. And now there is the problem of interpreting those latent variables. Like we want to map those latent variables to the genes. But we can do that because we train the encoder. And so we apply this sharp deep lift to the encoder and found what are the important genes. And so essentially at the end, we identified 16 latent variables that map to a set of 8,000 and something genes. So this is basically how we proceed. And as I showed you before, those As I show you before, those 800 genes have different profiles of expression in the four subgroups, which is quite interesting. Okay, of course, you can apply Toto's concept not only to rare diseases, but to whatever situation in which you have small data. Like, for instance, if you take TCGA and you start disaggregating by ethnicity, for instance, you will end up with. Ethnicity, for instance, you will end up with very small data. So, always remember that rare disease is an example, but whenever you apply precision medicine, you always have the problem of the small data. So, this is just some example here. This is another database of cancer. Again, if you disaggregate by ethnicity again, like in this case, you end up with very few data for specific ethnicities, and this is a problem. So, essentially, the take-off message is that we are always talking about big data. We are always talking about big data and learning big models, but small data is something that is always there, and we need methodologies that allow us to study this kind of scenarios. So, well, thank you so much. And I will not go through the conclusion for the sake of time. If you have questions, please let me know. Thanks. Yeah, thanks for the presentation, Jeremy. So I was wondering about generating synced data. Because of course, in a way, it's kind of like machine learning-based bootstrapping. So you're augmenting your data, but it's not exactly the same as getting more data. So can you say more about to what extent you can trust this data and using that to have more data? And using that to have more data. Yes, absolutely. It's really a matter of trust because, and the only thing that we can trust is the data that we have. So, this reliability, like in this work we implemented a score of reliability, which essentially means that if you are generating synthetic data close to one of the subgroups that you know, okay, you can trust this. If you are generating synthetic data, like Generating synthetic data like in the middle of nowhere, no, this is not really reliable. And this is essentially what happens when you have like a rational decoder. You can sample from places that are empty basically in the latent space. And this will give you something. Achimeric, strange image, no, that is a mix of something else, but you can always do that. So, in order to avoid this, we introduce this reliability. Reliability score, let's say, no, that okay, not all the data space you can trust, like you can trust regions that are somehow close to the original data. So this is the strategy. But in a way, when you have so many patients, you're kind of undersupping the state close to your existing patients or still but but but would you take the risk of uh But would you take the risk of creating a synthetic expression profile that is completely different from anything that you have seen before? That's a big thing. Obviously, you have to deal with the limitations that we have. I was just wondering if there's any certain types of questions that we can and cannot answer in this first one. For sure, yeah, definitely, definitely. Actually, even the sample size, because here, like, all those points are the actual points that we use during training, 700 patients, which is a lot for a rare disease. For rare disease, like we found a study that has so many patients, otherwise, it cannot be done. So, like, yes, the necessity of producing more data in order then to apply this type of approaches. But, yes, not definitely, very things that you can study and cannot study. About the community detection, how do you do that in multi-layer networks? Or maybe you only do it in single-layer networks? Because I was wondering, you have canstar networks with gene expression data, prototype information, or copy net graduation. How do you detect communities such as different types of data? The algorithm is really like maximizing the sum of this modularity at the same time in the entire, in all the layers. So it's like finding a kind of a, like, you know, like a, let's imagine that it's a three-dimensional. Let's imagine that it's a three-dimensional space in which you have more density of connections. Then, what those connections represent is, I mean, it's in the multi-layer network, it's how you construct it, but like algorithmically, it's exactly the same as in one single layer, only at the same time in all the layers. But in that case, I guess that you're putting on the same path ages within the same network. Because, for instance, if you're seeing gene expression data, an age would be correlation in gene expression instrument if an age. With an edge that connects different uh layers, the multilayer, like for instance, gene expression uh information with protein expression of the same gene. Okay, you are considering that all edges carry the same information when you look at this multi-speaking. Yes, the way, and this is why we generally like simplify and just use this identity interlayer edges. So, yes, exactly. The more like the way you connect the different layers will affect the Layers will affect the final results. So, the easiest way is just if you have the same entities, just make this connection through those. Because it's like having one gene, but it's always the same at the same time in all the networks, in all the layers. Which also makes even more sense then to apply this type of modularity, you know, some of modularities, because you will always know that one node is always the same, then so you don't have to. So, you don't have to, but there are multilayer networks with which you have what you described, like you have things that are connected in many different ways. Those are called heterogeneous multilayer networks, and you can have different in that case, in that scenario, probably you need another way of finding communities. Yeah. One last question, and then the rest is just talking with you. So, your idea is you have the genes and those. 