Capture from signature methods in roughware theory and apply them to other objects. In this talk, it's about graphs. So this will be the broad goal. If you have any questions, please interrupt at any time. So let's start familiarly here with iterated integrals. This is the key object I think everybody here understands. We had Joseph talk about. We had Joseph talk about this this morning, and his viewpoint is a dynamic one. This is also Terry's and most people's viewpoint. Yeah, you have a dynamic viewpoint. It's a very fruitful viewpoint, but it's not the only one. In particular, this viewpoint is not so helpful when we want to talk about discrete structures, graphs, for example. So what I will do now is think about it differently and strip away as much Strip away as much as I can so that I can apply these ideas to graphs in particular. So the first thing I want to strip away here is analysis. So I have here an integral and I have derivative. Let's drop both of these times, things. I should switch to the Let's drop both of these things. So we make the integral a sum. So the integral was over the simplex as usual. I still sum over simplex. Okay, so yeah, so I make the integral a sum. So I don't need integration anymore. And then I have to do something with the derivatives. I just take With the derivatives and just take discrete derivatives. We saw this in Java's talk today, also. So, sums instead of integrals, increments instead of derivatives. And then we have something that we call iterated sums, introduced like two years ago, two, three years ago. And one immediate advantage of this is and I think Saba touched on it a little bit, is that you have immediately a whole lot more features. So A whole lot more features. So, actual data is always time-discrete. You never have continuous data, except when you work with analog devices. But let's say we're on discrete computers, digital computers. So, you always have discrete time. And so, it doesn't hurt to work with discrete objects. We have to do it anyway. And if you think about it in terms of iterated sums, you get more features. So, in particular, for one-dimensional paths, where we know that the signature is Where we know that the signature is trivial, it's just the increment to some power on every level. There's no information. That's why we always have to do this lift with an additional time or something like this. Here you don't have to do this, you get it for free, you get a lot more features that describe the signal. So this is just a short advertisement for iterated sums instead of iterated integrals. Okay, so let's strip away more here. So, first of all, I want to get rid now of the increments because Because for several reasons. One of them is I want to work sometimes in semi-rings. And in semi-rings, I cannot take the differences. There's no minus in a semi-ring. And so I want to get rid of this. So let's drop it. Is there a way to some pointer here to click forward or no? I mean, try it. Ah, straight. Ah, spacebar. So I just drop it, I called it Z. And if you want, you can come along, take your path, take your increments, and call this Z and plug it in. But you don't have to do this. So I just have now iterated sums over some Zs, no increments anymore. Iterated sum. And the advantage now is I can do this over any semi-ring. This is again, that's why it's in italic. It's just a short advertisement for this. This is not the Short advertisement for this. This is not the main focus of the talk. You can now do iterated sums over semi-rings, which for example allows me to the last expression on the slide. Yeah, the last expression on the slide is in the max plus semi-ring. So the plus is now maximum, and the times is a plus, it's max plus. Is a plus, it's max plus seminaring. Obviously, I cannot invert the maximum, so I have no minus sign here. And if I then calculate iterated sums, I get expressions like these, which even for a real value time series now give me a bunch of new features, which might be particularly interesting, I think, maybe for this stopping problem, right? Because these running maxima of the path are quite hard to detect in an iterated integral signature. So maybe there's some. So, maybe there's some application there. Okay, that was just a short advertisement for why we switch here to just sums of things. So, Z, it's now just data on my real line, a discrete real line, so on M. Okay, I didn't do anything, but is there any question so far? This is just somehow trying to meet huh? How did you get to the max? Sorry, I just did. How did you get to the max? Sorry, I just did. Oh, if you choose a specific type of product. Yes, so now I have instead of the field, for example, R or something, I now have a semi-ring S, and I pick now the semi-ring max plus. Luckily, I can use the same base set, which is just the real numbers. I just impose a new algebraic structure. And then I get a whole bunch of new features for free. Yeah. That's the idea. There was another hack going up. No. So data is often not just indexed by the integers. Oftentimes it's placed on more complex structures. So here this is pictures from Mario Schankin in Greiswald. This is bioinformatics and there you often have data on poset-like structures. So for example in the top left corner here, In the top left corner here, you see a possible sequencing where, at some places in the sequencing of the DNA, say, you're not 100% sure what the result there was. So you have some uncertainty there. So one way to interpret this is you have really a time series here, then you know for sure a C. There you know for sure a C, then a T, but then I'm not so sure I have several options, several options, and then it goes back to being an honest time series. So you can model this, of course. Sorry? Why is this not? I mean, another way to think about this is as a stochastic process, right? Absolutely, yeah. You could put a distribution on this, yeah. But in this particular case, it's really, you could model this as a Markov giant, say, where the state space is ACGT. The state space is ACGT, but for example, here this is quite uniform. I how I depicted it here. So, this in this case, sometimes it's possible to model it as stochastic process. Yes, I agree. But let's maybe look at this picture here, where you have some genealogy. There's also, of course, a stochastic process to model this, but the data you then have in the end, the realization, is data on a tree in this case. Another example. Another example, maybe this is, I find it a nice example if you have a criminal investigation, yeah? So you have some of the stuff that you find out then when you work the case, you know this happened after this because you have cameras or you have a receipt where the exact date is printed, a time is printed. So you have some form of time series, but some things you don't know when, which happened before which. So you have some answer to D. For which, yeah, so you have some uncertainty. Did this person die first, or did this person fall off the ball first? You don't know. There was no camera around, right? So, this you could also model on the proset data, where these events are not strictly ordered in time anymore. They're incomparable, for example. Okay, this is poset index data. Why do I sorry, I should interrupt you. No, no, it's good. You had the sum with the ordered indices. Yeah. So when you don't know. When you don't know if one thing is before another, yeah. I'm just missing. It's a good question. Hopefully, it's answered on the next slide. So how to extract some features, yeah. Yeah. And why do I talk about posits at all? Because I want to think of what we did here, this iterated sum. I'm coming back to this very simple object of an iterated sum, which was a stripped-down version of an iterated integral. And I want to write this very com very complicated. Complicated to this all. So let's stare at this for a second. I go here over all homomorphisms in the category of partially ordered sets. What are the errors, the morphisms in the category of partially ordered sets? It's increasing maps, or non-decreasing maps, I should say. So order-preserving maps. So I go over all order-preserving maps from the linear order, 1, 2, 3, up to n, into the linear order. Into the linear order 1, 2, 3, up to t. There should be a weak inequality here. Sorry, I switched it. Weak inequality here. So then I have order-preserving maps here from n to t. I pick out exactly these tuples here. That's exactly what this is. So it's just a complicated way of writing this with morphisms. And then using this morphism, I've built this product. And now the good thing is this I can. So this bracket end means 1, 2, 3. Yeah, 1, 2n, yeah. 1, 2, 3, up to n. Linear order. The small linear ordered set. And now it's nice because now I have here form and I have just put a category here. So I can change the category. Change the category. And not only can I change the category, but coming to your question, I can change here this and this, the indexing sets. For example, I could have here the indexing set of the data could be the tree from before. And then I could ask, please find all incomparable points in this tree. So if they are descendants, they're not allowed. What if they're incomparable? I find them. Parable, I find them, I sum over them, and calculate the sum. So you get very exotic sums there. Does this answer the question? Yeah. And now the good thing is I can change the category. So now we're in business. Because now I can do this with cosets. I sketched a bit here. This is a project with Emmanuel Ele. Then we can do this with double cosets. If you don't know what this is, don't worry. If you don't know what this is, don't worry, you've seen an incarnation order already in Deonuts talk yesterday. You can think of Z2 as, of course, a poset. Z2 is a poset, where you say not all points are comparable. It's not a total order. But if you regard Z2 as a poset, you cannot distinguish up and right. You cannot distinguish it. They're the same. So what Leonard yesterday did, he somehow had a concept of left-right and up-down. Left, right, and up, down. And to encode this, you need a bit lock structure. You could do it with the double post. Just saying that what Dionard talked about fits also in this, what I'm talking about here. Then you could do trees, previous slide, and what I'm talking about today is grass. That's what I'm talking about today. But now you have quite a general cooking recipe. Just change the category, yeah. Graphs, yeah. I just quickly put in, because I realized I didn't have pictures of graphs in my talk here, so here's two small pictures. They appear everywhere, as you know. For example, as molecular graphs, there you have quite a graph of modest size, yeah. So extract feature of such a thing is maybe not so weird, you can even draw it and I don't know, reason about it this drawing. About this drawing. But most graphs look like this. So you draw it, okay, I draw on here all the nodes and all the edges. And of course, just by looking at this, you know nothing. So you need some feature extraction methods. And this is. What is this good picture? I don't know. I just googled for gigantic graph. I first thought of just putting a black slide and s saying I Slide and saying I drew a graph here, but then I found this picture, I took this one. Okay, so what's the category? Simple graphs. These are, I have vertices, I have edges in between, no double edges, and I have no loops. This is the category of simple graphs. And the morphisms in this category are just graph homomorphisms. That means if I take this. That means if I take mapping the vertices to the vertices, and if I have an edge, I have to have an edge in the target. So here's an edge, I map this point here, this point here, and luckily there's an edge here as well. So this is a graph homomorphism. And then we can just do the recipe. I take some big graph, this is where. I take some big graph. This is where my data is indexed now, for example, the graph on the previous slide. And let's say I have data on this graph, for example, I don't know, the probability that this person is sick or something like this. This is my data X. And then I extract features here by going over all small graphs, taking all morphisms from the small graph and the big graph, and then just taking the product over the vertex. Product over the vertex labels. This is an iterated sum. Or has the flavor of an iterated sum? You just say that, but somehow if you want to analog of the signature, you need another sum over the small graphs. Yeah, yeah, this would be an infinite sum. Yeah, yeah, I'm getting to this. Yeah, this is one incarnation. There's one coefficient of the signature. Yeah, thank you. What's a graph of momophysics? Can you just explain? Yeah, so graph of momentum. Yeah, so graph homomorphism is so if so f is a map from the vertices of tau to the vertices of lambda and you demand that if f x y is an unordered edge for tau, then you want that fx target f y is an unordered edge for the target. For the target. So, what I could have done, and I think it's on the next or slide after, I could have also mapped, now actually here it is done. They collapse here. You see, this is what's called also the cherry. And somehow it's mapped to an edge, so there's a collapse. Okay. So, this would not work, for example, this would. So this would not work, for example, if there was an edge here. This would not work, because then you would have to have a self-support here, which I don't have. Okay, so the first thing I want to do, I want to simplify even further. Let's say I don't have data, because oftentimes already the indexing structure, so this big lambda, is already interesting. This, of course, does not happen for time series. For time series, the indexing structure is. The indexing structure is 1, 2, 3 up to t. It's very boring. I cannot learn anything about this. So, time series is only interesting when I give you data on the points. Here, already the structure where the data comes on is interesting. So, already doing this is interesting. So, this is crossed out, it's not so visible, this is not crossed out, and I say, okay, I take my data to be constant one. Take it to be constant one. Take it to be constant one, and I calculate this iterated sum here, let's call it sum, and then I realize it just counts the number of homomorphisms. So it just counts how often can I map this cherry inside this big thing here. And this is a feature. It tells me something about the big graph. Okay. Let's do some examples. L let's do some examples. So if you want to compute your sum, you have to have a description of all your computationally this is a very hard problem, yes. And it's only feasible for modest sized indexing graphs. So it's, for example, I don't know exactly this is something we're looking at at the moment, but just to give you a roundabout, so if your graph here, the small graph, has four, I think, Graph has four, I think, or five edges. You can deal with big graphs of the order of 100,000 vertices. This still works. But as soon as the small graphs here get bigger, it becomes infeasible quickly. Okay, yeah, yeah, you can have extra assumptions, for example. Oftentimes you have sparse graphs, for example, oftentimes the big graphs are very fun. Oftentimes, the big graph has very few edges, and there's all this research on doing it fast for special cases, yes. But the problem we have with iterated integrals, that if you go high, you have a lot of terms is a problem here also, but here, even each term to calculate it is quite expensive. So there's an additional computational complexity. Let's just do quick examples here. So we all agree. I dropped the We all agree. I dropped the. I work in the same category from now on in the remaining five minutes. So I will not write it anymore. So, how many homomorphisms are from the vertex here? I can put the vertex anywhere I want. I have three options. Okay. I have 30 total, right? So I have 25 talking. Is this correct? Yeah. Yeah. Okay. I actually have about 10 minutes left with questions. Good questions. Yeah. Okay. Thank you. Yeah. So then the edge here, where can it go? So then the edge here, where can it go? I can go here, here, but I can do this in two ways. I call it morphism. Homomorphism. Is this clear? I can... Yeah, I think it's clear, right? And then, did you use this for erasing the blackboard? No. Ah, there it is. And here this is maybe And here, this is maybe a bit weird, yeah? So, why is there a six? So, I have the cherry, and I want to map the cherry. So, what I can do, I can move this one here, this one here, this one here. This one here, this one here. This is one map. I can flip this down below. I get another one, I get two, and then I get a bunch of these maps here that are not injected anymore. So I just folded together here. Yeah? So this goes here. So somehow here you hit. So somehow here you hit somehow the whole graph and here you only hit the edge. And here you can do this in two fashions. You can just flip this and here you can do this in four fashions. This, this, and then you can flip it. So this gives you the six. So you already see that maybe homomorphism counting is not so intuitive. I will come to this in a second. And then here as an exercise, I put some up, just maybe for time I will not, where we can try. For time, I will not. But we can try to do one. Yeah, maybe let's look at this one. What's the number? What do you think we have here? Six, but well, yeah, you can always write this way. Yeah, yeah, you can always, yeah, thank you. Okay, so of course we saw these coefficients in a big formal series and Series and this is this graph counting signature, what we call graph counting signature. And yeah, so three times the vertex in here, and then you see the coefficients from the previous slide. So what's the other graph when you do that? Um yeah I see I see you pick out this coefficient. This is the small graph. This is the small graph here, yeah. Okay, and so sadly or not, interestingly, this has been studied for decades by Lobach who won the Arnold Prize last year. But he has been studying these kind of slightly different viewpoints, maybe different language, but he has been studying these homomorphism counting objects. It's quite interesting. And it's quite interesting to see. We discovered this, yeah, that he did this already. So, what we maybe the small addition we can provide here is a half-algebraic viewpoint, which simplifies, at least for us, some thinking about this. So, here's the theorem that we get. This signature is a This signature is a character with respect to disjoint union. And we have a very weak chance identity, which only tells me if my big graph is the disjoint union of small two other big graphs, then I can compute it one by one. If I have this graph that I've shown you earlier, which looked very much connected, so I could not decompose it further. I could not decompose it further. This chain of side identity doesn't help me at all. I cannot further decompose the problem. And this is where complexity comes into play. Yeah, unclear at the moment how to do this compatible with the hop algebra structure. So I want to maybe spend two more minutes about the problem I already sketched on: is that this homomorphism counting is not very intuitive because here you have six, which Which I explained. So you have two inclinations of the edge, sorry, four and then two of the cherry. It's not very intuitive. So that's minus, it's a minus point number one, and then there's another minus point. It has infinite support. So graphs are really intrinsically finite objects. So why should the signature I calculate be an infinite object? I definitely need only finite data to describe my graph. Describe my graph. So this should be reflected in my signature. And it's not the case. GC HOM here has infinite support. If I count the h to the power n, for example, inside here, I can put each of these guys either here or here. So I have two choose n, and it's not zero, that's what I'm saying. So I get an infinite. Ah, sorry, it's the wrong way around. Sorry. I wanted to count this guy in this one. Yeah, sorry. It's the other way around. Yeah. So what I wanted to say is. So what I wanted to say is that infinite support, I mean that if I write it here as a formal power series, a formal series, then I get blah blah blah blah blah and then I get two to the power seven h to the power seven two to the power one hundred thirty three h to the power eleven. Edge to control vertex on the three. This never ends. This is what I mean. Yeah. You're sure we have three more minutes. Yes, yes, yes. Thank you. Okay. So let me wrap it up. Yeah. What would be nice, have something that is finite support, have something where I count the cherry inside of the cherry, I get a one. Should be natural, yeah. And I can really say, okay, there's one cherry inside of the cherry. And this can be done. You get a different kind of object. And the interesting part there is that we get a quadrati shuffle. We get a quadrati shuffle. Sorry, this slide, I have to be very fast now. Just for people interested in shuffle objects, yeah, we get a quadratic shuffle there. I have to thank Havald here for the reference because there's a paper that is cited by. There's a paper that is cited by exactly zero people, and somehow Havel found it, and he already mentioned this Krazi Schuffle. So we somehow rediscovered what we already had. So it's an interesting paper for sure. I don't know why it wasn't cited. So you have a nice, more interesting structure there. And what we show with half-algebraic methods, maybe this is the last point I want to make, is. The last point I want to make is that the Hopf algebra helps us to somehow sort out: is this algebra now complicated, interesting algebra? Is this new counting operation, which I haven't described to you yet, but this better counting operation, is this, has this the same expression, is this as expressive as the other one? And you get this somehow for free with top algebraic methods. So this quasi-shuffle algebra is free commutative because it's a hot. Commutative because it's a hop algebra that is bi-commutative. We get translations between the countings, and I cannot go into detail there, but let me just say that the profile drawer really helps to get cheap results. So we have I I mentioned two counting operations. We actually in the paper we have six because you can have different concepts of what a subgraph is. I I've shown you two. What a subgraph is. I've shown you two. Either I say a subgraph is when I can map this graph and the other with a morphism. The other would be if I have an injective morphism. And there's, in fact, six in total that we have in the paper. They are all equivalent. And the Hopf algebras are isomorphic. So this is quite nice. What we're looking at at the moment is: is there a chance, a useful chance identity and something about cumulants? Let's match. So okay, this is the paper, and thank you.