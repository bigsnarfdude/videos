I will mostly, for this reason, skip the very first part of my talk, which is definition. We've already seen them. I'm just gloss over them for the most part. And then we're going to go towards the bulk of the talk. The first section is consider a faithful invariant Gaussian state. It is a section because we will need a Gaussian invariant faithful state, but But as we will do for many other things, one of the nice properties of Gaussian quantum microsemic groups is that they can be analyzed on an algebraic vector, a finite-dimensional vector space level. So one starts with the definition that can be given for a general quantum microsemigroup, like considering a faithful invariant state. And then one can see what this assumption means in terms of linear algebra. So matrices. Linear algebra, so matrices of the coefficients. This is a recurring theme for Gaussian quantum microsemigroups. We will see, then again, we start the study of GNS symmetric semigroups, and then we study algebraic conditions for symmetry. So, again, we start with some results that are maybe valid, and they are, for more general classes of quantum microsemigroups in general. And we specialize them for Gaussian quantum microsemigroups to obtain explicit conditions. Explicit conditions, matrix algebra conditions for them to hold. And the very last point will be a structural result, actually very inspired by Mike here on the vector. For given GNS symmetric semi-group, we can finally provide, let's say, canonical, even though it's not canonical in the sense of category theory, but it's a canonical form. Canonical form for the structure of the generator. And maybe Federico will speak a bit more on this topic later. But for a long time, we've tried to look for a standard way to present Gaussian quantum microsemigroup. Let's say, okay, they are generating in some way, but we wanted to give a nicer form for them. And we are starting to get close to those results. Here is the first one. Here is the first one, I hope you'll find it interesting. So let's start with the definitions. As I said, I'm mostly will be quick about this. So I will just briefly show what are the settings. H is the bosonic fact space. We have an inhalation enclosure operators. They act in the usual way. They satisfy the usual canonical commutation relations. They are unbounded operators for the reason we need to be careful around them. We also define position and momentum operators, which are. Operators, which are again self-adjoint operators, they satisfy the usual commutation rules and so on and so forth. Given this, we define a Gaussian quantum microsemigroup in many ways, but here we are going to start with a JKLS generator, and instead of bounded operators, we'll put operators that are in of this form. And at this point, let me stress that in this presentation, unlike the others, the Hamiltonian. The Hamiltonian part will be mostly disregarded because we are going to look at symmetric semigroups, and we'll see the one condition first for a generic semi-group to be symmetric is that there is no Hamiltonian. So even though it is present here in general, it will not play an important role in the following. In particular, we see the linear part in the Heidager will be zero as well. And as you may have seen in the previous operation, it would mean that the invariant state It would mean that the invariant state has zero mean. But we'll come to that later. The other condition, the only one which remains relevant for this talk, is that we ask for matrices U and V of the coefficients of A and A dagger in the definition to satisfy this condition, which simply translates into all the cross operators we are considering are linearly independent. So, this is all we are asking for now. Asking for now. Given this, we define a semi-group which satisfies the usual explicit action that we already seen many times. It maps a Gaussian quantum Markov semi-group maps value operators into other value operators, multiplied by a constant, and modifying the argument of the operator. Here are the real unit operators. We'll come to real union operators in a bit, but maybe I can answer the question a bit more or we can discuss them later. Or we can discuss them later anyway. So, actually, one could see that instead of looking at all the parameters that appear in the generator of a semigroup, one could simply look at the generator by considering these two operators. If two semigroups have the same Z and the same C, they coincide. Okay, so why are they Gaussian? Well, we have seen many times. Many times now, that it all has to do with Gaussian states. If one takes a Gaussian state which has the quantum characteristic function, so the quantum Freeman transform, let's say, that has the expression of a standard normal random variable. Then we call this state a Gaussian state, and we say that omega is its mean vector, and S is its covariance operator. Now we can see. operator. Now we can see one of the reasons we speak of many different ways of the same real linear operator because you see here in the first expression for the quantum characteristic function, one has a very simple expression, very easy to write and a compact expression. But if one wants to have a better understanding of what's going on in the quantum conditive function, well maybe it's more natural to Well, maybe it's more natural to look at it by an isomorphism or really vector space and looking at acting instead of Z on its real and imaginary part. So this is the same expression just written, transforming the space on which it acts via a real vector space isomorphism. And what happens here is that the expression is much more complicated, but it's easier to see that this is It's easier to see that this is the characteristic function of a classical normal random variable or normal random vector because this is a real matrix and these are just coefficients. Not the coefficients, but the variables. Okay, and we have seen already that, again, the very same operator Z and C regulate how any given Gaussian states evolve under the pre-dewaldia joint semigroup in this case. Pretty well the adjoint semigroup in this case. Okay, so we're going to start by what we need to speak about symmetry to consider a Gaussian invariant fateful state. And before moving on, this is the point where I addressed the part about relinquiant operators. I've not pointed out in the previous slide, but a valid operator can be seen either. Operator can be seen either as wz, which is simply an operator, or the exponentiation of a combination of p's and q's, position and momenta, or a linear combination of a and a dagger. And in some sense, depending on which way I want to think of a value operator, I can map it to an element of one of the three isomorphic real vector spaces. Real vector spaces. The first one, the real vector space is the real vector space C D. It's the starting point. So we take Z, which is an element of a real vector space. This is isomorphic, of course, in considering it as an element of R to the 2D, which decomposing it into its real and imaginary part. And this corresponds to thinking of the value operator as being a real combination of position and momentum, suitable combination. And the last one. And the last one, this is just a real vector space, well, a real vector subspace of t to the 2d thought of as a real vector space. So this would be a 4d dimensional vector space as a real vector space. We just consider a subspace, a real subspace, which has dimension 2d of elements of the kind z and conjugate. Yes? Yes. Well, yeah, I don't quite recall the correct, let's say, constants in front, but it's essentially the same classical scar product on CD. You maybe have to multiply by your imaginary identity in front, maybe divide by two, but essentially what it takes to go from here to here is simply make it work. Here is simply make it work. You take the vector space you have here, and you know that it should be isomorphic to this one, and you evaluate the inner product that it takes to make it a real vector space isomorphic to the first one. And depending on which way you think of a real vector space, a given real linear operator acts on elements of the space in a different way. This is the This is the way we have presented so far because, again, it's slimmer. It's maybe easier to think of it. But as we will see, when one thinks of the spectrum of an operator, it's maybe more natural to think of this operator as being a 2D by 2D real matrix. So that we know that whenever we find a complex element in a spectrum, well, we started with a real matrix, which is an operator on a real vector space, but we are thinking of it instead. Space, but we are thinking of it instead of acting on c to the 2d of its complicification, and there we can diagonalize the matrix because its spectrum contains complex eigenvalues. Either way, the isomorphism induces a different way we can write the explicit form of a real operator. And depending on the application, one chooses one or the other, just to make computation. The operator is the same, it acts on the same way when you follow a commutative drag. You follow a commutative diagram, let's say. The first identity. Yes. The real page, but with complex scalar product. No, with a real scalar product, because you have that real scalar product in the first place. So here is C D and the scalar product. Just you do with bile operators. Yeah, exactly. Exactly. Okay, that is natural. That that is natural. Okay. Okay. All this checks out? Okay. Okay. So whenever one needs to think of this, this form is quite more complex to work with, but it's easier historically also to work with. And so whenever you see we talk of a spectrum of a real uni parator, you should think of looking at these 2D by 2D metrics. These 2D by 2D matrices and computing spectrum. Just like we do in the next slide, where eventually I come back to symmetric question quantum Marcus semigroup, just in this slide, where we assume that at Z is stable so that we can have a unique invariant state and all other initial states converge in trace norm to this one. And while this is not actually too broad of too strict of an assumption. Broad, too, too strict of an assumption. Federico will speak about this a bit more later. But in general, if you want to have an invariant state, you should consider matrices with spectra, which is all the eigenvalues have less or equal to zero real part. And if one instead requires that it's stable, there is a unique Gaussian variant state which Variant state, which was this, let's say, matrices or parameters are given explicitly, and all other states converge towards this one in trace norm. And let me point out that, as I said in the beginning, if zeta, the parameter in the Hamiltonian, which was the linear part of the Hamiltonian, is zero, automatically the mean of the Mayan state is zero. So that's one nice assumption, as I said earlier. As I said earlier, we're going to disregard completely Amingtonian. So, this is the first part. Now, this is an invariant state, this is a Gaussian invariant state. Is it faithful? There is one. So, we all want to have a faithful state, then it should be. And in order to look to faithfulness, let me briefly recall what J was. This is the expression we have already seen many times. And just for reference, these are the other ways one can think of the same. Way one can think of the same j when looking at different isomorphic real vector spaces. And as is the covariance matrix of a Gaussian state, if and only if this equality holds. And also, if we could take the minus sign here, it's just conjugation. So it should be positive, even with the minus sign. Then again, okay, so S plus AJ works. Okay, so S plus Ij works when it is faithful, where an invariant state is faithful if S plus Ij is strictly positive. And this condition, by exploiting the explicit form of for S, this condition can be translated into conditions for this matrix Cz, which appears inside the integral here. And while And well, for this integral to be strictly positive, we may have some loser assumptions, but a quick and dirty way to get rid of this, and that's not actually that bad, is to assume that Cz is strictly positive. In this way, this integral would be strictly positive trivially. And as we already seen, Cz strictly positive means there are exactly 2D linearly independent clause operators. So the assumptions for So the assumptions for now on would be Z is stable and CZ is strictly positive. And this assures that we have a unique, faithful Gaussian invariant state. Right. Another step before looking at symmetry is that we can simplify our lights, our endeavor in working towards symmetry, exploiting the so-called Williamson. Exploiting the so called Williamson normal form of a positive real map, real operator. And simply is, the result is simply that we can diagonalize any positive operator, like the covariance operators of a Gaussian state, via a symplectic matrix. So a matrix that preserves the commutation rules of vile operators. So the preserves the structure of the Structure of the box space, in some sense. And moreover, this symplatic transformation can be implemented at a unitary level on the Fox piece itself. So if the starting semigroup had an invariant state which was not diagonal, we can perform a unitary change of the Fox space and obtain another semi-group which is equivalent to the first one. First one, but where now the invariance state is diagonal. Well, it's there. It has a diagonal, it has a diagonal covariance operator. Yes. Okay, so from now on, we want to work in this setting where Z is stable, Cz is strictly positive, and we assume that the invariant Gaussian state, which is unique and faithful, is diagonal in this way. Now, we are finally ready to. Now, we are finally ready to speak about GNS symmetry. And to speak about this, we first need to introduce the GNS inner product. So, given this environment faithful state, we can define the inner product in this inner product on B of H in this way, which resembles closely the Hilbert Schmidt inner product. And actually, it can be thought of as arising in this way. One could embed B of H into embed v of h into hilbish meet operators and the scalar the scalar product there the inner product there will be exactly given by the definition we given before and i'll briefly touch on more generalization what could have and even though we are not touched uh with them um in this very talk but uh it's nice to see that well it's something that i will say are probably easily generalized generalizable in other embeddings Generalizable in other embeddings, which arise by simply splitting the multiplication by row in other ways. So that this is not commutative, this is a non-commutative setting. So multiplying, splitting the multiplication has different effects. Yes. So what is when a semi-group is genesis symmetric? Well, since it is an inner Well, since it is an inner product, we can compute the adjoint with respect to this inner product, which is the this delta operator with tilde operator here. And we say a semigroup is GNS symmetric if the two, the generator and its semi-group and its adjoint with respect to this gen asymmetric product are equal. And this is the main property we're going to speak, we're going to talk about. The other one is here for future reference and to give you an idea. For future reference, and to give an idea, since one of the results that we present shortly is related to Z, this is another condition that maybe we can exploit in the future. But we say that a semigroup satisfies the one detail balance condition if there exists a bounded operator for which the generator of the semigroup and its dual with respect to the Digeness product differ just by the action of accumulator. And again, let me stress, this is. And again, let me stress, I will not speak about this. It's just here because we will need it shortly after this slide. But it's maybe a nice continuation point for this research. For example, K here is assumed to be bounded, but in our setting, there are no bounded operators involved in the generator. So you can see how the problems may arise and why this definition is not related completely to this very talk. But I will see sure. But uh we'll see shortly. So uh yes yeah it's defining here this since this is an inner product on B of H let me briefly go to the back slide. We can define an inner product here so I act on TTY and its tilde is when I brought TT on the other side. Okay, simply this one. You can see here TTY gets transformed into the Tildy. TTY gets transformed into the tilde when I brought it into the other on the other side. Okay? And this is essentially TT is self-adjoint with respect to this inner product. Okay. One observation that I was going to do it later, but maybe it's right now. We don't even know if this dual semi-group is a semi-group in the first place. So we can adjust. First, place. So we can adjoint every map, but do we get the semi-group eventually? And this is what answers the questions, for example. There is a result due to Franco-Veronica, which says, let me call the definition of a modular automorphism in the modular group. This tilde is actually another semi-group if and only if every n entity commutes, actually, n entity commutes with the model. NETT commutes with the modular automorphism, or the generator itself commutes with the modular automorphism. Let me stress these are facts known for uniformly continuous quantum microsemigroup. So there was a generator without some domain considerations made on it. And one nice thing that, again, I'm not going to touch into further for this talk, but might show how these things progress in this. How these things progress in this line of research is that the previous result also holds for all the other embeddings, which apart from the KMS1, that is with S, which is exactly a half. So what I mean for the previous true change in embedding, well, we can define another dual, and this will be a QMS if, again, this any of the two equivalent conditions. So, in some sense, one can solve the symmetry with respect to Gen S inner product and then sort of get for free symmetry with other embeddings apart from the KMS one, which is quite peculiar in this case. What do we know so far? Well, there are some results on matrix algebras. One of them also, maybe a survey was due to a Caroline here. Was due to a catalyst here. If we have a semi-group of matrix algebras with a faithful invariant state, then its generator may be written in some other ways, but as this expression where the set of cross-operators is self-adjoint, and every cross-operators is an eigenvector for the modular automorphism, and a sort of co-inversal salt. One could say a bit more about this semi-op. A bit more about these operators, but this is the basic part. And as you see, as I told you at the beginning, in this generator, there is no Hamiltonian part. So the Hamiltonian needs to be zero. The generators are eigenvectors for the modular automorphism, and the set of curls operators is self-adjoint. Franke Veronica, let's say, extended this result to not only semi-groups on Semi-groups on matrix algebras, but to uniformly continuous semi-group. So general not on finite dimensional spaces. And they did in here comes the one detailed balance condition. So given a uniformly continuous quantum microsemigroup with a faithful invariant state, then the generator must have this form. H must be a multiple of k, which is the operator that appears in the detail balance definition. In the detail balance definition. And both LL and LL star must be sort of eigenvectors of the modular automorphism, as well as another condition, which replaces the one of self-adjoint set of cross-operators, is that we can obtain all cross operators by taking a unitary combination of the cross operators of the semi-group. The cross operators of the semi-group. So, all the cross-operators can be obtained by taking unitary approximation of this cross-operator. Note in particular that in the case we are interested in, where a semi-group would be genus symmetric, that is, L is equal to its tilde, then this condition shows that k is equal to zero and therefore h is equal to zero. So, all the very the same things we saw earlier still apply to this point. Now, we get to. Now we get to what we did for our case. Let me warn you, it is not a final result. And any of these, I'm saying, is completely final in this version. In particular, you see a result which is, let's say, half of a result. These are just necessary implications that get complemented by some sufficient conditions. By some sufficient conditions coming from other places, other ways. Maybe we're going to complete this result in order to be more similar to the one I showed you earlier. But for now, let me see what it reads. Let Achaemas with a faithful invariant state, row, and here with a faithful invariant state means that it's stable, since that is positive and yada, yada. If T gen as symmetric, then it may be generated choosing these two. H is equal to zero with C. These two. H is equal to zero. We've seen the Hamiltonian must be zero. And on the domain of G, so now we have some domain restrictions since the operators we are working with are unbounded. Still, H cross operators is an eigenvector for the modular automorphism. So this is some necessary results we obtained on it. What's missing? Well, what's mainly missing is the fact that the set of calcium is separate joint, or as you see in Set adjoint, or as you see in the previous result, if we can obtain every adjoint of the cross operators by taking combination of the other. So, this is what the characterization that is really missing in this picture. However, we'll see how we can deal with it. First, let me present an example of all these conditions in practice. It is the D equal one example. We have already seen it in action. Let me take h is equal to zero. We understood this is a necessary condition. We understood this is a necessary condition. And take two class operators, which are really simple. One is just the creator, the other one is the annihilator. Stability of Z, so we now try to assess whether this semi-group has an invariant state and whether it is faithful. Well, Z is positive provided this parameter is positive. And the condition simply means this semi-group must have more annihilation that it does creation. That he does creation. So the coefficient in front of A must be bigger than the coefficient in front of A dagger. Otherwise, we would escape at infinity, let's say. The inviolate state can be computed explicitly with all the formulas I showed you earlier. And it's the multiple of identity and this multiple of the identity. In particular, we see that if mu is equal to zero, so if there is Maybe there's a mistake here. If lambda is equal to zero, this should be it. Lambda is equal to zero, that is, there is no creation. The invariant state is the projection on the vacuum. While if lambda is different from zero, so again, this is lambda should not be mu, it is, let's say, a conventional Gaussian state, just an exponentiation of the number operate. Just exponentiation of the number operator. And here you see the constants here are related with this cotangent, hyperbolic cotangent relation. Okay, so this state is fateful only if lambda is different from zero. And lambda different from zero, if lambda is different from zero, we are in this situation where in particular we have exactly two cross operators which are linearly independent. Operators which are linearly independent. And one may note that on suitable domains, A and A dagger both satisfy this condition on the modular automorphism. So it's in a sense from this equation, one reads that both L1 and L2 are eigenvectors of the model automorphism, and that if we had chosen any other possible Possible choice, any other possible choice for L1 and L2 would have not worked because the only possibility for a linear combination of A and A dagger to be an eigenvector of the model automorphism would be if Bay consists exactly of only A or only A dagger. So it was all artificially put directly in the definition of the semi-group that we're working on, but it's actually the only choice we put upon. What remains, as I said, is some conditions. What remains, as I said, is some conditions on the fact that the set of Kraus operators is self-adjoint. And we see that the condition that Franco Veronica had about how we can recollect all adjoint of cross-operators by taking a suitable unitary combination of the cross-operator themselves. Well, we can, we can write every adjoint of cross-operators as a linear combination of A and A, of the star. Of A and of the starting class operators, and the matrix we do, we achieve this, is unitary if and only if this condition works. So, if and only if we can relate in some sense what is mu and what is lambda. Yes, exactly. A condition is that this was the condition that on the result of Franco Venorica, which said that you can recollect all adjoints by collect all joints by taking a unitary combination linear combination of all the cross operators so we can achieve we we can collect every we can well we can reconstruct every cross operators a joint of cross operators starting with cross operators we just need to do with the unitary matrix here and the condition for it to be unitary translates into these relations with the coeffici between the coefficients of the two does it come easier? Does it convince you? Okay. Okay. So this is what we are for now. This last part, sorry, this last part is just what if we want to look at the condition that Franco-Veronica had. It's not something that we have proven so far. What do we have instead of this? We have some algebraic conditions for symmetry to hold. And this is actually a complete result. Maybe Maybe a bit not really related with the literature. That's why I started with some review of the literature so far. We know that a Gaussian chemist with a faithful inbiogaus state, the following are equidivalent. First of all, is Jen asymmetric or one of the two algebraic conditions hold? And the way we obtain these algebraic conditions simply start looking at the definition of a GNS symmetry semigroup and put in. Of a GNS symmetry semi-group and putting plugging in file operators. And with some work, you obtain the actual results here. Both solutions are interesting for two reasons. The last one just depends on the matrices C and Z. So on the parameter of the semi-group. Given a semi-group, you can immediately understand whether it is genesymmetric or not, provided you know that Z is stable, Z is positive, you compute this. positive you compute this this uh you you look at this equation which simply states cz is alpha joint is the this is the the condition actually and you can immediately know whether a semi-group is genesymetric or not the second one actually is what makes the link what constitutes the links between our algebraic result and what happens in the literature and we what i we're gonna do what i'm going to do now is to relate these two conditions Is to relate these two conditions with what happens in the literature, let's say. Let's start with the condition on Z. And the proposition is yes. Yes, at this point, the only thing that, if you want, is the requirement is that we are starting with a zero mean vector space. But at this point, H is this point, H is whatever you like. So these are actually conditions that provide you with the, as I show here, provide you with the fact that H must be zero, because this equation works only if h is equal to zero. And well, this not only tells you this, but also gives some insight on what's to come. Because Z as an operator has two contributions, one from UMV and one. From U and V and one with from the Hamiltonian. If the Hamiltonian is zero, this means that Z just depends on U and V. And these conditions can also be reformulated by saying that Z is self-adjoint with respect to the symplatic form. You see here, I just put these line of computations. I'm not going to go through them if you know it, or if you want to feel free to ask me. But this simply means that you can. This simply means that you can z itself adjoin with respect to the sympathetic form. And this way, apart from giving you h equals zero, this provides some insight on the fact that if this condition holds, maybe the semi-group can be simplified because the very parameters of the semi-group behave well when seen on the sympathetic form, which encodes the structure of the Fox space. So, this is for starter. The other condition. So this is for starter. The other condition, right, provided z start j is equal to j z, provided what we just saw, so that h is equal to zero, provided this, the other condition is equivalent to this algebraic expression, which is a bit difficult to parse. So let me spell it down in the example of It down in the example of d equal one. This last condition means, or well, provides us with exactly the condition we had for a semi-of-time genesy matrix using results from the literature. So this is where the circle closes, and we can formally provide a result for when the semi-group is DNS symmetric. As a last part, let me get to the structural result. What we have done so far using these provide some algebraic conditions. Now, let me go back to claus operators and let's provide some structure for these clauses operators. Let me see if those algebraic conditions can give us some form for the clause operator. So, since S is the So since S is diagonalized, we can spell it out using the multiplicities of each of its eigenvectors, eigenvalues, sorry. So we spell S out as being diagonal and each eigenvalue as some multiplicity. The condition on the fact that each cross operator must be an eigenvector for the modular automorphism and the fact that SD structure Structure tells us that each cross operator can only be of one of two forms. Both of them only consists of an inherent or creation, so they cannot be mixed between each other. And the contribution from the set of coordinates on which these creation and relation operators come from is related. From is related to the coordinates of the blocks of the S matrix. So the S matrix provides you, say, an amount of amniation increasion operators that may be present in each Kraus operator. The set of coordinates in which you are summing depends on the blocks of S, so on the multiplicities of the eigenvalues. And this can be brought even further. And this can be brought even further in a picture way, let's say. So the matrix of all possible cross-operators have S this expression. It is block diagonal, and each block corresponds to one of the blocks of the matrix S. So the block form of the covariance operators translates into a block form. Operators translate into a block form for the Kraus operator. And we can take it even further because actually we can diagonalize even more the Krauss operators matrix and say that provided everything we have said so far holds, then the semi-group can be generated with Kraus operator that consists of exactly one creation or one annevation operator. And since actually, since that is strictly positive, there must be all of them. There must be all of them. And they are related with a constant, which is exactly the eigenvalue of the covariance of matrix of the invariant state. And that's it. That's what I wanted to say. So thanks for your attention. Do we have any questions? Yeah, hello. Yeah, let's see. There is something analogous. Actually, I'm not going to make any statements here, but it should be true if we replace this condition with another S, which is related to, I don't know if you recall what Emmanuela said. I don't know if you recall what Emmanuela said about. Is there any talk? Yes, it is. I can read it here. It's already, what was called like S Brebe. If you recall, there was this analogy between the spectral gap of GNS and ChemS by switching from F to a Brebe version. There is something like this. However, and maybe There is no counterpart to this equality. Well, there is not. At this point, we do not have a counterpart for these equivalents, because this comes down to the fact that S is written in terms of C. Let me bring it up for you. Yes, for example. Yes, for example, here, S is related to C, but the way S preva is defined is a bit more implicit, starting from S, diagonalizing it, changing its eigenvalues, and bringing it back in a full form. So at this point in time, we do not have the probably the same result holds even for K-mass symmetry, slightly modified, but we miss the last part. It may not be really that important for you. Really, that's important for you? Depends. But it is as to say that I'm going back, I'm going forward. That's what it was. You don't know how to assess symmetry without computing the actual environment state. Sorry? I've not checked. I don't check. Even in the 1D case, computing the explicit. Computing the explicit invariant state is not really easy. Okay, maybe you can diagonalize it, put it in some nice space, and obtain some explicit results, but I've not tried it. I don't know, probably. Okay, thank you. Thank you very much for your question. Any other question? Yes. Okay, one well, they're actually related. So the first question would be. So if basically the support of the coefficients in the expression of the jump operators, is it related to basically when you bring your Gaussian state into a product state to the different temperatures of your thermal state? Yeah, it should be. And so does this. Yeah, it is easy. Does this uh yeah it is okay and does this reflect uh basically in the uniqueness of uh of the representation you show uh in the last slide? So basically uniqueness is a strong term. I don't know if it is unique in the presentation. This one? Exactly. So basically here you're only you're bringing your jump operators to be uh either creation or annihilation type. Yes. Would it be that if you have like more modes with the same temperature, you could have like many You could have like many ways of that. That's the point. I don't think so. Okay, because at this stage, which let me remind you, the inverse temperatures are related to the again values of the invariant, the covariance matrix of the invariant state. So every block has the same inverse temperature. And at this point, just at this point, we arrived at At this expression for travels operators, which are block diagonal, actually are still entangled between each mode with the same inverse temperature. That's the point. However, actually, when speaking with Merchio, who brought it up, he thought if it was very actually different, if you could not disentangle the modes with the same inverse temperature. And it turns out you can. The easiest way I can convince you of that, I think, is that. Of that, I think, is that if you look at this condition, it spells out that u star u and v star v commute, actually. And therefore, each block commutes. And the action of taking symplatic transformation of the space or unitary combination of Krauss operators, so all the actions you can perform on Kraus operators to modify them. To modify them can be performed by preserving the block structure of the cross-operator. So since U star and V star commute, they can be simultaneously diagonalized. And therefore, you can go from a block structure like this to one which is actually diagonal. Because you symmetrically simultaneously diagonalize both U1 and V1. V1 and U2 and V2 and so on and so forth. Does that convince you? Okay. You're welcome. Any other question? Okay, let's come in again. Thank you, Legion.