No, we have plenty of time. We have a coffee break right after, so it's it will be okay. So it's my pleasure to introduce Franklin. And the present to that to it. Hi, everyone. So hopefully I can't make the talk any worse than the start. So I was invited to give a talk on chromatic symmetric functions and LLT polynomials, which fits in with the title of the whole overall conference. Lara will be giving Laura will be giving the talk in the next talk, which will finish off the conference title, which is the Hessenberg varieties. So, the first thing is that there's no way I can cover everything I want to cover in this talk, so I won't. But I'm going to point to these lecture notes that we started playing together there in, well, just before June, because we gave a graduate summer school on the geometry and commentaries of Hessenberg varieties. So these will be available, and so you can. Will be available, and so you can find a lot more details in there. Okay, so I'm going to try to give this. This talk is kind of going to give you an idea of maybe what you might find in here. There we go. So are you going to upload those notes to our file repository? As long as my computer allows me to. Yes. All right. Also, this is a talk which is aimed towards A talk which is aimed towards, I guess, the geometers in the room mainly. So, some of the combinatorialists will know some of this stuff. And again, I'm going to try to give a flavor of some of what's going on. And there are a lot of details that I left I'm going to be digging out. Okay, polynomials. Let's start with polynomials. So, first thing, let's fix some notation. So, n is going to be n is going to be all the non-negative integers. So these are the natural numbers. x throughout this talk will denote a family of variables. So x will just be x1, x2, x3, up to 6n. This is 1 indexed, even though n is 0 index. But that's fine. If we're given a vector of natural numbers, then we can actually construct a monomial. And so the notation will be x to the power a, so x to the power a. x to the power a, so x to the power of some vector. And what we're going to do is just write x1 to the a1, x2 to the a2, all the way to xn to the an, with the convention that xi to the 0 is equal to 1. Okay? And this means we can write our polynomials in the following form. So f of x looks like something like this. So we're going to use this basically as just a compact way to write down a polynomial. Just a compact way to write down a polynomial. So we're going to have alpha, which is some vector, it's going to index the coefficient, and it's going to index the monomial. Okay? And then, so the coefficient is the number you have in front, the monomial is this thing, and the term is the whole thing together. I have to do this. There we go. All right. Symmetric polynomials. We're going to let Sn be the symmetric group acting on the set from more. Acting on the set from 1 through n. So we're going to use this notation, which is very common these days, for the set 1 through n. A polynomial is symmetric if you can actually permute the variables and not change the polynomial. So here we can just change the, and we just swap a whole bunch of the variables, permute them, and this will not change the polynomial. So this has got to hold for all permutations in the symmetric group. Of a symmetric group. Since the symmetric group is generated by simple transpositions, you can just verify this, all you need to do is check that swapping xi and xi plus 1 keeps your polynomial invariant. So here's a quick question. Monday morning question, which of these polynomials are symmetric? And how many variables? You're cheating. I wanted to check people. All right, so this one here is. Alright, so this one here is not symmetric because x2 squared is not in our polynomial and it should be because we can swap x1 and x2. This one here is symmetric if you only have three variables. So it depends on the number of variables. If you have four variables, you're missing some variables here. So we're going to assume we have. Here. So we're going to assume we have lots of variables in general. We're going to assume that we have enough many variables. Not iffully many, but enough many, just to make sure all our computations work. So even this one here, this one's symmetric, but it might not be if we don't have the right set of variables. I'm going to use this notation here for the algebra of symmetric polynomials. So you can quickly verify that the product of two symmetric polynomials is symmetric polynomials, as is their sum, and so on. As is their sum, and so on. And so this is going to be the polynomial ring with a superscript SN. So it's just going to be all the elements that are fixed by this action of the symmetric group. All right, so first observation that we can make is that if we have some term in our symmetric polynomial, like this one here, 17x20 squared x22 to the 5, then we can say replace x20 by x1, x22. By x1, x22 by x2, and we get this other monomial. Or we could replace the first one by x2, the second one by x1, and we get this monomial, and so on. And so this observation is telling us that we can just start with a monomial and symmetrize that monomial to get a symmetric polynomial. And so here's an example. So say we start with x1 squared, x2 to the 5. We symmetrize, so we just swap. We symmetrize, so we just swap things as much as possible, and we end up with something that is a symmetric polynomial. We could do that with this monomial here, so x1, x2, x3 to the power 2, 5, 2. And we symmetrize. Okay. There we go. So what we have here are called the monomial symmetric functions. So what we do is we prescribe the exponents. So we say we have exponents lambda 1 up to lambda L. Exponents lambda 1 up to lambda L. And we're going to write this in a non-decreasing way. And then just take a sum over all the monomials that have that exponent sequence. And so these are the monomial symmetric functions. And this forms a basis of the algebra of symmetric functions. So because these lambdas are going to be important, I just want to give them a name. So these are called partitions of length L. So it's just going to be a sequence of positive integers. So it's just going to be a sequence of positive integers satisfying this sequence of inequalities. So we just want them to be weakly decreasing. So a partition of length L is a sequence of length L of positive integers that are weakly decreasing. And then every symmetric polynomial can be written uniquely as a linear combination of these. This is one basis. We're going to see several other bases. And a lot of the magic that happens with symmetric. And a lot of the magic that happens with symmetric functions is actually switching between bases. So, I'm going to actually tell you about a whole bunch of bases. But before I get to other bases, well, I'm going to tell you about one in particular, I want to talk about this recurring theme that we're going to see actually throughout this talk. And that's that you can construct a symmetric polynomial starting from an action of the symmetric group on a bunch of objects, combinatorial objects. So we're going to be given a set of combinatorial objects. Some map epsilon, so here epsilon, think of exponents, that gives that associates to each combinatorial object a vector. So it's going to be that exponent sequence. And then we're going to have some action of the symmetric group on our combinatorial objects that's compatible with this map. And what does compatible mean? Well, compatible means that it's what's going to make this statement true. This polynomial you get here will be a symmetric polynomial. Polynomial. So here the takeaway is that we're going to sum over all the combinatorial objects, and for each combinatorial objects, we have some monomial. Okay? And if you pick your action and your exponents appropriately, you end up with a symmetric polynomial. So let's do this in a bunch of different cases. So first case is Tableau. So we're going to start with a partition. And for example, this one here, 5, 4, 4, 1. Here, 5441. So, we're going to represent this pictorially, graphically, with a diagram. And so, this is called a young diagram, and it looks like this. This is what a young diagram looks like. So, the young diagram of this particular lambda looks like this. So, there's lambda one elements in the first row, so five elements, then four elements, then four elements, then one element. And because I gave the first talk of this conference, I said the notation we're using for the rest of the week. Rest of the week. So, this is the French location for partitions. Everyone say no. A semi-standard Young Tableau is now going to be a filling of our shape. So, we're going to replace, so we're just going to actually, I mean, we drew these little boxes. So, we're going to put things inside the boxes. And we're going to put our positive integers, and we're going to have two conditions. And we're going to have two conditions. One is that they're weakly increasing along the rows. So here we have 2, 2, 4, 5, 4, 4, 5, 7. So weakly increasing on the rows. Strictly increasing in columns. Okay. So this is an example. This is a nice, very, very ubiquitous combatoidal gadget that we find in the algebraic kamaturics literature. And so now we have a set of Common Toy objects. We have a set of commercial objects, and we're going to associate a monomial to each one of these. So, if I just go back to the previous slide, wait, how do I do that? This way. The way to attach a monomial to this is actually to think of each entry as a variable. So like the 2 here is x2, the 5 is x5, and so on. And then just take a product of all the variables that you see there. And so we're going to sum up over all the semi-standard Young Tableau. Over all the semi-standard Young tableau of a given shape over a certain number of variables. And we're going to take the corresponding monomial. So here, the epsilon i, the exponent is just the number of copies of i that you see in your tableau. So here's an example. This was supposed to be interactive, but I'm not going to attempt anything. I'm not going to touch the computer anymore. So there are actually eight semi-standard tablets. Actually, eight semi-standard dev rows. I'll just show them actually. Here we go of this shape. And so each one gives us a monomial. So this one's going to give us x1 squared times x2, x1 squared times x3, and so on. And so we get here the Schirt polynomial indexed by the shape 2, 1. It's expanded in three variables. Okay, so you have to actually provide the number of variables here in order to do this expansion. So here's some special cases of these polynomials. So if we take a shape which is 111, for example, so just a column of length of height 3, then these are the only semi-standard young table you can have on four elements. Okay, so you can't have repetition in the columns. So this polynomial here, we're going to give it a special name. Here, we're going to give it a special name. This one is called the elementary symmetric polynomial. And so it's going to be a sum over all the monomials that don't have any repetition, so square-free monomials. There we go. These are algebraically independent. And so this tells us that products of these monomials, sorry, products of these polynomials are linearly independent. And they're going to give us another basis of our us another basis of our algebra of symmetric functions. And so for notation, we're going to write E indexed by a partition to be just the product of the E lambda i for each lambda i inside our partition. We can do something similar with rows instead of columns. So instead of looking at column tableau, we're going to look at row tableau. So for example here we're looking at a row tableau of size three. Size three. So here we have one, one, one. This is one example. Here's another example. Here's another example. So these are all semi-standard young tableaux because we're allowed repetition in the rows. So here, we have a lot more. So those dots mean there's a lot more. Actually, we get every single monomial that you can form of degree 3 in your set of variables. Okay? So these are called the complete symmetric functions. These are called the complete symmetric functions, or the complete homogeneous symmetric functions, or just the homogeneous symmetric functions, and they're denoted by h. So I guess complete in some sense is accurate because we're looking at all the monomials, so we have them all, so it's complete. Homogeneous is very inaccurate because all the polynomials that we have are homogeneous that we've defined. But we're going to call them HK because that's standard. So we're going to sum over all the possible monomials. Over all the possible monomials that you can get. That's it. So it turns out that these are also algebraically independent, and so we get another basis just by looking at products of these H's. And we're going to use the same notation where H sub lambda of a partition is going to be just a product of H sub lambda I. Any questions? Please interrupt. Okay, this one doesn't kind of fit into that whole idea. I couldn't figure it. Idea. I couldn't figure it, I couldn't make it fit into the idea of summing over a bunch of commentator gadgets, but I'm going to tell you about them anyway because they're algebraically independent. It's in a good spot. There's the k power sum symmetric polynomial. So what we do here is we just sum the powers of the variables. So for p, so pk is just the x1 to the k plus x2 to the k, and so on. So they're power sums. They're very nicely named power sums. And these also are algebraically independent, and so we get yet another basis of Basis of the algebra of symmetric functions. And these are also we're going to use the same notation, p lambda, and so on. But let me point out, not all bases are algebraic. You can't just assume that all bases satisfy this kind of formula. So for example, the m lambda that we started with, if you write m subpartition, it's not equal to, like m3, 2 is not equal to m3 times m2. Okay, so that's something that tricked me up when I was learning about these things. Something that tripped me up when I was learning about these things. All right, let's switch combinatorial gadgets. Instead, let's look at chromatic symmetric functions. So, here we're going to start off with a graph. So, a graph is going to be called gamma. And it's going to consist of a vertex set and an edge set. And here's an example: so, a vertex set 1, 2, 3, and our edges are just going to be pairs of vertices. So, this representation. So this representation here encodes this graph. No way. A coloring of our graph is just a function, kappa, which goes from the set of vertices to some set of colors. We're often just going to take positive integers for the colors. And we're going to say that our coloring is proper if adjacent vertices have different colors. So if we have some edge in our graph, then the colors that we assign to the vertices That we assign to the vertices of that edge should be different. So here's a nice colorful graph. So I've colored it. This is our common colour object. To this, we can associate a monomial in this way. So here I've made a choice. I labeled the colors. Red is x1, I think it's orange, orange is x2, and so on. And so this allows us to write down a monomial. And then we could define the chromatic symmetric function just to be this. function just to be the sum, oh sorry, the chromatic metric function of a graph to be a sum of monomials, one for each proper coloring of the graph. So one for each proper coloring. And so for this one here, you get a lot of monomials. So this one here, for example, it was like use red twice, use purple twice, and then I guess green and yellow once. And there are 28 ways to do that. So here's two ways. Computers are very useful for these kinds of applications. So trying to figure out which thing? I'm hiding that. I'm hiding that. Yeah. So I didn't say how many variables. You're right. So this is one of the things I wanted to hide away. This is a question we could talk about later, right now? The difference between finally making The difference between finally many, enough many, and infinitely many variables. Let's do another example. So, this is a small example that I stole from John and Michelle Thiefer. So, here we have our graph. There's three factorial ways to color it with three colors. Just different colors for each vertex. And we're happy. This gives us the monomial x1, x2, x3. If we want to use two colors, say i and j, So, if we want to use two colors, say i and j, then there are two ways to do that because we could put the two extremities with one color and a different color in the middle, and we're done. There's no way to color this with only one color because we won't get a proper coloring. So this gives us our polynomial, our symmetric polynomial in three variables, our chromatic symmetric polynomial in three variables. So we have six copies of x1, x2, x3, plus all the other ones. If we expand this in the monomial basis, we get six. Basis, we get 6M111 and M21. If you do it with 10 variables, you're going to have a lot more on this line. So this line is going to have a lot more variables, but this line will be the same, which is the nice thing, right? So this is a much more compact representation. Thank you. Yes? So we are here thinking of the vertices as like labeled, so that's three factorial instead of. So that's three factorial instead of over two like you're not is there yeah the graph is a label label versus not yeah do you ever consider like the unlabeled case where you would just divide that coefficient in half because the symmetry or is that like not study? Does anybody in here study that? I labeled my graphs. In general, the unlabeled case is much harder to label. Unlabel case is much harder than the label case. I hope you're having enough problems with the label case. So let's go back to this graph that we had before, because it's nice, it's pretty. So what we can do here is we can expand it in a different basis. Sorry, not the graph, but it's chromatic symmetric function. We can expand it in different basis. So we can start off with this one. We can write down its expansion, the monomial. Is expansion the monomial basis, or we can switch to the expansion in the sure basis, or we can switch to the expansion in the elementary basis. And these are all giving us different bits of information. The first one is the least compact way to represent this, as François was pointing out, because it depends on the number of variables. But once you have enough many variables, then these representations here hold. These representations here hold, and you don't have to worry about anything else. So, some numerology since we're looking at this. You might pull out your computer and start to ask questions, and you notice that the coefficients 28 plus 32 plus 108 add up to the number of acyclic orientations of this graph. You might also note that 28 plus 32 is the number of acyclic orientations of this graph with two six. 108 is the number of acyclic orientations of the graph with exactly one six. Exactly one sig. So we have some nice coincidences, it seems. Well, it turns out it's not a coincidence with a theorem. So if you take a graph and its chromatic symmetric polynomial or symmetric function and expand it in the E basis, then you can compute number of acyclic orientations with exactly J sincs by summing the coefficients whose partition has J parts. Okay? Okay, so it's kind of like what we did in the previous slide. Here we got for two things we have to sum these two coefficients because this partition here and this partition here have two parts each. What's a scientific orientation? Pardon? An acylic orientation. You take every edge of your graph and you decide on a direction for it. So either going from one vertex or the other. This gives you a directed graph. And then you don't want any directed Then you don't want any directed cycles in that graph. So, if our graph is this graph here, which is called the cloth, then this is our chromatic symmetric polynomial. And this theorem still holds. You can check that actually if you want to look at the number of acycular recantations with exactly Number of acyclic orientations with exactly two sigs, you add up these two coefficients here, so you get three. There's three orientations possible. Okay? But one thing we notice is that we do get a negative coefficient here. And so it's an open problem to characterize the graphs for which this chromatic symmetric function admits an E positive expansion. And by E positive, I just mean you write in the E basis, and then you want all the co-expression. You write into E basis, and then you want all the coefficients to be positive. Or non-negative, I guess, is more accurate. So the Stanley-Stanbridge conjecture, which has drawn us all together, is that if you start with the incomparability graph of a 3 plus 1 free postet, then you get something that's E positive. So the takeaway here for this, don't worry about what incomparability graph and 3 plus 1 free postet means, is what the takeaway is we have a The takeaway is we have identified a class of graphs that we think are e-positive. And there's been lots of work on this, and that's why we're all here, is because a bunch of us believe it, and we think we have a way to attack it. It's not one of the seven problems. It's not one of the seven problems, no, because it's kind of, I don't think we'll solve it by the end of the week. So we're going to, again, repeat the same recurring question. Going to again repeat the same recurring theme, except now we're going to change our component gadgets just slightly. So we're still going to look at finite graphs, and we're going to consider, so labeled graphs, and we're going to look at the colorings that we're going to use are going to be positive integers. What we want here is also a total ordering on the colors. And then we can define what's called an ascent statistic of a coloring. So what we do. So, what we do is we look at each edge and we check whether the edge is colored in the same way as it's labeled. Does that make sense? So, we're going to have two vertices i and j. We want the label for vertex i to be less than j. And we want the coloring attached to vertex i to be less than the color attached to vertex j. So, we call this an ascent. So, for every edge, you So, for every edge, you either are in ascent or you're not in ascent, okay? And so, the ascent statistic just counts the number of ascents we get in our color. And then, what we're going to call the chromatic quasi-symmetric function is the sum over all proper colorings of our graph, where we're coloring by positive integers. And we're going to take a monomial just like we did before. And the only thing that's changed here is that we're going to introduce an. Is that we're going to introduce an extra variable. So t to the power of the number of a sense. So very similar to what we did before, but we have this extra variable. So it turns out that this is not always a quasi-symmetric, I'm sorry, it's not always a symmetric function. So this is why we put the word quasi-bear. Okay? Don't ask any more questions. So here's the graph we had from before. So, here's the graph we had from before. There are two ways to color this graph with two colors. So, here our colors are black and white. So, we can do this coloring. In this case, there's one ascent. Because vertex I, so this white color is less than the black color here. And so, this edge, the second edge here, gives us an ascent. The first edge is a descent. And this is another coloring. Um and this is another coloring, and here we also get one ascent, because the first edge is now an ascent, because uh you know the label, the color here, i, is less than j. And we're using the labeling here for the orientation on our edge. So this gives us a whole bunch of monomials that belong to our chromatic quasi-symmetric function. And you could do the same thing here. And I had a little bit of fun drawing these. So these are the six different ways of coloring. So, these are the six different ways of coloring the edges. We have three colors. This one here has two ascents. I mean, this is exactly the same order as this here, and so there's two ascents. This one here is the complete opposite order, so we know there's zero ascents. And so each one of these is going to give us a monomial with some power of t associated with it. And so this is going to give us t squared, one copy of t squared, four copies of t plus one, times some. Sum of monomials in the text variables. Now, if you sum up all this, you actually do get a symmetric function. So in this example, our chromatic quasi-symmetric function is symmetric in the x variables. But this is not always the case. Actually, if you take this example and you change the labeling of the original graph, you'll find one where you don't get a quasi-symmetric function. Again, I stole this example. Function. Again, I stole this example from Michelle. Sorry, that is not a symmetric function. It's always quasi-symmetric by my weird definition of it. So John Charashi and Michelle Wax identified a class of graphs for which the chromatic quasi-symmetric function is always symmetric. And they also conjectured that these graphs are E-positive. Okay? And one thing we'll notice is if you're. Okay, and one thing you'll notice is if you set t equal to one, you recover just the chromatic symmetric function. And so if we can prove this conjecture, we can prove the other conjecture. So it's a generalized version of the Stanley-Stambridge conjecture. Yeah, I was very vague so that I didn't have to say anything like that. But we have to worry about the labeling, right? You can't just talk about the graphs and stuff. Just talk about the graph this stuff as Frege pointed out. That path with one labeling is symmetrical with the pipeline. Underlying graphs of this name, but you also is it the same? Because I thought the standard standard said that 3 plus 1, 3, and for this, is it supposed to be positive just for all 3 plus 1 or 3 plus 1 and 2 plus? And two puts. That's a matter necessarily. So if that's a result of Matthew's, it's right. If you can prove the same semiage conjecture for 3 plus 1 and 2 plus 2 free, then it's true for 3 plus 1. But that depends on a theorem effective. Oh, but even for the t, I thought Matthew proved that for t equals 1. But if you can prove this for general t, that's at t equals 1, you get the case that it's. You get the case that is sufficient to imply the general selling. No, but I'm just asking, I'm not thinking of this one, but let me ask you. I know. So if you just require 3 plus 1, then you don't necessarily have a selection. So that this doesn't apply. I was going to say, yeah, so class of graphs here, deliberately vague because I couldn't talk about everything. Deliberately vaguely because I couldn't talk about everything. You can find the definition in the notes, but there will be people who talk about these coming up. So I think they'll probably come up and allow us talking directly. And yeah, I guess maybe to follow up on Fruit's observation on a class of labeled graphs, right? Yes, yes. Yeah, the labeling of the vertex does play an important role in this world. Okay, so now we're going to change our conversational gadgets once again. So we're going to look at top Gadgets, once again. So, we're going to look at tuples of skew partitions to start with. So, a skew partition, what you do is you take a partition lambda, so 4, 2, 2, 1. You take another partition. Oh my god, why don't we close it? Just get the movement. I move the mouse to the closed button and it disappears. That's really ghost. Oh, I get no, it's awesome. Okay. All right. So you start with a partition. You start with another partition. You take two partitions. One whose diagram is included in the other, like we have here. So we have the dark nerve partition on the inside. And the skew partition is all the cells of lambda that are not contained in mu. Okay, so this is going to be a skew partition that can. So, this is going to be a skew partition that contains three cells. And this is the diagram that I'm going to use to draw. A topical of skew tableau is going to be something that looks like this. So we're going to take a bunch of skew partitions and we're going to draw them on a grid, and we're going to line them up by diagonals. Okay, so this corner box is called diagonal zero, and then you can just move left and right from there. Okay? And then a tableau, a topple of skew tableau, is just going to be a filling of the entries that you have, the boxes that haven't been crossed up. Okay? Is it by accident that all the labels are different? I copy and paste in the example I had, so it's just. And it wasn't accidental in that case. So these aren't even, these are not standard. So these aren't even, these are not standard or skew standard because in this column, for example, it's not increasing. But there is some method to that labeling, but don't worry. But that's what the colour looked like. So now, when we have something like this, let me show the picture, what we're going to do is define something called an inversion pair. So we're going to take two different cells from two different tableau in our tuple. So here we have C below. Tuple. So here we have C belongs to C Ti, D belongs to DJ, and we're going to call this an inversion if, well, the entry in Ti, no, sorry, the entry in C is bigger than the entry in D. So here, like, for example, here's C, here's D. So C here is actually the cell itself and not the entry. So then, because you can have repeated entries, so you So then, because you can have repeated entries, so you want to actually label by cells. So, here's the cell. So, what you want is that the the label in this box, the entry in this box, is bigger than the entry in this box here. And either they belong on the same diagonal, or oh sorry, there's a bunch of different parameters here. So, we're going to take two things from two different tableau, right? So, we can take something from ti, something from tj. From Ti, something from Tj. So if i is less than j, then we call this an inversion pair if the entry in C is greater than the entry in D and they belong to the same diagonal. So you can compare something, a cell in one tableau with a cell that comes later in your tuple, if they're in the same diagonal. Or you can compare a cell in a tableau with something that comes earlier in your tuple. With something that comes earlier in your tuple, but there you have to shift diagonals. So you have to look at the previous diagonal. Okay? All right, just like the picture kind of like, you know, tells you, we're doing this weird thing where we're comparing cells and we're going to call these inversions. We're going to come back to this and we're going to reformulate this in a second. But this allows us to define what's called the LLT polynomials. So you start off with a tuple of skew partitions and you're going to sum over And you're going to sum over all tuples of semi-standard skew tableau of the given sizes. And then you're going to get a monomial. And so the monomials that we get here, we're going to have x variables. So these are just going to be the usual ones that we associate with the tableau. And then we have some parameter t, which is a statistic that depends on the whole tuple. So each of these monomials, like you can factor them so that each one depends only on the individual. So that each one depends only on the individual tableaus in our list, so like T1 up to TK. But the T parameter is like mixing together the information. So it turns out that these polynomials are symmetric in the X variables. And here's an example of one. This is what one looks like. So it has like some, you can expand it in the share functions. And some of the parameters will be T's. Okay, so just Okay, so just sort of just to give you an idea, what you kind of look at. So we're going to be interested a lot, I think, this week in the unicellular case. This is what's going to actually make the branch to the chromatic symmetric functions. So in the unicellular case, we just look at the skewed partitions that only have one cell. So this is aptly named unicellular, one cell. So here there's only one cell, here there's only one cell, and so on. Cell and so on. Okay? So this is like after you do the skewing, there's only one cell. And what we can do here is actually we can define a graph. And so what I want, this graph, I want this to remind you of this picture that we had here. So we have some edges here. So I want these edges here, I want these to define a graph. And so that's what we're going to do here. We're going to look at all the And so that's what we're going to do here. We're going to look at all the possible places where we can have an inversion. So, like, if you're in the same diagonal, you can be an inversion pair, but only if you're going this way. And if you're in adjacent diagonals, you can also be an inversion pair. But this is only using those two conditions. It's not actually comparing the entries. But we get a graph from here. And then you can remark that, well, what is a semi-standard, sorry, a taboo? Sorry, a tableau. So many words here. A tuple of semi-standard skewed tableau in this case. But because we only have one cell in each of these skewed partitions, all we're doing is picking one element, one positive integer for each of these. So you can think of this actually as a coloring of our graph. Because the vertices are going to be these unique cells. And we're going to put in one color. One color, one label on each of those vertices. But it's an orient oriented graph. Yes. But we want it to be labeled. And so now we can go back to what you were asking before, like why are there those numbers there? So actually, this is kind of like the way I want to read my cells to give it a labeling. I think all you need to do is just look at the orientations that you're going to talk about. Look at the orientations are going to tell you which one's bigger than the other. So, when you're comparing, when you're doing an inversion pair, and also when you're comparing colorings, you need to know which vertex is bigger than the other. And so, these arrows should be enough to do that. And so, this inf statistic that we have to find on tuples of skew, tuples of skew tableau, equals the ascent statistic of the associated coloring now. Associated coloring now. So we kind of set things up and we picked our graph appropriately so that this inversion statistic just equals the ascent statistic of the underlying, of the, I don't know, underlying graph, associated graph and the coloring. So this gives us a nice little proposition. If we start with a tuple of skewed partitions that is unicellular, then we can write our LLT polynomial as the sum over all colors using this. Using this, and then you know, the same t-statistic, but instead of in, we use the ascent and the same monomial that we had before. Okay, so we're just rewriting this. Instead of saying summing over all tuples of semi-standard skew tableau, we're doing a sum over all colorings of certain graphs. So here's an example. So if you want, you can actually recover what we did before just by picking your skew partitions appropriately to get the graphs that we wanted. Really, to get the graphs that we wanted. That we had before. Like for this one here, we can look at this sequence here. So this is starting to build. Yes. So it seems like something, since you're only unicellular. Yeah, it does it does not matter like you think you might as well just we could have put all we could have pushed these things all like over because we're kind of like we're looking at diagonals like pairwise that like pairs of diagonals and so if we just skew each one by a lot more just push everything over that's fine or push everything up yeah so there's a lot of a lot of freedom a lot of freedom maybe you just said this uh Yes. Maybe you said this, maybe I said that, but what kind of graphs do you get from these? Oh, there's a there's an answer. You get the 3 plus 1 and 2 plus 2 avoiding ones, the same as the installing stem each. Okay, exactly those? Yeah. Oh, wow. Okay. So you can't get other graphs. I asked more for something. Yeah, yeah. So there's like a cat on a number of things. Oh, wow. This is gone. Can you say all colorings? You mean all proper version of colours? All colorings, not necessarily proper. No, this is all colorings. This is a different symmetric function. This is a different symmetric function than what we had before. So it's not just proper colorings, but it's all colorings. Okay. To ask about, again, this one, when you pass T to 1, is it for all? Is it for all the graphs we're considering that you get the regular representation? Yeah, so actually over here, that's over what we see here, I think. This is the regular representation for S3. We get one copy of the trivial, one copy of the sign, and then two copies of the other one. Each square is independent, so it's multiplying E1 ten times. Yeah. Yeah. But is that for every graph? Actually, for every the LLT polynomials, let's see, if you go back a little bit here. A little bit here. Imagine setting t equal to 1. I had actually written this, then deleted it, but thanks for reading that. Set t equal to 1 here. Then what you have here is a sum over tuples. And these tuples, like the monomials, don't interact with each other. Like the t parameter is forcing them to interact. So once you get rid of the t parameter, you can actually write this as a product. And it's going to be a product of sure functions because a sure function is your summing over some. Sure function is your summing over semi-standard young tableau of a given shape. So this will be then the product of sure function, of the sure function indexed by mu1 times that indexed by mu2 and so on. So in the unicellular case, these are just going to be one box. So it's going to be S1 times S1 times S1 times S1. Skews your function. Skews your function. Thank you. There were two questions. Thank you. There were two questions. I think Juliana had her hand first. You just argued to us that that's the regular representation and that what the graph, the choice of graph is just changing the gradient. Yes. Yeah. Yeah. The underlying representation, I mean, if you just forget the T, you're always going to just end up with the regular representation. So it's just a question of how things are graded. And this, there's a lot of freedom and a lot of There's a lot of freedom and a lot of interesting stuff that happens. Yes. So more generally, we're looking at a very specific case here, the unicellular case. I won't say too much more about the actual LLT polynomial and why they're interesting, because this afternoon there's going to be tons of presentations about them. Presentations about them, like in the terms of representation theory and so on. Can we go forward one slide again? This is kind of backwards for everyone who's going to do this. You have three of them. Forwards? You had three shapes. Oh, maybe. Okay, yeah. So if we made the middle shape one one slash one. 1 1 slash 1, then the white square would be above the diagonal, above the dark one. And that would have. 1 1 slash 1. 1 1. Oh, 1 1 slash 1. So the cell would be here? And that would affect the arrows, wouldn't it? That would definitely affect the arrows. Yeah. Yeah. Okay. So this would. Yeah. So to follow up on Bruce's question, what difference does it make how you're skewing? So there is something different. Oh, it's true. I guess what I was saying is that you have some. What I was saying is that you have some freedom in that you can skew everything, do like a common skewing to everything, and that won't change because you're just pushing things around, and that's not going to change the relative positions, and so the arrows will be the same. But if you push just one thing, a couple of diagonals over, that's going to break the the air the edges that you get. Yeah. So if you move one box, like the rest fix and move one box, then And move hardbots. And at some point, it's to the left, there's no inversion, it becomes, interacts with the other, and then it stops interacting. So there's a fun story. You move one parts. Yeah, so like, exactly. So let's consider this one here. We start off, I guess over here is fine, but as you push this further and further and further, as you skew this further and further and further, it's going to break all the edges. It's going to break all the edges that you have. Like, you're not going to have any edges. And so, the interaction between the cells is going to just break down. And so, if you think about this thing, actually, just like a nice animation, and you see this partition moving along, it's going to start interacting at some point, and then it's going to stop interacting afterwards. And so, yeah. It'd be nice if someone made that animation. It's also true that if you move one, the rightmost partition to the leftmost. Partition to the leftmost and shift the diagonal, and it doesn't change. The right most? Take this one, bring it down here. So Francois mentioned this to me at one point. He said, When you're thinking about the in statistic, we'll go back to this picture here, you can kind of think about it as, if you think about it. Think about it as: like, if you think about going off in one direction and then coming back, wrapping around the other way, you're when you're looking at what you're comparing with, I guess maybe I want to go like this or something. So I can compare with this cell, I can compare with any cell that comes later, or I can wrap around and compare with this cell, and so on, up until I get to this point. And so this kind of just explains that cyclic symmetry that you're seeing. That you're seeing. There's a lot of skew, I guess to summarize, is that there's a lot of tuples of skew tableau that are going to give you the same thing. Any other questions about this? Ask the Mariana coffee break. Oh, I don't know. I feel like it's going up. Where do we, where are we? Oh, yes, here we go. So, next theorem. So, it turns out. So it turns out that if you start off with a unicellular tuple of skew partitions, then there's this nice relationship. You can actually pass from all colorings to proper colorings using an algebraic operation. So this is the LLT polynomial. This is the graph that we have. And so you have to use some algebraic operation to kind of do this swap. And so, what is this algebraic operation? Well, it's called the plethistic substitution. And then, here's a definition which you can actually work with. You take a power-sum symmetric polynomial, and you want to do the substitution here. You want to substitute x with t minus 1 times x. So, all you need to do is multiply that by t to the k minus 1. That by t to the k minus 1. So you multiply by that power sum by 2 to the k minus 1. Unless I mirror that rule. And so the important bit here is that you have to be careful. You can't just do this in any basis. You have to switch bases. So if we want to compute this in this case here, we have to first start with the shear function, switch to the power sum basis, do this substitution, and it's actually multiplicative, so you have to use that fact too. So, you have to use that fact too. And then you could switch back to the share basis if you want. And then you end up with your chromatic symmetric function. Oh, and you have to divide by a certain power. And then you get your chromatic symmetric function. This operation is very interesting from representation theory perspective. It's a little bit bizarre when you try to describe it just in terms of symmetric functions. But you can see my notes for more information. You can see my notes for more information. Or you can ask questions afterwards during the cover break. Oh, yeah, more details in the notes. I do have other slides, but I don't know if I should continue or stop here. We did start late. We started like 10 minutes late, so if you have. Well, there are questions. What I was going to ask, well, I'm curious to hear your representation theory explanation, because someone has struggled with this concept. But that could be a coffee break. But what I wanted to know is, is, you know, like in the dog action world, Like in the dot action world, if you go to the coefficient of t, then the dimensions match. Is that clear from the platistic transformation? So like the dimension of each piece, well if you if you think of the sure, switch from the sure polynomial to the dimension of the corresponding irreducible. So in other words, like it's one, four, one in this example on both sides. I'm just wondering if that's clear from the I'm just wondering if that's clear from the this picture. Oh, um sorry the coefficient of t on both sides of the degree of each one or I look at the sum of sure and I convert that into I don't know how to say it in certain ways isymmetrical. I I go from like dimension of sure S lambda and I replace that with the number of standard young tableau of lambda. Yeah. This might be a question for Coffee Break. Okay, coffee. Because we can actually write down two, we can write down the two examples and look at them and compare the point, like which coefficients you're actually talking about. Okay. You're asking if the graded dimensions are the same? Yeah, great dimensions. So then if you just look at the value with the identity and stare at that for a minute, you see that. Yeah, it's really true because you're multiplying by t minus one to the k and dividing by t minus one to the k. Okay, perfect. Oh, oh, so just like because of this okay, yeah, that seems to work. Yes, it did. Yeah, so I gather there's some E positivity conjectures for LLT polynomials somehow, and they're somehow related to the. I don't know if it's easy to say what some of these conjectures are. Yeah, so I was going to leave that for people this afternoon, but maybe like just That for people this afternoon, but maybe, like, Jim, do you want to say something quickly? I don't know anything about E-positivity, but they're known to be sure-positives. That's the theorem. But there's no known combinatorial form. You've got like sure positivity with unknown combinatorials in the LLT, and then you've got E-positivity, unknown combinatorius. Unicellular case? Well, so there is a way if you go back to the LLTs that you have, then if you replace uh T by T plus one here, you get something which is You get something which is and they behave like incremental spectral functions in the sense that you get a sum over all orientations now, not your specific orientations. And in this case, we have a collatorial form. So that's. But it's an analog, there's no direct implication of one? No, because we have these flattens of one side just because you have it in the multi-site, unfortunately. Alright, so I see the underlying question here. The underlying question here is: We want to prove e-positivity here. Can we do something here, like e-positivity here, to imply that? And pleptism is weird. It's probably that's the short answer. And we don't quite understand, I guess, like, yeah, I mean, when you get a positive result from plethism, I don't know. So it falls into the world quite a bit. Yeah, it's not e-positive, the example that was given. Right, so you're going to want something else. Else some other property on this side that would map onto E-positivity. P positivity. P-positivity. Well, that maps, that's easy, actually. E-positivity is referred to. On one side, so because you brought up the Hessenberg, it kind of matches up a little bit with the other slides I had. So it's like, how do you get symmetric functions from representation theory? So quickly, a representation of a group is a morphism. Representation of a group is a morphism from our group into GLE, so general linear group of some vector space. We fix the basis and we get a map into a group of matrices. And the character is going to be just the trace of the matrices that we get. So this is very standard kind of basic representation theory. You know the definition of a representation, you know the definition of a character essentially. But what we can do is we can use this, like the character values. Use this, like the character values of characters and representations of the symmetric group to get symmetric functions. And this is the idea. So we start with a representation of the symmetric group. Let's call its character chi. And we're going to call the Frobenius characteristic of our representation, or of the character, to be this symmetric function here. So here we're using the p basis. So this p here is for the power sum. So, this P here is for the power sum symmetric functions. Cycle type of sigma. So, given a permutation, we can decompose it into its cycle type. So, this gives us a partition. So, this is going to be a symmetric function right here. As a coefficient, we're going to take the value of the character on that permutation. We're going to sum up overall permutation, divide by n factorial to normalize everything. So, this gives us a way to associate to every representation. Associate to every representation of the symmetric group a symmetric function. So we could do this, for example, using the dot action on Hessenberg varieties. So you can associate a symmetric function with this dot action on Hessenberg varieties. When our representation is graded, we can take that into account as well in the Frobenius. So basically, our representation can be decomposed into a direct sum of subspaces. Of subspaces which are each equipped with the group action. And so, what you do is then just use a certain parameter t to encode the dimension of that space, essentially. Or the grading, sorry, not the dimension, but the gradient. So here we're grading it by d here, and so we're going to put t to the d there. Otherwise, we just use the same symmetric function. So the result of this will be something that lies in, say, the algebra of symmetric functions. Algebra of symmetric functions where you can have t polynomials and ts coefficients. So this allows us to pass from representations to, or greater representations to symmetric functions and symmetric functions where the coefficients are polynomials in t. Or series. Or series, yeah. Yeah, this was more general. So the big result in this area, or in this This area, or in this world, here is that if, so let's first let's Cf be the algebra of characters. This is also called the class functions of Sn. So we can take a direct sum over n here, and we're going to get this big algebra. We can multiply characters and add characters and so on. And then sim here is going to be the algebra of symmetric functions, again over all n. The Frobenius characteristic map actually sets up an isomorphism between these algebras. So multiplying So, multiplying characters on one side corresponds, in some sense, to multiplying symmetric functions. I didn't want to say shear functions, but I wanted to, but it's not right. So, multiplying symmetric functions. And it turns out that if you start with an irreducible representation or a irreducible character of the symmetric group, then its image under this map is the shear functions. So, this kind of explains why shear functions are interesting from a representation theory. Are interesting from a representation theory perspective. The h lambdas and the e lambdas also come from certain representations. And then if you have two characters, well, there's two things you can do. You can compute their symmetric functions and multiply them. Or you can multiply the two characters and then induce up to another symmetric group and then compute the symmetric function associated to that. And so this is how the two. And so, this is how the two operations are related. So, product of symmetric functions corresponds to induction product of characters. And then it also turns out that there's a way to define a inner product on the algebra of symmetric functions. And if you do that, this corresponds nicely to the inner product that you would get from representation theory of characters. So, this map for Venus, we're going to see a bunch, I think, throughout this. We're going to see a bunch, I think, throughout this week as well. So I just wanted to briefly bring that up as well. And I think I'll stop there. Before it starts beefing and yelling. It never really did. Well, it hasn't. It's not time yet. Any questions? Any other questions? Yes. I had a question from before. When you did the LLT substitution, it could be in a cellular case. What happens if you do that same substitution? What happens if you do that same flatistic substitution and divide by t minus 1 to the k for a non-unicellular LLT? Is that meaningful, interesting? Does it look nice? Well, I think that it's not always divisible. You can't always, like, the coefficients are not always polynomials in the end. I see, I see. So I think this is going to be special in this particular case. Okay. But I don't know of other ones, so maybe that's a good point. So if you take. It's hard to hear your question. Oh, I'm sorry. I'll just repeat. Oh, I'm sorry. I'll just repeat it. I didn't speak up before I was. It's okay, I'll repeat it. So the question was: when we had this case here, we're taking a symmetric function, we're doing this plethism, and then we're dividing here. And so the question is, can we always do that? Well, I guess the answer is yes, we can always do that, but the problem is that, again, for any symmetric function, but the problem is that the result is not necessarily. Is that the result is not necessarily polynomial coefficients. You're just going to get rational functions as coefficients in general. And then the question was: I guess also one could ask, are there any other interesting classes of functions where dividing by t minus 1 to the k gives you something meaningful? And I think Pera was about to comment on that? Oh, yeah, I mean, it's basically only the family of unicellular graphs that this makes sense. Makes sense. You can extend unicellular graphs to some bigger families, but this relationship doesn't. You can combinatorily interpret LT as a sum over non-attacking sign fillings. You can't divide by that and still get there are other folks who have some. But I thought Anne was a similar question. What what if you It's a similar question. What if you, instead of enumerating over all doubles of double load, enumerate over those which satisfy this further assumption that if two cells are comparable, then they have to have distinct entry. Well, if you do that, then yeah, I think you're subming over the proper color. Yeah, but not on the indicellar case. Well, just the general case. So here. So here we constructed this graph to kind of like make this connection. So he's saying let's drop the case where we're unicellular, so we're not other things. And then also when we're computing in and so on, let's assume that the two entries are distinct. So you're kind of like summing over proper fillings in some sense, proper colorings. But in the non-cellular case. Non-unicellular case, thank you. And so then the question is, what happens there? Is that some variant of the chromatic metric? And my answer is I have no idea. I'm not sure that's a metric. So what if you paint just a graph and do the non-property color rings? So it's not the same. Color rings. So that's not necessarily an LLT polynomial unless it's a 3 plus 1, 2 plus 2, 3. And error to the quasi-symmetric function for the same graph. Can you... What happens then? Has that formula. No, that doesn't work. That doesn't work. It doesn't even work for the circular case in the cellular. So it's kind of like somehow this algebraic operation. Well actually it can make sense of that for a positive symmetric positive operation. I guess one way to interpret the question is to say we have this algebraic operation which is essentially say allowing us to pass from all colorings to proper colorings in this particular case. Can this also work with other graphs? Other graphs? And the answer in general is no. Is it possible there's something in between? Like, do we know which graphs or which this would give us something interesting? Because you could define this operation here. I mean, there's two ways. You can go from LLT and see what happens, what you get inside that. You can do chromatic and go to something and see what you get. So there's two questions now. Because the equation is not true. Right. So if So it depends on what you want. I mean, that would be the interesting thing to do. Yeah, I think that actually. Could you change the T minus one, getting back to Maria's question, right? Maybe the T minus one has something to do with unicellular, right? And you just replace it by some other function, T, and it works in person. So that is an interesting question. I think that I think that my reflex here would be to look at the geometry a bit more, because there is actually a geometric explanation of what's going on here. We've alluded to the chromatic symmetric function here being the Frobenius characteristic of some representation. It turns out the same thing is happening on the other side, but it's a different variety manifold. And then you can actually interpret the Then you can actually interpret the plethism as some operation that's happening there. So I think I would look there first to see what kind of things one might be able to do that make sense geometrically, that might give us an inkling of what might happen here. I think that's a that's a nice question as well. Other nice questions that we should think about this early? If you take the limit as t approaches one, does that say something about the normal chromatic semantics? Something about the normal chromatic selector function, not a photo symmetric. Anyone? If you take the limit as t goes to 1, does that say something interesting about the normal chromatic symmetric function? So here you can't just set t equal to 1, but you can take the limit on that side. Is that telling you something interesting here? There's another way of thinking about this setting t equal to t. about this setting t equal t plus equal side then the right hand side becomes easy to understand and then you can set t equal to zero wouldn't you get the same limit on the left hand side but if the limit as t goes to one it's on the right hand side but this there might be some combinatorics here yeah that are Here that's revealed in the limit that might be telling us something interesting on the other side. I think that's the intuition behind the question. I think that maybe we should have other questions like coffee so that we have time to answer coffee. Thank you.