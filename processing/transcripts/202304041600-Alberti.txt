Let me first thank the organizers for the invitation. I've never been here before and it's really worth the time. Okay, so this is an ongoing project with Andrea Marquez and Andrea Merlo and possibly other people, depends on who gets involved. Because when we started this, it was just Andrea Merlo. Okay, anyhow, so and it's an ongoing project because we don't have any final answer to anything. Okay, so we have only partial results. Results and but let me okay, so the conjecture is a Banish mask conjecture, which apparently can be called in different ways, and maybe and okay, anyhow, the way I know it, it was formulated by D in the mid-90s. And I mean, it has the motivation was optimizational life structure, okay, which can't it's not an unreal. It's not an unrelevant point, this one. Okay, but let me go on. What I want to do is to explain the meaning, at least the geometric meaning of this conjecture, and give a few partial results. And about this, it's important. There is this work of Baba Jana by the organizers. Essentially, what they do as if you l look at the at their results, seems to have no connection with what I'm going to say, but there is a connection. I'm going to say, but there is a connection. And the connection is exactly the optimization of light structure that motivated the conjecture. So essentially, the story goes like this. If this Vanishing Mass conjecture, in the formulation I gave, is true, then it can prove a certain formula for the relaxation of certain energies. In general, what the organizers did prove is that this... Organizers did prove that this formula is correct for a specific energy. Now, what we somehow believe, I'm not sure it has been proved, is that if you really prove their result for all energies, you get exactly the Vanisch-Marx conjecture. So, the conjecture I'm going to say can be formulated in a geometric way, that there is some kind of dual formulation, and what they proved is the first step in the proof of the dual formulation. If they can go on, and it's not clear how much they have to go on, but still. How much they have to go on, but still, that might be the way you prove the conjecture in full generality. Okay, so that's how the connection is, where the connection is. So we really have to go to the problem of optimization of light structures, which I will not touch today because I look at the conjecture from another point of view. Okay, so to simplify the talk, I will restrict to dimension two. So this is about vector field in dimension two. Vector field in dimension two, not general dimension. And the other thing is, these vector fields will be divergence free. Otherwise, there should be some bound on the divergence. But divergence-free is already interesting enough, and in any case, we can't prove anything, so why bother generalize? If we could prove the results for emergence preventative, then it could be the right question to go beyond that, since we can't. Okay, so let me go quickly. Who are the main actors in this problem? Our divergence-free vector fields in the plane. Okay, and I will consider today two classes. I mean, regular vector fields, which are perfectly fine, but actually to understand the geometric meaning, maybe piecewise constant vector fields are good to look at. They are somehow more interesting. Okay, geometrically, you see better what Geometrically, you see better what the divergence constraint does. So, what does it mean the divergence is zero in case of a constant vector field? It means that the normal components of V across the discontinuities, the discontinuity lines, is constant. That's what divergence equals zero. So, for instance, if this is this wise constant vector field, you have different areas here where the vector RS here where the vector is constant, okay, and this you can consider this discontinuity here, okay, and cross this separation line. This you have two vector fields, the two normal components are the same. Okay, that's what it means. Now, there will be pictures in the rest of the talks, and the pictures you will see something like this. So, how should you read this picture? The blue part represents The blue part represents the support of our vector fields. So y means that the vector field is zero side. Here where it is supported, and it will be piecewise constant vector fields. And yeah, the direction. I mean, the vector represents the vector. So it's important to remember that somehow there is outside this is simply the vector field is zero. Okay? So now So now the other example of vector field that is important is the singular one. Now what do I mean by singular one? Singular vector field, I mean that my vector field is actually given, I call it V, but it's actually given by a singular measure, mu, times a unit vector field. Okay? And okay, what is an example of this? Well, the simplest example. Of this, well, the simplest example is when your singular measure is just the length measure on the curve, C1 curve. So you have a curve, a length measure on it, and then you have a unit vector. Well, at this stage, the vector could be oriented in any possible way. And then the first things you see, okay, so in some sense, your vector field is supported on a curve. If you want, think about a limit situation where the support was thinner and thinner, and in the end it becomes just a curve. But the total mass, that the L1 norm of the vector field doesn't vanish, of course. Okay, so now what does it mean that the divergence is zero? It means first that this vector has to be aligned with. Spectral has to be aligned with the curve either in one direction or the other. And the second thing says that there is no boundary. Because if you have something like this, so it means that there is, okay, one way of thinking about emergence-free vector field is, for instance, can you imagine that they are the velocity field of a steady flow? Sorry, of a steady flow. Of a steady flow of an incompressible fluid, right? So if you have something like this, it means that you have a river going from here to here. Here there is a well, and here there is a sink. And that's, when I say divergence-free, means that there is no well, there is no sink. So essentially, it's a flow going around, and water is not generated, water is not dispersed. Okay? So, geometric, and now you can see that this divergence. And now you can see that this divergence-free condition is very geometric. It's telling you something about the geometry of the. Okay, now what is. This is what is known for this very specific type of singular vector filter. What is known for a more general one? So when μ is a general singular measure, so something which is more complicated than just length on a curve. Lengthen curve. And then what is known is the following: that first of all, this measure is zero purely and rectifiable set. Now, this is technical. If you don't know what I mean, it's never mind. I mean, just think about this, okay? So, but it cannot be, for instance, it cannot be sitting on a point. So, a point cannot support a divergence-free vector. A Dirac mass. A deal of mass counts, but there is divergence effective. Okay? And so, for instance, a curve has this, the length measure on a curve has this property here. Then the second is that again, the vector, the direction, tau, is aligned with the specific tau mu, which is now I'm not going to define, but it's Now, I'm not going to define, but it's a vector field direction which is associated to the measure mu. So, let's say just the measure mu tells you what is the direction of the vector field. Okay? So, you don't, for instance, if mu is Lebesgue measure, of course you have nothing like this. Lebesgue measure, I mean, your vector field could be the constant vertical vector field, that's divergence-free, or the horizontal one, and that's also divergence-free. So, the direction of the vertical field Verge free. So the direction of the vector field is not determined by the measure. When you have a singular measure, this result essentially says that there is a direction which is somehow determined just by the measure and that your vector field must be aligned with that one. Okay? And this is a result, it's a consequence of you have to put together something I did with Andrea Matisse some years ago and this round point theorem for Theorem for derivative of B B functions that I put many years ago. Okay, so this is what is known. By the way, if you want to go to higher dimensions, there are similar results, but in that case, you need the alternative way to go toward nature. There is no y you have no proof in that way. Because you have to prove that no, I I looked at that. You can't prove it like that. This it doesn't work. I tried that, by the way. It's this, but it is also not followed from Smirnov's composition. In a way. It's not a matter. No, this the only The only no no no this is the only generalization okay it does not follow from okay there are proofs I mean this statement here is equivalent to this random property. It's the same statement just by rotating vector field by 90 degrees. Right. Okay, because divergence really becomes curve-free then. So it's the same. There are other proofs at the moment. But the other proofs are even simpler. But okay, the only generalization I know to IO dimension is the To I automation is the paper by the result by Philippe and the Philippis. And it doesn't follow from. No, it doesn't. But your mu is not a matrix, yeah? Your mu is just a vector field? At the moment, it's a vector field. Just one single vector. T is not a measure of a vector field. Mu is a measure, it's a positive measure. It's a positive. So, this is the polar decomposition, right? So, this is a polar decomposition, right? Measure and unit vectors. That's what I'm doing. You mean that the tunnel space? Why is that point of view? Well, that the tangential space, I'm saying that it exists if and only if the measure is singular. Okay. Let's we can discuss that later. You have to agree because you are the chairman. Do you agree? In the previous ones, yeah. No, what I'm saying is that there is okay, we don't have to make. Okay, without entering into many details, there is a vector field here that is determined just by the measures. And whenever you have a, you take μ, you multiply by tau, and you get something determined as free, then you know that this tau must be aligned with this specific taunt. So the direction is determined by the measure, which, for instance, for Lebesgue measure is not true. Lebesgue measure you can multiply by this direction or this direction, and it's still the verge of screen. While for a curve, it's what I say. While for a curve is what I said before, right? The vector field must be tangent to the curve. That's the kind of result. For curves, it's an easy computation, and in this case, it's not so easy. It holds for every measure even if it's not the right time. If it's very complex, that's why it's faster than classifiable. But in terms of the specification, written this way, it's not clear. I agree with you. I can give more details later, okay? But yeah, no, I agree with you. This statement is a bit vague for me to put this. Formula qualities. Okay. It's what I'm trying to say is that the direction, the measure determines the direction, which is true for curves, for instance. The direction is the tangent vector to the curve. It's not true for the back measures. Okay? And it doesn't follow... Okay, bar. Okay. Let's skip this. Okay. Then, okay, what is the setting we're interested in? So this is the We're interested in, so this is the basic situation. We have a sequence of vector fields on, let's say, the unit square, which converge weakly to some vector fields. Okay, weakly in the sense of measure, to some vector field B, which could be associated to a singular measure or to an absolutely continuous one. So essentially, in the original, you could say, well, you simply assume that the L1 norms are uniformly bounded. Then after some sequence, you have a weak. Subsequence, you add a weak limit, right? So I go directly to the assumption that there is a limit. But what is the assumption there maybe is this? So this is a sequence of vector fields uniformly bounded in L1. There is a weak limit. The divergence is zero, okay, for each one of them. And what the vanishing mass assumptions is that if I look at the support of this vector field, then I look at the measure of the support, the area. The subport, the area, the area of the subport goes to zero. So these could be absolutely continuous, I mean it could be regular vector fields, but supporting on sets which are smaller and smaller. Vanishing mass in this context means that. I mean that the vector fields don't vanish because they are uniformly bounded as say in L1. The L1 norm doesn't go to zero. There is a good limit. But the subport Would limit, but the support is getting smaller and smaller in area. So, that's the situation. And okay, I've not told you yet what you want to see or what you want to claim, but this is what this is always about this. So, vanishing mass means that support is smaller and smaller in term of area. Now let's consider the case where the measure mu here, the limit, is singular. The limit is singular. Okay? So, for example, you could imagine that you have a vector field which is even a smooth one supported in a stripe like this. And the thickness of the stripe goes to zero. So, it's a strip, not a stripe. Strip. Okay. So, this becomes So, this becomes thinner and thinner, and in the limit, you get aligned. Okay? So, the vector field in the end, okay, of course, the models must grow in order to vanish, but okay, that's what you want. So, that's one example. By the way, I didn't say before, but um in this situation you can assume that these B n are smooth. Not the limit, but the B N could be smooth, could be very regular. That's what Be very regular. That's what typically one could be interested in. Okay, so let's assume for the moment that this is the picture. So they are smooth, but supporting on sets that becomes thinner and thinner, and in the limit you get a line. Okay, and in this case, we know that the limit direction, so the direction of the limit vector for tau must be aligned with the tau mu. Okay? So this means also that the approximation. Also, that the approximating vector must be aligned with term, at least in average, because there is a weak convergence in between. So, what do I mean? In average, I mean that if I look at the integrals, for example, of this Vn on both, then I converge to the integral of this tau mu on both with plus or minus. Okay, so that's in average because actually I'm not saying that Vn is convergent to V mu, only that the average. Is convergent to 10. New only that the average of both. Well, for every x, almost every r, this is one way of saying it. I mean, you can choose the one you prefer. This is just reconvergence. So let me give an example. Imagine that this is the situation. So the blue part is the support of the vector field, which is somehow this whole stripe, the thickness is 1 over n, it's going to 0. And the vector field is alternating these two directions. Alternating these two directions, and then when you pass to the limit, sorry, you get that the limit vector field will be on this line, okay, and will be the one directed right inside. You see in this case that alignment occurs in average, okay, because these two directions never gets close to the horizontal one, but in average they do. Okay, so that's what the situation that you're must have in mind. That you're master in mind. So, what I say, okay, so if μ is singular, we see some alignment to some vector that depends on the measure, the limit measurement. Okay, now let's go on. What happens if mu is an electr measure? Absolutely continuous. So, um let me give an example. Because I said that there is this valision mass conjure uh assumption, right? That the sub forces getting smaller and smaller in larger, but still. Getting smaller and smaller in larger, but still it can get more and more diffuse. So, for instance, it may be that this is the situation. The blues, the blue screens are the support of my vector fields. So, the total area, if you compute, is 1 over n, it's going to 0. But in the end, they will be, I mean, the limit measure will be. So, it's getting smaller and smaller than narrow. It's getting smaller and smaller in area, but it doesn't go to a similar thing. So, for instance, this is one possible situation, or this one also, similar situation. You have a grid of this time. Now, if you try to play a little bit and try to draw examples of divergence-free retrofit with this support, what does it mean essentially? The constraint, essentially, says that your vector, if you have, let's find like this. Because that would mean this is against the verge-free constraint. Because when you go at this boundary, you have non-continuity. I mean, the normal component of the vector field is not preserved. So what's really happening is that, in some sense, if you draw the integral lines of your vector field, they stay inside the blue area. The blue area. Something like this. So just imagine instead of drawing vector fields, you draw integral lines. These lines don't get out of the blue area. And then you see that in average, there must be some alignment, right? Essentially, what you expect is that here, in average, the vector field will align to a horizontal direction in this case. In this case, though, it's a bit more complicated, because the parts of the vector field inside the vertical The rectified inside the vertical boost. Okay, so the part in the vertical pieces will align with the vertical direction in average, while the horizontal uh pieces will contribute with uh something aligning with the horizontal one. Something aligned with the result or not. So the difference is that in both cases at the limit you get Lebesgue measure. But in this case you see alignment to a very specific direction. In this case you don't. Because there are two components. One piece is going to be aligned to the vertical line, the other piece is going to be aligned to the horizontal line. But of course then it's not clear what you're stating. So what is the statement? So it's clear that there is something that happens and that we would like That happens and that we would like to capture, but it's not exactly clear how you state. In this case, it's easy what you state, but in this case, it's much less easy because, I mean, if I look at an average in every ball, I see both the vertical and the horizontal parts, so I see no alignment. So if I look at the average, so I should split my sets in two pieces, the horizontal pieces, the vertical ones, and they separately contribute to a vertical alignment and a horizontal alignment. But together Result of the line, but together I see nothing. Now, this is a simple case when it's clear how you split your sets. But now, assume a general situation where you have a set of small art. You don't have such a clear division, horizontal and vertical paths or whatever. So, this is where somehow the vanishing mass conjecture comes. So it's a way of stating this alignment. This alignment. It's a way of capturing this idea that things should get aligned in some way. Okay? And the statement is a bit complicated, so let me give some time to explain, but the idea is essentially the following. Since, think about this case, there is no, if I look at the final measure, it will be Lebesgue. There is no preferred direction for Lebesgue. I mean, Lebesgue measure is. From the bank. I mean, the bank measure is isotropic, right? There is no specific direction. So when I try to state an alignment, so I make a statement about alignment, I don't have, I can't simply say my vector fit gets aligned to a certain given direction because there is no such direction. So, since I don't have a candidate for the alignment, what I do and what I want to ask is that. Why do you say that? I'm confused. Why would it be just two different directions? I guess I started first. Okay, see the screen? It's a similar question because really what you have is a field. So when you talk about the bank, it means that you have the vector field, I mean the vector field design in each point. Okay, there would be direction would be given by the vector. So the direction would be given by the vector field. It might be constant, but it's a given vector that is the one. So okay, what you should imagine here is that there is also a vector inside here, which I didn't draw. This vector is huge because it has to compensate the fact that the start is going to zero. But never mind that. You pass to the limit, you get a vector field with presumably, I mean in many examples, will be Lebesgue times. What I mean is that the first feature converts. The first picture compares to E1 on the bed measure, and the second picture to E1 plus E2 on the PET measure. E1 plus E2 if you decided that this vector field could be circular. I mean, you could have that the integral curves. Yeah, but I mean there's a certain curve and then the constant. In the simple case, we should make it constant in the one dimensional. In that case, you get two edges, whatever. In that case, you get two. But in principle, with something supported on this square part, I can get any detail. So I don't have a final direction which is coming from the measure. The measure gives... I mean, Le Deck measure doesn't have an intrinsic direction. It's a sympathetic. So the idea of the statement of push t is essentially to say, well, if I can't say that this vector fit gets aligned to something. This vector field gets aligned to something. I simply consider couple of vector fields and I state that they get aligned with themselves. So I don't know what they get aligned to. So he said, well, let's consider couple of vector fields. And when you take couple of vector fields, he tries to say they get aligned. Instead of considering a sequence of single vector fields, you consider a sequence of couples of vector fields. And then you say something like they get a lot. And then you say something like, they get a line among themselves. That's a way of saying something very weak, but it's one, I would say, the only one, the only way to say that there is only one direction in the end. So they must get aligned. You have two of them, and they must get a line between themselves. Okay? A line to what in the limit? It's not clear. Right? But okay. So. So, and okay, so we have to change the setting because we want to prove alignment and we have to consider a couple of vector fits, not vector fits, as I did before. Okay, that's necessary because of the statement we want to prove. Well, not to prove, to state actually. To give. Okay, so instead now I have two vector fields, sequence of couple of vector fields, or if you want to match. Of couple of vector fields, or if you want to matrix fields, they converge weakly to something, as before. The divergence of both components is zero, as before, and the support, the area of the support, goes to zero, as before. Okay, now you write these vector fields as a probability measure times well actually it's convenient for some reason to write them times a match. A matrix field, tau n, so these are two vectors if you like, which are uniformly valid. Okay? And you can always do this, for instance, you take this decomposition, okay, that's probability measure times, and you see that this vector field here is bounded, uniformly bounded in M. And now you consider, and now here is where the Young measure comes into play, because the statement is based on Young measure. Because the statement is based on Young measure. But let me, I mean, I don't, for those who don't know what Young measure is, simply means the following. Now you have this vector field, and you look at the push forward of the measure mu n according to this vector field. What am I saying? What I obtain here is a probability measure on the space of vectors. It tells me what is the probability that this vector field takes a certain value tau. Okay? So it simply gives me. Okay, so it simply gives me a probability distribution on the vector fields and it tells me each vector how much is how frequently how often it is taken. So in doing this step I completely lose any information about what is happening where. I only see what vectors I see in with which probability I see either vector in R two, or sorry, in R two times two in matrices. In matrices. Okay, no, because this, okay, so this is a map which associates to every point a matrix. So this is a probability measure on matrices. And it tells me every matrix how often it is seen. It's taken by this vector of the T. Okay. So it's something about the distributional values of tau n. And then, okay. And then we can assume, since these are all probability measure, then we can assume that they are the limit. And actually, this is a probability measure in R2 because I require here that this vector matrix field are uniformly bounded. It means that this measure is supported on a, this measure mu n are supported on a given ball of matrices. I don't get out, so I don't have any mass. So, I don't have any mass going to infinity on matrices because the only metrics that I see are a negative and boundary. Okay, so nothing escapes them. Okay, so I take this limit measure, probability measure, and here is the Vanishi-Mass conjecture is about that measure. Okay, it tells the following, that this measure can be decomposed as an integral, and then there will be examples to try to explain what this means. To explain what this means, okay? But let's first give the statement. I can decompose the measure row as an integral of probability measure with some respect to some parameter. It doesn't matter what the parameter is. So these are probability measures which integrated with respect to the STS-means rule. And here is the key point: the rank of the expected, the very center of each one of these measures. So these are measures. So, these are measures, all matrices. Each measure has a barycenter. They are not really on all matrices, but they are supported on a ball of matrices. So there is a barycenter. And this barycenter is a matrix which could have rank one or not. We assume it has rank one. So that's what the state has says. I can decompose the measure like this. Now, just Now, just a quick remark. Not all probability measure matrices have this decomposition. So otherwise, this thing would be rather empty. Okay, let me show. For instance, if you have a measure row which is supported on a convex set of one or two matrices, then there is no such decomposition. Because whatever, I mean, if I decompose this measure, it's supported on a given Suppose this measure is supported on a given convex set of rank two matrices. This also these measures must be supported on that same set. The barycenter will be an element of this convex set, so will be a rank of matrice. So there is no way you can do it. Okay? And okay, the original formulation of the conjecture is different, actually. But I think it's essentially equivalent to this one. So let me explain the meaning of So, let me explain the meaning of that conjecture with some examples so you see what's going on. Because it's rather technical otherwise. So, consider first this case, okay? These blue strips are the sumpers of my vector fields. There are two vector fields now. The first one is this one. So, this I'm drawing the first vector field. It's just a vertical vector field constant. And then there is the other one, which is an oscillating vector field, like this. Okay? Vector field like this. Okay? Now, this is a very simple situation because I can look at the matrices involved. So now, if you look at the matrices involved in this vector field, you will see that there are very few matrices, right? Because there will be a matrix, let's say, if I'm inside in this white triangle here. White triangle here, the first vector will be this one, and the second one will be zero. Right? If I am in this, what is it? It's not a rectangle, okay, whatever, parallelogram, I see this vector and this vector to map. And if I have in this vector fit here, in this parallelogram here, I see this vector like this. Okay, so essentially. Okay, so essentially all these vector fields essentially they there are only three matrices that are actually there. So what will be the distribution the probability distributional matrices for every end is the same because the scales I don't when I okay and it's just this one. Half of the time I see the matrix this first matrix here one quarter of the time I see this other matrix and uh so are three direct masses. So, are three direct masses. That's what I see. And now, if you look at this measure here, it's a sum of three direct masses. You compute the expected, the very center, and it's a round-pound matrix. Because if you look at the average, I mean this and this cancel somehow, and you get, no, they don't cancel, but the average of these two vectors is the vertical one. So, this is a round-back matrix. Okay? So, in this case, Okay? So in this case, the measure role itself has barycenter with run one. So there is no need. And the statement is more complicated, right? It was about the fact that you have to decompose the measure. Why do we have to decompose the measure? This example doesn't say what's happening. But now I modify slightly this example. And the situation is very similar, except I bent the strips, okay, which are no longer rectangles. Which are no longer rectangles. And now, of course, locally I see the same as I showed you here. I see the same two vectors with, but of course, the curve. And now you see that if I look, what I should do is that if I take this infinitesimal contribution to the row, it will be supported on a random matrix. But if I go around another point here, I will Another point here, I will get another one-quarter matrix, which is rotated. Now, if I put all of them together, it's no longer a measure, whose barycenter is a one-part matrix. But of course, if I ideally, so that's the idea of the decomposition in the conjecture, is that each piece contributed to one quan, adding to two vector fields with the same alignment, but somehow the decomposition captures what happens at different pieces. What happens at different pieces and different orientation. So, in this case, if I imagine that I split my measure into pieces, one coming from this around small rectangles, small tubes, then I can imagine each one of them will contribute in the limit to a random matrix. But if I put all of them together, I don't see a rank-common matrix. But, okay, so this is where essential why the statement of the vanishing mass conjecture must be this decoration. Mass conjecture, there must be this decomposition of the measure. Because you must follow somehow the fact that different parts of my vector fields contribute to a different direction, so to speak. Okay, so this is just to explain. And what? How much time left? Okay. Okay, let me go. So, this is a case where the composition is needed. Okay, let me see. What is partial results, which are not, this one is a proposition because it's nothing particular. So, let's say that if the final, so we have this sequence of metric skills. I'm sorry to interrupt, but I lost the connection to young measures though, because of the example you just showed, which involved averaging over sort of the entire entire domain. Entire domain. Young measures are a little more local, right? You fix a point and then you zoom in. In this problem, okay, so Young measure depends only on the also on the X. Right here, I deleted all depends on the X. The point is that it's irrelevant. Because whatever happens for... So you could say that my measure here Do not capture and lose any local information, right? But the point is this: let's say that you have, so you see, what happens is that take any situation coming from a sequence of vector fields. Now, you simply take this vector field and you scale them periodically. Then, everywhere you see the same field. You see the same thing. So, what I'm saying is that the information that comes from the full young measure, which also takes into account the X, is not different in reality. Because you can take your vector fields, let's say that they are, okay, I made them periodic and make them. So let's say at the beginning you have a certain vector field like this. Now I give you another example where inside of each one of these molecules I reproduce the same vector field. I reproduce the same metric and then finer and finer. In this case, this becomes automatically homogeneous. What I'm saying is that adding the X doesn't give you more information. Because whatever you see globally, what if you have an example that shows something on a global state, you can do a construct an Eigular example where whatever is happening globally now happens at every point. Globally, it now happens at every point. So you can add the X, but it doesn't give you more information really. So I guess the reason I ask is from the example you showed, it felt to me like row S somehow depended on X. In that one, yes. But you're saying... Yeah, exactly. But I could make a more complicated example where here I wrap inside this I put another structure like that, finer and finer. And now I lose the X. And now I lose the x will not help me. Yes, you're right. In this case, just keeping the x would be a simpler way to see the. But if I can construct inside here, instead of having this nice picture, I have something like this global picture just scaling back on a smaller scale. And then in some sense, I lose any information. I mean, keeping the X in the young measure doesn't give me more information. It's not more precise. It looks more precise, but it's It looks more precise, but it's not for this type of problem. At least the conjecture is not even proved for homogeneous young measures. It's enough to prove it. So, what we can show is that it's enough to prove it for the homogeneous young measure. Exactly. Which was probably the right way of saying it from the beginning. Okay, so let's go back. What is known and what is easy, by the way. Easy, by the way. So let's say that our Vn converges. Remember that this is the situation. So Vn will converge to some V, which is tau mu. Okay. Now, if this is singular, so the limit is singular, it's exactly what is... the examples are different. You get Lebesgue measuring the limit. But if this is singular, then the vanishing mass conjecture holds. That comes from the Comes from the results that I stated at the beginning of the lecture. The fact that the singular measure has a preferred direction, and what you show is that everything aligns with that direction. Okay? So now maybe it's more clear what I meant with the reason intrinsic direction depends on the measure. If you have two vector fields absolutely continuous with respect to the same measure, they must be aligned. If they are to go just free. Again, I could have told this at the beginning, sorry. Anyhow, and the other is that if they are the limit might be absolutely continuous with respect to the back, but if the elements are singular themselves, then again it's the the final row is directly supported on random matrices in particular. It can be decomposed in a trivial way, when it's as integral of the right masses. Integral of derived masses. These are not the interesting cases, of course. So this is a proposition because there is no interest in this data. I'm just reselling again something that I did 30 years ago. So there is nothing here. Okay? It's just statements. So that's it. Okay, let me say something. One of the words, but anyhow, thanks. But anyhow, thanks. I would like to go to another partial results, and this requires a little bit of preparation. So, what we saw in the example, in some sense, the reason why in this example we think this conjecture is true is essentially because we say that if we look at the right scale, I mean, our measure in the limit could be Lebesgue, but if I look at the right scale, it gets similar, right? So, the idea could be this. So, the idea could be this. The conjecture is true because if I look at the measures not on the global, on the natural scale, but I look at the right infinitesimal scale, then I see something singular in the limit. And I'm trying to formulate this property. So, the idea is that, yeah, the measure may go to something like a back measure, but that's because I'm looking at them at the wrong scale. I want to look at them at the right scale. Scale. I want to look at them at the right scale, then I see something similar, and that's why the conjecture. I think, in the end, that's behind the geometric. I mean, if you look at the example I've given, of course, if you look at the right scale, you see lines or curves. No, you actually see only lines, which are pretty similar. Okay, so let me state it. I okay, it requires a little bit, but let me go quickly. So I take a measure nu on the square, and now I take a family of squares that somehow are essentially disjoint. That some are essentially this joint and cover all of them. And okay, they have positive measure because if they don't, I throw them away. Those that have zero measure. Okay, now I simply take the map that this is the rescaling that takes each one of these small squares to the big one. It's a blow-up map. It enlarges the small square to the big one. Okay. To the big one. Okay. I take the restriction of this measure, each measure mu, to this small cube, a small square, I blow it up to the full square, and I also renormalize it in order to make it a probability measure. So I'm taking a small piece of my measure, I map it back to the large square, and I renormalize it so that it's a probability measure again. Okay? And now I do. Okay? And now I do this. I take a probability measure on probability measures, which is simply saying I look at each one of these measure BUI that I get by each single piece. I take delta, Dirac mass of that with the right weight. So this is a way to say, to put together all these measure mui. It's a sort of abstract way. Bear with me. Way, bear with me. It's sort of a convex combination. It's a convex combination of Dirac masses, okay? So, but this is a way to put together all these single mui, okay, single pieces. And, okay, so I call this P, which is a very abstract object, but I call it a blowout of mu associated to F. So I'm decomposing mu in pieces coming from the family. the family of this family of squares and somehow this encodes all the property of this all this little measure mu n. And now I go on and I say, okay, now I take a sequence of probability measure mu n. I say that it has a singular blow-up. If I can construct these blow-ups, each one, one for every singular measure. So for each one of these measures, I construct. So for each one of these measures I constructed the composition tubes and I look at the weak limit of this measure domain. If this is supported on a singular measure, I say that I have a singular law. So essentially what I'm saying, let's say, think about this idea of looking at the right scales. These cubes represents the right scale where I look at it. But I have also a limit of measures, so I must take the limit of these cubes. Of these cubes, so to speak. And the way I do it is simply say, well, I don't look at the squares, I don't look at the mui, I simply look at them globally as distributional probability measures, or probability measure, and I look at the limit. Depends if mu is uh is similar or not. You can turn something which is not similar to Turning something which is not similar into a bunch of Diracs essentially but I have some condition about the Dirac. You should think of this as a way of keeping tracks of all the pieces of the measure. I'm decomposing my measure looking at different scale in different places. That's what they keep doing. And then I want to keep track of them together so I have to choose a good family for you. I have to choose. Yeah, I have chosen. At the time, the going at the zero. So at the time they're going at the zero of the cubes? No, no, it's not required. Yeah, w you would imagine that in all meaningful problems the cubes must be smaller and smaller, but it's not required. So in principle if mu n this is constant, you can take just as one of them. Okay, let me go quick now because it's sort of. So the meaning is that this should corresponds to take the example with the curvesing. Take the example with the curved things, you make into small pieces, and then you keep track of all the small pieces. That's what you do. I mean, this phi here is just a way of keeping track of all the small pieces. So the result is the following. Take everything as above. Okay, if mu n admits a singular robot, it means that there are sc it essentially means that there are schemes where I see something singular. Where I see something singular. Maybe at the global level I see levite measure in the limit, but if I look at the right scales, these are the blow-ups, I see something singular in the limit. And then the vanishing mass conjecture holds. And so here is, and I conclude, the question. If I take any family of probability measures horse in all sets whose measure goes to zero, do they admit a singular grow-up? This would be a simple geometric statement, and the answer I'll forget. Geometric statement, and the answer unfortunately is no. For a long time, I hoped it could be true. That would simplify many things. It's not. So, even in dimension one, you can have singular measures that don't have meetings. So, whenever you look at them, they go to the bad measure, but if you look at them at smaller scales, they never become similar. So, there is no scale where they remain similar, or they become similar. Which is sort of counterintuitive, right? Because whenever you draw a data. Right? Because whenever you draw a picture, it's clear what is the scale where you should look at to see some case larger than it. I'm saying that this is not a general rule of such a scale, there is none. So this is the only new things that there is, so that it's a negative result. We hoped for a while that every measure might have, sequence of measure might admit a single blow-up, but it doesn't. So that's the end. And okay, I have some additional remarks, but it's too late. Sorry. So I guess there's a way to formulate this conjecture using level sets of stream functions, thinking about vectories of particles. You repeat them. So if you have a divergence-free vector field in the plane, you can introduce a stream function for it. Yes. And then it. You know, it's like I was just looking at a paper I have on my computer that Friedland wrote, where he introduces a tree, a graph structure, for labeling and counting the stream lines of the stream function and passing to a limit. So anyways, I'm wondering if there's a way to formulate this conjecture using stream functions and how it's more natural to study it that way in terms of integral curves. In terms of integral curves? Yeah, okay. Here it comes to the point. That's uh it's a good question. I do not know the answer. And if you like, um you could say that if the purpose here is to capture this somehow alignment, whatever it is, any alternative formulation to the Young measure formulation that Deep proposed, it's welcome. So it could well be that you look at things in a different way, maybe not an equivalent alternative. Maybe not an equivalent alternative formulation, just a different one, that would be also very interesting. So, what the situation is, I mean, for him it was a different story because he had a problem that he wanted to solve with this. But if I look at geometrically, I mean the choice of using young measure, it's just one choice. Maybe there is a better or alternative way or different that somehow tells you what's going on. So, I would be very happy to find an alternative formulation, not necessarily equivalent to the original. Only equivalent to the original one. That would be perfect. Maybe even something that gives you more information, it tells you to which vector you align. So, just to clarify, I think I'm hearing from you that it has not yet been formulated using string functions. No, that makes sense. Well, no, it's a bit vague. So having measures in general fleets will possibly measure methods by with the emergent fleet. Then with the divergent three, they come, for instance, as limits of dissolution density, then homogenization of dissolution density. So I was trying to understand. So in that case, it's really not really the thing structure is really one-dimensional, so it's a limit of one-dimensional object. So in that sense, you always have alignment with this. That is the approximate structure. So, what is really the mechanical meaning or the interpretation of this vanishing maximum conjecture? Once you prove the positive interpretive, you should ask him. Once you prove the positive, let's put this one. Yeah, okay. Yeah, okay. So let's say that you are interested in a structure like this. Now, if these are really lines, things are aligned. I mean, with themselves, okay? If I have two vectors in here, they must be aligned. What the conjecture is saying is that if there is a little bit of thickness here, so that this is really not lines, but they are strips, then the vector field can do something wiggly inside, which they can't on a line, but still. But still, in the limit, I see that's the only way I can say. This means that if you relax certain problems, when you do the relaxation, you don't have to consider the full relaxation.