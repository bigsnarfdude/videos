David for inviting me. Always a pleasure. Too bad or not, a Banff, but you know, maybe next time. So, today I'm going to talk about a stream of research that I've been pursuing over the past maybe eight, 10 years. And this idea, which is a very simple idea, and it's about how to leverage a model for designing randomized experiments in a way that preserves. Experiments in a way that preserves all the good properties of a randomized experiment. And, you know, to the extent that the model is actually working, it provides statistical efficiency. And we'll, you know, we'll get through it together. This is just a slide, you know, to so in this particular conference, there's a lot of people who are talking about experiments happening in. Experiments happening in industry. And, you know, I've been involved with a few companies over the years. And this idea really came through when I was spending a sabbatical at Google. And some of my students were also working there as interns. And, you know, over the years, it's been percolating throughout some of these other companies. I'll give more context later. So, first of all, I'm going to start with the acknowledgments. So, this is work, you know, it's a stream of work. Most of the paper that I'm going to discuss today and the example I'm going to walk everyone through today is with Guillaume Bass, who is an assistant professor at Stanford. And, you know, other ideas are, you know, and other papers are together with Don Rubin, Vishesh Karva, my colleagues at Temple, and Jean-Puget Badi, also my former student. And Jean-Poug√©-Badi, also my former student, who is now working at Google Research University. And there are four main references, and I put them here so people who look at the slides later can chase them down. The one paper to read maybe would be the model-assisted design of experiments with Guillaume Bass with details about the examples I'll be walking through are in. So I'm going to go and give some background. I watched some of the talks. I watched some of the talks already, people have talked about some of this background, so I'll try to be quick and then we'll talk about complications arising from a social network in word of mouth campaigns, and then we'll walk through the technical details of the idea. So at the high level, the motivating question here comes from Google. So you want to run an A-B test or randomized experiments. A and B are two options where the reserve price is set in a normal way within the A in the A. Way within the A in the control, and it's set in a very experimental way for the B auctions. And the outcome we're interested in, the causal effect is the estimate to estimate the lift on total revenues that is due to this experimental reserve price. There's lots of issues, there's dependent outcomes through a network, and we'll dive deep into this particular issue. Deep into this particular issue today. The strategic behavior, obviously, the advertisers are not IID units, they're all following their own objective functions. There's a dynamic environment because the experiment takes place over eight weeks. Typically, and there's heterogeneous units because there's mom and pop shop advertising, there are the eBays of the world advertising, and then there's sort of mid-level firms who have hired an advertising agencies to manage their budget, like max. To manage their budget, like max point or nanogans, in order to bid for these keywords in these auctions. And so the idea is that: okay, we would like to use a model predictive of advertiser behavior to provide more statistical efficiency, more power to this type of randomized experiments. But how do we leverage the model in designing these experiments? That's a question. So at the end, so how do we leverage the model in deciding who gets treatment and who gets control? Gets treatment and who gets control. And in particular, the status quo, or the majority of these firms still will look at designing experiment in a very traditional way. Let's ignore the model and let's take this design-based approach in which the model has no role whatsoever. What happens if you? Role whatsoever. What happens if you do that? You typically get a probably unbiased estimate of your causal effect and possibly higher variance because you're not leveraging extra information. Trusting the model is typically bad. So designing a model, sorry, designing an experiment, actually leveraging the model is bad because if the model is correct, obviously you get an unbiased estimate and you get efficiency, but the model is never correct. And so if the model fails, you don't know where the bias is. Model fails, you don't know where the bias is coming from, and you're still overconfident because typically you leverage extra information, you have lots of data. And so, our approach is to model it to use a model-free estimator, but impose model-dependent constraints on the randomization. And we'll be very clear through an example what that means. But the consequences of doing that, or the consequences of this strategy, if the model is correct, you still get an unbiased estimate and low variance. Estimate and low variance, and you know, we're not in that situation as I mentioned before. If the model fails, you now still get an unbiased estimate, which is a big deal, and you know, you pay a price in terms of variance. So, if the model is describing reality, you know, well, you get an efficiency gain, and if the model is poor, you will lose efficiency, possibly even with respect to the design-based approach. And what's nice about it is that's, I think, the situation everyone would. That's, I think, the situation everyone would like to be in. You want to have a good estimate, and you want that estimate to be uncertain to the extent that, for instance, the model is wrong. It's now putting some notation here. Model-based, you know, would be the outcome Y, there's a baseline alpha, there's some covariate X, Z is a treatment or control, so in our case, a binary treatment, and epsilon I is the model, so there's a normal distribution. A normal distribution around the outcomes. Design-based, and this is something that confuses a lot of people, you still have this linear model. So it's not really, you know, people will argue model-free, but, you know, but it's design-based because there's no distribution on the outcomes, or that's probably to say it better, the only randomness in this situation in the design-based approach. Situation in the design-based approach is due to the fact that treatment is assigned randomly ZI. If you condition on ZI, if you condition on the treatment assignment, there's no extra variation. So in that sense, there is no model, there's no distributional assumptions on why once you condition on the assignment. And assigning treatment, the randomness about assigning treatment is the randomization distribution that's under control of the experimenter. So it's a very different situation than the model. Very different situation than the model-based approach. You can see here we'll trade off, we'll have both of these errors, but we'll have a strategy that gets around some of the issues. So some caveats. So there's complexities in the application that I just described as many complexities. We're only tackling the complexity we call network interference. And so the idea that the response And so, the idea that the response measured on an advertiser in the application depends on whether or not other advertisers that are competing for the same keyword are treated or not. So, in general, you say whether the response measured on a unit depends on the treatment assigned to other units. And classically, there's no interference, or if there is interference, which people have known about this, People have known about this complication since Fisher, they will try to limit interference by design. So, if you think about the treatment being fertilizer and the outcome being how fast the crops are growing, well, the fertilizer may leak from this plot into the another plot, but you can put like guards or you can separate the plots to limit the interference because interference is never a feature, it's a nuisance, it's not something of interest. Nuisance. It's not something of interest. But here, like in a word-of-mouth campaign or this Google application, we do want to understand and quantify the extent to which treatment on a unit affects the response on another unit. Okay, and then there's other complications, homophily, strategic behavior, dynamic environment, budget and capacity constraints. You know, some of these we've mentioned, some we have not mentioned. We do have research and papers to address these other situations, but we won't discuss them here. Also, Here. Also, as we get into the technicalities, this complexity in the approach, so we are talking about here estimating the average Twitter effect or estimating this network effect. There's, you know, testing is another thing, but we'll ignore testing. We're taking a finite population approach here. We're going to look at randomized experiments and we're going to trade off model-based versus design. Of model-based versus design-based versus model-assisted. So, this layer of complexity will discuss it openly. The network data, we're conditioning on the network data being available pre-intervention and reliable. And in the application, that means that we are going to assume that who competes with whom for any given keyword is going to be fixed. Of course, that is also a quantity where estimating the competition network, and there's going to be additional bells or windshalls that you have to append. Belts or windshalls that you have to append on to the strategy I'm going to describe today to tackle applications in which that is an important source of variation. But you know, so you get my point. We're going to have to make some choices. We're going to look at the simplest, non-trivial set of situations in this talk. All right, so now let's talk about complications arising from a social network. So, again, we're trying to estimate the average treatment effect, and we have a situation in which In which advertisers compete. And as a consequence of that, we have, you know, the network is inducing a dependent structure on the outcomes. And I'm going to now describe what that dependent structure across the outcome is going to, what the consequences of that are. So, we're going to take this potential outcome approach to causal inference. The unit, you know, the basic sort of mathematical quantities, the table of science. Quantities: the table of science. Here I have an example with four units: binary covariate, sex, binary treatment. And so, with binary treatment, I can assign treatment and control in 16 possible ways to these four units. And these are the 16 columns in this table. And the table of science is essentially the outcome, the set of outcomes observed for each unit when any given column has been picked. And so, the set of signs is going to be four by So, the set table of science is going to be 4 by 16 table. And typically, in the design-based approach, we consider these 4 times 16 numbers to be constants out there, right? And causal inferential targets are defined as a function of these 4 times 16 numbers. Now, in order to estimate accurately a function of this large set of quantities without any assumption and only observing four, And only observing four outcomes, one for each unit, because we're going to commit to one column in this, to one treatment allocation, is hard. And so typically, we assume that the outcome of unit yi is only a function of whether that one unit is treated or not. And so this table of science from four times 16 collapses to a four by two table. And now, you know, we're estimating a quantity that's a function of eight numbers using four numbers that are available after. Four numbers that are available after you know the outcomes after the randomization is executed, and so that is a the typical situation. And you know, people say, well, it's individualized treatments, response ITR like Manske or economics, or in statistics, they'll say, oh, we are assuming sutva, the stable unit treatment value assumptions like Rubin and so on and so forth. But you know, the assumption is essentially a smoothness assumption that reduces the complexity of. Complexity of this space of potential outcomes. And so now, if you are assuming that the treatment on a unit may have an effect on the outcome of another unit, well, that ITR or suit of assumption are no longer tenable. So the next layer of assumptions is, well, we're going to have to assume how the treatment given to units other than unit I, Z minus I, are going to affect the outcome. Are going to affect the outcome for unit i. And here, you know, to write it down, why i is going to be a function like before of whether or not unit i is treated z sub i, and then some function e for exposure that is going to quantify in some way or you know the treatment assigned to units other than I. And so, here is an example of exposure. I is exposed if it has at least one treated neighbor. And so, in this Untreated neighbors. And so, in this case, this binary notion of interference. We have a competition network which is given. We can look at unit I and the neighbors, like in my four plots here at the bottom of the slide. And we can define exposure whether unit I has at least one treated neighbor. And so, here I'm going to walk you through quickly these four situations. So, here, unit I in the black, you know, in this, it's always in the center, is in control. In this, it's always in the center, it is in control because the unit itself is untreated. And in blue, the unit I is treated, none of the neighbors are exposed. So, this is pure control and pure treatment. Now, a situation in red wouldn't be allowed under suttva, would be assumed away by sutva, but it's not assumed away in this interference setting. Unit eye is untreated, but at least one of the neighbors is treated, and so unit eye is going to be exposed. And so, Unit I is going to be exposed. And here in purple, Unit I is both treated and exposed. And so, you know, we have additional, you know, we have two more potential outcomes, essentially, for each of the units. So now, what happens and what phase and why is this a problem? Typically, well, typically, what happens is that the inferential target is only a function of, let's say, Only a function of, let's say, treated units and control units, and so the typical randomization scheme, like completed randomization or cluster randomization, only tend, only try to control the number of treated units and the number of untreated units. So here, for example, now I have a network, a competition network with a bunch of advertisers. I'm assigning treatment and control randomly, and I'm controlling NT, which is the number of units treated, and NC, the number of units treated treated treated Units treated and NC, the number of units in control. So I'm controlling a number of zeros and ones. And now I'm color coding each of the units according to whether it's pure control, pure treatment. So I have three black units in pure control, one blue in pure treatment. And then I have some red units exposed and I have these purple units which is treated and exposed. And so because now I'm not controlling for red and purple units, either the ST. Units, either the estimator is going to ignore that some of the units that have a sign zero, like the red ones, actually are exposed and not pure control. And so, you know, if the estimator is going to ignore that, there's going to be some bias that's introduced. And if, on the other hand, if I have a smarter estimator that is now throwing away all these red units, because Units because it looks at them and says, Oh, this is treated, but one of the neighbors is treated. So the unit is exposed, not pure control, where you're essentially reducing the sample size substantially. And so the power calculations are going to be wrong. So these are the typical modes of failure of using a randomization like complete randomization or class randomization in a situation in which you do have network interference. And what do we need to do? We need to. And what do we need to do? We need to come up with strategies to define randomization distributions, randomization schemes that control for exposure using the graph. And so we're going to have to do that. And that's what we're going to do today in this talk. We have a couple of paper, one paper here with Vishesh Karva, which is in the list of references, in which we analytically, systematically explore the bias. Explore the bias and the variance that's introduced by ignoring interference, let's say, when interference is present and so on and so forth. And you know, there are some nice, interesting calculations. So here, you know, just to give you a flavor, I'm still assuming interference from neighbors. I'm assuming additivity of this exposure effect. I'm taking a complete randomized design, controlling for a treatment and control, and the naive estimator is just the difference in means estimated. Naive estimator is just the difference in means estimator for the ATE. And so the bias here is introduced by this second term. And beta bar would be the actual parameter that is equivalent to the average statement effect. And so the bias here is some parameter gamma that controls how much interference matters times the density of the network. So, you know, in a denser network where there's a lot more. So, you know, in a denser network where there's a lot more competition, you're more likely to find units in treatment or control that are actually exposed. And to the extent that exposure matters, the bias will increase. And that makes sense. And there's a price in terms of variance. And it's a very neat set of propositions that quantifies misspecification in different settings. There's another paper that I'm going to point you to with Dan Sassmann. To point you to with Dan Sassman on the archive, in which we also make the argument that if you have a complete randomization and you're fitting your regression to the outcomes, to the observables, then the slope of the regression is no longer a good estimator for the ATE. And in this paper, we explore under which additional assumptions, smoothness assumptions, and so on and so forth, we can. We can make that equivalence under which assumptions, you know, what functions of the parameters is now the ATE, and what function of the parameters is now, you know, a good estimator for the average network effect. And so it's a different take, and we'll propose some estimators in this paper. So now we can get to the main point of our talk here. So, model-assisted design of experiments. I'm going to walk you through. Assisted design of experiments. I'm going to walk you through a fully worked out example and then I'm going to suggest some extensions. So, first of all, I should say, when would I use and when do I use model-assisted design or experiments? Well, if the model is given by physics, you know, you can probably trust the model. So, there's a lot of situations in which model-based actually causal inference is not unreasonable. But in the situations I will. But in the situations I work with, where units are humans, advertisers, our users, well, there's a lot of uncertainty. The model is always an approximation of reality at best. And so really, you know, this approach or a randomization-based approach is typically the way to go. And so here, now warming us up to the model assisted design experience, imagine you walk into a corporate setting. Into a corporate setting, and you are consulting for your favorite tech company, the inferential target, like in my example, lift on total revenues and the estimator for it are written in some UI and they're hard to change. So, you know, in a corporate setting, you can imagine people know what they're measuring and they have good estimators for what they're trying to measure. And so, To measure, and so really the flexibility is in positing a simple non-trivial model for the outcomes. You know, how would users behave or how would advertiser behave in the face of competition or in the face of changing reserve price? And then given the estimator, which is model-free, comes even before the model is formulated. And given a simple non-trivial model for the outcomes, we can compute the mean square. Outcomes: We can compute the mean square error for that estimator conditionally on a treatment assignment vector, conditionally on a land location vector z. And then, you know, you can integrate that over the randomization distribution and get the marginal MSE. So now, what I'm focusing on here is this conditional MSE, and the conditional MSC will quantify how desirable an allocation vector is, okay, because it gives you the MSC for that particular estimate or given a particular allocation. Particular estimate or given a particular location vector. And so the idea is that we just want to select a subset of randomizations, or we're going to constrain the space of all possible randomizations, okay, by selecting a subset that's more desirable. And then we pick one of those, you know, allocation vector, one of those randomizations in that set of desirable randomizations, still at random. So preserving. Still at random, so preserving some symmetry, okay. But the set is no longer all the allocation vectors that have 50% treated and 50% control. There'll be some other constraints, and the constraints come from inspection or analysis of this conditional MSE. So now I'm going to walk you through this example. So there's a couple of examples. This starts from here, here, where I'm still assuming that there's no interference, so the outcome, the total revenue of an advertiser are just the Are just a function of whether or not this advertiser is in the experimental auctions or not. Treatment is binary. The competition network G is given. And the full model is that, well, XI is the budget the advertiser I has, is a normally distributed random variable. And then if the advertiser is assigned to this control non-experimental auction, the total revenue is. Total revenue this advertiser is going to spend is going to be a normally distributed amount of revenue centered around the total budget of the competitors. And so although there's no interference, there's still a dependent structure on the total revenues here. And so N sub I is the neighbors of unit I in this competition network. So they are direct competitors of unit I. And XJ is And xj is just you know the budget for all the competitors, and this definition of neighborhood includes the includes i, unit i. So it's called the extended neighborhood, includes i and all the direct neighborhoods. And so if I now assign this advertiser to the experimental option, what's happening is that it's gonna spend B more revenues, B beta, beta more revenues, beta more dollars. I'm gonna do the analysis here using a completely The analysis here using a completely randomized design for the naive estimator, okay, the difference in means estimator. And I'm going to now look at the conditional MSE for beta hat, difference in means estimator, conditionally on an allocation vector z. And so here I have my biased square term and a bunch of variance terms. And we're just going to talk about the biased square term. And see here, mu square is, you know, the average bias. The average budget squared. And here in this parenthetical, I have a difference of two quantities. On this side, I have essentially the average number of competitors for units for advertisers in treatment. This is the size of the neighborhood. This is whether or not units is treated. This is the number of units treated. And here I have the average number of competitors for advertisers in control. And so this is telling me that, well, Is telling me that well, if the average number of competitors for units that are treated is different than the average number of competitors for units in control, I bias, okay? Or said in a different way is the average degree according to the completion graph of the unit C3 that is different than the average degree of the units in control, then expect some bias. And so, what I'm going to do, I'm just going to further constrain my randomizations not only to have 50% treatment, 50% units treat. 50% units treated and 50% of the units controlled, and further constrained this randomization to make sure that the average degree of the units treated is going to be the same as the average degree of the units in control. And that will get rid of the bias should the model hold. And then we'll have a conversation about what happens should the model not hold. But before we move on, I want to make one more observation here that if I were to swap all the units in treatment to All the units in treatment to control, and all the units on control to treatment, the average degree of the treated and the average degree of the untreated wouldn't change because, you know, if, you know, especially if it's, you know, if it's the same degree of units treated and units untreated, once I swap them, the degrees are still the same. And so in the set of randomizations for which the treated and the untreated have the same average degree. Treated and the untreated have the same average degree. Okay, this constraint is symmetric with respect to the swapping operation. That will come in play later when we look at the insight behind the theorems. So now, constrained randomizations here, the balance randomization is the space all the zero and vectors, which you know, I would say space of all randomizations or space of all treatment allocation vectors, such that the number of units treated the same as the number of units in control. As the number of units in control, and now I'm going to introduce this new subset, the set of unbiased randomizations, for which the average degree of the treated is the same of the average degree of control. And, you know, I had more variance terms there, so I can actually define optimality by controlling all the terms in my conditional MSE. Okay, so now first, before you know moving on to what happens when the model fails, small The model fails. Small simulation here, just to get a sense. So, here I have four different topologies for this competition graph: Erdos-Reni, power law, small world, and stochastic block model. And I'm looking at the conditional MSC here on the x-axis. So this would happen if you do Bernoulli randomization or balance complete randomization. Now, if you start to add constraints and still random. Constraints and still randomly sample an allocation vector in the constraint space, you can see that you're doing better and better. This under the situation that the model holds, obviously. If you're doing re-randomization, meaning if you are using this object new objective function I gave you, the MSC, in a typical re-randomization scheme, you would stop the first time you meet that criterion. You meet that criterion. And so, for that reason alone, you're still not going to do as well as you could. And then, you know, obviously, you can trade off some bias for a larger reduction in variances. So if you do an unconstrained sort of optimal search for randomizations that minimize the mean square error, you do even better. Okay, so you know, this is telling you there are non-trivial efficiency gains to the extent that the model. Gains to the extent that the model holds. And now we're going to introduce this notion of design unbiasedness. So this estimator, the 80, the difference in means estimator, is unbiased with respect to a design or the randomization distribution, so a distribution on Z. If the expected value of that estimator now with respect to over Z, over the space of all possible treatment allocation vectors. Treatment allocation vectors is the same as is unbiased. So it's the same as the target parameter beta. Now, the theorem that we write here is that the difference in means estimator, which is the one we're considering in the small example, is a design and biased estimator of the average treatment effect with respect to the following distribution. Well, the first one is the uniform distribution on, I'm sorry about this, should just be Z superscript B. B Z superscript B. So this is the balanced set of distribution of randomization. So all the randomizations that I have 50% treated, 50% untreated. But if I further constrain the space by saying, hey, now I want randomizations that are both balanced and unbiased, meaning the average degree of the treated is the same as the average degree of the control, and I sample a random in a location vector from that space. Location vector from that space, I still get an unbiased estimator, okay, and so on and so forth. So, this is telling you, you know, this theorem is now specific to the ATE, okay, this estimator, and specific to this subset of constraints that I am taking out of the inspection of the mean square error. And so, if I were to change the model, I would get different constraints. If I were to change the I would get different constraints. If I were to change the estimator, right, I would get a different mean square error. And so this theorem has now to be repeated or reproved, you know, for any combination of model-free estimator and model-based set of constraints. But we've done now a number of these exercises. And if you play around with how complicated we want the model to be, and what constraints are you going to use to further restrict the set of randomizations and what the estimate. Restrict the set of randomizations and what the estimator is that you're using. Well, it's always happened, at least in my experience, that we find a good set of restricted randomizations that lead to design and bias nest for the particular estimator that we were interested in. And I'm going to give you some insights of what's going on. So, these particular results in this particular example follow from symmetries on the space of randomization. So, there's two notions of symmetry. There's two notions of symmetry. So, the first one is that for the difference in means estimator, this holds. So, imagine I am estimating, you know, the difference in mean, difference in means for a randomization Z and for the opposite randomization in which I flip everyone in treatment to control and everyone to control in treatment. So, if that is true, this theorem is telling me, or this lemma is telling me, that the bias is the mantle, the magnitude of the bias is the same. The magnitude of the bias is the same, only the sign is changing. Okay, so if the bias given a particular randomization is five, the bias given one minus that randomization is going to be minus five. So if I'm taking an expectation of the space of all possible randomizations, all I have to do to ensure bias equals zero is to make sure that whenever z is in that set, one minus z is in that set, and so they will cancel, you know, pairwise. You know, pairwise. So that's the first part of the intuition. The second is: well, how am I going to ensure that z and 1 minus z are both in the set of restricted randomizations? Well, the constraint itself has to be symmetric with respect to the swapping operation, which is why I mentioned beforehand this average degree. You know, if the average degree for treatment and control is the same when you swap treated to control, control to treatment. swap treated to control, control to treatment, that constraint will still hold. So if I have Z in that set, one minus Z will still be in that set. And so if you have a both set of symmetries, then you're going to get this design and biasness. That's something to keep in mind. So now we can compute the marginal MSC. The marginal MSC is another interesting quantity. What's interesting about it is that in our particular example for the ATE, you still get ATE, you still get the typical variance that you would get out of assuming additivity and no interference. But then, because now you do have interference, there's an additional variance term, which is not necessarily positive if you inspect it. And so given that you're committing to run these experiments, even in an optimal way on this competition graph. Competition graph, well, you know, the variance to the extent that the model holds, right, may still be larger or lower than what the completely randomized experiment would do. Now, the good news is that if you are Google, you can actually select who is in the competition network. So you have actually control over critical features, critical statistics of the competition network, and you can make this variance term. You know, this variance term, it's not the variance term, this additional set of terms be negative so that you get actually an efficiency gain. And here is the average degree. This is the variance of the degree in the graph. And this is the average number of shared neighbors. So some notion of local density. So all of these quantities are interpretable and they lead to great whiteboard discussions when you actually try to use this technique. Actually, try to use this technique to run it, to design an experiment. Now, this is a fully worked-out experiment in the absence of interference, but still with this homophily, this dependent structure on the outcome. What if I now have both homophily and interference? And that's actually the use case that we wanted to tackle in the first place. It's just for the purpose of describing the technique. It's okay, it's probably better to start from. Technique, it's okay. It's probably better to start from a less complicated situation. So, if that was true, you know, you truly have interference, then you would just enrich the model. So, this is a model in which the outcome for YI depends on whether or not ZI is treated and also whether or not the neighbors are treated. And same here, ZI is treated or not. And, you know, so you have different models and you have different parameters and so on and so forth. There's a separate paper that'll describe this in detail. We don't have to go through this in detail. What I want to show you is that. In detail, what I want to show you is that we can still, you know, compute analytically an expression for the conditional MSE. Now, for an estimator, which is the stratified difference in means estimator, we try the naive difference in means estimator and we would get, you know, quantities that were harder to interpret, still analytical. And so here, you know, if you inspect, you'll still see that there's the number of The average number of shared neighbors here between unit in you know between units with a certain amount of neighbors treated and so on and so forth. You know, the paper will give you the full details. My point is that you can still come up and go through the same process, although it's now a little harder, and then you can communicate to your colleague. Colleagues or to engineers, exactly how you're going to design a randomization and what you want, you know, how you're going to select optimally who is treated, who is controlled, and you're trying to minimize the number of shared neighbors and so on and so forth. So, you know, that's this becomes very applicable as well, very easy to communicate to non-statistician, I would say. Okay, so now. Okay, so now that's one way to do it on a small graph, perhaps. But what if you have large graphs? Okay, what's the next step in complexity? Well, you know, so if you have a large graph, what usually happens is that the graph itself tends to be measured more poorly. And so what would be interesting is to posit the model on the graph and then use the model for the outcomes. The model for the outcomes and the model for the graph in this model-assisted design of experiment sort of strategy pipeline. Okay, and so here, let's say that I have a network, an observed network G, and I'm assuming that G comes from a graphon or an exchangeable graph process. And a good estimator for graphon is the stochastic block model approximation, which I've written about before. And so, the strategy would be you take a graph. And so the strategy would be: you take a graph, you sort or cluster, or match the edges somehow, the nodes somehow, and then you get this sorted graph. And then you take essentially 2D histogram of that graph, and now you go from, let's say, 500 nodes down to 80 blocks, okay, or 80 bins. And so there's actually many papers that have been written on how to best estimate graphons and. How to best estimate graphons under different conditions. And the reason we have this, you know, three-stage is because all of the strategies boil down, or most of the strategies boil down, the vast majority though, to taking a graph, sorting, clustering, matching, and then taking a two-dimensional histogram over the sorted cluster-matched network. And sometimes there'll be even an additional smoothing step on top of this, you know. You know, 2D histogram or stochastic block model approximation, how people call it in the literature. Now, the beauty of this is that now I have groups of nodes, and instead of deciding to treat or not treat every node in particular, I can have a randomization distribution that tells me what fraction of the nodes I'm going to treat in this block, in that block, in that block. Block in that block in that block. So instead of having, let's say, one parameter that tells you fraction of nodes treated or untreated, now you will have, you know, in this situation, 85 parameters, which tells you for each of the block, what's the percentage of treating or not treating units in that block. And now you can optimize over this set of 85 parameters to find the optimal randomization distributions. Once you find the optimal set of parameters, you're still uniform, you're still sampling a random. You're still sampling a random, right? Random samples from this distribution, but now you have a parametric set of randomizations. And I won't bore you with the details, but again, if you look at the paper, we're going to have interference, for example, and there's an exposure function here that will decide whether we have binary interference or linear interference and so on and so forth. We're just going to run. gonna run our simulation and uh you know and compare this block saturated design um or in in in our situation i'm gonna argue i'm gonna optimize the block saturated design over this parameter set and see how that compares you know with cluster randomization or complete randomization and so on and so forth okay but again you know the idea is that you're gonna cluster the graph You're gonna cluster the graph and then you're gonna decide what fraction of nodes to treat in each block in the graph. Okay. And so here, just to skip to the results here. So the average treatment effect was $10 in this simulation. The average network effect or interference effect was $3. There's different ways that we consider, okay, so completely random. completely random um completely randomized uh design cluster randomized design in which you know you say you pick your 85 clusters like in my in my plot a couple of slides ago but you either decide whether to treat everything within the cluster or no one within the cluster so the cluster becomes the units of randomization or analysis really that will determine power and it's either everyone is treated or everyone's untreated and then here you know i skit the You know, I skipped a lot of details, but we have two ways of optimizing these saturated designs. One is optimal, requires some calculations, and one is a lot faster, but it's suboptimal because assumes the absence of edges in between blocks, which is never true, by the way. But if you do that and you assume that blocks are not, Block model approximation is diagonal, fairly, fairly diagonal, you can get analytical results a lot quicker, whereas the optimality is never analytic, it's seldom an analytical result, I should say. And so what's interesting here is that if you look at how well you can estimate the average freak and effect, well, pretty much everyone does okay. And you can be more or less uncertain. Here we were trying to optimize, Optimize, you know, to estimate the spear influence or optimize its network effect. And so, which is smaller orders, like only three dollars out of ten. And so, now the randomization, so the variance due to the randomization is going to be really be critical. And so, here, again, in terms of how close all these randomizations get to estimate the effect, they're fairly close. But if you now look at whether or not you're going to call this effect significant, given the variance due to randomization. Given the variance due to randomization, so the randomization standard error here. Well, if you were to do completely randomized experiments, no chance. If you're doing a cluster randomized experiment, which is a good randomization for the total treatment effect, which is a combination of main and network effect, not of the network effect, you do a lot better than with complete randomize, but still, you know, you. You probably can call significance, but you know, you still have, you can do a lot better, I guess. That's what I'm saying. And then, if you now jump down here, so this one was a set of 85 parameters that was optimized to detect network effects. So, now, you know, in the best possible situation, which is the bottom row, I get a very tight randomization standard error. And if I, you know, in Standard error, and if I, you know, in this particular set of simulations, if I were to use the suboptimal strategy, I would get still a reasonable standard error or still better anyway than the class randomization. So, you know, essentially using the network wisely, you know, if you're interested in this network effect, it will make a difference in practice, especially, you know, in a social science setting where you may not have millions of data points. Have millions of data points. So now I'm concluding here. I think that's most of what I wanted to say. What I'll say is that I discussed complications in the applications, complications in the approach. Well, there's a whole set of papers that we've worked on over the years in which we tackle some of these. And they are tied to the fact that these applications came from Google, for example, or from Amazon. And so we have it. Or from Amazon, and so we have heterogeneous budget constraints. So different advertisers have different budgets. We have heterogeneous capacity constraints. You can imagine different fulfillment centers have different capacity in terms of packages. There's a mixture of temporal spending patterns, especially in the advertising ad world context. What's going to happen is that some advertiser will explore, exploit, and so will spend a little bit of money to find which, you know. You know, set of demographics they should target, and then they just spend a lot of money on those demographics only. There's, you know, there's different ways of essentially consuming the budget throughout the day, I would say. And so you can think of that as a mixture of temporal spending patterns where you're consuming the budget over time. And we have a model for that. The latest. The latest paper that we are publishing right now is about approximate analysis of the, oh, I should have said marginal MSC. So the conditional MSC is always, or, you know, the vast majority of cases, you can tweak a little bit the estimator is always okay to get. But what really would be interesting at the end of the day is: well, okay, what's the marginal variance? What's the marginal bias over, you know, once you integrate that over. You know, once you integrate that over the restricted set of randomization distributions, and we have only been able to do that computationally for the longest time, and so we've worked out a strategy that gives you an approximate analytical control over the marginal bias and the marginal variance, which is a quantity that people who run experiments are interested in. For instance, in my slide here, this randomization is standard errors computed. Computed through simulation, but here I'm saying we now have a way to compute a good approximation for that quantity analytically. And the other sort of issue that is swept under the rug is, okay, so now that you add more and more and more constraints to the randomization, how do you find randomizations, many of them that satisfy this set of constraints? And well, you know, you essentially have to sample from the space. Essentially, you have to sample from the space of binary vectors of a certain size. And doing that naively is very inefficient. And so now we came up with some MCMC and sequential MCMC schemes to identify restricted randomizations efficiently. And so far, we also talked about this randomized experiment setting, but in reality, there's more that we have done for observational. For observational studies or for trying to leverage observational studies to prime this model-based analysis of experiments. One of the latest papers also is about this biparted causal setting where you have a biparted graph or a two-sided market. And the idea is that in certain applied situations, you may be legally bound by not being able to, let's say, randomize advertisers. And so what you And so, what do you do? Well, you may randomize keywords, randomization units, and then collect the outcome, like lift on revenues on the advertiser. Well, that's legally, that's a lot more acceptable. I'm not a lawyer, but the lawyer tells me that that's acceptable. And so, we sort of ported a lot of this model-assisted design of experiments to this new setting in which you have units of randomizations and you have outcome units. Of randomizations, and you have outcome units, and you know, you have to define what interference means over there, and you know, sort of translate some quantities, but then you can go through a similar pipeline to run optimal experiments in that setting. And I guess, you know, I'm going to conclude with the slide of what is exciting to me as a statistician about model-assisted design of experiments. Well, you know, it works with any given model-independent estimator, so you know, you can. independent estimator. So, you know, you can pick any, you know, you can take Horwitz-Thomson, the difference in means, stratify difference in means, any popular, easy-to-compute choice is fair. And then, you know, with some effort, but not too much effort, we'll offer analytical control over the space of distribution and space of outcome for, you know, finite sample size. You get a lot of interesting finite end results. And it does combine this design-based and model-based advantages. So it retains the property. So, it retains the property of design-based that are due to randomization symmetries, like design and biasedness, even when the model completely fails. And then improve efficiency to the extent that the model is accurate, but obviously lose efficiency if the model is not accurate or bad. So, model elicitation is key. So, I guess that's all I have to say. I'm going to skip this. I have to say, I'm going to skip this example and I'm going to end up here keeping these references that you know going on the top. Thanks very much. Thank you, Edo. That was amazing. This body of work is fascinating.