We're going to give the talk as a group. And also we kind of understood this presentation more like we interpreted it as not about being us teaching you something, but for us to learn something, basically. And this is also the way the slides are kind of built. And we just started off, because we didn't know each other beforehand, we started off having discussion on what we think, what we're supposed to talk about. And yeah, that's our emotional process that we had. Emotional process that we had through this discussion, we basically put that in slides and we'll guide you through it. I can say I already learned a lot. This question is a part. I hope everybody has a lot of fun. So yeah, even if you don't learn anything, we at least. And so Malik is also, so we just decided we always, everybody can jump in at any time. So I don't know. So we have some points we will. Some points we will discuss, just a little bit of a spoiler here. And yeah. I can take this. One of the biggest things was trying to come to an agreement on what different words meant. So I'll have a few slides later on what inference could mean and what the different types of inference, at least I think about, are. And we agreed that they're similar in some way. I learned a little bit what data assimilation meant. I did not know that before these conversations. Meant. I did not know that before these conversations. Also, like model, we've talked about that some. We'll also see like model-driven and data-driven, which will be more confusing because data-driven will be used to train a model. But like overall, for us, there was just a lot of communication back and forth to just establish, are we even doing similar things? And I think the answer was yes, and so hopefully that'll come across and we'll go through. I think that this is I can take this one. So we will consider all these models, all these approaches in two different settings, model-driven approaches and data-driven approaches. So talking about these communication issues, we are not going to go into detail about these approaches. We just want to include them to the We just want to include them to define our terminology before moving forward. So this is I will be just introductory and just to define what we mean with these approaches. So when we say multiple approaches, we talk about building the physiology a mechanistic. So this kind of approaches require some type of knowledge of the physiology and we reflect this understanding through the differential differential equations. Different generally differential equations. They could be ordinary differential equations, partial differential equations, or even stochastic differential equations. So, um, I just cannot see the slides anymore, but I have the whole computer in front of me. So, okay. And you can move on. Okay, no, sometimes the camera changes that I cannot see this screen, but now, yeah, that goes. So, and for this kind of approach, it's And for this kind of approaches to do estimation, forecasting or control, we generally use data estimation methods. That could be like filtering methods, common footprints or other types of NTMO footbrings or optimization-based methods or MPM thing methods. So, this model is just here for illustration purposes. I've talked about this many times. I've talked about this many times, so I won't go into the detail, but this is an example of a mechanistic model. So this is a very simple case that we there are, if we have seen so far, there are much more complicated models describing the glucose system of humans. This is one of the most simple ones. We developed this one method. This was birthed with Dave Andrew Street and Matt. So we developed. So we developed this method, this model to deal with some issues in the computation. So when we want to do forecasting or control, we are able to construct with the data, the real-world data that we have, and we generate very sparse. So to deal with this sparsity issue, we develop this very simple model. And as you see, we include And interestingly, we included our delete about the system. For example, the first term is regulating the blood flux level but with all effects and then we included some nutrition and insulin effects. So again I'm not going to the detail but here are some model parameters that are mostly representing some physiologic values. And in the next one this is just to show you how the model output looks like how model output looks like in a dummy user mechanistic model. Most of the people probably are familiar about this, but the model output in this case is in the form of a continuous function. This helps us a lot in many settings. With this kind of modeling, we do not do any more point estimation, point prediction, but we can simulate the model output as long as we Model output as long as we want, and we can just check if it makes sense, if it's in accordance with this reality. And so, here the small red circles are showing the actual measurement, and this is just the simulating model output. We learned the estimated the model on a timing circle before this one. So, this is data set that the model has never seen, and we are actually forecasting. And the blue curve is the mean of Blue curve is the mean of the model output, and since we have a stochastic differential equation here, it's a bit different than what we are used to before being, it naturally provides this confidence band kind of medium, those grade of band era. So it helps us to understand the mean behavior and the magnitude or amplitude of the observation. Or amplitude of the oscillation that could occur in the blood flow back. So these are the phases and things that we can do with the mechanistic model approaches. The next one, I think, Jana, to take this one. So yeah, as Marika now presented kind of the model-based approach where we already have a model and then we generate simulations which then ironically we sometimes then interpret this data or we use Data, or we use actual observations to fit parameters of the model that's already there. But then you can also start completing from scratch and say, I have some observations, some data I collected, and I want to somehow get some kind of information out of it. It could be trying to get a proper model, it could just be to get pattern, it could just be to do dimension reduction. I mean, Matt explained this or Matt explained this already pretty well yesterday, so we kind of reduced a little bit what we're going to do in that direction. But yeah, generally, like again, it could be just something like you could do just something linear, you could do linear regression, or you could do clustering, or you could try to really fit a super complicated model. You might even already have some kind of parametrization of it. But essentially, for all of these techniques, no matter what you do. These techniques, no matter what you do, you have these samples, and here I assume I have kind of an input and an output, but it could just even be unsupervised data where you just have input and no labels at all. And then all you're trying to do is really because you're actually trying to estimate an expected value, but since you don't know what the distribution was that was actually generating the data, I Generating the data. You're trying to somehow estimate it in a multi-colour fashion through samples, and all you kind of need to do is choose your favorite loss function, L here, and also make some kind of assumption about this family of functions that you're trying to fit. So, again, this could be super simple in the sense something linear, or you could go with a huge neural network and try to estimate the. Network and trying to estimate the weights of the biases of the neural networks. But in a way, it doesn't matter what you choose. All of these methods kind of come down to that choice here and trying to estimate or minimize this kind of cost function with respect to your data. And on that level, you can really also easily compare all of the methods you're using because one of you might be using. Like one of you might be using, I don't know, dimensional reduction, and the other person might be doing linear regression or putting neural networks, and at that level you're all coming together again. And it doesn't really matter what your specific choice of family functions or that bar. You just have the same problem, you're trying to find this unknown parameter w. And just to give you, I don't want to go into all the different versions. All the different versions, but just to absolutely convince you that at that level, we're all brought together on the other side, is you can really plug anything in here. You really just need to choose your flavor of loss function. And as I said, the parameter organization that fits maybe your data or your think, that's actually the other thing, that you think fits. The other thing that you think fits your data well, this might be also a completely wrong choice as well. So you just go with something and then hope for the best. But sometimes you also have some kind of like maybe prior information that you say, okay, that's why I'm using it, or that's my task that I'm actually trying to do. For example, I'm trying to kind of unsupervise learning and really divide something, but then later on I want to do prediction as well. And yeah, so the suggestions here. So the suggestions here are, for example, you could do linear regression, which I assume at some point in your life you all had to kind of do. And then here you actually have labels, but then similarly you could also just say, okay, I don't really have this information, the YIs. And then to do k-means clustering, which is the most basic, but at the same time, still a very popular form of pattern recognition, where you can cluster. Pattern recognition where you kind of like cluster centers and then your loss function looks like this. And your unknowns are basically the cluster centers. And here, for example, in the linear regression part, your unknowns are just the four factors that go before your input. So basically how much your input variables are affecting the output variable. And then I just chose a percept term because it's complicated to write down. Because it's complicated to write down a full neural network. But of course, you could try kind of here, you could, this is basically the very easy version of just a neuron. And your information you're plugging in is just minus one, like the labels are just minus one or plus one. But you also could think here of something more complicated where you actually have a full-on neural network with like many, many hidden layers. So that would also be. Hidden layers, so that would also be possible. Or you could go support vector machines again, just trying to what you're trying if you haven't heard of this, or trying to find basically a hyperplane separating your points. And this is also essentially to kind of distinguish certain data points from others. So, yeah, this is what you could do if you actually have no model or maybe don't like your model and try to find a new one, which I'm assuming here some people have. Which I'm assuming here some people had some issues with some models. And maybe one of these methods is here. And why did we separate the whole thing into kind of like our whole discussion started off with, okay, what do we want to present? And to have heavy discussion to us later, we basically came up with some of us have models, some of us don't. We all kind of have data. There are all these different words out there. Like, how can we connect them? Like, Richesh has been doing inference, we've been doing data estimation to realize, oh, they're actually kind of the same thing. They just have different words for them, I mean, even. And the data estimation community, I like for the particle filter, you could say sequential Monte Carlo, you could say bootstrap filter, you could say particle filter. Depending on what community you talk to, they would understand you. Talk to that would understand you. Also, sometimes you have particular communities like in the American weather petition community, you have people who work a lot with ensemble common filter. But then if you go to people who do more, for example, robotics or other communities, they do unsend it common filter. And if you like, either community basically has not heard of the other filter, and they're hugely popular filters also. And they just ignore each other and are like, yeah, they're doing their thing. Are like, yeah, they're doing their thing, and we're doing our thing. And it did actually happen to me. I was at a couple of years ago, but I had already worked in data simulation like two years, and I thought I knew my way around the world in data simulation. And somebody was like, oh yeah, and I'm using uncended camera filter. And people were like, you don't know this. How can you be in data assimilation? But it's like really Jerk and interface. Like midi-gear community-based. And yeah, he was trying to kind of like connect the dots and kind of say, okay, we have this model-based approach, we have this database approach, and how we can now kind of bring them together or at least use inference to kind of... Because you can also see it because you already, even if you have the database approach, since you do assume some kind of parameterization, you could say, I have a model, it's just not very elaborate, or I have to fit a lot of. For I have to fit a lot of things till it becomes a labyrinth. But yeah, we're like far from done with this slide in terms of connecting everything. So if you want to later on help us to make more dots and connect them somehow, yeah, we're looking forward to that. Yeah, or do you want to add something to that? I think I'll go on to these types of inference. I've had the pleasure or maybe displeasure of working on all these types of inferences. All these types of inferences. And I think it could be helpful just to set out what they mean. So, Bayesian inference is where I started. In Bayesian inference, you have a prior belief represented by a distribution. That's what this curve is here. You get some evidence. So you have some data. It tells you that for the parameter values that are over here, your data will be more likely. Your data will be more likely. That's what this plot is telling you. And then some combination of the two, a multiplication and a renormalization, will give you your belief afterwards, your posterior belief. And this is a very clean setup. You know, you start with what you know, you observe something, you update what you know, you get to see more things, you get to update it further based on your posterior. Because after you observe some observations, your new prior belief will be your posterior. The challenge for most The challenge for most of the Bayesian setup is that it's really hard to do exactly. This integral to renormalize it almost always you can't do computationally. Or sometimes it's even messy analytically when it's easy computationally. So you have to resort to Markov, Chain, Monte Carlo, or variation inference optimization approach. Another form of inference is probabilistic inference. It's Bayesian in flavor, but I like it. It's Bayesian in flavor, but I like to separate it out from the previous story because here you have variables that you're trying to make an inference about on a per-observation basis. So Noemi had some slides yesterday about uncovering hidden structure for phenotyping, I think. And here's an example of this for modeling documents. There's some structure that's hidden and shared across documents, which are the topics. Documents, which are the topics that are in those documents. But each individual document expresses a different proportion of those topics, and those specific to every observation. So this story in Bayesian inference, where you collect some data, make an inference, that becomes your posterior, you collect some more data and update it, is a little bit broken in this probabilistic inference setting because you only ever get to see this one document. It's not like you can go back to the author of the document and be like, type some more words here, don't make it long. Like, type some more words here to make it longer for me so I can make my inferences more precise. But these models are super helpful, and they come in older forms, like the topic model, and newer forms, things like a variational autoencoder, is an example of a latent variable model where you're doing probabilistic inference, and Bayesian computation plays a role, but it's not Bayesian inference in the sense that we just saw. Why do you think of Bayesian inference as having a sequential aspect? It doesn't need to. Aspect. It doesn't need to. It's just the way I think about it in terms of the world. Like when I write down something and I'm doing Bayesian inference, I'm like, what do I know already? You don't need to compute it sequentially. In that sense, is this topic model of Bayesian? I mean, you have a prior and you have a model, and you have data, and then you have the posterior. So one part of it is Bayesian, this part is like, when I think about, you know, action. Like you know, actually making inferences that will concentrate to a single value, then I'd say yes. But this part wouldn't fit the structure of like standard Bayesian, the Bayesian setup. You have a prior, you will get a posterior, but it will be fixed based on your belief, and that posterior is growing with the amount of data that you're observing. Every new example that you're getting is going to get a bunch of hidden variables with it that you'll want to make an inference over. So, even things like So even things like posterior concentration to the truth would go away. So there's one on statistical inference. This slide has no nice picture associated with it. It's maybe the old, Bayesians might disagree. But if you take like a traditional stats course, this is sort of what you learn from it. You learn how to estimate your parameters, things like the loss functions that we saw earlier, maximum likelihood is an example. That is an example. Probably, if you talk to a statistician, they'd separate this into just estimation and they'd call the latter two inference, where they're trying to quantify some uncertainty, either the probability that a parameter will fall in a particular interval if you resample the data a bunch of times, or hypothesis testing, which is kind of a binarized setup where you assume some null hypothesis and you want to reject or retain it based on your observations. Kind of important, but somehow. Kind of important, but somehow distinct from the Bayesian setup is what's random. Like, when people write down a Bayesian model, your parameters are random. Your data is fixed, you condition on it. When you do statistical inference, the data is really the source of randomness. When I ask for like a confidence interval, I want to know if I collect data of that same size. I would like say 95% of the time for data of that size, my parameter will fall in this interval. And finally, kind of causal inference. This is a picture of a causal directed acyclic graph. It's a little bit of a fancier one than the standard one. So the standard one you'd see probably from observational data, you'd have three variables. There'd be a treatment that influences an outcome, and there are confounders that potentially explain the relationship you might observe between the treatment and outcome. This slide has an extra variable on top of this. Slide has an extra variable on top of it, which is an instrument. Instruments are heavily used in economics to estimate causal effects. They are things that only influence the treatment, roughly. And so the idea is that they give you a little bit of randomness that can be separate from what you'd see from the confounder. There's an example, I think, in the earlier presentation, this idea that the provider you see conditional in your background creates. Conditional in your background creates variability, probably as an example of an instrument because it influences the treatments, it doesn't necessarily influence the behavior. There are different forms of causality. Like if you look at whether you talk to a statistician, you talk to a computer scientist, biostatistician, maybe there's other groups that make philosophy, they'll give you different notions of causality. Of causality. One notion of causality is Granger causality, which is predictive, which looks at the box kind of myopically, says you have a set of X variables, and then you want to know, say, in the next time step, how does each variable influence the other. The more, you'll see my bias coming in in about a second. The more general form that you would consider is like where you want to know if something that's interventional, whereas you have a functional view of the world. Or if you have a functional view of the world and you were to set some value to something, you want to know what would happen if you were to do that. Pictures that people use to represent the causal relationship between variables are causal networks. Each parent in this graph is a cause of the child. So here you have that y is a cause of z, z is a cause of x, y causes x. Y causes X, but it's mitigated through the security. And there's some important facts that are just worth highlighting. We were actually debating this first point here on the board for a second. There's a difference between causality and correlation. In one direction, if you don't observe a relationship between a pair of variables, it is not likely that there is a causal link between them. There's some math here we can talk about later for why the. Math here, we can talk about later for why there is mostly. The reverse is maybe the standard example that you see. If there is a relationship, you have to be careful because that relationship could be explained by confounding. That relationship could also be created by you conditioning on variables that induce a dependence. People call them colliders. So, like, if you condition on something that depends on both the treatment and the outcome, then suddenly the treatment and the outcome will look dependent. We'd like to always imagine for a lot of the problems that this causal structure is given a priori. Meaning that somebody is going to tell you, hey, here's one variable that causes another. And then your job is really to make an estimate of what the effect is, given that you know the causal structure between the variables. But this is like an important problem as well, too. Like actually figuring out what causes what. Out, like, what causes what. You can't do it straight from observational data, you can only reduce to a class of graphs. But then you can start asking yourself what experiments might you need to run to reduce to a singular causal graph. I think we wanted to point out, I mean, some of you work in causality anyway, and I'm not an expert on this, but I think it is a huge problem that people Problem that people mix up correlation with causality a lot. And despite this, actually, it is not trivial to actually then find your definition or fit your kind of cause of graph. But I think it's something worth talking about because I think that's a lot, like a lot of the problems stem from this kind of issue that people mistake one for the other. In particular, In particular, yeah, because it's not well explained on the math side sometimes, and then people in application just use it because nobody really pointed that out ever. And yeah. There's also the people who take the word causation seriously and the people who don't. For example, when I talked to Dave Sontag, he would say, no one takes causality seriously. It's not real causality. It's not real causality, because it isn't, probably. But there are people in the community who do take it very seriously, which is a barrier for medicine because if it's probably impossible to know. And doctors probably won't trust it if you take it from. I mean, I find it also funny because sometimes when you just go like open the newspaper and then you have and then you have all these articles sitting together as a family at the table for dinner has all of these effects that kids are better in school. And then there's this whole article, people get super excited. I want to basically always write to the newspaper take look but this might be people who actually have the opportunity to sit together at the table at night maybe are also have a higher income and that's why they have their kids are better in school and stuff like that. Their kids are better in school and stuff like this. And I think it's also a communication issue when we try to explain our scientific results to the general public, and then people really fast kind of go through a correlation and jump on it and kind of assume that's a causality. Yeah, this slide is. Yeah, this slide is more about then actually trying to fit somehow these causal graphs, depending on what you believe, could describe causality. And there's many different methods for this out there, but one option you have, just to give you a quick idea, could be, for example, look at correlation buck over his condition on all the other variables. And then if there's correlation, condition. Correlation condition on all the other variables, you can kind of have a better idea, maybe if it's going in the direction of causality. And then you could do things because obviously there are many complicated things you could think of because causality might change also in time and then you have to make up for this fact as well and have to do kind of regime, like figure out where the regime switches are in time. I have been fortunate enough to be on a study where we actually. Been fortunate enough to be on a study where we actually did that. So that's why this slide is in there. But yeah, it was interesting work, and I think, yeah, I would like to actually, that was my only collision with causality. I'd be interested to look at it again. And also it was an atmospheric modeling. Not biomedical at all. Do you want to say anything? And then we have a little bit of a detour because one question we also had was kind of. One question we also had was kind of a talk about batch versus sequential. At some point, we're kind of discussing if we should discuss this because personally, I would say I'm like sequential all the way, but a lot of people do, for example, for neural networks. I don't know about you, if you would consider that, even in the world of data assimilation, you can go basically batch, which would call it variational, and then you kind of approximate some kind of maximum. A maximum a posterior estimator of your means or something. But yeah, and then this whole sequential batch was kind of then going in the direction of sequential learning. And when you're in the world of sequential learning, one thing you could also consider is sequential decision making alongside of sequential learning. And just to give you the quick introduction to it, if you have not seen this before. Seen this before. So we have heard about reinforcement learning. I will actually also talk about this on Thursday because I use here in reinforcement learning. But the most simple example you can actually think of when you think of reinforcement learning, so the special case is actually multi-air branded. And the setting there is, just to give you, like, obviously the applications here are not. Oh, you do have ten minutes. Okay. You do have two minutes. Okay, so the name comes from the slot machines you have, but that's just, I don't think it has ever been applied in that setting. But the original application, actually, that was in the 1930s, was actually clinical trials, where they were kind of trying to figure they had different compounds, and they were trying to figure out which of them they should develop further or use in the next phase of the clinical trial. Of the clinical trial. And it was really just making a decision between a number of different compounds and then choosing at each point in time which ones to continue with or at least do a little bit more experiments with. And then you could gather this data sequentially and then over time make those decisions. Just to give you an idea, this is because it is like a somehow simple model, but it's actually being used everywhere. It's actually being used everywhere, like from Google to Instagram. They all use it because often they don't have a lot of information. They just have the choice of showing you an advertisement and then see you react to it. And that's all they go on. Basically, you click on it and that's their kind of reward. So they are happily studied in those larger companies. But despite this being maybe a little bit of an application you might not care about, it can. You might not care about, it can actually be super useful also in the medical application. Because sometimes you tend to not have a really complicated setting, you just need to make a decision. Just the mathematical setup is also actually fairly basic. So you have a number of actions you can perform, often referred to as arms, due to the original kind of association with the slot machine. You have like one arm bandle that you're trying to pull. And with each of the arms or actions, you kind of associate a distribution. This distribution is unknown to you, so this is part of the issue, kind of figuring out what the distribution would be like. And the whole procedure is just really simple. So in each round, you have a you can make a choice, you can perform an action, A. You can perform an action A, and then you receive some kind of reward. That reward is kind of tied to the underlying distribution union. And by receiving this reward sequentially in time for the individual actions you perform, you kind of gather more and more information. And just to give you a really simple example, I think just over Bernoulli settings with Bernoulli. Bandits, where you just either receive positive reward or nothing, and you receive the positive reward with probability mu A. And this is basically describing the underlying distribution. And let's assume we have three arms, because I will later come back to it. And these are the different means or probabilities we have for these three arms. Realms and yeah, just I mean, we don't need to get into it, but yeah, you basically, when you open your Netflix, what's happening is that somebody, there's a bandit algorithm behind it suggesting movies to you. And basically, each of those movies, like here it's a little bit complicated because you don't really see which one is the one that is being presented to you, but sometimes when you open the Netflix, there's one. When you open the Netflix, there's one on the very top, and that's basically the action that is performed. You open the site, and then they go, Okay, let's see if we can get them to watch that movie, and then they show you that movie. And yeah, that's essentially happening in your Netflix account right now. So, if you ever wondered why it is suggesting to me to watch Gilmore Girls, the algorithm seems this you might click on that and then you. You might click on that, and then the reward would be for you to click on it and watch it. Or even like it could be more complicated for how many minutes you watch and not just that you click on. Of course, the confounder is you watch things on many things that aren't Netflix, so it's missing like three quarters of your data, so it suggests things that you may not really be interested in. It could even be worse. It could be that it's not personalized to you, but that it's tied to every time somebody opens a web page. It could be that you classify. Web page. It could be that you classified in different groups that you belong to, they at some point recognized you belong in a certain category. But it could actually be every time somebody opens the page, so even not personalized to you. So every time, and then if we watch really weird stuff, you're being suggested. Yeah. Okay, but yeah, what's the okay, great. We have these arms we could potentially pull, but yeah. Five minutes, okay. Yeah. Five minutes? Okay, I'm a little done in there. So, um but what's the whole goal? Obviously, you want to minimize the regret, and what's regret? That's really just because there's one arm that is somehow optimal. For example, it could be having the highest probability to spit out a report. Think of a really good movie, or if you want to think of the medical setting, obviously, something the treatment that would actually ban like here that the patient would benefit from the most. And especially, like in the And especially like in the Netflix setting nobody cares if you have to watch one bad movie. But in the medical setting, this is super important. But funny enough, for Google and so on, it's also super important that you buy whatever or click on stuff. So they really put a lot of effort into looking into this regret that is basically saying if you would have chosen the right arm from the very beginning, or the right advertisement, or the right medicine, how How are you comparing to what you actually did, the choices of actions you performed? And you basically want to minimize this, and there's a lot of actually theoretical work going into finding this. And why is this interesting to you? Because this is a special case of reinforcement learning. Essentially, if you then infer something with reinforcement learning, it also goes in this direction: how bad are we doing if you learn on the fly, for example? On the fly, for example. Because in game scenarios, you could make it learn just by generating a lot of games or something, but maybe sometimes reinforcement you actually want to train while you're kind of performing medical treatment. And so you could do very stupid things. You could just say, I choose each arm, one like half of a specific time horizon, and I just go for each of the arms. I have K arms and I divide. I have K arms and I divide the time and I just check all of them out, which is like obviously not a great idea if I'm trying to introduce a rubric. But it would at least explore what's out there and I would have a good idea afterwards how good or bad the arms are. Then the opposite would be to exploit. Basically, I choose one arm in the very beginning randomly and then I see like I compute the maximum over how the arms performed in each Performed in each time step, and I always take that one. That would be basically exploitation. And that could also be really bad because I would assume basically it's being stuck in a local minima. I assume this is a good arm, but it's actually really bad. And this is the trade-off we're dealing with. This is what people always talk about, this exploitation and exploration. We had this yesterday as well, a reinforcement only talk. But this is really what it's all about in a really simple setting. And what people do. And what people do, just to finish off, is they obviously want to mix and balance between the two. And basically, you want to pull each arm, you want to explore to the point where you have a good idea how well, like what's how what's the mean of this arm. And that's what you do. I don't want to go into details here, but if you're interested, come and ask me. And that leads to one of the UCB one aggro. One algorithm, which is a UCB stands for upper confidence bounds, and there you kind of basically go with the best arm, but you also put some kind of uncertainty bound on it where you say, I actually don't know that much about that arm, so let's keep exploring it till the uncertainty is reduced. And then at some point, basically, the best arm should be kind of present itself. And yeah, that's where I'm thinking. But yeah, that's where I'm finishing, just to give you, like, this seems to be okay fine while she's having used this. But this is, for example, reinforcement learning, the best algorithms out there also use this exact scheme. So that would be Monte Carlo tree search with APA confidence bounds would be exactly that. And this is also, for example, the AlphaZero, AlphaGo uses exactly that. The lines of code are actually, it's like five lines of code. It's very simple. It's very simple. That's it. We have a few questions. We have some challenges. Sorry, sorry. Sorry. There are a few challenges that we'll see what we're going to get into. So there's some challenges that we just wanted to discuss that align the two. One question was why not more hybrid approaches between data driven and model driven methods for forecasting and prediction? for forecasting and prediction. Like if you're a me