 Okay, next talk my buddy Direct Sundiquiti of the direct master. Thank you so much. It's amazing to be to be here and see you all again. To be here and see you all again in person, in bath. So, yes, this talk is gonna be one day. Ah, this is not plugged in. I know no, it is. It is. Okay, so it should work, right? But it does not. Yeah, it is. Ah, okay, just to the other way. Okay, so there's gonna be three parts. I'm gonna talk a bit about the direct mindset, that's just introduction. And then these are the Just introduction, and then these are the titles of the three talks, or three parts of the talks, and I'm going to also tell you what these things mean. So, okay, so I guess you all want to do first passage percolation. This is a picture of that. For some reason, this patch was going in one way, but this was because it was, I don't know, it always worked out like this when I tried to draw it. But you look at this path in first passage percolation. Look at this path in first-passage percolation. I'd like to do KPZ scaling and hope that it goes to some limit. And the scaling should be n like this and to the two-thirds that way. And this limit, you know, we can't do any of this, but what we can do now really well is actually describe what the limit object is. And so that should be this directed geodesic. And then the way we do this in last passage, where we can do it, is that instead of looking at the geodesic, we kind of understand the geometry. The geometry in this window represented by the scaling. So you see, I put those scaling numbers over there. And so this distance, this first passage distance should behave like some linear part plus some fluctuation plus some error. And the fluctuation itself, you can think of it as a geometry. And that should be universal. So that geometry is the directed landscape. So, you know, it from this, even just from this setup, you can see that it should satisfy the triangle inequality and the points, distance from a point to itself is zero. It's more like a negative of a distance because it comes from the last passage. We also know that the one-point distribution, so we're seeing lots of other distributions, lots of joint distributions as well. And so, this object, which you know, it's not exactly. It's not exactly a metric, right? Because we have subtracted some linear term. But it still satisfies a triangle inequality, and you can do a lot of things with it, like you can do with a metric. You can construct geodesics, for example, in the usual way. So this is all I want to say about the directed landscape, because I only have short time. It's a random continuous function. And you can use it just like you use a distance function. Alright, so yeah, one thing that I want to say is like, you know, what exactly, what is the information that it captures? So the nice thing about this limiting object is that it's a lot like the original object. So it's just like original object with some kind of distances on the plane, and now you have some kind of directed distances. So in some sense, it's a full-scaling level. So for example, if you want to understand want to understand uh you want to understand geodesics you know let's say we're we also had all kinds of understanding in the past of this limit and certain you know certain you have various marginals on the directed landscape that tell you some information about the geodesic tracy law doesn't say much about the shape but once you have the airy process you can tell actually a 1d marginal law of the geodesic that's determined by knowing the airy process if you have If you have the KPC fixed points, then you can get the 1D marginal from a general initial and final condition. So that's something that you can express. But if you want even any kind of two-dimensional marginals, these previous things don't express any of that. So you have to go to this kind of language. So for example, if you want convergence of those things, you need convergence to the directed landscape. Convergence to the ARIE processor, the KPZ fixed point will not be enough. So, this is how it's kind of different from other limits when you describe to you that when you study point processes, the corresponding objects that are hard to express from the fixed points are second-class particles. So you can express from this, but harder from fixed points. Okay, so my first talk is about the fox and the rabbit, and there's going to be here a question. So there's a fox, and it's chasing a rabbit, and it gets to And it gets to some field. And then it doesn't know if the rabbit ran through the field. It's like to tell whether the rabbit has gone through the field or not, because that has obvious implications for a fox that is hungry. So so here is okay, so so here is actually the math problem. So so on top is just the field, so it's planar white noise. Is just a field, so it's planar white noise. And on the bottom, there is a field with the tracks of the rabbit. The tracks of the rabbit is exactly the graph of one-dimensional Bramian motion run until finite time. And so, what is exactly the math question here? So, this is some distribution, like this is some law on distributions on the plane. This other thing is the law of some. Is the law of the sum of two things, right? The white noise plus this occupation measure on the Branian graph. So they both live in this infinite dimensional vector space, and you put them independently. So this lies a convolution of two things in a vector space. Convolution of the Lao of the Branium. And the question is, right, you want to tell whether this is whether. Is whether the rabbit has gone through, whether you can see there's some track like that. The question is whether this law is absolutely continuous with respect to that. So that's the corresponding math question. What is the second law? So the second law is you take the white noise and add to it an independent measure, which is just a measure on the path of the graph of the gravity motion. Or a singular measure of the path. Singular measure. Everything is very singular here. It lives in this area. It lives in this dual space of something. But you can still be. And what you're asking, right, is that whether the law of this is absolutely continuous with respect to the law of it. So usually I take a vote for this, but this time I don't have much time. So actually, so the answer, there are two things I want to say. First of all, the answer is yes. It is. This is true for every time. And second, that the law of That the law of the derivative is actually solves the stochastic heat equation, one plus one dimensional. So its law, for example, of this radio neckline derivative is exactly a crossover distribution. But if you would take as a measurable set this path, sorry? If you would take as a measurable set this path of the it would have mars in the bottom, you're saying it would have some mars in the top? Or am I just Or am I just you don't you don't really know how to take that path. You don't see the path, you just see the convolution. So by the way, it doesn't really look like that you can tell, it looks like you can tell the ravage, right, from this picture. But actually when I was making this picture, I first just put on the ordinary Branier Motion on it, like what you pop got from the web, and I I couldn't see it. So I had to make it fat. So now it's like kind of a sausage. So first thing you can tell. Crazy, you can tell. All right. So, actually, I'm going to prove. There's a proof in this talk. Okay, so xi is going to be cleaner white noise. Yeah, you're trying to understand that. Yeah, yeah, of course. This is like a Poisson processor, highly dense, right? White noise. Sure. The rabbit is jumping over the Poisson points. No, there is nothing. No, there is nothing. There's nothing. This is much more simpler. This is just white noise. Don't think of it as a plus some process because it's the wrong. It's just white noise, which is an element of some infinite-dimensional vector space. This is a convolution of white noise with something else. And the something else is just this measure on the Branian path. It has nothing to do with each other. I've just convolved this to this convolution in this huge vector space. So I am going to prove actually. So I'm gonna prove actually. And I think the proof is, you know, maybe not for a beginner, but I think for you guys. Because you are all in this area. I've sort of seen something like this. And it's only one page. So B is going to be the occupation measure on the graph, okay, just brain emotion, but think of it as again as it's the graph, not the process or anything. It's just a measure on this path. And psi. So those two guys are random variables that now take values in the same. Of variables that not take values in the same infinite-dimensional vector space. So, in order to show that they're absolutely continuous, we can just find things that converge to these things. That the rate that they could think of those pairs is actually tight in R1. So, that's all we have. That's a little exercise. So, what we're gonna do is we're gonna take this Ïˆ and take this BN and project it down. And project it down to the first n basis element of L2 of R squared. You can pick your favorite basis, whichever you like. Okay, so once you projected them, of course then they will converge to xi and we never converge to p because it's a basis, it's just the property of these things. So they will converge, that's done. But we have to show that this thing is the L1 side. Thing is the element side. Now, but this is nice now because now we're talking about finite dimensional random variables and just already dominated in derivatives. So it is simple to understand. It's not like white modes and stuff. So we're just going to compute the L2 norm. Turns out if you do this, the Zn is actually martingale. But we just need to let it ZL2 bounded. And how are you going to compute it? So, well, it's not so hard to compute. You have some Gaussians and so on. And so if you compute. And so, if you compute the expectation of Zn squared, you won't be surprised that they get some kind of exponential of an inner product, because we can go similarly. And what are the inner product thing? You have seen this, this is kind of like replica. Take this Brownian motion and take an independent copy, look at the projection, and take the inner product, exponentiate, take an expectation. Certainly, something you have seen before. Take independent copies, right? This thing always comes. Independent copies, right? This thing always comes up in this kind of polymer world. And so the exponential moment of that is exactly the expectation of zn squared. This is a general formula for polymers, by now. Okay, so what is this, by the way? What is if you take two Branian path and inner product it? So first you make it kind of a sausage by projecting it into the lower dimensional place. Take the inner product of those things, right? And let them go to infinity. Well, it actually converges to the intersection local time of the two. Section local time of the two graphs. And in fact, this expansion moment is bounded above by the corresponding expansional moment of the limit. So we just have to know that this is finite. Well, what is it? So it's just intersection of two graphs. Well, it's the same as the intersection of a single graph with zero. So it's the local time of Branian motion of zero until time one. Local time, well, that just has the same well as the maximum at every time. Maximum at every time, right? Maximum just is the same as the absolute value of a normal. So this actually is an absolute value of a normal here. As an exponential moment, so we're done. Okay? So we had one proof. So the thing is, they're absolutely continuous. Why it satisfies the heat equation, I can't tell you because the shtock is too short, but there is a very nice story about that. So we basically it has to do with Feynman. It has to do with Feynman Katz formula and squirrel integrals and so on. But I can tell you that now. But here is one more thing. So you're probably familiar with the continuum directed random polymer, this polymer that lives in the K-P's equation or the stochastic heat equation, right? So here is the one-line definition of the polymer. So what is the definition? So you want to understand what is the chance that the polymer is in some set, right? The path. In some set, right? The path is in some set given the noise. That's how you define the pointer, just to say these numbers for all sets. So it is exactly the same as before. You just take the law of the noise, law of the Branian motion now, but just restrict it to that set. So this is the sub-probability measure here. Take the convolution and see the Rasenicadin derivative. This division is just a Raso-Nicotin variable. One-line definition. Definition. And it's actually quite nice to work with, surprisingly. But you know, often these definitions, they're hard to prove that they actually define the thing. Okay, so using this intuition, I want to tell you what we try to do and we did with Jeremy and Alejandro, which is sort of try to understand KPZ in these non- KPZ in these non-directed settings. So you want to do maybe the hidden model, but that's too hard. So maybe you want to do some kind of finite temperature version of that. And the stochastic heat equations, you can think of them as finite temperature version of first passage calculation, more or less. So you take Laplacian plus noise, and there are all kinds of versions of this story. You have seen various versions of it. Just pick one that is the It's version that we just pick one that is the easiest to work with, or maybe the most elegant, because it's try to show KPZ limit for this kind of things. So all this goes back, by the way, to the paper of Anderson from 1957. So the model that you choose is the following. You just take Laplacian to be the ordinary planar Laplacian, and the noise is just planar white noise. So that would be, it's a little trade-off, you know, it's harder to define things. Off, you know, it's harder to define things a little bit, a little bit harder to define things, but on the other hand, once you define them, it's nice to work with them. Much nicer than these discrete models. And all the difficulties that are in here, they also appear in the discrete models just more here than usual. So we studied this one. And here is a theorem. So let U satisfy the weak order planar stochastic heat equation. So this is stochastic heat. Heat equation. So this is stochastic heat equation where the noise doesn't depend on time. So if there is no noise, you just get a two-dimensional normal, right, or variance T. If you put noise here, you get Mount Fuji. So I don't know why, because Mount Fuji really shouldn't satisfy the stochastic heat equation, but it looks like it does somehow. And now you're asking, you're sort of looking out far out in the mountain, and looking at this. And I'm looking at this solution. And the theorem says that the theorem says that the law of this is converging to the crossover distribution if you go sort of time n roughly and and distance n to the three halves. You've got to go you have to go kind of reasonably far. Kind of reasonably far. So there is now an undirected model over which you can actually show some KPC. And how do you do this? So usually when it converts to the K-P's equation, what you do is some kind of chaos expansion. And the reason you can do chaos expansion is the following. So the object that you'd like to do here is just Here is just using this analogy that we had before. Is now you look at the now you look at the measure on the path of the Flanar Branian motion. You add this to white noise and see the radar necademic derivative of that with respect to the white noise itself. So you can sort of show that this is analogous to the previous thing, is kind of the solution of the 2D heat equation. And here, E is just the occupation measure in the two-dimensional. Occupation measure in the two-dimensional Brownian path. Again, this doesn't care about time, it just cares about how much have you been at what places. And it turns out that when you want to approximate this with finite things, this limit actually will not be in L2, so you cannot approximate in L2, except when t is very short. There is like a critical time in which this fails to be in L2. So you can do Clear's expansion. So you can do Cleos expansion because things that are not in L two don't really Cleos expand. So we have to use sort of the ideas that we have have done. And what you can do is basically what you can do is sort of throw away some set of the path space so that it's still in L. But as long as that set that we throw away is small enough, we can make it arbitrarily small, then you can still define this limit. And so that's what to do. That's a little bit of a technical part. Bit of a technical part. So, this way we sort of give an explicit solution of this 2D stochastic heat equation using these radio kitting derivatives. And then the next thing we do is convergence to KPZ. So, the main idea, of course, is that the path of the two-dimensional Branier motion, when you stretch it out very much, looks just like the graph of one-dimensional Branier motion. So, that converges it converges is okay, but you need some kind of L one tightness. Not surprisingly, Not surprisingly, I find as well. And that's sort of hard and technical. You have to sort of analyze self-intersection local time in 2D or mutual intersection local time in 2D. So that's roughly the proof. Okay, so I'm going to talk to the next topic. So this is about this topic is about the following. So balanced vector. Valued vector. So this thing is coalescing random walks going up. And now you're going to define a distance, which is very simple. So you're only allowed to go up with the path. Here is like two points. You want to understand their distance. You can go along this path for free, but every time you use an edge which is not in the path, you have to play unit one. So for example, here is So, for example, here is a path that optimizes in between these two points, and you didn't use this. This is outside the tree, that's outside the tree, that's outside the tree. So, the length of this path is 3. So, we have defined the distance on this tree. By the way, this coalescing random works from a single tree and with some kind of last passage model. And so, what we looked at is what. And so, what we looked at is what happens when you rescale this thing. So, the natural rescaling in which this collessing random ones converges to a nice object called the Branian web. And what we showed that if you do the 0, 1, 2 scaling, the discrete web distance also has a limit. This limit is a distance on the Brownian web. And the distance on the Varyan web is very, very similar to this original distance. Basically, you can go in the trees for free and you. You can go in the trees for free and you can jump integer number of jumps between various trees and you just count how many jumps you have to do to get from point A to point B. So since we have taken a scaling limit here, we've got a scale invariant this time. And just like KPZ is 1, 2, 3 scale invariant, if you do the same numbers, this is 0, 1, 2 invariant. So that may seem like a contradiction. It's very hard to have a 0 invariant. It's very hard to have a zero invariant, but it's actually not. Partly because there is an interesting thing. So, this distance, once you get to the Brownian web, is typically going to be infinity. So, typically, you won't be able to get from point A to point B unless point B happens to be on the skeleton of this Branian web. So, that basically means all the interiors of this Branian path, which correspond to the random walk path that you saw before. To the random walk path that you saw before. And another interesting fact is that in the Brainian Web, this distance is not symmetric. So if you replace xs and yt, you get something completely different. For example, if you just look at when this thing is finite, when the distance is finite, well, the condition for being finite is that yt is on the skeleton. Xs doesn't have to be on the skeleton. So you have this kind of a web and you can travel. Have this kind of web, and you can travel around it, and you get a discrete value of distance, which is scale-in-variant. You may have seen Martin Hayer talk about the Brownian castle, which had this nice property. It was a scale-invariant process. It wasn't the distance. Now, this is scaling variants, it is a distance. This is a directed metric. And so, we think maybe it's a universality class, it's just its own universality class, and there is no way to go from here. But it turns out that actually. Here. But it turns out that actually this still converges to Kpz. Okay, so let me tell you what is the theorem. I'll go back to that future. So basically, if you go along a straight line and rescale, then, of course, there's a scale invariance, you get nowhere, no convergence, you just say the state actually possible. Say the state of the process. But if you go along the slanted line, things will be different. Then you actually have a limit. It turns out along the limit, you can show that there's a KPZ limit. In fact, you get the directed landscape in two parameters. We do believe that it actually converges in all four parameters, but we don't know how to prove it. That's completely open. So this is another metric that we now kind of. Yeah, so this is another metric that we now kind of understand, and it also fits into this, in this, in this case, the universality class. Let me tell you what this previous picture is, because it's kind of nice. So here what I did is I looked at some kind of interval here. And I wanted to understand what is the distances of the points in this direction from that interval. So first I wanted to understand what is the what are the what are the points that are of distance zero. The points that are of distance zero. Okay, and so to get the fields that are of distance zero, they're basically all the points whose Brownian motion, or coalescing Brownian motion, will end up in this interval. The way to describe the boundary of that is that you start a Branian motion here in some kind of dual lab, another one there, and you wait until they'll meet. So the ball of this. So, the ball of distance zero looks like this. Now, if you want to understand a ball of distance one, it turns out you do something similar, except that you start from the same place and you sort of reflect from this. And then reflect from this, and you wait until this makes it. So, you have this kind of interesting walls. They have it seems like they also converge to some nice limit shape. So, they in this world there is an interesting random limit shape. An interesting random limit shape, which should have some nice description. Okay, so this is all I wanted to say about this model. There are all kinds of interesting features, but I'm not sure I have time to talk about them. So maybe I'll go to my last topic, which is a work by my student, Julian Ransford. And of course, he can say much more about it than He can say much more about it than I can here, but I want to say the basic result, which is just a universality result of directed polymer. So I think it is the first universality result in this world, which doesn't go through the K-Pz equation. So this is actually going directly to trace the rhythm. And what is the result? Well, here are directed polymers, right? So if you Directed polymers, right? So if you fix for each vertex, you have some value xi, then the polymer partition function is just some overall path from 0, 0 to nn of the weight of the path, which is just product of e to the beta times the xi. Beta is a parameter that you can adjust. And you expect this to be, you expect this to converge to tracy rhythm. Converge to trace rhythm in many, many regimes. So n will, of course, always go to infinity, and beta can go to infinity in various ways as n goes to infinity. Or beta can change. So beta can go to infinity also, but beta can also go to zero in various ways to n goes to infinity. In all those regimes, for many of these regimes, we expect a trace of rhythm limit. So here is So here is a here is a here is the result of Julian. So if you look at the partition function and you let beta n to the n to the minus alpha, so it's going to go to zero with n, where alpha is in this regime of one-fourth, one-fifth and one-fourth. If alpha is one-fourth, then you have the result of Walberg's castle and Knin, where you see that Where you see that these things actually converge to the Kapeans equation. But now we're in a different scaling regime, so you should go to the traces of the non-GU. And yeah, so how do you so with this so what are the variables, right? The variables are just IID. Fix them in advance. If you fix the distribution in advance, the variance sigma squared, you need a small exponential moment. Exponential moment, but that's it. So you do the polymers with respect to these variables and this sequence of betas, and as n goes to infinity, you converge to the trace of the MGV. As you can see, the scaling here depends only on beta. The shift here has a more complicated dependence. It depends actually in. It depends actually in this log of the moment generating function, but beta is going to zero, so it only depends on a few moments. But I think it depends up to moments, maybe even the first, I think first four moments, if I remember right. So one interesting thing about this is, if you look at it, is how the, it's my favorite thing, how the standard deviation of the random variables relates to the standard deviation of the limit. So I wrote it so that you can see as you that if the standard deviation of the variable is a sigma, the standard deviation of the vote is sigma to the 4/3. So I tried to get some heuristic interpretation of that. I guess you can, of course you can get this, it's proven, but it's interesting to come up with some heuristic interpretation of why it happens like that. So this result is based on a result of Krishnan and Kasal, who prove similar results for the Logamba polymer. And they also introduce this Lindeberg method into KPZ, but for them to But for them to be able to use it, you have to sort of change the moments of the variables as n goes to infinity. So it's kind of the first time you can just fix the variables and make an actual universality. And then, of course, they proved this for Log Gamba, but their results were based on previous results of conversions. Log Gamba in a more bounded regime of or a banded regime of these variables. Okay, so it's an interesting effect. So you can ask, you know, how well can you do this moment matching? And there is a formula. So if you want to match k moments to these logammas or whatever integrable model you have, the log gammas are the only integrable model you have, unfortunately, so you don't have a lot of choice with your matching. Have a lot of choice with your matching. You can ask how big alpha has to be, and this is the equation. That's actually seems to be sharp. 2 over 3k plus 11. So this method should work for alpha that is at least 2 over 17. There is a slightly different, but not as important, obstacle at 1 fifth. So that's why this is stated for 1 fifth and before. Okay, so. Okay, so okay, so I think I'm done, right? I have half an hour. So, okay, so I just want to say, you know, when you do these things, you always stand on the shoulder of giants. But this particular giant here seems to need a lot of standing on his shoulder. So every single one of these projects, I think one, two, and three, is based on some result of Timo. And and uh yes, I'm I'm I'm very happy to to be here at his birthday. Thanks. Thank you very much. So is there any question or comment? So far. Yeah, so the the for the the Berlinian way of this convergence to the directed landscape, so is that like a nice scale mode or how do you Is there like an RSKing hole? How do you. Sorry? Is there like an RSKing hole? How do you... Oh! No, so what it is. Okay, so what it is based on is these reflective Brownian motions. Okay, so if I describe the ball, it involves reflective Brownian motion. That's reflected Brownian motions, right, they're essentially last message percolation. So we use the last passage percolation results to prove this. Results to prove this. But you said that sometimes this is infinite. How would that happen here? Yes, so these things describe a distance from an interval. As you send the interval to zero, and most places there'll be just no ball at all. So what you have to do, and this is the way I stated the theorem, is that you look at distance. Theorem is that you look at distances to a half line. So this is R minus. And this half line, so you really want to point to points, distances to converge. But in this scaling, this half line essentially acts as a narrow edge. Because even though it's a half line, it's kind of very slanted in this scaling. So that's the way that it will pick up kind of the best. You know, it will pick up kind of the best point to converge to. The best point for the distance. So, indeed, let me see, how should I. So, there's a general question, like what is the notion of convergence for these distances that have lots of holes, which are infinitely far away? And there is a nice notion, which is epigraph convergence of lower semi-continuous functions. And so, in that notion, And so, in that notion, you just have convergence, so that's okay. Even though it wouldn't necessarily hold for typical points. That's okay. If rapicup convergence, that's okay. How did you do the first picture with the icons? Oh, this one? Yeah. Oh, just uh Just uh I think I'm not oh that one I just got from the web yeah oh I'm sorry there doesn't matter that one that was the point process and called Well it was just you know they they just colored it grey according to what the value of the normal is it's so you take a small pieces and put normal up or down and And put normal up or down and yeah, yeah, yeah, yeah. So it's like a discrete version of why noise on the lattice. It's for the fish, yes. What is the obstruction you see at alpha equals one fifth? What's the obstruction? So the proof goes by breaking things into little pieces and then you do the moment matching on And then you do the moment matching on pieces. And when you go to these pieces, you sort of have to understand what the, so this, it's like a piece, by a piece I mean a little time interval. So in that piece, you have some kind of local polymer whose outer boundary conditions are imposed by the rest of the environment. Now, the rest of the environment, this band, you can use this kind of thing as long as. You can use this kind of thing as long as these boundary conditions that the rest of the environment imposes are not too crazy. For example, if it imposes boundary conditions that you have to, your random walk could just go straight in some direction, sort of maximal possible distance, that's bad. And we don't want that to happen. So you can compute where the cutoff for that is when you use the simplest method to eliminate this case. Case. That's what they want to say. That's all I can say. Questions? 