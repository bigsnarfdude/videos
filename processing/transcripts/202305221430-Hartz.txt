And for giving me the opportunity to speak, and thank you for coming. So, I'm going to talk about finite-dimensional representations of operator algebras. And let me begin by explaining what I mean by operator algebra and what I mean by representation of an operator algebra. So for me, an operator algebra is a sub-algebra of the algebra of all bounded linear operators on some Hilbert space. So k is some Hilbert space, B of K is the algebra of all bounded linear operators on that Hilbert space. Linear operators on that Hilbert space. Then I look at sub-algebra, so things that are closed on addition and multiplication. And I want to do analysis, so I also want this to be closed in the operator norm. And for simplicity, I'm going to restrict to unital algebras, by which I mean that the identity operator on the Hilbert space is contained in this algebra. I also want to point out that I'm not assuming that the algebras are self-adjoint, so these don't have to be C-star algebras. They can, but they don't have to be. They can, but they don't have to be. So C star algebra you would get if you have the property that whenever T is in there, T star is also in here, the adjoint, but this does not have to. Now what do I mean by a representation? Well, first and foremost, a representation is a homomorphism for my operator algebra into the algebra of all bound linear operators on some potentially different Hilbert space H. So I'm reserving H for the target here. This space K usually will not play a role. A play role. So again, this should preserve addition, scalar multiplication, and the product. But I also wanted to play well with the norm. So I wanted to be contractive, which means that the norm can only go down. And okay, if I'm completely honest, and if I don't want to lie, then I have to say it should be completely contractive, which is a slightly stronger notion. Now, if you know what this means, great. If you don't know what this means, feel free to ignore it. You know, once you get into the weeds of the theory, the distinction is important. Weeds of the theory, the distinction is important, but if you want to understand what I'm talking about, you don't have to worry about the distinction. Completely honest, you're honest alone with the implications. Completely. That's right, yes. How about we like to know what that actually means? Yes, so if you want to know what this means, so this adverb completely will come up a couple times in this talk. What it means that it's not just contractive if you plug in elements in the algebra, but it's contractive when you plug in matrices with elements in the algebra. So it's contractive by every matrix. So it's contracted by every matrix level. We've seen this. Yeah, we've, that's right. So we've seen this philosophy in Ego Club's talk this morning that sometimes you get better properties by looking at the matrix level and not just at the scale. Here's an example. So I'm going to denote the open unit disk in the complex plane by D. And the example I want to look at is called the disk algebra. It comes up in complex analysis. And it consists of all continuous functions on the closed disk that are holomorphic in. That are holomorphic in the open disk. So the norm is the sup norm. And this is an operator algebra. If you like, you can think of the elements as multiplication operators on L2 with respect to the payment. Now, studying representations of this algebra is basically the same thing as doing single operator theory. So there's a result that goes back to work of John von Neumann and a refinement of it to Dunach that says that whenever you have an operator on Hilbert space that has Norman most one, On Hilbert space that has normal most one, then you can find a unit representation of the disk algebra that extends the obvious map on polynomials, the plugging in map or the function calculus to be more precise. And conversely, if you have such a representation, then pi of z of the identity function has to be a contraction. So there's a one-to-one correspondence between unital representations of the disk algebra and contraction operators in the butt space. So that's why I say that studying the representation of I say that studying the representation theory of this thing is basically not. So, as you can imagine, for this reason, this disguise comes up when you try to understand what they go. Also notice it's not self-adjoint, right? The obvious conjugate you would have is taking the complex counting pointwise, but the conjugate of a holomorphic function is not holomorphic. Now, the property I want to talk about is called residual finite dimensionality. And loosely speaking, it means that you can recover your algebra from its finite dimension. That you can recover your algebra from its finite-dimensional representations. And more precisely, what I mean is that whenever you had taken an element, so ignore the matrices for now, if you take an element A in your algebra, then you can compute its norm by looking at all finite dimensional representations, applying them to your element, taking the norm of that, and then you take the soup of the quantities you get. Again, to be precise, you should do things on the matrix level. So this should hold true for all matrices with entries in your algebra, and then this pi n of a, that's the amplitation, so you just apply pi to it. Applications, or you just apply a pi tool to every entry of them. But again, if you're not familiar with this, feel free to ignore the matrices. So equivalently, you can also say that you can embed your algebra into a product of matrix algebra, or to be slightly more technical, you can have a, there should be a family of finite-dimensional Hilbert spaces and a completely isometric homomorphism that embeds your algebra into this product of these bounded linear operators on these finite dimensional spaces. Linear operators on these finite dimensional spaces. So if each H lambda is finite dimensional, this V of H lambda is just the matrices. The matrices are size and dimension of H lambda. And so this embeds you all together to a product of the matrix. So to my knowledge, in the non-self-agent setting, these were first introduced by Mittal and Paulson. They were interested in certain algebras of functions. And so then this notion came up. And then, you know, just like it always goes, people start studying these things. It always goes, people start studying these things as objects in their own right. And so, in particular, they were systematically studied by quite a few Canadian mathematicians: Carfret Croato, who I believe is an online participant of this conference, Ron Marcoux, Cluart and Chris Ramsey, Clarato and Adam Duran, and also Ian Thompson, who is a graduate student at the University of Manitoba. So, examples, we need examples. First of all, okay, the algebra could be a C-star algebra, and in the C-star, C-star algebra. And in the C-star context, this is a well-developed notion. And what I just introduced boils down to this usual C-star notion if you start with the C-star algebra. So this is consistent with the C-star term algebra. Next example, finite dimensional operator algebra RFD. Now this sounds like it should be totally trivial, but it's actually not totally obvious. The reason why it's not totally obvious is it's not true that finite dimensional operations It's not true that finite-dimensional operator algebra is embedded into a matrix algebra. You may need to use infinitely many replications, but it's nonetheless true. It was observed by Clark and Ramsey. Uniform algebras, so sub-algebras of commutative C-star algebras, are RFD. In fact, it's much easier, so one-dimensional representation suffice in that case. So if you are sub-algebra with community C-star algebra, then you consist of functions with a supernova. Functions for the super one. And so the representations you can look at are just pointy valuations. And that's enough. In particular, this disk algebra that I introduced is an RFD algebra. Here's another example. If you look at our upper triangle operators or little L2s, so think of your operators as big matrices, then this is a non-self-an operator algebra and it's RFD. Because what you can do is, so you take your big matrix, and then you have some entries here. And then you have some entries here, and then you have zeros everywhere else. And what you can do is you can truncate to corners. You can truncate to this bit, you can truncate to this bit, you can truncate to this bit, and so on. This gives you representations. And if you take all these finite truncations, that recovers completely your bar as well. Let me also point out at this stage that if you want to construct representations, usually the tricky bit is to ensure that what The tricky bit is to ensure that what you get is multiplicative. So, if you start with any big matrix, you can always do this truncation, but typically that's not going to be a multiplicative map. But it is multiplicative if you take upper triangles, right? So if you take the product of two upper triangulars, then the 1, 1 entry is the product of the 1, 1 entries, and so on. So in particular, this argument doesn't show that D of L2 is RFD, and indeed it's not. In fact, it doesn't have any finite dimensional representations at all. That's good because. That's good because sub-algebra of RFD, algebra RFD, and so if P of H were RFD, then we wouldn't have much of a theory overhead. Here's an example that dramatically fails to be RFD. Here are two examples for experts. The first one is multiplier algebra of reproducing kernel-Libyte spaces, in case you know what this means, RRFD. That was shown by Mittall and Harlson. And this is actually what got me into. And this is actually what got me interested in this subject. So, much of what I work on is about space of holomorphic functions. And so, understanding this RFD property and certain refinements of it tells you something about the complex analytic structure of your space. Second example for the experts, universal upgraded algebra generated by decommuting contractions is an example of an RFD algebra. It's not obvious at all that it's RFD. At all, that it's RFD. But it is, it follows from a fairly deep theorem of Jemagler, and it was also shown using slightly different methods by Metov and Koltz. So yeah, two examples for the experts, but now let me get on with the general theme. So as I said, this is a well-developed notion for C-star out class. And okay, so people prove theorems about it. And the one theorem that motivates what I'm going to talk about is due to. Going to talk about is due to Excel and Bohr. So, suppose you have a C-star algebra. And so, what I'm trying to do here is I'm trying to use fracture letters for the C-star algebra and polygraphic letters for the non-selogen. And I want to extend the notion of finite-dimensional representation a little bit. So, certainly, if H here is finite-dimensional, then this should count as a finite-dimensional representation. But I want to be slightly more general, you'll see in a second why. In a second, why. My representations don't have to be unital. They don't have to send the unit to the identity. But they always send the unit to an orthogonal projection. And if that orthogonal projection is finite rank, then I also want to call it a representation finite dimensional. So in other words, what this means is that you have a unit of finite dimensional representation direct sum zero. I also want to call that a finite dimension. Okay, and then here's this theorem of Excel and Loring. Of Excel and Loring, they show that you can characterize the RFD property for unital C-star algebra as follows. Namely, whenever you have a representation of your C-star algebra on a Hilbert space, you can find a net. Or if you don't like nets, in the separable case, you can get away with sequences. You can always find this net or a sequence of finite dimensional representations that converge point SOT to your given representation. To your given representation. And what I mean by this is that whenever you take a vector in the Hilbert space, you look at the difference of these operators, apply it to the vector, take the norm, that should go to zero. So you see why, so these should all be representations on the same Hilbert space, and that's why I also want to call these guys finite-dimensional representations. Yeah, so the easy part in this theorem is to go from two to one. In fact, if you have any faithful representation, Faithful representation that you can approximate in this sense by finite-dimensional ones, then you're always by SPRFD. That's very easy to see. And the remarkable thing is that you can go the other way. So to me, in some sense, this theorem says that if you have one faithful representation that you can approximate by finite dimensional ones, then you can approximate every representation by finite dimensional ones. And so that's extremely useful because you may have some representation you want to understand. You want to understand. And that may not be the representation that shows you that it's RFD, but this says, well, you can still approximate it by finite number. So this theorem gets used quite frequently in this study of RFD, C-star. Now I want to do non-self-adjoint algebra. And so, because this is a useful theorem in the C-star world, Mafé Rat and Adam Buran asked, Is there a non-self-adjoint quality of this? So, can you prove something? Of this. So, can you prove something that looks like this, but where you don't have to assume that you have a C-star optical, just a non-self-joint of the optical? And okay, so the proof of excellent loin that uses adjoints left and right, so you have to think about this a little bit. So, that's the first thing I wanted mentioning, the advanced self-adjoint version of this Excel one information. So, now let's take any unital alphabet algebra. It doesn't have to be a self-adjoint. Algebra, it doesn't have to be self-adjoint. And I'm going to use the same terminology. I'm going to call a representation finite-dimensional if this space is finite-dimensional. So pi of 1 again has to be on an orthogonal projection, and the range of that should be finite dimensional. In the unital case, that's easy to define. In the non-unital case, you have to think about a little bit what this should mean, but we're only going to talk about unital algebra, so then it's fairly straightforward to define. And so, this is the version of this accelerant theorem for non-self-joined algebra I would like to offer. So, two things are equivalent again for unital operator algebra A, namely one is A is R of D, and the second one is again an approximation property by finite dimensional representations. What 2 says is that for every representation of the all-square-Hilbert space, you can again find a net or finite-dimensional representation. Of finite-dimensional representations that converges point-weak operator topology to a given representation. And okay, what does point-weak operator topology mean? Well, it means what I've written here: you look at the difference of the operators, you apply that to a vector, take the inner product with a potentially different vector in the Hilbert space, and that should tend to zero for all choices of vectors in the Hilbert space. And again, in the separable case, you can replace nets with sequences. It's similar to the accelerating theorem that two of Similar to the accelerating theorem that 2 implies 1 is essentially trivial. If 2 holds for any completely isometric representation, then your algebra has to be RFD. That's fairly easy to check. And so again, what requires proof is that 1 implies 2. Let me also make a comment again for the experts. So in the Axel-Loring theorem, there is another equivalence, namely, the other equivalence says that your CC. Says that your C-star was RFD if and only if the states that have a finite dimensional GNS representation are dense in the state space. And so you could ask if there's a version of this here, and there is. You have to replace states with matrix states. So you should look at all matrix states that have a finite dimensional dilation, and then your algebra is RFD if and only if those guys are. What's a matrix? A matrix state is a unitable completely contractive map with matrix algebra. So at level Matrix out. So at level one, if you have a one by one matrix state, that's just an ordinary state, sort of map into C, but then you go into n by n matrix. And so behind the scenes, which I'm not explaining here, this uses this theory of matrix convex sets, because this matrix state space is a matrix convex set. That's just, again, an aside. Now you may have noticed that it's not quite the same as in the accelerating theorem. I'd have had a strong operator, in the accelerating theorem, we have weak operators. In the accelerometer theorem, we have weak operator. Now, in general, of course, the strong operator conversions applies to weak operator convergence. And you can say, well, what's going on here? What's the difference? First of all, let me mention that in the C-star world, there is no difference, actually. So it's an easy, well, I don't know, it's a functional analysis exercise. Let me put it this way. That if you have representations of a C-star algebra that converge point-wig operator to call. converge point weak operator to quality then they also converge point strong operator to quality. The additional thing you can do in C star algebra is that you can look at things like A star A and then you can if you apply weak operator convergence to A star A that gives you strong alpine. So in the C star world the two notions are the same and so in particular you can recover the original XL1 kilometer from this non-self adjoint version. But of course this is not a game you can play in the non-self-adjoint world. A star A is not something Self-joint algebra. A star A is not something you're allowed to do in a non-sulphog algebra. And this is actually the question that Dr. and Ron raised. So they ask, if you have an RFDR pair of algebra and any representation, can you approximate it point SOT by finite dimensional ones? Or even better, can you approximate it point SOT star by finite dimensional ones? So SOT star means that your operators converge SOT. Operators converge SOT and the air joints converge SOT to the L joint off your limit. I'll explain towards the end why they cared about this SOT and SOT star, but for now you can just say, well, okay, this is an obvious strengthening you might like. Can you do this? And the answer turns out to be no. In the non-self-adjoint world, you cannot strengthen this to SOT convergence even. So T-convergence, even. And so let me explain an algebra where this phenomenon occurs. So again, this involves the disk algebra that I defined at the beginning. And now I don't want to think of it as a sub-algebra of continuous functions on the closed disk, like it's defined, but I want to think of it as a sub-algebra of continuous functions on the unit circle. And you can do this by the Maximum Modis transport. If you know a holomorphic function on the boundary of the circle, then you know whatever you want. And so the algebra I want to look at, well, first of all, it's a sub-algebra of 2x2 matrices over continuous functions on the circle. So that's the universe. And it consists of 2x2, let's say, lower triangular matrices. Make it upper triangular if you like, it doesn't matter. And the structure is that the 1, 1 entry, F, should be analytic. So in the disk algebra, the 2, 2 entry should be co-analytic. entry should be coordinating, so the adjoint should be the disk algebra, and the two one entry can be anything you like, any continuous function. Okay, so then two things. The easy thing is this is a unital operator algebra that is RFD. Well operator algebra you just verify, you have to check that the product of two such things is again of this form. That's an easy exercise. RFD is also easy because it's a sub-altar of two by two matrices over continuous functions. And so two-dimensional representations are enough to recover. Representations are enough to cover all of them. The representations you need to look at are point evaluations at points in the circle. That gives you a 2x2 matrix, so that gives you a 2-dimensional representation. Look at all of them, and you will recover your algebra. But I claim that there is a representation of this algebra that's not the point SOT limit of finite dimensionality. Now, by this non-self-joint X-Alorian theorem, it will be a point-weak operated apology limit, but not strong-operated abroad. But not some operative alternative. I claim that this representation that you can, so this in V is actually not too contrived, so I want to show you what this representation looks like. But I need a little bit more terminology. So first of all, if I have a function in L2 of the circle, I'm going to denote the Fourier coefficients by f hat of n. So just this integral. I think you can tell that I've been hanging out with harmonic analysts because. That I've been hanging out with harmonic analysts because of the Fourier coefficients as integrals and not as innovators, but it doesn't matter. And then the hardest space is the space of L2 functions whose negative Fourier coefficients vanish. You can also think of it as a space of functions on the disk, but on the interior, but here I just want to think of it as a subspace of L2 on the circle. And on this space, we have what are called turbulence operators. So the way it works is you take a continuous function h on the circle. On the circle. You start with the Hardy space function f, you multiply it with h. Typically, that's not going to be in the Hardy space again, it's just an L2 function, but it might have negative Fourier coefficients. But you can get rid of the negative Fourier coefficients by just putting the orthogonal projection onto H2 in front. That's a bounded linear operator. And I think it's fair to say that these are among the best studied operators in operator, certriplets operators in model space. And so here's the representation that you cannot approximate S of T. Cannot approximate S of T. I like to call it the triplets representation, because what it does is it takes one of these 2 by 2 matrices in my algebra and it spits out an operator on two copies of the Hardy space, H2, the rectsum H2. And what you need to do is every time you see a function, you just write a triplets operator with that symbol. So it sends this matrix F H G bar to the triplets operator with F, triplets operator H, triplets operator G bar. And so I claim that this is a representation. Claim that this is a representation. That's again not too difficult to verify, but you cannot approximate a point SOT by finite dimensional representations. Now, if you want to check that this is a representation, you have to check that it's multiplicative again. And this is a fundamental property of turplets operators. Because in general, the map that sends a symbol H to its turplets operator is not multiplicative, but it's sometimes multiplicative. Maybe if the thing on the right is analytic, then this is. Analytic, then this is this. So if f is in a disc algebra, and similarly, if the thing on the left is co-analytic, so if you have T G bar, T H is T G bar H again, if G is in the disk algebra. And so these two things are exactly what you need to verify that this map is multiplicative. If you write down the matrix product, those are exactly the things you need. And this is all the multiplicativity you get for turbulence operators. So in general, it's not multiplicative. So in general, it's not multiplicative, and so in particular, this shows that this doesn't extend to the representation of the whole algebra of all continuous functions of all 2 by 2 matrices with entries in the continuous functions. Okay. Yeah, so that's the representation. I'm not going to explain why it's not the point SOT limit. That requires a fair amount of work. But yeah, if you want to know more about it, you can ask me later. Now, let me finish by explaining why. Finish by explaining why they cared about why Glaude and Duron cared about approximating with finite dimensional representations. And well, it goes back to the fact that this RFD property is well studied in the context of C-star algebra. And so one way to understand non-self-done algebras is to associate some kind of canonical C-star algebra to it and then study the C-star algebra. And there are multiple ways to do it. The one I want to focus on here is the Want to focus on here is the maximal single star algebra. And so I'm attributing this due to David Becher. I'm not entirely sure if he was the first one to observe this, but certainly he studied this maximal algebra in great detail. And so this maximal C-star algebra is a C-star algebra. It contains your non-self-adjoint algebra. It's generated as a C-star algebra by the non-self-adjoint algebra. And it has the universal property that every representation of the non-self-adjoint algebra extends to whether. Of the non-self-dead altwa extends to representation of the maximal C-star altogether. So, this is some kind of canonical object that's associated to be a non-self-drowned altar. And the question that Clarato and Doran ask is, if you start with an RFD, R-bit or altruistic SC submaps, RFD. So the converse is certainly true. We got sub-altras of RFD, altruist of RFD, and so they wanted to know if this is true. And that's actually, to me, this is a totally reasonable guess because the C-STAMAX will have lots of time. Because the C star max will have lots of finite dimensional representations by the universal property. A has lots of finite dimensional representations because it's R to D. And so by the universal property, you will get lots of finite dimensional representations. It's just not clear if you get enough to norm the whole C star algebra. And they actually showed that in many cases it is actually RFD. So they have some additional assumptions and then they can show that C-stimax is RFD. But they also have this, which says that C-stime X is RFD. Says that C star maxes are F D if and only if, well, here it comes, every representation can be approximated point SOT star by finite dimension ones. And so the necessity of this just follows from the universal property and the XR1. And so then maybe you can see where this is going, right? So then this algebra B that I constructed will be a RFD algebra for which C star max is not RFD, right? Because there's this one representation, the triplets representation, that you cannot approximate complexity star. So that's why they were interested in this product. So that's it. Here's a quick summary. So I explained that operator algebras are called residually finite-dimensional if you can recover them from their finite dimensional representations. This is the same as saying you can approximate every representation, weak operator topology, by finite dimensional ones, and you cannot upgrade a weak operator to strong operator compact structure. Thanks. Thank you for a very interesting talk. Are there any questions? Let me start with the question. Have you thought about the groups establishments? So what kind of a group will give you this RFD problem? I have not personally thought about this a lot, but I know that this is, you know, like C-stack people and people in group theory have studied this. So I think, so if the group C-Stars, So I think so if the if the group C star algebra is R F D, then your it's a full group C star algebra, then your group has to be maximally almost periodic, like the unitary representation finite dimensional unitary representations have to be enough. And I think for amenable groups, that's also sufficient. But I think in general it's not if I'm memocraphically I think SL3Z is an example of a maximally almost periodic group for which the groups used to otherwise not RFT. I would imagine that there is paper by backpack and lower as far as Backer and lower, as far as I remember, uh which uh study these questions that you're asking. Questions? Well, I'll ask an embarrassingly simple question. What's an example of a finite-dimensional operator algebra that's not representable? Ah, yeah, so uh the problem is being an operator algebra is not. Being an operator algebra is not super restrictive. So, what you can do is you can take any operator space, so you just take some subspace of B of H, and then you put it in the upper right corner. So you say you put lambda, mu 0, and then you put the elements of your operator space here. So lambda, mu are complex numbers. And I say then it. And x is just an m. And then this is an operator algebra. And then this is an operator algebra because when you multiply two of these guys, you never have to add these things, right? Yeah, and then you can construct examples from this. So being an operator algebra is not super restrictive because you have to construct something like this. Thank you. More questions? I mean, why is it a finite dimensional? No, so yes, so you have to take M to be, so I'm not saying this is always finite dimensional. You have to take M to be finite dimensional. Take M to be finite dimensional. And then, if you do it right, then you can rig it so that I think there's a two or three dimensional example so that this thing, I mean, it is RFD, but you will need it for the many work associations. Do we know anything about like this theory of any of your examples of C-star's birth? So, I'm not a C-star person, right? So, yeah, if you want to know about K-theory, there are probably better people to ask us, to be honest. In this non-self-adjoint world, I'm actually not sure people have really studied K-theory. I mean, a lot of, I mean, so sort of, you know, very heuristically, this non-self-adjoint theory feels a lot more like doing complex analysis as opposed to topology. And so, yeah, people don't, to my knowledge, haven't really studied K-theory in this non-self-adjoint theory. One more point. Sorry, I'm still fascinated by this notion of a matrix state, because state implies some kind of positivity, but you have no star operation, right? So why is a matrix state like a state? Yeah, so if you have, so A is my unital operator algebra, right? And then I have, so this should be unital, completely contractive, right? So this is I call matrix state. Your right state should have some positivity, but what you can do is you can look at the operator system. But what you can do is you can look at the arbiter system this generates. You look at A plus A star. This is not the dual space, it's a set of all adjoints. And then this thing always extends uniquely to a completely passive map. So this goes to MN of C and what you do is A plus B star maps to phi of A plus phi of B star. And this will be completely positive. And so yeah, there's positivity in the negative state even tiny. It's a little hiding. Even though you can't see it initially in that. Oh, look at that. Thank you very much, Jesus. Thank you very much.