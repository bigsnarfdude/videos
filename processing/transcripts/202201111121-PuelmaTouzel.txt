So, today I'm going to talk about transient computation. And just so we're sort of all on the same page, let me just go over why we care about dynamics when it comes to neural computation. So we care about dynamics because the dynamics can strongly constrain the functions and computations that any learning system can perform, right? Right, so and when it comes to dynamics, an important property is stability. So, let's say we have some complicated high-dimensional neural circuit system, which I'll represent, whose activity I'll represent as a trajectory in activity space given by this line. And we can think of inputs as perturbations, so inputs to this learning system as perturbations to this trajectory. And so you could. And so you could imagine a perturbation that can either persist or be erased because of the recurrent dynamics. So, for instance, this blue one could decay in time, and that might be a useful type of dynamics to learn if you want to be resistant to noise, if that perturbation is just sort of a noise perturbation or maybe is about some task irrelevant feature. On the other hand, there might be another such perturbation. There might be another such perturbation that is amplified by the dynamics, and that type of amplifying dynamics might be useful if you need to maintain information about that perturbation going forward in time, which would be useful for memory computations or the like. So, stability is really an essential aspect to understanding how dynamics affects computations. Computations. And back in my PhD in GÃ¶ttingen, I and other students in the group of Fred Wolf began building up a suite of results on spiking balance networks, which is a, you actually saw a sort of spatial version of that yesterday with Cheng Cheng's talk. And these are simple networks that replicate some of the activity statistics of cortical networks. And each of the results that we came up with explained how a particular system parameter affected stability and consequently information processing. So we looked at how the unit dynamics in the form of spike activating currents or sub-threshold currents would affect the stability. We looked at how synaptic delays. How synaptic delays or synaptic currents would affect it. We looked at how connectivity motifs and structure in the connectivity matrix of the recurrent dynamics would affect its stability. And importantly, for questions of sort of function, we looked at how external drive, so the statistics impinging on the network from outside. So the non-local input. From outside. So the non-local inputs affect stability. And our tool for assessing stability here was the Lyapunov spectrum, which is a set of exponents that characterize the system's sort of long-term expansion or contraction to small perturbations. And looking at small perturbations means that. means that you can use a linear approximation of the dynamics. So this is typically called linear stability. And again, this is a property of sort of long-time autonomous dynamics. And so there's two obvious limitations to this, which is one that these are responses to small perturbations. And I'll get to Perturbations and I'll get to something that looks at finite perturbations later in the talk. But what I want to focus on throughout the talk is the idea that there are sort of short-time stability properties that aren't captured by these kind of asymptotic stability metrics, but can be useful and sometimes important for the phenomena that you might be interested in studying. So, what do I mean by sort of short-time stability? Well, I mean, transient behavior. So, if we have some dynamics given by on a state variable x given by some function f, if we think of stationary solutions, attractors, something like a fixed point, we know locally there's going to be some contraction onto that stable fixed point that's going to be determined by the Determined by the smallest eigenvalue, right? And so if we have sort of a simple two-dimensional system, the transient is just the time it takes to get to the attractor. And locally, you know, you can derive some of these the duration of the transient, but of course, there's Transient, but of course, there's a lot of, let's say, issues with just thinking about local attraction to attractors in particular. There can be non-stationarities in the system. So instead of just being some function, some autonomous function, there's some non-autonomous. So some, for instance, like a slow drift that moves the attractor structure around. Structure around. There can also be issues when you're actually not local to the attractors, but far away, and then there can be all kinds of complex features of these transients. And like I was saying on the last slide, the stability properties locally to the attractors don't have to be the stability properties of Be the stability properties of the transients. And so that's something I'm going to focus on in this talk: looking at finite time stability properties. So transients have been studied in various contexts in computational neuroscience for decades now. There's sort of an interesting application of what are called heteroclinic cycles, which are the cycles that emerge when you have a bunch of. That emerge when you have a bunch of saddle nodes distributed throughout the space. So the system is sort of constantly chasing, being pulled towards a fixed point, but then being pushed away. And so there's no sort of exact analytical sort of attractor object that it rests in. Another topic in transient computation. In transient computation, that's been studied a fair bit in computational neurosciences: balanced amplification, which is this idea that, so again, if you have this two-dimensional system, if your eigenvectors around a stable fixed point are oblique, so they're very far separated from each other, the distance to that attractor can actually. To that attractor can actually grow initially before it eventually attracts. And that's this idea of transient amplification, which depends on how the eigenvectors are oriented. And that's been, you get this type of property from just imposing something like Dale's law on your connectivity matrix. And people have studied this in the context of motor of pattern generation and Pattern generation in motor systems, and also for the idea of maintaining memory in a stable system for some finite amount of time. Okay, so today I want to talk about two particular examples that I've come across in my own work on transient behavior. So, the first is a problem more in AI and sequence modeling with recurrent neural nets, where I'm going to show you that transients are actually important to balance the expressiveness of the RNN. And then, in the second top, I'll go back to some more sort of traditional dynamics in computational neuroscience and show how trans And show how transients are involved in destabilizing a type of spiking network in the balanced state. And then I'll try to close with some thoughts about how the sort of two traditions of dynamics and computation and the sort of newer black box approach. You know, black box approach, for lack of a better word, can sort of come together. And I think there's a lot of fruitful work to be done there. Okay, so I'm going to use the more AI notation here, even though this is more of a comp neural crowd. So we have our simple RNN. It's recurrent, obviously. We have some input projection u into a Into a recurrent system with some hidden state H, and there's some output projection W that in a supervised learning setting we would like to reflect some desired output. And there's this recurrent matrix V. So this is probably all familiar to everyone. And these models, at least. And these models, at least in the sort of AI world, are typically learned with some variation of back propagation. And this is essentially just following gradients of whatever the lost function is that you've chosen with respect to the weights, which can be the recurrent weights or also the sort of input and output weights. And this is a sense. And output weights, and this is essentially just the chain ruling through the dynamics that produces the output going back into the weights of the network. And so to compute this gradient, we're going to need the Jacobian. And so that's just how vectors are propagated across an iteration of the recurrent dynamics. The recurrent dynamics, and that's going to depend on both the state of the network, but also the connectivity. So it's a combination of those two things. And so the loss gradient is this sort of large complex term. And because of the beauty of automatic differentiation, in principle, we can just press a button and we don't actually have to compute things. Have to compute things analytically, but it's pretty useful, I think, to actually look at what's going on under the hood and see what actual terms are in this loss gradient. So one of these terms is the gradient of the loss with respect to the hidden unit vector. And so this actually So this actually, so here J is the Jacobian, W is the output. So there's this term on the right here, which has to do with the output at some future time S, how that's affected by some change in the hidden state at an earlier time t. And what we see here in the middle of And what we see here in the middle is this product of Jacobians, which is going from the time t to the time s. And this is where stability comes into the picture. So for instance, let's think about asymptotic stability. So let's say S is some far, far future state that's effectively infinite. And let's say we have some sort of stationary input statistics. What we can do is see how a basis set of vectors is transformed by the Jacobian across a step. And we can apply something called the QR decomposition, which basically just orthonormalizes what the transformed basis set does. So it gives us some expansion coefficients that are on the diagonal of this R matrix. R matrix. And if we use this representation to express the Jacobian, so we move this Q on the right here over to the other side of the equation, we get this form here where we get a product of these R's and on the diagonal of this matrix product are going to be. Product are going to be exponentials of a set of exponents, which are the Lyapunov exponents. So if these are positive, then this dynamics is chaotic and perturbations will grow, whereas if these are all non-positive, then you get then the dynamics is stable and you get contraction of. Contraction of small perturbations. So that's the sort of asymptotic case. But remember, I was saying that transient stability can also be important often when you apply recurrent neural systems on sequences. These sequences have some finite length, and so it's not clear that the asymptotic stability properties are really the relevant one. Properties are really the relevant ones to study. In fact, transient properties might also play an important role. And so that's what, I guess I should say here, just to connect this to the sort of larger field of deep learning in general sense, we've had some talks on deep networks for Deep networks for vision. In that case, it's the same types of expressions. It's just instead of the recurrent, the sort of layer by layer weights here would come out in the Jacobians from one layer to the next. So it's basically just you just unroll the R and N, and you get the same type of behavior. So this type of stability. Of behavior. So, this type of stability property is not only for local recurrent stuff, but also sort of sequence processing and principle. And there's been a lot of interesting work, like for instance, by Jeff Pennington and Surya Gang Guli on studying the stability properties of deep neural networks. Okay, so given, so let me now talk about how. So, let me now talk about how we understand transient stability properties. For instance, as a result of the structure in the recurrent weight matrix. So we all know that there are matrices for which we can come up with an orthogonal basis of eigenvectors and diagonalize them. And diagonalize them. So here, P is that matrix of eigenvectors. And theta is, for instance, a matrix, a diagonal matrix with the eigenvalues on the diagonal. But that's only for a subclass of matrices, not for all matrices. However, there's a generalization, a kind of generalization that does apply to all, which is another type of Does apply to all, which is another type of decomposition of a matrix, which still allows for an orthogonal basis set, but now includes a triangular part to this middle matrix here, which is about, which now couples the modes to each other in a hidden feed forward structure. Hidden feed-forward structure. So different diagonals correspond to interactions between some iteration of nodes. And as we sort of tune up the magnitude of the size of these lower diagonal entries, so the strength of the interactions, we can get strong. Interactions, we can get stronger amplification properties. So, here on the right, here, I'm showing a very simple example of a simple RNN where we initialize the state at a certain time, and there's some stable dynamics in the long time sense, but in the transient sense, there's some transient to get there. And the amplitude. And the amplitudes, as measured by in this top plot, the variance of the hidden unit activities, or for instance, the norm of the vector. Both of these can grow first and then decay. And the amount by which they grow first can be controlled by the strength of this part of this matrix. Matrix. Okay, so one of the questions we were interested in thinking about, in part inspired by the work on this kind of top topic in computational neuroscience, was how can we use this to make for better RNNs? And we thought, well, what if we give the learning direct access to these parameters? So we allow. So we allow it to learn how to orient the basis set and we allow direct parametrization of the strength of these interactions between the modes rather than the direct synaptic connections between neurons. And it turns out that that can help control the Help control the expressivity of the dynamics because it controls how much inputs are how long they're maintained in the dynamics as well as their sort of strength. And so we have this NERAPS paper. So this is led by Guillaume. This was in the group of Guillaume Lejois. This was a NERPS paper from This was a NURBS paper from a couple years ago now. And doing this, this, so we call this the non-normal RNN because this dynamics, I should say, matrices that have this kind of lower triangular structure are called non-normal matrix, which is sort of similar to the Similar to the non-elephants. So basically, all many arbitrary matrices are going to have this lower triangular structure. Again, being diagonalizable is not a general case, right? So anyway, so we developed this non-normal RNN where we give back. RNN, where we give back prop access to these to the interactions between these nodes and it learns a particular recurrent structure. So, here for a copy task where the network was just tasked to reproduce a particular input, after some fixed delay, we find that the network learns the optimal solution. Optimal solution, which is just to rotate it around without changing its size. So here's no lower diagonal part. But when, for instance, we give it a character prediction task, so this is the PTB task, we find that indeed it does use interactions between different Different, what are called sure modes, or these eigen-like basis vectors, which means that it's combining information about different time points in the past. And so we got the, because we have the bold text, that means we're better. So this approach was able to outperform some other approaches. approaches on at least for this task. And the intuition here, at least for the character prediction task, is pretty simple. So if we have a sequence that the RNN is trying to predict the next character of, each letter is progressively inputted into Inputted into the network. And because of this feed forward structure, letters inputted at previous times have been sort of shifted down towards through the different modes. So for instance, this A, which was inputted four time steps ago, has been shifted down into this last mode here. And so by controlling the output weight from this. From this mode or the projection of this mode into the output, we can allow for correlations among inputs to inform what the prediction of the network is. And that's exactly what the RNN needs to do. It needs to learn about correlations among letters to be able to predict what's going to come next, right? So that's sort of an intuitive. So that's sort of an intuitive application of why this sort of transient behavior can be useful. Okay, so that was the first part. The next part is about balanced networks and stability properties. So I think most people here are familiar, but let me just quickly go over it. So if we look at cortex, we see asynchronous and irregular activity. Regular activity. This looks noisy. Is it noise? There's various experiments that show that single neurons are reliable transmission devices, although of course synaptic transmission is quite stochastic. But it was surprising initially that you could get this type of activity from a deterministic mechanism, which was called the balanced state. And the key here is when the number of inputs coming in. When the number of inputs coming into a neuron and their coupling strength scale in a particular way. So if there's k inputs coming in, that the synaptic coupling scale is one over the square root of those. And there's been a lot of work sort of working this out. I'm going to talk about some more analytical work, but there's some, you know, the latest work has shown that there Work has shown that there are sort of looser notions of this balance that are useful and explaining a lot of response properties and statistics of cortex. I'm not going to get into the balanced state theory. I just want to talk about their dynamics. So I'm going to look at a very simple model. It's the leaky integrated fire with recurrent inhibition. So there's some external drive. Drive. If these neurons are isolated, they just fire periodically. But now, if we start to wire them up with this corresponding scaling here, we can get this asynchronous and irregular activity. This isn't a great model of the balance state because it doesn't involve some of the correlation properties that are really interesting to analyze when comparing it to real neural circuits, but it does have the mechanism. Does have the mechanism by which you do get this asynchronous and irregular activity. I wanted to draw, to sort of motivate this from a perspective, from a sort of transient perspective, which some of you might not be so aware of. But now, if we start to connect, so let me just work through it here. So, if we now start to connect these neurons up and we have an all-to-all connectivity, you get periodics. You get periodic solutions. If you now start to introduce disorder into the connectivity by, for instance, deleting all but K connections for each neuron, and you can do that randomly, you still get periodic solutions, but the time it takes for the system to arrive at them, so the transient time becomes exponentially long in the in In the network size. And in the meantime, during the transient, there's this asynchronous and irregular behavior. And now, if you introduce even more disorders, so instead of deleting the same amount of connections for each neuron, you delete a random number of them so that you get what's called an Erdos-Reny graph, where there's just some average, some mean input, but there's some variation around it. Then you get Then you get complete asynchronous and irregular solution. So, this actually suggests that the trajectories of that the spiking trajectories in the system are actually quasi-stationary transients, which is something that might be surprising. Whether this is actually at play in more complex models, I think is unclear, but I think it's an interesting sort of perspective to take. To take. So, what's the stability property of these networks? Well, in some work with Fred Wolf, I did some simulations around trajectories where I made perturbations and then looked at what the spiking activity was. And so on the bottom, there was one trajectory. So on the bottom, there was one trajectory. I also carved out the, I also measured the sort of the neighborhood around a trajectory, and that's what I'm showing up here now. So the trajectory is going through the center, and this gray thing is the region of the phase space in this sort of two-dimensional projection that still attracts to that trajectory. trajectory. Oops. So that's that's the this is effectively the basin of attraction. This was a phenomena first seen some years before and characterized numerically. And I did some analytical simulations on its structure, which turns out to result from what's called the decorrelation event. So the boundary of this, so the cross-section The cross-section of the attractor basin around these stable trajectories of asynchronous and irregular activity are actually determined by some future event. So as you perturb away, you shift the future spike times. And if that perturbation is strong enough such that two spikes that come from connected neurons cross, you induce this decorrelation event from which the from which the the the which which leads to a divergence of trajectories and then that's now the the boundary of the attractor basins so finally getting to to transience let me talk about um the degree to which this interesting phase space structure of flux tubes is is is uh how brittle it is so this Brittle, it is. So, this was the original leaky integrating fire problem. If we add a synaptic current, so just an exponential kernel, but now that it's a full dynamic degree of freedom in the system with some time scale tau i, we can actually take the system from this weird stable trajectory case to conventional chaos where small Chaos, where small perturbations lead to divergence. And we already knew that such a transition existed, which is why we decided to study this, but it was unclear what the mechanism of this transition to conventional chaos was. And so I did a bunch of simulations that showed how the critical synaptic time constant scaled and also Scaled and also how the flux tubes shrink as they approach this transition. And something I never finished, but I started was to look at what the cause of the transition were. So here what I'm showing is the distance between the perturbed and unperturbed trajectory as at the beginning at the time of one of these decorrelation events. And what's shown in black is the full distance, but I've separated. Is the full distance, but I've separated that up into distance components that come from particular spike time events and ones that come from the dynamics between spiking events, which is the isolated single neuron dynamics given by the synaptic currents. And so that's the blue component, whereas the spiking, just the spiking component is in red. So the blue and the red. Is in red. So the blue and the red combine to make the black. And then when there's no synaptic current, we see that the isolated single neuron distance component always decays. But if we start to increase the synaptic time constant now, there's now a transient growth in the distance component corresponding to the single neurons. Corresponding to the single neurons. So, this suggests that there's some single neuron property involved. And if we write down the two-dimensional dynamics of this voltage plus synaptic current, we can solve it. It's linear. We have some propagator S. And lo and behold, we get transient. As we tune up the synaptic time constant from blue to red here, we get the same type of. Red here, we get the same type of oblique eigenvalues, sorry, eigenvectors, which increase the non-normality. One measure of which is called the spectral absissa. So if we take this two by two voltage and current linear transformation matrix A, and we look at this quantity, which is essentially the slope at At the initial condition of the norm of the matrix, we see that it increases as we increase the synaptic time constant. And since this non-normal matrix A also appears in the propagator, which appears in the Jacobian of the event back, of the event map, now going back to the expressions of, for instance, the loss that have all of these Jacobians in it that I showed you in the previous part of the talk. You in the previous part of the talk, this is a likely candidate for how the dynamics transitions out of this weird flux tube phase space and into more conventional chaotic dynamics. Okay, so I just have a few minutes left. I wanted to just touch, I wanted to actually change gears completely and just talk a little bit more generally about the workshop and this field. I feel like. Field, I feel like we've seen sort of a sort of class traditional sort of computational neuroscience approach where there's an emphasis on understanding, but potentially the phenomena that we're studying are narrower or potentially not the ones actually used in the brain. Whereas there's this other engineering approach that has the benefit of being quite predictive about brain phenomena, but potentially But potentially is using the wrong mechanisms. And I think this diversity is actually okay. I think it's something we should encourage, but I think it requires a bit of better integration. So while computational neuroscience has a tradition of bio-detailed modeling, Detailed modeling alongside analytical results, whereas artificial intelligence has also numerical phenomenology, but also has contributed this sort of powerful language and rich concepts for learning phenomena. I think as we combine them in a context of understanding complex task learning with RNN, I think we need to try to maintain that tradition in computational neuroscience of. Computational neuroscience of deep understanding of dynamics, but focused on particular maybe dynamical phenomena that arise in these more sort of engineering style approaches. So I'll end with that. I want to thank the organizers. This has been really fun so far. And this obviously took a bit longer. There's not much time for questions, but happy to take them. Going through not Max. Yeah, we can take a couple of questions. Oh, Sarah's hand's up already. Thank you, Max. Beautiful, beautiful work. Great talk. I wanted to go back to the first part of your talk because now I see, I think I see where Guillaume was coming from in the comments that he made yesterday. Because if I understand you correctly, you're describing a learning. You're describing a learning scenario. You look at the cost function, you compute the gradient of the cost function, and what you find is that the Jacobian that appears in the gradient of that cost function, once you go a little bit through the algebra, is the same as the Jacobian that appears in the equations of motion for the network itself. Forget about learning. Now, the net, you know, for a given configuration of the weights, the same, the Jacobian that controls the dynamics of the recurrent network. Controls the dynamics of the recurrent network is the same as appears in the learning problem. And so I wanted you to elaborate a little bit of that in the context of the question that Jom asked yesterday, which was he was looking for a connection, which I had really surprised me because I couldn't see how these things could be connected between the stability of the dynamics of the train network and the stability in some sense of the properties of the landscape of the aerofunction. Of the landscape of the error function, and you should integrate in the sense for training the network. So, could you elaborate on that a little bit more? I know I take you back to the very beginning. No, no, that's fine. It's an interesting and important point, I think, because I think it's, you know, deep learning approaches involving automatic differentiation are sort of so widespread now. And I think it's not everyone knows this fact. This fact. So, I guess this was what you were talking about, right? This product of Jacobian. So, here J is really the bona fide hidden state recurrent dynamics Jacobian. And it's just because as you wiggle a connectivity parameter at some previous. At some previous time, in this case, I guess it's T, that gives you a little vector that's propagated forward through the recurrent dynamics, through the sequence of Jacobians, and then goes out into an output, which contributes to the loss that you're trying to minimize. But the interesting thing is that the definition of this Jacobian as dht, dh t minus one, if I was not asking about If I was not asking about learning, I would say this Jacobian is just related to the dynamics of the recurrent network itself, right? So, this is really tying the question of the properties of the dynamics of the recurrent network with the properties of learning the weights of the recurrent network. Yes, definitely. I mean, it's this idea that if you have tasks that require sort of information propagation over time, that's going to depend. Right, that's gonna that's gonna depend on whether perturbations decay in time or or not, and has this has a lot to do with the so it it also maybe in the learning context and in a more AI sort of language, it's this exploding and vanishing gradient problem. Exactly, exactly, but it's probing the stability of the network dynamics. Very interesting. Lovely talk, thank you. Thanks. Okay, Guillaume has made a comment. Okay, Guillaume has made a comment. Guillaume, you can unmute and say it if you want. Sure. Yeah, I think Max's answer was great, but maybe just to add to that discussion, Sarah, is that, you know, the link between the signal propagation and gradient propagation is very tight, right? So gradient propagation, the stabilities is linked to these Jacobians. And it turns out that if you restrict And it turns out that if you restrict yourself to manifolds in the parameter space, say of normal matrices, then you ensured very robust gradient propagation. So there is a link between gradient propagation. So if you backpate through time and you control the eigen spectrum to be unity, then you're ensured that you have very minimal explosion and or decay of the gradients. And these are very linked to expressivity. So there is this link there. Expressivity. So there is this link there. And so in this particular case, what we did is we learned on these manifolds and relaxed the orthogonality condition by allowing non-normal parts that help with transient dynamics. But so that's, I just want to add to Max's really good answer there. That's wonderful. So you could learn in such a way that, okay, so you take a learning step that takes you away from the manifold, and then you could project back on. And then you could project back onto the manifold. And you could do this iteration if there is a good reason, if there is an advantage to staying in the manifold of the normal connectivity. So actually in a way, it's very similar. To ensure that we kept this sort of sure form in this non-normal RNN application, we actually did that with the orthogonal. With the orthogonal basis set. So we perturb the matrix itself, but then that's going to give you basis factors, but then that's going to take you away from an orthogonal set. So we then projected back into an orthogonal space to do that. Maybe just one last thing to add. This is, in a sense, very similar to imposing low-rank structure, in a sense, right? Low-rank structure, in a sense, right? Like if you impose low-rank structure, you're restricting yourself to a manifold of low-rank matrices through parameterization, which is the same thing we do here. Although we're not constraining the rank, we're constraining the eigen spectrum, right? But it's not obvious to me, and I guess it's very further discussion, that that would result in dimensionality reduction. I know that low rank would restrict the dynamics to the lower rank substrates, which is actually span. Substrates, which is actually spanned by the connectivity matrix. But it's not clear to me, just because I'm ignorant, I haven't worked on this particular area, that restricting the connectivity matrix, forcing it back to the normal subspace or the normal subset of connectivity matrices would result in dimensionality reduction. But I don't think it necessarily does. So, I mean, you can write down a SURE form for any matrix, and I think. Matrix. And I think what the emphasis of this approach was was to give the learning direct access to these parameters that otherwise it would have to sort of have some complicated correlation in terms of its updates in order to control the amount of interaction between these feed forward weights. But we provide, we sort of offset that complexity by giving the direct access to these interactions. Direct access to these interaction parameters. Alex Pagori is making a good point in the chat. He says the shape of the lost landscape is connected to the Hessian and not to the Jacobian, which is true in the sense of the flat curvature. And generalization will be related to these Hessian, more second-order properties of the local landscape. And that goes back to Guillaume's comment yesterday about the flatness of the landscape. So he's Of the landscape. So he's correct in that observation. Very interesting discussion. Beautiful work, Max. Thank you. Thanks. Okay, well, I think maybe we'll close it up there, but thank you very much, Max. It was a stimulating discussion. So we come back in a little bit under an hour for our next set of talks. So we come back at one o'clock Calgary time. Calgary time. So, yeah, a little bit under an hour. Thanks very much, everybody.