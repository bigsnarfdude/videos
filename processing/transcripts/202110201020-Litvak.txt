The fourth speaker of this section is Professor Alexander Ledvak. The title of his talk is New Bounds on the Minimal Dissupping. Let's welcome. Yes, thank you very much. So I'm going to, well, first I would like to thank organizers for inviting me. And I'm going to speak about minimal dispersion. To speak about minimal dispersion, very nice and easy to explain notion, but some bounds are not very easy to obtain, at least sharp bounds are not known. And it's based on two works, my previous work and our recent work with Galilee Leipschitz. So I will start with probably, well, with definitions. So what we want to consider, let me try to draw some pictures. So if we So if we have two parameters, say epsilon, which will be responsible for volume, and d, which is dimension, and we consider just cube, 0, 1, z-dimensional cube. And we want to ask what is smallest possible n such that if I put n points inside the cube in such a way that In such a way that any axis parallel box which has volume larger than epsilon contains at least one point. So very easy formulation. Of course, you can ask about best configuration. But this is essentially setting. I will give formal definition in a moment. But just to understand, so this is a question. So, this is the question. And such integer smallest n, which gives me such result, we will denote by just n of epsilon d because it's a function of two variables. And equivalently, of course, I can consider an inverse problem. So, I can fix endpoints and dimension also will be fixed, will be d and then. Be fixed will be D. And then an equation that you put some endpoints: what is the largest epsilon such that you can find an axis parallel box, which contains of volume epsilon, which contains no points? Axis parallel, I mean that all facets of the box are parallel to coordinate hyperplanes. So I think this. I think this the setting is simple and let me give more formal definitions and past results. So such function is usually denoted by dispersion star, it's called minimal dispersion. And I will use the following notation. So once again, what are axis parallel boxes? It's formally they consist of course of Cartesian products of intervals. Products of intervals, so something like this, and we denote it by Rd. If I have some finite set P, then dispersion of P, it's maximal volume of box from this set, which has no points, which doesn't intersect this P. And minimal dispersion, which I just explained in the moment, just moment before, is this. Before is dispersion star, so I take infinite over all possible configurations of sets of endpoints. And then, so it's mini max. So I take infinum over all configurations and then supremum over all boxes which contain no points. So this is minimal dispersion, but it's in many proofs it's more convenient to work. More convenient to work with inverse functions. So instead of finding minimal volume, we will find how many points we need to have that minimal dispersion smaller than epsilon. And meaning, as I just explained, that any box of volume larger than epsilon should contain at least one point. So this is setting. And now I will discuss a few results. us a few results. Yeah, our goal of course to understand the symptomic behavior of this function and epsilon z as both parameters go to extreme values. Epsilon is going to zero and dimension goes to infinity. So let me discuss several previous results. So first Rott and Tiche provided the proof which gives this bound so asymptotic This bound, so asymptotically it's C to the D over epsilon. Actually, the wrote and teacher they got even worse bound using paramorials and their bound was improved by larger to what I wrote. And then very recently, just last year, preprint of Buch and Chow appeared where they improved significantly. They improved from this C to the D behavior to prevent. Is C to the D behavior to polynomial behavior, which is already close to sharp. We just need to work a bit on power. And why I say I also will be discussing sharpness. So let me give immediately lower bounds. So what I wrote here, it's very easy to see because if you have endpoints, you can split just in n plus one pieces like this. Pieces like this, your box. And then, if you have just end point endpoints, one piece will be empty, will contain no points. So, we have immediate bound which is trivial, dispersion star larger than one over n plus one, which in terms of function n capital means that essentially n capital larger than one over epsilon. So, if you consider d as a fixed number and as a constant, then behavior. Constant, then behavior is given here that n capital behaves like one over epsilon, if epsilon goes to zero. But this, if we don't care about dependence on D. And another result of Dimitri's quench, they slightly improve constant here. So they put five over four, which probably is not very important if you don't care about constants. But there is But there are a lot of papers where constants are improved, especially in two-dimensional case. But my talk will be devoted to large D, to behavior when D is going to infinity. And what is also very important in Dimitriez-Pojank paper, they also proved, probably they didn't formulate, but from their result it follows. But from the result, it follows that if I multiply epsilon by n epsilon z, then limit exists when epsilon going to d to zero. And then, of course, it's a very natural question to find the limit as a function of d and for example from book chow result it follows that cd should be larger than smaller than d square. They also prove that it I will say but that this cd is larger than d so it's between d and d square. So it's between D and D square essentially. And first result, first low bound, which is which is going to infinity as dimension grows, was proved by Eich Sleitner, Henriks and Rudolph. They proved that this number should be at least logarithmic in D. So when D grows, it should grow to infinity with Should grow to infinity with at least logarithms, they have very nice proof. Essentially, they reduce to two-dimensional case. They consider some projections on two-dimensional spaces, which is very simple, but very smart proof. So, you have this lower bound, and let me summarize what I told. What I told, I have we have the following bounds. If you want dependence as one over epsilon, then what is known that n is larger than Lagrangian form of d and divided by epsilon and smaller than d squared divided by epsilon. And like I said, if we want just estimate as epsilon going to zero, then we know that. zero then we know that it behaves like one over eps like constant over epsilon constant depends on d and now uh what uh what is bad about this that we have huge gap between uh in behavior of this constant so this is a result but just say two years ago it was c to the d which is uh which is which is even Which is even worse. But even now, it's a difference between log d and d square. So what is if we allow some function which is not as good as one over epsilon? Then from paper in 89, this paper is more general. It's about learning ability. It's about learning ability, and there are many examples. And they prove quite a general theorem from which it follows that n epsilon over z could be estimated like I wrote, that it's z over epsilon, but there is logarithmic term and one over epsilon. They proved, like I say, they deal with different classes of sets and essentially they proved in And essentially, they proved in terms of VC dimension, but it's I'm not going to define it, but it's known that and it's not difficult to get that VC dimension of our set is D. And Udov provides a direct proof using random points uniformly distributed on zero on the cube, which gives essentially the same bound. So, and because we start to deal with a function of two variables, not really one variable, it's already interesting what regime one estimate is better than another estimate. So, one can see that actually this bound is better than this bound unless epsilon is very, very small. So, if epsilon is larger than d to the Larger than d to the minus d, then a second bound is better. So such bound is would be interested only if epsilon goes to zero much faster than d goes to infinity. And it was also we have now many results, but historically there was a conjecture that n epsilon d behaves like d over epsilon. It was very natural. D over epsilon. It was very natural to conjecture. And another result of Book Chow, which I said, but now I will write, is that indeed if epsilon is very small, less than d to the minus d, then n epsilon d is larger than d over epsilon. So it would support this conjecture. But before Bukhao, Sosnariets proved that essentially this conjecture is wrong. And if we don't And if we don't care about behavior on epsilon, if we agree to put C epsilon, which could be better as epsilon goes to zero, then upper bound in dimension is also logarithmical. So using his result and previous result on lower bounds that I mentioned, we can also say that n epsilon d behaves like constant. like constant depending on epsilon times logarithm from D. So if you can produce picture because I will be interested in general behavior not fixing any any variable but if you have a plane in which I put here z and here one over epsilon so it's epsilon z or one over epsilon z plane what what is known that if I fix d so I'm essentially here D, so I'm essentially here. Then we know about this essentially a constant depending on D divided by epsilon. If I have that epsilon is constant, some fixed constant, then we also know behavior. We have here some constant dependent on epsilon times Lagari from D. And big question which we don't know what happens here. And basically, I will try to what we did. Try to, what we did with Galena and what I did before, we just improved some bounds that appear in this region that we don't know sharp bounds. So, and before I explain our bounds, I say a few words about this C epson, which appears in source narratives. His dependence was His dependence was quite bad, and it was almost immediately improved by Uli Reich Weberl. They essentially repeated the proof of Sostnowetz, but with much more delicate calculations, and they managed to improve the bound to essentially one or one over epsilon square. And because it was a paper before Buch Chow, they also conjectured that it should be Lega From, but now we know. But now we know that it cannot be Lagarifon because if epsilon is small, we should have at least D in numinator. And again, because I'm comparing different bounds, so when this bound is better than previous bounds, when epsilon is relatively large, large means that it's essentially larger than one over d to. D. So we have different regimes, and in each regime we have different bounds because we use different proofs. And of course, it would be nice to get some function of two variables which works for any regime, meaning any dependence between epsilon and d I also, if I will have time, I will return to this. The Sosnovet's proof, the Sosnovet's idea is slightly different from previous proof. From previous proofs, because Rudolph used random points uniformly distributed in cube, and Sosnarier suggested to use random points which are taken from certain lattices. So you have some lattice and you take randomly end points from this lattice to construct. And it's was main idea and idea and in this paper that gave logarithmic and the dependence on dimension and also i would like to say a couple of words about large epsilon and say very large and very large means essentially constant so of course if epsilon is larger than half then we can just take point in the middle in the center In the middle, in the center, and you cannot avoid this point if volume of excess parallel box is larger than half. So, N is D. If epsilon is between half and quarter, then Sasnariets also obtained bound which says that n is bounded above by one over. It above by one over distance between epsilon and quarter. And what is interesting about this bound is that in terms of dimension at a constant, it doesn't grow when dimension grows to infinity, which is probably not surprising because we should have some point of phase transition because for half, n is equal, epsilon equals half, then n equals one. n equals one if epsilon less than quarter then n should be at least logarithm of this so at some moment this function should change behavior and uh yes essentially proved that this phase transition happens at epsilon equals quarter i remind that uh i said that uh for epsilon less than quarter n should be larger than algarish from d and also they And also, they proved that if, say, epsilon is between 1 over 8 and 1 over 4, then n essentially behaves like logarithm. So my master student, Kurt McKay, he slightly improved behavior in this bound. He proved that one can take square root of epsilon minus quarter. But still, it would be interesting. Still, it would be interesting to understand behavior when epsilon equals exactly quarter, because you know more or less if epsilon is larger than quarter, you know that if less is less than quarter, but it's not clear how it behaves at quarter and also how it behaves as function of epsilon quarter if epsilon goes to quarter. Because sharpness of good result is not clear. I believe both proofs, Snarias and Kut, they played with points on diagonal, which is a very natural choice. So you create some configuration on diagonal and you get the bounds. But Kurt also proved that this behavior is best possible if you take points on diagonal. But of course, it But of course, it could be other configurations, so it's not really clear. Okay, this is this results about large epsilon. So I just combined what is known in this formula. So we've booked our result for very small epsilon. We have essentially d square log d over epsilon. If epsilon is not that small, we already can improve. Can improve the power of D or sometimes logarithm by paying price with logarithmic term in one over epsilon or with additional power of epsilon and denominator. So what we proved, we improved bounds for relatively small epsilon, but not extremely small, to the following bounds. To the following bounds. So it is what we have: we have the following that we have that it's D times double Garifon plus over epsilon plus Garifon epsilon over epsilon. So first remarks that are in my previous paper I had some additional factor here, land G and which we were able And which we were able to remove with Galena. And it also improves what was known before, because before here, in this term, there was additional factor D, and in this term, there was Logarifone one over epsilon, and we got double Lagarifon. But probably more important to understand the sharpness of this result, because what we are using, we take random points uniformly distributed on the cube. Uniformly distributed on the cube. And it was proved by Hendrix Crik, Kunsch, and Rudolf that for random points, the best what you can get, I put n random, meaning that I don't take best configuration, I take random n points uniformly distributed, independent, of course, uniformly distributed on the cube. Then you cannot get any. The cube, then you cannot get anything better than essentially d over epsilon for it is d over epsilon plus log one over epsilon over epsilon. So the second term is exactly like we got in the first term. We have some parasitic factor double gariffon. But we we get almost sharp bound for this method. Method meaning that you use uniform randomness. Use uniform randomness for choosing configuration. And yes, that's what I already said, that just to compare that from book char result, we essentially eliminated this. Oh, from Rudolph result and book chow is this result. Which one probably should compare? Because it doesn't have Logarium factor in epsilon, but. Have Logarithmic factor in epsilon, but it has a second power of d and also for small for large epsilon. I have such result here improvement with Sosnavias Ulrich and Vibral result is small. We just removed square and Lagarifon, but our method is different. But our method is different. So, and if I will have time, I will explain the difference in methods. But here, here, random points are not random choice of points doesn't work. You need random choice with respect to uniform distribution. One needs to adjust randomness. So, and bound once again, if you forget about logarithms, bound is better if epsilon. Bound is better if epsilon is larger than one over z. So essentially. And like I said, we need to adjust randomness. So I created such picture. So we have in this epsilon z plane, we have four regions. And in each region, we have some bound which doesn't well which are different and which Different and which, in my opinion, a bit annoying, and should be a function of two variables which works in all plane. And probably one of reasons for this is that all our methods, almost all methods, in a sense, split function of two variables and product of two functions of one variable. And of course, it's a huge disadvantage. It shouldn't be a product of two functions. Not be a product of two functions of one variable, it should be some function, more complicated function. So it's what is known now about this problem, and I will say a few words about proofs. So, some ideas of proofs. Well, like I said, the I will take n random points, independent points, of course, which are uniformly distributed in the cube. What I want to show, I want to show that every box of volume larger than epsilon contains at least one such point. And this will be enough for me. So, and how to do it? And how to do it? We just apply well, standard technique. We will use union bound. Problem as usual: that we cannot check every box of volume larger than epsilon. We need, because we have infinitely many, we want to have finitely many. So we will construct a set of, let us call them, test boxes. So I will have finitely many text boxes, and what I want from them, Boxes and what I want from them, I want that if each rectangle in my set test contains a point from P, then I can say the same about all possible rectangles of volume, at least epsilon, larger than epsilon. So how to do it? Well, I formulated in such a way because there is reason, but of course, the simplest way to do it. But of course, the simplest way to do it is just to ask that the following: that if I have a rectangle from Rg, so it success parallel box of volume at least epsilon. Then I can find a box, a smaller box, but from my set, say B0. This is B. But B0, so what I want that for every So, what I want that for every B in R d I want to be able to find B zero well volume of B larger than epsilon want to find B zero which is an M and such that B zero is contained in B then it would be clearly enough because because then my B zero if each B zero contains points then each B contains a point. Each B contains a point. So it's a very natural idea, of course. So, in such idea, when you apply union bound, you need to balance probability that you have good event or bad events, say, because you want to estimate bad events and to say that bad events will happen with small probability. And you want to do it for every box. So, in fact, It for every box. So, in fact, what we really need from this construction on set of test boxes that we need to control cardinality, that it is not very big. So, it's what I say that we want how we estimate probabilities that we want to estimate. We want to estimate bad probability, meaning that we can find a rectangle in N which contains no points from P. No points from P. How to do it? We check each rectangle in N and then we add up probabilities. So it is union bound. So we take some of bad probabilities, individual probabilities. What does that mean? That we estimate probabilities that given rectangle in N contains no points from P. It's very natural idea. And usually, of course, this step is not easy and many problems to do. Not easy and many problems to do because it's not very easy to estimate individual probabilities. But here we are lucky because what is individual probability? We work with uniform distribution and we work with excess parallel boxes. So we need just to evaluate the volume of a box which is very simple, just product of lengths of corresponding segments. Corresponding segments. But what else we need? In order to get at the end a reasonably good bound, we also need to ask something more because we will get probability which depends on volume. So what we really need that boxes in my end, in my net or set of test boxes, the boxes are not very small. So we need I need the following. Let me write again. So, for every B in R D, I want that there exist three B0 in N with two properties. Before I wrote only one, B0 inside B. But I also want, well, B is larger than epsilon. But I also want to introduce some parameter and to say that the volume of B is larger than delta. Than delta. So, delta, of course, depends on epsilon, delta epsilon. And of course, I don't want to lose too much. So, my delta should be comparable with epsilon to get something reasonable. But this is the only obstacle here. And main difficulty, of course, to construct such set N with cardinality which is not exploding. Rudolf used this idea, and what he used here just concept of delta cover, which I'm not going to explain what it is. It was some construction due to Gnevich, but it was constructed for completely different reasons. And because of it, the construction is not very well fits our problem. It's why we were able to improve his construction. Prove his construction. So let me introduce a bit more notation. So first we will put B epsilon D. So it's my rectangles, but now I control volume larger than epsilon. And we use so-called delta approximation, which is exactly as I said, that for every B of volume, at least epsilon, I can find the B0 in my B0 in my set of test boxes such that B0 is inside B and volume is larger than delta. And if you just, well, then we just use the following lemma for delta approximation, which is straightforward. Rudolf proved this, but in fact it's well Rudolph proved not exactly this, he proved slightly different. Yes, he proved slightly different, but it's essentially the same because it's exactly what you would do if you try indeed to prove such results. So what we can get, we can get that if we have such delta approximation for b epsilon z, and if n, well, usually n, of course, will be much larger than 3, but say let's n larger than 3, cardinality larger than 3, then we have high probability, it's 1 minus 1 over. probability it's one minus one over n n will be huge in fact if i take n points which is logarithm of cardinality divided by delta then i will get my property and my property is that any box any box in in this set contains at least one point so with high probability such amount of points Such amount of points drawn uniformly from the cube will give me a result. This is, I would say, rather standard lemma. You just compute probabilities of any box contain a point. And like I said, it's simple because we deal with Liebbeck measure. And then you do some estimates. I will skip the proof because I don't have time. I wrote it here, but it's really simple. Really simple. And then I will probably explain some ideas how to construct sets because it's really what is new about this proof. So what we will do, we will approximate boxes in the following way. So we start with boxes which contains a region as a vertex. So such a set of boxes. Oh, by the way, I should say that this idea comes in. In my paper, I use different ideas, but this idea comes from your paper on random matrices. So first it's clear that, in fact, any such axis parallel box can be defined by two points, but one point here we fixed at a region. So we can identify this with just a set of We just set of points upper right corner, so B1, and so on. And it will be more convenient to deal with such points, not with boxes. And that's what I wrote here, that we will identify our box with this point. And then another basically obvious observation that we don't need to. That we don't need to consider boxes of volume larger than epsilon, we can decrease them because if you have a box which is large, of course, I can slightly decrease it. And it's enough to consider smaller boxes. They should contain point. So I will consider boxes only of volume epsilon. And then we consider the following set which Which is essentially set as before. So B corresponds to rectangles. So I said product of B i corresponds to volume, of course. And instead of epsilon, I will put epsilon to the beta. But in fact, I will apply it twice. One with beta equals to one. It's my original set. And second time, I will apply it with beta equals to one. applied with beta equals one plus gamma so it will be error well if beta equals one plus gamma then epsilon to the beta will be epsilon epsilon to the gamma this will be my error and this will be my data just to to give intuition why why why i'm writing like this so uh and uh this is some And this is what we will do. We will construct set N0 in this set with epsilon to the 1 plus gamma. And what I want, I want that for every B in my initial set, when beta equals to 1, which corresponds to set B epsilon Z, I can find points. I can find point in my net such that all coordinates of my point less than bi. And how to do it? This is the idea. So we apply the following function. We apply a function f epsilon of t, which is written here and to each coordinate. So f capital epsilon will be application of this function to. function to coordinate wise. And why this function is good? Because of course we don't want to work with products. We want to pass to sums. So if I apply this function then point B, which has product of B i's equals epsilon to the b comes to sum of images of bi which is better. And it's also easy to see that if epsilon It's also easy to see that f epsilon is bijection and between sets that we want to consider and the simplex. So what we get here that from some set, the product of b i equals to epsilon, which looks probably like this. This goes to very easy geometric object. We have hyperplanet, we know that. Hyperplanet, we know that sum of xi equals is fixed. So, in fact, we have simplex because we know that all of them are positive. And in fact, it comes to the following fact that, well, I almost don't have time, so I will explain a bit that when I apply this function, what I will get? My initial set will come to the... My initial set will come to the plane where sum equals to one. And set by what I want to approximate will live on the plane where sum of coordinates equals to one plus gamma. And what I want, I want that for each point here, I can find point here such that each coordinate is logic, which means that each point here can cover. Each point here can cover, in a sense, or approximate all points here. And if you look carefully at this picture, then what you will get, you will get that you will need to cover simplex, which is here, by minus simplices, which are here. So it reduces to covering number of simplex in smaller dimension. This is regular simplex. This is a regular simplex by minus gamma times simplex times simplex times the same simplex and such bounds are well known. So we get a bound on cardinality and we apply this bound. Well, I should stop here. So this was step one. Step two is, we will say probably just two words. So we have so-called anchor board. We have so-called anchor boxes which are here. But if you have a general case, what we will do? We will take this box and shift it here. And notice that this box, initial box, will be a box, shifted box. And we need to know this shift. So we use our approximation, which I just explained, using simplicity. And additionally, we need to approximate low corners. Approximate lower corners. So we need to calculate some net for left lower corners. And this gives a result. So unfortunately, I should stop here. We have several more results about large epsilon and about dispersion on torus. But I stop here. Thank you very much. Okay, thank you. Any question or comments?     