Thanks, William. My talk has nothing to do with matchings, so I don't have to compare this work with anybody else's. On the other hand, I hope I don't bore you because it's a different topic from what's the theme of the workshop. But I blame the organizers because they said that it's better for me to talk about something different because there's so many thoughts on matching. So I'm going to talk, what does this talk about? So it's a technique to prove. Technique to prove the central limit theorem, and not only to prove the central limit theorem, but to also establish rates of convergence for the central limit theorem. Meaning that we all know that 1 over square root of n, sum of some random variables minus the mean, will converge to a Gaussian random variable as n goes to infinity. But the question is, what happens if after you stop after n samples? And how close are you to Gaussian? Okay, so that's the question we're gonna ask in this talk and try to answer. Asking this talk, we'll try to answer. And the motivation: I'm going to give you a few motivations from machine learning and graph sampling. And we'll define what it means to be close. I mean, what does it mean for a random variable to be close to another random variable, in particular, to a Gaussian random variable? And we'll use the Wasser-Stein one metric for it. If you're not familiar with it, I'll define it. And we'll use Stein's method to quantify the rate of convergence. And I'll go back towards the end. I'll go back towards the end if there is time and talk about how non-asymptic central limit theorems can give several complexity results as well as insight into hyperparameter choices for algorithms. And I don't need to go through all of it, so please feel free to stop and ask questions. This is supposed to be a tutorial. Okay, so yeah, so I'll start with some motivating examples. Since this is a talk on, this is a workshop on matching, I thought I should. This is a workshop on matching. I thought I should at least have one slide, even though it's a trivial example. I just thought I'd have at least one slide with some cues on the left and cues on the right, and people can match stuff like that. So all of you have given talks on matching problems. What are you doing mostly? You're using theory to come up with some algorithms, and then you try to test it out in simulations. So, how do you run a simulation? You have a performance metric. So, often in this case, So often in this case, you're controlling prices that you paid, that you ask from the customers, the payments to the Uber drivers, or Lyft drivers, or whoever. And then you make matches, you assign customers to taxis. Then you have some performance metric, maybe the overall revenue minus the delay or some weighted performance metric, weighted revenue minus delay. And you would run a simulation, right? You would run a simulation once, it would run a simulation. Simulation once, it runs simulation twice, ten times, then you can draw nice error bars. And where do these error bars come from? It comes from the central method. So if you run the simulation n times, and the first time you observe x1 as your performance metric, second time x2, and so on, you run it n times, and then you sort of pretend that the CLT approximately implies and say that x1 plus x2 plus xn divided by n is n, it's not quite. By n is n, it's not quite true, but it's approximately n times, I mean, a Gaussian random variable with a mean mu and variance sigma square per n. So you estimate the mean from assimilated data, estimate the standard deviation, and if this quantity is much smaller than mu hat, or you know, like one tenth of mu hat, then you think, okay, it's good. And you can actually give some probabilistic guarantees, even in the sense that with. Guarantees, even in the sense that with high probability, this is the correct. Of course, if you're a real statistician, you may not be happy with this, and you won't use the central limit there. I mean, you won't use a Gaussian distribution, you might use the student T or whatever, but I think if N is R, I don't think all of these matters so much. But that's a boring example. The main reason I put it up is because I wanted to show that picture. But really, what I'm interested in is: can we use the CLT to design Apple? And there is a lot of work going back to the early 1990s on this topic. And I'll talk about the very early work on this topic and then transition to something newer. So an application that is still relevant today is STD, stochastic gradient descent. So where you're interested in minimizing some function and you do gradient descent, but you don't have access to the gradient, you have the access to the gradient. Gradient gaps with gradient personals. And then this algorithm, let's assume F is convex and all that. And then if the step size satisfies some conditions, then it's known to converge to the correct answer. So one example of the step size could be 1 over k, for example. That satisfies summation goes to infinity plus summation 1 over k squared or pi squared over 6 or whatever, right? But there are other step-size choices too. Which one of these step-sized choices? To which one of these subsizes should we use? You can use one over K power 0.9, for example. Right? Which one should we use? And this is a question that was answered in a series of classic papers by Polyak, Rupert, and Polyak and Judiski. Everybody calls it Polyac-Rupert averaging, even though Rupert's work looks like it's earlier. But I think Rupert's, from what I've heard, Rupert's paper was rejected. So I think the only version of the paper we have is a technical report for NL Technical. Paper we have is a technical report, Cornell Technical Report. So, my guess is that Polyak probably did it also around 1988, and his was actually published in a journal in 1990. And it was further analyzed by Polyak and Juritsky in 1992. What they showed was this amazing result that said that you should not choose 1 over k, even though that's like the most natural thing to choose if you look at the step size through conditions. You should choose 1 over k power delta for any delta between half and 1. Any delta between half and one, not one. But then at the end, average the answer, give it out as an answer. Okay, so this is called polyacropic averaging. Of course, averaging is sort of a trivial thing to think of, but I think the point here is that you use an epsilon k that's 1 over k power delta between delta and r. And what they mean by this is the right thing to do is that they established a central limit theorem, assuming that the noise is a martingale, and showed that the scale Ensured that the scaled version of the error has a covariance that is the smallest possible covariance amongst the large class of map duplicates. Good question. So they show that this is true for every delta between half and one. So partly more. Sorry? Not one. But not one. So the depth set is deliberately open in the sense that yeah. Yeah, yeah, yeah, yeah, exactly. Exactly. So you can pick any of those? Any delta, and then do the average, and then asymptotically, the noise would, I mean, the variance would be sigma. So any such delta will keep the same covariance matrix. Exactly. But the rate at which it converges to the covariance matrix, which is one of the computers I went off, would be, you can use Stein's method to figure out the rate at which it converges at the answer. There is a the rate of convergence could depend on it. What they show is that asymptotically, as n goes to infinity, there is no dependence on delta, as long as it chooses between half and others. But it hasn't possibly been a problem. And they showed that this is the smallest possible error covariance. For those of you who know estimation theory, this is very closely related to the Trameral one. Okay, so they actually, the six. Okay, so they actually, the sigma is related to the Fischer information matrix and so on. But this is not, you can't directly apply Cramer-RauBound to STD, but if you think of a regression problem, for example, you can apply the same algorithm, and there it makes sense to call it the Cremer-RauBound. I think Polyak and Sipkin first came up with an algorithm for regression that they showed can achieve the chimera bound in an online fashion. The Tremero mount in an online fashion instead of collecting all the information and doing it. But then later, Polyak realized that their algorithm requires problem-dependent parameters. And then he came up with this rule, and then he showed that it doesn't work just for regression problems, it works for much more Dindle. And since SGD is still popular, I mean, it's the most widely used algorithm in machine learning, so this results are still relevant even though it's very old. A newer result is in the context of reinforcement learning. We all know reinforcement learning has been widely successful in multiple problems. One particular subclass of reinforcement learning algorithms is temporal difference learning, where you're not trying to find the optimal policy, you fix the policy, and you try to find a value function associated with it, or some performance associated with it. So, reinforcement learning can be thought of as. Reinforcement learning can be thought of as this recursive equation. Theta is some parameter representing the performance of a Markov gene x k. Theta time k plus or minus theta time k minus some step size times a linear looking term. It is a linear term in theta, but the matrix A and the vector B evolve according to a Marshall. So this problem is different from the polyak injury problem, Polyak-Cooper-Polyakjudisky problem, because Hooper-Polyacticity problem because the noise is multiplicative here. It's not additive. And so, this is actually that the theory doesn't directly apply. It's quite complicated to prove convergence here. But in a very nice paper, which is one of the motivations for my interest in looking into Stein's method, was this paper by Burkhard Chen, Devaraj, Pontianis, and Maine, where they showed that a scaled version of the error vector converges to a Gaussian. And this sigma is also. Gaussian, and this sigma is also the smallest possible covariance amongst the class parallel sigma. But this question that you asked about, again, delta, it doesn't seem to depend on delta. And so one of the questions they asked in this paper is that, is it possible to establish a non-asymptocentral delta? Okay, that may give some insight into the use of what delta one should use. Okay? Sorry, just a little question. Sorry, just one question. So, theta hat is the average over n. Right, sorry, yeah, I didn't define it here. Yeah, theta bar k is the average average question. So, the syntax is in n, huh? Yeah, since so that is for anything. It should be theta n, if that's your question. Yeah, yeah, yeah, sorry. Yeah, yeah. Sorry, yeah, sigma though, would that be like you take your A matrix? A matrix spatial distribution of x, and then I'm trying to think like how cross multiple times. Well, I mean, it is, I mean, when you talk about Markov chains, if you write a central limit theorem for Markov chains, then normally because it's IID, you can just think of it as variance of each individual term. But if it's dependent, you have to take the covariance of the whole thing and take the limit as n goes to infinity. So you can write it in terms of something called the solution to Poisson's equation, but it's not an easy thing to write it. Easy thing to write it. And you need some assumption on the market chain in terms of the yeah, so you can assume this result holds under if it's finite state, irreducible aperiodic, or you can also do it for general state space Markov chains if you assume something called geometric products. Okay, yeah, some sort of like a generalization. Yeah. Quick question. So the fishery information matrix depends on the function, is that correct? Like for example, in the Like, for example, in the SGD itself. Yeah, so the A and B function. Yeah, here. In the SGD case, it will depend on the function. The objective function. And then, in cases, for example, that you have a strongly convex function that is smooth and therefore you can get linear convergence rates, what would happen? Because here it is about one hour square root and it's one hour square root of n, yeah. But what would happen if the TD converges faster? Because I can't give you functions probably. Because it's the faster case in STD. I don't think you can, right? You can't get extra. I don't think you can, right? You can get exponential with STD you can get exponential, yeah. It's a fundamental limit, yeah. The final example that I'm going to give is one from graph sampling. This is a paper that appeared in 2023 in ICML and won the Best Paper Award. So, in this paper, they're interested in graph sampling. They're interested in estimating. Sampling. They're interested in estimating some quantity, let me call it summation i pi i f i. fi is a quantity associated with a node. So, for example, at node i, let's look at an indicator function whether the number of the degree is greater than 5, for example. And if pi i is some weight, and for example, if you take pi i to be 1 over n, uniform distribution over all the nodes, then this would be the average number of nodes with degree greater than the phi. And the natural estimator for this is to set up a random walk whose stationary distribution is pi. And you just simply estimate it, just run it for t time instance. And if t is large, this will converge to this. That's okay. I guess what do I mean by random walk? So you started some node, and then you pick one of the neighboring nodes at random. And then again, you go from 0, pick a neighbor at random, go from 6, neighbor at random. At random, go from six neighbor at random, and so on. This is what people do in web crawling for lots of reasons. You have to do random walks to crawl the internet or Facebook graph, for example. If you do deterministic walks, then you can get stuck in loops and so on. So people design a random walk that's a stationary distribution, is pi i. Now, the question is that there could be many transition probability matrices that give the same stationary distribution, pi i. Which one should you use? This sort of then reminds us of the question. Sort of then reminds us of the question Polyak and Judiski asked, where they said, Amongst all step sizes, which one should we use? Yeah, these people, Doshi, Hu, and Yoon, asked the question, you know, what type of probability transition matrix that one could use. For example, one could just use a regular random walk, or you can be clever and try to not backtrack, not go back to a node that you were before. Both of them will get the same stationary distribution. This has been. Same stationary distribution. This has been widely studied in math and CS theory. And people compare mixing times for comparing various random blocks from other applications. What Doshi Hu and Ayun argue is that mixing time is not the right metric for this graph sampling problem. You should look at the asymptotic variance because you are, after all, interested in estimating summation i, pi i, fi. So you run it for t time instead, you're interested in the variance, the error in your answer. Okay? And so, what they prove is for a large class of random blocks, I think they've been working on this problem. I think the last author, Do Yang Yoon, has been working on this problem since 2012, Sigmetrics. And finally, he came up with something, a radically different algorithm in this paper. But so, what they showed is that the FIK satisfies, again, essential detail. Yeah, right? Yeah. Right? And so from this, what he argues is that, or what they argue is that choose a probability transition to minimize the covariance of the error. They don't actually minimize it in this context like in Polyak and UDC. They just say that compared to other random walks that are known, they came up with something called self-repelling random walks. And that random walk has a much better covariance compared to all others. And then they do simulations and show how much better it is in terms of estimating the parameters. Estimating the parameters to won it, and this paper won the BISC paper award in ICM in 2023. Surprisingly, it's a very mathematical paper, so it has five or six citations, whereas normal ACM this paper awards 20 gallon citations. So, typically, how are the pipe different? That depends on the application. So, the application that I mentioned, you might be interested in some statistics about the node degree, then you might make it even. But I think here, I think you should look at the paper. I'm not very familiar with the examples that I give in this paper. Examples that they give in this paper. I think they talk about federated learning. So you have nodes have different amounts of data, and you have to somehow give different weights for them to get a final good answer. And then, yeah, so they come up with different examples of what pikes could be. I don't know all the applications, yeah. So for your SGD case, you wanted the rate of convergence to figure out the right delta to use, but here, which random walk to use, they figured out based on the sigma of the delta. They figured out based on the sigma of the no, they did not figure out which random lock to use. They came up with a random, that's why I put minimize what they don't actually minimize any. They just came up with a random lock that seems to perform much better than any other known random lock that we can use on the past. You'll be able to go one step further with that. That is an interesting question. I mean, that's why I put quotes here. I don't know if there's a way to say that some random block is the lowest asymptotic coherence. Isn't the analog of this in the SGG example of something? Analog of this in the SGG example of something like picking the distribution of the martini? Yeah, exactly. So here it is that you have to pick the next state to go to. Yeah. Any other questions? Okay. Okay, so the common theme in all of these examples is that you have some, either you have xi minus x bar or some function of a random variable. The random variables could be IID, they form a Martingale different sequence, they could form a Markov chain. And in all of these papers, what they show is that the scale quantity converts to a Gaussian random vector as a vector, and shows that this vector has the smallest possible variance. So I put that in black and this in blue because I wanted to say that I'm not going to talk about the lower bounds. It is, as I said, it is very strongly motivated, at least in places where they've shown lower bounds, it's strongly motivated by the Chimera law. But it doesn't apply for some of the problems, so you have to. But it doesn't apply for some of the problems, so you have to sort of come up with a class of algorithms for which it applies and so on. Okay, so now I'll get to the technical portion of the talk. So I want to present science method as a method to prove both the central limit theorem and also figure out the rate of convergence. So the first use itself might be useful because it's a validator lease. I feel like it's a relatively simple method to do it. It requires slightly stronger assumptions. Instead of the second moment existing, you might require the third moment to exist, or 2 plus delta moment to exist, and so on. So to do this, we need to first quantify what do we mean by distance between two probability distributions. And I'm going to use this definition. So this is the definition of a Wasserstein on metric. Well, some people may not call this a definition. Well, some people may not call this the definition, they might say this is the dual characterization of the Poisson sign distance. I'm just going to call it this as the definition. And I'm also being very loose, it's actually a distance between two distributions, not between two random vectors, but just for notational convenience. And it's easier when I'm talking about applications to put in whatever I'm measuring there. So it's really, it's just the maximum soup over all Lipschitz function h, expected value of h of x minus h of y. Okay? So you may have seen. To point. So you may have seen the earth-mover distance definition of Wasa-Sein distance. This is just exactly the same thing, except in Zoom or form. But this is what is needed for the proofs in this area. And the nice thing about the Wasser-Stein distance is that if you prove that you have convergence in Wasser-Stein distance or you have a rate of convergence in Wasser-Stein distance, it immediately gives upper and lower bounds on the rate of convergence in the first moment. You get rate of convergence in the pth moment. In the pth moment, you have to work harder. Sometimes you may have to prove something in the Wasserstein 2 metric to prove a greater convergence in the second moment, 3, you know, Wasserstein P metric. But you can also, even without that, if there are boundedness or uniform integrability, you can get additional moment balance. But I won't get into it. I'll just simply stick with talking about convergence in the most statistical sense. And in particular, I mean. And in particular, I'm interested in the situation where y here is a Delsian random vector. So it's z is n0. If it is not i here, I might put a subscript here to say z sigma would mean n 0 sigma. Okay, so the key idea, this beautiful observation by Stein, is a random variable is n01. So I thought myself. I thought I said most do this light. Okay. A random variable is Gaussian if and only if it satisfies this functional equality. It's a beautiful result. Just for every function f, you put every imports because you need some conditions. For example, expected value of f prime z should be finite, f should be absolutely continuous, in which case f prime is what are called the means. But yeah, so instead of getting to the Yeah, so instead of getting into the technicalities, essentially it says that for every function f, expect a value of f prime of z minus z of z equals zero. The intuition behind this is very simple. The proof of this is extremely simple. If you think about the Gaussian distribution, it's like e bar minus z squared over 2. The derivative is minus z pz, right? So it's as though taking the first part and integrating by p of z, and the second part you're integrating with respect to p prime of z. So it's obvious you should do some integration by prime. So, it's obvious you should use integration by parts. And then you can see that it actually becomes zero. Okay? The second motivation is useful actually for in the case of vectors when you're dealing with Gaussian random vectors. But if you're not familiar with Ornstein Ulanpath processes, you can forget it. But if you don't know Ornstein-Randpath processes, you think about the generator of an Onstein-Ulamp process. And the generator just simply tells you how much a function changes in expectation from one time step to the next. To the next time trip goes to zero. And so, if you take the stationary distribution of a non-strinyl input process, which is a Gaussian distribution, you stick it in the generator and take the expectation. Of course, in steady state, nothing should change. So, the expectation should be equal to zero. That actually gives you this equation again. With some one more minor massaging, you'll get this equation. Okay, so there are two useful interpretations. The first one is easy, very easy. The second one requires a little bit about diffusion processes to understand this. Diffusion processes to understand this. But if you know it, that's a useful definition to know for, especially when you're dealing with convergence to vectors. So, this motivates an interesting question. So, Stein basically said that Gaussian random variable satisfies this functional equation. So, is the following conjecture true? Suppose that random variable satisfies this equation approximately for all f, and you show that it's close to a Gaussian distribution. Is the question clear? So, this was his. Question clear. So, this was his motivation. So, what does this mean? We have to somehow relate this to the Wasserstein distance. So, if this expected quantity is close to zero, we want to show that the Wasserstein distance is also close to zero. And so he came up with this beautiful inequality. I'm not going to prove this, but it's not that difficult to prove. But I'm just going to state it. This essentially states that the Wasserstein-1 distance between x and z, if z is a Gaussian random variable, or Gaussian random vector. Gaussian random variable or Gaussian random vector. Well, variable, it's a scalar case. And then you take the supremum over a certain class of functions. This is what the hard part in Stein's method is to figure out what class of functions you have to use to upper bound. Because if you just allow all functions, this will be a very loose upper bound. So you have to try to find the correct class of functions to get the tightest upper bound. Okay? The interesting thing is the right-hand side depends only on x, on the function. only on x, on the function you're trying to approximate by a Gaussian. So it just simply says that if this quantity is close to 0, there is no mention of z on the right-hand side, except that the information about z is encoded in the functional equation that we saw in the previous picture. Okay, so this is the basic idea behind Stein's method. So now the rest of the talk will be about how to get a bound on the right side. Is this clear? Yeah. Sorry, I guess I didn't understand. What do you mean you can find a What do you mean you can find a smaller class? Because I thought the upper bound is for all the boundar functions. So I guess. No, yeah, that's correct. So, good question. So, this inequality is for, this equality is satisfied by Gaussian distributions for all functions. But here, this upper bound, you have to find the right class of functions so you get a good upper bound here. So, why can't I pick just one function? If I just pick one function, it's not an upper bound anymore, right? Is not upper ball anymore, right? So my question is: should it be infible or is no, it's soup. Soup. Is it inf? No, wait. So the point is that if you consider all class of functions, if you consider all functions, this will be a very large number, right? So you want to try to restrict the class of functions. Even though the intuition comes from this functional equality. comes from this functional inequality that's true for all f. Yeah, but surely I can't just pick anything. Like if I pick a function that just only has to be zero, then it can be an upper button, right? If f is just a zero function. Then it's not an upper button. So it has to be a when it is normal. So that would be zero only if x is normal. If x is not normal, then it's not going to be zero. So when if it is normal, then you have that. If x is normal, z is normal, then this consists nothing. Okay. Must contain some functions, really. Yeah, yeah. So in the definition of the Watsonstein metric, you've got like Lipschitz. So the left-hand side is the Watts-Stein distance. Watts-Stein distance in the definition from a few slides ago was super-all Lipschitz functions. You cannot take all possible functions. I cannot take quadratic functions. I have to take super overall Lipschitz functions. And that family being Lipschitz translates to this F. Translate to this as being some other fact. So, one way to think of it is that you set this quantity equal to a Lipschitz function and then solve for f in terms of a Lipschitz function. And then since H is Lipschitz now, that puts constraints on what f can. Does that answer your questions? Maybe I should have gone through the proof of that to be together. Yeah. Should we have this A? Should be F is a class of bounded functions. Is the class of bounded functions that there is a particular spawn on the first and second point? Yeah, pretty much functions. Yeah, sorry, trivial question. What does a function bounded second mode mean? What is that? What does it mean for a function to have bounded second mode? Do you mean a bounded second? F prime, f prime is bounded, f double prime is bounded. Oh, okay. Oh, okay. Yeah, yeah. Did I say derivatives is what I meant now? Okay, sounding sorry. Derivatives, derivatives. Yeah, yeah, yeah. This is a cheap trick to get questions, to make mistakes. So our goal now is to understand. So we all know the central method theorem for IID random variables with mean 0 and variance 1. And the question is, how quickly does convergence? Is how quickly does convergence occur? So, this was answered by Stein in 1972. Of course, you probably all have heard of the various sync elements, which was much earlier, but Stein's method can be extended to all kinds of dependent random variables. That was a big breakthrough of Stein's, not just for IID random variables. But I'll go through the idea behind Stein's method for IID random variables because Stein did it much more generally. Okay? So, what is the idea? The idea is Taylor series. I mean, we know, for those of you who know the proof of the central limit theorem, For those of you who know the proof of the central limit theorem using moment-generating functions or Fourier or characteristic functions, it's basically a Taylor's theorem proof, right? So, this is very similar. So, we need to study this quantity and show that this goes to zero for a certain class of functions. So, you define W minus I, I hate the notation, they call it Wi in most of the science weather papers. I like W minus I. I assume most of you here are familiar with gaming. So I assume most of you here are familiar with game theory and I like the W minus i notation. So I'll use W minus I. So it's the entire, it's the scale sum except for I. Okay? And now everything is Taylor series from this point onwards. So F prime minus W Fn. So I'm going to replace Wn by its definition, which is the scale sum of the random variables. Now then I'm going to, there's f of w n, I'm going to add and subtract f of w minus i. f of w minus i. Now the whole point of defining this is that w minus i and xi are independent, right? Because xi is not there in w minus i. So therefore this is 0. Now we have an opportunity to use Taylor series, right? Because there's f of w n and f of w minus i. Let's use Taylor series. So if you use Taylor series, then you get f prime of w minus i. And the difference between w n and w minus i is x i divided by square root of n. So you get x i square root of n n n n n n n n n n n n n n n n n n n divided by square root of n, so you get xi squared over. And by assumption, expected value of xi squared is 1. So this is this. And now again you get an opportunity to do Taylor series again. So this is why I needed bounded first and second derivatives. And if you do it again, you have this, and again, these are independent, this goes to zero. This doesn't give you rates of convergence. I'm just telling you the intuition behind it. And what Stein showed was actually the rate of convergence is 1 over root n. Of convergence is 1 over root n, and the basic ideas don't use any approximations. Use the remainder in Taylor's theorem, and if you have used the remainder in Taylor's theorem, you need the second derivative to be bounded. So you can just upper bound fiber prime by something. And you need the second derivative term, if you do it very carefully, will involve the expected value of xi cubed. So we have to assume that that's bounded. So this is the additional assumption that you need compared to the classical assumption where you only need the variance. Is it an assumption or is it something that you have to actually show? Is it something that you have to actually show? Because this is it. Is it an assumption or something that you have to actually show? Because this. No, this is an assumption. The definition. Just say that under this assumption, this is true. So I'm saying that if xi is a sequence of independent random variables with bounded third moments, then this is true. For Berry Academy theorem too, I think you need third bounded third moments. I guess my point was in the previous slide when It was in the previous slide when these are all approximations, so I didn't need anything. I mean, so the correct way to do this would be the Taylor's theorem. I'll have an additional term here. So what was the? So my question is that the first proportion of the second order portion of F should be counted, right? And this is something that you have need to show because. Oh, I mean, I don't need to show this. That is, sorry. Yeah, maybe that is. Let me go back. This basic result says that f is a class of functions with a certain upper bound on the first derivative and the second derivative, then it upper bounds so much as time distance. This is a result that I'm using without proof. Yeah, I mean, yeah, I wasn't sure what what all to use, what ought to include and what ought not to include in the in the it's a forty five minute tutorial, so I did not include the proof of that, but uh might have been useful to include that. I might have been useful to that. But anyway. Okay, so this is Stein's result. And the remarkable thing he showed was that this works for weakly dependent random variables of all kinds. And this is not the only thing that is called Stein's method. Stein's method is basically follows from the functional equation that we wrote down a few slides ago. There are many, many variants. This is one variant of Stein's method. But all of them start from. But all of them start from that functional equation that a Gaussian random vector has to satisfy. So, the central limit theorem for IID random vectors, surprisingly, is still an active area of research. People don't know whether the rate of convergence, I think, is well established, the constants in front. In particular, people who are interested in high-dimensional statistics, this last two papers by Zai and Putkari and Fatih and Panajari, the rate of convergence, I said, is 1 over the square root of n. Over square root of n, but it also depends on something that depends on the dimension of xi. So the question is: what is the optimal dependence there? People believe it's square root of d. So I proved it under the assumption that the random variables are bounded. Furtadev, Fatih, and Pananjari proved it, assuming that the random vectors satisfy Poincare inequality. Not all random vectors, even just simply discrete random vectors, really don't satisfy the Poincare inequality. So typically, you need log concavity at the PDF. Need a low concavity at the PDF. So it's still an open problem to figure out what the optimal dependence on D is here. People believe it's square root of D, whether it is actually square root of D generally, it's a moment. Okay, so this brings me to the Markov chain central limit theorem, which will be the sort of the concluding part of this talk. So everything that I've talked so far is known, and this is relatively new results. So to the best of my knowledge, there was So, to the best of my knowledge, there was no non-asymptotic central limit theorem for Markov chains until recently. So, in the case of a Markov chain, I am interested, if x is a Markov chain, r is some vector value function of a Markov chain, I'm interested in R of xk minus r bar, where r bar is the stationary expectation of r. And this is well known. So, the Markov chain central limit theorem is well known. The question is: can we obtain rates of convergence in the Markov chain central limit theorem? The Markov change in terms of the problem. Here, sigma infinity, as Neil asked, is a somewhat complicated question. It's a covariance of this in the limit as n goes to infinity. That may not always exist, so you have to impose some conditions on the Markov chain for that to exist. And then if it exists, then you get the same limit. So the question is: how quickly did D go to zero? And so, this is what I was able to prove in a paper last year. Paper last year that it's of the form log n over square root of n. I don't know if the log n factor is necessary. It could be an artifact that we do. The lower bound for the IIDT is the square root of n. I don't know if there's something fundamental about Markov chains. At this point, I haven't cleared it out. So the main idea is to convert a question about Markov chains into a question about martingals using something called Poisson sequence. This is a bold Equation. This is a bold idea and probability, and I don't even know who was the first one who came up with it, but it's been around at least from the 70s. And then the idea is to obtain a rate of convergence for vector-valued martingales. In addition to the Burkhard paper that I mentioned earlier, this paper by Rowling was another motivation for me, which was he proved a rate of convergence for scalar martingales. Going from scalar to vectors somewhat difficult. So the basic ideas in the proof is to first provide something. In the proof, is to first prove a central limit theorem for, or a non-asymptotic central limit theorem for vector value in Martin Dias, and then translate that to a non-asymptotic central limit theorem for Markov chains. So, this is an equation that many of you who, I think there were lots of talks on dynamic programming, the value function is equal to the value function plus average reward equal to the reward plus, I just wrote it slightly differently, reward. I just wrote it slightly differently. Reward was the expected value of the value function in the future. Because I want to obtain a central limit theorem for this, so I wrote it in this form. Okay, so everybody see that that's just simply the Bellman equation for the average word MDP without any min or max. So let me repeat it again. R bar plus B is equal to R of XK plus expected value of B given the past. This is how it is normally written. I just simply reload it. Okay. Is the right hand side like a Martingale difference of sort? Not yet. It will become a Martingale difference in a second. Yeah. This is called Bellman equation. Normally, when you're looking at Markov decision processes, R is a scalar, but I can think in this case R is a vector. And there are conditions under which there is a solution to this, there's a solution V to this equation. One condition is it's the irreducible aperiodic finite state, and the other condition is that for general states, Other condition is that for general state spaces, you need something called phi irreducibility and geometry algorithm, but let's not worry about that in this talk. So, the key idea behind this is not my idea, this is a known idea in PowerW, which is you add and subtract B of xk plus 1 here. Now, this is a Martinian difference, Martinian difference. If you take that, condition it on the past, then this is zero because by the, this is just expectation, this is by the property of expectation. Property of expectation that's also zero. So this is a marking value, right? And this is B of xk plus one minus k minus b of xk plus one. So when you sum it, what's going to happen? Only the first and last terms are going to remain. So a central limit theorem for Markov chains can be converted to a question about central limit theorem for martingales plus a term that you have to handle. For finite state Markov chains, this term. For finite state Markov chains, this term is very easy to handle because v would be bounded and divided by square root of n as n goes to infinity, it's going to zero plate one over root n, and that doesn't change anything else in our results. But for general state-space Markov chains, this requires some work to do this. So basically, now we have converted this question to a question about convergence rate for. I guess I have five more minutes. It's a sort of gone. So the goal is to show that m1 plus m2 plus mn is close to z1 plus z2 plus divided by square root of m is close to z1 plus z2 plus zn. And the beautiful idea due to Lindeberg in his proof of the central limit theorem, the way he proved the central limit theorem was to replace one term here by the corresponding zn. So replace mn by zn, mn minus 1 by zn minus 1. So each time you're sort of looking at the difference between You're sort of looking at the difference between two quantities where only one random variable is different. This is what's called the Lindevergu decomposition. So, what I'm going to do is use the same idea to compare h of m1 plus m2 plus mn and h of z1 plus z2 plus z, which is what we need for the Wash time distance. So, essentially, I'm going to write this as a telescoping sum, where each term differs by only one random variable. This is not in the form. This is not in the form that one can directly apply Stein's method, as I talked to you earlier, because in Stein's method we looked at h of some random variable plus h minus h of a Gaussian random variable. But still, there's only one thing that's different here, which is obviously suggests that we should still be able to use Taylor series ideas here. And so the key part of the main part of the proof is a very complicated calculation that shows that this quantity of This quantity will go to zero in the limit as n goes to infinity for Markov change, for Merding Yes derived from Markov change. The key part of the proof is to use regularity conditions. Regularity conditions are the conditions that I mentioned. I said that you need the first and second derivatives to be bounded, though that's not enough in the case of vector-valued random variables. We have much more complicated conditions, and we use Use some from this paper and some from this paper to get the result. And you get this amazingly ugly martingale. I just gotta paste from the paper. The main reason is it's not surprising that it's ugly, right? Because I didn't put any conditions on the martingale. Of course, martingales may not satisfy central galaxy. But for example, if you use a stationary martingale, where the sigmas are the same at each step, then this will become zero, for example. So I won't do that, but I'll just So I won't do that, but I'll just, but so you have to stick in, you have to make some assumptions on the marking. This result is generally true for some, it has lots of parameters I haven't defined. But the result is generally true, but of course to show that it goes to zero, you have to put some conditions on the martinga. And in the case of Markov chains, you get a specific martingale, and the specific martingale satisfies some properties because the D is a solution to the Bellman equation or the Poisson's equation, right? Of the Poisson sequence. So it has very specific structure, and using that structure, you can show that you get an ordering of square root of m for Markov chains. An open question, I've already mentioned this. There is some progress in the IID case, something I'm interested in thinking about, but some of you maybe have a stronger background to look into this. And open question for those interested in high-dimensional statistics is: what is the optimal dependence on this D here? or D here. In the what's called, what people call the isotropic case where sigma infinity is I, then you get a D here in the proof. But I think it should be, well, I don't know if I think it should be, in the IID case, it's where, with some assumptions like the Pompeii inequality, boundedness, and so on, whether this is true for Markov chains. I think in the finite state, irreducible, aperiodic case, we know that the value function is bounded, the cost both. Function is bounded. The cost per function, value function, whatever you want to call it, is bounded. So it's possible that you might be able to get a square dependence based on the earlier work of Psi for the IID random case. But it still remains open. Just take one minute to do this and I'll wrap up. So, what is the point of all of this? Since in the interest of time, I'll skip some of the slides and then show that one can use. One can use the central limit theorem for Markov chains to get a non-asymptotic central limit theorem for TD learning. And here, some of you have already asked: how do you choose the parameter delta? Now you can try to optimize delta. And then if I optimize delta, you get it to be two-thirds. Of course, it's optimizing an upper bound. I have no idea if this is the optimal thing or not. But at least it's just something. But it also complements the finite time bounds of the covariance by Happ, Kudadadi, and Sigma. Kudadadi and Sivateja this year. I think in Sivateja's result, they have a slightly different dependence where they get three-fourths. But I did some basic simulations, and it seems like it doesn't seem to matter itself. Something in that range, it at least gives you some insight into how to choose that. But this is definitely there's more work to be done. There are lower bounds here, that would really help. But I don't know if lower bounds for this. I mean, there are lower bounds for IID random variables, but I don't know if anything for market. Variables, but I don't know for anything for Markov chains. And then now for TD learning, that's going to be even more complicated. I'll just skip the slides. And hopefully, you've gotten something from this talk, even though most of you are interested in matchings. Maybe when you match, you can think of the central door on how. When I was, yeah, wrap up. So the excellence remains. So the excellent surveys by Ross. So I was wondering, you know, what should I do that's different than Ross's survey? And so I decided to focus on the Martingale and Markov chain case, which are not surveys. And there is also a, I want to give a shout out to this undergrad student who I think was a senior thesis in Toronto. She did a survey, Cindy Zach did a survey of Ross's survey. And it's got a lot of interesting detail that I've noticed. A lot of interesting details that are not in Ross's paper. So, I don't know what a proper reference is, it's still available online. So, if you just search for Cindy Zhang Stein's method, you will see it. And it's got nice, I mean, a nice, fills in the gaps in process paper. There's actually now a survey of Zank's survey of Stein's method, but that's not as simple. So, that's anything. Thank you. Our next talk starts at 5, so we do have 8 minutes in 12. So I guess people have to ask questions. Okay, questions? Or we can go outside and ask whatever. That's worth it. Yeah, yeah, so people can get on the legs. Have a quick minute and then come back at five. Yeah, I'm happy with that. But if it comes to questions, that's all. Hit the button. The survey of the survey is appropriate to call it a whirlwind to talk about