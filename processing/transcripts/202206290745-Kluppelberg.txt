Invitation to speak about my work here, so it would be nice to be with you guys in Canada, but anyway, to sit in my living room is also quite okay. So, I talk about Maxine Abayesian networks and about special properties of these networks. And this is joint work with Carlos Amendola, Stefan Lauritsen, and Nok Tran, and it has appeared in a It has appeared in a big paper in the Annals of Applied Probability. I will give you precise references later. So, and what I do is I speak about some network, and you see here a small network with four nodes. Each of the nodes represents a random variable, and then also it has four edges. And you see, for instance, so from one to four, you have two possibilities of a path from one to two to four, and from one to three to four. Four and from one to three to four. So, this should just give you an impression of what we talk about. It's, of course, a very simple example. Okay, so I want to start saying something about graphical models. So, what do we want with graphical models? We want to represent high-dimensional distributions in order to facilitate statistical analysis. So, that means we want to describe these high-dimensional distributions. These high-dimensional distributions by a careful combination of lower-dimensional factors, and these lower-dimensional factors are usually just conditional distributions. So, and the way how to do that is you use graphs as a natural data structure model for an algorithmic treatment. And what you also do with graphical models is that you use them for causal interpretation by defining. By defining them through a recursive system on a directed acyclic graph, namely a DAG. So, and this goes back to Pearl in the 1980s, and I just refer to Pearl's book in 2009. And what is really important is conditional independence and Markov properties. So, to know all this. And what I want to do in this talk is I want to present conditional independence. Conditional independence properties of Max Linear Bayesian networks. And I want to emphasize on the difference to classical Bayesian linear networks. So this is my goal here. Okay, and these linear Bayesian networks, we know all that they are based on classical linear algebra. In contrast, max linear Bayesian networks Max linear Bayesian networks are based on tropical linear algebra in the Max time semi-ring given here, and the operations are defined by the maximum operator replacing the sum and the classical multiplication as we used to know it. Now, these operations they extend coordinate-wise to R to the D and also non-negative numbers and to Negative numbers and to corresponding matrix multiplication. And well, the only interesting thing is, of course, well, this operator here. And then when you have two matrices with fitting dimensions, then you can write this topical sum here as the maximum over these things. So well all this is classic and also Anna used such a maxilinear model. Use such a MaxGrinea model. And now for Max Grinier Bayesian networks, well, the first thing to know, and that's not difficult, is that is Markov with respect to its DAG, to its directed graph. However, while the Maximo operator, as everybody knows who works with Max Sudina models, is a strange operator because it creates also deterministic effects in such a network. Effects in such a network. So, and this means it has also various consequences for conditional independence properties and also for statistical analysis of the model. Okay, so now conditional independence. So, linear graphical models, they identify conditional independence relations through separation criteria applied to a graph. So, the graph. A graph. So the graph should help you to understand conditional independence. And the standard separation criteria is given by the following definition. So it is the D-separation which plays the most important role in linear graphical models. And while two nodes I and J, they are deconnected, so they are linked together given a set K, which is a node set. Which is a node set but has not i and j in it. So if there is a path pi from j to i, such that all colliders on pay, on pi, sorry, are in k or its ancestors. So I will tell you what the collider is. And any non-collider on pi is in k. So and then for three disjoint subsets, so k. So Kd separates I and J if no pair of nodes is D connected relative to K. So if you don't understand this immediately, don't get worried. It's a complicated definition and I won't need it again. I just wanted to show it to you so that you get an idea of what is happening in a linear graphical model. Now, conditional independence properties for Conditional independence properties for max linear Bayesian networks, they are very different from those in linear Bayesian networks. In particular, they are often not faithful to their underlying DAG. So what does that mean? It means what you would like to do is you would like to see the graph. And then from this graph, and let's say the nodes you can observe, you would like to see this separation. Separation. And whereas in linear graphical models, this is often the case, well, this is much more difficult in a max linear Bayesian network. So this separation criterion here will typically not identify all valid conditional independence relations. So in contrast to most Bayesian networks. So it means we have to do something about it and we investigate it. Investigated well, what would be a good concept? Okay, so let me. What I want to do here is I want to give you three examples which explain the basic difference between max linear Bayesian networks and classical linear Bayesian networks. So, my first example is here this. Example is here this diamond. So, when which you have seen before, so we have four nodes, each representing a random variable. We have four directed edges. So, each, yeah, so each of the edges has actually coefficients. So, and now, well, we would like to understand, well, if Understand, well, if one and four are conditionally independent given the node two, so given x2. And how to see this is you simply write out the model. So we have x1 is set one, it's just the initial nodes, and x2. How do we compute this in our max linear network? Well, it is just C21 X1, maximum a new one. Maximum a new one, set two. And then, well, we have actually X3 is just the analog here, just reads similarly. So it would be C31 X1 maximum Z3. And the only interesting one is X4, which we write here as C42X2, maximum Z4, maximum C43, X3. And then we X3 and then we plug in what we know already. And what we see is we have all these independent network is these independent innovations here, set two, set one, set four, set three, set one. And now you see the only one which appears twice is set one. And now when actually, well, this factor here is greater or equal to this factor, then well, we factor then well we would have this this expression and what we see here now oh i'm sorry i'm jumping around here what we see now is that x1 is just set one x4 does not contain any uh any independent set one so it's written as x2 which we actually know here maximum set four maximum Maximum Z4, Maximum Cis. So it means that X1 and X4, given X2, are actually independent. So now this does not follow from the desparation criterion because what you see is because, well, this here, this factor here is bigger than this, that means that actually this path here does not contribute to the model. To the model at all. So, and it means that, well, in a desperation criterion, of course, the X3 would matter, but not in this Max Lina model. So that means, well, the fact that, well, we assume that this here is bigger than that means that, well, the path one to four is actually, well, doesn't matter. So it's independent. Independent, it's unimportant for the conditional independence. So, this is also true if both paths had the same value. So, and I just notice I have probably jumped over something. Let me just check if it's understandable. So, that was the conditional independence. Well, this was the network. Yeah, so I think I even missed this definition. So, let me come back. Definition. So let me come back to the definition. I think it's all clear from the example, but nevertheless. So the Max Linear Bayesian network is actually given by a DAG. So I've explained that this is a directed acyclic graph and each node represents a random variable xi. Now, and this here defines our Max Linear Bayesian network. So you have seen a MaxLinear model from already. So what All right. So, what this Bayesian network does, it looks for the parents, takes actually the maximum over all parents here, with the maximum over a new innovation, an independent one. And the nice thing is, and we will use that also, is that we get a solution for this definition. And the solution means that we have. means that we have we can express all the node variables xi in terms of the innovations set j yeah so but now instead of just looking at the maximum with the parent as i've shown you already in the diamond graph we have now here that we take the maximum over all ancestors and the node i itself yeah so So that means instead of looking at all directed edges from a parent of i to I, we now look in the solution from all set of nodes which have a directed path from J to I. And then while the solution is now given by Z, so you look over all innovations which are ancestors, and now you have different points. And now you have different coefficients, and these coefficients are actually this coefficient is a maximum taken over all the products of the coefficient along this path j to i. So, and any such path which realizes this maximum we call max weighted under the coefficient matrix C. So, I apologize for this. So, I hope all this has now become more clear than in More clearer than in the example I've given to you. So you have for the solution, you look actually over all paths which are directed from an ancestor to I, and then you take the maximum path weight here. Okay, so I think this should do it. So now, well, and this is what I have done here in this very simple tag here. So from one to four. Here. So from one to four, I have the two paths, and I take the maximum over these two path weights. And this gives me then a representation. And in this representation, given I know x2, then x1 and x4 given x2 are conditionally independent. So that means, well, the path which is not max weighted does not. Is not max weighted does not count at all in this model. Okay, so this was my first example, yeah, which shows the difference between a Max Linear Bayesian network and a Linear Bayesian network and its conditional independence properties. So my next graph is called the Cassiopeia DAG, and it has actually double colliders along a path. Along a path. Now, a collider I have mentioned already in this definition of deseparation, and here you see two colliders: namely, a collider four, which means it has an edge directed from one to four and another edge directed from two to four. So, this is a collider and the same situation: an edge directed from two to four. From two to four and an edge directed from three to five. So it means we have here another collider. So now, well, how do we understand conditional independence between random variables here given four and five? And here, well, I'm looking actually at special values four and five, so they are just numbers. So, they are just numbers. And what I actually want to convince you of is that x3 and x1 and x3, given x4 and x5, for all coefficient matrices, they are independent. And for the sake of the argument, I just said all these coefficients equal to 1. It doesn't matter. I'm just scaling factors of the C's. So now, well, x1 is set one. Well, X1 is set one and X2 is set two, X3 is set three. So X4 now, given the Max Guinea network, means X4 is the maximum of X1, X2, and an independent innovation X4. And the same for X5. So it's a maximum between Z2, Z3, and Z5. So now, what will happen? Well, what will happen? So, we have our given nodes x4, x5. And I want to recall that all set i's, I have chosen them as being continuous, so having no atoms. So it means they are all almost surely different. So, what I can tell from the model here is that x4 is greater equal to, or is equal to this maximum. is equal to this maximum and so it must be greater or equal to z4 and that means x4 is greater equal to z1 x2 is also greater equal to z1 maximum z2. So and the same or similar for x5. So it's bigger than z5 but also bigger than z2 maximum z3. Now well we have different situations. Have different situations here, namely, it depends now on the relation between x4 and x5. So, for instance, if x4 and x5 are equal, then it's pretty clear, because the sets are always all different, that, well, it cannot be that the source of x4, I would say, is set one and the source of x5 is something else, because x4 and x5. Because X4 and X5 are the same. So it means when they are the same, then this maximum must come from set two, because it's a common source for both. And similarly, you can argue when X4 is bigger than X5 or X5 is bigger than X4, then you get similar relations. So it means that actually here when X4 is X, means that actually here when x4 is bigger than x5, then x4 must be greater equal at z1, x5 bigger than z2, maximum z3, and the other way around here, and this case I have already treated. So what we see here is that, well, under the observation of x4 and x5, all the set I must be bounded in all the three cases. So sets one and set three, also when you look at this, Also, when you look at this, well, they never here occur together. So it means that x1 and x3, given x4, x5, they must be conditionally independent. And in the situation when x4 is greater than x5, then the causal source of 4 is 1, and in the other, and the other way it's reversed. Reversed, yeah, and this causal source of x5 is then two and three. And here we have then a fixed node. So you see that there is some deterministic going on in this network. So when x4 and x5 is known and the same, then already x2 must be fixed. So this is something which happens in these networks. In these networks. So, conditional independence does, on the other hand, not follow from the desparation criterion since the path from one to three, is deconnecting relative to four and five. So, this is known because these two guys are colliders. So, again, well, it means there is a profound difference in conditional independence relation in this network. So, now, well, the last example I want to show. The last example I want to show to you is actually something also very specific because here it's important that x4 and x5 has a special value, namely the value 2. So here, well, we have here what we call the tent tag. So that is this one here given. Again, x4 and x5, I know, and I even know that they are two. They are two. And now, what I give you here without telling you how this happens, I give you some ideas about that, is that this is a source tag. And you see the difference is that the 3, the value of X3, does not influence the network or the conditional independence structure in this network under the observed value 2. Observed value two. So that means you can remove these edges and this gives you then the relevant tag. Okay, so how do we do that? Again, we write out the model. So x1 is set one, x2 is set two, x3 is given by this here, x4, well similarly, and x5 here. So now all the sets are different. Now, all the sets are different. Remember this. Now we have here that x4 is set for maximum this is equal to. x5 is set five, maximum this is equal to two. So it means that x4 and x5 actually must be x1 maximum x2. So that means on the other hand while that actually well this and this they are different. So x1 maximum x2 must be equal to x2 must be equal to 2. So that is this value. And this introduces again bounds on the innovations. So we know that well, z1, z2, z4, z5 is equal to 2. And it also must hold because of this. So x3 is the maximum of z3 and 2. That means that x3 must be greater or equal to 2. And then we plug this in. And now what we see here is now. What we see here is now well the dependence of x3 you see on x1 and x2 has disappeared. It's just the maximum of 2 and z3. So it means that x3 is conditionally independent of x1, x2, given x4, x5 is 2. Now, this independence statement is then reflected by the lack of edges here in this source stack. So this is a bit more complicated. So, this is a bit more complicated example, and what I wanted to say is that, well, with these three examples, you get an idea what the difference is between conditional independence in the classical network, in a Max Grinea network. And you see also that this deterministic effects that they come into the network and they have to be taken care of. And they have to be taken care of, and they may influence also the conditional independence. Also, the desperation criterion is certainly not a good criterion in these networks. So, let me say a little bit about the summary and the conclusion. Now, as I said already, Maxineer Bayesian networks models have very different conditional independence properties. I have given three examples where the D-separation. Where the deseparation criterion fails. So, for an observed set of nodes k, a representation then of the vector of components whose indices are in k bar, so not in k, given x k equal to k guide us to find the reduced representation. I haven't shown this to you because this would have one. Because this would have gone much too far. But what you see, then, when you do this representation, you see certain deterministic features coming in and you can try to reduce this representation. Then, well, in this paper, we define a lot of different graphs to explain what happens in such a network. And in particular, we And in particular, we define impact graphs which describe how extreme events spread in these networks. And then, well, we define the union of all these impact graphs, which are compatible with the condition. And this is then the starting point for tracking possible sources of this event, of this conditioning event. But of course, it contains many. But of course, it contains many redundant things. So that means we have to clean this up for fixed and redundant nodes and redundant edges. And this leads us to this source graph where I've given you one example. And then we get a compact representation of the conditional distribution given xk. So, and well, then finally we define. And well, then finally, we define a new separation criterion, and we show in three theorems if and only if conditions for conditional independent statements. And we do this in a context free, meaning for all coefficient matrices C or for Or for certain Cs with certain inequality properties or context-dependent settings where actually the conditional random variables, the conditioning random variables have fixed values. So, and for all these settings, we formulate We formulate if and only if theorems for conditional independence. So that's quite a complicated issue and proofs are very complicated. So I certainly would not have been able to tell you this here. I apologize again that I was jumping over my most important slide. I'm sorry about that. And let me just conclude with some references. So this is the paper. So, this is the paper which I refer to. Then, well, we have also done statistics for this model. And well, there is a paper which is submitted for learning Max Linear Bayesian networks where we add a small measurement error because, of course, the model is very strict and data would never fit to such a restricted model. So, we allow for some measurement error. We allow for some measurement error, and we also prove that we find consistently the correct tree. So I want to mention Sebastian's work and collaborators. So they go in a different way, which is a bit closer to standard linear models with densities. So they look at graphical models in By defining peaks over threshold and general aspareto distributions. This is the first paper where we invented this model with Nadine, who was a student, and with Stefan Lauritsen, we started first to do estimation. Mario will actually give a talk after me. So he has also done his master's thesis about this model, and we will hear about his more recent work. Hear about his more recent work now, and these are the two books which are relevant. Okay, thank you very much.