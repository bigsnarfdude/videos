This morning, because I had to go and teach. But what we're talking about here is a kind of slightly different situation in the sense of the challenges of a slightly broader problem class where you have interpolation as part of the sort of ODE or PDE problem. And this is joint work with the student Tajo Taj Owermi and with TAJ Owermi and with Mike Kirby. And it sort of relates to some interpolation work that was done that was a long, long time ago, now 2007, 208, but builds on that actually, and has new proofs and all sorts of things. So there are a number of different cases that I'm going to look at, and they include sort of mapping from physics to dynamics in weather code. So that sort of complements the last talk. Thinking about discrete remeshing when we adopt adaptive mesh techniques. Particle methods where your particle and Where you're, you know, particle in cell methods, where you're continuously mapping between sort of particles and grids and doing sort of calculations on the grid and then mapping back to particles. An area is sort of removed as resilience in parallel computing, where we have to worry about the fact that a processor might die or cause might die. We need to rebuild the solution without doing a complete restart. And we can do that by basically keeping a kind of refined version of the solution around. Refined version of the solution around, we then have to rebuild the fine mesh from the fine mesh solution from the coarse mesh. And I'm going to give you some analysis towards the end that sort of shows you why you really do have to control these errors. And then I'm going to show you how you can fairly simply construct a simple algorithm that allows you to preserve both positivity and data boundedness. And by data-boundedness, I mean that, say, in 1D. Data boundedness, I mean that, say, in 1D, between an interval, between the values on either end of an interval, the solution doesn't go beyond those values. And by positivity, I mean that the solution on that interval is bounded by a maximum value and a minimum value that you specify. And I want to show you how you can do that to sort of arbitrary order. You know, there are plenty of examples for cubic quintix lines and so on, and lots of we know examples that sort of do this, but I want to show you how you can do it in general to arbitrary high order depending on the solution. Arbitrary high order, depending on the solution and the mesh that you're using. So, there's a whole bunch of other people that need some thanks as well. But some of this work I'm going to show you is goes back to some work with Peter Jimak and his student Phil Capon. Damodar Sahasra Budi did the work on resilience, and Alex Reineke from the Naval Research Lab has been involved in the work that we've been doing with their weather code, or with this aspect of their weather code. So, essentially, the situation is this: that we have a main integration, and we have some process involving interpolation. And we have some process involving interpolation, and we have to sort of make sure that we satisfy boundedness conditions, physical consistency, positivity, maybe, and other things too, maybe that I'm not even going to talk about today, like conservation, for example. So the idea is that we need to somehow think about the interpolation approaches that we use in order to do this. So I want to go back to a problem that was actually given to us this morning. Problem that was actually given to us this morning by David Ketchini's talk. And this is a basic oscillator problem expanded to three variables. And here I've got three variables that represent X and sigma and V, which represent sort of movement of particles, stress, and velocity. So this is a kind of analog of material point methods or particle and cell methods. And we would expect to sort of conserve energy for many of those problems with those solved with those kinds of methods. And so here we're looking at the preservation of a Hamiltonian, which is in the red box. And if you look at sort of forward Euler, obviously that doesn't preserve a Hamiltonian. But if you go to symplectic Euler, which is often using the material point method, which for those of you who've not seen it, is essentially a particle and cell applied to mechanics, solid mechanics. You get the scheme shown below. Get the scheme shown below. And if you take what happens with sort of forward Euler, which is on the right-hand side, we have a Vn of the second equation in the second equation. And on the third equation, we have a right-hand side as Vn. We replace it with a Vn plus one, we get the sort of simplest, one of the simplest symplectic methods that preserves a Hamiltonian. It's kind of only satisfactory to use low-order methods like that, and we would like to use higher-order methods. So, one of the classic ones is the Storm of Verle method that goes back. A storm of Verle method that goes back to Newton, according to Feynman, and sort of so, and maybe even Munge-Kudden-Neistron methods that would scope to sort of order four or whatever, or even beyond that. And so, the idea is that we would like to ensure conservation for this class of problems of the Hamiltonian and understand what that means for energy conservation in particle and cell methods and material point methods. So, what I'm going to do is take that simple problem and apply the Storm of Erlay method, and I'm going to add a term to it. So, the term I add in red called interperrify interpolation error is the error that you would get in a particle or cell method when you interpolate from grid to particles in this case. So that's a sort of term that sort of, you know. A sort of term that sort of you know comes into the fundamentals of the methods, and one that is often a low-order term, or could be higher-order, depending on the sort of methods that is used. So, we have these four equations with this term in red that models this error that comes in from velocity interpolation. And the really interesting question then is what that does to the accuracy of the schemes, and depending on what sort of Depending on what sort of order of magnitude that interpolation error has. And so, if we go back and look at the Hamiltonian, for example, for a whole bunch of different methods, what we'll see is that it can really destroy any point in having a higher order method if that term is sufficiently large. So, let's look at what happens with that error set to zero. And what we're looking at. To zero, and what we're looking at here are the sort of errors in the Hamiltonian as we go forward in time for the different methods. So, in blue, we have the symplectic Euler first-order method, and you can see that that's, you know, the error in the Hamiltonian is sort of bounded in some sense, but really quite large. If we go to the Storm of Erlay method, what we see is that we get much better accuracy in the Hamiltonian. And if we go to the Runge. Hamiltonian. And if we go to the Runge-Kurb-Neisdrawn method, the lowest curve, we see that the Hamiltonian is approximate to about 10 to the minus 12. So that everything looks great. So then let's start to ramp up this interpolation error and see what happens. Well, if we set that equal to the time step squared, then we get the same sort of behavior, roughly speaking, for symplectic Euler. Storm of Eurole is roughly similar, but now for the But now for the Runge-Kudder-Neisjohn method, the Hamiltonian error has dropped down to about 10 to the minus 6. So already you see an impact of that error on the calculation, and that starts to become problematic. If we go one step further and set it to the time step that's being used, 1 e to the minus 4, then what we see is that now the storm of early method and the nice drum method. Storm of Euroley method and the Neistrom method have the same error in the Hamiltonian. So there's no point going to the Runge-Kutta-Neistrom method in this situation. It's still better than symplectic Euler, so that's good, but the fourth order method really is not worth having at this point. If we go one stage further, things get worse. If we introduce the interpolation error as the square root of the time step, then Root the time step, then all three methods have the same accuracy in the Hamiltonian. So, why this is important is that in many of these particle and cell codes and the material point method, people have been trying to use higher order integration to improve the accuracy. But the problem is that the spatial error that's president has completely destroyed any chance of improvements in accuracy because the spatial error is just too large and it. Is just too large and it pollutes the time integration, and you don't see that conservation. So that sort of explains a remark that some of my colleagues made. They experimented, a couple of my colleagues in engineering experimented with a whole bunch of different methods and saw no point in going to higher order. But that was just an experimental result. This simple problem shows you what's happening. And we'll actually do some little bit of theory towards the end of the talk, which kind of illuminates that further. Okay, so I mean, that's a very simple example. I mean, that's a very simple example. Well, let's go on to weather codes. So, we've been working with the Navy for a while on their spectral element weather code, not on the main code, but on issues to do with the physics routines. And this sort of refers back to the talk that was given previously, immediately previously, in the sense that the physics routines deal with all these things like rain, sleet, hail, and all sorts of other quantities. All sorts of other quantities. And there are some very complicated physics routines that we've done some work on speeding up in the past. But the physics routines have one very sort of nasty property in that they really only work well if you have an evenly spaced mesh. Well, why is that so bad? Because you think, okay, evenly spaced mesh is fine, but the Navy wants to use high-order spectral elements. Right now, the order is not that high, it's down at about four, but the potential. About four, but the potential is there to go to much higher order. And so, what's happening is that there has to be a mapping between the mesh that's used to compute the dynamics of the weather problem and the mesh that's used to compute the physics. So, we compute the dynamics quantities on the spectral element mesh, and then we have to interpolate those to the physics routines, calculate the physics component, and then map that information back. That information back. And so this diagram sort of shows what's happening. You know, in 1D, physics values are the ones that are needed in purple, and the dynamics values are the ones that we're starting off with green. And so we have to do this interpolation. So somehow we have to make sure that the interpolation that's being done doesn't violate the kind of underlying physics that's there and things that should stay positive, stay positive, and so on. And typically, what's And typically, what's been done up to now is that very low-order methods have been used for this. The code is still in development, so it's gradually getting better. So, I'm not sure where they're at at the moment in terms of bringing in higher order methods. They've been trying to bring in what we've given them, and that process is ongoing. And in fact, if you don't preserve positivity, you get overestimation in moisture, and that sort of harks back to the same quantity. I think it's called QV. It's called QV in the previous talk, but I think QC is often used, which is the same quantity was used in the Smolakovich work as well. So, if you see that, if you look at the two forecasts here, there's one that, I mean, often the term that's used is preserving positive definiteness. And so, hence it says PD. So, the positive definite forecast on the right has less moisture than the non-positive definite forecast on the left. And so, this is a real problem. And what's often done at the moment is just And what's often done at the moment is just truncation. If you have a value that's below zero, you say it's a zero and carry on regardless, and that's not entirely satisfactory. So I mentioned particle and cell methods and the material point method, which is a sort of variant on particle and cell. And what's done in these methods is that the particles carry the physical properties of the solution. A kind of regular mesh is used as a scratch pad, as is Scratch pad, as is typical with particle and cell, to calculate accelerations and velocities. And then you sort of, you know, well, you interpolate to get to the values on the mesh. And then you calculate the change in acceleration and change in velocity on that mesh, and then interpolate the information back. And these methods are very widely used. Material point method is kind of, I guess, famous for doing the snow in Disney's. In Disney's movie, and essentially they use a lot in geotechnical in Frozen and basically in geotechnical models for all kinds of problems. And lots of different national labs use these methods for all kinds of calculations. So here's a diagram of what's happening with this method. There are some basis functions up top that up top that are used for the different mappings, and they're basically kind of like a modified linear hat function to give you a sort of To give you a sort of continuous derivative, and the sort of four steps are shown there: that you map from particles with properties on the mesh to the mesh, and then you calculate forces, acceleration, and velocities on the background mesh. And then you calculate the mesh motion, but you don't actually move the mesh. You instead pass that information back to the particles which move. And the important thing is that these particles also cross grids. So when they cross grids, there's a kind of grid crossing error. Grids, there's a kind of grid crossing error, which introduces a kind of high-order discontinuity, or maybe even not so high-order discontinuity, into the time derivatives of the underlying calculation, depending on the smoothness of the underlying basis functions used in the material point method. And the challenge that we've got is, you know, as you might imagine from the simple example that we looked at earlier, is the following one. We need to understand how the kind of errors that we introduce. How the kind of errors that we introduce through interpolation have an impact on time integration and what exactly form they take. So, if we look at the calculation, and there's a couple of sort of more complicated slides here, the first equation here is an acceleration calculation depending, so the i's are the values at grids and the p's are the values at particles. So, we essentially differentiate sort of stresses and deformation gradients using this DIP, which is differentiation from. Differentiation from sort of particles to grids. And then we essentially calculate an updated velocity at the grids. And then we use that updated velocity to kind of calculate an acceleration. Sorry, we use the acceleration at the nodes to calculate the acceleration at the particles. And then we use that new particle acceleration to update the velocity at particles. And we use the updated particle velocity dp to count up. Particle velocity Vp to update the calculation of the movement of the particles XP. At the same time, we have to calculate a spatial derivative, which is done in terms of the velocities at nodes and another mapping matrix. And the update for the stresses and the deformation gradients is in terms of this, at least for a simple problem, in terms of this velocity gradient at the particle. Velocity gradient at the particles. So that is an error that's introduced in there, and so on. So you can see there are multiple sources of errors. One error is what the acceleration is. The other error is in the sort of interpolation of the accelerations to the particles. And then we have the differentiation of the velocity values at nodes to calculate the derivatives of velocity at particles. So plenty of opportunities there for introducing error into the calculation. But not Calculation, but not formally very well understood. Although I've been publishing a lot on this and sort of got a much better understanding, and all the material that you see is in papers that are available. So if you look at the errors, you sort of see that if you, for a reasonable method like the one that we're talking about, the acceleration error is basically order delta x, where delta x is the spacing of the background grid. And then that trans that sort of comes through, that magenta turn comes through into the velocity. Magenta term comes through into the velocity error at the nodes with a sort of a time derivative error that comes from essentially the Euler type calculation. We have an error in the particle acceleration that's the interpolation of the acceleration error at nodes, plus a bunch of other stuff that we've neglected here just for simplicity. And then those errors sort of feed through into the velocity, the error of the velocity at particles and the error in the And the error in the displacement, too. So those sort of feed their way through. And if you go back and kind of substitute all of this in to the VP, what you see is that if you look at the displacement error, it has a term that comes from sort of like first order time integration, local first-order time integration error, but it also has a term that looks like the spatial interpolation of the acceleration error at the nodes. Notes. And so you've got the same order term that comes in into the calculation. And overall, even if you go from a standard symplectic Euler type method, which is using what I call the standard NPM method here, to storm of Erlay, you see the same thing with storm of Erlay in the sense that the errors in the velocity, you might have a smaller term involving the most sort of up-to-date error in the acceleration, but you Today, the error in the acceleration, but you've got two of them in some sense, so it starts to look very similar to this, and the displacement errors start to look very similar too. Now, Storm of Erle has better approximation, better properties with regard to what you might think of as the Hamiltonian or the energy is what we're interested in with the material point method. So, we would expect to see velocity and displacement errors being very similar, but better performance in terms of the energy error. Energy era. So here's the simplest possible example. It's a sort of vibrating bar example. This is one of the sort of hello world type examples for the material point method. And what we're looking at here for different cases are GIMP is the standard material point method. Storm of Eurole material point method is a sort of the better version for energy, hopefully. And so on. We're looking at different time steps: one e to the minus three, one e to the minus six. And we see that, in fact, And we see that, in fact, the theory we can prove we can prove and have proved that you would expect to see better energy conservation with Storm Le Verle is borne out numerically. So when we have a time step at one e to the minus six, essentially what we've got is a round-off energy error here for one of the cases. And this one is a slightly harder case. But if you look at the displacement errors, they're pretty much the same in both cases, a little bit of variation here. Variation here as we go to the harder problem, but the overall and a little bit of variation in terms of the sort of accuracy that you get. So clearly a problem in terms of trying to use higher order time integration with regard to these methods, because it's this interplay between space and time error that's causing the problem in a way that we've articulated. It kind of gets worse actually because if we move to a case in which we have a lot of particles crossing grids. Of particles crossing grids. So here's a really a bunch of cases that become gradually more challenging. And if we look at the sort of velocity and stress errors for Storm of Urlay and Euler and Runge-Kudden-Naistron IV, what we see is that for easy problems, Runge-Kuddenystron IV does quite well, actually. We get a better stress error than with the other methods. As the problems get more challenging and there are more grid crossings coming in, the error drops off with one of the The error drops off with Mungerkud and Istrom as opposed to Storma Verle and Euler. And for this last case, there's a lot of grid crossings, like 53,000 particles crossing grids. And each time you do that, there's a higher order discontinuity kind of time derivative error kicking in when particles cross grids. And you see that there's really not much point in using the higher order method for those cases. So let's just go on to another example. Go on to another example. This one goes takes us way back in time, but it was a sort of first point for looking at this. If you do kind of discrete remeshing where you remesh at certain time steps and move on to with the new mesh from that, if you use high order time methods, those methods often have some history information. And that information is defined obviously on the mesh spatially and sort of higher order derivatives in time. You know, higher-order derivatives in time or divided differences or whatever. And so you can interpolate the history information as well, if you want to, onto the new mesh and see if you can carry on integration with the same order of accuracy that you were using before, or same order of method, I should say. And so the graph on the right shows you three runs. One of them is what happens as you sort of restart the integration after each remeshing from first order method. Reach remeshing from first-order methods, and predictably that requires a lot more time steps to reach a certain accuracy and is a lot slower. If you do linear interpolation on the history array, then it's rather better and you can pick up a bit, but it's not until you do cubic interpolation on the history array that you really pick up, which is the left-hand curve. So the choice of interpolant matters, and again, you really want to use something like a you know a positive preserving cubic spline or something like that. You're preserving cubic spline or something like that in that case. On the same theme, I mentioned at the beginning that one of the things that you have to think about with regard to parallel architectures is resilience. There was a lot of focus on this a few years ago. It's sort of gone away for the latest generation of machines, but we think it's probably coming back for the generation after. And so, what we're worried about is situations in which cores or nodes fail. In which cores or nodes fail frequently. And the traditional approach to this is to use checkpointing, by which I mean that we essentially dump all the information at certain time levels from the integration and then restart from that time level if there's a crash just after it. One approach to kind of improving that process is to use adaptive meshing in a kind of algorithm-based fault tolerance approach. Algorithm-based fault tolerance approach. So, what we do is that we might have a fine patch shown up in the right-hand corner of the slide and a coarse patch. And so, what we do is we save the fine patch as a coarse mesh patch on another processor. So, after a 3D calculation, we've got a decomposition of kind of eight to one essentially. So, we need about 12%, 12.5% extra storage, which is reasonable. And the challenge that we've got, though, is. And the challenge that we've got, though, is that after we do this and we have a crash, then we have to rebuild the fine mesh solution using interpolation again. And for calculations that involve, say, combustion, that can be potentially problematic because what we need to worry about is whether or not the solution values, if their concentrations are positive, for example, and so on. And so we need to use a good interpolation technique that preserves the Technique that preserves the essential properties of the solution in terms of moving the solution back onto the fine mesh. So there's a diagram here which sort of shows what happens. We've essentially got four MPI ranks and essentially two of these ranks F0 and F2 fail. So what we do is we essentially recreate the solution on F0 and F3 and map back to the original nodes. So we can recover the lost data using this. Recover the lost data using this limited interpolation and coarse patches, and then we carry on execution in this way. And the examples that we've done involved sort of deliberately crashing the code very frequently to see if what we had was resilient and robust in that sense. And for the most part, it actually works pretty well. But there are some challenges that come from the different types of problems that you might solve. And as I said, combustion is a particularly tricky one because you have to worry about exceeding what you might. To worry about exceeding what you might think of as a physical range of variables. So, to show that, here's a very nasty 3D advection example with a lovely source term. And this source term is a cubic term that involves u and 1 minus u. And it has a multiplier that depends on the constant c. And what happens is if u goes out of bounds, this Goes out of bounds, this source term flips sign and blows up. So it's a really very challenging problem in this way. And so, what we've got here in the table below are the results for different values of C, and we're looking at sort of what happens of the error that you get in these cases where C goes from one down to 0.2. And what we see first are here are the values that you get with no crash and no interpolation. These are the sort of accuracies from the These are the sort of accuracies from the raw numerical method. And if we do linear interpolation, you see that we preserve that sort of process because linear is obviously sort of bounded in a very nice way. If we try and do a higher order interpolation, like using an Eno method in its original form, then for at least two of the, well, more than two of these values, actually, I shouldn't show on all the results. Essentially, the calculation blows up different values of C. And if we do this sort of limited Eno approach, And if we do this sort of limited ENO approach that I'm going to talk about later, then essentially what we get is we go back to the original accuracy that we get almost for the method in this case. So that's another example of why we need to do this. So the real question is, can we sort of, well, sorry, before we get to that, the overhead of this is pretty low. The overhead of recovery is less than 10 milliseconds. And it's about 10 times faster than checkpoint restart for the simple calculation. Um, for the simple calculations that we did and published, so and these graphs show that this is what this is a resilient, not resilient cases. You see, it's a little bit slower here. And then this is what happens if you do checkpoint restart on the right against the sort of algorithm-based fault tolerance. And that you see that essentially the wall clock time is much lower than if we're restarting all the time. And this involves a lot of restarts, as I said, a lot of failures. So, let's do a little bit of a little. So let's do a little bit of analysis on this. Let's assume a dynamics grid and a physics grid. We've got a solution UD on the dynamics grid D, and an auxiliary solution V D on P. And essentially, we're just looking at the case where we might have forward Euler time stepping, but now the function, the time derivatives, involves the auxiliary variables and the dynamics variables. And the variables that we get are sort of That we get are sort of created by some interpolation values that we have here to get to the we interpolate from these values, we do this mapping and sort of create the new values and go on in some way. And essentially, IDP of the mapping is the mapping from the grid D to P and IPD is a mapping from grid P to D. So those are the interpolation operators. And the analysis looks complicated, but it's not really. But it's not really. We just define our exact solution on the dynamics mesh. The global error on the dynamics mesh at any one time is this term in the magenta term. And the actual solution, computed solution, is the exact solution plus the error. And the exact interpolated solution is shown here. This is this V tilde. And essentially, what we get then is a kind of interpolation. A sort of interpolation error that we're looking at here. And we can expand this in terms of the difference between the two sort of equations. And we basically just expand, do a first order expansion on this function g. And what we get is essentially the error that we get in the velocity at m plus one is essentially the sort of back interpolation error applied to this kind of exact interpolation. This kind of exact interpolation applies to the exact solution minus some term that involves the Jacobian of the auxiliary function and the forward interpolation error and the interpolation of the global error that was present in the solution. So that's essentially the result. And so what we see actually is how these interpolation errors combine with interpolation of the global error and to contribute to the solution that we've got. That we've got. And, you know, essentially, if you do standard global error analysis on the forward Euler method, what we've got here is that we've got some kind of extra term that comes in from these interpolation errors. And if these terms dominate, then in some sense, the interpolation error can be greater than the local error that's present on that step and can become the dominant source of the error in the calculation. And, you know, in some sense, this situation is not so different. I mean, there was some discussion this morning. I mean, there was some discussion this morning of the Decker and Verber work, but you know, work that Jan Verber did with boundary conditions and Runge-Kutta methods, where you're introducing an extra error that comes in from the way that you treat the boundaries. And this sort of applies to the remeshing case as well, because now the P-mesh is just the new mesh that we have in the calculation. And so essentially, what we've got is some simple theory that tells us the obvious that, unless we're careful about That, unless we're careful about interpolation, it's going to pollute the calculation. To construct an arbitrary positive preserving method on this is kind of challenging, you know, arbitrary order methods and not straightforward because for one particular method, we might be able to do something. So the approach that's used here for tensor product meshes is to use kind of Eno methods without a fashion, pretty much divided differences. And so, you know, we all saw this. And so, you know, we all saw this in numerical analysis courses, and I guess you know, we all-I mean, I hated the notation at the time, it's kind of funny to be working on this. And so, we've got divided differences at the next stage in terms of the differences of the previous divided differences, and so on. And so now, what we've got is a kind of, you know, well, this is the linear case is shown here. And what we've really got to do is to choose the lowest derivative, as in an Eno method. Derivative as in an Eno method. Sorry, the smallest divided difference is in an Eno method. And the question is: what extra conditions do we need to preserve positivity and or boundedness of the solution? And what I'm going to show you is how we preserve boundedness, and the positivity proof is not so different. So, the idea is that we pick the smallest divided difference at each stage, and we stop the process at some point when. We stop the process at some point when positivity or boundedness of the solution between the two neighboring values is going to be violated. So here's an example of the simple divided difference interpolant that we might put together. And the idea is really very simple. The idea is that you successively build up a series of bounded or positivity preserving interpolants, each one from the next. So how do we go about doing that? So, how do we go about doing that? Well, essentially, what we do is we write the fundamental polynomial, the polynomial, the interpolation polynomial as between two points, xi and xi plus one, as the difference between those points multiplied by this polynomial in red, SNX. Now, SNX involves a whole bunch of sort of ratios of divided differences and sort of a kind of x minus xk e where the where the e is the edge of the. Where the E is the edge of the different stencil that you're using, and this is the XKR minus XKL is the width of the different stencil that you're using. So quite a complicated formula in some ways, but we simplify this. We refer to this term of ratios of differences as multiplied by this difference here as lambda k. And we write this in terms of a local coordinate as s minus tk, where s goes between zero and one. And tk is defined in terms of these three. And Tk is defined in terms of these three quantities here. So to get boundedness, what we need to make sure is that S is bounded between zero and one. And then for positivity, if we want this to be between zero and U max or U min and U max, we have to define bounds that say that SNX should be between U max minus UI or UI plus one minus UI of less than UI plus one minus UI minus UI divided by UI plus one minus UI. And you're assuming that UI plus one is greater than UI. Ui plus one is greater than ui. You can deal with all the different cases, it's just we have to keep it simple. And the way that we do this is pretty straightforward. We just write down the simply simplest expression for the quadratic case and do some manipulation. And here the lambda bars are the sort of multiplication of all the lambdas together, essentially. And so that tells us that the first lambda bar is bounded between these two quantities here. And so it's bounded between And so it's bounded between these values d1 and minus d1 and d1 in this case. And then we can go on to the cubic case because we note that to go to the cubic case, we just insert something after lambda 1 bar, which is this expression in blue brackets down here. And so essentially, we now have to bound this expression, which involves this one s minus t2 divided by d2, where those Values were defined in terms of these quantities here. And so now we go on to the sort of here's our cubic case. We get a similar expression here that we can again modify to get a value for lambda 2, as is done in these next two lines here. So we get this expression for the bounds on lambda 2. Now we can go on to the quartic case and we build up the polynomial in this way, successively picking the smaller. Successively picking the smallest divided difference, but applying these extra constraints on the ratios of the divided differences to limit how far we go in terms of building up a stencil. And I've shown you very quickly and sort of the basic idea, which is that it's a constructive kind of proof that relies on building up this family of interpolants. And the algorithm is just as I've said, we successfully picked stensor. I've said we successfully pick stencil points so that these inequalities are satisfied. And we can, I think I showed one of the TKs being negative. We can do the same thing if TK is positive. We try and ensure a symmetric stencil and pick the smallest values of the differences. And we can go to arbitrarily high orders, but this is stencil and solution bounded. The positive preserving proof is similar, but care is needed in picking the limits that you might read to avoid overshoots because you have to. You know, you have to hold the polynomial down a little bit. If you just allow unbounded positivity, then the algorithm that you might employ will allow overshoots because those overshoots are positive. So, here's an example for the Runge function of what we see with the PPI, which is the positive to preserving interpolation, the true solutions in black. And then the other solutions basically just truncate because the mesh points are at either side of the extremal value. The extremal value. So we can actually preserve extrema in this way quite nicely when those values are not at data points. And we, you know, we get the positive preserving method goes up to order 14 and a rounding error in terms of the accuracy that you can achieve on this very nice but often very well studied function. And if you look at the rates that you get for Runge's function with missing extrema, what you basically get is if you bound the solution, If you bound the solution, so you stay in the regime where everything is kind of flat here, what you see is only a rate of convergence that's two or two and a half as you go down, even if you use try and use higher order polynomials. If you use a positivity preserving interpolation, the convergence rate goes up to about 14 and you get down to round-off error by the time you get to 256 points with 16th order polynomials. And so this is very satisfactory. And so, this is very satisfactory in terms of being able to build up bounded missing extrema in some sense. 2D, we use a tensor product approach, and I'm going to show you examples for the sort of 2D version of Runges function before I move on to some final weather examples. So, if we go straight over to the 2D Runges function, here are some sort of early results. And what we're seeing is that the rate that we get for this function goes up to That we get for this function goes up to about 12 for the positivity preserving method on the LGL mesh that's like the one used in weather forecasting. Of the uniform mesh, we run into some difficulties in terms of probably rounding error kicking in here, although that's where the error is. The error is down at minus 15. These are some early results. So there's one feature of this that was really rather interesting. On the very coarse mesh, we didn't do better by going to higher-order polynomials. Going to higher-order polynomials because we were essentially allowing too much positivity. So, we've since refined the algorithm so that when you go to higher-order polynomials, even on the very coarse mesh, you get better accuracy. So, you have to be careful about how you increase the order in the case of problems where you're just asking for positivity and so on, because it is easy if you are not careful to start to introduce extreme again, which is the whole point of not doing this. Not doing this. So here's a weather example, and this goes back to the cloud mixing ratio that was talked about, you know, QV or QC in a previous talk. And essentially, what you're seeing is standard polynomial interpolation. And you see this blip here that's unphysical at the beginning of this peak for this simple example for atmospheric model of an atmospheric model, it's called the Bomex example. And then if you clip negative values, things get worse because the Things get worse because instead of a spurious positive bump, you get a decidedly unphysical value. And now, if you apply positivity-preserving interpolation, you get a value that is what we expect to see without the spurous values. And the same thing is if you look at the values again of QC, then essentially, if you use no limiter, you get the maximum QC to be too high, you get too much moisture. See to be too high, you get too much moisture, which goes back to the picture I showed you at the beginning, which said that if you don't do anything, you can get too much moisture being generated. And if you clip by setting things to zero, you do a little bit, well, a tiny fraction better, but not much. And if you use positive preserving interpolation, you do a lot better compared to the target value that you're expecting to see. And the total amount of moisture is still a bit larger than target value, but a lot better than you see with using either no limiter or using the clipping value. Or using the clipping values. So I'm getting close to the end. And, you know, I guess what I've tried to show you is that interpolation arises in a lot of different cases with time-dependent PDEs. And, you know, failure to conserve physical properties or to understand the effect of that interpolation error leads to extra errors. And that, you know, means that, for instance, with those particle methods, people are using much lower order methods because for the time that the interpolation The interpolation methods they're using, there isn't a lot of point. Data-bounded interpolation and positivity-preserving interpolation, a low and high order, helps to reduce these errors and they really seem to help improve stability and accuracy depending on the problem. And I should thank the Army Research Lab, Naval Research Lab, Intel, Department of Energy, through PSAP for funding many of these different projects and a lot of the work described here. Thanks very much. Describe here. Thanks very much. Thank you very much, Martin, for this very interesting talk. Any questions from the audience? In the meantime, I can ask one quick question. Ask one quick question. At the beginning, you showed us some simulations, and you mentioned that the liquid water is overestimated if the positivity preserving property is not implemented. I was expecting the other way. Does it have to do with the interpolation process? Well, it has to do with an aspect of the interpolation to do with the underlying. The with the underlying physics, yeah. And I don't know, I can't remember back to that old paper by Scamrock as to what it was. I mean, you can find the paper for sure. I mean, if you search for positive, you know, definite and weather and Scamrock, I'm sure you'll find it. But, you know, those are the results that they quoted. And, you know, you see these spurious effects coming in because of the numerical artifacts. And, you know, I mean, there are several. And you know, those are there are several sorts of issues. I mean, it's not clear that you always, there's an awful lot going on in these problems, and so many different variables, especially in the physics, and such complicated schemes that it's not always clear where the magnifiers are for those errors that are introduced in the numerics. And essentially, as I say, what they've been doing so far is basically just truncating. Basically, just truncating those values and hoping that that gives you a good solution, and it's not really satisfactory. I see. Okay, yeah, take a look at that paper. Thank you. Any other questions? Well, if not, with this, we conclude our first day at the workshop. We will resume tomorrow. We will resume tomorrow morning at 8 a.m. Mexico time. We'll see you tomorrow. Thank you much for attending.