Thank you. Thank you very much for the invitation. It's a pleasure to speak here. I found the talks very interesting and I learned a lot. And so I'm going to talk about a problem which is perhaps not in the center of the conference, but perhaps there are some parts that some of you nevertheless find interesting. Nonetheless, find interesting. All of this is joint work with Michael Loss from Georgia Tech. And there's some problem in the background that we've thought about for quite some time and which we still have not solved. So in some sense, these are all partial results towards that, hopefully. But let me try to put the whole thing before I tell you about this concrete problem. This concrete problem, let me put the whole thing a little bit into perspective. So, what we are interested in are sharp functional inequalities for vector-valided problems. Okay, so the point is that we know a lot about functional inequalities with sharp constants if we deal with scalar functions, right? Think about Sobolev inequalities, isoparametric inequalities, all the variants, all that. We know a lot, and we have lots of. We know a lot and we have lots of tools to deal with these problems. I mean, we know rearrangement inequalities, we know optimal transport, you name it, there are many techniques. And in contrast to that, as soon as we leave the scalar world, the results become very, very sparse. And what I mean in particular by non-scalar object, I mean Sobolev-type inequalities for vector fields, if you have spinners. If you have spinners or you have differential forms and so on. And so, just to give you a brief idea of what I mean by that we know functional inequalities for scalar functions, let me give you a prototypical example, and that's the ordinary Sobolev inequality. And just for the sake of concreteness, let's stick to R3. Okay, and so as we all know, the inequality says that if you have a function who is weak. Whose weak gradient is square integrable, and the function vanishes at infinity, then the function belongs to L6, and we have a bound on the six norm in terms of the two-norm of the gradient. And so what I'm interested in is the optimal constant that appears in this inequality, and that can be determined here as the explicit number. And what's perhaps a little bit more interesting than the value of this. Interesting, then the value of this constant is actually the functions for which equality is attained with that particular function. And these are these functions with the one over one plus x squared square root profile. And then, of course, the inequalities translation and dilation invariance. So you can create lots of other optimizers. Now, depending on which proof of this inequality you choose, You choose, you might, so there are proofs using rearrangement, there are proofs using optimal transport. In some of the proofs, you will exploit the conformal invariance. So that's a symmetry of the inequality that you do not see immediately, but which is actually present and which explains, for instance, why this inequality plays an important role in geometric analysis. Geometric analysis. Okay, and so by conformal invariance, I mean that if you have a Merbius transformation of R3 union infinity and you just compose your function with that Merbius transform and you pre-multiply with a power of the Jacobian of this transform, then both sides of the inequality, so both the gradient side and the L6 integral, they do not change. Okay, that's Change. Okay, that's the conformal invariance. And as I said, I mean, that's both for proving inequality very useful and also when you want to apply the inequality. Anyway, so this was about the scalar setting. Now let's move on. Let's talk about the vectorial setting. And so here's an inequality that's a conjecture. We don't know whether this is true. This is an inequality about vector fields. Okay. And let's stick again. Let's stick to R3. That sticks to R3. So we have functions from R3 to R3. And what we want to assume is that the curl of such a vector field belongs to L3 halves. And then it's... I mean, the idea is then you can control the three-norm of the function. Now, that can't be quite right because, of course, if your vector field is a gradient field, Field is a gradient field, then the left side of the inequality is equal to zero. So in order to have an inequality that is possibly valid, you have to remove all the gradient contributions from the right side. And so the most natural way is simply to take the infimum overall gradient. Okay, and so the fact that such an inequality holds with some constant, that's not hard to prove. Not hard to prove. What the conjecture is about is about this value of the constant, plus that we say that there are a's of a very particular form for which equality is attained. All right, so that's what the conjecture is about, about this precise value, and about, I mean, that's essentially equivalent, having this characterization of optimizers. Of optimizers. And you see again this characteristic profile, the 1 over 1 plus x squared to some power appearing in the inequality, which is always a sign of conformal invariance, simply because, I mean, this appears as the Jacobian of the stereographic projection if you map it to the sphere. And so that's a quite remarkable thing about this inequality. See, if you think about the ordinary Sobolev inequality for functions, Inequality for functions with gradients in L three halves. This is one where Ober and Talenti computed the Schop constant. That inequality is not conformally invariant. However, when you look at this vector-valued inequality, then you do have conformally invariant. So you have this additional symmetry group, which should somehow help you finding these optimizers and Optimizers, and it hopefully also makes the inequality useful in some geometric sense. When I'm saying that we believe that these functions are the optimizers, well, I mean, the simple thing that we did and we can do is we write down the Euler-Lagrange equation for the inequality and we check that these functions are solutions of the Euler-Lagrange equation. So that's at least a little bit. Equation. So that's at least a little bit of evidence. There's some more indirect evidence, but I don't want to talk about this because, anyway, it's open. So, I mean, I will not be able to prove that. I want to emphasize this. And I should also say that, I mean, I've written all this down in three dimensions, but at least in odd dimensions, there's a similar conjecture. The result I want to talk about today with you is a proof that. That there is an optimizer for this inequality. Okay, so meaning we look at the corresponding minimization problem. So we're fixing the right side, say, to be one, and then we want to make the three halves norm of the curl as small as possible. And not only does there exist an optimizer for this minimization problem, we also prove that any minimizing sequence Any minimizing sequence is relatively compact in the natural norm up to symmetries of the problem. And I mean, the symmetries of the problem are translations, dilations, and then these gauge transformations. All right, so it's about the existence of mini-masses, and I will tell you a little bit more about this. Let me now go to the problem that's sort of really in the back of our minds. In the back of our minds, and this is a problem that's motivated by mathematical physics. Again, I will be brief about the motivations, but let me at least mention this. So, here's a certain equation down there, right? Okay, that's a spinner equation that's posed on all of R3. By spinners, I just mean that psi has values in C2. Okay, it's a function defined on R3 with values in C2. Nor three with values in C2. Now there are these sigma matrices appearing. These are two by two matrices that I've written down explicitly up there. And then when you see sigma dot a vector with three components, then I mean just that you multiply the first of the Pauli matrices with the first entry of the vector plus the second matrix times the second entry of the vector and so on. Okay, so in that sense and then this object So, in that sense, and then this object is called the Dirac operator. Okay. And so, you have here a coefficient, this A, which is this vector field from the previous slide. And so, the question is, look at this equation and look at solutions, psi, which are non-trivial. In order, and which are sort of small at infinity. In order to have solutions for these equations, non-trivial solutions, A needs. Non-trivial solutions, A needs to be big in some sense. And the good way to measure the size of A is that it has to have a substantial, its curl needs to have a substantial three-half sign. Okay? I mean, put differently. So if the curl of A is too small, then this equation does not have a non-trivial solution. And what the problem is about And what the problem is about is what is the smallest three-half norm of the curl of A such that there's a non-trivial solution psi? That's the problem. And I hope that you believe me that this equation is important. It really arises in some physical context, and it would be very nice to find that number, that minimal number. Okay, and let me leave it at that. Let me leave it at that to justify this. I should emphasize that just like the problem we looked at previously, this is a conformally invariant problem. Okay. And then I should also mention that Loss and Yao, this is H.T. Yao, in 1986, they constructed solutions. So they constructed a pair of psi and A such that you have this satisfied. This was Satisfied. This was somewhat of a surprise back then. And these are the A actually that appears in the Law Ciao construction is exactly the A that we believe to be the optimized industic quality that I mentioned before. And these solutions, when you look at them, they seem to have a lot of geometric content, perhaps even topological content, and they are related. Content and they are related in particular to hop fabric. If you compute now, the three halves num, remember this is the thing that we want to make as small as possible, then you get four times s3. So for that value, you certainly have a solution. And what we could show is last year is that if we have a solution, then we're at least at 2s. Least at 2s3, which is not opt. So, the question is: probably we believe that 4 times S3 is the optimal lower bound. So, our bound is off by a factor of 2. And to make this story short, if this conjecture on the previous slide would be true, we would actually get the lower bound with 4 times S3 instead of 2 times S3. I mean, that's just one motivation. That's just one motivation. That I'm just trying to explain to you how we arrived at this conjecture. But I think by itself, the conjecture, if you're interested in functional inequalities, sharp functional inequalities, then the conjecture by itself is also interesting. This is just one of the possible applications. And then, so I mean, this is now, I've told you two problems that we cannot solve. There was this little progress that we made on one, namely saying that there is an optimizer for this. Is an optimizer for this Sobolev inequality for vector fields. And if you use the very same techniques that we're using there, one can show that there is an optimal A solving this problem. Okay, so there is really an optimal A. By optimal A, I mean there is a particular A for which this three-half snow is smallest. And but anyway, so that's our background, but now Now you can forget it again. So let's go back to where I started on the previous slide. This is our minimization problem. Fixing the three norm of A in this gauge invariant form to be equal to one, we want to make the three halves norm of the curl as small as possible. Now, and we claim that one can do this. That one can do this. There is an optimal vector field for that problem. Now, in general, when you think back, where was I here? For the ordinary Sobolev inequality in R3, once you have shown the existence of an optimizer, you're very happy. Because now you can use all the tools that you have in your toolbox. Say you use a symmetric decreasing rearrangement or you use the method of moving planes. Method of moving planes, you get that any minimizer or any positive solution of the equation is spherically symmetric. Once you know that they're spherically symmetric, well, you're dealing with an ODE. In fact, you can make this in an autonomous ODE. You can write down all the solutions and you're happy. You have proved the optimal inequality. So, for scalar problems, the existence of a minimizer essentially gives you the sharp inequality. Now, Now, we don't know how to use any of this in the vectorial case. Okay, so for that reason, this is really a small step. But it is at least a step and now we can try to work around it. There's something for the future. Now, there are many results about the existence of optimizers. So when you see this for the first time, probably you don't find this very surprising. You don't find this very surprising or interesting, and well, it's just another one of these theorems. That's what we thought, too, definitely. But it was actually surprisingly hard to prove it. And so, what I want to tell you is that all these standard methods that you have in order to prove existence of minimizers, they all have a problem that does not, that it doesn't really work. Okay, and so it's a combination of the quasi-linearity, right? I mean, that you have a three-half. Right, I mean that you have a three-halves power here, and actually later on, we see also the three creates problems. There's a certain non-locality that's because for every A, you have to find this optimal gauge chi. But you know, when you superpose two A's with disjoint support, then the chi for the sum is not necessarily the sum of the chi's. That's what I mean by non-locality here. That's what I mean by non-locality here, and then there's the vector-valuedness that's inherent in the problem. And the combination of these three things really make all the standard methods not work. So perhaps the nicest method, the most elegant method to prove existence of optimizers is Lieb's method of the missing mass. But that method really wants you to work with in a Hilbert space. I mean, the dominant term should be in a Hilbert Hilbert. I mean, the dominant term should be in a Hilbert space, or you need extra information, which we don't have here, so that does not work. Jung's concentration compactness principle, that wants very much that this, the weaker norm, has a certain locality property. And then there is a method that was originally suggested by Yamabe and then used by Trudinger, and that consists in supercritical applications. Consists in supercritical approximations. See this power three here, that's of course critical, right? So we'll have critical. So what Yamabi and Trudinger did, they somehow used similar variational problems where you make this power subcritical, then you prove L infinity bounds, and then you have enough compactness to pass to the limit. But of course, this proving boundedness of these subcritical minimizers. Of these subcritical minimizers is not so easy because of the vector values. Okay, so these were the three standard methods. Now, we found somehow by coincidence a paper by Garthia Peral about scalar quasi-linear problems. And there's a little remark there, about half a page long, where they, I mean, in their problem, any of these three methods would work. But they have a little half a page paragraph where they explain. A page paragraph where they explain how one could prove differently the existence of optimizers. It turns out that this strategy essentially can be made work in our problem if one supplies additional ingredients. And I will tell you more about those. But let's step back a little bit. So, how does one prove the existence of minimizers or relative compactness of minimizing sequences? Minimizing sequences. And any proof that I know, also the proofs that we will be using, work always in two separate steps. The first step is we want to show that there is something somewhere. And the second step is we want to show that there is nothing else anywhere else. What do I mean by this? So these problems, right? The problems are, I mean, of course, you take a minimizing sequence and you want to pass to a weakly conversion sub-sequence. weekly conversion subsequence. The problem that you have in these kinds of problems is always that the weak limit can be easily equal to zero, just because the problem is invariant under translations and dilations. When I say I want to find something somewhere, I say I want to find a sub-sequence that converges to up modulosymmetries to a limit that is not identically zero. Okay, so in the limit, I want to find something. Limit, I want to find something. And then to say that there is nothing else anywhere else, I want to say that we didn't lose any mass, any norm when passing to the limit, right? A weak limit could have lost norm. But that typically would mean that there's something else somewhere else. And so, what you want to show in the second step is all the stuff stays together. Stuff stays together. And that's in particular, I mean, hard. The second step is hard because of this non-local dependence. But I'll talk about this a little bit later. Okay, so let's talk about these two steps. Okay, we're in the first step right now. We want to show there's something somewhere, meaning we want to show that a minimizing sequence has a weak limit, which is not identically zero. Limit which is not identically zero. Okay, and so how do you do this? An approach that was in the scalar setting advertised by Patrick Charin, Mayen Uru, was to prove so-called improved Sobolev inequalities. So that's roughly speaking the same Sobo-Lev inequality we had before, right? You recognize the three norm here, which is gauge correct. Which is gauge corrected. And you recognize the three-halves norm of the curl. But then you see there's another term coming. And so on the right side, we have a geometric mean of two quantities. And if you like Besov spaces, you recognize this as a certain Besov norm. The point is that this Besov norm is bounded Is bounded by the corresponding three-halves normal. So meaning this term here, the second term on the right side, is bounded by the first factor in this geometric mean. All right, that's just Hölder's inequality, right? We know that this is the heat kernel. This is integration against the Gaussian. You just, I mean, do a Holder, three halves in three, put the kernel in L3, compute the three-norm of the kernel, find that this exactly cancels the T. Find that this exactly cancels the t, and that's it. So, this is a triviality. Okay, but the point is really that because we have here this weaker quantity on the right side, therefore this is very important for us in order to show that there's a non-trivial weak limit. That's the statement that I wrote down there as a corollary. So, if we have a minimizing sequence, right, so we normalize it in L3 to B1. L3 to be one. We assume that this we have sum of the curl does not go to infinity. And then what this says is I can find translations and dilations such that if I translate and dilate my minimizing sequence, then I have a weak limit which is not identically equal to zero. Just a little Just a little two words about how this comes about, right? So the assumption is that this is bounded from below. This term here is bounded from above. So that means that this Besov norm here is bounded from below. But that means, see, what you have here is you have a double soup, right? You have a supremum over T and you have a supremum over space. And you have a supremum of a space. And these are exactly the two, the dilations. The T is the dilations. If you think how the T enters into the heat kernel, that's by dilations, right? And I mean, this is just over translations of the heat kernel. So knowing that this supremum is bounded away from zero tells you that you can rescale and re-translate such that a certain integral is bounded away from zero. The integral is bounded away from zero, and that's by definition of weak limit. This tells you that the weak limit is non-zero. Anyway, so that was the first ingredient. And I mean, this can be done in a rather similar way to what you do in the scalar setting. What's perhaps more interesting or harder, certainly, is to do the second step. To do the second step. Okay, so there's nothing else anywhere else. That's what we want to prove. So we found now our weak limit, which is non-zero, and we want to show that along the way, we didn't lose any mass. The norm is still the same. And so the first step, I mean, in this second step, the first step is to apply Eclawn's variational principle. And Eclon's variational principle, that's a little bit similar to what Yamabi and Trudinger wanted by going to a subcritical approximation. They wanted to get an equation. Okay, so that's our equation now. So we can, by applying this Eclon's variational principle, we can now assume that our optimizing sequence really solves an equation. And the equation is almost the Lagrange equation. The Lagrange equation of our problem, except that there's a small right-hand side, but the right-hand side tends to zero strongly in L3. So that's really not very important. All right, good. And now what do we want to do? Well, so let's pick a weak limit of A. Okay. Normalized in those ways. What we want to do is. Those ways. What we want to do is we want to pass to the limit in the equation. Meaning, wherever you find, see an a n here, you want to put an a. So that's the equation that we want to prove. And of course, the difficulty here is that we want to pass to the limit for a non-linear functional. Weak convergence is nice when you deal with linear expressions. Nice when you deal with linear expressions, but you have a problem when you deal with non-linear expressions. Rupert, I'm sorry, I think that part of the screen is again covered. Maybe you touched something. I didn't. Let me go to another slide. So I'll go back and I'll go now. It's fine. That's strange. Okay. I'm sorry. Right, so what I'm telling you: so the we want to pass to the limit in the equation, and that's always a problem when you only have weak convergence because the expressions are non-linear and non-linearities don't like weak conversions. So you need something extra. And what I ask you to believe is that if you have this equation for the limit, then you're done. Okay, how do you see this essential? Okay, how do you see this? Essentially, you multiply the equation by a and integrate, and then you, because this Lagrange multiply is actually the optimal constant, then you get a relation between the norms, which can only be satisfied if you're an optimizer. So just believe me, that what we want to do is we want to pass the limit in the equation. And so the equation has two pieces, right? There's the first piece which involves... There's the first piece which involves gradients, and there's a second piece that is a point-wise nonlinearity. And both passing to the limit in each of those is non-trivial. And so there is, however, for the gradient term, there is an existing technique which goes back to Bocardo Murat, and then in the case of system Dalton. And then, in the case of system Dalmasse-Mova, where they, under these assumptions that I've written up here, you get convergence in LP for any subcritical LP, which is enough. Okay, so that's a truncation method. And it's nice, it doesn't quite, I mean, even though this is for system, it's for a special class of systems and our curl. Our curl system doesn't fit there, and you have to do some games, but I don't want to go into that. I mean, it's a truncation method. The thing that I want to tell you a little bit about is how to pass to the limit in this term, which might look very, very innocent. But anyway, here's the proposition that we prove. So we have a sequence of vector fields, a n, which belongs to L3. Which belongs to L3, and we want to assume that the curls converge weakly in L3 halves. It's exactly the situation that we have. And now we want to have a side condition. Now, the side condition is that the divergence of absolute a times a is equal to zero. Now, where does this come from? See, this was our normalization, right? Remember, the normalization is that the three-halves norm in this gauge correct. norm in this gauge corrected way is equal to one so now what i'm saying is i want to choose my optimal chi there is an optimal chi by convexity and i just replace a n by an minus the gradient of this optimal chi and then the euler lagrange equation of this thing right you vary with respect to chi that is this equation Okay, and that's the constraint that we have. See, sort of the curl alone is not good enough. You want to have, in addition, some divergence information. But the problem, that's what I want to emphasize, is that this is a non-linear divergence condition. It's not the divergence condition on A alone, but on absolute A times. Alone, but on absolute a times a. But nevertheless, the conclusion is that then converges to a in LQ locally, where Q is any subcritical power. Okay, so that should remind you very much of the Rayleigh-Kondachov theorem. And in fact, if we had the condition that just the divergence of A would be equal to zero, then it would be the standard Rayleigh-Kondachev lemma, right? The standard Ray-Kondrichov lemma, right? Because we know by Helmholtz decomposition that the whatever three-halves norm of the curl plus the three-halves norm of the divergence together are as good as the gradient. So if we have information about the three-halves norm, and if we had that the divergence was equal to zero of A, then we could just apply it. I mean, for for the, we could just apply the ordinary. For the, we could just apply the ordinary Relikondrichov lemma, just, I mean, component-wise, if you want. Okay, but we don't have this, we have this non-linear constraint. That makes life quite a bit harder. And so, first of all, there's a little technical thing that we want to, we can only prove it on the sphere. That, I mean, that's at this point we use. That I mean, that's at this point we use conformal invariance, but there should be a different way, anyway. So, it's the same thing now written on the sphere, and instead of talking about vector fields, I talk now about one forms. Okay, so in D is instead of curl, and you know, these are instead of divergence, but it's the same thing. Okay, you have strong convergence in subcritical spaces. And now, the thing that all this relies on is Where all this relies on is the following lemma. And that's the very last thing I do in my talk. But this is now really where a P-Laplace equation appears. So the setting is the following. I told you before, we would be happy if we had a constraint on the divergence of A, not this non-linear, really the divergence of A. So what we do is we write our A as one A which is divergence free, but has the same curl. But has the same curl plus a remainder, and the remainder is a gradient. That's what I've written over here. So you should think of psi as the guys that we know. These are the guys for which we know the Rayleigh-Kondachov theorem. Okay, and then the d phi1, this is the correction to it. And so read this equation, this like a P-Laplace equation, where p is equal to 3 and we're in dimension. To three, and we're in dimension three. Okay, so once again, so we change the gauge to the good guys, and then we correct it. And so, what we know about the correction, that's the d phi, is that they satisfy this P-Laplace equation. And now we have actually two such guys. I mean, think that these, so the xi is the good sequence about which we know convergence. And so. And so, the question then we solve: we get two different phi's. And what we are asking is: if the size are close to each other, then the phi's or the gradients should be close to each other. Okay, and that's what this theorem tells us. Now, what's the difficulty? This is a P-laplace equation with p equal to 3. For Rayleigh Kondratov, we want to go below 3. Below three. But that's exactly, I mean, where you get problems when you work with P Laplace equations when you go below P. Okay, and so what are the problems that we're having? Well, so I mean, if, so these two guys are close, so this is a small quantity, that's fine, okay? But the difficulty is that this additional term here, this is not small. I mean, when psi one is very, very close to psi two. To psi two, then this second term is not necessarily small. This second term just tells you in which, I mean, how far we are below three. But it doesn't, if you think about the size as elements in a sequence, it doesn't get smaller as you go along the sequence. Right? So, you have to deal with a problem where the remainder gets small as you change the exponent. But that can be overcome by some diagonal argument. And so you're left with proving this lemma. We use some ideas of Ivaniech and collaborators. He looked at different equations, P-Laplace equations, where you have changes in the right side. So it's sort of stability of solutions of P-Laplace equations. We have them sort of inside. Have them sort of inside, but we could apply some of his techniques, and then finally we could prove this bound. Anyway, I think my time is up. I thank you for your attention. Thank you. Thank you, Rupert, for a nice talk. So are there any questions or comments? And yeah, I have some comments because this remains. Because this reminds me several things that I've done some parts of the past, because essentially these are problems of passing to the limit in the nonlinearity in non-linear equations. So weak convergence is not enough. And therefore, the trick, okay, what I would like to say is the following, is that there are actually two dual approaches. The approaches you are using here is the one below the limiting exponent. Limiting exponent. So you have to go below, and then you have to use this, which is so-called stability of non-linear Hodge decomposition, that you can do with making a small perturbation around the critical exponent. And then you can combine with this truncation lemmas. This I know because I also applied in some limiting equations involving a P-Laplace. And then you go a bit below, and in some sense, you go below the limiting exponent, and you recover some extra equi-integrability that allows you to pass to. Integrability that allows you to pass to the limit. But there could be another approach. In fact, what you're dealing with are systems which are in some sense elliptic because they are elliptic outside this kernel of the curl. They have been studied. And essentially, and then to allow for this, you get Equelan's variational principle to get an equation. So an equation involves more information. Equation involves more information, but it also involves another thing, which is so-called higher integrability. Sometimes, if the, I mean, you could also see if this could work. I mean, you get this kind of system that comes from Meckelon variational principle. So, you pass from a minimizing sequence to a sequence of real minimizes. Then you get an equation, but then you could wonder if you get some X. Could wonder if you get some extra higher integral bit in the spirit of Going's lemma, which is the dual of what you're doing. What you're doing is you go a bit below, and then you do all the estimates below the natural growth exponent. That's what you're doing. And therefore, this is artificial. So if you go a bit below the limiting exponent, you create artificially some extra high equintegrability that allows you to pass the limit. Now, my question is: would why not? Why not having also try in the dual way? So, this you get this system that comes essentially after Egland's variational principle. And this system is essentially, what is this system? It's a Pilaplacian system involving a curl. Therefore, it becomes elliptic if you project on the curl. There are results. And then why not try if Goering's lemma, the real Goering's lemma applies? Because what Divanians does is essentially. Tivanietz does is essentially proving the dual of Goering's lemma. So you go a bit below and then you go up again. And with Goering's lemma, you recover this extra higher integrability that you emulate via this truncation selection of Bocca d'Omura, which is essentially finding the good cuts where you get extra higher integrability. Yes. So since you get a system, it would be a system it would it would be a uh a try to get um the other the the upper the upper way if there's of course this might fail if the system is critical but if it's critical then shouldn't work also the other the other regime um yeah so we we have not tried to get higher integrability i understand that this is um because if you Because if you go essentially, what you're doing, you pass through a system and then you go to estimates below the natural growth exponent. But philosophically speaking, if you go into the proof of this nonlinear Hodge decomposition stability, then you see that the ideas are the same ones of Goering's lambda. There's a little tilting around the limiting exponent. And therefore, if you go a bit down, every time you can go a bit down, you can also be up. And the other way is doing. Yeah, and the other way is doing. In fact, I think that if you look at these Ivaniet's papers that actually rely on Maya's estimates ideas, then you can tilt a bit around the exponent. Then every time you can go down, you can go up and so forth. I mean, it's a possible alternative. Yeah, it would be interesting to explore that. I mean, the whole thing is critical. I mean, the exponents are critical. I mean the exponents are critical. So I mean one now when you are in the critical range when you are in the critical range usually higher integrity fails but also but also it fail but also it fails if you go down with this method. So this must be that you have to exploit it in some sense at some stage some smallness that gets you converged. Yes. Yes. Yeah. Thank you for the suggestion. Thank you for the suggestion. I'm happy to look into this. Essentially, what you're doing, what you're doing, you are using. So, after minimization, then you do Egeland's variational principle. You get a system which is on the left-hand side critical up to the kernel of the curl. And these systems have been studied with a right-hand side, which has some critical features. At this stage, you decide to go down. You decide to go down and to pass into the limits using this truncation lemmas. And another possible way is to use maximal operators. Use maximal operators and to do truncations via maximal operators, because in this sense, this is yet another approach. So you get a limiting sequence, then you get the maximal operator of the gradients. In this sense, you should get the maximal operator of nabla. Of NABLA wedge an. Then you make a truncation of this, and then you replace with the sequence of lip sheets, of lip sheets functions with controlled integrability. You can also look in an old paper by Frank Duser and myself, where we do something called the pyarmonic approximation. Okay. And it's postpone this a bit. But it's just that this reminds me because no, no, I just want to talk because of the time. I understand that the kind of problems they are studying is from functional inequalities, but then when you jump to these things, there's a whole knowledge that can be injected to them. Yes. Okay, so if you don't mind, I would stop sharing it. Thank Rupert again and stop the recording. And stop the recording.