Also thanks and Yang Yuan for organizing the meeting. I think it has been great so far. So yeah, thanks a lot. So I'm going to talk about censored covariance. That's quite obvious. But in a context that has not been discussed yet, it's the context of quantile regression. So I will start first with explaining what is quantile regression, because I'm not sure everyone is familiar with it. Sure, everyone is familiar with it. And then I will go to the case where we have sensors responses. And once that is clear, then I will move to the more complicated context of sensory covariates. And then, of course, some conclusions at the end. So, what is quantile regression? So, I guess you're all, or I'm pretty sure you're all familiar with mean regression, where you try to model the conditional mean. The conditional mean of the response to the covariates. In the case of quantile regression, we want to model quantiles. And why do we want to model quantiles? Well, there are several reasons. The first one is that quantiles are more robust to outliers, so they might be more appropriate in certain contexts. And also quantiles, if you take several quantile levels, you can have a better idea of the whole distribution. Of the whole distribution of your response, whereas if you look at the mean, you only look at the center. So it's more restrictive than using quantiles. And so, yeah, that's the reason why sometimes in certain contexts, especially in survival, people prefer quantiles instead of mean. And why do I say especially in survival? Because, well, we know that in survival, if you work in a non-parametric context, it's hard to Context, it's hard to estimate means. Okay, and so if you think of quantiles, then the first example that always comes into my mind is the example of growth charts of babies, right? Because these are contact curves. So here we have the age of the baby, and then on the vertical axis we have the length. And then, yeah, I guess you're all familiar with these curves where you can look for a given. Curves where you can look for a given age, what quantile the length of or the height of your baby corresponds to. And so these curves are really quantile curves. That's the kind of thing that we want to estimate, but now in the presence of sensors. Okay, so here is a quick comparison of quantal regression and mean regression. So we will suppose some linear regression model, but it Repression model, but instead of assuming that the mean of the residual or the error, even the covariance, is zero, we will assume that a certain quantile is zero. And typically, if you think of estimating the regression coefficients, then in mean regression, the most natural loss function will be the quadratic function, and for quantile regression, the loss function is this function. The loss function is this function. It looks a bit weird, but if you make a plot of it, it looks like that. And it's called the check function, and the reason is because it really looks like a check. So that's a quite natural name. And so you see that it's doing a bit the same as the quadratic function. So it's kind of a concave, no convex. A concave or no convex function that has a minimum in zero, but it puts different masses, and that's why you get quantals instead of heats. And so this has been proposed a long time ago by Conquer and Bassett. So that's really the first paper on this check function approach for quantal regression. Yes? And you set the tau before you start getting the regression model. Yes. So the tau is not a parameter that you estimate. A parameter that you estimate, it's something that you specify. Okay, so now we come to the link between this check function approach and the asymmetric Laplace distribution. At first it might look like these are two different things, but actually they're very much linked to each other. Now, first of all, what is the asymmetric Laplace? It's this density. And so you see that it's defined. And so you see that it's defined again depending on that check function rule. And so instead of minimizing the sum of this check loss function, you can also say that you maximize the product of this asymmetric Laplace densities. Because yeah, it's a monotone function of this rho function, so minimizing the rho is equivalent. Minimizing the row is equivalent to maximizing density. And so that shows that you can actually view quantile regression as a maximum likelihood problem, where you maximize this asymmetric Laplace likelihood. And that's nice because then you're really framing your estimation problem as a likelihood problem. And that's something that we will use when we go to sensor data because for sensor data, we know how to correct the likelihood. And so we will use that expression of the likelihood in order to estimate competence. Okay, so here is then this density, this asymmetric Laplace density in black for tsau equal 0.5. Taur equals 0.5, so that's for the median, and then the first quartile tau equal to 0.25, that's the red curve. And so you see, if you take the black one, it's like you put two exponential densities together. And so this leads to a symmetric Laplace, whereas for all other values of tor it will be asymmetric. And so you give more weight to large. Weight to large values or to small values, depending on which value of soil you take. Okay, so far so good. All this is now for uncensored data. So the question that we are going to ask now is what happens if either the response or the covariance are censored. So can we still use that link between the check function and this asymmetric? And this asymmetric lock was distribution. And unfortunately, the answer is going to be no. So this equivalence breaks down. You cannot just use the asymmetric ones. And that's what I'm going to explain now. So if you consider first sensor responses. And sensor responses, it can either mean that we have interval sensoring. Interval sensoring or right sensoring. It doesn't really matter that much. Here I'm writing down the likelihoods in case of interval sensoring. So basically what it does is that instead of observing the response, you observe an interval to which you know that the response belongs. And then you can show under certain assumptions of non-informative censoring that your likelihood is equal to. That your likelihood is equal to what Lupe called yesterday the simplified likelihood. So it's the difference between the distribution in your right endpoint and your left endpoint. So a naive and simple idea could be to work with that likelihood and to use here this asymmetric Laplace distribution. Solution in a similar way as we did it for uncensored data. And this actually has been done in a paper, this one, Botai and Zhang in 2010. They did it for right sensor data. They also worked with the corresponding likelihood for right sensoring, and they just plugged in the asymmetric Laplace distribution. It's a simple idea, but it does not lead. But it's it does not lead to a consistent destination. But you cannot do that. And the intuition behind the reason why this doesn't work is that when you have sensoring, you don't use the density, but you use the distribution. And if you use the distribution, then it matters really where you put the mouse. So if you have censoring, then you can. If you have sensoring, then you cannot just replace the check by the asymmetric Laplace, because the shape of this asymmetric Laplace is going to influence your distribution, and it doesn't work if you have sensory. So, yeah, as I wrote here, the probability mass is redistributed in the interval according to the ALD, so the asymmetric Laplace distribution, and that is in general not a true distribution. And that's the reason why it doesn't work. The reason why it doesn't work. And so, actually, Roger Kanker noticed that this is actually not correct. And so he wrote a letter to the editor, and it was published right after the original act. Okay, so this is nice and it's simple, but unfortunately it doesn't work. So we have to do something. And so what we will... And so, what we will suggest as a solution is that we use a kind of sieve approximation of the true density that will be based on this asymmetric Laplace, but we add or we enrich this asymmetric Laplace in a way that it becomes a CF approximation. And so, with the CF, it means that we have a growing number of parameters, and indeed, we will show. And indeed, we will show that if the number of parameters grows to infinity, then we can approximate any density by a density in that form. Now, since we are in the context of a quantile regression, we have to do it in a way such that the thought quantile of that new distribution still has to be zero. By construction, it has to be zero. So, the question is: how do you construct? So, the question is: how do you construct this C approximation or this C family in a way that still guarantees this quantile to be C. Okay, so here is now the idea of how we do it. So, here is our model. So, it's a linear model in the covariates, and then we also have some possible heteroscedasticity. Possible hit wasched. And so the quantile of epsilon is zero, but we don't specify the distribution of epsilon. It can be anything, but we know that the quantile is zero. And so the approximation of the density of epsilon should also satisfy that constraint. Okay, so um under that model you can write the density of y given z in uh this way. In this way. And so that's true in general. And if you then assume the asymmetric Laplace, then this density of epsilon has this form. So where does that come from? Well, it comes from this s from this where is it? Here, from the asymmetric clock plus. You can write this in another way by using the definition of rho. By using the definition of rho. Remember, the rho was this thing here. So when you have negative values, you have here to minus one. If you have positive values, you just have to. So that's why when you write it in full, you have this. So for negative and for positive values, you have different exponents for the exponential function. Anyway. For the exponential function. Anyway, that's another way of writing this asymmetric. What? I might have missed this. Why enriched? What makes this enriched? That's coming now. I have another question. So when you're assuming that epsilon is independent of that, maybe you say something about it because you didn't do that before. No, that's right. So before it didn't matter. So if you have uncensored data, you don't have to specify it. You don't have to specify it, it always works even if you don't have this independence. And what we will do now, it will matter. So we will have to assume that we have a location scale. Okay, so now comes the enrichment. So instead of working with this simple density, we are going to add something. And here is what we add: the thing in pink. Pink. And what is this thing in pink? It's a square of a linear combination of Laguerre polynomials. And what are Liguero polynomials? Well, it's a certain type of polynomial which has this shape or this formula. So you see that we add certain polynomials, let's say, of a certain degree, m. Certain degree m and m tilde, and they depend on certain parameters, theta. And these thetas, they have to have more than one, you know, otherwise you don't have a density. So the trick is going to be that these polynomials, they are going to make the family much richer, and rich in the sense that we show that any continuous density. Continuous density can be approximated by densities of this family if you let m and m tilde go to infinity. Okay, so once you have this enriched Laplace density, which is still on the level of the epsilon, you can now go to the boats. The boats. And so again, we use this formula of the density of y divided, which is, thanks to this vocational scale structure, equal to this. And so we plug in now this enriched Laplace here. Okay, so at the end we have a family, which of course depends on the regression parameters that we are interested in. We also have the gamma. We also have the gammas, and then we have these thetas that are coming from these enriched labels. Okay, and now we do whatever you would do with sensor data. So you can use the likelihoods for right-sensor responses or for interval sensor responses, and you use this formula of the density. The density to maximize that. So that leads then to estimators of all the parameters in this family, and in particular, also for the betas in which we are interested. It is completely parametric and right. If you fix the value of any. If you fix the value of m and tilde. Because these thetas, they are a vector of length m plus 1. So if m and m tilde are fixed, it's purely parametric. But in the theory, we let m and m tilde go to infinity. So then it becomes a sieve, so it's not parametric. Okay, is that? Okay, is that clear? Yeah? Okay, so here are a few key properties. So the first property is that this enriched Lagwaz, we can show that it's always a density, which is not that obvious because why would you still have a density if you add these terms in pink? It is still a density, and the reason for that is. And the reason for that is that these Laguerre polynomials are orthonormal with respect to the exponential density. So it means that if you integrate two of these Laguerre polynomials multiplied with the exponential density, you get zero or one. And that property makes that this is always a density. It's a little calculation. It's a little calculation. And so that's also the reason why we work with Lager polynomials and not with any other family. It has to be like that. Otherwise, we don't have a density. Okay, so that's one important thing. And then the other thing is the fact that, yeah, the quantile has to be zero by construction. And also, that property holds thanks to the fact that we work with like. The fact that we work with Laguerre polynomials. So that's also something that you can show. It's a few lines of calculations and it works because we have properties on the Laguerre polynomials that guarantee this. Okay, and then the third property is also an important one. So there are again properties on the Laguerre polynomials that you can use to show that A. To show that any continuous density can be approximated by a member of that family of enriched Laplace if you let M and M tilde grow tweet. And what I mean with approximated is that the Helminger distance goes to zero. Okay, and then the last property, and that's the one of course that took the most Took the most of the work, it's the asymptotic properties. So, what did we show? We showed that the resulting estimator of the regression coefficients is consistent, asymptotically normal, and even efficient if you have right-sensored responses. And if you have interval-sensored responses, which is a different paper, in that paper, we showed consistency, but we didn't do all the Consistency, but we didn't do all the work to show asymptotic normality in patients. Okay. So that's, yes? Is that when you let the C So the expansion tends to infinity? Yeah, yeah, yeah, yeah. So in in this in this results we let M and until the goal to infinity. It's very interesting. Uh definitely Yes, yes. For this property, the Laguerre polynomials are not at all essential. The reason we choose The reason why we choose Laguerre polynomials? Yeah, that's for these two first properties. For these two first properties, but not for the third one. Thanks. Yes? I was wondering if you have any practical guidance on how you would choose the degrees of the polynomials. Very good question. Oh, okay. The next slide. But unfortunately, I'm not giving lots d lots of details. So we we use it we choose it by using cross validation. Um but of course you need to take censoring into account. You need to take sensoring into account. So, we adapt the cross-validation criterion in a way that it can handle either right sensoring or multiple sensor. So, that's one thing we do in practice. And then a few other things in practice. Well, in practice, we take m equals one tilde. So, in theory, we let them be different, but in practice, we notice that it doesn't really matter that much. Really matter that much, and then it becomes more feasible to choose the degree by cross-validation because you only have to do it once and not twice. And then a third thing in practice, so what we do is we, so you know this asymmetric Laplace, it has like two densities or two exponential densities that you glue together, but of course, at But of course, at this top point, we want them to be really together. It would be a bit weird that the density looks like this. So, to make sure that they come together in zero, we have to impose this constraint. It's again a little calculation that shows that under this constraint, they are really together. Yes? I'm sorry, just the last slide, you. So just the last slide you mean efficiently does that mean if you know the true distribution of epsilon and you can calculate anybody with the last efficiency we mean that we calculated the semi-parametric efficiency bound and we show that our variance is equal to that bound. Okay. Okay, so that's all I want to say about sensored responses. Of course, in the paper, we have also simulations and things like that, but I want to focus more on the sensored covariance, which is work in progress. So what I'm going to show here is still quite preliminary, especially for the simulations and the data analysis. There are certain things that I don't think are still. Are still not yet the way they should be, but that's because it's working for us. Here are a few papers on censored covariates in the context of mean regression. So yeah, some of them have already been discussed here this week. And so yeah, for mean regression there are some papers, but for quantile regression, as far as we know, there are no papers. As far as we know, there are no papers on the censored documents. Actually, there is a paper. Judy Wong wrote a paper 2012. Oh, I should look at it. I'll send it to you later. Okay, good, thanks. Okay, so here is the context. So now we are assuming that Y is fully observed. Of course, Y can also be censored maybe, but we are not considering that. But we are not considering that. And for the covariates, I'm going to use a notation xz. So x is the one that is censored and the z is completely absorbed. And we are going to assume that x is one-dimensional. Again, this can be generalized, but we focus on the one-dimensional piece. And we consider both the case of right sensoring and interval sensoring. So if it's right. So, if it's right-censored, then what we observe is the minimum of x in a certain sensoring time, which I denote by xc. That's w. And then also the delta, of course. And when it's interval sensoring, we observe, as before, an interval to which the true x belongs. Okay, so the question is now, of course, how can we adapt this methodology for? Adapt this methodology for censored responses to the context of censored covariance. And this is maybe not that difficult anymore because we have done most of the work already in the case of censored responses. Thing that we, oh, yeah, this is a warning that everything that follows is a bit preliminary. So, yeah, let's first go to the model, then I will explain how the methodology works. So, we have again this model for Y, which is a linear model, so linear in X and in Z, and then again we allow for heteroscedasticity. And for the moment, I'm assuming that the heteroscedasticity only depends on C. only depends on C for simplicity. Because if it also depends on X and X is censored, then it also affects the heteroscolastic. I don't want to do that for the moment. Okay, so the quantile is again zero and we again have independence between epsilon and covariance. Okay, so that's my first model. And I will also need to assume a second model. Also, need to assume a second model now, which is the model for x given z. That's because, yeah, we are going to work with the likelihoods, and in the likelihoods, we will have to work with that density of x given z. For the simulations that we did so far, we worked with a parametric model, purely parametric for x-givens. But this is not at all essential. You can do Not at all essential. You can do whatever you want, semi-parametric, non-parametric. You can assume that X is independent of C, like look at it yesterday. It doesn't really matter. But yeah, in what I will show, it's going to be parametric. Okay, and then we come to the likelihood. So, of course, the likelihood, if everything would be completely observed. Would be completely observed, you can decompose it in that way. We are not really interested in the density of C, so it's becoming this product. And if now your covariate is right-censored, then you're going to use this contribution if you have an uncensored value of your covariate. And if it's censored, so if the delta is zero, then you take an integral of You take an integral of this from your sensoring time up to 88. And for interval sensoring, it's similar. So again, you start from this contribution in the case of an uncensored value, but now you know that the true covariant lies in this integral, so you integrate over that integral. Okay, so I have these two light. Okay, so I have these two light cables and I guess you can guess what I'm gonna do. So I'm gonna plug in my enriched Laplace for this density of y given xz and I'm gonna plug in my parametric density for x given z. And this can, as I said, also be something else, it doesn't really matter. Okay. So, yes? So I thought that enriched Laplace was because Y was censored. So wouldn't you plug in Enriched Laplace for X given Z since X is censored? It's the same thing. So yeah, that's true. I didn't explain that well. So if X is censored, I have the same problem. I have the same problem because it's the epsilon at the end that is going to be censored. And the epsilon is y minus your regression function. So if your x is censored, you're also going to get a censored epsilon. And so it boils down to the same problem. Is this because we have quantile regression? Like if we had. Like, if you had continuous mean regression, so it seems like flipping what's sensor, the y or the x in this case, and the sensor, in the quantile case, it doesn't matter. Correct. Yeah. Okay. Yeah. That's that's the main reason. Okay. Okay. So, as I said, we are going to plug in these two densities in the formula, and then in principle, you could just maximize. Principle, you could just maximize this likelihood with respect to all the parameters. So, you have parameters here from your enriched Laplace, and then you also have parameters for this density. So, in principle, you could do it in one step, where you maximize everything starting from this likelihood. But we suggest to do it in two steps. So, first, you take the likelihood just for the model. Just for the model for X, given Z. And so again, you have two formulas depending on whether you have right sensoring or interval sensoring. So you have this one, this one, and this likelihood you can maximize with respect to the parameters of that model for X given Z. And so once you have these estimated parameters, Have these estimated parameters, you can plug in these estimators in your likelihood and then maximize this likelihood only with respect to the parameters of the other tests. The likelihood in your first step is not is not a part of this L. No, no, it's so it's really a different uh likelihood. Um Likelihoods, and so you won't have the same estimators than when you do it in one step. The reason why we prefer to do it in two steps is that you get here estimated parameters that will not depend on the quantile that you have chosen, which makes sense because it's a model for x given z, which has nothing to do with the quantile level that you choose for y. So that's random. So that's one reason, and the second reason is that it's probably also more stable because otherwise you have one maximization over all the parameters and you might have many parameters. That's given the model has to be correct. For the theory, you mean? Probably, yes, I think so. Yeah. But from this likelihood, right, from this likelihood, the X given Z model, sensor part of the X given Z model in the part in the likelihood of your first step They cannot be separated. Yeah, and you mean you are separating them? Yeah. Yeah, it's true, but maybe we should do assimilation to see which one works better. We only tried this approach so far, but yeah, maybe we can compare and see which one works better. But it's yeah, it's really a different method. Okay, five minutes, okay. Okay. Um so that's the proposal for our method. So we do it in two steps. And here is just a recap of this enriched Laplace. So it's the same formula as before. So we have this density under the location scale model and here is the formula of the M rich Laplace density. We take again M and M tilde the same. M and M tilde the same, so there is one regularization parameter that needs to be chosen, which we choose again via cross-validation. So we get then estimators for all our parameters. And again, we want to have a smooth density in zero, so we use that same constraint on the thetas that I showed earlier. Okay, yeah, here is why we use a two-step. Yeah, here is why we use a two-step approach. And then, also, these properties that I mentioned before for sensor responses, they are still valid here. So, we still have a density, we still have quantiles equal to zero by construction, and we can still approximate any continuous density by density in that family. Okay, now complete case analysis. We have been talking. We have been talking about that already in other talks. So, of course, here it can also be used. And if you do complete case analysis, you throw away, of course, all the censored observations. And in that case, you can just use the classical check function approach because you only have uncensored data. So we will compare with them in the simulations, but as has been mentioned already. But as has been mentioned already earlier, we throw away a lot of data, and so even if it's still consistent, it will not be optimal. And here is a quick visual kind of intuition why it still works. So at the left-hand side, you have a right-sensored covariance. So everything that is red is coherent, is censored. And here you have a right-sensored response. Sensored response. So everything above a certain threshold is sensored. So I'm taking for simplicity type 1 sensoring, so fixed sensoring. And so you see that if you have a sensored covariate and you throw away all the red points, it's still going to work, right? Whereas with a censored response, if you throw away the red points, your regression line will have a different slope. And so it's Have a different slope, and so it's not working. Okay, and here is a very quick proof because we also discussed already conditions under which complete case analysis works. The conditions that we need is that you need independence between the sensoring time for your covariates and your response, given the sensored covariates and also given the other covariance. And that's a quick. And that's a quick proof. And so a sufficient condition would be that your censoring time is independent of that. Okay. How many minutes do I have? Two. Okay. That's not much. I think I'm gonna skip the simulations. So yeah, we did simulations for right censoring. We are maybe very quickly we see Maybe very quickly, we see that the complete case, which is this column, works. So its bias is quite small, but the variance is much larger than what we get with our enriched Laplace. And we also compare with the naive. And with naive I mean that we use the asymmetric Laplace and not the enriched Laplace. And then it leads to biased estimators. Minus estimators. That's a quick summary of the simulations. We also looked at the misspecified model, but I'm not going to talk about it, and then we also have interval sensoring. We have for the moment a data example, which is on the PBC data from long ago. If any of you have other data that could be useful in this context, I would be very interested because I don't have any for the moment. Interested because I don't have them for the low ones. So we worked on this data so far, which is for right sensors, but we don't have any data example with interfaces. Yes. Okay, I'm going to skip this and then the conclusions. So we looked at Grand Tal regression with right or interval censored covariates and so we could consider Could consider a few extensions. Of course, you can have sensoring both in the covariance and in the response. You can have more than one sense of covariance. And as I said, this model for X given Z can be more flexible. This is actually very easy to do. The other two points are more complicated. And so I think something. Oops. Something that could be nice, I think, about this approach is that it's likelihood-based, and so you can easily adapt it to other contexts. So it's not really specific to the context of censoring. If you have other incomplete data or biased data or whatever, since it's a likelihood-based method, it suffices to change your likelihood. And of course. And of course, on our package, that's also something that we are still working on. And then here are the papers. So the one for right-sensitive responses is under revision. This one is submitted. And then the last one is in my language. So you can take questions during the break, but we'll come back at 9:15.