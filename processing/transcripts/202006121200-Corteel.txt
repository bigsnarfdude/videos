Okay, so we're going to start with the ultimate talk of the day by Sylvie Curtile on multi-species ASAP and McDonald polynomials. Okay, thank you very much. I'm really honored to talk during this OOPS meeting. So as Ivan said, So as Ivan said, we should really thank all the organizers of Oops, Luigi, Omer, and all the others. I thank Alvin for being such a great host this week. And I also want to thank my daughter who helped me do the slides. During the last three months, I've learned to really do multitasking between being the teacher of my kids and being a professional mathematician. So it's been definitely It's been definitely challenging. So, I want to talk about some joint work that we've been doing with Olia Mandelstein and Lauren Williams. The connection with Canada, I guess, as OOPS is organized by Canadian. Also, Olia will join the faculty at Waterloo in January. Okay, so let me start. So, I'm coming from a So I'm coming from a different world than most of the people here. I'm a combinatorialist and so what I'm interested in is often understand the combinatorics between different objects, different polynomials and things like that. And my goal today is to understand things about McDonald polynomials. And to do that, I'm gonna use exclusion processes. Okay, so the McDonald polynomials. Okay, so the McDonald polynomials, I'll explain a bit about what they are way later in the talk, but they're going to be polynomials indexed by composition, so a list of non-negative integers. They are polynomials in several variables, okay, and they have two extra parameters. And for these extra parameters, you can see that the coefficients are rational functions in T and Q. In T and Q. And the type of exclusion process we're going to look at is we're going to look at particles on a ring, so of a fixed size. Okay, the size of the ring is going to correspond to the number of variables here in my polynomial. And then two particles can exchange, okay, if they are different. So for example, here the two particles are equal. Two particles are equal, so they could exchange, but that doesn't change the configuration. And then, if two particles don't have the same number here, okay, they can exchange and the rate depends whether one is bigger than the other. So, four can exchange with one with rate one because four is bigger than one, and one can exchange with four with rate t because one is smaller than four. So, our priority. So a priori, there's no reason that these things are related. Okay, but my goal is maybe the opposite of most of the people here, is I'm going to use this exclusion process to understand this algebraic combinatorics object. Okay, hopefully that will give you some motivation to study these also things coming from combinatorics. Okay. Okay, and so what I'm going to do to make the link between these exclusion processes and these McDonald polynomials is I'm going to use what are called multi-line cues that were first studied in a case where we have two types of particles by Homer and John for the TESP. And then so the case T equals zero, then they were generalized by Ferrari and Martin. By Ferrari and Martin, also for the test set, but for the multi-type, where we can have several types of particles. Okay, and recently James Martin generalized them for generality in 2008. Okay, so that's the plan. Okay, maybe for some of you you wouldn't follow the same path, but my path is to start with the particle system, then goes to these queues coming. Then go to these cues coming from probability theory, and then get to some combinatorics. So let me start with part one. Okay, my talk will be just in three parts, part one, two, and three. So let me start with the asymptotic exclusion process. With the asymptotic exclusion process on the ring, asymmetric. Okay, so let's look at the simplest model where I'm having a ring with n sides and k particles. The particles, I label them by one, and if there's no particles or whole, I label them by zero. Okay, here I give an example for n equals four and k equals two. So I have six different configurations, all the ways of putting. All the ways of putting two ones and two zeros on a ring of size four. And let's look what kind of Markov chain I built for t equals zero. So for t general, you just have extra arrows. Whenever you have an arrow with rate one here, you have an arrow with rate t in the other direction. Okay, so for example, I can go from this configuration to this one because this one can move this way. Okay. This way. Okay. And I go from this one to this one when this one moves here. Okay. And the question I'm asking is when I run this Markov chain for a while, what's the probability that I am in a given configuration? Okay, so that's my question. So does anybody have an answer? I don't know if I can see the chat. I don't know. I don't see the chat. So let me give you the answer. So the answer is just simple. It's uniform. Okay, the probability to be in a unique configuration is to the same 1 over n truth k. And the proof is very simple. Is whenever you go around on the circle, you're going to see as many 1 followed by a 0, then a 0 followed by 1. And so you have as many. One, and so you have as many incoming arrows as outgoing arrows, okay? And that's true also for generality. So, when we ever have one and zeros, things are just like trivial. You just have a uniform probability to be in any state. Okay, but when you look at with n types of particles, okay, things become more complicated and computing that stationary distribution is. That stationary distribution is not as simple as usual. Okay, so let me define exactly what's my problem. So I'm going to have n types of particle where n is an integer, I guess, bigger than two. As we see, where we have just two types, things are very simple. And the number of particles of each type is going to be given to me by what we call a partition. So a sequence lambda of non-negative integers where they Negative integers where I write them as non-increasing. And now the states of the Markov chain is all the permutations of lambda, okay, all the way you can permute all the types of particles. And now I'm going to have transitions between two states, if they are exactly the same, except they differ at two sites. Okay, so one site has I and J with I greater than J and J and I Than j and j and i with i again greater than j. And this will exchange okay in my Markov chain with probability one over n if i is greater than j and t over n is i smaller than j. And I'm asking the same question. Okay, what's the probability to be in a given state? Okay, so if n is 2, we saw that it's just uniform probability, but if probability but if n is bigger so i gave the smallest example when now we have three types of particles okay so we have six states all the permutation of zero one and two and here is how you the markov chain would look like okay you can go from one to the other whenever two sites exchange and if you compute the stationary distribution okay you can see that the probabilities Okay, you can see that the probabilities to be in a given state is a, I guess, nice polynomial in t divided by a nice polynomial in t. By nice, I mean it has a positive coefficients. So here the example is very small, but it's true in general. The probability to be in a given state would be a polynomial with positive coefficients in t divided by a polynomial with positive coefficients. Okay. So there was. So, there has been a lot of work on how to compute this stationary distribution. And I'm citing one paper, but don't be offensed. There were other types of work too. Okay, one way to compute the stationary distribution was given in a thesis of Prolac and in a paper of Prolac, Evans, and Malik. And the idea is to compute that using traces of infinite matrices. matrices. Okay, so the building blocks are three infinite matrices, alpha, delta, and epsilon, and they have to follow this type of relation that are usually called the matrix anzats. And you can choose any matrices that follow these anzats. Okay, one solution of this ansat is this one. Okay, like these simple matrices. Alpha is a diagonal matrices. Alpha is a diagonal matrices, epsilon is a matrix that just has an upper diagonal field being one, and delta is all zeros except in the diagonal under the main diagonal, where you just have just powers, simple powers of t. And so they build this kind of complicated matrices that are tensor product of these three simple matrices alpha, delta, and epsilon. Delta and epsilon. Okay, and here i is also the identity matrix. Okay, you have these like very simple building blocks and with this like construction, you can compute exactly, you can solve exactly your problem. Okay, the probability to be in a given state is going to be proportional to a truss of matrices where the matrix you choose here is just the type of particle. It's just the type of particle you had at each site. And it's kind of a cursive construction here to construct this xjn. So here there should be some n here, I forgot them. You need to know the matrices for n minus one types and you add some extra information of the nth type. Okay, so you can solve this problem exactly. Okay, you have this like. Okay, you have this like machinery to build recursively this stationary distribution for any n. And it's like recursive, so that's the idea we're going to use. So we have people use these for asymptotics in any effective way? That's going to be one of my questions. So I think there should be some asymptotics done on those. So Prolaki has some papers, but I think there's Some papers, but I think there are still a lot of things to do. So, and there's a similar construction, not exactly the same, but that's going to explain to us why we can see McDonald polynomials coming from SEP, coming from Continued Gear and Wheeler. So that's more recent from 2016, where they have some types of Macdonald polynomials. So they are not the symmetric ones. I'll explain exactly what they are. Exactly what they are, but they can write them again as a trace of matrices. Okay, and it's very similar to what we have. The indices here of the matrices corresponds exactly to the parts of the composition here that you have. Okay, and then there's one, there's more information in them because now each matrix has a variable that's associated to it. And there's also that last thing, okay, that give you. Okay, that gives you some kind of weight at the end. Okay, and so these matrices, when you set all the xi's to one and the S matrix that has some Q's in them to the identity matrix, you recover this result of Evans, prolag, Evans, and Matic. Okay, so somehow they added these Xi's and these Q's and And these cues, and they could get to the McDonald polymer. So, here we're going to do something similar to what Contini, Dogge, and Wheeler did using these matrix handsets. But what we're going to use is something more combinatorial. We're going to use these multi-line cues. So, as I said before, the multi-line cues were first studied. I hope I don't make a mistake. Hope I don't make a mistake by Omer and John for the type two, and then by Ferrari and Martin and General Capes for TESP. But here we're going to use a more general version where we can put any T value, not only the TESF, but the asymmetric self-exclusion. Okay, so let me try to explain to you how this construction works. So as I said before, a system is first with system is first we fix a partition so here i i took a very simple example so lambda is three two two two one zero zero zero okay i take n small n to be eight and i have four types of particle okay and then i define by mi the number of i in lambda now i'm gonna take i'm gonna put balls in an array so i'm gonna put An array. So I'm going to put first m3 balls in row three, so because I have only one three in my partition, okay. And then I'm going to take m3 plus m2 balls in the second ball, in the second row. So anywhere I make a choice, okay. So here I choose to put one ball in column two, one in column four, one equal five, one column six, and then m3 plus m2 plus m1. So here plus m2 plus m1 so here I have one three three twos and one one so I put five balls in row one okay and I'm seeing this array actually as if it's wrapped on the cylinder okay so the column n plus one so here the ninth column is still is really column one so that's my ball system so my partition tells me how many balls I put in each row but now to build a cube I'm Now, to build a cube, I'm going to match the balls in a given row to some balls in a row below. Okay, and I can always do it because the number of balls increases at each step. So, for example, here, as I start in row three, I put a free in my particle, and I'm going to associate to it to a particle under. Okay, and I can choose anyone. So, for example, this free I associate to this one. Associate to this one, and then I continue. Okay, when I start doing, I'm going until the bottom, and for example, I am going to associate this ball to this ball. And I do that for everybody. And I have a special rule that's really important is whenever I start with a ball and a ball under it is present, I'm forced to associate it to him. Okay, so for the balls who don't have somebody under, they can choose anybody, but the balls that have somebody But the balls that have somebody under, they are forced to it. So, this one I have no choice. This two has to be like that. But this one, okay, even though it had a ball under it, it's been already associated. And so he can choose anybody. And I remember we wrap around the cylinder. And so he can be associated to anybody and he can wrap. Okay, I continue like that until all my balls have been linked to somebody. Link to somebody under, and I have one last board here that has not been chosen, and so I give him one, okay, because he hasn't been chosen by anybody. Okay, so that's my what I call, I guess what James Martin calls a multi-line queue. So now I need to associate two things when I build my queue. Okay, first, what's the type of the multi-line queue? So the type of Multiline Q. So the time of the multi-line Q is just information in row one. So the labels of the balls in the bottom row here. So I have a two and a two, and then I have nothing here. So I write them as zeros. And I have a three, a two, and a one. Okay, so my first row is a permutation of the part of my partition lambda I started with. Okay, and now I have to associate also some polynomial. Some polynomial in t. And it's pretty simple. To each arc, I'm going to associate to it a rational fraction in t. Okay, so the arcs where I was forced to go straight, I give them weight one. But the arcs where I had a choice, okay, I had a certain number of free balls, so I could have chosen any of the free balls, and then I give t to the number of skipped. give t to the number of skipped the number of balls that I didn't choose okay so here it's a the t analog of n of f one plus t plus t squared to t to the f minus one so let me go back to the example and see how we compute the weight of each arc so here I started from there and I didn't choose this ball and so I get a T to the one because I skipped one ball and I had four free balls. And I had four free balls. So the weight of this arc is T to the T analog over to the T over the T analog of four. Okay, so here I had five balls, but I chose the first one. Okay, so my numerator is one divided by the T analog of five. And here I had no choice. I had to go straight. So the weight is just one. And now for this one, I skip two balls. Okay. skipped two balls okay and i had three so the weight of this arc is t squared divided by the t an arrow of three okay so from this okay construction when i build this multi-line queue i get the type okay that's the information in the first row and i get also the weight of the queue which is just the product of the weights of all the arcs okay so if you Okay, so if you go back to the example here, so what I get is a t to the cube for this q. So for each q I'm going to get t to some power and divide it by a nice product of t of numbers in their t analog. So we see straight away why we get at the end a nice polynomial in t in the numerator and denominator. And the result of a Result of Jess Martin is that if you look at the multi-type SF, okay, you're going to sum on all the Q's, the probability to be in a given state mu. You're going to sum on all the Q's where the type, the first row, is fixed. And you're going to get for each Q this rational fraction in T. So somehow the proof goes back using these Using this generalized matrix construction, that's a way you can prove this result. So, that's a nice way to give the exact description of this stationary distribution. Okay, so let me just give you, I guess, a simple example. I guess a simple example, the same example that I've used before when I define the Markov chain. So here I build all the multi-tiline q's that you can get from lambda being 2, 1, 0. Okay, I'm not going to detail all of them. But for example, if we want the first row to be 2, 1, 0, there's two ways to do that. Or I put a ball here and I go straight, or I put a ball here. Or I put a ball here and it could wrap around and be linked to this one. Okay, and I compute the weight and this gives me one for this Q and the weight of this Q is one over one plus T and then you can do the same things for the six the 12 different multi-line queues that you can build from there. Okay, and you can check that you recover the stationary distribution. So the case t equals zero, okay, we have just a bijection between the ball system and the multiline queue, because when we go back to t equals zero, we cannot skip any ball, and so we have no choice. Okay, whenever we start from a ball, it has to choose the first ball that's available. Okay, so we get back to the construction. So we get back to the construction by Ferrari and Martin for t equals zero. And so that tells you also why the partition function of the multi-type is just a nice product of binomial coefficients, because it's just the number of choice you have for each row to put the balls, and you link them in a trivial way. So now let me go to, I guess, the part where I'm the most comfortable, the part coming from. Comfortable, the part coming from algebraic combinatorics and these McDonald polynomials. So these are pretty complicated objects, so I'm not going to give too much details. And so these are been out since the 80s. So as I said, it's going to be polynomials in n variables with two x parameters q and t. The t is going to be Q and T. The T is going to be the T coming from SF. Okay, but I have more information. So usually they are indexed by a partition. They are symmetric in X1, X2 up to Xn. Okay, the coefficients of each monomial in the XI's is a rational function in Q and T. And then they can be written as they are symmetric in the monomial basis in a triangular way. Okay, so and they are orthogonal to a certain inner product. So here is an example on how you write these McDonald coefficients, okay, where it's my example that I've been using from the beginning, lambda is 2 and 0. Okay, we can see that these are not particularly simple polynomials. So to go back to what Ivan talked about just a bit. Talked about just a bit. Okay, these are generalizations of the show polynomials you've seen in Ivan's lecture. Okay, when Q equals to T, they degenerate to show polynomials. And in a lot of work about like McDonald processes, okay, there's a lot of specialization of them. I just listed a couple. So when Q is zero, you have what you is called a whole little wood polynomial. But here I'm interested in non-symmetric versions of MacDonald polynomials that were defined by MacDonald and Scheretnik using a lot of affine acquired algebra techniques and studied recently by Ferrera and Alexanderson. Okay, so they are usually denoted by E mu of sigma, where here mu is a composition, okay, so a sequence of non-negative integers. Of non-negative integers, but also a permutation sigma. And it's known that these are like the non-symmetric analog of McDonald polynomials. And what I mean by that is that if I fix mu, okay, composition, and I sum on all the sigma of this E mu sigma, I'm getting back my symmetric McDonald polynomial. Okay, and this is true for any rearrangement of Any rearrangement of lambda. So I fix the composition mu, that's a permutation of the parts of lambda, and then I will sum on all sigma of this E mu sigma. It's quite magical. Okay, if I want to compute the symmetric Magnum polynomial P31, it's going to be equal to E31 with the two different permutations and E13 with the two different values. Okay, so I'm going to look at a special case of them. To look at the special case of them, okay. I fix a composition, so a sequence of non-negative integers. I'm going to look at the partitions that's just the reordering of the part of mu. And sigma is going to be the permutation that tells me how I reorder my partition to get my composition. Okay, and my f mu here, okay, are just this e lambda, so where lambda is the sorting of my composition. The sorting of my composition and sigma is my partition. So, for example, if my composition is one to zero, my f indexed by one two zero is the non-symmetric Magnal polynomials to one zero indexed by the permutation two one. Okay, so this F mu, they are defined uniquely by some operators, so I won't define too much. So, I won't define too much. Two types of operators, the t i's that are coming from the atine and algebra that are t deformation of simple transposition in Sn and some Donkhole operators that are defined from the Ti's and some extra omega operator. And they define uniquely these non-symmetric McDonald polynomials. So, if mu is a partition, if I apply yi. If I apply yi to fu, I'm going to get f again. So fu is an eigenvector of the Duncan operator when mu is a partition. But then I also have some conditions on the t i's, okay, when they act on the x mu. So this, if I act by ti on f mu, it's going to correspond to switching two parts of my composition. So it looks really like Composition. So it looks really like what's going on in SAP when two things are exchanged. Okay, and when two parts are equal, if I add by Ti, I just multiply my F mu by T. And then we have something that's also about wrapping up. Okay, if I multiply my F mu by some Q to the right power, it's going to be the same as changing my variables, putting Xn in front and multiplying by Q. n in front and multiplying by q but also doing some circular action on my composition and so if i find some polynomials that satisfy all these relations okay these are going to be exactly my non-symmetric mcdonald polynomials that i'm looking for and so let me i won't show you why this works but this is exactly But this is exactly what the multi-line cues are doing. Okay, so now I'm going back to the multi-line cues, and as I want to compute my f mu of x, i, q, and t, I need to add some information. I need to add variables x1, x2 up to xn, and I also need to add an extra parameter q. Okay, and when q is 1 and all the x i's are 1, I want to go back to the construction. I want to go back to the construction of JEMS marketing. And what's pretty surprising is that it's very easy to do. Okay. So if you look at the weight of the arcs, we're here from rho r to rho r minus one. So before I had a t to the skip divided by a t analog of three. Okay, and I just have to deform that just a bit. Okay, in the denominator, I add this power. I add this power of Q. Okay. And in the numerator, I add the power of Q and only when I wrap around. When my arc was wrapping around, I add this power of Q. So it's just a simple Q transformation of what was done by Jason Smart team. And the weight of the balls, it's just very simple. Whenever you have a ball in column, Whenever you have a ball in column i, you add the xi. So we see straight away with this desperation that if I put all the xi's to one and a q to one, I'm going back to the construction of Trem's mark. Okay, so now if I go back to the example that I've done before, okay, it looks more complicated, but it's not really. Okay, so I get a monomial in x and then I get a rational fraction in q and t. Rational fraction in q and t. Okay, so on the numerator, I have a one minus t to some power of the number of arcs, and now my denominator is just about as simple as before, except I have some cues. And the q power here tells me like which arcs were went around. And on this q, I had just one arc going around. And so the theorem, yeah. The case that relates to ASAP again is q equals one and all the x's equal to one, right? Yes. So if q is one and the x's are not all equal to one, does that correspond to some sort of inhomogeneous version of ASAS? No, but there's some ongoing work by uh James Martin and Olya and Arvin Iyer to put the Xays. So so this should come up uh soon. Should come up soon. So, the theorem is that if I do that, if I summon all these Q's of a given type, and for each Q I give this weight, so each Q has a monomial in T times a rational fraction in Q and T. I'm going to get these non-symmetric McDonald polynomials. And I also get a formula for the symmetric McDonald polynomials. Okay, I'm going to sum on all the Q. Going to sum on all the cues where they have a fixed number of balls on each row. Okay, so how many numbers, the partition lambda tells me how many balls I have in row n and minus one and minus two, etc. And so the proof goes using like these algebraic operators to show that the multi-line q satisfies those things. So it's really not very So it's really not very simple. And so it would be nice to have a simpler proof of why this is true. So this is a pretty simple construction. And so it would be nice to understand in a simple way why this is true. So this is one new model. There are other models, combinatorial models for McDonald's point, but it's one new. So I go back to my example here that I given you for the That I given you for the SF, okay? And it's just simple, except now my denominator has a power of Q, and whenever I wrap around, I get also a Q in the numerator. So let me just give you a couple of remarks that are more maybe interested to combinatorialists. So, but let me just go quickly. So, when Q and T is zero, okay, these Until zero, okay, these MLQs are just going to be in bijection with semi-standard York Tableau or sequence of interlacing partitions. And so you go back to the case of short polynomials. So it should be true where Q is T for any Q and T, but there's no nice way of seeing that. But in Q and T is zero, it's very simple to see why you get backshot polynomial. And then what was pretty Then, what was pretty interesting is that using this construction coming from exclusion processes, we have very nice applications also in algebraic combinatorics. So, it's joint work with Olia and Lauren, but also with Gina and Glung and Sarah Mason. Now, we are studying these like compact formulas for McDonald polynomials for overtype, the modified McDonald polynomials, and we could also define some And we could also define some quasi-symmetric Magdalena finding. So let me finish because you had a long week. Okay, so I guess my open questions are related to some of Ivan's question already. So in general, can we define a particle system whose stationary distribution is going to be this ratio of a non-symmetric McDonald polynomial divided by a symmetric Magdalena polynomial? Divided by a symmetric Magnonorphon. So it seems, as I said, for Q equals one, there's some hope to put the XI's, but I think to put Q in general for now, there's not a good idea of what it could be. But it would be interesting to be able to go back to the particle system and be able to put these excise. Okay. And so I said the multiline cues, they give us a particular non- A particular non-symmetric Magdalena polynomial, where here it's indexed by a partition, so a non-increasing sequence of integers. But there's also that whole family of non-symmetric Magdonal polynomials where this E mu sigma for any sigma can be also indexed by any composition. And so can we build multi-line cues for all these E mu sigma or is there also some kind of SEP for these more general? For these more general things. And also, as I just said, there's also a whole, even in type A, there's also these modified McDonald polynomials that have a lot of application in algebraic combinatorics. And it would be interesting to see if there's some particle system that explains this modified Magnum polynomial. So another my question that you should be able to answer more than me is that could we use now these T multi-line Q's to compute nice. T multi-line cues to compute nice asymptotics. Okay, these are beautiful objects constructed by James Martin. And the same way that they were used in the case t equals zero, there should be some ways to use them in this general setting with t. And finally, my, I guess, something I've been interested in for a long time now is to look also at the other type of exclusion process. So for example, Process. So, for example, James Martin explains how to extend his construction on the ring to the infinite line, okay, to have these like multi-line cues on the infinite line. And of course, the case I am interested in is the case of open boundaries. So now I'm looking at a finite line with some particles that can enter and exit from the left and the right. So it's known that in the simple case, it's related to ASCII-Wilson polynomials. ASCII-Wilson polynomials. And there's some multi-type version given by Luigi Cantini that works for the case. I guess here I made a jump. This Q should be a T there that were defined by Continu for Q equals one. But they don't, in the same way that in this type on the ring, it's easy to go from ACEP to the McDonnell polynomials. On the open line, Polynomials. On the open line, for now, it's nobody knows how to go from a multi-type asset with open boundaries to these general conwinder polynomials, who are the analog of the McDonald polynomials for this. Okay, for just one site, actually, what you're looking for is the ASCII-Wilson polynomial, which is already a very complicated object. And so it would be nice to have some good insight coming maybe from probability or intelligence. Maybe from probability or integrable system on how to do some combinatorial model for this window polynomial. So, thank you very much and have a good weekend. Thank you, Sylvie. Okay, we have more questions. If also James Martin wrote something, and I guess we're Wrote something in, I guess, responding to my question. He said, There are some inhomogeneous versions of a multi-type QTASAP or Q-long-range exclusion process, which corresponds to the case that Ivan asked about. Okay, so that's is there a reference for that, James? Or this is no, not yet. That's work that's kind of going on with lots of so Omer and Svante Lindelsson and Arvid I. Arvind Aya. And then, as Sylvia mentioned, there's also some work with Arvind and Olya, which is more about zero-range process versions as well. Cool. But there's some really nice, there are some really neat things. We'll try and get it written up soon.