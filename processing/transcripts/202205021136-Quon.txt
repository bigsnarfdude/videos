And today he will be telling us about identification of rules of cell-cell interactions via time-lapse in between. And I hope that he's online already. Who was online before? I don't know. Well, we're a few minutes before schedule, so hopefully he's joining in a few minutes. Who wants to go actually wants to? He wasn't your act, he was there. Yeah, you're ready.  I was actually logged in earlier, but my internet has literally just died. So I'm just trying to. So we were a couple of minutes behind ahead of the schedule. So that's why we got not connected. But that's great that you're here. Yeah, no, I'm just trying to fix the internet right now, but just give me a minute. No, we have to stop it and restart. I know like I already started the one for general. Oh well, I mean, Wash Cups is no idea what is. Okay, so why is one? Here we go. Okay, so I will introduce you, so I have to leave people note. The floor is yours, whatever you are. The floor is yours whenever you are ready. Yeah, hold on. Let me just share my screen. Okay. Can everybody see this? Yes, we can see. Thank you. Yes, we can see. Thank you. Great. All right. Hi, Ron. Yeah, sorry for the little technical mish out there. My internet conveniently died just as the previous speaker's Q ‚Åá A was ending. But here I am. So yeah, there's a slight change in title, but today I'll be talking to you guys about some really kind of preliminary slash work in progress project in my lab about measuring kind of global shifts in covariation patterns in genomic. Coveration patterns in genomic data. And yeah, the purpose of the talk here is really to kind of hopefully solicit some feedback on how we can better use things like feature attribution and other interpretation methods to kind of help us achieve our goal. And so the work I'm talking about today is primarily done by a graduate student in my lab, Roshan. And so just to give you guys some very kind of brief biological background, the kind of data. Background. The kind of data that we work on is what's commonly referred to as single-cell transcriptome data. But the way in which you can kind of think about it as being represented is really just a matrix where the rows represent different kind of genes that are encoded in the human genome, and the columns represent different cells that you can measure. And so, one of the big things that people are doing in biology nowadays is kind of measuring different kinds of samples on a kind of single cell level. And what people are currently have And what people are currently have now really appreciated is that, obviously, as you kind of expect, different cells in the human body look very different at kind of at a gene level. And so, here on the right, I'm basically just showing you like a classic TSNI or like a UMAP plot where individual points represent different cells collected from different tissues in the body. And you can hopefully appreciate that different cells from different regions of the body basically cluster in different regions in the space, kind of as you'd expect. Kind of as you'd expect. And so basically, a lot of people in single-cell biology spend a lot of time basically trying to use these kind of single-cell sequencing technologies to kind of really identify like an atlas of cell types in different tissues in the body. And so it's very kind of typical for people to draw these plots like this one I'm showing you up here, where basically points again represents. Where basically points again represents cells. And basically, a lot of people spend a lot of time trying to figure out how many kinds of clusters do you see in these plots, and what are the kind of differences in gene activity levels that you can identify between different clusters in these plots. And so what the talk today is about is about something a little bit different. So instead of being interested in understanding kind of what is the kind of global differences in gene activity between different clusters in these plots, what we're more In these plots. What we're more interested in is in trying to characterize how the gene co-variation patterns vary between these populations. And so, what I mean by that is that what people oftentimes do is they take like a cluster of cells, like this blue one here, and what they'll do is that within that population or within that kind of sub matrix of data, what they'll try to do is they'll try to correlate different pairs of genes with each other to see, to basically identify which groups of genes kind of are close. Which groups of genes kind of are co- or correlated in expression, right? And so, what they can essentially do is draw these network diagrams, like I'm showing you here on the bottom right, where in these network diagrams that you'll also see later, the nodes here represent individual genes and connections between these nodes represent correlations, essentially something like correlations between the expression patterns of those genes. And so, the general idea is that if you, for example, do a whole bunch of sequencing and you collect, you know, suppose that this group of cells down. Suppose that this group of cells down over here in blue were like normal cells, and these group of cells up here were, say, tumor cells. The general idea is that people would try to construct these networks for both kind of cancer individuals and normal individuals, and then try to compare these two things to get a sense of, you know, how do these covariation patterns between normal and diseased people change? And that kind of tells you something different from just looking at, just trying to find differences in activity pattern. Find differences in activity pattern. And so the overall goal that we're trying to solve here is really to find an efficient way of kind of representing these graphs, these co-expression networks, and trying to find differences between them. And so we're not the first people to think about building these kind of networks. Lots of people have thought about constructing these networks before. The two general kinds of analyses or data sets that we Analyses or data sets that we typically try to look at are, you know, deal with either something related to like development, where you might imagine there's like a process like embryo development, where a human is like kind of continuously changing with time and you kind of want to characterize how these networks change as a function of continuous time. And there's other kinds of studies where you just have like a group of like case and control individuals and you're really just trying to compare like two groups of individuals. To compare like two groups of individuals to see what the big differences are between them. And so the reason why we're interested in this problem is because people have been tackling this idea about constructing and comparing networks for many decades now. But basically, how a lot of current methods are, or how a lot of methods work is that they use these matrices and they compute correlations between rows or genes in this matrix, or do something like that, and they construct these discrete networks. Construct these discrete networks and then try to do analysis on these discrete networks. But the problem is that when you construct these discrete networks from correlation data, the structure of the network changes significantly depending on how you threshold your correlations or things like this. And so we basically wanted to come up with an alternative way of constructing and doing inference on these networks. And so the way in which we try to approach this is that This is that is basically by using autoencoders. And so we all know that autoencoders are, you know, one way to think about them is as a dimensional eye reduction method. And so for the purposes of this talk, the way that we think about, for example, the encoder of the auto of the onencoder is basically a network that basically computes some kind of like gene modules or like metagene groups where individual hidden nodes in your onencoder represent non-linear combinations of individuals. Represent nonlinear combinations of input genes. And then in the decoder, these gene modules are then used to reconstruct the rest of the expression profile. And so the reason why that's particularly important is because if we think about, again, going back to this diagram where we might have multiple cell populations, you might consider that if you train two autoencoders, one on each population of cells here, and you appropriately try to regularize, you know, so if you have, for example, two. So, if you have, for example, two auto-encoders, one for each population, if you regularize or do some multitask learning during this model training, if you learn, the principle of our approach is that if you learn sparse differences in the auto-encoder networks between these two populations, then maybe those small number of parameters that differ between these two populations might capture differences in covariation of the Of the genes. And so just to try to rephrase that a little more elegantly, basically, when you center, when you pre-process the gene expression matrix in a way such that all the genes are then kind of centered and scaled in a similar fashion, the autoencoder essentially is forced to learn covariation patterns between genes. And so we're hope, basically our hope in our work is that if we train these autoencoders in a more Train these autoencoders in a multitask way, then the differences in the parameters should only encode differences in correlation patterns of the genes. And so, by using feature attribution, we can then kind of look at those individual parameters and try to figure out, you know, what are the major differences in the structure of the networks between these populations. And so, basically, in the graphs that I'll show you on the following slides, there's basically two types of analyses that we focus on, one of which is in kind of parameter visualization where we draw Visualization where we draw scatter plots like we've looked at previously, but in this case, each point actually represents an entire population of cells. So it's not a single cell, but it's actually in some sense a whole data set. And so we try to use these parameter visualizations as a way of kind of visualizing the relationship between multiple data sets as opposed to multiple cells. And then the other kind of visualization, we actually try to use feature attribution to understand, you know, for those parameters. Understand, you know, for those parameters that are varying between the networks, what genes are they influencing? What genes do they imply are different between the conditions? And so let's see. And so we kind of, yeah, so we started out with some basic simulations where we try to simulate those two scenarios that I discussed earlier, where in one case, you might have In one case, you might have more like discrete steps or discrete differences between populations of cells in their network. And the other case in which we try to simulate where cell populations differ more continuously in terms of their network structure. And so let's see. So basically, in this first case, where we basically simulated discrete steps in terms of changes. Discrete steps in terms of changes in network structure. What we did is we basically designed three kinds of synthetic networks shown up here on the top of the slide. And for each one of those network structures, we basically sampled, simulated a data set, like a population of like a thousand cells. And so what we then did is that we trained an on-encoder for each one of these thousand cell data sets and in a multitask way. And then we basically took the We basically took the parameters that differed between each of those onencoders, and then we basically just drew a UMAP of those parameters. So again, each point represents a population of 1,000 cells, and this is visualization of just the parameter space, not of the original data, but of the parameters of the onencoder. And so you can kind of see in general that the data sets that were simulated from the same network structure basically cluster in this visualization. Visualization. And in terms of the distances between the blue and the red and the blue and the green, the distances in the actual parameter space, not in the UMAP space, but in the parameter space actually roughly correspond to the distance in the network that we simulated, which was kind of nice. Yeah, just briefly in the same way when we try to design like a continuously evolving network structure and we kind of retrain the same autoencoders on different data sets, you could also kind of see. Data sets, you could also kind of see that populations of cells from the same network, basically a cluster, which is also a nice sun to D-check. And so at that point, we kind of moved to some real data, and so the real data that we've been focusing on mostly is on this data set of IPSC differentiation. And so the general idea here is that in biology, there's kind of one of the kind of cool problems that people work on. Of cool problems that people work on is this idea of regenerative medicine, where you know, instead of if somebody needs like a heart transplant, instead of trying to find someone else that has a compatible heart, you might take some like skin cells from said individual and you might try to reprogram them into essentially first stem cell-like cells, which we call like IPSCs. Then, these IPSCs could then potentially be differentiated into some kinds of heart cells or hopefully heart tissue that you could use. Cells are hopefully heart tissue that you could use for transplantation. And so, here in this particular data set, each population is, each data set essentially corresponds to one person and all of the cells that have been sequenced from that one person. And so, this particular data set is of interest to us because for each one of these individuals, we basically know how good their stem cells are, we know how well. Are we know how well the cells that we basically reprogram from them look like stem cells? And so, what we're trying to understand is: you know, how does vary, you know, are there variations in this gene network structure that correlate with how good of a stem cell this, you know, these cell lines are? And so, that's really the problem that we're trying to address here. And so, again, just to further emphasize what we're trying to do here, on the left here, basically. On the left, here basically is a visualization of the individual cell. So, this is each point is a cell, and the visualization is of over like a bunch of cells from a bunch of people. And what we did first is we basically just removed the differences in gene activity between all of the people. So, basically, in terms of the marginal distribution of like measurements for each individual person, they should look exactly the same across people. And so, now the only differences there exist between There exists between data from different people is that the co-expression or correlation patterns differ. And so basically, what we first did is we, again, we, for each of basically 117 different people, we trained an autoencoder in a multitask way. And then here again, we visualized this. These are UMAP visualizations of the parameters of the autoencoder for each person across the 117 individuals. And so up here in the top left is Up here in the top left is basically that plot where each point is a person. And we've colored the individual based on, broadly speaking, like how, you know, whether this reprogramming procedure resulted in an actual stem cell or not. And so when you color each data set by this phenotype on the patient, whether or not it was a successful reprogramming effort, you can broadly see that the red points separate. See that the red points separate from the blue points, which is another way of saying that, um, at the kind of like cell-wide level, there's huge differences in terms of the network structure of real successfully reprogrammed cells versus not, which maybe isn't too surprising. But if you kind of look in the bottom left-hand plot where you throw away all this all the cell line or all the individuals. Cell line, or all the individuals where the reprogramming failed. And now you just ask the question: if I look at a bunch of people for which I have successfully created stem cells from their, you know, for example, their skin or whatever, how, you know, is there any relationship between the parameters of the onencoder and how, you know, quantitatively speaking, how good of a stem cell is. A stem cell is the data. So you can kind of quantify how good a stem cell is in this particular experiment based on how well you could kind of change that stem cell into like a brain cell. And so basically, if you look on this plot on the bottom left, you can see that among this, you know, among the individuals for which this experiment was successful, you can actually still see separation between the poor efficiency. Poor efficiency cell lines are the kind of worst cell lines colored in red, and the dark green or the green points, which are basically the cell lines for which this reprogramming experiment was kind of successful. And so this is kind of a bit surprising to us because the thing is that gene, like basically cells, when you take skin cells, for example, for one person, you compare them to skin cells to another person, there's a lot To another person, there's a lot of reasons why skin cells from one person versus another look really different. Like one person may have been exposed to more like UV, you know, maybe one person sits outside all day, and the other person is like a programmer and you know, sits in front of his computer all day. And so there's lots of reasons why two cell lines should look very different, but in terms of their network structure, but actually it turns out that at least according to the autoencoder parameters, one of the major factors that might drive differences. Factors that might drive differences between these people is actually their efficiency or how good their stem cells are, which was really surprising to us. And so, this is kind of the point at which we started to try to do some interpretation on our networks. And so, again, it's great that we can show that the parameters of the, basically up to now, basically our findings are that the parameters of the onencoder somehow encode the differences in. The differences in network between these different cell lines. And so, the goal that we're trying to accomplish now is: how do we actually figure out what those parameters are doing? And so essentially, what we've been trying to do is kind of look at both the encoder and look at the decoder and say, okay, can we actually figure out what gene modules are being encoded by this autoencoder? And so, we started out with the encoder, right? Encoder, right? And so basically, in these autoencoders, we've, you know, they basically have 128 hidden nodes in the bottleneck layer. And so what we've been trying to do initially is basically just ask them, you know, use, I believe it was integrated ingredients to just ask a really simple question, which genes on average, which input genes contribute the most to each of the individual 128 hidden nodes in this network. And so this heat map on the bottom here basically represents that attribution. Basically, it represents that attribution. So the rows represent each of the 128 different hidden nodes, and the columns represent the genes, and there's about roughly 2,000 or so different columns in this matrix. And so one of the most striking features of this heat map is that basically a very small number of genes actually end up contributing to the latent layer of this auto encoder. And so it's fairly easy to ask the question: well, which genes actually contribute? Ask the question: Well, which genes actually contribute to you know contribute the most to these hidden nodes? And so we pulled them out here. Um, you know, here's two examples of different clusters: cluster five and seven out of this out of this heat map. And basically, what I'm showing in these heat maps are the linear correlation in values across the matrix of these different genes. And you can basically see that the genes that contribute to the same hidden node basically are linearly correlated in the original data, which is which is kind of what you'd expect. Which is kind of what you'd expect. But yeah, so basically, our conclusion from this initial analysis was that our hidden nodes are actually basically represent covariation patterns in a very small number of genes. And so we also basically tried to look at the decoder and say, well, you know, what differences in the decoder might, how might differences in the decoder translate to differences in correlation patterns? In correlation patterns. And so this is kind of where we've been particularly struggling because essentially, again, we want to identify groups of genes where their correlation patterns or the network structure changes across these lines. So maybe they're correlated with these efficiency values of these stem cells. And so things get a little bit complicated, but basically, when you do feature attribution using approaches like Integrigarian, Approaches like Integrigarian, per cell line, you can basically get an attribution of how much does each gene on average contribute to each, or sorry, how much does each hidden node essentially contribute to each gene in the output of your auto encoder. And so you can end up with basically a matrix of attribution values, one per cell line. So you have 117 different attribution matrices. And from here, we basically said, okay, for each individual gene, Each individual gene and each individual hidden node, you can calculate the contribution of that hidden node to that gene in that cell line. And so basically for each gene and each hidden node, we basically collect all of those values, and then we correlate them against the efficiency and say, you know, for this gene and this hidden node, how correlated are those attribution values to efficiency? And then basically, you end up getting a visualization. And basically, you end up getting a visualization like this: where again, the columns represent different genes in the genome, the rows represent the different hidden nodes in the autoencoder, and the value in that matrix represents the correlation of the attribution of that gene and that hidden node with efficiency. And so this is kind of the point at which we're kind of stuck. So, basically, from this visualization, you can kind of see bands. What we kind of see are horizontal bands in this diagram, which basically Diagram where which basically kind of suggests that the same hidden node in the onencoder, like basically, some hidden nodes in the onencoder tend to oftentimes be correlated with, like the contributions in that hidden node with the genes tend to be correlated with efficiency, whereas there's other nodes where you don't really observe any correlations with efficiency. But yeah. But yeah, but we, yeah, this is this is still really work in progress, and we're just still at this phase of not really understanding or not really knowing what the best way of taking these trained autoencoders and asking which groups of genes are changing across autoencoders or across lines the most in order to try to basically interpret our results. And so. And so, yeah, that's really just to say we've applied this model to many other kinds of data sets besides these kind of stem cell data sets. And we've also, we've again kind of shown that, you know, even if you look at cases, real cases in biology where you take data where like the same kinds of cells have been sampled across development, across embryos in humans, for example, you can see. Humans, for example, you can see a very clear separation in terms of, you know, for, for example, you know, when we were looking at these liver cells, if you look at one particular kind of immune cell, you can see very clear distinction in the gene network structure between kind of, you know, the first trimester of development versus the second trimester. And so, you know, we think in summary, we think that these onencoders, you know, really are capturing big differences in terms of the gene network structure. In terms of the gene network structure, but it's still been a struggle to kind of try to figure out how to interpret these networks in a way that allows us to figure out which parts of the network, which subparts of the network are really changing the most. And so, yeah, so in yeah, and so that's basically all I wanted to cover today. So, again, most of the work here was done by my graduate student, Roshin. At one okay, I think my screen just went blank. Yeah, sorry, so most of the work was done by Ro Xing, my graduate student here in the middle, as well as Jungian over here. And it was also done with the collaboration of some other individuals in the human cell atlas. And yeah, so at this point, I'd be happy to take any questions or any suggestions on how we can better use network interpretation to pull out some genes here. To pull out some genes here. Thank you very much. Yes, thank you, Gerald. Questions? Here in the audience or online? Maybe I'll go for the question, but I don't know if it's receiving. Can you hear us? Can you hear us, Giral? Yep, I can still hear you. My screen's frozen for some reason, but. Okay, so you said you try to evaluate which genes are contributing to the hidden dimensions in your autoencoder the most. I was wondering if you did this for the multiple models with different parameters that you had, or like you just did it for one model? Yeah, so I skip. Yeah, so I skipped over some of the details here, but basically, when we train these autoencoders, what we're currently doing, anyways, is we're basically forcing all networks to share the same encoder, but we only allow variation in the parameters of the decoder. And so, when we run attribution to figure out which genes contribute to which genodes, it's basically all. It's basically on a single model which is being shared across all data sets. Then, as a follow-up question, how do you know that a single encoder would actually be the most informative? Basically, when we initially did this experiment, we actually did allow all prior We actually did allow all parameters of both the encoder and the decoder to vary. We didn't actually, when we actually tried to restrict the encoder to be shared across all networks, there wasn't actually really any drop in test set performance. And so just to make our interpretation task a bit easier, we decided to restrict the encoder because it didn't seem to make too much of a difference. Thank you very much. Can you hear me? Yes. I was wondering whether you have checked your autoencoder based analysis with respect to more traditional, let's say, partially square-based network modeling or Bayesian factor modeling, hierarchical Bayesian modeling, so on and so forth. So on and so forth. There are some works in this area. So I was wondering whether you have checked the efficiency of your model. Right. So, yeah, so I mean, yeah, so you're definitely right. There's been like 30 years of worth of network inference method development. And so broadly speaking, if you were to, so what we have done is run like a whole bunch of different classic network inference methods. Different classic network inference methods on the raw data, and then ask the question: can you identify, like, are there genes, for example, whose degree is correlated, degree across the networks are correlated with efficiency? And the answer is you can find a few number of genes, and those genes also get kind of systematically different attribution values on the autoencoder side. What we haven't been able to What we haven't done yet is basically try to use like some kind of like module detection algorithm to see whether there's certain modules in the high versus low efficiency lines that then correspond to genes that get groups of genes that get different attribution values in the high versus low lines. But yeah, it is, it definitely is something on our list. Right, you have to do the module detection, hub gene identification. Gene identification. So there are plenty of work in the area. So I was wondering, I mean, if you right. So I think there's a few problems that you can't solve with classic network inference methods. So for example, the visualization that I showed where each point represents a single data set or single network, and you visualize like 100 points at once, it's difficult to do with cloud. It's difficult to do with classic network inference methods because when you infer this ball and stick method, or infer this ball and stick representation of networks, you can't easily embed it into a visualization without doing some extra work there. And secondly, what we found is that it's really hard to find differences in network structure correlated with efficiency when you infer balanced methods because the networks are really noisy. Networks are really noisy. But yeah, we have also been extensively trying to use the classic network inference methods to do the same problem, but we just haven't been very successful. Thank you. Okay, any final question before we wrap up the modeling sessions? Gerard, have you ever considered graph neural networks? Like referring naturally to those visualization that you were mentioning, like you can embed. That you were mentioning, like you can embed the network using graph neural networks and then you can visualize it as you as you're doing. Yeah, yeah, so we yeah, so I we've definitely tried to use graph embedding algorithms to do this. I think the problem we have is that it's the construction of the discrete network itself is kind of challenging. And so Uh, yeah, it's. I think that's basically the rate limiting step that we're trying to address, and that kind of prevents us from using those embedding algorithms as well. Okay, so let's take Gerard again. Thank you. With that, I would like to close the morning session. Just to remind you that today, for the people here on site at BAM, that will be. On site at one, that will be a tour session from one to two. So, actually, to join the tour, you have to go to the front desk. It's of course important that is, but I think it's good to have. And otherwise, the lectures will start at 2 p.m. Let's try to be on time. So, yes, on time as we have on this model. So, thank you very much, everybody. 