During my master thesis I wrote a couple of years ago then there weren't that much work into evaluation, but so much has happened in the last year and so I think it makes sense to have a quick view on that too. And then the main focus of today is the library itself. So I would like to explain to you like why, the project motivation, the content of the library, what it can be used for. And also I would like to do some sort of live coding exercise. It keeps you a bit alert and I think it's usually quite fun. And then some summary. Usually quite fun. And then some summary and QA at the end. So let's go. All right, so when it comes to the evaluation problem, I think it makes sense to formulate it a bit so we are all on the same page. There's a couple of key ingredients that we can think about, and we can simplify this to a supervised learning setting, at least for now. Although I think some of this notation is generalizable across. But let's think about the supervised learning setting where we are concerned about also using some sort of future importance methods. Also, using some sort of fitch importance methods. And in this specific space, we have a model, F, very parameterized by some theta, and then that maps some input of dimension D. And D could be the size of the input image, for example, if we think about image classification, and C is the number of classes if we think about an image classification task. And in the evaluation problem, we also have some other ingredients. So we have some exponential function. It could be one or several that we would like to evaluate. Several that we would like to evaluate. And also, this expansion function: we have a generalized parameter space, and we are seeing some mapping basically that assigns a feature important score to each of the features in the dimension. And in addition to this, I see it also as some sort of generalized way to think about evaluation as we have a set of equality estimators, like that also has some other parameterization, lambda in this case, and where we have some exponential. And where we have some exponentials that lie in the same space as the input, but also we usually evaluate our exponentials over several samples. The output there is very different, and it depends on the quality estimate itself. If you are interested in evaluating, for example, robustness or faithfulness or complexity of the explanation, the output can differ. It can either be the just the flow that we output, or it can be the list of integers or Booleans, or it depends on the quality estimate or cube. On the quality estimate Q. And I think this concludes kind of the key main variables or the ingredients in modeling the evaluation outcome, I. And so we have these different ingredients that make up the problem. And it also kind of, I think, while looking at this, it also kind of, at least for me, appears to be a fairly complex problem in terms of the hyperparameter spaces that we get to work with. So we have some model. In this case, we can see that, you know, in the inner function, we're evaluating. function we're evaluating we're evaluating the function at x and so we have some sort of pre-trend post hoc explanation here going on and we also have some exponential as well as the quality estimator and I think that this is just like gets us started to to think about evaluation right but what makes you know this this problem space fascinating and why is it difficult I think in a very general sense I at least the problems that I face on a day-to-day basis That I face on a day-to-day basis in my research is that we lack ground truth labels. And of course, this is not true for every explanation system there is. We have anchors and counterfactuals and other types of explanations like decision trees and self-explainable mechanism built into our networks that allows us to kind of acquire some sort of sense of ground truth when it comes to explainability. But in the context I face, when it comes to high-dimensional I face when it comes to high-dimensional data that are typically associated with highly complex models, I consider explainable evaluation almost like an unsupervised learning problem. I think it helps me to understand how what type of method is applicable to this evaluation. And when it comes to circumventing this issue of not having ground truth labels, there's been, of course, various ideas proposed. We can think about, for example, some works that have focused on simulating ground truth labels, for example. simulating ground truth labels, for example, through different packages or or also through different types of releases of code where you kind of have a smaller toy example where you can basically assert that certain explainable evidence should be located at certain part of the input, for example. Also, there's been other works that try to kind of induce some sort of artifact into the explanation, so to kind of make sure that the explainable evidence should not be Should not be distributed in a certain area of the image, for example. We also have seen, you know, some sort of also other sense of ground truth, which is when you try to benchmark against MRI data, for example, when you can see that when we know, for example, the region in the brain that were important should be important for decision and see how much of the different explanation methods actually attribute this to this region. When it comes to circumventing the ground truth issue, The ground truth issue, this fundamental issue of explainability, I would say. Also, other techniques have been proposed, like unit testing. So, this dates back to the Adobio paper that has been quite influential in our field. There's also been some follow-up work on that, of course. Maybe the biggest category is the third one, where when it comes to measuring the relative fulfilledness against human-determined properties. And so, this list can be very, very long, but essentially, it's like, you know, oh, we want our explanations to be faithful or robust or come. To be faithful or robust or sparse or non-random, etc. And then we measure how well they are doing against this. There also has been another line of research that looks more into axiomatic evaluation that kind of are free from these empirical interpretations. Like we want our exponentials to be complete, we want them to be non-sensitive to non-inferential features, etc. Then there also is this fifth category, which is more about built-in interpretability. About you know, built-in interpretability, and so when you are kind of having this direct feedback loop into the model training process, you know, the evaluation schemes are different. Um, no, but what I would like to say in this video, okay, we have all of these different fantastic research directions, but I would suggest have it, at least based on my learning in the past year, is that we do have some growing pins in our field. Because there is no single way to evaluate exponentials, kind of every researcher has to almost reinvent the visual. We stand there. Reinvent the vision. We stand there when we have developed our method, and then we are almost free to employ whatever parameterization we want. We can also evaluate almost to a certain extent in the way we want, because not everybody's doing it the same way. It then kind of forces us to kind of rely on other types of measures like qualitative assessments. We have seen a lot of reliance on this kind of assessments because we don't really know kind of the single path forward. Path forward. But in the end, I think this can be a bit problematic that we don't have a common will or standardized will to valid explanations because we cannot kind of achieve the state of the art benchmarking, which could be quite nice to have. To my knowledge, I don't see that we have the same kind of benchmarking state-of-the-art across our expansion methods, which would be nice to have, I think, in a general sense. So let's move on. Let's move on. Right, so I think in a general sense, the motivation should be quite clear by now. I mean, I would like, I would, I wanted to make a contribution for which we could help to standardize the way that we evaluate our explanations. And quantus is just one preliminary step of that. So essentially what it is, is a software toolkit to help automate explainable AI evaluation. And so I think on the image there, I think it kind of explains to you why. Why we need it. So, you have the different images there, and you want to do some sort of comparative analysis of the different exponential methods. You have resiliency, integrated gradients, gradient shapes. And in this comparison, it's quite hard to say which one is better. And so, how could we even start saying that without using only our subjective, you know, some sort of alignment to what we expect? Maybe the fusion could seem to be more localized. Gap could seem to be more localized in its sounds, but does that actually reflect the model decision process? So, we cannot be sure. So, Qantas aims to fill this gap, or at least provide with some complementary analysis. And so, in DMC, you can see kind of some at least some type of functionality that Qantas can help with. So, on the left side there, you know, I've created this graph based on scoring, ranking, it's basically ranking the different explainable techniques on different criteria. So, we have physics. On different criteria. So we have fitful localization, robustness, randomization, and complexity in this sense. And we can get a sense of, like, and in this way, I've normalized the scores so that higher values are better. But in this very sense, we can definitely see that, you know, we can get a sense of which one of the methods are performing better in certain aspects. What we also can do with Qantas, I think, is an even more interesting analysis than the comparative one, which is essentially trying to take a deeper look at how different parameterizations actually affect the evaluation outcome. The evaluation outcome. So, what you can see there on the bar plot is seeing how, you know, based on the pixel replacement strategy, which is embedded in the fitfulness correlation test, which is one of many fitfulness tests that you have available at Qantas, you can see if I change the value in which I replace the pixels in my evaluation scheme, then how does the different methods rank? And it's far from what you would hope, or far from what you would expect. You wouldn't hope that. From what you would expect, you wouldn't hope that, and you wouldn't want that, you know, the pixel replacement strategy in the very evaluation step would influence the ranking so much. You would rather want actually the quality of the explanation itself. So, there's so much more work to do in this space of understanding how the different parameterizations affect the outcome. But this is just an example of what you can do. Right, so just to be a bit more concrete about it, what it offers today is that we have this growing list of implementations, so close to 30. If not, So, close to 30, if not 30 already, reference metrics that help to quantify explainable methods. And then, you know, I sorted this into six different properties according to similarities between the different evaluation strategies. And sometimes in the paper, it's not super clear. And so far, it seems like many papers have very different types of categorizations, but they all share some common traits. threads and and so they are categorized in that way. And you can read more on this on the readmit file on the repository. And in Qantas also we have some tutorials and you know tons of documentation for how to use the live and how to extend it for your own use case. We also built an abstract layer for popular deep learning frameworks. So we support both PyTorch and TensorFlow. And then we have also, we started off by you know developing it for you know image tasks because this is the most common one I visited and I know that other Physicist, and I know that other people do that as well. But in the recent releases, we also made it compatible with time series data. And hopefully, MMPL, we're working on the MMPL also making it compatible with that. So we hope to have that also in the next release. But yeah, maybe worth mentioning there on the table is that, you know, at least to some extent, I would say that evaluation at least were before a bit overlooked, a bit understudied compared to all the different interesting. Interesting explanation techniques that we have, evaluation has been left quite understudied. If you look at just the comparison between the different libraries of how many kind of evaluation metrics they offer, it's a big difference there. Yeah, so when it comes to the syntax, so how would you use quantus in a general sense? I will show that in the code example later, but it's basically two options. It's basically two options. Either you just evaluate in a one-liner, or you compute the scores more involved by using a function evaluate function, which allows you to score different metrics over different explanation techniques. So you can see there, for example, that we just first in the, you know, we import the library, of course, and then what we do is that we initialize the metric. So we create a instance of the metric. We specify some hyperparameter. For example, in this pixel flipping experiments, we would like to have. We would like to have, you know, the baseline, the way that we replace the pixels with black pixels, for example. We don't want to normalize the exponentians, and we would like to take, for example, 28 pixels in every step. And then in the next call, so we have now created an instance, and then we would, in the next step, we would just compute the scores. And for the computing the scores, of course, we need a couple of things. We need the model, you know, a test, some. The test, some test data, the X batch and Y batch, and L batch representing the exponentials and some hyperparameters that you might want to have in the call. And then there is also some plotting functionality that you can use. So it should be fairly straightforward how to use it, but yeah, we can show it. I will show later how it actually works. Yeah, so before we're going on to the coding example, I just wanted to mention a bit how we thought about when it comes to the software design principle. Thought about when it comes to the software design principle. So I'm into programming quite a lot and I've really enjoyed thinking about how we can best create this to be as flexible and easily extendable as possible. And so what we have in general, I can maybe easily just share the code in PyCharm, for example. So maybe it's a bit too small, let's say, but just to showcase basically the simplicity of the metrics. Of the metrics. So basically, we have this class, a metric class, and in this, we of course need to initialize the metric with a certain hyperparameters. And you know, it's very basic in a certain sense, but they all share these different hyperparameters. And then we have, apart from the initialization, we have just the call for which we are scoring. And in the model call, in the metric call, we just need a couple of ingredients, you know, the model itself, what we are, the input that we are looking at, and also, you know, the explanations that we're. At and also, you know, the explanations that we're scoring. So, each and now, in the different categories, we will have, for example, they all inherit from this metric class, and then you can define in the initialization, you know, in the sparseness metric, you know, maybe there is not that many extra hyperparameters, but otherwise you just write them there. And then in the call, you just explain here, this is the implementation of the main logic of the metric itself. And so, in this call, you essentially just go over all the, you just write the code for. You just write the code for your metric and then it will become a part of quantus. So that's it. Right. And yeah, so for example, when you are doing any type of, if you go back to this example in the syntax, for example, there is a lot of different hyperparameters to choose from here. So we have, you know, per baseline, and it might also be sometimes it's callables there. So we might want to, you know, have the flexibility to define whatever we want. So on the image there, we can see, for example, that, you know, some. For example, that you know, some robustness metrics, for example, this is the local Lipschitz estimate metric that typically relies on maybe a Spearman correlation calculation, but we can also do whatever we want in this case. We can, you know, customize it to what makes sense for our data in our use case. So we can just define it and pass it on there. And it should be from the documentation in the library, it should be straightforward how to do it. Right. So, yeah, there's a pre-printout. So, if you're interested in this work, then you can find it there. Work, then you can find it there. And of course, on GitHub, you have more the repository itself here, you have like you know much more kind of basic descriptions of what is what. Right, let's do some live coding. Right, hopefully, I have some variables still in there. Okay, let's see if I have something. And if you have any questions, you can always ask them now. Super happy to. But okay, so when it comes to just like doing some sort of you know explaining how Compass can be used, I'm sorry is it possible for you to miss the size of the phone? It's quite difficult to read online. Is it too small? Yes, I think too small to go. Is it better? Okay, it's fine. I guess. So it's better, yes. Yes, thank you. Fantastic. Okay, great. Fantastic. Okay, great. So when it comes to using Qantas, you could use, for example, pip install simply. It's available at Python Package Index. So this is, I would say, the simplest way to do it. And you will get the latest release. Of course, if you're interested in the changes that happens in between releases, we actively develop Qantas. So it's quite a lot of stuff happening also in between releases. And if you're interested in those, I would just clone the repository as is. Anyways, you can do this. Anyways, you can do this: a pip install. It's already installed, so no need to run it. Also, you know, there's just been some code here. And I will make this, if you're interested, I will make this tutorial available at the library or something like that, if you're interested. But first of all, just a few preliminaries, just some import stuff, not a little, but some stuff. Also, there is some supporting functionality that we might use. And I would like to have everything self-contained so you don't have to care about importing additional files and stuff. Care about importing additional files and stuff. So, everything should be whatever you need should be in the tutorial itself. So, what we can start with, for example, we just decide to start. And we know when it comes to evaluation problem that we need a couple of things, right? We need some data, we need some models, and we need some exponential to evaluate. So, we can start by doing this. We start by just loading some data, you know, that we're interested in. We can simply use TorchVision for this. And then we take some, you know, we take a test loader of the size of the batch size. We can take just 12. The batch size we can take is 12 here just because it's an illustrative example. And we can, for example, look at what the input looks like. So we have some, you know, doggy class and some ladybugs and airships. This is an image net data set. And so we see here, for example, that it would be cool to, you know, generate some explanations for this and then to evaluate. We also can simply use a pre-trained model for this example. So we just load a pre-trained ResNet, it's in model. And then what we can do is to generate And then, what we can do is to generate some attributions. And you can do it in any way you want, but I've created some extendable functionalities. So, there are some basic methods like sales integrated gradients and gradient shaft that is available, but very much Qantas is not an extendability library such as Captum and DiffExplan. We are very much focused on the evaluation part, but some explanation methods are built in. But you're free to use whatever method you want, and also to define your own, of course. Right, so we have some attributions of silency, integrity, gradients, and gradient chap, and then you know we could have a look at how they look like. So we use plot them and say, hey, okay, we have a little bit of a, at least for me, it's not clear if they're good or not. It's very hard to judge the quality. And it all depends on like how well you, if you normalize, how well you, what kind of, you know, setting you have in your visualizations. And it's silly to be. Visualizations and it's silly to be fooled by such things. So we will try to now just quantify them with quantus instead. Right, so when it comes to quantus, we have some available metrics. You can look at it in this way, but maybe more easily to have a look by printing them like this. So this showcase a bit like what kind of categories that we have implemented so far. And again, these categories are just like kind of kind of all of these metrics underlying here share some you know common traits but it's not I would say the black and white case where where every where every kind of metric truly belongs to one category I would say for example that the randomization category is very much related to the filthfulness category that all kind of that both checks for example like how how to what extent the explanation function follows the the the model function and randomization is more the kind of And randomization is more the kind of edge case when you randomize most of your model parameters. So I would say that you could argue for them being together, but there are certain differences, and so we keep them separate. But again, this list could be extended, and there's also been later releases more additions to the filthiness category. But yeah, let's go about, we have 10 more minutes on the presentation, so let's continue. So let's have a look at what we can do. Let's have a look at what we can do. So, when we are, the first alternative we have is used in a one-liner, we will score. So, we have this metric, top K intersection, that looks into, you know, it's a localization under the localization category, and it looks into like how many of the top K pixels lies inside the segmentation mask. And so, you have this, for example, this outcome here, and this would be the list of floats, for example. And again, Of loads, for example, and again, you can add, for example, additional. There are, for example, other hyperparameters you can use here that you can specify in the initialization. And then you just, as you've seen here, the model cost looks very simple. Like it's just the model and the data and the labels and the attributions on the segmentation masks. Right. And then we can try another metric in another category like sparseness. We check how sparse are these exponentially. And then we also get one value per exponent. And now we should. Explanation and now we're doing this against the salience explanation. So, yet we have not done any type of comparative analysis. We can also do a filthiness correlation metric, for example, and we can see how faithful the explanations are. And again, here higher values are better. I've also created these kind of warnings and information so you can understand, for example, the hyperparameters that this metric is extra sensitive to. And we also have, you know. And we also have printed some ways to point to the original publication and so on and so forth. And again, here you can see the differences, as I explained in the first slide when I defined the evaluation problem, that yeah, these quality estimators do output different things, and I think they should. But maybe we would like to, for example, you know, specify a bit more associated with our delta what this fitness correlation should do. And we can use this method called get parents to understand. Use this method called get parents to understand like what are the different hyperparameters that we can choose for this evaluation metric. So here we have them all and we can kind of twilk or I wouldn't want to say actually twilk but we should enumerate over different options here and see how they affect the evaluation outcome. But we have quite a lot of them to choose from and yeah we can so for example then we can now take these different hyperparameters and then score again. Scoring down and if something changed, if I wanted if the fertilizer changed. Takes a little longer now. I think what we did was maybe you know the number of rounds for the cell, the subset size. Yeah looks like okay, we have a little bit higher, I think. Yeah, the finishness correlation is a bit higher. I wouldn't necessarily state it as better, but because we don't have any ground truth to compare with, but but um To compare with, but but at least it differed. So you see that there can be quite a big difference between if you change type of parameters there. And then the other alternative that I talked about, we have some evaluation function in contest. And in this function, what we are specifying here is, okay, we want to evaluate against different metrics. So we create this dictionary with fitness metrics, localization, and complexity, for example. And then we specify the hyperparameters as we've done like this. And then we said, okay, what type? We also can create a dictionary. We said, okay, what type? We also can create a dictionary here with you know the different exponentials. So this contains, you know, the salience exponential associated with this test batch, as we defined earlier, and then the integrated gradients and the gradient shop. And then we get an output. So in this result here, we can create, just for simplicity, we can put it into Pampa Siddefran. And here we have a little bit of better view about, hey, okay, rather than just these visualizations as we saw before up here, like we're starting to get a bit more sense about how our expeditions are doing. How our exponentials are doing. So, what we can do, for example, in addition to this, is if we're only interested in the ranking of the different exponentials, we can just compute the ranking of them and say, okay, for example, which one ranked the highest? Which one is the better in this very sense? And again, here higher ranking is better. And then, just to show how that could summarize, similar to what was included on one slide, there, you can do something like this. Do something like this. You can compare on different dimensions the different properties and expansions. So, yeah, I think that was that of the live coding example. So, let's see if I have some more. Yeah, so just to summarize, I guess, this work, I mean, some key takeaways of this presentation and Qantas and what are some next steps. So, I mean, Qantas is basically built to help explainable AI researchers. Explainable AI researchers towards a more standardized, speedy, and transparent evaluation scheme. And basically, what I hope this is an effort is for more reproducibility in our field. And this is kind of what I'm also very interested in in the general sense, like how can we, you know, I would never argue or believe that we should have one way to evaluate explanations. There should be a holistic approach to it, but I do believe that we should at least be more transparent in the way that we parameterize our evaluation. Parameterize our evaluations and very much also report on the influence of that in our papers. And if you're interested in contribute, I would like to you are so welcome to join in on really coding. And we're already a few collaborators, some from the original contest group, and also some people who are joined externally. So that's super exciting. And yeah, for next steps, I mean, in various joint collaborations, I'm currently working on a few different things, everything related to Few different things. Everything related to evaluation, this has been my core interest since the start in my research. So I've been developing some new metrics based on all the learnings that I've gathered throughout building so many papers in this field and trying to kind of make up what makes sense. I'm also developing some sort of analyzers. So I'm interested in how we can kind of evaluate the metrics themselves. Like, could we understand, you know, how consistently they rank? Could we understand how sensitive they are to different parameterizations? Could we understand how much variance they have in their score? How much variance they have in their scores, or how lengthy they are to compute, and maybe get a sense of how good they are. Then I'm also interested in understanding more the causal links between the different variables that make up the evaluation outcome. So, this is kind of a few things that I'm looking into. And yeah, thank you so much for allowing me to speak. We can do a Q ⁇ A. Thank you very much. Okay, so we'll have a few minutes for questions. Yeah, sure. Thank you for the talk. It was very interesting. And also, I'm also wondering how active is the community? How active is that community? Yeah, how active it is. Okay, so I cannot have so much to compare with, but there is like, you know, we have since. With, but there is like, you know, we have since the first release done a few, I don't know, what is it, the third or the fourth release already. And I mean, I get fairly active, I would say. There has been quite some interest. I mean, it is a tricky problem evaluation, and just having, you know, some type of toolkit that allows you to speedily evaluate and not having to kind of re implement every metric all the time, I think it can really be helpful. But so yeah, in in general I would say it's the yeah, pretty active. Yeah, pretty active. Yeah, you mentioned that you implemented a few explainability methods in Radiance and something more. I was wondering if you are planning on having IPI for integrating this with Quantum or with other explainability libraries just to add more methods, right, so that we can evaluate the larger pool of explainability. How would we insert our like I don't know if I wanted to evaluate chat or line or how would I have how would I do it? So what you would do simply so how we have decided right now is that you would simply pass like the column, the function that you defined, so whatever exponential function you want, and there is a hyperparameter called like explain func. And this one is essentially the one where you specify that. And as long as you have similar input arguments, so like the way that Input arguments, so like the way that you define the signature and so of your exponential function is the same, then you could use an arbitrary exponential function. So I've already done this on some other methods, so I know it works. But then it might, as you said, like it might be interesting to actually build up a wrapper around more popular frameworks. But it can also be a bit more of a headache because we always have to kind of make sure we are compatible. So I'm not sure how. So, I'm not sure how if we're going to do it, but we have actually talked about it before. So, I understand the question. Thank you so much. Thanks for the talk. Maybe it's a studious question, but all the examples are using images. How we see this for researchers working on table alpha. Sorry, how easy is it to? I didn't hear you so well. Easy to, I didn't hear you so well. The sound is quite low. Yeah. Okay. So, all the examples are using that you saw is for images. How this is to implement, to use quantum for tabular data, any other data? Yeah, but that's a good. I mean, they're defined on flitching importance methods. So, tabular data, I would say that the extension would be fairly easy to implement. This is the first time someone has raised to me that they're interested in tabular data. To me, that they're interested on Tabular Delta. What I've heard, like what is already implemented since start, is the time series compatibility. And we're looking at NLP. But I wouldn't say that, in general, how it's kind of constructed right now, there would be that much of a, you know, that big of a scope to implement it. So if there's interest, then that I will create an issue on it on the repository and then we will work on it. Simple as that. Thank you. Yeah. Yeah, another question which is somehow related. question which is somehow related like how many if scenarios did you did you do you did you test quantus on in your experience and if you see like differences in the type of metrics that are preferred from you know people working in a field compared to another like different types of data different goals so and then i haven't really i think it's a very interesting line of research to try to kind of establish on preferences across the metrics within the different categories and there is this Different categories. And there's still like, you know, we have some ideas on that we need to understand how, if they're even, if they're even kind of measuring the same thing. You know, in the filthiness category, for example, we have different types of tests that are all kind of in the same group. And it kind of remains to be understood, first of all, if they're measuring the same thing and if we can actually make a recommendation for different use cases of what is the best. So I think before we even get there, I think we have to establish which are there i think we have to establish which are the ways we want to analyze them to be able to see if they are better or not like what are the different what are the different criteria in which we have to benchmark them and the different evaluation metrics so i think we need to start there and then we can at some point hopefully in the future say whether or not they are good thank you okay any other question yes okay so you have like a donor matrix which is really cool Like a ton of metrics, which is really cool. I'm wondering, because since you have so many metrics for users, it's pretty hard to choose from them. I'm guessing that since all of them is like very recent work, very few of them have been properly tested. So we talk about these analyzers that you're working on, which is great. But in the meantime, are you considering having some recommendations or more means or disclaimers or use of metrics that have not yet been Not yet been properly assessed? That's a very, very good question. I haven't, you know, I haven't, as I said, I haven't made a ranking myself for which one is better. I mean, to start off with, even implementing them was a bit of a challenge because it varies so much in how the different researchers have defined them. Some even leave out sometimes the mathematical notation for the metric itself. So, of course, there is varied quality and also there is varied assessment in how they did it and how. And how they assessed kind of the metric that they proposed. But in a general sense, like if there were so in the development process, that there was completely, you know, if the metric itself was not, how do you say, doesn't seem to be a reliable metric, then of course left it out. But it's it sounds like it might be on its place to actually do some sort of ranking exercise or or trying to do some sort of recommendations. To do some sort of recommendations for what could be the more useful metric in the different categories. But also, not everyone is kind of not all the metrics, for example, in the fitness category, can be applicable on all types of data. For example, pixel flipping or region segmentation or iterative removal of filters, all quite similar, but they all kind of focused on image data. So it is nice to have some diversity that you can choose from, I would say. I can also just quite hear it. Okay, so thank you very much. Let's thank Anna again. Thank you so much. The next speaker, so now we have Nicolas Deutschmann, which is a post-work on AI and interpretive.