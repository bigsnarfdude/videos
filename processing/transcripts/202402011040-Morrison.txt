Related application with a couple of teams. So it's an honor for me to be presenting it to you. All right. So I'm going to do a little bit of background about the application. Is Thor on this thing or Thor is on the. No, it's on that. You have to point it at the computer. I did that a couple times. Oh, yeah. No, I can't help. It's just backwards for what I expected. Okay. Expected. Okay. All right, so here are my co-authors: my PhD advisor, Ron Brookmeier, and Oliver Leindecker at Johns Hopkins. And they've been working on this area for a while, and I joined for my PhD. What we're trying to do is estimate the incidence rate of HIV. So the incidence rate is the leading edge of an epidemic. It's the rate of new infections over time per susceptible person in some particular population of interest. Of interest. We want to estimate HIV incidence in order to allocate public health resources and also to measure the effects of interventions. Hopefully, that's familiar to you all, but because not everyone's an epidemiologist. So any questions about that? Okay, great. Okay, so how do we estimate incidence rates? Well, traditionally, you use a longitudinal cohort study where you find a bunch of people who are negative, and then you watch them for a while, and you see how many people develop the disease you're interested in. Develop the disease you're interested in. We all know that covert studies are costly and they take a long time. And particularly with diseases like HIV, there are issues with recruitment and dropout. Some people might not want to be followed when they have a sensitive infection like HIV. So it can be very difficult to run those studies effectively. So the idea, especially in developing countries and limited resource settings, is that we just want to go in, take a snapshot, a cross-sectional survey of the population. Snapshot, a cross-sectional survey of the population, and get an incidence estimate from that one time point. Now, it's a little bit counterintuitive that you can get any information about time from a single observation, but under some strong assumptions, medium assumptions, you can. Okay, so here's the basic idea. We're going to recruit a representative subset of your population. We're going to collect blood samples from each participant, and we're going to test for biomarkers that indicate a resistance. That indicates a recent infection. For example, high concentrations of the virus particles are indicative of the acute phase of infection, and also if the immune response is immature, and that also might indicate a recent infection. And then we're going to do some mathematical modeling to infer an infection rate from these biomarkers. But before we can do that, we have to estimate the relationship between the biomarkers and Between the biomarkers and duration of infection. And that will have to come from a longitudinal covert study. So we're going to have to do one longitudinal covert study. You can't get around that. But if you do one, then you can use that as calibration for a bunch of cross-sectional surveys later. Yeah. So you have a high concentration of viral particles when you're that virus concentration decreases when you get other bioparticles. That's a good question. Other viral markers? That's a good question. I will show you viral load in just a second so you can judge for yourself. It's complicated because as the HIV destroys your immune system, and so as it destroys the immune system, then it's not producing itself as much because it doesn't have the substrate. I guess, you know, I'm not an expert on HIV by any means, but it's one of the reasons that CD4 were moving away from looking at viral load and also CD4 counts because you can kind of bounce. Because you can kind of bounce back into something that looks like a recent infection later on. So that's why this is sort of where we're going towards something that indicates an image her response. Okay, any other questions? Okay, so here is one of the more recent biomarkers. This is an avidity index made by a company called Bio-Rad. Avidity is how strongly the antibody binds to the antibody. Strongly the antibody binds to the antigen. And so, as you can imagine, you know, the longer you've had an antigen around, your immune system specializes. You know, it finds specialized antibodies that bind really tightly to the antigen. So we're not looking at how much of the antibody exists, because, again, HIV destroys the immune system, so your anti-lines go down, but just how specialized the antibodies that you do have are. And you can see that there's a pretty clear signal here. Clear signal here. It's the log scale. So within the first year, we're getting a pretty significant increase in specificity of the antibody binding. There's obviously a lot of noise around this curve, but it's a pretty good signal. Some of the other biomarkers we have, we actually don't stop with one biomarker, we usually use a panel. So an older avidity assay, it's a little bit messier. Assay is a little bit messier, and then here's CD4 count and viral load, which you were just asking about. So, yeah, you can see that the viral load is pretty high at first, and then it kind of bubbles off, and then it tends to drop eventually, at least in this case. Questions? This might be too clinical a question. Are any one of these panels? I was wondering, like, the overtime aspect of people getting onto effective treatment and how that might have some. And how that might have some part to play. Do you know if any of these measures in particular are differently affected by art? Yes, they all are to some larger or less extent. So one of the things that Oliver and some of the other people at Johns Hopkins are working on are trying to find biomarkers that are more that are not affected by art. We've also tried We've also tried to come up with statistical methods for adjusting for that. So, if we know what proportion of our calibration data set was on ARC, and then we're going to a new population that has different, you know, it's a different time or different place, they might have different protocols, and we want to adjust for that. But it gets complicated. Yeah. Okay, so we're going to make it even simpler. We don't want to deal with the covariance matrix of all these biomarkers, so we're going to do Markers. So we're going to do the simple thing and take the easy way out and dichotomize them. So we're going to dichotomize this one, for example, at 40, and this one at 2.8. And let's say we drop the other two. We have different ways of categorizing people with more or fewer biomarkers. These ones are nice because they're a little bit less expensive to run. And the other nice thing about dichotomizing is that we can assay one biomarker. Assay one biomarker, and if that biomarker indicates a recent infection, then we're going to continue on to the second biomarker. But if it doesn't, we're done. We're saying this person is not a recent infection. We don't even need to run this assay so we save some resources. So what we get in the end is a binary variable from these multivariate continuous biomarkers. We get a single classification that we're going to call y, where y equals one means you're in the window period. means you're in the window period or recent infection and y equals zero means you're in the later disease state and so it's the it's the intersection of these dichotomyz biomarkers that we're interested in so we've condensed you know we've collapsed this multivariate problem into a univariate binary problem that's great we're throwing out a lot of information but it makes our lives easier yeah how did you find the humble behavior oh uh great question uh I'm about to get there. I'm about to get there. I won't tell you how I defined the cutoffs, but we have a process of defining the cutoffs. So the window period turns out to be the, if this is my model of that binary outcome over time, the window period is the area under the curve. Let me go back to the previous slide and just say a couple more things. This is the schematic of what's happening. People are uninfected, they go through an Uninfected, they go through an acute infection period where you can't even detect antibodies. You don't know, you know, you can't test them for HIV. At some point, they seroconvert, meaning HIV antigen or antibodies becomes detectable in the blood. We kind of ignore this acute infection period. So we focus on, we're actually not looking at the incidence rate of infections, we're looking at the incidence rate of seroconvergence. But there's only a lag time of about 28 days. Only a lag time of about 28 days, so it's not so bad. So then they're in this early disease state, the window period, and at some point they're going to move out of it and go into the later state. And they're hopefully, if we've picked good biomarkers, never going to go back. So once through, and they stay here. And then the length, the average length of this red arrow, is the mean window period, which we're going to do by mu. So this is the key parameter that my caliber. Parameter that my calibration study is trying to estimate. So that's when I show you simulations at the end, I have one parameter that I can assess bias and standard error, and everything's great. Okay, so here's what our data looks like once we've dichotomized and combined and everything. Looks sort of like a survival curve, though it's not necessarily monotone. Questions about this? Okay, so why is the area under the curve the mean window period? Well, I need three variables and then I can explain it to you. So let's say x is the infection status. So x equals 1 is HIV positive. x equals 0 is HIV negative. There's a hidden subscript of time, so people use a stochastic process. Why is that biomarker classification? So in the window period, So, in the window period, or not, and t is the elapsed time from infection until we measure the biomarkers. One important note that we're going to use in a second is that x equals 1 if t is greater than 0, right, if the time since infection is positive, if they've already been infected, and x equals 0 otherwise. Okay, three variables: x, y, t. So, here is So here is the mean window period, which is the mean of the integral of this curve. But if you switch the order of these two, which we can do, then you get this expression. So this is this curve, the probability of having y equals 1, positive biomarkers as a function of time. And we're just going to integrate this curve out and get the area under it. And that's the mean window period. Good. So if we can estimate this model, we're good to go. It's not intuitive? I agree. If this was monotone, you could see this as a survival curve for having these biomarkers. And then if you integrate a survival curve, you get an expectation. Yes? Okay. It's a little bit weird. Um it's uh it's a little bit weirder when there's bumps, but it works. Um okay, so here's how the incidence estimate works. Uh you we have we looked at this, this is the same expression I had before. Here is what I have in my cross-sectional survey. I have the proportion of people who have positive biomarkers, and I'm going to expand this using conditional probability, log total probability, into that curve that I was looking for. That curve that I was looking for, and the density of the infection time, time since infection. And now, if I hope that this thing is effectively unit constant over time, you know, over the time when this is non-zero, then I can pull it out and take this integral that I just had and get mu times the densities. And that's almost my incidence rate, but not quite. But not quite. If this function is not quite constant, but it's linear, then you can calculate, you're actually estimating incidence at some point in the past prior to your survey. And you're getting like a weighted average of the past incidence rate. But if it's linear, then you can calculate exactly how far back you're unbiased. And we call that a primary shadow. Go ahead. Call that corner. Do you say t is zero when x is? Let's see. So if x is zero, they're not infected yet, which means that t would be negative. Because t is time, time to infect, it's the infection time in reverse. So it's a little bit tricky to think of that. Okay, so it can be negative? T can be negative, yes. What's the probability that t is zero? That t is zero. Oh, no, no, so that's a useful line. This is a, yeah, this is a density. This is not a mass function. Okay. Yeah, sorry. Okay, so here's how we get our incidence rate. We're going to divide the number, the proportion in our sample who are biomarker positive by μ to get rid of. To get rid of this term. And so now we have pt equals 0 in the numerator. And then in the denominator, we're going to add the probability that x equals 0, which, as we were just saying, is the probability that t is less than or equal to 0, meaning you haven't become infected prior to the time when the survey happened. And that just turns out to be this conditional probability, which should look familiar to you as a hazard function or an instance, right? That's the instantaneous. It's the instantaneous density of infection given that you haven't already been infected. Question? Yes. So that last line there, where you get the approximation just above. So you get there by assuming that the probability that T z is everywhere. That's right. Which it has to be everywhere that this expression is non-zero. So if we go back. Is non-zero. So if we go back to our curve, at some point this curve bottoms out and then it doesn't matter. If that's not exactly true, but if it's a linear function, then you're looking at incidence sometime in the past. But we can quantify how far, how lagged your estimate is. That's a parameter we call the shadow. But I'm skipping over that because you've probably noticed there's no sensored covariance yet. So this is all just a buildup to why we need to. This is all just a buildup to why we need to work with sensored covariates. Okay, so the sensored covariates are sneaking in right here. That I don't actually know in my data set, my longitudinal data set, when someone gets infected. What I actually do is I enroll a bunch of people who are negative in a cohort study. So that means they're left truncated. They can't have been infected prior to the study start. And then you check them for HIV every six months. Check them for HIV every six months or every year, and at some point they have their last visit where they were negative, and then they have their first visit where they were HIV positive. And seroconversion happens sometime in that censoring interval. But I don't know when that was. Once I find that they were HIV positive, then you start measuring biomarkers. So all these blue dots are the times when we measure biomarkers. And then the duration of infection is O1 minus S, O2. O1 minus s, O2 minus S, O3 minus S. But I don't know S. I only know L and R. So here's the sensoring problem we have to deal with. Okay. Questions about this schematic here? Okay. So the time that you're using, say that you are in O5, it would be that time minus S. That's what I want, yes. Unless it's the Yeah, what I don't understand is the idea that T can be negative. Ah. So because, okay, if you're not infected, you have this, I mean, this interval, right? Right. And so what is the mean of T? Well, so I fudged things a little bit. There's T in the cross-sectional survey, and in the cross-sectional survey, Cross-sectional survey. In the cross-sectional survey, some of the people are not affected yet, so then t is negative. The data set that I'm going to use to fit my curve that I've been showing you, this one, I'm only going to use people where t is positive. So all the t negatives are out because they don't have the biomarkers I'm interested in. And I'm not interested in this. I mean, this distribution should be flat. It should be bottomed out before t equals zero. It doesn't. Well, it isn't exactly kind of. Yeah, there's. I'm being a little bit fast on this with the notation, and I apologize for that. Okay, so I have this problem that I don't know s, so I don't know t. Here are three options. You can use the midpoint of your interval, which you might get away with if the intervals are pretty narrow. Because the intervals are pretty narrow. You can use uniform imputation and do some kind of multiple mutation process. Or you can do a latent variable model where you postulate some model for the distribution of SARA conversion dates. And then the observed data likelihood is an average and expectation over the complete data likelihood, which you can either try to tackle by direct maximization or you can do something iterative. Can do something iterative, which is what I'm going to do on Dr. Gomez's example. So, again, this is highly dependent on this previous paper. Any questions about these three strategies? And then I'll explain this one in detail. Okay, so the idea here is: I'm going to use an EM algorithm to maximize. Going to use an EN algorithm to maximize that marginal likelihood, which I've decomposed. So, this is what I want. I want to find the maximum value, or the value of theta, that maximizes this marginal likelihood. But this is difficult to do because I have the log of an integral. And so when I take the derivatives, I'm going to have a problem. This thing is going to be a problem. It would be. It would be easier if I switched the order of the log and the integral, and the EM algorithm allows you to do that. You now are going to have your prior estimates of your parameters, which you're going to use as the integrating function, and then you're going to update these parameters and then do this again. Right? If you haven't seen the EM algorithm, I'm not going to explain it today, but that's how it works. But that's how we can make it even easier if we can replace this integral with a finite sum or discrete sum. And so we're also going to do that, which again, I stole directly from the previous paper. Okay, all right, maybe I should pause here. Questions? Okay, so I'm going to use a discrete model for my Sarah conversion dates. So for example, Dates. So, for example, I'm going to say you could have your Sarah conversion date on May 1st or May 2nd or May 3rd. It doesn't have to be a grid of days, it could be a grid of weeks if you want, and I will show you that it still works. But there doesn't seem to be any harm in using this fine of a grid, except for computation time. I was just talking to Dr. Gomez earlier this week, and she suggested some ways to reduce the number of points I actually have to. Reduce the number of points I actually have to check. So that might speed things up quite a bit because this does take a while. So this is what I'm going to get. This is where I'm going. These really should be bars because it's discrete. A model of the possible SARA conversion dates for the whole population. Note that I'm not using the density, I'm using the hazard function or a discrete hazard function. And that's because of the left truncation that I meant. The left truncation that I mentioned at the start. In my study, everyone has to be negative. And so it's a lot easier to work with hazard than with density. So I'm going to have this parameter, omega s, which indicates the probability that you have the event on day s if you haven't had it already. Okay, if I have this hazard model, I can convert it back into a density model by just multiplying. By just multiplying the probability that you don't have the event prior to a particular date, and then the hazard of having it on that date, divided by once we're in the interval, we know you don't escape the interval. So this is one minus the probability of escaping the interval. So this is a pretty straightforward, you know, discrete version of what you'd see on survival. But there is a lot of notation here, so maybe I'll stop. Okay. So I need some assumptions. I need the individuals to be independent. I need the HIV test dates to only depend on the enrollment date. So that gives me non-informative censoring. And I need the follow-up dates to only depend on the date when infection was detected. So when you have your first HIV-positive test, we're going to schedule your participation in the study and we're going to measure you. In the study, and we're going to measure you for a few weeks and then for a few months and then maybe once a year, and then you're done. So everything is scheduled. You're not coming in because you're feeling sick and getting a test. That obviously could be violated in practice, but we're going to assume that for now. And then we're also going to assume that the biomarker values y only depend on the time since infection, so that I can reduce this complicated. So, that I can reduce this complicated expression to a very simple expression. Not even written here is that I also want there to be no random effects. But one of my colleagues back at UCLA is working on models with random effects. So that's coming, but it's not in this work. Okay, so once you do that, then you run the EM algorithm, which, if you're not familiar with it, first you have to If you're not familiar with it, first you have to get those weights. So you calculate the distribution of the variable you're missing given everything you can see, which turns out to be just an application of Bayes' theorem. So this is what it looks like schematically. The blue is my likelihood of my outcome. Green is the prior probability. And then red is the posterior probability. Pretty straightforward. You can see that the posterior is. That the posterior is being pushed upwards, pushed over here by the likelihood. So everything kind of plays together here. Questions? Okay, and then we have an M step, which also turns out to be pretty simple. You update the omegas, that's the distribution of your Sarah conversion dates, as the sum of the probabilities that you have an event on the day you're interested in. That's how you. You're interested in, S, divided by the total probability that people are still at risk on that date. So, this is a lot like Kappa-Meyer curves, except I don't know exactly who was at risk on a given date because some of them are in their censoring interval. So I have to add up probabilities instead of just counting people who are at risk. And then my update for my regression model from Adcom, it turns out to be just a weighted generalized linear model where the weights are coming from the E-stat. Come in from the E-Stack. So this can be, you can just plug this into GLM, or maybe you need to use big GLM because there's a lot to. This is existing software, this is direct calculation, and you're done. And then you repeat this until you get convergence in the likelihood, which we can compute the likelihood, so we can check for convergence. And that's the, you get an MLA out of that, or at least a local maximum. If you're not, the way the E-Mouter works is it help climbs, but it has no guarantee of finding a global maximum, so you have to be careful. Okay, so where are we? We have like just a few minutes left, right? Oh, I started at 10.40, huh? Okay, great. Okay, so I'm going to show you that this all works with a simulation. I'm going to define a data-driven model where I know I'll put Define a data generating model where I know all the parameters, so I know this area under the curve that I'm looking for. And then I'm going to generate a thousand data sets for each of 28 scenarios. And those scenarios vary in cohort size, so either 4,500 or 100,000 participants. This is realistic. This is to check large sample properties. But only 5% of the individuals are going to be at any risk of infection. So there's latent heterogeneity in my population. Heterogeneity in my population. And so I'm going to pick a random 5% of my subjects, and those are the ones who are going to, I'm going to generate a SARA conversion date and then some biomarker data for it. The rest of them are just going to stay HIV negative the whole time. So they immediately drop out. I don't actually keep track of them. The mean interval between HIV tests is either three months or a year. And these are both scenarios that I saw in my actual data sets. In my actual data sets, and then that the hazard functions are going to be one of these seven hazard functions, which rather than reading numbers, let's just look at them. So I have a bunch of relatively small hazard functions, and then a really big hazard function to see if I can break things. Importantly, some of the hazard functions start at zero. So the idea that the affection of the epidemic is just beginning when I start tracking the study, which isn't really realistic. The study, which isn't really realistic, but it does have an important property, which is that because we have non-informative sensoring, the conditional distributions of the serro conversion date within the sensoring interval is going to look like a truncated version of these unconditional densities. And so if you start at hazard zero, then you can occasionally get zero conversion intervals where the density is increasing. Otherwise, you're almost certainly going to get a decreasing Mr. Bianca. Increasing. That's your idea. So these look like, so on the magic, those are all linear functions of T. Why do they look like curved? Okay, these are the hazard functions. And if you go from hazard to density, you get curves because you have to get the cumulative hazard and take the exponential of minus that and multiply by the hazard again. And then, right, so it's a non-linear transformation. Makes sense. Yeah. Linear transformation. Makes sense. Yeah. Okay. So this, the orange and green curves are going to be particularly interesting because we want to make sure that this doesn't just work when you have strictly decreasing densities. I just threw in the survival functions for completeness. I don't have much to say about them, but here they are. And that's the end of the simulation setup. So if you have questions about it, now's the time. Okay, so here are our results. There's obviously a lot of results, so I'm going to highlight the ones that I think are interesting. We have our three methods, remember midpoint imputation, uniform imputation, our joint modeling procedure. We have our two different spacings of HIV tests, and then we have all our seven hazard functions. What you can see. What you can see is that, especially with the very extreme hazard functions, we get some pretty severe bias in these two methods. But with reasonable hazard functions, things are not too bad. And that's not too surprising because if you take any density, like these densities, and then take a very narrow window, everything will look flat. Take anything and chop it up, it looks flat. So uniform and mid-person. So, uniform and mid-point amputation are actually not that bad in a variety of scenarios. But there are cases where they break. And they're particularly bad if your window, your testing window, is very wide, because then the curvature does matter. So then you can see that they're pretty severely biased. Power model, and because I'm showing it to you, it shouldn't be a surprise that it works pretty well. Except here, we have a little bit of bias. We have a little bit of bias, but obviously nothing compared to these other two. And we also have some standard error inflation, but not nearly as much as the bias. So the bias is the dominant square error. So this is my finite sample results. Questions about this, and then I'll show you the large sample results. So with large samples, these numbers stayed pretty much the same, and we were relieved to see. And we were relieved to see that the bias in our method went away. Still do have a little bit of extra standard error here, but I think it's a reasonable price to pay overall. This is a pretty extreme, unreasonable scenario that I don't think you'd run into, so I'm not too concerned about that. One thing we tested is whether we have to fit a grid of one-day spacings or whether we can make it the grid. Things, or if you can make it the grid one week or one month, or I guess this is six weeks. And it turns out that you can, under most hazards, you can do pretty well for our model. If you have a very extreme hazard, then you do kind of need a granular grid. And also, if the window is wide, then you need a granular. And then the last thing we tried is we didn't actually have the enrollment dates in our data set. Dates in our data set. So I didn't know how to deal with this left truncation problem. So we hoped that maybe we could just assume that everybody enrolled on day one and be done with it. But no, that doesn't work. You do need to adjust for left truncation. And so that's it. These two methods usually work, but they can be biased. The joint modeling method works pretty well, and this is published, and there's Uh and this is published, and there's there's code available. Uh yeah, that's it.