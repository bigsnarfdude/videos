Okay, thank you. We'll keep the C Boulder going this morning and I'll try not to hold you too much before lunch. My name is Shay. I'm a graduate student in the Applied Mount Department at C Boulder. And today I'm going to be talking about covariance propagation from a continuum perspective with the ultimate goal of trying to understand covariance loss, particularly in the context of effective systems, which is highly relevant to everybody here. Here. So we'll go ahead and begin with an introductory discussion. Data simulation is often formulated as a discrete problem. Therefore, when we consider covariances, we consider them as covariance matrices. And when we think about their time propagation, it is done in a discrete sense with this flooring example given here. Now, it is very well known within the community that covariances tend to suffer from spurious loss of variance, where the estimated variance Variance, where the estimated variance underapproximates the exact variance. And this can be very problematic. If this is not dealt with, this can lead to filter divergence or ensemble collapse on some of the schemes. And there is a very active body of work out there trying to deal with this variance loss problem. Now, one of the approaches, which is probably the one most we are thinking of, is variance inflation, where the covariance diagonal is artificially inflated, either with a multiplicative factor. Either with a multiplicative factor or an additive factor to try to combat this variance loss. However, there has been more recent work looking at the impacts of numerical discretization, particularly in the context of the covariance propagation, and how discretization errors can contribute to variance loss as well. Now, in this presentation today, I'm actually going to have us take a step back from discrete space and return to the continuum. To the continuum. And in particular, look at the underlying continuum covariance propagation itself to try to gain an understanding of these underlying variance loss that we're observing, particularly in this propagation step. And so we'll return to the continuum dynamics that govern covariances, and we'll actually gain some really interesting results and important insights, not only into variance loss, but also just covariance propagation in data sets. Covariance propagation and data simulation in general. Now, this presentation is broken down into two parts. In part one, we will be studying the covariance propagation and the continuum for various systems. And we'll actually identify a discontinuous change in dynamics. And then in part two, we'll see how this discontinuous change and the continuum dynamics impacts what happens in street space. And in particular, we will derive the dynamics being approximated along. Dynamics being approximated along the diagonal and relate to this distance and this change in the continuum behavior. And I'll just preface by saying that part one is published work and then part two is in preparation and should be submitted home playing it. So we'll go ahead and begin with part one, which is looking at the continuum covariance dynamics. So we'll go ahead and start with the problem setup. So we'll be considering states Q on the surface of the sphere of radius R that satisfy this generalized Satisfy this generalized effective dynamics. So we have Q satisfies this equation here where V is our deterministic velocity field and B is a scalar. If we take B is equal to zero, we get the standard transport equation. And if we take B is equal to the divergence of the velocity field, we then get the continuity equation, which represents mass conservation. Now, from the same equation, we can go ahead and derive the covariance in the usual way given here with the expectation. Way given here with the expectation operator, and then the overbar is just shorthand for the mean state. And using both the state dynamics and the equation for the covariance, we can do a little bit of math and derive the corresponding continuum covariance equation, which is given in this box here. So what we can see right away is that the covariance satisfies the same set of dynamics as the state. However, it's now in two spatial dimensions instead of one, where the subscripts one and two denote the two spatial variables. 2 denote the two spatial variables x1 and x2. And what we'll see is that the behavior, particularly in the spatial variables here, will have an impact on the dynamics that we observe along the diagonal of the covariance. And we can actually get a lot of useful information by just playing around with this PDE here. Now, like I said at the beginning, we are interested in dynamics along the diagonal and trying to understand variance loss. And so in this slide here, we'll really be focusing on the dynamics. slide here we'll really be focusing on the dynamics along this hyperplane x1 is equal to x2 which is where the variance lives. Now what's interesting is that to really obtain a clear understanding of the behavior, we're actually going to look at covariance operators, which is given up at the top here. The covariance operator is an integral operator here over L2, where the kernel of this operator satisfies the covariance information given here. And by using this operator formulation, we can actually get a very clear Formulation: We can actually get a very clear understanding of how the solutions to this equation depend on the initial covariance, and we'll give this discontinuous change I keep talking about. So there are two cases to consider that depend on the behavior of the initial covariance. The first case is when our states are initially spatially correlated. So our initial covariance in this case is the product of some initial standard deviation function, some correlation function. And if we follow through, we find that the dynamics We find that the dynamics along this hyperplane satisfy the variance equation, which is given here. And this is exactly what one would expect. In fact, you can derive this variance equation directly from the state equation using a definition of the variance. Now, this is something that we expect. However, there's another case to consider where some interesting behavior occurs. And that's when the initial state is spatially elevated. So in this case, we will represent our covariance as a product of a continuous function p dot c, and it function P not C and a Dirac delta. Now because our initial covariance involves this Dirac delta we actually have to consider weak solutions to the covariance PDE which is why this operator formulation is very useful. And so when you work through this operator notation very carefully you actually uncover a different set of dynamics entirely. And those dynamics are given here which I will work in the second equation or PC because solutions to this equation Because solutions to this equation actually describe the continuous spectrum of the covariance operator. But the main point here is that if you work through this analysis very carefully, we uncover this discontinuous change in dynamics as correlation lengths tend to zero along this hyperplane. Meaning that when correlation lengths are non-zero, the dynamics along its hyperplane satisfy the variance equation, as one would expect. However, when our initial correlation length is However, when our initial correlation length becomes zero, the dynamics change discontinuously to that of the continuous spectrum equation here on the right. And these two sets of dynamics are distinct in the case that the divergence of the velocity is non-zero, which is what is most common in physical situations. And so this is a very interesting result from the continuum. However, the question then becomes, how does this discontinuous change in the continuum impact what we observe in Impact what we observe in discrete space because, at the end of the day, we have to be propagating these numerically in computers. And so, in the next few slides, I'll kind of give a taste of that in a very simple one-dimensional example. So, here in these next few slides, we'll be looking at a simple one-dimensional continuity equation over the unit circle with a spatially varying background velocity field. And what we're looking at here is the solution to the exact solution to the variance equation in black dashed. Equesion in black-dashed and the continuous spectrum equation in solid brown plotted on the unit circle. And what we can see before we move on is that these two solutions are different because the background velocity field does vary in space. So the question then becomes, if we go in numerically and we propagate covariances and we shrink their correlation lengths, how is this change going to impact what we see along a diagonal of these matrices? Matrices. And so that's what we're going to look at in the next slide. So let me explain a little bit what these additional curves are. So, what I have done is I've taken four different initial covariance matrices that are constructed using the standard Gasmari-Kolm combatly supported approximation to a Gaussian, taken those matrices with different initial correlation lengths, and I propagated them forward in time using the full rank scheme, the Crank-Nicholson scheme. And then at this time, capital T, I just extracted the diagonal. Capital T, I just extracted the diagonal of those matrices and plotted them here in these four different colors along a unit circle. And so as we move in the legend from right to left to right, we're decreasing the correlation that's down, initial correlations down to zero. So what we can see right away is that the diagonals extracted from this full-range propagation are smooth, but wholly incorrect. All of these curves, with the exception of the pink one, should be approximating the variance solution. Should be approximating the variant solution in black-dashed, and we can see very clearly that they're not. And the pink curve should actually be approximating the continuous spectrum solution in brown, and that isn't doing a very good job either. And what's also very interesting is that we don't just observe variance loss. If you look at these curves, we also observe variance gain as well. And that can bring its own set of problems in the context of the isolation, in addition to variance loss. To variance loss. And I just want to reiterate that this is a full rank scheme that is doing this. And so if you're doing this, say, at low rank, not only will you have these problems, I suspect, but then you also have the low rank issues on top of that as well. But what's very interesting about these results is it suggests that this discontinuous change dynamics is impacting the numerics somehow. These solutions, it's hard to tell what they're approximating. Maybe they're approximating a linear combination of the two or something like that. The two, or something like that, but clearly something weird is going on. And so I'll conclude this part by saying what I just mentioned: that we've identified this discontinuous chain of dynamics in the continuum, and it's clearly causing problems in discrete space. We have seen variance loss and we see variance gain, but it really begs the question: well, what is this MMV transpose actually trying to approximate a logger diagonal? And so that leads into part two, where Part two, where I will dive into this a little bit deeper. Now, before I get into the nitty-gritty, I just want to think about what covariance propagation is doing from an intuitive sense. So, we can look at the equation for the propagation given here, and the reason why I've been writing it in this way instead of the standard MPM transpose is because it makes the behavior a little more explicit. So we can see that this action onto the columns and the rows. Onto the columns and the rows will imply for any given spatial discretization that you're going to be approximating the diagonal elements with off-diagonal quantities. That is just a fact when you discretize in space. And you can kind of get a flavor of what that looks like in this very simple schematic here. We're using off-diagonal elements to approximate the diagonal elements in some way. So the question becomes, if we're using off-diagonal elements, If we're using off-diagonal elements to approximate diagonal elements, how does that impact what is evolving along the diagonal? And so you have to keep in mind that I just showed you, there's this discontinuous change in dynamics along the diagonal. And so if you're using off-diagonal information to approximate the diagonal, that is necessarily going to cause some problems. And so in the next section here, I will dive in to see, well, how does this approximation impact the Approximation impacts the dynamics of my order to do this, I'm going to again consider a simple one-dimensional example where the mathematics is actually tangible and you can actually see pretty explicitly what's going on. And so for this, we'll consider again the generalized invection equation that we had before. This is on the one-dimensional unit circle, and I've written this in flux form because this would be one way that you would implement this numerically in order to. That you would implement this numerically in order to conserve different conservation properties, mass conservation, for example. And so, what I'll go ahead and do is we'll apply a first-order upwind spatial discretization to the flux term bowling. And so then, as a result, we get a system of OPS and time, and I'll refer to this as a semi-discretization for the state. In that time is still a continuous variable, but we have discretion. Variable, but we have discretized in space, and that's really the impact that we see on the diagonal dynamics. Now, from the state equation, like we've done before, we can define the covariance in the usual way, and when we take i is equal to j, we can then derive the corresponding semi-discretization for the diagonal dynamics given here. Now, what I want to point out is this term in red. So, this is what I was alluding to. So, this is what I was alluding to before on the previous slide: is that we have discretized in space using this standard first-order scheme. And when you follow through with the math and you look and see what's happening on the diagonal, you're actually averaging across the diagonal with these two elements here to approximate a diagonal element. So you're using off-diagonal information when you're trying to propagate the information along the diagonal. And so, what's very interesting is that this averaging across the diagonal. That this averaging across the diagonal maybe looks very benign, but actually has very important implications when you actually sit down and do the error analysis. And so that's what we'll be looking at on the next slide here. So before I show you the results of the error analysis, I'll just ground you into dynamics I was talking about earlier, where we have the variance equation here on the left, which is written in flux form, and then the continuous spectrum equation on the right, which again is the case when the initial states are spatially uncorrelated. Initial states are spatially uncorrelated. Then, if we go through careful error analysis, particularly of that averaging term, we end up getting approximated dynamics that look like this. So there's a lot going on here and some interesting behavior to digest. So when you do the error analysis, I've rewritten things in terms of a correlation length L, where I'll assume my covariance is at least twice continuously differentiable. Continuously differentiable. And if you work through that very carefully, you come up with these coefficients that are the ratio of your grid spacing, delta x, to your correlation length. And so when correlation lengths are sufficiently long, these terms in red are equivalent to zero. This term in blue will also go away. And what happens is when you combine the first and the second lines, you actually will recover the variance equation, which is what The variance equation, which is what we expect to see. It's well known when this correlation lengths shrink, variance loss gets worse, but for long correlation lengths, it works well, and that is verified with this analysis here. Now, what's interesting is when the ratio of the grid spacing to the correlation length becomes order one. Because what happens in that case is that these terms in the brackets here effectively go to zero. So we have that this flux term drops out, we have this additional zero. Drops out, we have this additional zeroth order term drops out, and the dynamics completely change, which is verified with what I was seeing numerically, and is a consequence of this averaging across the diagonal and performing very careful error analysis. Now, in the first order up one scheme, we have this additional term here in blue, which looks a little funky, but I've rewritten it in a way to kind of make it very clear what's going on. When the ratio of the grid spacing to the correlation length. The grid spacing correlation length is order one. This term here is actually order one over x. So this term can actually get very, very large and acts like a dissipated like term. And so what can happen is that in this first order optical dynamics, you're seeing variance loss because it's being dominated primarily by this dissipated like term that can maybe overshadow this change in dynamics that's being impacted on these terms in brackets here. On these terms and brackets here. Now, when I went through this, I thought, okay, maybe this is just something very specific to the numerical scheme. And my suspicion is that, unfortunately, it's not. On the top here, I have, again, what we were looking at before in the first-order upland scheme, but I also performed the same analysis for a second-order centered difference scheme. And what I find is that the results are nearly identical. We still have these same problematic coefficients that involve a ratio of Coefficients that involve a ratio of the greatest result to the correlation length. However, this dissipative like term that we observe in the first order scheme actually gets pushed up to higher order in the second order scheme. So you don't have this dissipative like term, and instead in the second order scheme, the errors are really dominated by these delta x squared over L squared terms that change the dynamics completely, which jives with what we saw in our numerical experiments. Jives of what we saw in our numerical experiments before, where we not only see variance loss, but we also see variance gain. That is driven by this change of dynamics here. Now, a question that you may have at this point is, well, okay, how large is this delta x over L term? You know, how big can that be? And is this something that if we just increase the spatial resolution, won't be a problem anymore? So I looked into that a little bit, and that was being summarized in this figure here. For these dynamics, Here. For these dynamics, we actually have a specific explicit equation for the correlation length, which will evolve over time because of velocity field dust vary in space. And what we're looking at in this figure here is a time series snapshot of this coefficient here, this delta x squared over 8L squared, for two different initial correlation lengths in the rows, and then two different spatial discretizations in the columns. And so what we can see here is that one, this See, here is that one, this quantity evolves over time because the correlation lengths evolve over time. And this can actually be a significant quantity. Particularly in the panel C, we start with a very short initial correlation length and a modest spatial resolution, and this quantity here can reach values upwards to 1.5. And remember, this quantum, we have a 1 minus this quantity, so this can become very significant. And then what's interesting is you may think that. And then, what's interesting is you may think that, okay, I can increase the spatial resolution, that may help. I've done that here in the right column. And even by increasing the spatial resolution, that may not be a viable solution. One, in terms of computational expense, but the second is that this ratio can still be significant enough to cause problems in aerodynamics. And in particular, this problematic term in the upwind scheme, what I'm showing here is delta x. What I'm showing here is delta x squared over 8l squared. So then imagine that multiplied by 1 over 2 delta x. That term can get very, very large very, very quickly. And I suspect can drive what looks like variance loss in the first order type scheme. Now, I started out in this work trying to derive a correction to the full rate propagation. First by trying to understand what is actually being approximated on the What is actually being approximated on the diagonal, and then from there trying to back out a correction based on what we understand is happening. And that's kind of summarized in this figure here, that the approach that I took was say, okay, we have this averaging term. Can we increase its accuracy by adding higher order terms to that? And that is what is represented in these dotted curves here, trying to approximate these solid curves. And the general conclusion is that, unfortunately, And the general conclusion is that, unfortunately, that's not as straightforward as one would think. There are cases where higher-order approximations do work pretty well, but other cases, as you can see particularly in the bottom half, that don't work very well. However, though I didn't come out with a very clean solution from this approach, it doesn't mean that there are other solutions that may work out there. And this also just provided some very important insights mathematically to what covariance propagation is doing in this. Covariance propagation is doing in infective systems, and how this behavior impacts the variance in particular, and what that implies about variance loss in general. So I'll go ahead and conclude with the main takeaway from part two being that when we approximate the diagonal with these off-diagonal elements, it alters the dynamics that are being approximated along a diagonal. And these dynamics depend. And these dynamics depend on the ratio of the grid length to the correlation length, and it can have a significant impact on what's actually being observed along the diagonal. And in particular, it can lead to not only variance loss, but also variance gain, because you're changing the dynamics curve. And so I'll just conclude this presentation with a question, and that covariance propagation is essential in data simulation, whether you're doing it at full rank in a column filter. Doing it at full rank in a color filter. You're approximating it at low rank in ensemble-based schemes, or it's being done implicitly in a variational scheme. But fundamentally, it boils down to this kind of NPM transpose behavior here. And so, though I've really studied this work in the context of deductive systems, it kind of lends to a broader question, as Rasha talked about earlier this morning, of what this impact of approximating the diagonal with off-diagonal elements, which will happen when we. Elements, which will happen when we discretize in space, what does that imply about what's going on along the diagonal in the broader data simulation community and schemes that are being implemented now? So effective systems is a nice problem to work on, but this does have bigger implications that theater is. So I think I'm good, and this is the paper that is published for part one, and I'm happy to take any questions. Well, I think my understanding is the problem is because of the numerical scheme for the adlection, right? You mean for so it's kind of like a numerical diffusion introduced by the discrete form of the direction? So I hesitate to say numerical diffusion because what it really boils down to is this averaging term here. So I have an exercise that will Here. So let me, I have an extra slide that want me to make it a little more clear why I don't call it diffusion, though I understand why. So this slide I think may answer your question. So if we go back to the semi-discretization for the diagonal, we're looking at this averaging term here. And I want to see, okay, well, what is that actually approximating? And the natural choice is it's averaging across the diagonal to this half grid point here. So you're not even actually approximating one. So you're not even actually approximating on the grid, you're approximating on the half grid. And when you go through this Taylor expansion, and you do it very carefully, you actually end up getting an equation that looks like this. So you're approximating two times the quantity on the diagonal, and then you have this term here. So this P2 term is a linear combination of second derivatives of the covariance along the diagonal. So that's why it kind of maybe feels like a diffusion term, because second derivatives are typically associated. Because second derivatives are typically associated with diffusion. However, if you recast what P2 looks like in terms of diagonal quantities and the correlation length, it actually takes a different type of form. So it's not quite just diffusion. You have this kind of like diagonal quantity, which I would argue is probably like the diffusion that you're thinking about. That you're thinking about. But in terms of magnitude of these two quantities, this quantity is, we can sufficiently say, is small if you assume enough derivatives. This guy can get big and large, and so because of the negative sign, and then when you go back here, this can, it's not. This can, it's not quite classical diffusion where things are just kind of going away. It's a little more complicated than that. Does that hope answer your question, Louis? Right, so the error is from you use the variances to isolate the variance so that the error is always here regardless of the order of the information. I suspect that it is. I've looked at these two seams because it's actually tangible to do by hand. Deriving this is actually quite involved. But because I've seen this occur in two different schemes that behave differently numerically, I would suspect that yes, you would see something like this in different schemes, but what it really, what it boils down to. It really boils down to is this averaging term, which I don't have it on the slide, but you actually obtain in the second-order decentered difference scheme a pair of averaging terms. So not only do you have this one that approximates the half-grid at minus one-half, you have an additional one that's on the plus one-half grid. So even though the schemes are different, they're producing these average approximations in slightly different ways that are consistent with the behavior. Ways that are consistent with the behavior of the scheme. So I would suspect, yes, that it is, it would occur in upper schemes. Or you did some better interpolated terms, like including lighter sensor, more covariance to interpolate variance so that hair order. In terms of including more information off of the diagonal, I Um I have not looked at that because it gets pretty tedious. Um but I wouldn't rule it out. However, you have to keep in mind that by positive definiteness and also the fact that the correlation is the maximum on the diagonal, you know, you're still trying to approximate a maximum with off-diagonal elements. So that's definitely one approach that could be taken that I haven't looked into. Approach that could be taken that I haven't looked into. But I just have a feeling that because of the center difference, the first order schemes are producing identical quantities, that you might still be running into the same problem. But the goal of this figure here was to try to improve the order of the average approximation with higher order terms, whereas P1 and P2 effectively come from a Taylor series expansion. Come from the Taylor series expansion. So adding back those Taylor series terms. And as you can kind of see with these dashed curves, sometimes it works, sometimes it doesn't. So that also would give me a suspicion that adding more terms. I have a question. And Wes has a question. Yeah, Marie as well. So let's. Let's do one more question and then at 1:30, we can continue. Possibly you launch too, I don't know. Yep, go ahead. I can bug her anytime. Okay, just for me, it's just a remark. And I see things a little differently, okay? So basically, you're using the flux form to all. All the infection. You don't need to do that. You can do just the infection as a semi-lagrange. Wait a minute. And then use this B term as the uncertainty on the emissions. And that's, I think, a practical way of attacking this problem. And yeah, that's practical way. And so this is just a remark, so maybe we'll discuss that after. Thanks, Lady. How would the picture look like if you resort to the method of characteristics rather than shaping pictures? Well, as a mathematician, I always want to go back to mathematical characteristics. That's something that I have not looked into detail into yet. I would suspect that it would be advantageous because in this particular set of dynamics, the hyperplane x1 is equal to x2 is a characteristic surface. So if you start uncorrelated, you will remain uncorrelated for all time. So if you try to So if you try to preserve that, I would suspect, yes, that would help. And I know Richard's 2021 member actually does a change of coordinates to try to... Yeah. There's like there, like, there's been work out there that has looked at that. There was some work with Brown Gene on the folder, I think back, I think this listed paper back in 2004, tried to do that. Tried to do that, but they actually still saw variance loss. And their conclusion was that it was because correlation lengths were shrinking to the point that they were smaller at the greater resolution. And so I would suspect that once you get to that point, you're in that realm of this kind of discontinuous change. And unless you have a memory of the screen that knows that, you're just That knows that, you still might be running into problems, is what I would suspect. So, like, I can tell you that from experience, if I go back here, you can't really see this behavior clearly if you just solve the covariance equation for a method of characteristics as one class clue to. Because this continuous spectrum solution is actually a weak solution, it's like hidden. It's it's like hidden kind of within there. So unless you have a smart empirical scheme, which I'm not aware of, I'm not sure if it's the solution. I would hope it would be better. I'd like to make one announcement to all of the participants, those here and those on the line. Participants, so those here and those online, please use the link that was sent to you on Friday by the station manager to upload your talks. If you decide to give us permission to store your talks online, and try to upload your talk at least an hour before your actual talk, because the website is updated.