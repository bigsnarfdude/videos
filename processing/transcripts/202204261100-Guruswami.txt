Session of today. It is a pleasure to have Pencat Guruswami here from UC Berkeley speaking about recent progress on binary deletion correcting codes. Go ahead. Yeah, thanks a lot, Felice, and thanks to everyone for attending. It's a great pleasure to give this talk. Yeah, obviously, it would have been nicer if we could all have been in OHA, but you know, virtually. In Oxaca, but you know, virtually there, and yeah, so the talk today is going to be a survey. So, I won't get into too much proofs and stuff, except maybe one or two places. I'll hint at the ideas. There's been a lot of progress on codes to correct deletions in the last six, seven years. And for this talk, I've chosen to focus on, in some sense, the hardest case of that problem, which is the binary codes case. So, you want binary codes to correct deletions. I'll explain all this as we go along. Okay. As we go along, okay. And okay, so what's the model? So, in the bit deletion model, you have a string of length n which you transmit on the channel. And now the channel is going to corrupt some, sorry, it's going to delete some arbitrary t bits. And the receiver is going to get a sub-sequence of length n minus t. Okay. And the point, and a few things about the model. The model is very simple. Things about the model. The model is very simple. That is really the model, but just to contrast it with other things you might have seen. First of all, this is a worst-case model. We don't assume any probability. We just say the adversary can delete up to T bits, but the bits can be wherever they are and in the most nasty way. The other thing is you do not know the location of the deleted bits. This is what contrasts it from erasures and makes the model much more challenging because you lose synchronization with the original transmission. Transmission. And here I've indicated t equals two. So these two red bits have been deleted. So you get the sub-sequence. And the deletion correction, and this is basically model synchronization errors. And the basic coding question we will ask naturally is in such a model, how much information can you send? So what are the kind of code words X you can send so that you can recover that despite these deletions? And by the way, at any point, please, I'll keep an eye on the chat. I cannot see the video. I cannot see the video that well on my because I'm sharing the screen, but please just unmute yourself and ask. It's not such a big group. I think we can do that. Okay. And okay. So how do you transmit reliably on this channel? Naturally, you pick an binary error correcting code. And if you think about it, what is the property you need? So if you want to correct arbitrary bit flips, you want the Hamming distance between code words to be large. Here, the notion of distance is slightly different. It is what is called the Levenstein metric or edit distance. Is called the Levenstein metric or edit distance. In particular, what you want is you want to pick a code such that if you pick two distinct code words from it, they won't have a common subsequence of length n minus t. Because if there was a common subsequence y of both x and x prime, of length n minus t, then you can go from both x or x prime to y. So if you receive y, you have no idea if you got x or you got x prime. Okay. And so that's the property you want. And LCS stands for the longest common subsequence. CS stands for the longest common subsequence. And note that the subsequence need not be, it's not a substring, it's a sequence. You can skip things, it doesn't have to be contiguous. And that's the problem. So you want to pick a lot of codewords so that no two have a long common subsequence. And I've written the same thing on the slide here. So a T deletion correcting code is simply a subset of strings such that every no pair shares a common subsequence of length n minus t. And t is the number of deletions such a code can correct. So then the natural question is, So, then the natural question is: you want to have as large a code with this property as possible. In particular, you want to minimize the redundancy, which is n minus log of c, or the rate of the code is log c by n, as we know. And in addition to this, obviously, we also want, if you want to use these codes, an efficient encoding algorithm, which say maps messages indexed from the size of the code to the code words, and also a decoding algorithm, which given a string y will find the unique code word. The unique code word which contains that as a subsequence. Okay, so this is just the inversion problem. And these are the two things which we will also like to be efficient, which let's say crudely means that there is a time which is polynomial in the length of the number of bits. Okay, so this is the model, the basic deletion correcting model. And of course, you can ask this over any alphabet as well. So I'll come to that. But as in any model, there are three central. You know, three central challenges. The first one is a purely information theory combinatorial thing. What is the best trade-off between the number of deletions you can you want to correct and the redundancy you need to have in the code? And this is really a sphere packing problem in the edit distance or Levenstein metric, exactly like in the Hamming metric case. And then you want to construct codes which approach this trade-off along with efficient algorithms. Okay, and this is exactly. Okay, and this is exactly the you know, and you can even make the second part into two sub-problems. You want to first construct the codes and then maybe think about the algorithms, but I just merge them into one thing. Okay, so first thing, the first part sells you what is the baseline, what can you hope for, the second tries to achieve it. And the point is this model, despite having been introduced in the 60s, is very far from well understood. Even two deletions, we don't know the complete answer even to question one, just for two deletions. One just for two deletions in binary case. But there's been a lot of progress in the last six, seven years. And I'll tell you a little bit about that today. Okay. And again, this is, I'm focusing on binary codes just because there is a lot to say even there. And in some sense, it's the hardest case, but you can ask this question over any alphabet, sigma. The setup is the same. You delete these symbols and you get a subsequence. Now, one thing is that if I allow the alphabet to grow with n, then what if I, yeah, if I don't. Then, or if I yeah, if I don't restrict the alphabet size, then one thing you can naturally imagine is that you can just put a packet header in each code word symbol. So, if you had a code word which had symbols alpha one to alpha n, you can make a new code word which has symbols i, alpha i. So, you package i together with alpha i into a bigger symbol. And now, whenever you receive i, alpha i, you know that it is the iat symbol. So, this effectively reduces the deletion model to the simpler erasure model, because whenever you get a symbol, it comes. Because whenever you get a symbol, it comes with its position information. But of course, this means that the alphabet size is at least n, right? Because you're putting an index from 1 to n. And in this case, of course, if you don't care about the alphabet size erasure model, we know completely that read-Solomon codes are the optimal solution for collecting T erasures. You can have redundancy T, which is the best you can do. But you can also do better. So there is this beautiful concept called synchronization strings introduced in the work of Oplar Shah. Introduced work of Oplar Shehrasby five years ago. And they effectively mimic this construction, but they keep the alphabet size a constant. And in particular, you can make do with a little more redundancy, t plus epsilon n. And the alphabet size is a constant which only depends on epsilon. Okay, so this is also an interesting chapter of progress. And there has been many follow-ups to this in this in various settings, list decoding and other things, but I won't. Coding and other things, but I won't touch upon that here. I will focus primarily on sigma as 0, 1. And most of what I say will extend to any fixed alphabet like ternary, q equals 4, etc. Pretty much without any exception. But I will focus on 0, 1, which in some sense is the most basic case. Okay, hopefully the setup is clear. And in the talk, I'm going to split the talk. And in the talk, I'm going to split the talk into three parts and basically corresponding to three regimes where there has been progress. The first is when t, the number of deletions, is small. Think of t is one, two, log n, something like that. A small number of deletions. Think of low noise. The kind of regime where we like hamming codes, BCH codes, things like that in the conventional bit flip case. The next is the small constant fraction of errors. Fraction of errors, think of 1% of the symbols can be deleted. So there's low asymptotic noise. So what can you do here? And both of these are going to be high-rate regimes. And then the third part is the exact opposite. What is the zero-rate regime? What is the largest noise you can tolerate with positive rate? Okay. So large fraction of deletions. And there's been a lot of progress on all of these regimes of the noise level. And there are many other models. Level. And there are many other models you can also. I'm focusing on worst case unique recovery. You can imagine, you can ask, hey, what if symbols, bits get inserted and deleted? That's a more common form of synchronization error. So that has been studied. You can depart from worst case to some random models or oblivious models. People have studied list decoding. One thing is that in deletion code setting, traditional methods like linear codes, et cetera, are not that useful, but people have nevertheless studied linear codes, large alpha. Studied linear codes, large alphabets, and so on. And some of the overarching aim here is to say we have this really well-developed theory for bit flips or Hamming model. Can we improve the understanding of the deletion correcting codes to kind of come close to or even match what we know for bit flips? Okay, that's the long-term goal. We are still far from it, but there has been a good deal of progress in the last several years. So I'll start with really the most crisp, I mean, easiest to To deletions, and let's start with the smallest number of deletions, which makes sense. One deletion, right? So, here you have n bits, one bit, this zero got deleted, you got an n minus one bit string, you have to figure out what was the x which was sent. Okay, and I want to pose this question in the following way: you got y, you need to figure out x, right? So, obviously, you need some extra information for you to be able to do that. You need to know something about x, which will allow you to, because Something about x which will allow you to because a priori you could have got this from inserting a zero or a one at any of these places, so that's going to be like two n possible strings, give or take, depending on runs and so on. How do you disambiguate and pick the correct one? So what extra information would would help? Okay, so think about it. What extra information S1 of X is like sort of a small sketch of X to correct one deletion, hence the name S1, will allow you to permit the recovery of X from Y and that S1 of X. y and that s1 of x right obviously you can take s1 of x to be x itself but that's not meaningful i want s1 of x to be much uh smaller than x and for this let's imagine you get a sequence of all zeros y is all zeros then you basically need to know whether x was all zeros or which x might have had just one one you would like to know the location of that one that's the information you need to know and so that's the the So that's the story. So the location of sketches one should reveal the location of one, if any, in x. And I can algebraically package that into this nice expression, summation i times xi. If x has at most one one, this tells you exactly, if it has exactly one one, it tells you the position of the one. And if it has no one, of course, it is zero. So in this case, when you get y is zero to the n, n minus one, this s1 allows you to recover x. And note To recover X and note that S1, um, and in general, and this is what Levenstein proved: um, is that you can recover x from S1 of X. This is from back in 66, S1 of X and N mod n plus 1. In fact, you don't need the full integer, you just need it mod n plus 1 and any one-bit deleted subsea. So, if you knew this, it's not just in the special case when y is 0 to the n minus 1. In general, you can always do deletion recovery if you knew. Ways do deletion recovery if you knew S1x mod n plus one. And then the way you get this code, which is the Warshamau, Ten and Gold's single deletion correcting code, is to simply take the code word to be all those strings x for which this S1 of X happens to be zero. So basically, you basically pre-code so you know what S1x is going to be. You don't need to have this extra information. And you can prove that this code has size to the n by n plus one, and that is. One and that is a code which corrects one deletion. And it has more or less size very similar to the Hamming single error correcting Hamming code, which also has size like 2 to the n by n plus 1, exactly when n is power of 2 minus 1. So it's really has a very nice parallel there. So now you can ask what happens for more deletions, the size of the largest t-bit correction code. And here I want to think of t as small and n as growing. The number of deletions is going. The number of deletions is going to the size of this optimal code. You can construct a code of size 2 to the n by n to the 2t, right? Ignoring constant factors depending on t. And this is basically the usual 3D construction. You pick a code word, you throw away things which you cannot pick and keep going. That's a very inefficient exponential time construction. And the upper bound, just by volume packing, a typical sequence of length n will have about n choose t sub sequences of length. Sub-sequences of length n minus t. So the volume packing argument will give you this upper bound. And therefore, the optimal redundancy is basically t log n up to constant factors. But note that there is a factor two gap here. So we know we need at least t log n redundant bits. And we can make do with two t log n redundant bits, but there is a gap. This gap doesn't exist for t equals one because I showed you a construction in the previous slide where In the previous slide, where, I mean, I didn't prove it works, but I'll actually give a proof later, which has size 2 to the n by n when t is 1. So for t is 1, these two things match. So we have the optimal redundancy. But for every other t, including t equals 2, there is a factor 2 gap. And again, this is only a non-explicit construction. You can also ask about explicit constructions. And again, it's to compare this with the story with T-bit flips or Hamming distance model. There again, Distance model. There again, the volume-packing upper bound holds. The greedy L thing will give you this thing. This is also like the Gilbert-Warshamer bound in this regime. But we can do better. The BCH codes allow you to correct T-bit flips with essentially T-log N redundancy. So there is no factor 2 gap, but here this factor 2 gap is persistent. Okay. And again, explicit constructions are also interesting for any team. So I wanted. Any t. So I want to now talk a little bit about explicit constructions of t deletion codes for small t. So for t equals one, this VT code was long known, but even for t equals two and small values, we didn't really know good codes for a long time. There is some work we did in the low noise regime, which implicitly gave codes of redundancy T root, you know, order root n, which is better than simple things. Simple things. And then with a couple of then prudence at CMU, we had these codes which have redundancy, t square log, log n essentially with an extra log t. And this was the first construction with log n redundancy, even for two deletions. So it was, it said that, okay, you can at least get order log n redundancy. And then there was a line of work attacking the problem from. Attacking the problem from something called the document exchange problem, which is which I'll come to in a second, which is closely related to this S1 of X sketch-like thing I said. And there was one line which basically got good redundancy for kind of largish, you know, this kind of thing, T square plus T log square n. So it had this log square n. And then that line of work was improved to get the optimal redundancy in a couple of nice works, which got order t log n. And then you can also ask about the constant in front. Also, ask about the constant in front. And this work, Seema Gabbers Brook, basically got 4t log n, which is the best explicit construction we know. Note that existentially we can do 2t log n. So we are about factor two away from the existence results. And one special case of this is, you know, in a particularly interesting case is to study t equals two. And there, in work with Hostad, we got codes with redundancy forward logging. With redundancy forward log n essentially, and this matches the existential bound. So, the summary of all this is that so we have constructions with redundancy order t log n explicit and for small t. So, we can match, we can get explicit codes with redundancy essentially optimal up to this constant factor. But these factor two gaps remain between the lower and the upper bound and also the existential and constructive. Existential and constructive results, except for the cases when t equals one and two. Okay. So I want to talk a little bit about this document exchange buzzword I used, which is one way to think about deletion codes and string reconciliation. So imagine Alice has a string X and Bob has a subsequence Y of X, and Alice would like to communicate X to Bob. To Bob. And the idea is that since Bob already has the subsequence, Alice would just like to send a short hash such that together with S of X and Y, Bob can surely figure out what X is. And again, this is the setup we had earlier for one deletion. S of X was just summation IXI that allowed Bob to recover. And if you have such a scheme, it is very intimately connected to deletion codes because you can simply take the code. Codes because you can simply take the code to be all those strings which have a pre-specified value of the s of x, right? And by pigeonhole, there must be one such alpha for which this code is large if your sketch is short, just by counting. And then Odlitsky showed that you can have such a you from more general results in communication complexity, follows that you can have such sketches of size t log n over t and that immediately. n over t and that immediately implies codes with this redundancy for t deletion correcting codes but this is non-constructive because i don't tell you what alpha is but you can essentially make it constructive too because alice to protect x can send x and the sketch but of course now the sketch can also suffer deletions so what you can do is you can protect the sketch with some deletion correcting code so that's the idea the encoding of x is sort of a system The idea: the encoding of X is sort of a systematic encoding. X goes to X, S of X, and the protection of S of X. And it feels a bit circular because you're trying to construct a deletion correcting code, and I'm assuming one here, but the point is C prime operates on a much shorter string because the sketch is much shorter. So you can actually use some fairly inefficient code, which we do know for C prime. And this basically says if you solve the string reconciliation problem, you can essentially solve the deletion correcting problem. And this problem is a little bit easier to think about. Is a little bit easier to think about, right? So, somehow, what extra information do you need about the string to recover the string? And sometimes you can also assume some, you don't have to solve this problem for a worst case X. You can also assume that X has some properties which are typical of strings, like good run-length distribution, and so on. So, that can also make this task a little bit easier. Okay. Okay, so I guess it's good to give at least one proof. This is after all like a workshop of mathematicians. Okay, so here is an old proof, sort of a nice exercise type thing. So, how does this sketch S1 allow you to recover from one deletion? Okay, Bob has this one deleted bits. Bob has this one deleted bits version of X, Y, and the goal is he wants to insert a bit to get some string, and he obviously wants to get X. And the claim is there is a unique position and bit where he can do this insertion for which he will get a string X prime with whose S1 matches that of X. And since he got S1 also, he can figure out what this X. Okay, and this is again, I always almost give this as an. Almost give this as an exercise in coding theory classes, you know, because it's very combinatorial and people come up with many different proofs and so on. But here is one quick proof you can give. The idea is that you try various positions. So insert, you don't know where to insert. So let's insert one at the left end of y. And then let's sweep the one rightward. And the point is when a one, this is the idea. So this is the y and I'm inserting This is the idea, so this is the y, and I'm inserting this red one in there. And the point is, when a one jumps over a one, nothing happens, right? It just the string stays the same, so we don't have to worry about it. But when a one jumps over a zero, you can see that the s1 increases by one because the position of this one increased by one. Right, so this was y, and I'm inserting this one. So as you sweep rightward, this S1 is monotonic. Okay, so for various positions, you might. Okay, so for various positions, you might insert one. Okay, and maybe none of them worked. You went to the right, you ran out of room. So when you reach to the right end, you change the one to a zero, and then you sweep the zero leftward. And whenever a zero jumps over a zero, nothing happens, but when it jumps over a one, once again, this S1 increases by one because the position of the one it jumped over moved one to the right, and that will then increase the sketch S1. And that's the argument. So, this is a monotonic process. So, this will be exactly one spot where it matches S1 of X, and that must be the correct spot. And a lot of the arguments for two deletion codes, three deletion codes, kind of work this way. And you can imagine that as you're inserting two bits already, there are many cases, things get a little bit more tricky. But the flavor seems is this. If you can get some monotonic potential function, then that can. Potential function, then that can be used to define a good code, and again, that's this summary. So, this is monotonic, so there's a unique bit and position where this works, and this is a proof that this S1 allows you to correct one deletion, wherever it might be. Okay, and maybe I'll just make one connection here, which is also sometimes how I teach even single-bit deletion, error correcting code. If you take the Hamming code, it's If you take the Hamming code, it's really the same sketch. The sketch is something very similar, except instead of I, you put the binary representation of I as a bit vector. And that is your sketch or what you call the syndrome usually, right? The syndrome of the code word or the string X which is sent as that, right? Summation IXI, where I isn't binary. And somehow here, you keep it as an integer, you get these non-linearity. So this code is obviously not linear over F2 or anything, but it allows. Linear over F2 or anything, but it allows you to correct one bit deletion as well. And one can actually also show that this allows you to correct one bit flip also. So in fact, sometimes I so you can introduce this code as a way to correct one bit flip and then move to the Hamming code by saying, hey, if you replace I by the binary representation of I, you now get the Hamming code. And why you do that is that because if you want to correct two or three or more bit flips, that's better than this idea. That's better than this idea, which doesn't quite match BCH codes and so on. All right, so that's that. And so, for two deletions, what can you do? One idea is that you can naturally try more sketches, something like IXI, I square XI, and so on. And in each of these cases, it's enough to give it mod something because this doesn't change too much, right? So, it's enough to just give it mod low how much the maximum it can change with two deletions. These two things don't work. Then, we also have something called. Then we also have something called the run sum. So we take the run number of each of those and add them up. Don't worry about the exact formula. There's one more sketch of, and then the total redundancy is log n, two log n, and log n. And essentially, these three, knowing these three values about x, one can correct two deletions in x. That's what we prove. It's not quite what we prove, but you know, model of some annoyances, that's what we prove in this work, which gives two deletion codes. But already there are more scales. But already there are more sketches, there are more cases, it becomes more delicate. Okay, how do we handle more deletions? Once you go to large T, like 10, I don't think handling all those cases is feasible. You need a more principled approach. So the idea here, in some sense, was what we did, which implicitly gave the square root n redundancy, was actually a very simple idea. Let's take a read-Solomon code and let's give it indexes so that we. Give it indexes so that we know which position is where. And maybe we work over a field of size 2 to the b, read-Solomon code of size 2 to the b, and let's make sure that it can correct t errors, like symbol errors. And you can do that with redundancy t times b, right? And now the idea is that you would like to know where deletions have happened. So you just insert some big buffers of zeros between these blocks, essentially, as some sort of synchronization. Box essentially as some sort of synchronization information. And the idea then is that if the number of deletions is small, most of these buffers would be fine. So you look for those buffers and that's going to identify all but when a buffer, you know, when two buffers are both identified correctly, you know, you kind of know the value between them. So by looking for the buffers, you can essentially find all but Essentially, find all but order P of the symbols of the Reed-Solomon code correctly. And whatever is not correct, you can just do error correction using Reed-Solomon codes. And that's the idea. So essentially, it's sort of a little bit of a hack, right? You take Reed-Solomon thing, you put some markers in the code word to kind of help you identify where deletions have occurred and where things are okay, etc. And the square root 10 comes because you have to balance the redundancy. There are two things you need. The redundancy. There are two things you need, two places where you're paying in redundancy. Well, first, the read Solomon redundancy is T times B because it's correcting T errors. And these buffers are going to essentially, if you have a string of length N and each of these blocks is B bits, you have N over B buffers, and each of them you're putting log N zero. So roughly N log N by B is the size, and you balance these two. You know, turns out B is root N is the correct balancing point. So that's the idea. But then how do you? But then, how do we go better? I promised we can get redundancy like t squared log n or you know something like order log n for fixed t. And to do that, you cannot really use these explicit buffers which you add. That becomes too much. Instead, you use some patterns in the string itself as the buffers or the markers. So, for example, here, this 111 occurs fairly regularly. So, you use those. Regularly. So you use those as the markers in the string, essentially, instead of putting zero. So you're not adding any extra redundancy. And the point is, if these patterns occur relatively frequently, then the gap, you know, the portions between them have only length, like say log n, then you can protect them with some read-solven code using only like t log n redundancy. It's the same as before, but this was t times b, but now b is like log n. The blocks are of size log n. Of size login. But then, of course, why should the pattern be frequent, right? Maybe there's a huge gap between this pair, and then you have to use a much bigger field, read Solomon code, which will be expensive. And also, the pattern itself can face a deletion, right? So then you will make a mistake in identifying this pattern. And you don't know exactly which pattern you're using. And the point is here, neither of these is a big problem. Is a big problem once one looks at it the right way because the pattern itself will, for most strings, every short pattern will occur frequently. This is like just a property of random strings. And you can do some pre-coding to avoid those atypical strings, which fail to have this sort of pseudo-randomness property. And the other idea is that, okay, it's true that some particular pattern can get deleted a lot, but you can try every pattern of some length. Pattern of some length. And the point is, a majority of these patterns will face no deletions or very few deletions just because you don't have too many deletions in your budget. And you can use those to do this. And then, but then that would mean that you'll have to do this sort of coding for every pattern. And that's where we pay this extra factor t to get t square log n. So we got a sketch size of length t square log t log n. That's because we have to do this for every of patterns of length about. Every of patterns of length about log t, so about t of them, and for each one of them, you do this Reed-Solomon type syndrome sketch of size t log n, so it becomes t square log n. Okay, just a question. Yeah. How long does it take to identify the patterns? So these patterns are just going to be really constant size. So you can just do, just sweep the string and find it. Okay. Let's just run like a finite automata or something to find because really the patterns are of you. Find this because really the patterns are of you should think of t as constant, the patterns of small. Okay, any other questions? And then you can make this t square log t into using this idea of syndrome compression, which is a cute idea. You can make this, you can essentially start with the sketch and compress it or hash it down further to length about 40 log n, and this supplies. Log n and this suffices for t-deletion correction. Let me skip this idea, but it's a very natural idea. You take our sketch and you essentially hash it by just giving a prime and the sketch mod a prime. And you can prove that you can find such a modulus so that every pair of strings with small edit distance or long common subsequence will have distinct values of the sketch. So, and this is very general actually. And this is very general, actually. You can do this for any error model, and this gives you redundancy 40 log n. But one distinction with this is that this will make the runtime of the decoder and even the encoder n to the t, something like n to the t, because you've essentially brute force over all possible strings in a ball of radius n to the 2t. So the sort of is gets the redundancy low, but it's inefficient. The running time is not, is, has an The running time is not has an exponent t, so that is in some sense undesirable. So, let me close with some sort of many things we don't know about small deletions and then move on to the second and third regimes. And probably I might just only touch upon the second regime for time reasons of time. So, first, we don't know the optimal redundancy for t-deletion codes. Is it t-log n or 2 t log n or something in between? That's fascinating open question. In between, that's fascinating. Open question, even for t equals two, it's open. There it is two log n versus four log n. And can we get constructions with matching 2t log n for t bigger than 2? We don't know that. We know it for t equals 2, but not bigger. And for two deletions, it also would be fascinating to go below the existential bound. If you can get a construction which beats the existential bound, which I think should exist really, that would be in some sense killing two birds. Would be in some sense killing two birds with one stone because you will actually prove that this 2t log n is not the right answer and you will also get a construction. And maybe that's the way to go. And if you think about algebraic in the world of bit flips, BCH codes do both. They show Gilbert-Warsma bound is not tight. The greedy thing is not tight, and they also are an explicit construction. And finally, these constructions with like 4T login, they have fairly large input. 4T log n, they have fairly large encoding, decoding complexity. It'll be nice to get that with better runtime as well. Okay, so those are some many things we don't know. Okay, so we are in a better place than we were like, you know, around 2015 or so in terms of our understanding, but we still don't have the final answers. And some of these questions look very basic to me, like, you know, what's the size you can have for two deletion codes? We don't know. Let me briefly talk about small constant fraction of deletions, but I really want to, then I'll quickly move to the last part, which is a little bit different and maybe it will give you more variety of things. So here I want to talk about constant fraction of deletion, some percentage delta of bits can be deleted. And Schulman and Zuckerman showed essentially by ideas similar to what I told earlier, in fact, we borrowed it from them. These read Solomon codes with buffers, et cetera. Codes with buffers, etc., that you can get asymptotically good codes in this regime. You can have both the rate and delta be bounded away from zero. But both of these were very small, and existentially, you should get a trade-off like this, kind of like the G V bound. So one minus delta log one over delta, like entropy of delta for small delta. So this you can get by the greedy construction. And again, this idea I showed with this Reed-Solomon and buffers, which is essentially a more careful version of Schulman-Zuckerman, got us to rate one minus order square root delta. One minus order square root delta, but it's not quite this thing. And there's another way to get the same result using synchronization strings as well, but both have the square root delta thing. But then a couple of these nice works, which I said about this document exchange or this way you think about this as a reconciliation problem. What sketch should you send so that I can recover the original X? Thinking about it this way and bringing a lot of ideas, they gave explicit constructions of almost all. Implicit constructions of almost optimal rate, one minus delta log square, one over delta. And this is very close to the optimal thing. And what I find very fascinating is that this, as far as I can tell, is new even for bit flips. For bit flips, where you flip a delta fraction of bits, just good old Hamming error model, the best constructions get you one minus delta polylog one over delta. And these codes were based on expander codes. You take a certain expander, suitable expander, and do the A suitable expander and do the Sipser-Spielman-like construction. And that would give you this, but they have somewhat large polynomial here. I think not better than log cube, I'm sure. And this one gets you better. And note that if we can correct, okay, it doesn't immediately follow, but if you correct delta, note that you can simulate T errors with T deletions followed by T in. With T deletions followed by T insertions. You can delete the bits and insert the opposite bit. And these codes, pretty much all the results I said, work also when combination of insertions and deletions. And because of that, they also imply codes for the Hamming method. And as I said, this is based on this document exchange idea where you send a sketch which has size like T log square n over t to correct t deletions. Okay, and so there was an old randomized protocol from 2005. Protocol from 2005, and there was a very nice work of Bella Zugi who gave some improvements of those and also gave deterministic questions. And then, this line of work by Cheng et al. and Hopler got this final result. Okay, so I'm going to skip this part because what I have here is pretty vague anyway. I'm not sure it'll give too much. But I will just say that the idea is it's a very, it's a more sort of a distributed computing kind of flavor or a coming. Kind of flavor or communication complexity type thing. So, of course, in the end, it's a non-interactive thing. You only want send one sketch, but you sort of the genesis is that way. You break the string into parts, you send hashes, you use the hashes to find some foothold in the string, then another round of hashes come and you get more and more, you piece together more and more of the string, and then you finally are done. Okay, obviously, this is easier said than done, and uh, and that's the but that's the way. But that's the very thousand-foot view of these constructions. Okay, but very nice. And as I said, get something new even for the Hamming case. All right. And that was a very brief part. I didn't focus too much on it, but there are still some challenges. First, can we get the optimal thing? Can you get redundancy delta log one over delta for delta fraction of deletions? And again, this is open even for hamming errors or worst case bit flips. And if thinking about all these deletions, if you could. And if thinking about all these deletions, if we could solve this old problem in flips, that would be amazing. And that would be really nice. And the other thing is that there is this construction, this delta log square one over delta is nice, but it is somewhat intricate. And it is, you know, it's not a very sort of coding theorist friendly construction where you just say, oh, here is the code. It's sort of more. So it'll be good to have a nicer, sort of more explicit construction. More explicit construction which gets the job done, but we don't seem to know how to do that. So, right. Okay, any questions about that part? Again, I was very brief on that part, but you can find more information in the references and slides. Okay, let me get to the final part of the talk for which I'll take, I guess, 10 minutes. Large fraction of deletions. Okay, so this is slightly. Of deletions, okay. So, this is slightly very different ballgame in a sense. So, now I'm going to really make the channel more powerful. It's going to corrupt pn deletions where p is some large fraction, I don't know, one-third, one-half, something. So, you get a subsequence of length n minus pn. The game is still the same. It's just that the channel is more noisy. What's the best rate you can have? That's the question. Again, the coding problem remains the same. But here is the regime now, it is different. Earlier, you were in the high rate. Regime now is different. Earlier, you were in the high-rate regime, very low deletions. Now, a lot of deletions, you want to know where the rate hits zero. So, what is the what is the large, what deletion fractions can you correct with non-vanishing rate, with rate of C bounded away from zero as n goes large? Formally, what is the supremum of deletion fractions P for which you have an asymptotically good code of size like of positive rate alpha p bounded away from zero with which allows you to correct. With which allows you to correct P and deletions. So that's the question. And just to put this in context, if you ask this for bit flips, if you ask this for erasures, P star is one half, because the best distance you can have for binary codes is one half. For bit flips, it is half of that, so one fourth. And for random deletions, we know it is one. For random erasures, we know it's one. For random bit flips, we know it's one. For random bit flips, we know it's one half and all this. So, so this is like a very fundamental thing which we know for every channel, right? In substance, it's the first order of business. But here, what is the value of p star? And I've written the same question here. And there's a trivial upper bound that p star is one half. There are two ways to see this. One is if you allow the adversary to delete half the bits, the adversary can delete the minority bit, and you will basically only. bit and you will basically only get two possibilities either some n over two zeros or n over two ones so you can only push one bit of information through so the rate is certainly zero or goes to zero another way to see the same thing is that if you have three strings if you have three code words in the code some two must have the same majority bit and therefore they will have a common subsequence of length at least n over two so you cannot really push p star more than half but for the but we didn't know if p star was half But we didn't know if p star was half or it was bounded away from this trivial limit of half, so that was open for a long time. And again, I told you that for erasures, bit flips, all these other models, we know the answer. But I'll come back to this upper bound thing for us later, but let me just at least tell you something about the lower bound, which is really a construction of or existence of codes. So, again, whenever you are given a coding problem and you want to know what are some trade-offs you can do, you try random codes. Trade-offs you can do, you try random codes and usually works well. It does some job here too. For example, it'll show you you can p star is at least 17%, but that's not the best. In fact, the best construction can do quite a bit better. And for a long time, this was the best known, this random construction. But about five years ago, we had a construction which allowed us to correct deletion fractions about 41.4%. Basically, anything less than square root 2 minus 1. And further, this was an explicit construction. And further, this was an explicit construction and came with an efficient algorithm as well. Okay. And this beats random codes, and it also has, and it's a bit surprising because this number is bigger than one fourth. So in a way, deletions are easier to correct in this setting than even bit flips because you can correct more deletions. Okay, so there's an easier construction for P is less than one third. Let me at least just tell you what the construction is because it Least just tell you what the construction is because it's sort of very concrete. More work was done to push it up to 0.414. And the idea is again, it's sort of a workhorse in coding theory, which is concatenation. So you take a code over a big alphabet, but a fixed alphabet of size K, and then you map these symbols in K to an inner binary, via an inner binary code. Okay, so and that gives you a concatenated code. Nothing new here. You take an outer code word over with symbol K, you map each of these. Symbol K, you map each of these by an inner code word, you get this concatenated binary code word. And the point is, what are the properties you want in this code of these outer code? The thing is that if you have a long common sequence at the outer level, that will be inherited after your encoding because those bits will be encoded in the same way by the inner codes. So, a good idea is to pick an outer code which has very good LCS, but that's not really. But that's not a problem because you're allowing me a big alphabet size. So if you make the alphabet size big enough, I can make the outer LCS not n over 2, but much smaller, like epsilon m for any epsilon you want. And you can do this explicitly with efficient algorithms, et cetera, as well. Okay. So that's kind of a good thing. Think of these as some sort of like the AG codes in this world, except they are not algebraic at all, and in some sense, much more elementary. So they allow you. Some sense much more elementary, so they allow you to have good performance if your alphabet is big. And for the inner thing, again, having a small LCS seems like a good thing because typically when we do, for example, codes meeting the Ziablo bound, we will take inner codes which meet the GV bound and etc. And that's good. But this is like the original problem. But the point is that the inner codes can have much worse rate in this regime because we are only after the zero rate regime and the inner codes are constant size. So the rate can be as bad as it wants. Size so the rate can be as bad as it wants, and in this regime, you can actually have a code whose LCS is almost half, in fact. And I'll just tell you roughly what the code is. Basically, if you take an inner code, if you want to have two strings with the smallest possible LCS, you of course pick all zeros, all ones. If you want to pick a third string, maybe you pick half zeros or half ones. What about the fourth string? It already gets interesting. And for that, the idea is that if you have two binary strings who which Binary strings who which oscillate at very different periods, then their LCS is going to be small. So if you have 01010101 and 0000011111, you can easily check that the best common sub-sequence between them has essentially about half the bits. Because when you match all the zeros, you have to go twice the length here. You run out of the second string. And you can formally, you know, it's a lemma to prove this that if the strings oscillate with periods A and B. Strings oscillate with periods A and B, then the LCS approach is one-half if A by B is very small. Okay, so that's the idea. We're going to take these as the inner code word strings, and that's the entire that specifies the full construction. Okay, so I'm sort of really only giving you a high-level idea here. And there are some additional ideas to so this will prove, this will give you a code where no two things have a common sub-sequence of length two-thirds. And you can also give an algorithm to correct. Can also give an algorithm to correct one-third deletions using additional ideas. For that, you need to tinker with the outer code to make it efficient, etc. Okay. So, okay, so I'm running out of time. So, let me not do this y two-third. It won't make too much sense, perhaps, in the time I have. Let me go to the other point. So, this said that, okay, P star is pretty good. It's at least 41%. What about upper bound? As I said, it's not more than one half. So, the state of the affairs about a year ago. About a year ago, was that P star was between these two numbers, 0.414 and 12. And at last year's Fox, we had this result with Joyuhe and Rayleigh that P star is bounded away from one half. So it's a half minus some obscenely small, tiny alpha, like 10 to the minus 60, but nevertheless, it cannot approach one half. Okay. And another way we, this is a formal statement, is that we proved in any code of In any code of quasi-polynomial size, something like two to the polylog, you can find two strings which have a common subsequence non-trivially larger than one-half, like half plus alpha times. And a super high-level strategy here was the following. If in the code there are two fairly imbalanced strings, say both with noticeably more ones than one half n, then you just match up the ones to form a big LCS. That's like the easy case. Form a big LCS. That's like the easy case for you. Otherwise, what you do is that you keep matching ones like this, right? This is how you form a long common subsequence, right? So you just match things, you keep matching ones. But since they're only exactly half ones and half zeros, if you get a zero rich region, then you opportunistically switch to matching the zero rich region. So there might be such things here. Things here, so for yeah, so yeah, I think it's indicated here. Let's not read all these things. So I said green flags. So the idea here is that these black things here, including these dotted lines, were matching ones. And when you match this one, you realize that between this matching and this matching, you were earlier only matching three ones. But if you switch to zeros, you can match five zeros. So between those pairs, you don't change anything on either side, but between those pairs, you Either side, but between those pairs, you switch matching ones to zeros. And the same thing happens here. So this is really the high-level strategy where we are able to somehow say if the code is big enough, then there are enough places where we can do such opportunistic switches to get a non-trivial improvement. Okay. And that's the idea. So again, I'll. The idea. So again, I'll skip this proof here. But there is, it's relatively easy. And in fact, this slide is this slide plus the previous slide is essentially a full proof that you can find two strings with LCS n over 2 plus n by log square n. So you can beat n over 2 by a non-trivial amount. So yeah, this thing which I've written up here. So you just, it's a fairly short argument to get LCS, which is non-trivially large in this way. But of course, we want something. In this way, but of course, we want something stronger, we want constant times, and that takes much, much more work, and which is what we managed to do. Okay, so I'm sorry, I'm skipping over that part. But essentially, the idea is: if we have to save just one sentence, I would say we basically prove some structure lemma about how the oscillations between zeros and ones must look. Between zeros and ones, must look in any balanced string. And so that's some sort of oscillation statistics. And then we say that if your code is big enough, we use some heavy pigeonholing to say that there must be some two strings with very similar oscillation statistics. And that allows you to somehow do this opportunistic switch to these regions which are zero rich quite often and get. Often and get a non-trivial benefit over the trivial n over 2, which you can get just by matching the ones. Okay, so and this is you have to formalize and implement this, which takes a lot of work, but that's the foothold on the problem. And there are many challenges here. One, the obvious one is, well, what is P star? We don't know what we know this for every, you know, all these models. We don't know this for this basic deletion model. But even closing the gaps would be good. Can we improve the construction? Would be good. Can we improve the construction? Can our upper bound is half minus alpha? It is more like a model victory. We said it is bounded away from alpha, but it's a tiny alpha, right? So, can you get better upper bounds? And another thing which might also be good is that maybe we can improve the constructions, not explicitly, but perhaps with some probabilistic ideas. Okay, simple random coding doesn't work, but perhaps there are other ways to pick random codes. Again, to summarize, we had notable progress in all these three different regimes: low noise, low asymptotic noise. Regimes, low noise, low asymptotic noise, high noise. And there's a lot of work on many other models as well, which I have a few slides here, but I will maybe literally just run over each in like, you know, half a minute, just so that if anybody is interested, you will have a reference point or you can ask me questions. So, for example, you might say, hey, deletions alone is not too interesting. What happens if you have insertions too? And combinatorially, if you can correct T deletions, you can actually correct any combination of T insertions and deletions. This is an Insertions and deletions. This is a fact proved by Levenstein, but this doesn't preserve algorithmic efficiency. But pretty much all the results I stated, one can extend it to handle insertions as well. Okay, so that's one part. And there's random deletions is again something well known. Each bit gets deleted with probability p. The capacity of this channel remains open. And there's a beautiful survey of this by Charak Chi Ribero. There was a Transactions of Information Theory ran a special. A transactions of information theory ran a special issue last June, which is a great reference point for a lot of work on deletion codes, and you can find the article there. Let me skip this oblivious online. Another model is list decoding, where you don't get a single answer, but you're allowed to output a list of answers. And many of you are familiar and have worked on this. And here, we do know in some sense what the best we can do is. For binary codes, you can correct deletion fractions approaching. Codes, you can correct deletion fractions approaching one-half. And this is the zero-rate threshold. And here again, if you allow insertions and deletions, even combinatorily, it is not, they are not equivalent for list decoding. And what we did in this work with Hoplar Shah Razbi is we mapped the zero-rate region exactly for any fixed alphabet. And it has a shape, at least for non-binary, which was not what we initially expected. So this one for binary is maybe what. One for binary is maybe what you will expect: two times deletions plus insertions is less than one. That is very reminiscent of erasures plus errors type of thing. But the general picture was more complicated. Okay. Okay. So I think I should wrap up. So the deletion model is very challenging because when you try to decode, you don't know which symbol corresponds to which position, which actually makes a lot of techniques, classical techniques, not directly applicable. But there have been many developments. Applicable. But there have been many developments using combinatorial ideas in the last few years and probabilistic ideas leading to good progress. And again, I just gave a peek into some developments, but really pinning down on worst case big deletions. And there are many other models one has studied. And a good place is this special issue I mentioned. And many, many mysteries still remain in this thing. And hopefully, the next decade, we will see even more progress. So thanks for your attention. I think. So, thanks for your attention. I think stop and take questions. Thank you very much, Penkat. Are there any questions for Pekat? And please unmute yourself or write in the chat. I can look at the chat and can read the questions if you don't want to unmute yourself. I maybe can start with a question that the situation of the large deletion. Question of the large deletion. Okay, I was thinking, you know, the torn paper channel, where, like, basically, you have a string that is divided in substrings, and then you have all of these, and you have to reconstruct the trace reconstruction type problem, right? Yeah. Right. Could this help for those channels? Because it's kind of like you could see any pieces, a large amount of deletions. Yeah. Yeah, um, yeah, there is a yeah, these sorts of questions come up in shotgun sequencing and stuff like that, right? But usually, in that setting, at least from the from the application standpoint, the sub-sequences you see are very small. So, they will, so in fact, there are things like what length, you know. So, here I'm sort of, I think that is. I think that is so you will see subsequences of length t, and I believe if t is like square root n or something, you can piece together the string or something like that, right? So, but that's a lot of deletions in every one of them. Yes, but you get a lot and a lot of samples. So it's a slightly, I would say it's a somewhat different regime. But yeah, and there also, there are, but you can ask the trace reconstruction thing in our kind of regime too. You could say, I delete. You could say, I delete each symbol with probability one-half, so I typically get strings of length half this thing. How many sub-sequences do you need to reconstruct the original string? The answer is open because exponential gap between lower and upper bounds for both random original strings and worst case original strings. And again, there has been good progress on that, but it's still exponential gaps persist. Yeah, but the techniques for those are a little bit different. In fact, the best techniques tend to use even some complex analysis. Techniques tend to use even some complex analysis and so on in the algorithms. So, and the lower bounds just tend to be information theoretic. So, yes. So, thank you. More questions for Franka. So, I give the 10 seconds. And okay, so if there are no more questions, thank you very much, Benkat, for the Thank you very much, Vagat, for the good talk and actually.