This is optimal learner with a question mark that I'll get to at the end of the talk. And I'm just done work with these nice people, Ishak, my student Michael, and Nikita. And Michael made most of these slides, so thanks a lot to him. I'll try to go a little bit quickly over the basic setup, because I think you all know it, but I'll still do it anyways. And then maybe if there's time, we'll get to the proof at the end. We'll see. Okay, so the setup we study is just simple binary classification in the so-called realizable case. In the so-called realizable case, I guess I'll first give it like a vague definition, then an action definition. So the learning task here is: in this example, we want to combine education, so maybe you get a picture and you have to say whether this is a mammal or not. We've got two classes. The learning algorithm gets, as we all know, the input that is labeled data where we know this is a picture of a mammal, this is not a picture of a mammal. And what we have to produce is some prediction algorithm, which means that basically we have the learning algorithm, which would be small gears. Algorithm, which are these small gears, and the learning algorithm gives its input this labeled data, and then it chooses one of these predictions algorithms as its output. And then this prediction algorithm we can then pass the data points in, and hopefully it produces the correct label and also on new data. So I think you spend a lot of time making that nice. And okay, so what is the extra ingredient? There's also a data distribution that generates these pictures, and also what we're then hoping for is. And also what we're then hoping for is that this prediction algorithm that we output makes good predictions under this data distribution. And if you want to be a little bit more formal, on the next slide here, I guess the inputs, we have some input space x, for instance all the images of particular size. An output space we'll just say 0, 1 for the rest of the talk. And then the learning algorithm is just some function that takes n labeled data points and outputs a predictor, which is just a function again from the input domain to. function again from the input domain to 0, 1. There's an unknown data distribution over the input domain in this setup and the part about it being realizable, this is like a simplifying assumption. So we have this function class H consisting of, it has a fixed VC dimension D, and the true labeling function, the one that actually assigns the correct labels, lies in this class. So there's some H star in this class, and we don't know which one it is. Okay, but we're probably Okay, but we promised that it lies inside this class of these dimension D. And the labels that we see now are just samples from this unknown distribution and the corresponding evaluation of this unknown target function that we're trying to write. So this is just deterministic labels given the xi in this very simple setup. Okay. And the model that we're studying it in is this prediction model of Household Littlestone Wall from 94. So here, I guess as in all other places, it defined the error. I guess, as in all other places, we define the error of a predictor that we produce. So, H sub S is just the predictor I produce on the labeled data set S. It's just the probability under a fresh data point from this distribution that I misclassify the data point. I'm also going to write it like this because if we get to the proof, it'll be convenient. This probability is also just the expectation of the indicator variable that says whether you mispredicted it. And the last part about this prediction model is that the thing that we care about, the performance that we're trying to. About the performance that we're trying to do is to produce a hypothesis H that minimizes the expected error. So we don't want to say with high probability the errors at most something like in the PAC model, but here we're just saying I just want the expected error to be as small as possible. And I'll talk about the PAC setting at the end as well. This is the model for now. Okay. So what is known in this model, I guess the simplest album you can think of is empirical risk minimization. So what is the algorithm? I guess you just, when you get a training data set, you When you get a training data set, you look for a hypothesis in H that correctly labels all your training data points. And by the assumption that the unknown target function lies in H, such a hypothesis exists. In particular, the unknown target function satisfies this, but there might be more functions that satisfy it, so you might not actually produce the actual target function. But this is a simple algorithm. We don't care about running time now, we just assume this can be done. You can actually do this in here for some microservice. And so the learning algorithms really, I get the learning. So, the learning algorithm is really: I get the labeled data, I look for some predictor that correctly labels the training data points. And what's known about this one, this is also very well understood, it's been known for a long time, that if the VC dimension is D and I have n samples, then the expected error is d over n times log n over d. Okay. And this is also known to be tidal, many lower bounds over the years for ERM. And in particular, it's known that not just ERM, but any proper learning algorithm, meaning any But any proper learning algorithm, meaning anything that outputs something from the class, this is the best error you can hope for. So this is basically fully understood. But then the question is, what if you want to be improper? What if I don't actually insist on outputting a function from the class, but I can do something else? The only lower bounds here are just d over m, not without this log factor. So this is what we'll try to look for: elements that kind of match this d over m. This DO-RAM behavior. This has also been done before. There are several works and different algorithms that actually achieve this. I'll go over all of them in the following. So the first one was this one inclusion graph algorithm. Without going into proof, so this also dates back to 94. If I get a training data set of labeled data, what I do is when I need to make a prediction on a new data point X, I basically construct a graph that has a node for A node for every possible set of labels that my hypothesis set can assign to these data points that I have. So I look at both x and x1 to x4, so I take all five data points, and I look at all the possible ways that my hypothesis can label this. Then I connect every pair of vertices that where the Hamming distance is one between the two predictions, so that they only disagree in one single labeling. I take this graph. I take this graph now and then I make an orientation of the graph. So I take all of these edges and I orient them to minimize the maximum out degree. And then the hard part is proving that you can actually minimize this maximum out degree to be just D. If the class is VC dimension D, you can make the maximum out degree D. So you minimize all of the, you orient all these edges. The orientation does not use what the concrete labels are here. That's important. You only use what the Labels are here, that's important. You only use what are the elements. You make this orientation. And what you do then, now that I need to make a prediction on X, I look up the nodes that are consistent with the labels that I see on my training data set. So here I see the labels 1001. And so I look in this graph and I find that these are the two nodes that start at 1001. They disagree in the last labeling, right? The labeling of the point X. So I have to make a choice. The two things that might be considered. Choice, the two things that might be that are consistent with what I've seen in my training data set. And the prediction that I'm going to make is: I'm going to predict the label where the edge points to that. There's an edge between these two, right, and has been oriented in some direction. And I make the prediction that chooses the node where you point to. This algorithm isn't, I don't know, to me it's not super intuitive. It's very impressive that it works. And you can prove that this is an error of D alray. Yeah. So this thing doesn't, this algorithm doesn't depend on the distance. This algorithm doesn't depend on the distribution. Doesn't not depend on the distribution. No. Yeah. So you just, this is all you do. So it's a kind of strange algorithm, but it's, yeah, I don't know. Maybe some have more intuition about it than I do. But the whole point is that you can minimize this max out degree, and the error basically becomes the max out degree over m. And the proof is showing that if you have easy dimension d, such an orientation exists. Why is it a direct graph? It's not direct to begin with, but you as. It's not direct to begin with, but you, as a part of the algorithm, the prediction algorithm, you orient it to minimize the max out degree, and then you can prove that the expected error becomes the max out degree over n. So that's the kind of idea we try. You try to improve the connection between the VC dimension and the best out degree machine. So this gives the optimal expected error, like DORN. So what other algorithms are there that also gets this DORN performance? Hans Ulf Simon beforehand. Oh, there's one more? Oh. Yeah, there's one before. Hans Ulf Simon got this result before the screen. Oh, there's a... Oh, but that was off by a log log factor. Oh, yeah, there is. No, this is. Yeah, there's a log star. There's a log star. Yes, there's a log star. That's true. Okay, yeah, maybe I should have mentioned it. It's now a true star. That's true. There's a log star. Yeah, so there's some slight, almost not suboptimal in between. Algorithm in between. So here's Steve Hadigan's algorithm from 2016. So what it does, it's based on this kind of strange sub-sampling procedure. I'll try to explain what it does. So there's this function that takes two arguments, a training set S and a set T that you're kind of constructing as you go along. So if this is your training data set represented by these squares, these are the data points, what you do is you kind of have a base case. If your training set is less than three of most 3F size, you're done. So let's get to that later. Otherwise, you take your data set and you split it into four. And you split it into four pieces of equal size. So it looks like this: just four equal size pieces, and now you do three recursive calls. And what you want to produce at the end is a whole set of collection of subsets of your training data set. So you want to kind of choose some different subsets of the data. And what you do is you do three recursive calls. And in each of these recursive calls, you're kind of saying, my new training data set is the first chunk is zero. And this Is zero and this set T is whatever it was before and then I take two of the remaining three pieces and add it to T. So I leave one of them out. So let's see. So the first recursive call looks something like this, right? This is your new S, and this is your new T, like the pink data sets there. And then you do this, there are three different recursive calls corresponding to leaving out one of each of these three chunks. Then you do the same recursively, right? You partition into four chunks, you leave out one of the three pieces. Leave out one of the three pieces recursively, and you do analysis. And then now you're down at the bottom case here. So the data set is of size less than four. So now you're done, and you output a subset that's just basically this S and T. So everything that's left is now a subset of the training there. So the pink squares are some carefully chosen subset. You take all of these many subsets that you constructed, all of these many ones, and add them all together, and now you have all these. And now you have all these subsets, and what you do now is you say: now you take your final learning algorithms, take each of these subsets, run empirical risk minimization on each of them. So take whatever empirical risk minimizer you have, run it on each of these nine different data sets, but you only give it the pink data points. So you leave out the gray ones. It doesn't get to see the gray data points. So run it on all of them, and then the final classifier is a majority over all of their outputs. Of their outputs. So that's his algorithm. And you can prove again that this has an error of DO RAN. I'll get back to some reason why we even look at this algorithm. We had the one inclusion graph algorithm. It's because this algorithm has a better performance if you actually want a high probability guarantee. And we'll get to that at the last slide. But, okay, so how many subsets are you creating? You think about it, you do three recursive calls, some data sets of size a quarter of what you had, so some into the log. So some into the log base 43, which is about into 0.79 many subsets of the training data. That's all of the linear constant fraction of the data size. Quite a big overhead in terms of training time if you do this. Okay. So the next one, so the next one is this, I guess, popular heuristic by Bryman from 96 called Bagging of Bootstrap Aggregation. So this is the simple version of Random Forest, where Where you take your training, you have your training data set, and then you draw subsets, you just sample with replacements from this data set. And so the twist is that my contribution here is to actually give an analysis in this framework for this algorithm. Oh, yeah, I guess we shorten it to an LG, right? Sorry. Yes, so you do a logarithmic number of times in the train data set size, you draw. Data set size, you draw a random subset of your training data point at the training set, the sampling with replacement, say, is half many or in half many points. So it basically looks something like this, right? You assemble this subset. You can have duplicates so that this puts a higher weight on some of the data points. And you keep doing this a logarithmic number of times. And then the final algorithm is to just do a majority vote over ERM run on each of these subsets. So just from empirical risk minimization, do ERM. Risk minimization, do your answer. So that is exactly what Bryman's original album was, but just without an analysis in this framework. So this contribution is to show this also gets the optimal error. And I guess I would say it's simpler than Hanek's previous construction. This album is also optimal in the PAC learning framework. So if you want this with probability 1 minus delta your errors at most something, then this is also optimal there. Good. So now we have these three different algorithms, and I'm trying. These three different albums, and I've tried. Yeah, but Casper is trying to be very modest. He received the best paper award for that last year from Colt. Just yeah, very making me blush. But anyways, okay, so now we try to say something negative about this really cool algorithm just to motivate our results. So for the one inclusion algorithm, I guess the negative thing is that you have to build this giant graph. And I guess the size of this graph is pretty large, right? N choose D or something by the standard arguments. By the standard arguments, and even just computing it might be difficult. For Hannigas algorithm, you have to do quite a lot of these sub-samples, like into the 0.79. Maybe it's not the most natural way of creating these sub-samples as well. And so also the overhead and making fast predictions. Yeah, it's a significant overhead, right? And the bagging one, I think, I guess it's not as bad as Hennekiss, but you still have to do this log and many subsequent steps, right? And that also gives you some overhead. Come over there. So, what we ask in this work is: what is the simplest optimal algorithm you could think of? So, is there something simple than all of these algorithms? And I think this is probably the simplest you can come up with. Majority of free. So, what is our algorithm? So, take your trained data set S, partition into three equal size disjoint pieces, and your learning algorithms just do impair and risk minimization on all three and do them adjoint work. On all three and do a majority book. So I think if you think one is not, by the way, we can prove that one alone is not good enough. What is the simplest above that? I think it's majority of three because the majority of two is not really defined when they disagree, right? So it's probably the simplest you can think of. This has an error, expected error of just d over m. And I think I do have time to actually give the proof. So the proof is also significantly simpler than the previous proof. So I think we actually do it here in the paper. I think I actually do it here in the paper. It's like a page and a half or something like this. So let's try and see if I can convince you of the alchemies inside. Okay, so how does it go? Okay, so the first obvious and key observation is that if the majority is to make a mistake, two out of the three has to be wrong. So that's the first observation, right? So if I have our input domain x here, each of these three ones that I trained, I trained one on each of these pink subsets. If I make an error, uh two out of the three has to be has to be wrong. Two out of the three has to be wrong. Okay. So, and also the second key observation is that each of these three predictors are IID, they have the same distribution because you get data sets of the same size, the samples are IID, and it's the same ERM that I'm running. And that's actually important for our analysis. You can't take two different ERMs and run them. At least our analysis doesn't work then. You run the same ERM algorithm on all three subsets. So, what does that allow us to do? So, let's try this. So, I'm looking at what is the expected error of this. So, I'm looking at what is the expected error of this classifier or predictor that I produce. Yeah, that's right, just a clarification about the algorithm. So, for a particular subset, it's going to allow you to eliminate a bunch of hypothesis space, and there will be a bunch of potential hypotheses. So, how do you decide which one it votes for out of the set of feasible ones that are consistent with that? Just choose an arbitrary one. The proof works no matter what. The only requirement is that your album that you run picks one that makes no mistakes on the training data. You can pick an arbitrary one as long as it's the same album you run. One, as long as it's the same element you run on each of these instantaneous ones. Okay, so what do we say? So, what is the expected error? Well, it's just the expectation over the training data set, the expectation over fresh data point making a mistake. And then what we see is, well, since two out of the three have to err, basically there are three choices of which two to error. S1 is two, as one is three, and as two is three. They all have the same probability because it's the same distribution. So by union bound, the error is at most three times the Bound, the error is at most three times the probabilities that both of S1 and S2 make a mistake. Okay, so then we just swap the order of expectations. So we say three times the expectation of a data point, and now the expectation over two fresh training sets, and that they both make a mistake on this concrete data point X. And I guess now we can again use the data IID so that each. This is that each of them make a mistake, which is the product of the probabilities that they each err. So basically, the error is at most three times expectation over x, expectation over a single training data point, basically the probability squared that a single one makes a mistake. Okay. Good. So let's now, the idea is let's try to look at an x in the input domain. So this is the next idea. Let's look at some fixed x and let's just let And let's just let px denote this probability over a fresh training data set that I make a mistake on this particular x when I run my error. So, right, so this basically just exactly the probability that over a fresh one I err on this data point. Now, this is just the expectation of px squared. Okay, and so now the next key idea is to try and look at the input. The idea is to try and look at the input domain and partition it into different subsets. We're going to take the whole input domain x, now we're going to partition it into those x where my ERM errors with the probability that lies in the range 2 to the minus i, 2 to the minus i plus 1. So I'm kind of partitioning into data points in the input domain where I kind of err with roughly the same probability when I train a fresh ERM. So basically, there's some portion of the input. So basically there's some portion of the input domain where my ERM often makes a mistake on these data points, and there's some where you make a mistake a little bit less often and less often and so on. So you kind of group them based on how often does my ERM make a mistake on this data point. So then I can just kind of sum over all these RIs and say, well, my expectation here is just the sum over all these RIs, the probability that I get a data point from within this region, and then the condition. Region and then the conditional expectation of px squared. And then the next trigger is that okay, I can get rid of this first complicated term because conditioning one lying inside Ri, I know that the probability that I err lies in this region. So I can kind of just all simplify it also, so just say, well, it's at most two to the minus i plus one for all of those lines. Okay, so now I'm down to, well, it's the sum of all these groups, two to the minus two i plus and squaring it uh plus two. And squaring it plus two. So, all we need to understand now is just this probability of seeing a data point from within this region Ri. Good. So, somehow Ri, like for small values of i, you often error in this region. So, somehow we want to use this to our bound the probability of actually seeing a data point from Ri. So, how can we want to show that the probability of seeing a point from RI is small? And so, what is the idea here? So, there are basically two observations. So, the first one is: how many data points do we expect to see from within our eye? The expected number of data points is just what the size of the train is times the probability of seeing a point inside. So, let's call this Ni, and that's just n over 3 times the probability of seeing a point from the NRI. So, if RI, if you have a large probability of seeing data from RI, you expect to see many samples from the considerable. Okay. Okay. And just for the simplification, let's assume you always see an eye point, but just for the goal, otherwise just the concentration boundary, you often see a bounded many. Now, the second observation is that the hypothesis you train, right, because we're in this realizable case, so you know that the ERM is going to produce something that's correct on all of its training data. So in particular, it's also correct on these NI data points, right? So now you have something that's correct on all of the data points that you see. Of the data points that you see basically from this conditional distribution of a sample from our eye. It's kind of the same argument that's used in Hannicke's proof and Simone's proof and my own. So basically you can show that the error under this conditional distribution where I give you a sample from Ri is basically you can just a classic bound score ERM where you get this D over N times law N over D, but now you only have Ni samples instead of N samples. Does that kind of make intuitive sense? You see Ni samples. Intuitive sense. You see NI samples from within this region. These classic results, basically, this uniform convergence balance that tell you that everyone who gets all the labels correct has an error of at most D or N log N over D. In particular, since we didn't realize the case, it makes no mistakes on these data points. I don't really know what NI is, so like. We don't know. Yeah, we don't know, no, but the important point is that it has this term inside of it that we're trying to bounce. So that's the crucial part. Crucial part. So we know the expected error is now most is n d over n i log n i over d. What is the other observation? By definition of these R i's, we also know that we often error inside of it. We know that we err with probabilities at least 2 to the minus i. So that's just kind of, we truncated this region where the error could lie. So now we just have this inequality. 2 to the minus i is less than or equal to d over n i log n i over d. And now it's just solving this inequality for Inequality for as Ni is exactly contains this term that we're looking for. So, what does it solve to? So, it looks a little ugly. There's something with D over Ni. Now I wrote it out as the probability of seeing a point from inside the Ri times N, and then log of the ratio. And if you stare at this for a little while, you see that, okay, maybe I have to simplify it. Let me let Y be n times the probability of the X line in our I divided by 3d. X line in Ri divided by 3d, and then you can see this is just c times log y divided by y. This is these 2 to the minus i. You solve it, you get something like y has to be ordered 2 to the i times i. So you cannot, so that's the y here. And if you then reinsert what y was, you basically conclude that the probability of seeing a data point from within RI cannot be too large, right? It's 2 to the i times i times d over m. Otherwise, you just error too much. From just error to my error very often in there, so you cannot see many data points. And that's enough. So now we just take this and put it back into the previous formula here. If we insert 2 to the i times i times d over m, and we get, well, we have this 2 to the minus 2i term still. We have a 2 to the i d over m. And the nice part is, of course, this 2 to the minus 2i is going to peel both the i and the 2 to the i. This goes. This term goes down much, much faster than these terms go up. So the final sum here is the sum of i over 2 to the i. So why did you use the fact that the interbound is bounded from below by 2 to the i plus 1? We did that when we squared the px squared to get this part here. Yeah, exactly. So one of them was to show this is very small and the other one is to show that the size is small. Oh okay. That's the minus 2 times pi plus 1. Minus 2 times i plus 12. Yes, exactly. So now it's just the sum of i over 2 to the i, which sums to 2. And so all of these terms just go away, and we just have t over n. So I think it's like a super short proof compared to if you look at the previous ones. But the whole key is to look at these sets ri and just to, you know, unbox. Yeah, so I guess the point was that px squared, you go from an error of 2 to the r to 2 to the minus 2. Go from an error of 2 to the i to 2 to the minus 2i, and that kills everything. We had a follow-up paper where we had some bounds that were, instead of being sub-optimal by a log factor, they were sub-optimed by polynomial factors, like a square root by dO rn for some similar problem instead of d o rn. And then it turned out you could just do a majority of five, because then you get it two to the minus three i, and that killed the suboptimal T. So somehow it's a nice way of killing suboptimal behaviors if you just do a majority of a couple of these things. Good. So I think that was the conclusion. Just take your data set, petition it into three. Just take your data set, partition it into three, and do a majority vote of all of them. That gives you the optional error. So I think there's still some open question: what if I actually want something to hold high probability that with probability 1 minus delta? Our algorithm actually has a non-trivial guarantee. It gives you this. So the optimal behavior is d over n plus log on over delta over n. The suboptimality in our bound is this log log term here. Bound is this log-log term here. But it's not very sub-optimal. It's the log of the log of the minimum of n over d and 11. So there's something that we couldn't get rid of. We conjecture that it is actually this optimal also. Yes, that's right. In Steve's proof, yeah, in Steve gets it, and in my follow-up for banging, we also get the optimal one. Yes, that one inclusion does not have the optimal one in this version. There's another paper by Nikita and Nishak and some other Berkeley students that do a twist. Students that do a twist to the one-inclusion graph album where I think you do n-half, do a majority road amongst one-inclusion graph albums run on different prefixes albums. So there are a couple of different other albums that gets the optimal also in the high probability museum. We haven't been able to squeeze off this last log log on what else affected, but and then, yeah. Interestingly, Simone, in the other paper you mentioned, had an analysis of majority of three. Majority of three, where he had a DRN log log NOD without this. So after Speda with the minimum of the two, we actually, so he did. An interesting thing is in, I think I have two minutes left. In his analysis, he didn't do three disjoint pieces. He said, this piece, this one includes all of this, and this includes all of the data. And we actually show that if you do that, there is an ERM you can plug in, and then the log lock is actually there. So it's not optimal to do it the way he does it. So his analysis was tight. If his analysis was he type. That's right, yeah. So it's actually important that the disjoint, completely disjoint these pieces for this to work, which is, I think, is an interesting effect. Yeah, that was it. Thanks. Nothing comes very short.