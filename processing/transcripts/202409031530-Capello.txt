Finally published, so I'm kind of giving it a victory lap. The second part is something that actually I've been quite actively working in the past year and a half. And you know, we are essentially to a point that we really like the idea, but the current framework that we have, I don't think it is working that well. So actually, I think I'm going to point it out pretty strongly. And since, you know, I think because of the nature of the workshop, Because of the nature of the workshop, we are a small group. I think it's nice to bring up almost an open problem. Well, we have a way that we would like to pursue, so it's not fully. It's already a little bit set up, but I think hopefully there is going to be the chance to discuss. So the agenda is bringing, so it's not really what we achieve in this talk, but the agenda is the scalability in this framework of coalescent models, which I'm going to try to review for those of you that are not very familiar with coalescent. With coalescent, which I think is a very fascinating area of research. It's a very fascinating area of research. And I think it's very fascinating also that I think it's one of the few areas of application where vision models are truly dominant. Like I think that that's the approach that everyone uses in the literature. People are very familiar with this idea that get talked into our conferences. And this is a joint work with Skooi, Madin, and NJ towards the end. So I start with the background. So I start with the background and actually mostly like, so what are we doing? So the data set that we deal are kind of molecular sequences. So we get kind of molecular sequence from a single population. So we don't observe stuff from different species. We're working within a species. And we are really trying to model the dependencies of this data. So we don't treat them IID, but actually what we really care about is modeling the full evolutionary history of this that led to these data. That led to this data set. And usually, that is done in a way where it's kind of a combination of two stochastic processes, a tree-valued process. It doesn't necessarily be a tree, it could be a graph, but I think about it for today that is a tree-valued, that sample genealogy, and a process of mutation that gives rise to essentially the diversity that we observe in the sequence. Both of these processes depend on a bunch of parameters, a bunch of parameters auxiliary. A bunch of parameters, auxiliary variables. And depending on the scientific question that you are looking at, all of them could be interesting. You could be interested in the genealogy, so in the tree, you could be interested in mutation parameter. There is a measure, for example, that I'm going to talk at the beginning of the talk, mostly because I've been doing a lot of work with it, which is the effective population size that is a measure of genetic diversity over time that has a definition within the coalescent framework, but you could. Framework, but you could roughly think about it as a paroxy for population size. Like the larger is the demographic, the genetic diversity is the larger is the population size with some new assumptions. And you know, this is an area also that is finding new applications. For example, there is this very exciting application on single-cell lineage tracing, where a lot of these things are being repurposed at the moment. So, just to maybe give some Maybe give some idea of the application that we do. For example, this one is one that I've been working a lot. Kind of you can take, for example, ancient DNA and studying like how the population of an extinct species has varied over time. There was this very famous nature paper that put these ideas on the map, where essentially like they took specimens from a bison that went extinct, they study how the population varied over time and Varied over time, and starting from understanding the location of the decline, they try to argue whether this was a human-induced extinction or a climate change extinction. And, you know, for example, with the model that we're going to talk about, we analyze a new data set that has been recently collected, always by Best Shapiro's group. And, you know, essentially, like, we get less strong of our evidence for our environmental. For environmentally induced extinctions of the bison, because our decline is actually closer as well to both the last glacial maxima and there is evidence of human presence in North America at this time. This is very recent evidence, like I think we saw a Mexican group that start suggesting that already around 30,000 years ago they were already humans. 30,000 years ago, there were already humans in North America. On a completely different side of applications are these people that work in molecular epidemiology. In general, I think that groups in the UK are very strong about it. So essentially, they are trying to pair this molecular method to traditional epidemiological counting methods. So there is a way, for example, Eric Woth at For example, Eric Voss at Imperial has done a lot of work about it to bridge this idea of how the genetic diversity of a population varies over time with how many infected individuals there are around. So essentially, like you compare traditional methods like counting, doing swaps of certain products, with the kind of like while you do the swab, you can also get information about the molecular sequence. The molecular sequence you can sequence and you can compute this measure of effective population size. That obviously, here I'm kind of just eyeballing, but somehow mimic the number of infected individuals at a given time. This was in California in the first month of the pandemic. So how do we do this analysis? So essentially, like in the most basic ways, there are many flavors of doing this type of inference. Probably fell in love with Globalists felt in love with coalescent inference, and they're coming up always with new coalescent methods. But the most basic one is a Kim and coalescent, and essentially, like it's a process that is fully defined by two chains. So it's an homogeneous primary microchain that is fully defined by two chains. So somehow you should think about that we're looking at evolution backward in time. So here, for example, the sequence are sampled today, and we're looking going backward in time. We are trying to understand. We are trying to understand who was a common ancestor to who and when someone was a common ancestor to. So the jump chain defines the mating. For example, A and B have a common ancestor. The pure deaths essentially correspond to the jumps of a polymer process and tells you when the qualities event happened. So quite essentially notice that in this Kieman qualescent tree, both the leaf label Both the leaf labels and the internal node of the tree are all labeled. Given a tree, you need to assume a process of mutation. There are many that you can assume. The simplest one is the infinite size model. It's a very simplistic model that kind of works still for humans. It depends essentially on the mutation rate. There are other alternatives, but essentially I correspond essentially to what was some process acting on the branches of the tree. Process acting on the branches of the tree. I think it's good to think about a simple implementation model for today. So, just to reinforce, so we have nano-observed trees that mimics evolution. Obviously, it's just a cartoonish representation of evolution. And we have a mutation process acting on it, but our data set is actually the set of molecular sequences. So, what we target, like we will be targeting different parameters. In this talk, I chose three, like the effective operations. Three, like the effective population size, the topology g, and the vector of times where coalescent events happen. Usually, the two together are called genealogies, what are referred by genealogy. So, essentially like it's a discrete structure where we add also the time information matters. Note that this structure is essentially like one of the most common formulations of the problem, because if we were just interested If we were just interested in the factory proportion size or in the mutation parameter, for example, we would need that the genealogy and the coalescent times anyway, some sort of like augmented likelihood, you know, because otherwise we are not really able to evaluate the likelihood. There are ABC approaches and so on, but generally I would say that this is essentially like the state of art that is implemented still by most of the methodologies. Is this setup clear? So, there are various computational challenges. There is an understanding, and this I think has been since these are spaces also that are difficult to represent, that there is an understanding that this posterior is highly multimodal, and the cardinality of the state space, the cardinality of the space of topology grows super exponentially with the sample size. And, you know, and there is still an understanding. There is still an understanding, so these are always kind of soft numerical understanding of people that are working on these models a lot, that the multimodality gets worse as I'm increasing the size of this discrete space, which is not necessarily the case, no? But it seems that is the case in this case. And we observe it numerically and theoretically, even if you study the mixing time on this space of Markov chains. Of chains. You have that the mixing times of these chains, even not targeting the posterior, targeting even simple stationary distribution, are very, very low, like polynomial to a very high order. So we have, we essentially like we are factoring scalability in two ways. So this is the part that I was saying that is already published, that is a bit older, but I think it set up the stage to some of the ideas that we are failing. idea that we are failing at at the moment at completing but so a looking at what so we're saying that one of the issues is the size of the states of topologies so essentially like we're looking what what is this space comprises of and for example one if i take for example if i fix this tree the space g of topologies in bijection with the Keeman coalescent has most of the permutation for example most of the permutation for example of ABCD. So like not really all of them, it doesn't account for the permutation within this structure, which you call archery, but includes most of the other permutation. But actually when I look at the cartoonish data set that I was working with, where I only have a single mutation that characterizes four sequences, essentially like I don't really care about permutation of these four sequences, because essentially like any permutation Essentially, like any permutation of ABCD does not change the likelihood. It's somehow equivalent, defines some sort of equivalence class in the space of like good. So, I think that, you know, the fact that I'm pointing this out, I think that you can understand where I'm going to with the idea that the first idea is to work with a, let's say, it's not really a sufficient statistic reduction, but it has this vibe that we are dropping the leaf labels from the tree. So, quite importantly, we have So quite importantly, we are keeping the internal node labeling, we are keeping the internal node labeling, but we are dropping the leaf labels. And the space still grows super exponentially, but we are cutting the rate massively. And we can do statistics with it, essentially. We can define an equivalent process to the Kiman coalescent. We call it Tajima because in 83, actually, one year after Kiman, it was the first. Actually, one year after Kimani was the first one that suggests to do inference somehow this way. And essentially, the process is identical to before, except that we need to account for the fact that the leaf elements are unlabeled. This creates pretty giant complications to compute the likelihood, which I don't want to really enter into it today. But the only comment that I would like to make is kind of It's kind of we have a more formal result in the paper, but I would say that a first question that we were struggling quite a bit: they say, Well, you know, why? So we are still talking about massive spaces, but not even countable. You know, as statistician, we just place a probability distribution on a countable space. So you should not be scary the idea that the spaces are just very big. So I think that this is a cartoon that we're, that at least I try to represent why. That at least I try to represent why we think that it is a good idea. So we take one data set that I sample at random of six samples, and I compute the likelihood under this Tasijima coalescent for all the possible topologies. There are 16 of them. And for all the Kieman topologies, there are 2,700 of them. And I think that what I kind of would like to point out in this is that. Would like to point out in this that the let's say that the ratio between the mean the minimum like the tree and the maximum like of the tree is substantially smaller under the Tajima than under the Kingman. And kind of what we hope that this effect has is that when we talk about multimodality, hopefully we get an effect of this type. Obviously, this is just a cartoon, it's nothing, but the idea is that we always But the idea is that we hope that by changing, by kind of somehow making the likelihood value closer to each other, we actually make it easier to traverse these values. Why this effect should consistently happen is with this idea of the equivalence class. For every topology that we fixed, essentially we account for a massive amount of king matrix that corresponded to apage matrix. So we are summing a bunch of likelihood together. So that consists. Together. So that consistently happened. We have some results more formally on the Monte Carlo variance reduction, but I think that this is still intuitively a little bit what we are a little bit more satisfied about. So, and you know, we have just have other extension to accommodate for features that happens in applications. So, the rest is kind of really, it's still a lot of work, but it's really making it work. Making it work, you know. So, like, for example, we can accommodate the fact that we have different sampling groups, like, for example, in viral application, in ancient DNA application, data are not sampled today, you know, that data are sampled, for example, are carbon with carbon dating and deciding 30,000 years ago, 40,000 years ago. So, we have samples at different points in times, and so it happens for the viral sequences. So, we can accommodate that. Accommodate that we can accommodate other mutations model. The price that you need to pay for fancier mutation model is essentially like that you need to work at hybrid resolution between the fully labeled tree at the bottom and kind of some form of partial labeling. For example, here you say, okay, I have three sequences of one type, I have two sequences on one type. Type. I have two sequences on one type, another type. Note that maybe I don't know if this was a comment that I could have made before. While in phylogenetic tree, people generally really care about the labels of the leaves. In this case, we don't really care. You are taking just a bunch of viral sequences. Who cares who is really carrying them? We are trying to use this information of the tree for other downstream analysis. Downstream analysis. So that's not really an information that I would say that no one is really bothered in losing, except that it could make the right code calculation quite a bit more challenging. We started with backtracking and now we have polynomial time algorithm to do kind of those. Because the problem is that essentially you need to find all possible ways that you can allocate samples on the tree. So that's quite a daunting task. Okay, but so these kind of two concludes. So this kind of to conclude the first idea, which is essentially like say we hope that this gives an intuition of being more efficient by mean of a better model. And essentially, so yeah, that's the change that we're doing. We are changing a Kingman coalescent with this idea of Tajima coalescent. And for inference, we are pretty much using what everyone else does, you know, so with this, you know, we're trying to do our best. You know, we're trying to do our best, but mostly, I would say that what I kind of want to point out is that in terms of the topology move, we are essentially relying a lot on state-of-the-art software that do this type of analysis, like Mark Souchard Group has a big one that is called Beast. And essentially, like, they alternate a bunch of teleoperators. So, lumping of the coalescence, scalabilities to a model. Scalability through a more efficient stochastic process. So that's the first idea. The second idea is in the direction of algorithms. So like plug in already these models and move forwards with the idea of the algorithm. That's kind of what we hope. And you know, I think that we are using some ideas that are in the literature, but I think again we want to build this bridge with this unlabeled language. Unlabeled a lung tree, which I think that they become useful of to build an algorithm as well. And this idea of building, while in the model space, in scalability, I think that there hasn't been done that much. In the algorithm, the space has been much more crowded, not really for this type of problem, but there is kind of a neighboring literature with this phylogenetic literature. So there are some ideas, but still, it's still really an area. But still, like it's still really an area of ongoing work. And I think that some of you are familiar with all this work, with some of these projects. Everyone knows that these authors keep coming up with new proposals because it's really an active area of research. So we're kind of trying to chip in coming more from the coalescent world rather than the phylogenetics world. And I think that kind of the key ingredients of our DIY comes in these next two slides, in the next two slides. In the next two slides, that essentially like that analabel ranked tree has a unique representation as a lower triangular matrix, which I don't know, JE, Julia, J. Kim, Noah Lausenberg, Julia call it F matrix, but essentially like it's a lower triangular matrix that satisfies some linear constraints. So it's an integer value matrix that satisfies some linear constraints. That satisfies some Eurolian constraints. And these constraints are fully characterized. To the point that, you know, one can prove that these F matrices are in bijection with the space of Lung Tree shape. There will be a set of linear inequalities. Some of the inequalities, you can already guess them. Within a column, you can always be equal or go down. The elements of the diagonal matrix are increasing by one. There are all these types of constraints. There are three sets of linear constraints. Three sets of Reiner constraints, but these F matrices are fully characterized. Like any encoding, the beauty of the encoding from a discrete object, from an unstructured data to a more familiar numerical representation is that we can start working with more traditional tools. So we can define distances, many ways of choosing a distance, but we can define a distance, for example, an L2 type of distance. Example, an L2 type of distance where we are taking difference between F matrices and so on. We can compute summaries, you know, in this space. For example, you can compute Fresher means and there are algorithms that are quite efficient to compute the Fresh A mean in this space. But I think that in the interest of being an algorithm, our idea is building an exponential family, kind of a Gibbs family. So essentially here we are having, like, no, it's just an exponential family. It's just an exponential frame. I don't think that there is that much to say. So we are raising a distance, an arbitrary distance to the exponent. We are normalizing it with a partition function. And essentially, like we have two parameters, M, which is some sort of central tree, and beta is a concentration parameter. If you set beta equal to zero, you have the uniform on the space of trees. The uniform on the space of trees, so essentially that you have the Kieman coalescent. So, the Scardo-Z model includes the Kieman coalescent, but is a bit richer than the Kieman coalescent. I think that we can compute many properties that the Freshmine under this model can be unique for certain choices of beta and so on. But I think that really, like the key things that we're kind of obsessed about is that M, while I think that While I think that in its initial definition we can think it as a central tree, M can be a lower triangular matrix that has also continuous entries, so that somehow is interpolating within the polytops that is defined by this F matrix. And I think that that's fairly interesting to us. That's what That's what started a little bit the obsession, because essentially, like, we are changing the problem from having to deal with this complicated, discrete object to something that we kind of compute gradient. So, if we want to optimize over this M, we can compute gradient. So, that's so that's that's essentially like the source of the obsession. That's the source why we think that it is potentially a good idea. So, the first attempt. So the first attempt has been saying, okay, why, since you're not thinking about optimization, why don't take our posterior and approximate it with variational base? Let's not worry too much. These are continuous parameters. So it's really like most of these have parallels that belong to the exponential family. So it's not really an issue to infer these. But the idea, I think, that, you know, that's what we That you know, that's what we hoped for a while that would have been our secret source was having a variational family, which was the scale location exponential family that we presented in the previous slides. And you know, we kind of push it through, we kind of push it through in the algorithms. I think that those of you who are familiar with variational inference kind of already see that is a partial, yes, very stupid variational family, you know, because. The variational family, you know, because usually we choose this family because it's very tractable, you know, whereas these ones, we are essentially like having the same complication of an Ising model, you know, that we don't have the normalizing constant and so on. So the big plus here would have been really like the continuity of this parameter, despite that is a continuity of this parameter lying in a parallel, weirdly constrained space. So we cannot even come up with an easy transformation that makes such. Easy transformation that makes such inequalities such that a transformation of m take is a real value, you know, something like that. So we have never really been able to come up with a transformation of m that makes these constraints these dapiers. But you know, we tried, we have been working quite a bit this year. And actually, you know, a lot of the things actually can be written down quite analytically. So actually, we were able to write the stochastic variational inference. The stochastic valuation of inference algorithms for the normalizing constant. Well, you can plug in, so at least you know, we know the cardinality of the state space. So, like we can use some of this work that has been done for other Gibbs family. And most of the gradient step we can compute them in close form. So we have the gradient of the likelihood in close form, it was a paper by Mark Soucher and co authors. Balmar, Suscher, and co-authors. And for this exponential family, we can obtain that most of the things, so this is just a piece that enters into the family, but we can actually write the stochastic gradient descent in pretty construct. So we almost don't even need to use much of a numerical approximation of the gradient. Sphere kind of, in our experience, is not bringing satisfactory good results to kind of justify pushing these things much. Pushing these things much farther. Like, we have some nice examples, but it doesn't really, I think, justify justifying continuous. So, but still, like, do I still have a little bit, two minutes? Yeah. No, no? Ah, okay, no, I can interrupt, but just to give you maybe like a background to start the discussion, is that what are our ways forward? So, one way is forward. So, do you remember that in the Forward, so do you remember that in the previous step, we substituted the stratigima coalescent as parallel on the topology? And essentially, that's why. So, essentially, like the idea would be like actually to get rid of that and directly work kind of with this new family, which with the idea of preserving the benefit of continuity. So, maybe one can do something like. Do something like Hamiltonian on everything since we can compute the gradient. Or looking, for example, these little weighted Bayesian bootstrap, essentially, you know, where when sampling is costly, exploring is costly, but it's good to optimize. So I was mentioning we can compute fresh meanings easily. So that's actually something that we are trying mostly at the moment. Or, you know, like I said, this is something that I would be happy to have some contribution from. Have some contribution from your all. Okay, so these two ideas known to scarability. Offer a more efficient model and offer a hint to start a better algorithm. And this, I think, that is very necessary just because things become more complicated. For example, now I'm working a lot on this strode sinkway coalescent that we are not going to enter into it, except the fact that the Keeman coalescent, the Tajuman coalescent, is just a simplistic. Coalescent is just a simplistic type of coalescent. As I said, probabilities went crazy, and the inference didn't really cap up with the literature. So there is a lot of work to do on this. Okay, thanks. Thanks a lot. In the sake of time, now we're running 10 minutes later.