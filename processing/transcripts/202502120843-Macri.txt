Thank you. Thank you for first of all for the invitation and the opportunity to be part, even though virtually of this workshop. I'm based in Marson and I know some of you from previous interactions. I would really love to have the opportunity to meet you with you in person at some point very soon. So, today. Very soon. So, today I want you to talk about some of the activities that we've been doing at Coera Computing. Quera Computing is a quantum computing company that is headquartered in Boston, next to the Harvard and MAT campus. And we actually started a spin-off of these two academic institutions. And as one of our goals, I mean, our main mission as I mean, our main mission as a company is actually very close, very attached to what many academics have been trying to do in their labs over the past 20 years or so, which is basically to build quantum computers based on a specific technology called neutral atoms that, as of today, is considered one of the most promising technologies to really develop fault-tolerant scalable quantum computers. In a sense, In a sense, we are considered both by other peers as well as by customers as one of the leaders, both scientifically and commercially, to really develop this type of technology. And we are working very closely, actually, with many academic groups through grants or specific collaborations, really push the boundaries of this technology. As a company, we are focused not only at building the computers, but also Really, at building the computers, but also really creating all the stack to make these hardware or these devices really useful for a broad audience. So, we build a software. In particular, we have our own open source toolkit, which is called Blockade, that is available in Julia and Python. And is really a tool that can allow people to start really programming quantum. To start really programming quantum computers, both for their analog and digital modalities. And I will dig a little bit more into that in a couple of minutes. On top of that, of course, the goal is to really build and create applications that are interesting for scientific discoveries, but potentially also to develop to attack potentially business verticals and this is because this specific technology also Technology also got a lot of attention recently from very many stakeholders that go also beyond academia. And in particular, what we found is that there are three main classes of problems that can sort of summarize, that can exploit at the best the capabilities of these devices that fall into the categories of combinatorial optimization, quantum machine learning, and quantum simulation. And I will tell you a little bit about. Simulation, and I will tell you a little bit about that in a few minutes as well. So, why do we believe that neutral atoms is actually a very good platform to work with quantum computers? First of all, differently from other devices that are based on what are called artificial qubits, such as superconducting qubits, neutral atoms do not need to be characterized individually because they Characterized individually because they are all identical. We use Rubinium-87 as a species, atomic species to perform really stored information and execute operations. They are, in a sense, really pure and resistant to noise. And most importantly, they react to external electromagnetic fields all in the same way. Second important feature is that they are relatively easy to scale. We have machines. We have machines today with the hundreds of qubits. There are demonstrations with thousands of qubits. And we also believe that the technology can scale to tens of thousands of qubits without the need of really creating interconnects between different processors. Then there are certain capabilities. I will mention one overall, which is called the qubit shuttling, that allows qubits to be moved across the computational region. Computational region to make basically to create more efficient applications or more specific and more technical, technically speaking, more very more efficient problem encodings, and also the possibility of creating or implementing quantum error correction schemes without the limitations of having a fixed topology. Last but not least, this technology is. Last but not least, this technology is actually very helpful in the sense that, or is actually very efficient, energetically speaking, as well as from the perspective of the experimentalists in terms of addressability, because we operate to room temperature, meaning that we don't need any cryogenic device that would otherwise complicate the layout of the machine. Layout of the machine, as well as its footprint and accessibility from the experimenters. A little bit of physics, which is what we are eventually interested in. I mentioned at the beginning that we work with the Rubidium-87. There are in principle many other atomic species that might be really useful for the implementation of the encoding of the qubits as well as their operations. The basic concept underlying the choice of a specific atom is due to the fact that, of course, there are, in principle, infinite atomic bound electronic states. And depending on the type of encoding, meaning the specific choice of energy levels that we decide to store information of our to Of our two basis states for the qubit, we can encode or we can realize different modalities of quantum computation. For example, if we choose to pick one of the aperfine states of rubidium, as well as a highly excited electronic state that goes under the name of Rydberg state or Rydberg level, we then can create what is called an analog quantum computer that basically works as a Works as a continuous time evolution under a specific intrinsic Hamiltonian that is time-dependent that acts on a set of qubits that corresponds to our register, typically initialized all into the same state. This zero would represent in this case this F equal to a refine state. And then we evolve in time, continuous time, under enamel. Continuous time under an Hamiltonian that I will show in a minute. And finally, take some measurements and post-process the data on a pure classical basis. The second modality that actually the other feature important feature I want to highlight is the fact that we have also the possibility of creating very different geometries. And these geometries are actually very helpful to encode different types of problems. Encode the different types of problems, whether machine learning and quantum simulation of exotic phases or matters, and so on. For example, here, what I'm showing is just a square lattice, but we can create, for example, a Kagome lattice, or even a map of the world, so really a sort of random displacement, random positions of the qubit array. Or we can even, by superimposing different images, really write down the first hello world. Really, write down the first hello word with a neutral atom quantum computer, which is sort of amazing. Each one of the dots that you see is indeed one single individual atom. The other modality that I wanted to mention is the so-called digital or GAT-based mode that is instead based on a different type of encoding where we map the two. map the two basis states of the qubit into two aperfine states. Here we choose for example f equal one and f equal two over bedup. Now this guarantees this choice guarantees a much longer coherence time where in the analog mode we have typical coherence times of the order of microseconds. Here we can achieve orders of seconds, meaning that we can instead do many operations. And here the evolution or the, if you want, the evolution or the if you want the the processing of the information takes place by writing down what is called a circuit that can be implemented by performing many operations either at the single qubit two qubit or in principle also multi-qubit level in a discrete purely in a purely discrete way so in contrast to what we have seen in the analog modality I also need to sort of highlight that even in this Sort of highlight that even in this digital orgate-based modality, we still employ the highly excited Rebecca state, and this is actually necessary in order to perform multi-qubit operations, either two-qubit or multi-qubit operations, because of the strong interactions present between states, between atoms that are excited to a Rydenberg level. So, sort of to summarize, there are essentially three main ingredients. We have atoms. We have atoms, we have lasers. These lasers are, in particular, the ones that trap the atoms are called optical lasers that basically stochastically pick these atoms and make them fixed into a very well-defined position. We can take pictures of these atoms. So, what you see here is actually a picture of a single atom. By using additional laser fields, we can then excite these. Excite these atoms from the ground to their brake states and create, for example, rabbi oscillations between these two levels. We can then extend the system to multiple qubits. And here what you see is a system essentially just two qubits. And eventually we can make them interact by exciting them to a Ryberg state. Now, depending on the distance between Depending on the distance between the atoms, the interaction can be actually very strong or totally negligible. The interaction strength for given the levels that we excite are is of the type of a van der Bas interaction, therefore decays as one over R to the six. And if we place the atoms sufficiently far away, so of the order of five to ten micrometers or even more, then we can expect the interaction to be totally negligible and to be able Totally negligible and to be able to excite simultaneously the atoms or independently from the ground to the reblack state. On the other hand, if we place the atoms close by, then what happens is that the interaction is so strong that we basically inhibit the simultaneous excitation of these two atoms into the rebirth state. We then can create what are called correlations or correlated states. Correlations or correlated states, in particular for two atoms, this goes under the name of a Bell state. If we have multiple atoms within the same locade sphere, we can create what is called more appropriately a W state. And this is also at the basis of the creation of interesting gates or two-qubit or multi-qubit gates for our atomic arrays. The mechanism overall is called a reburn blockade, and I'm sure that you. A red burn blockade, and I'm sure that you might have seen also in different contexts because the mechanism is essentially very similar to what can happen also in other condensed matter systems. Let me, of course, if you have any questions, feel free to interrupt me. I'm also happy to go a little bit more into the details of what I'm describing. But before going to the application, let me just mention what is the structure of the What is the structure of the Hamiltonian in the case of the analog quantum computing? And here, what you see is that we have essentially three main ingredients or three main terms. One is the interaction term, VAJ, that again is of the type of a van der Waas interaction, so decaying as 1 over R26. This Ni Nj represent the population of the atoms into the excited Reberg state. We have an additional term that commutes with the last term. With the last term that is proportional to a parameter called the tuning that can be either local or global. And from the statistical physics perspective, this tuning is, or this term, delta N is what we would call a chemical potential, meaning that if Bij is anti-ferromagnetic, so if Bij is repulsive and delta is positive, given the fact that there is a minus sign in front, basically the larger. Basically, the larger the value of delta, the larger the name of excitations that we can squeeze in into our system. So, basically, the same interpretation of what the chemical potential would do in a classical statistical physics system. Last term, which is the first one here in the Hamiltonian, is proportional to the real quantum term that flips the state of the qubit from the. The state of the qubit from the ground to the excited state, or from the state one to the state R, and of course, is non-commuting with the other two. And so this is really the term that makes the whole system a quantum system or a quantum Hamiltonian. It is proportional to a parameter called the rubber frequency. And what I want to highlight is that these terms, omega, its phase, and the tuning, are actually time-dependent. time dependent. Now, at the experimental level, so also from the user perspective, what we can play with is precisely the path shapes that make this system only lemmatorium time dependent. So we can create, for example, rectangular passes, which are typically used, for example, in implementations of certain Of what of certain algorithms that are called that go under the name of QAOA, that are used for optimization problems, for example. We can create more regular pulses that can be used or can be employed for the study of more adiabatic type of quantum evolutions, or we can create really sort of arbitrary pulses that would drive the system very far from equilibrium. On top of that, let's keep in mind that we can still play. Let's keep in mind that we can still play with the geometry of the system. So, we really have a combination of two important tools at our disposal: the geometry. We have systems that in our device can go up to 256 qubits, and we can play with the structure of the or the time dependence of the pulses. Very good. So, all of these tools that I described have been incorporated into Have been incorporated into a device that we call the Aquila. In English, Aquila means eagle. And in November of last year, Aquila, our first quantum computer with 256 qubits working in the analog mode, completed two years since it was made available to the general public through the cloud in combination of a service or thanks to the Service, or thanks to the partnership with AWS, which bracket, which is the service from Amazon, giving access to a number of quantum computing devices. And from the engineering point of view, which is also a very important part of the work of the job of our team, we were able to extend the availability window of this device from 10 hours back in 2022 to more than 100 hours per week. 100 hours per week from as it is today. This machine can be used to address a number of different problems. I mentioned it at the very beginning that span from machine learning for classification and prediction or regression, time series. And we developed a specific algorithm that we call regional computing and that I will describe in a minute. In a minute. The second class of problem is called optimization or combinatorial optimization. I will not have time to dig into that that much, but I want you to highlight that there are certain specific problems that are native to our machines. That is specifically the type of problems that go under the name of maximum independent set. And there are also other non-native optimization algorithms, such as what are called the quadratic. As what are called the quadratic and constraint binary optimization problems, or CUBO, to make it shorter, that can still be solved or approached with our device, but require a certain overhead in terms of the number of qubits with respect to the number of variables that describe our mathematical problem. So, if you want, I can go a little bit more into the details of that, but let me also just mention that. But let me also just mention that typically these are the types of problems that are more relevant for commercially relevant applications, finance, logistics, transportation, and so on. And finally, probably the area that we like the most as a physicist is the area of quantum simulation, following the idea, of course, of Richard Feynman more than 40 years ago to study a variety of problems that can. Variety of problems that can go from range from the equilibrium, non-equilibrium physical matter to the study of more fundamental problems in physics, such as the lattice gas theories. So if you're interested in the details of the machine or the applications, we had a preprint about a year and a half ago describing all of these details. But let me maybe just flash on one of the specific applications on Specific applications on more appropriately called quantum machine learning or quantum reservoir computing. That is a technique that is really borrowed from the classical machine learning literature, where basically the structure of a standard neural network is replaced here by still keeping an input and an output layer that you see here on the left figure. Here on the left figure, and the set of internal layers of neural network are replaced by nodes that satisfy that in this specific case are placed randomly, but this randomness here is actually mimicking a very chaotic dynamics of the connections or the connectivity of our reservoir. Now, I will go, I will show it. Now, I will go, I will show a little bit more in detail what I mean by this in the next slide, but let me also mention that this type of approach is already quite well known in several fields, for example, meteorology and climate models, where this methodology of reservoir computing has been successfully applied over the past decades. In the quantum regime, the overall idea is to essentially replace the dynamics of the array of Of the array of these classical spins in the internal reservoir with our atomic arrays. So, let's try to solve or let's try to see how the pipeline to approach a classification problem would work in the context of quantum resource computing with neutral atoms. So, suppose that we have an image, let's say that this is an image of a horse, but it would be a set of images of horses, cats, dogs. Of horses, cats, dogs, or some other animal. And then we want to basically predict whether the machine, or we wanted to check whether the machine is able to classify that image as a real horse. So what we typically do is to basically we take the picture, we downsample, we create, we do, we perform some downsampling processing to extract the relevant features of that image. This can be done through in several ways. Through in several ways, one of the most common ones is the so-called PCA or principal component analysis. And then we take we map those features or we through the specific encoding into our atomic array. This can be done again in multiple ways by exploiting what is called the tuning, either global or locally. The tuning, says I wanted to highlight, I wanted to recall are one. Recall, are one set of parameters that enter into our Hamiltonian. And the second thing are the second possibility is the encoding through the positions of the qubits. And the positions, again, are important because moving or changing the positions implies that we are actually changing interaction strengths between the particles. Finally, we run some quench dedicated quench dynamics out of this. Dynamics out of this configuration. Finally, we take measurements, so we are able to extract a set of correlation functions, either single-body to body or in principle, even multi-body, run a classical linear regression algorithm, which is just a couple of lines in Python, and eventually extract information whether the image is a horse, a cat, or something else. So the big interesting So the big the interesting element of this algorithm, contrary to what our standard neural networks can actually provide, is the fact that it is actually very easily to be trained. The reason being the fact that typically in a standard neural network, what you need to do is to update the weights associated to the connections of different nodes. In the quantum case, given that In the quantum case, given that you need to run multiple dynamics in order to extract the statistics, as you can see here from the measurements, this would make the process extremely slow. Instead, in the quantum reservoir computing approach, you don't really need to train, you all only rely on the sort of chaotic dynamics or very complex dynamics coming from this specific implementation. And you also try. And you also try, you can also show by doing specific experiments that you basically overcome certain specific limitations of quantum neural networks, such as the so-called baryon plateaus. Now, there are several implementations, there have been a couple of few implementations of this algorithm in our machines. Probably the best example that explains in more in detail at the In more in detail, the details of this algorithm has been described in this paper that we put on the archive in July of last year. And we made a number of experiments, both in emulation, meaning with the software tool, basically, just by doing by running, doing standard state evolution, as well as on the machine. And what we see is that typically the performance, actually, the accuracy of this algorithm is very much. Of this algorithm is very much in line with the standard neural network approaches for a number, even by increasing by a lot by substantially the number of qubits that here actually resembles the number of the dimension of the PCA, so the initial downsampling approach. The other important feature to be checked is the fact that by increasing the statistics, meaning the increasing number of samples that we use in order to Samples that we use in order to extract our correlation functions, the reliability or the accuracy of the algorithm also increases. To be very clear, the type of example that I'm showing to you relies on a very well-known data set, which is called MNIST, which is a multi-class classification problem of handwritten digits. All these approaches were then attached. were then you know tested on a much more much larger system sizes with more than 100 qubits. Actually this is actually the largest, the most useful demonstration of a quantum machine learning algorithm to date. For reference, the previous one was executed on a Google machine with up to 40 qubits. So here we really scale the system up to by more than a factor of two. And in particular we tested the particular we tested the this uh the the uh accuracy of this algorithm or the specific data set of what are called the tomato leaves where we basically classify whether a set of tomato leaves are display a certain disease or instead are represent healthy tomatoes. And what we see here is that by increasing the number of qubits, the accuracy of our algorithm also increases. Algorithm also increases. This is with the reference line here is QRC Aquila, so we did precisely the execution of this algorithm on our device. I think, can you guess correct me if I see here it's already 30 minutes. Is that correct, Gesco? Or do I have some additional time? Yes. You're in 30 minutes, so yeah, so I should kind of Are you with question period? You have 10 minutes. I can spend perhaps five more minutes just highlighting the really cool stuff that we are being doing on the digital. Give me the modality. Yes, I will try to be very, very fast. But I just wanted, so to wrap up the results on the analog mode, let me just sort of give a very broad picture of the results on the simulation of quantum phases of matter. Of quantum phases of matter that, of course, build up on a number of results from Harvard as well as from many different groups around the world building a similar type of technology. But most recently, we really started to work very closely with several collaborators to try to approach a number of problems that span different areas of science, Newtonian physics, and are really inspired by high-energy physics, chemistry, and condensed matter. Chemistry and the convex matter. And of course, I will not have time to discuss those in detail, but I will be happy to either take questions or really respond offline. The second modality, which is actually the thing that we are really focusing on very heavily at the company over the past year and a half or so, is our very recent developments on quantum error correction. So we all know that the motivation is the fact is. Motivation is the fact is the following. So, we all know that errors or high error rates for physical qubits are way too high in order to perform or to execute algorithms with very high efficiency. So, one way to overcome this problem that borrows ideas from physics and mathematics over the past 20 or more years is the area of quantum error correction, where the key concept is the concept of logical qubit. Is the concept of logical qubit, meaning a set of physical qubits. In our case, the physical qubits are the atoms. So a logical qubit is a set of atoms such that if the fidelity of the operations acting on physical qubits is high enough, we can expect that the error rates acting, the error rates associated to operations on logical qubits will be lower and lower, much lower, or in prison, much lower than the ones we. Much lower than the ones with physical qubits. Now, these reducer rates at the logical level would potentially allow for longer and more meaningful execution of quantum algorithms. So, in terms of usefulness of quantum computing, we all know that there are some sort of favorite areas that have been sort of advertised for this field over the past few years, but they may. But the main message here is that with the current resources, what we see is that it will be very hard in general to execute complex tasks, for example, factoring or the study of quantum chemistry problems or even quantum dynamics, if we don't really improve not only the hardware, meaning the device that we are currently building, but also working at the algorithmic level in order to improve or to level in order to improve or to make these algorithms more efficient, more efficient to the hardware that we are actually building. So in this respect, I wanted just to highlight a couple of results without going into the details that are sort of recent achievements to reduce space and time overhead to really execute quantum algorithms based inspired on ideas coming from the specific technology that we are developing. Developing that we were developing, which are neutral atoms. There are a couple of papers: one on the so-called algorithmic fault tolerance, and the other one on the execution of QNDPC codes. So at the very high level, and I will try to conclude very quickly, there are a few building blocks. Of course, we have neutral atoms. The second one is the so-called qubit shuttling, meaning the possibility to really move the atoms in real time while we Atoms in real time while we are basically executing a specific algorithm. This has a very strong relevance not only for the execution of the quantum simulation of algorithm, but also for the possibility, which is actually a very strong constraint for systems with fixed topologies, such as superconducting qubits, in order to reduce the number of control lines that are really at the basis of the. Of the classical control or classical signals to really operate the qubits. The other ingredient is the so-called possibility of running parallel operations, in particular what are called transversal gates, which means basically that if we try to encode logical qubits into a set of physical qubits, and we want to really transfer information or make operations, logical operations. Operations, logical operations with the different qubits. We typically need to do it by run by executing what are called the swap gates, which transfer qubit pairwise from, let's say, qubit from a bit among very distant qubits. Transversal operations are actually much easier to be realized on our neutral atom systems, thanks to these qubit shuttling capabilities, such that we are able now to execute pairwise. To execute pairwise qubit operations basically without the need or by reducing by a lot the number of swap gates that are instead present in fixed architectures. The other important element is the possibility of really creating a device that really resembles very much the architecture of a classical computing system by, for example, having a so-called storage zone where we basically Called storage zone, where we basically put all the qubits, either the physical, the logical, or the uncellers that are not undergoing a specific operation at that instant of time. Then we have an intangible zone where we execute operations, either a single qubit, two qubit gates, or multi-qubit gates at the physical or logical level. And finally, a readout zone where we can take pictures. So extract information either during the Extract information either during the computation, which are called mid-circuit measurements, or at the end of the computation, and after that, of course, per se for performing some classical post-processing of that specific information. So, just to wrap up, there are a number of very relevant results that were published recently, starting from a very important milestone led by Harvard. Led by Harvard in collaboration with Couer and other universities back at the end of 2023 on the execution of complex algorithms with the 48 logical qubits, followed very recently in December of last year, so really a couple of months ago, by the first demonstration of magic state distillation with logical qubits, which is another very fundamental tool in order to execute a fault-tolerant or to really perfect. Tolerant or to really perform a fault tolerant operations with the quantum computers. So, with this, I just wanted to show you a picture of the second generation machine on which these latest experiments were executed. This is called Jabinoi. And it is actually very interesting as a technology because it will be this specific type of machine will be also located. Located next to an high-performance computing center, in particular, in the recent engagement that we had with Japan. So, I don't have time unfortunately to speak about our recent commercial achievements, but I wanted to highlight that yesterday, for example, we announced a very important fundraising round of $230 million that will basically boost the development of our technology at Development of our technology at Quero. Lots of work also with the academic and commercial customers. And if you're interested, I'm happy to give you more details. So with this, I would like to thank you for your attention. And of course, I'm open to take questions. Thank you so much. Do you have any questions? Thank you for a nice talk. So I have a question about the reservoir computing. Is it training? How does it work, essentially? You mean how does it work? So there is a training phase and there is a training set, there is a test set, of course. So the way in which you basically split the data works essentially. Essentially, in the same way. I see. So you train. So the way you choose those. If you want, what probably was missing in the, or was a little bit hidden in the description of the algorithm is that, of course, there is a final layer, which is a classical linear regression layer, that takes care of this training part that we're talking about. But this is That we're talking about. But this is much simpler than what is typically needed in order to train a quantum neural network, where instead you will need to train the weights of the inner layers of what here we replace by the reservoir. So instead of having different layers, here we have a reservoir that does not need to be trained, but the updates of the weights is just a purely classical post-processing, if you want. Processing, if you want, that takes place at the very end of the algorithm. So, a question. So, what are you, when you're reading out the information with the microscope, what are you really reading out? Is it the density collection function, or what is the information you're collecting in physical terms? Right. Yeah, that's very good. Right. Yeah, that's a very good question. So in general, we take measurements in the computational basis. So we typically, if you want, in the sigma-z basis, we measure whether the atom or the electron sits in the ground or in the red back state in the case of the analog mode. Or in the case of the digital modality, we basically take again either zero, one, but in two different hyperfine states. And of course, the pen, I mean, you can, of course. And of course, the pen, I mean, you can, of course, measure different basis in the digital case by just making rotations. So, in place where you can extract information also in different basis. Does this answer your question or was it slightly different? In terms of quantum mechanics, so you're calculating matrix elements in a sense, so the probability to have a certain state in the space is right. Yeah. That's basis, right? Yes. Okay. Maybe that was a slight question. Thank you. Other questions? I had a question. In one of your last slides, you showed shuttling groups of neutral atoms of hormonal local qubits into an entangling area. I assume that takes some amount of time. So how many gig operations can you do? How many gate operations can you do before you lose computers? I think I lost the last couple of words of your question. So the question was how many operations we can perform. Is that correct? Yeah, because moving the groups of add-ons around will take some time. So that's going to limit how many group operations you have for comparisons. Yeah, that's correct. Yeah, that's, of course, a very good question. So the So, the number of operations, of course, depends on the typical and the equivalence type of the system, which is of the order of seconds. And the typical duration of the operations is actually very small, of the order of a hundreds of nanoseconds. But then the qubit shuttling, which is the thing that you were asking at the beginning, is precisely the sort of limiting factor, which takes potentially milliseconds to be operated. So, the expectation is that we are able. So, the expectation is that we are able to perform thousands of the operations before the within the coherence time. And this is actually quite similar to what other platforms are currently basically can actually perform. And there's no optimization tricks in terms of you said you had all the all connectivity of the qubits. Of course, of course. Yeah, yeah, of course. Better to have some sort of two-gate operation on a set of adjacent logical qubits that are sitting next to one another in the memory. It's actually easier than having that are far apart in the memory. Absolutely, absolutely. I mean, what I was telling you are what are just the so-called building blocks. And then, of course, you want to create at the classical level a very level a very powerful very strong software stack in order to optimize the what is called the compilation of an algorithm and the compilation of an algorithm of an algorithm really takes care of the optimization process associated to which type of gates you can try to parallelize meaning that you can do at the same time in order to minimize the impact or to minimize if you want the circuit depth that you execute on Execute on the machine. And the circuit depth again is a relevant parameter in order to understand or to get what is the usefulness, if you want, of an algorithm in the long term. So yes, the answer is absolutely yes. And this is something that many groups are actually working on, both hardware groups, such as our team. Groups, such as our team here at Coera. There are many people really working on classical software that are really borrowing ideas from classical compilation that can be really useful for quantum computing. Does your software stack include a simulator? That's a very good question. So I can anticipate you that very in just in a few weeks we will also make available the We'll also make available the new version of our software, which we call the blockade digital, that will include the simulator and also a noise modeling, meaning that that emulator will be able to basically mimic the behavior of the machine, of the digital machine that will become online, that will be online in the next few months. So the answer is absolutely yes. The answer is absolutely yes. Thanks, Tomaso. Other questions? Well, let's thank Tomaso again.