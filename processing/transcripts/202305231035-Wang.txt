  All right, thank you. Yeah, so in this talk, I plan to break into three parts. First, I want to just recall a little bit the intuition, recall the parabolic Anderson model. The parabolic Anderson model in Euclidean case, so that we get some intuition. And then I'm going to introduce the geometry part that is the Heisenberg group, not assuming that everyone has seen it. So we'll spend some slides there, and then later we are going to introduce our model and give the main result. So to start, let's recall a parabolic Anderson model. This is a Cauchy problem. This is a Cauchy problem with random potential. And if we look at this stochastic PDE, it has on the right-hand side, there are two parts: right? There's the diffusion part and the noise part. And we can see already that there is the competition between these two parts. That if we for a moment ignore this noise, we see that this is just a heat equal. We see that this is just a heat equation. Well, maybe before that, let me still address this. Okay, well, we have a Laplacian, right, on Rn, and then we have the noise. Maybe here for now, this is a general notation that stands for some random field on this zero to infinity times Rn. And then, of course, if we consider Of course, if we consider parabolic-Anderson model, most commonly taken noise are the space-time Gaussian noise. And then, of course, to understand this stochastic differentiation, we could take either Stratonovich or Ito-sans. And in this work, we took the Itosans. And now it comes the computation, right? If we If we ignore the noise part, this is just a heat equation. Therefore, there's a smoothing effect that is playing a role. But if we just look at the noise part, this is a kind of random ODE. And formally, we could write it in terms of this, the solution in terms of exponential of some noise. And this would suggest a high. And this suggests a high irregularity. So, at the end, if we try to write down the solution to this PEM model, we see that these two effects would compete with each other. And therefore, there's many interesting questions pops out. And here I just list these two sets of questions, right? The very basic question would be the existence and the uniqueness of the solution. Uniqueness of the solution. And then, of course, for this classical PEM model, there's the very interesting localization phenomenon, which comes together when it was introduced by Anderson. And this localization phenomenon is quite interesting, right? And this had, well, one of the classical ways to Classical way to capture this phenomenon is through the intermittency, which could be captured by comparing moments. So, of course, that is related to the moments estimate for the solution. And in literature, there has been lots of AD literature in discussing the intermittency of the solution. And we also heard Davas talk yesterday. So, intermittency basically So intermittency basically describes the captures the phenomenon of the concentration of the solution of a parabolic Anderson model, that it concentrates its energy in high peaks. And it could be characterized by comparing this moment's intermittency. And this has been studied in literature, particularly in RN. In particular, in Rn, the many, many results here. These are lists, maybe partial lists of results in white noise in time and also fractional noise in time. Here, several of them are in this conference. So, and then let's come to our goal, right? So, for us, So, for us in this project, we want to understand how the geometric role is playing in this model, right? And so then if we change the underlying, replace underlying Euclidean space by some other geometry, would we see different behaviors? And come to the choice of the underlying geometry, would like to go to some Would like to go to some geometry that is rougher than Euclidean or Riemannian space. So, the first set of geometry could be considered are the sub-Riemannian manifolds. And intuitively speaking, these are manifolds that if you try to diffuse, certain directions are constrained. And the first non-trivial example for such manifold will be the Heisenberg. For such manifold will be the Heisenberg group, and that is exactly the space we chose to do the research to do this project. And then let me also mention another set of interesting geometry would be fractals. And we have also seen fractal yesterday in the talks. And they are interesting as well for their, they have fractal dimensions for both the underlying space and the different. Both the underlying space and diffusions. And a typical example would have been the Serpinsky gasket. And then let me mention also both class of manifolds are, they all belong to the metric measure space. So there's also the possibility to extend the unify techniques so that it can be maybe treated to more general setting to start. More general setting, just start with the metric major space. So, okay, so this is the introduction part. Now, let me say something about the Heisenberg group. I don't want to assume that everyone is familiar with that, so I try to keep this part as intuitive as possible, minimizing the geometric differential geometry terminology. So the Heisenberg group. So the Heisenberg group, well, let's just study the first Heisenberg group for simplicity. So we stay in R3, three-dimensional space. And then you can be just identified with R3. However, you get constraints in the movement. How does the constraint come in? Is that it follows this group law? But for now, let's just consider this as the increment that you want to do. That you want to do to make a movement. Okay. So, therefore, if it were Rn, then you should forget about the last part. But now this is where the Heisenberg group geometry come in, the group law, right? The first two coordinates in the plane, you still see the exact same way to move as in Rn, R2. But last one, in addition to this vertical. addition to this vertical increment, you also have an increment that comes from this x delta y minus y delta x. But if we consider this in a continuous sense, you should realize that this somehow connects to this, if you use Green's formula, this will represent some kind of area. Imagine if this is infinitesimally small and you want to integrate, this will give the area. The area. All right. And then, if this is infinitesimally small, it will suggest you three vectors at each point. This gives you a vector field that suggests you, if you want to do a random walk or diffusion, this is how you should do. All right. But so far, there's no degeneracy. There's no constraint. Where does the constraint come from? It said we will penalize the z-direction. So namely, if you want to do Namely, if you want to do a random walk, you are allowed to walk along these two directions or the direction, any direction spanned by these two vectors, but not this one. And in other words, if you look back to this group law, it's to say if I want to do sub-Riemannian random walk or sub-Riemannian Brownian motion on the Heisenberg group, the vertical Well, the vertical increment is gone. So, if you want to make any vertical increment, it has to come from that you make enough windings, right, in some sense, or you accumulate enough areas in the plane in order to get height. So, that suggests that if you do a Brownian motion or whatever, however, we want to call it Markov process. To call it Markov process, it will be degenerate. All right. Therefore, if we will try to write the generator, we call it the stable appliance, meaning that you are not allowed to use this one to generate diffusion. And therefore, the Markov process generated by this operator will be the so-called horizontal Brownian motion. It, in fact, has Has a stochastic representation. It just simply, if we start from the identity, which is the origin, you will realize that it's given by a planar vonian motion. And then on the vertical direction, it's given by the area that is swept by the planar bony motion. Okay, so this is a degenerate process. You have two-dimensional. Process you have two-dimensional noise, but you get a three-dimensional diffusion. And here are some here are some simulations. This tells exactly that if you try to do a random walk, you always have to find the plane first. Of course, keep in mind the plane tilts everywhere. So eventually you can still go to wherever you want to, but it takes a longer path and more energy. All right. Sorry, and B1 and B2 are independent. Yes, yes, they are independent. And yeah, this the left-hand side is this the simulation of horizontal brown land motion and locally you see it's quite flat. That's the idea. All right, so this part, okay, let me say a little more because we eventually will need the heat kernel of this. Of this horizontal Brownian motion in order to do the calculations. Oops. Sorry. Yeah, so the good news is that in literature, heat kernel could be computed explicitly, more or less explicitly, but bad news is very complicated, not easy to handle. But again, good news is that we don't need the explicit form, we just need the stop Daushan up and down. We just need the sub-Gaussian upper and lower bounds, which has been available in literature. And if we look at this sub-Gaussian bounds, it looks, you know, if you are familiar with sub-Gaussian bounds, this is quite typical, right? We have the Gaussian part, and in this Gaussian part, we have the sum distance squared divided by t. And then we see this part is polynomial, but this is corresponding. Is corresponding to the volume of the geodesic bow of radius root t. But you see, the power is different. Oh, by the way, I switch here to from n equals to one to general, but go back to if you want to go back to R3, n has to be one. But if you remember Gaussian kernel in R3, this was t to three halves. But here, if we plug n equals to one, this is t squared. equals to one, this is t squared. And this discrepancy is comes from the fact that the Heisenberg group has degenerate geometry. It takes more energy because you cannot go directly up. You have to do this. And this costs energy, right? Or say longer path. Therefore, going vertical, it counts twice, it's t squared. So that's why we see a t fourth instead of, well, t squared instead of t to the three halves. Halves. Let me also just explain this distance. Of course, we can no longer have Riemannian distance or Euclidean distance. This is the CC Carnal Carthori distance, which is the Riemannian one, a sub-Riemannian one, sorry. Again, this distance is quite, it doesn't have an explicit representation. Explicit representation. But good news is that we can characterize its order. At the end, this is equivalent to this homogeneous norm. If you want to compare a point to the distance of any point from the identity, this is equivalent to this homogeneous norm. And again, we want to see that the z-direction behaves differently. All right, so so far we are good with this term. If we try to write down our PEM model on the Heisenberg group, we are done with this part. Now, what about the noise part? Okay, again, about this stochastic differentiation, we can just take the idle sense, but the noise was the most natural choice of the Gaussian noise. Choice of the Gaussian noise. We would like to go with the intrinsic one that is corresponding to the degenerate geometry structure of the Heisenberg group. So, and for in this work for simplicity, we went with the one that is white in time and the colored in space. And before that, let's come back to us, the noise, the space-time noise in. Noise, the space-time noise in our end case, of course, these are the classical definitions that we deal with. These are Gaussian families of Gaussian families of random fields, which are characterized by their covariance structure. And then, if you look at this covariance, can we call it covariance kernel? If these are direct If these are direct mass, then they are white noise, either in time or space. But if we want to go for colored noise, then this is the we can we can go for the risk kernels, right? And so for us, if we want to switch to the Heisenberg geometry, we should also We should also consider what is the best choice of this space kernel, right? And we should go for the risk kernel. So, well, still in this RN case, the covariance structure could be written in either Fourier mode or the direct mode. And in particular, in this direct mode, we can, you know, it can be written in terms of Can be written in terms of the negative power of fractional Laplacian. And this would be the point that we can now try to switch to our geometry. We can just, instead of using the Laplacian, we will use the sub-Laplacian to generate such noise, to describe such noise. And of course, if you go for integration by part, well, first of all, then this risk kernel can be just in. This risk kernel can be just in written in terms of integral of the heat kernel. And again, we can characterize the order of this risk kernel. And this will be. And then if we want to write down the covariance structure here, if you consider integration by part, you will see that it should be a G sub 2 alpha, and that is the risk kernel we went for. Went for. So at the end, here this is our noise. It will just be a Gaussian family whose covariance structure are characterized by this formula. And in particular, if alpha equals to zero, then that will just recover the space-time white noise on the Heisenberg group. And let me also mention that we can, in fact, write this. Write this fractional noise in terms of this applying the okay. Alpha here is between zero and n plus one over two. This is a negative power of fraction of Laplacian applied to the white noise. Okay. All right, so now we have our PEM model and everything is fine now, so we can start. Everything's fine now, so we can start to discuss its solution. And to address the uniqueness and the existence of the solution, we just go with a classical approach that is in the ito sense, that is by considering the Wiener chaos expansion. And well, basically, first of all, if we write in the integration integral form, this is given by this expansion. integral form this is given by this expression but then if you keep keep iterating this you get the wiener chaos expansion and then you try to prove the the convergence and get the existence and the uniqueness and so here this is the main result right then we realize that alpha has to be more than n over 2 in order to guarantee the existence and the uniqueness and as a byproduct we also are able to estimate the second moment To estimate the second moment of the solution, and this no doubt has the exponential growth in time. And this is the so I will talk a little bit about the proof because the proof really, yes, we use the classical approach. However, the estimating itself, we have to use the Fourier, we want to do it in Fourier. We want to do it in Fourier mode, but the Fourier transform in Heisenberg group is non-trivial. So, so maybe I have some time to just show you a little bit of that. But before that, I want to also remark on this main result by comparing it to the classical case that we see that it looks as an analog to the classical. As an analog to the classical case, but however, if you compare this condition on this on the alpha, classically, in order to guarantee existence and uniqueness, we need alpha to be more than n over 4 minus 1 half. But in our case, we cannot use the topological dimension. If so, the topological dimension of Heisenberg group is 2 plus 1. Instead, we n plus one instead we have to use this guy that has appeared already before this is the hausdorff dimension uh when n equals to one this was the four that appeared before so so that is where we see okay the the sub Riemannian geometry comes into play okay um yeah so let me just say a few words about the approach of course we use care we Of course, we use wiener chaos expansion and we need to estimate this quantity in Fourier mode. And to do that, we'll have to come to the Fourier transform of the Eisenberg group. And however, you know, if you just consider classical Fourier transformation, if you want to consider a, well, you have to use the A well, you have to use the group representation, okay. And then, like, in this equation, it's just to say that you can identify a or say represent the element in the Heisenberg group by a unitary action on the L2 functions in Rn. Okay, it's not convenient because if you look at the Fourier transform of a L1 function on the Heisenberg group, it gives you a family of It gives you a family of well, this is operator valued function, it's a function of the parameter named, and it's not easy to work with. But the good news is that in literature, there's already a consideration of the projective version which takes advantage of the intertwining relation between Fourier transform and the Transform and the Laplacian here. Actually, this is the sub-Laplacian on the Heisenberg group. And on the right-hand side, you see that this you obtain a oscillator. And then you can just do decomposition with respect to the spectrum of the oscillator. So that gives us the advantage in the estimate. So imagine that when we deal with the Wiener chaos estimate, we would need to. Estimate, we wouldn't need to estimate for the heat kernel, but now it gives us a nice way because we can simply consider a projective version. So you can chop it off into pieces, right, in terms of a summation. And then, well, here, these are just, you just consider the projection of the Fourier transform of L1 function onto the eigen space of this oscillator, which are Oscillator, which are just spanned by Hermit functions. Okay, so here is one more slide about the advantage of choosing this projective version for your transform. For instance, the Plancher identity can be written now on the right-hand side. You have a nice kind of three-fold integral, but of course, this is this. This two are on discrete space. So we have the summation. And then it also behaves nicely when you want to deal with the Laplacian. Here again, this is a sub-Laplacian. The Fourier transform of a sub-Laplacian applied to a function will be just given by this multiplication of a constant. And that also gives the convenience of dealing with the fractional power of. Of Laplacian, sub-Laplacian. And well, this is super convenient. We just need to raise the power corresponding power to this eigenvalues. All right. So fast enough. Just a very last slide to give us to say a few words about what we can do next. This project is the first step trying to Trying to move or do things about PEM model on exotic geometry. Can I say that? So Heisenberg group in Ito sense is the first step. Of course, if we want to stay on the Heisenberg group, there are lots of more questions to ask. The next one we are actually considering is the stratonology sense and the knowledge sense and try to make the existence and uniqueness path wisely and um and another another direction that we are considering is to just consider these problems on fractals but actually we don't have to restrict to fractals we can just go for general metric measure space and these are the directions these are These are questions we are currently considering with a larger group of co-authorship. We have Li who is here and another person that is Chao Hong Huang who is not here. All right. That's all. Thank you. Thank you very much. Thank you very much, Abel. Any questions? Yes. So is this going forward to generalize yourself to the case of higher order potency? Higher order new potency. Yeah, that's a good question. There is exist the the kernel. Yeah. There's a cherry log or bit method, not the transform. So Fast forward. So, yes, I think we can. We need the methods. No, we are going to replace your moments like to end up directing. And then we can use the meta as well. Yeah, thanks, Atlanta. Yeah, thanks for that. Yeah, the estimate of the subject and yeah, excuse me. What is the issue with the Fourier transform of the asymptotic group? You mentioned it's completely Yeah, well, if you don't go for the projective version, then it's not easy to get hands on the estimate. Like in this one, we take advantage of the spectral decomposition, and then it gives you a nice expression when you do the estimates. Otherwise, it's rather, I would say, abstract. Because the fully transformed component is a breaker. Yeah. And there's not really a frequency space, the frequency space is not as nice as the Euclidean Fourier transform. Here you get operators instead of a point in Rn. Follow up in that session. Follow up in that sense. So, what is it? Can I ask a question? Yes. I don't hear it. Hello. Hey, Marshall. Hello. Yeah, I couldn't hear the discussion now, but I guess you're discussing the generalized Fourier transform, right? Yeah. Yeah, so I kind of had a question about that. If you think. About that, if you think this can be still applied in other, say, karna groups, yeah, that was uh, um, yeah, so repeating Fabri's reply to she, yeah, yes, there there's a possibility to do that because at least for step two, that is yeah, it's as long as yeah. As long as, yeah, well, I'm not very familiar with the projective version on the kernel groups, but according to Fabrice, such method exists. Yeah, it's actually explicitly written now. So I can, once we meet, we can talk about it. Okay. Sounds good. Yeah, also another question was whether you can use some kind of polar type decomposition. Type decomposition. It still would be step two, but it would reduce it to emotes on something compact and will make this Fourier analysis a little bit easier to deal with. Yeah, maybe if you can give me more. Yeah, maybe if you can give me more details. I'm not completely sure about the polar deconversion. In the beginning, we decided we wanted to move to a non-compact setting because of the original motivation for this Anderson model. And well, I thought we would only see non-trivial effects if we considered. Non-trivial effects if we consider non-compact manifolds. Yeah, but what I mean is polar decomposition, meaning that you have, I don't know, one direction which is still unbounded, right? Plus compact, like plus a sphere like thing, which allows you to do something discrete times continuous decomposition. Anyway, sorry, I don't want to some kind of decomposer decomposition for your transform group. Yeah, maybe it's similar. Yeah, maybe. Yeah, so yeah, kind of specifically this. Yeah, maybe we can discuss that. Yeah, sure. So I think it will be great if people ask questions or say something in the microphone so that we can hear, because otherwise, I, yeah, thank you. Okay. There is another question. Hi, I have no idea if people can hear the microphone. Microphone. When I encountered this problem before, the motivation was trying to do some variation of optimal transport, but via rough paths. I wonder if that was something that you were considering in this project as a kind of wider interest, because the Brownian motion on the Heisingberg group is like the rough path of Brownian motion. So if you're doing optimal transport with that. Optimal transport with that object, that you'll have this Brownian motion in the Heisenberg group driving this SPDE, kind of giving the diffusive nature to it. Then is there like an equivalent way of thinking about some rough differential equation and then seeing how that's being pathwise diffused? I'm not sure if I know we lose this. I don't reduce the noise driven by the noise or by the process. Right, so the Brownian motion which you described on the Heisenberg group is the rough path of Brownian motion. Yes. Yeah. So that, and then in terms of this, the parabolic Anson model, on the Heisenberg group, the noise is. The Heisenberg group, the noise is the Brownian motion on the Heisenberg group, right? So, not particularly. Oh, okay. It's a Gaussian field. Okay. Yeah. Yeah, the noise is not the Brownie motion. We constructed another Gaussian noise on the, yeah. The Brownie motion is only related to La Passion. Yeah, yeah. Are there any more questions? Well, if that is not the case, we will afterwards check the chat. Hopefully, this is not a question. I don't know. Can someone tell if this is can you maybe check if this is a question? No, it's just a confirmation that we can hear you finally. Okay, fantastic. Okay, fantastic. Okay, in that case, we will thank Jing again and go for coffee. Thank you, Marcia, for coming. Yeah, thanks. See you. See you, Marcia. 