We'll talk about joint work with Rojeka Sals, Eugene Gorski, Michael Gorsky, Ian Lei, and Lin Hui Shen. So let me give you right now what the goal for this talk is and what the plan to reach this goal is. So the goal is for any positive break, construct a cluster algebra structure on the algebra functions on some variety that I have not defined yet, but I will define it for you. But I will define it for you. And for example, these varieties include the positive varieties that Matt was talking about. So the plan is to first introduce what these varieties are, how you define them, and to give some special cases of these braid varieties. In particular, Doublebot-Samuelson cells are special cases of these varieties, and Richardson varieties are special cases of these varieties, and positroids are special cases of Richardson varieties. So, okay, then I will explain. Okay, then I will explain how to construct a cluster structure on these varieties, meaning how to give a cluster algebra structure to the algebra of functions on these algebraic varieties. And finally, I will give you properties of this cluster structure. It turns out to be very, very nice. Okay, so let's start. So before, I'm sorry for this slide, let me set up a bunch of notation. Notation. So we start with a simple algebraic group with linking diagram D. I am going to assume it is simply laced. We have results for non-simply laced type, but let me just keep it simple here. So for example, you can think that G is SLN. Oh, and I always work with coefficients over the complex numbers. So everything is defined over C. Okay. Then I consider a Borel subgroup B. So you can think that B is the upper triangular matrices. I consider a maximal torque. I consider a maximal torus, so for example, diagonal matrices, and this determines the Weyl group, which is a reflection group, meaning that it is generated by reflections SI, with relations that SI squared is equal to one, and some other relations, which are the braid relations that I don't want to write, and they depend on the group itself. So, for example, in the running example, W is just a symmetric group in N letters. Letters and I will denote by W0 the longest element of this group. So, in the example of the symmetric group, the longest element is just the one that flips one through n around. Okay. With this, we also need the positive rate monoid. So, this is the monoid generated by element sigma i, where i also runs over the vertices of the thinking diagram with some relations. So, these ellipses. So, this ellipsis is the same as this ellipsis. Okay. I just delete these relations that the squares have to be equal to one. So this is the combinatorics more or less. And the geometry will come from the flag variety, which is G mod B. In that case of SLN, you can think of the flag variety is really just the variety of flags in C to the N, meaning the variety of sequences of subspaces Vi, what the dimension. Vi, where the dimension of vi is equal to i. And this variety admits the blue-hat decomposition. So this v is these double cosets for b mod b where w runs over the elements of the valve group of the evalu group. I need to lift the elements of the value group to the 2G, but it doesn't really matter. Okay? Okay. Now, with this, to define the bread variety, I need to define what the demo. Bread variety: I need to define what the Demasur product or the zero-Heke product of elements in the positive bread monoid is. Let me just start not with the positive bread monoid, but let me just take a top of vertices of the Lincoln diagram and L top of vertices. We define the demonstrative product, also known as greedy product, inductively, as follows. If you take the empty word, then the demonstrator is just a trivial element of Doctor. Okay, and then you start reading the word length. The word left to right, and you start trying to greedily increase the length of this element. So, for example, well, if I append one more letter to a sequence, then I'm going to have delta of i comma il plus one to be just delta of i if when i append as i l plus one i go down in brugad order and if i go up then i really go up so i read this gridily Really go up. So I read this greedily. And the way I defined it, it depends on the order of the il or of the ijs, but one can check that if we define beta i to be the corresponding element of the positive bread monoid, then this doesn't depend on which expression for the positive bread monoid you take. So you can apply bread relations and it won't affect the end relations. And it won't affect the end result. So we have a function from the positive rate monoid to the by group, which I denote by delta. So for example, when w is s3, delta of sigma 1 squared, sigma 3, sigma 2 cubed is just as 1 is 2. I start reading left to right. With the first sigma 1, I add an S1. With the second one, I don't add anything because that would make me go down. With the third, well, with the S2, I add this S2 and all the others one would make me go down. And all the others on the code. Okay? Okay. Now, let's recall that two flags, let's say XB and YB in V mod B in the flag variety, are said to be in position W for an element of the while group if X inverse Y is in BWB. So, for example, for G equal to S Ln, two flux are in position Si, where Si is a simple reflection, if and only if they differ in precisely. If and only if they differ in precisely the ith subspace. This is a generalization of that. And we're going to denote this relation by xb is, well, with this little arrow with double n top. And now I'm ready to define the braid variety. So I start with a top of vertices of the Lincoln diagram. Then the braid variety is the space of L plus one topos of flux. So I just choose L plus one flux. And I want it to satisfy. And I want it to satisfy three things. Number one is that I start with the standard flag, meaning that this coset just is just B. Number two, I want to end with a coordinate flag, which is given by the demonstrative product of this word I. So the last one has to be delta of IB. Again, I need to pick a lift from W to G, but it doesn't matter which lift I pick. And the most important. Peak and the most important condition is that whenever I move from one flag to the next, I want the position to be given by this word that I have here. So I start with the standard flag and I move and move and move and move and move according to how this braid word is given. And I have to finish with the demasure product of the word. So you can think of the demosure product is as far away as possible as you can finish from the standard. As possible, as you can finish from the standard flag, given what the word is. Okay, well, and I want to, I really want to finish with a coordinate flag. So that's it. Okay. And well, I call it the braid variety, and there is no brace here. The reason why, well, sorry, well, here there is a reason why we call it the braid variety, but there are some other things as well. So this is a theorem essentially due to Larai's cover. Essentially, due to Laura's cover. But, well, there's also work of other people in there. Number one is that the bread barret is smooth, which is not obvious from the definition. It's affine, which is also not quite obvious from the definition. And the dimension is the length of the word of the topo I minus the length of its demos for product. Number two, if I take two topos, I take two tuples that give me the same element of the positive bread monitor, then the variety is isomorphic. So that's why we call it the bread variety. It only depends on the braid defined by I and not on I itself. And number three, if it happens that I take a word whose demonstrat is not the longest word, I can increase the word in a smart way, and the bread variety is not going to change. Variety is not going to change, okay? So, if when I append this j, I actually increase the demonstrated product, then the bread variety for i is just the braid variety for i when I append this j. So most of the time, I will assume that the demonstrated product of the braid word is just w naught. Somehow, it was psychologically important to me to get away from this case for this project, because I'll tell you why later, but Why later, but if you want, you can assume that the demos are proof of this topic. Can that's a question? Yeah, so your phrase variety really depends on the choice of pulsive minimal path in the hyperplane arrangement, the reflection arrangement. That's how you, because the phrase corresponds to paths in the hyperplane arrangement. How much are you actually using as a fact? Because you're saying it's not to do a phrase. How much are you using the fact that that arrangement is a reflection arrangement? Like, is this a general hyperplane fact, or is this somehow very specific? Facts, or is it something very specific which costs you? Well, I also need to have the bread variety to define the flag variety to define this thing. So a bit special in this case, but you can still define this for cats moody groups and it's fine. Only that the cluster structure is a bit shape. Cluster structure is that it's shape. Well, it's not clear to me whether there's a cluster structure in this more generosity. Okay. So, for example, let's take G to be SL2, and let's say that the word beta is sigma squared. So, what this means is that I need to choose three flags. The first and the last one are fixed. The first one, well, if you think of the flag variety of SL2 as P1, the first one has to be zero, the last one has to be infinity, and in Infinity and in this condition for the great variety, I need to change when I move from one flag to the next. I cannot stay the same. Okay, so then the flag in the middle, I am free to choose. It can be any point which is not zero and not infinity. And this means that the bread variety is just. Okay? All points in CP1 which are not zero or infinite. So that's the easiest. Well, depends who you ask, maybe a non-trivial example. Depends who you ask, maybe a non-trivial example of a red variety. Okay, uh, yeah, so this was the argument. So we need to choose this x1b, and we are free to choose anything except for b plus and b minus, zero infinity. Okay. I told you that the braid variety is affine, and I'm going to need an affine realization of it. I want to give coordinates to it in order to be able to express my cluster variables. So to give coordinates, we use a pinning, which is Use a pinning, which is a family of compatible maps from SL2 to G, one for each element of for each vertex of the Dinking diagram. Compatible in some sense, I don't want to be too precise here. And I'm going to define an element in G, B i of C to be Pi of this matrix. So for example, in the case of SLN. Comment G is SLN, then this PI, well, it just takes this two by two matrix and it puts it here, somewhere in the middle, where you have a matrix, okay? Where this is the I and I plus spheres, row and column. These are the pinnings in this case. But in general, it is known that appends always exist. That appeals always exist and that any two pinnings are conjugate. Okay, and with this, we can define an element in the group. So it depends on as many parameters as the length of the braid work. So this B beta of Z is just the product of these elements in the order in which they appear in the work. So this is an element of the group that depends polynomially on these CIs. And then we have that an affine presentation of the braid variety, which is given by all those points Z in C to the L, such that this matrix, this is a fixed matrix, this is the demos of product, times this matrix whose entries are polynomial belongs to the Borels. So for example, in the case of SLN, here you are saying that this matrix is of rectangular, which means that you want all n You want all entries below the diagonal to vanish, so it's really given by the vanishing of several equations in this, okay. And well, this is a generalization of that. So the breadth variety is always smooth, and that is a sorry, it's always affine, and that is a presentation. The smoothness is a bit harder, and I'm not going to go. Okay. So a couple of examples of varieties that appear as braid varieties. Here, as braid varieties. First of all, the double-blood Samuelson cells. As far as I know, they were defined by Balash, Elek, and Liu. So you take a positive break, then the half-decorated, ignore this half-decorated part, double-bot-Similarson cell is the locus of those points in C to DL, such that this matrix admits an LU decomposition. If you want to think about it that way. If you want to think about it that way. So, this matrix is in B minus B. So, in this case, this variety is also affine, and it is Sarisky principal open in C2DL, because you can check whether a matrix is, well, it has an LUD composition by checking the non-vanishing of principal miners. So, this is what this is saying. And turns out that every double-bot Samuelson sale is a bread-break. Is a bread bread. What we have to do is we have to take the word W naught, write it in a minimal way as a product of S i's, substitute every S i by a sigma i, and append that thing on the left to there. So this red variety is naturally isomorphic to this double bot semi-sensor. And it is an abstract theorem of Guder and Yakimov that this variety admits a cluster structure, meaning that its algebra. It means a cluster structure, meaning that its algebra of functions is a cluster algebra. And this cluster, well, it's not well. A specific cluster structure was given by Shen and Wen. And somehow this work of Shen and Weng is the basis for our work. I will tell you how in a minute. The other important example of braid varieties, and the reason why some people here may care, is that open recursion varieties are braid varieties. So, what is an open-enderson variety? You take two elements in W. Each element has a corresponding Schubert cell and opposite Schubert cell. And then you just intersect the Schubert cell with the opposite Schubert cell. It is known that this variety is not empty if and only if B is less than or equal to W in Blue Hat order. And this is an affine smooth variety of dimension, the difference of the length between. Of dimension, the difference of the length between W and B. And it turns out that these of Richardson varieties are also special cases of bread varieties. How you obtain a Richardson variety from a braid variety? Well, you take a reduced lift of W, meaning again, you write W in a minimal way and substitute S for sigmas. And you also take a minimal lift of this element, the inverse W naught. So there is an inverse here, but the minimal. So there is an inverse here, but the minimal left does not have inverse powers of sigmas. I want to stay in the positive world always. And then the rich of some variety is just the braid variety for this world. And the isomorphism is not very hard. So remember how a braid variety is defined. So I want to start with the standard flag, finish with the anti-standard flag. In this case, the demos of product, you can check is W not. And I want to move every time I have a I have a reflection. Sorry, a reflection. So here you start, and let's say that you are finished with this vera w here. This gives you a flag, and it's not hard to check that this flag is going to be in the intersection of those cells. And vice versa, any flag in the intersection of those cells, you can find a unique chain in this direction and a unique chain in this direction that completes everything. So the isomorphism is just given by picking up. Isomorphism is just given by picking up this flag. Okay. So that operators and varieties at mid-cluster structures was conjectured by Leclerc a few years ago. The case of positroids. So the case of positroids is given when W is a Grassmannian permutation in Sn. A Grassmannian permutation is just something that looks like space. like uh this uh and in in that case sorry uh we have something known as a positroid okay and that positroids admit cluster structure well as a matt was saying was proven by galashin and lama a few years ago actually surprisingly recently uh based on work of serienko sharman bennett and williams which in turn were based on work of liquor In turn, were based on the work of Liquor. Okay. But the general case of Richardson varieties is: well, this is the first construction of a cluster structure on Richardson varieties that I am aware of. So the theorem is that for any simple algebraic group G and any element in the positive break monoid, the bread variety admits a cluster structure. So we can give the structure of a cluster algebra to this. To this commutative algebra. So there is also independent work of Pavel Garashin, Thomas Lam, Melissa Sherman-Bennett, and David Speyer that also constructs a cluster structure here. It is, I strongly suspect the cluster structures will be quasi-cluster equivalent, but there is no proof of that yet, besides some very special cases. I should mention that their combinatorics are based on a very beautiful. The combinatorics are based on a very beautiful generalization of plavy graphs. They call it 3D plavid graphs. So it's like plavy graphs on a surface embedded in R3. Or construction is based on something completely different that I will explain in a second. Okay. So how can we show that this algebra is a cluster algebra? Well, the strategy is as follows. First, we need to find candidates for what the cluster torus should be. This is the easy. This is the easiest part. So we need to populate this variety with a bunch of tori that cover it up to co-dimension two. Second, we need to actually find the cluster variables. So we need to find a coordinate system for each one of those cluster tori. And we want the coordinates to be regular functions on x of beta. So as we will see in a second, it's not hard to find tori. It's not hard to find coordinates, but the coordinates will in general be rational functions, and we want to make it regular. Rational functions, and we want to make it regular to make them regular. And finally, the hardest part, I guess, is to find a mutation rule, to find a quiver that will give you the mutation rule and show that whatever we found here remains regular once you mutate. That's the hardest part of this project. Okay, so I will explain how to do one and I will give you the idea on how to do two and three. Yeah. Yeah, are learning capabilities of this cluster structure beyond the post-choice case? That's a good question. Based on the combinatorics, the answer should be yes, but I don't know. Yeah. But let me give you the cluster structure first in that. Okay. Okay, so our combinatorics are based on something that we call the break with. And this reminds, well, it reminds us Reminds well, it resembles a lot, um, circle calculus, and that's why the answer should be yes. But and it should come from circle calculus, but I don't have a precise statement, okay. So now I will really assume that G is simple is. Okay, so an algebraic width from Vera to its demonstrative product is a graph and a rectangle R. I will give you an example in a second. So I want the edges of the graph. So, I want the edges of the graph to be colored by the vertices of the Linking diagram. And I want to only have vertices of three types. Vertices of degree one, univalent vertices, which are located only on the top and on the bottom sides of R. That's it. And I want that if I read the colors of the edges attached to these vertices on the top, I get the word vertices. I get the word vera, and on the bottom, I get the word delta vera. I also allow the following types of vertices, which are located in the interior of R. Trivariant vertices that look like this. I want these three edges to have the same color, okay? I and A. Vertices of degree four, so this vertex. And I want the edges to have this curves and I want to. The edges to have this curves, and I want i and k to not be neighbors in the dinking diagram. And finally, I want six variant vertices that look like this, and here I want i and j to be neighbors in the dinking diagram. So I am codifying the grid relations with these two, and this is a special sort of vertice. That somehow, well, for loss of knowledge, I'm really codifying the zero Heke monoid here. So this means that sigma e squared is sigma i, essentially. Essentially, more or less. Okay. So here's an example of such a wheel. So I start with the word sigma one squared, sigma two squared, sigma one squared sigma two squared, and I start doing all these vertices here, and I finish with the demonstrative product, which is just sigma one, sigma two, sigma one. So that's the main combinatorial object that we will use to construct the cluster structure. So there is a few more ways to. A few more ways to think about this graph. One way is to think about this as a moduli of flux. So I somehow put the definition of the bread variety on steroids. And now I want to have a variety which is just a collection of flux, one for each region. Well, one for each connected region of the rectangle minus the graph that satisfies some things. I want that in the region. That in the region to the left extreme, I have the standard flag. On the region to the right extreme, I have, well, the coordinate flag given by the demos for product. In this case, the demos for product is W0. That's why you just put the minus here. And I want that if I have two flags labeling neighboring regions, then they are in position given by the corresponding border. Okay. So and these two flags to be in position. This to position these two flags to imposition S1, for example, these two flags in position S2, S2, S1, and so on. And this looks very complicated. It looks appearing much bigger than the braid variety because I am picking many more flags. But it turns out that all these flags, in this case, all flags from B8 to B19 are completely determined by the flags on top. On top, once you know that, there is at most one way to feel the entire thing. Okay, there might not be a way to feel consistently, and then it's bad news, but there is at most one way. And those, well, by definition, those flags on top, they form an element of the braid variety. Just those flags labeling regions that touch the top border, they form an element of the braid variety, and one can change. And one can check that those elements of the bread variety for which there is one way to fill up this consistently, they're from a torus on the bread variety. And those are the candidates for closer tori. So we have a way to communatorially define some tora inside this bread variety. Something special that says you have to have the top. It's not. Sorry? No, this isn't just somehow simple combinatorics where you pick some initial set of regions so you could walk everywhere else and then. Regions, so you can walk everywhere else in the area. Right, if I did the ACD 19D minus, that would be that thing, yeah, yeah, yeah. Yeah, it looks like it's just a slow-off. Yeah, well, I rather think of it as a slowing down. Yeah, yeah, yeah, yeah. Yeah, and and and somehow, well, if you want this to be consistent, this these B18 and B19 cannot be any flux, they have to be coordinate flux. There is not enough space to not have coordinate flux here. To not have kernel flags here, and then you can start trying to build something. Yeah. Okay. Okay, so there from an open torus. There is a relation between these waves. It could be that two weeks give you the same torus. I can do something silly, like put here again a six valent vertex, and that will give you the same torus. So there is some equivalence relation that I am not going to give, but okay. So we have a way to construct. So we have a way to construct, right? There is another way to think about these WIFs, which is more combinatorial. They give you a recipe to compute the demos or product. So if I cut this with generic horizontal lines, I will have some words. And I want a sequence of words that starts with Bera, ends with the demonstrative product, and satisfies some relations like I can do. Some relations like I can do break relations and I can substitute. If I see two sigma is consecutively, I can just merge them into one. So it sort of gives you a recipe to complete the demos report. Okay. And there is another way to see these wheels, which is as equations on the braid elements that I define. Okay, so I have these, well, let me call them braid matrices because I really think about Let me call them braid matrices because I really think about them as matrices. But so I have elements in my group G, and I have some equations between them. So if I have, for example, well, you can, this is a GL3 computation, I guess. If I have like B1 of C1, B2 of C2, B1 of C3, is this other thing. So these two things are related by a change of variables. If the vertices are far apart, then these two elements just commute. Two elements just commute, and I have that in the braid variety. And the one that is a bit more subtle is this trivalent vertex, which is going to be really what gives us the cluster structure. So if I have Bi of C1, Bi of C2, assuming that C2 is non-zero, I can merge them. And I obtain Bi of C1 minus C2 inverse times a matrix in B, an element of B. Element of B. And because I have this times an element of B, if I do the following thing, if I decide to do something like this, so I have this width and I label everything by these coordinates, CIs, I can start populating the edges from top to bottom using those rules. The complicated rule is that whenever I see a trevalent vertex, because I have this matrix. Because I have this matrix U here, I will affect all variables appearing to the right. That's why I cheated a bit to not have any variable appearing to the right of a trivial invertebrate. Okay. And if you see here, these two are very nice regular functions on the variety, but this one is not because I have to invert C2. So, I'm going to define some rational functions on the red variety. So, if I have a trevalent vertex v on the width, I define the s variable to be the labeled on its right incoming edge, which is the one if you see here that I require to be non-zero. Okay? And these variables are actually coordinates for this torist. Coordinates for these stories, T-tool. For any weave, that's how I find coordinates for these stories. As I told you, the bad news is that these coordinates are not regular functions, they're only rational. And the challenge here is to, well, make them regular and find a mutation. To make them regular, we are going to create like we're going to make like an opera on a triangular change of variables that will give me coordinates that will. That will give me coordinates that will now be regular. And simultaneously, we will also define a quiver. We will do these two things at the same time. So what I am going to explain next will be a swift off version of this procedure. So here I have this width, okay? And here I have the S variables. So I have C5 is the precise variable. The next one is C4 minus C5 inverse. C3 minus C4 minus C5 inverse inverse. 3 minus C4 minus C5 inverse inverse, and so on. Okay? And in this case, it's not hard to see how to produce regular functions. When you multiply S1 by S2, it's regular. I get rid of denominators. S1, S2, S3 is regular. S1S2, S3 is 4 is regular. And it's not hard to check that they are in a cluster configuration of type A3 with one frozen. So this was the first example that we computed. And what I'm going to explain next is really a version of sterics of this procedure. But the philosophy is the same. I want to find a non-perctangular change of variables that will make these things regular. Sometimes finding x-cores is not a fine control actually. In a sense, yes. Yes, yes, yes. Okay. So to find So, to find these upper triangular change of variables, I need some box on this graph. I'm sorry for this slide, but essentially, so if I start at a trivial vertex V, I'm going to create a flow on the edges of the V using some rules that are here. Don't bother reading these things. Things. Okay. I will give you an idea on how to do this. So this first thing says that if I have the flow, which is in yellow here, and I hit a trebury vertex here, the flow stops. Okay? Essentially. The second one is that if I have, oh my god, if I have a flow here, here is the yellow part. I just go through. I just go through this four-valent vertex either here or here. And the last part: if I have the flow on this six-valent vertex, well, I have many rules. If I only enter on the right side, I just go through, also on the left. But if I enter on the middle, it bifurcates. And if it enters on the sides, it merges. I didn't explain to you all the rules how to do this. These are just the most basic ones, but usually. basic ones but usually not always usually these are all you need and and if you well if you are in a situation where you're not in one of these cases and just go back to to to this slide and figure out okay okay uh so for example if i start at the top vertex there i put a one on the edges that are part of my flow uh here there is an example Here, there is an example where all the rules that I mentioned are not enough. I just wanted to show they are not enough because here I have something of weight two. But usually, they are enough. And with this, I can already tell you which trivient vertices, which remember they give you coordinates on the torus, which ones will give you frozen coordinates and which one will give you mutable coordinates. So if you start doing this. If you start doing this flow and you get all the way down, all the way here, it means that the variable is frozen. Otherwise, it is muted. So the top one, well, this is the flow, and I do get all the way to the bottom. That's why it's frozen. This one, I start doing this flow. I start here, I go here, and I stop here. So it's mutable. This one, I go here, I stop here, so it's mutable. This one is frozen, and this one is frozen. And then this one is false. If you didn't like this, I mean, it's fine, I don't take offense. There is another way to decide which tribe vertices will give you frozen variables as follows. Let's say that you don't like the vertex and you remove this edge, this outgoing edge of the vertex. And then you read and you compute the demonstrated product of this horizontal line. Of this horizontal line, ignoring this particular line. If that makes you go down in Bruhat order, then it means that the variable is frozen. Okay, so for example, if I decide to ignore this, I would just have sigma one, sigma two. That's why this variable is frozen. If I decide to ignore this, I have sigma one, sigma two, sigma two. Sorry, sigma two, sigma one, sigma one, I forgot. So the variable is frozen. But if I decide to ignore this, I still have doubly naught here. That's why the variable is not. That's why the variable is not. So the frozen are the ones where you cannot make a mistake somehow. So those are the rules. This is what is going to give you frozen and mutable vertices. And then I need to give you the quiver, which is going to be given by some intersection rules between these flows. Flows. The intersections will happen only at two types of vertices, either trivalent vertices or hexavalent vertices. So for the trivalent, let's say that I have this flow, this flow, and this flow. Then the quiver will look like this. For the hexabalent, let's say that I have this flow. This is the same color. I don't know why it looks different, but it's green in both. Reading both. Then I have this. So those are the rules. There is a formula to actually compute this, but these are the rules. Okay. Let me give you an example. So I have this width, which is pretty complicated. So first thing you do is you draw all these flows, starting from the travarium vertices and following the rules. And now you compute intersections. So for example, there is going to be one arrow from the pink to the blue, one arrow from the blue to the green. From the blue to the green, two arrows from the green to the pink because they intersect here and here. Okay, they intersect twice and so on. So the quiver is going to be the same. The mutable variables are the ones on top, and the frozen ones are the ones on the bottom. Because, well, I have black goes all the way down, then this goes all the way down, and this yellow also goes all the way down. Also, goes for the okay. Okay, so this is how you compute the quiver. And to find the cluster variables, well, that's an upper triangular change of variables from the S variables. So if I want to find the cluster variable corresponding to a trivial vertex, I multiply its s variable with the cluster variables that I already know that were above, essentially. So just like it happens here. Here. So here I just have this variable. Here I multiply S2 by the previous cluster variable. Here I have here I multiply S3 by the previous cluster variable, and so on. So it's a sub-subversion of that. And this gives us the cluster structure and the braid varieties. So in this example, we can compute all these cluster variables. They appear in order in the same order in which they appear on the wheel. The width. So, if you're ever bored, you can check that you can actually mutate these things and you get this. And I want to point out that even though the algebra of functions is a quotient of the polynomial algebra, because you have some relations, these exchange relations are valid in the polynomial algebra. You are not going to need the relations to find this. Also, we can mutate the wheels. So, we can mutate the weaves. Well, sometimes we can do this, and this is a weave mutation. There is a more general notion of a weave. So the weaves I talked about are somehow oriented from top to bottom. There is a version of a non-oriented weave in which there is a way to reach cluster virals, and then we have more general mutation. But that is much harder. And this is somehow enough to construct the cluster structure. How enough to construct the cluster structure. And any two weaves are going to be equivalent by a sequence of equivalences and mutations. So the cluster structure we define does not depend on the weave we start with. All of them give us the same cluster structure. As I mentioned, these variables are polynomial. I mean, the exchange relations are buried in the polynomial algebra, I should say. Geometrically, this means the following. So I have the bread variety given by the zero. Variety given by the zeros of some polynomials in C to D L. I can take the frozen variables and invert them. This will give me an open set in C to D L such that the embedding actually embeds here. And the fact that we can define the cluster, the algebra of functions here inside the polynomial algebra means that we have a section of this injection. And actually, this view is the following. Actually, this u is the following variety. So, again, I have chains of flags. I only don't require that the last flag is the anti-standard flag. I just require that it's in the biggest sugar cell. So, the fibers of this spy are actually defined spaces of dimension doubly naught. So, this is what is happening geometrically. And just to finish, sorry for that, I'm going over time, the cluster structure is. Over time. The cluster structure is actually very nice. We can find quasi-cluster isomorphisms. Just we can move a letter at the end of beta to the beginning at the change of, for example, in type A changing i by n minus i. That's fine. And you can do this many times. Well, I guess two times a lot of better times to go back to better. And that turns out to be the DT transformation of this. Of this cluster algebra. Sorry? Oh, I have until 20. Oh, okay. Okay, okay, okay. So okay, I have five minutes. Let's see what I can do in this next five minutes. Okay. So this cyclic rotation is given by quasi-cluster transformations. And well, you can take some word better and start rotating. Word better and start rotating it. If you do a full rotation, you're not going to end up in better because you are having this change. But if you do it again, you end up in better. And that is the T. And this theorem that Matthew mentioned, that there is another approach by Cassals, Les Charon, Beta and Wenc, is based on this observation. We have a cluster algebra which is equal to its own. Cluster algebra, which is equal to its own upper cluster algebra, and moreover, these coordinate elements, these coordinate functions, are cluster monomials for maybe different clusters. That's fine, but they are cluster monomials, and some of them are even cluster variables. Like here, C5 is a cluster variable. The extension matrix has full rank over the integers. If you know what this means, the cluster algebra is locally acyclic, which means that it can be covered by cluster. That it can be covered by closer varieties of acyclic quivers. We do use it. So I should maybe change the order of the six. Yeah, yeah, yeah. Yeah, we do use it. Okay. Upon the identification that I told you before, when you have this delta at the beginning with the double buttons cell, we obtain the same cursor structure as. We obtain the same cluster structure as Shannon Wink. And if you notice, and I actually did it here, this weave is built in a very inductive kind of way. Like I can start with just the red one and I can start reading the word left to right. And I always put these tribal embeddings when I, the moment they should appear. Okay. For example, if I put some other letter here, this will give me another. Some other letter here, this will give me another vertex, but it won't change the concern variables that appeared before. The concert variables that appeared before are only dependent on things on top. Okay. So this tells me that there is an inductive way to construct this quiver. So you start with the empty quiver, and every time you add a letter, either you stay the same or you unfreeze a frozen variable and add a new frozen somewhere. So, these quivers are very closely related to the class P quivers. But it's not entirely clear to me whether they exhaust the class P. I don't think so, but they're very closely related. So this is what this is saying. If by appending this sigma, you don't change the massive product, then the cluster variable on the smaller board is, we can get it from the cluster variable on the big board by deleting a process and sync. Deleting a frozen sink and unfreezing all the things that were, sorry, and freezing, sorry, all the things that were adjacent to this frozen sink. And similarly, when you put it on the left. So this cluster structure admits a reddening sequence, so it has a DT transformation that I just explained. And the full token chart of conjecture holds for these varieties, so you have a basis of theta functions. More recently, the braid variety means an action of the The braid variety means an action of the maximal torus of the group G adjoint, just diagonally everywhere, because I fixed the first and last flag, essentially. And in some cases, this action coincides with the action of cluster automorphisms. Sorry, with the torture of cluster automorphisms. And the thing I talked about in the Gong show, this deep locus, it is all based on the It is all based on the fact that we have very concrete geometricalizations of the cluster varieties using these breed varieties. So, this is very concrete, very computable, and the clusters are, the cluster tori are given in very geometric terms. The quiver admits a unique non-degenerate potential, which was very recently constructed, like last month, I guess, in the double-bot-Simonson case by Rojeka Sausson-Hong Hauga. And they Hong Hao Gao, and they use it to solve a very famous problem in symplectic geometry that I don't want to talk about, but this is this infinitely many Lagrangian fillings sort of problem. Okay. As Matt was saying, in the doubles, in the positric case, this also very recent work of Roger Casalsi and Nehem Elisa Sherman-Bended and Daphne Weng, this is something called T-duality, whatever it means. Called T-duality, whatever it means, I don't know what it means. But what they do is for each plateau graph, they construct a width. So they connect the more or less classical combinatorics of Posnico to this with combinatorics, and then they can work with it and construct the DT transformation that they show is this mirror spire thing that Matt was talking about. And therefore, it is quasi-close. And finally, this is ongoing work. Ongoing ongoing work, we are looking at parabolic versions of braid varieties because parabolic retroson varieties should also be like some sort of braid varieties. And the conjecture is not only that retroson varieties on the full flag variety admit cluster structures, but also what parabolic ones do. I believe that going to the more general variety case will shed more light on this problem. And well, thanks a lot for your attention.  Is there any known categorification of a varietal variety? And not that I know, as I mentioned, these diagrams look a lot like diagrams appearing in surrogate calculus. These whiffs. If you look at papers of Ben Elias and Jordi Williamson, you will see this sort of thing. So, this makes us think that. So, this makes us think that there might be a way to go there. But one crucial difference is that for Elias and Williamson, this is an equivalence. And for us, this is a mutation. It might be that, I mean, so this surrogate calculus gives you morphisms between surrogate by modules, so just in an abelian category, it might be that we are really talking about morphisms in the derived category of both samples and bimodules, and that's why we have this. Modules, and that's why we have this, but I cannot say anything more precise. Something I use a previous talk, so yeah. Sorry, additive or additive categorification. Um, I haven't looked into it, but yeah, thank you. Any other question? Okay, so is there any So, is there any question from the audience? No? Okay, so if not, let's thank the speaker again. And is there any announcement from