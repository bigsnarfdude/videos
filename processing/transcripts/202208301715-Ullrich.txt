This beautiful country. Yeah, so I'm happy to be here and to present the results of my latest research. It is joint work with collaborators from Chemnitz, Felix Bartle, Lots Kamava, Daniel Potts, and Martin Schaeffer. Yeah, and the whole And the whole thing, the whole core of the subject is we want to recover functions, multivariate functions from finite number of samples. This played a role already in the talk by Mario this morning. That is good for me, so I do not have to repeat all the notions in very detail. I can refer to his talk. I hope that everybody. I hope that everybody of you were aware of Maria's talk. Okay, so we want to recover a function from samples. And yeah, let me start very basic. So if our function belongs to a finite dimensional subspace, think for instance of a finite dimensional subspace spanned by Fourier monomials. So trigonometric polynomials with a fixed set of frequencies or a set of algebra. Or a set of algebraic polynomials. Yeah, a finite dimensional subset where you have, say, an orthonormal basis. Think of orthogonal polynomials, Fourier basis, whatever. So if f belongs to such a subspace and we want to recover it from samples, everything reduces to solving a linear system. So these eta1 to eta m are basis functions in this linear subspace. And if you rephrase And if you rephrase actually what we have here, f is a linear combination of the basis functions. If I plug x1, then I get f of x1. And you can write actually all of this as this matrix vector multiplication. And we want to recover the coefficients in the basis expansion. That's all what we have to do. Okay. And actually, the question, and this is also what Mario raised this morning, is. Raised this morning is what are good points x1 to xn in order to have a stable system matrix here to recover the function. Why do we need a stable matrix? I mean, the world is not always precise. We have, say, noisy data, or the function does not belong exactly to a finite-dimensional system. It might belong to an infinite-dimensional space, like a Sobolev space, and so on. Space like a Sobolev space and so on. And that is why we need a stable recovery in order to cope with disturbed samples, for instance. Okay, so we like to choose the nodes such that this system here is well conditioned. And actually, as usual, we are interested in a rather small set of nodes. We want to take as less samples as possible. So, and here, So, and here we sometimes have given a good set of samples x1 to xn, and we want to shrink them, we want to throw away samples such that what remains is still good, still satisfies the good, say, stability condition properties here. So, this is what I refer to as sparsification. So, given an excellent set of points, then find a good subset that still enables us to recover F. Enables us to recover F. So this topic is Russia rather fashionable. Right now, many, many people dealt with it in different contexts. So this list is far from being complete. So I just put the names which came up in the last years where I dealt with that. Yeah, and especially I would like to mention this connection to the Marcin-Kievic Sigmund. The Marcing-Kiewicz Sigmund inequalities. So, the condition property, what I wanted to have here for the system matrix, can be rephrased as such a, oh, my goodness, can be rephrased in terms of such a Marcing-Kewich-Sigmund inequality. This is, if p equals 2, say, this is exactly the condition of the matrix. Here you have the lowest eigenvector. Here you have the lowest eigenvalue, or a bound for the lowest eigenvalue. Here you have a bound for the largest eigenvalue. So the Massen-Kewich-Sigmund inequality is kind of an equivalent concept in that. So as I said, if P equals 2, then the lines of the system matrix constitute a proper frame in CM. What a frame is, I will come and I will explain that on the next slides. But you can rephrase all that in terms of the language. That in terms of the language of frames. Okay, and in this talk, I will be mainly interested in P equal to. I will not do any theory on P not equal to, mainly P equal to and recovery in L2 is here our goal. Okay, I mentioned the notion of frames here. This is a rather, yeah, this is a notion which plays a role in many different The many different areas in mathematics. Let me just repeat what that is. Let me start with a Bessel sequence. So what is that? If I have a system in a Hilbert space Fi, then a Bessel sequence is actually a system that has the property: if I take all the inner products, sum them up in L2, then I have. them up in L2, then I have a constant which can bound this by a constant times the norm of this element A here. And the frame is actually a Bessel sequence where you also have the lower frame bound. And this is much, the lower frame bound here is actually the important thing in the whole theory. Because once you have the lower frame bound, once you have a system Fi in a Hilbert space with a property With a property that all the inner products with a fixed element i can be estimated from below by a constant times the Hilbert space norm squared of that element. Then this set of coefficients here contain enough information to recover f. In a sense, the operator from a to the coefficient. A to the coefficients is invertible. So, this is actually a consequence of this lower frame bound. So, this will play a particular role in this talk. Yeah, so we call this frame tight if A equals B, and it's a passable frame if A equals B equals one. So, and just to a frame is not necessarily a basis. It might be, it might have redundancy. If you, for instance, If you, for instance, take the union of orthonormal basis, this is a frame, but it is not linear independent. Yeah, and in contrast to that, a respasis, this is just this, this came up in Ben Hunt's talk and other talks today. This is really a basis satisfying this property. So, every respaces is a frame, but not every frame is a respaces. So, we will work in this context. If you do not like the notion of a frame, The notion of a frame, then simply think of a matrix where all the elements are the rows of the matrix. And if you apply the matrix to a vector, then you compute the inner products and you ask for the smallest and the largest eigenvalue, which will be actually the constants here that frame inequality. Okay, so we will be. Okay, so we will be particularly interested in finite frames and finite frames will be. So, first of all, we are interested in frames in CM. So, vectors in vector space CM with the usual frame property. And our goal is if we give us a good frame with good constants, good constants always means that B. Good constants always means that B divided by A is not too big. This is the condition number, okay, of the frame. Then it's a good frame. So, if you give me a frame, I would like to sub-sample the frame. I would like to shrink it such that we have as many, roughly as many frame elements as we have dimension here. So, the size of the frame should be bounded by a constant times this M. A constant times this m here. So, this is then a good frame. It's not too big, it's almost quadratic. In a sense, it's almost a basis. So, it's a small frame. And yeah. So, a first approach of subseaming a frame can be done in a, yeah, we can use a random approach. So, you give me a frame, which is denoted by this yi. which is denoted by this yi index y equal i equal one to m so we have m many frame elements in cm and this m is much larger this capital m is much larger than the small m so and um now we can randomly choose frame elements yeah think of a matrix which is very high and we randomly choose the the the the lines of this matrix by a Lines of this matrix by a certain discrete density, which actually measures the contribution of the particular line of the matrix to its Frobenius norm. If this contribution is high, then the probability of choosing this line of the matrix, this frame element, is then also higher. Okay? And so we can reduce the size of the frame to m times roughly. M times roughly log m. By this procedure, we end up with a new frame which is much smaller, only dimension times log dimension. This P is the failed probability, is a probability involved, of course. It's a random process. Yeah, and this T is this disturbance here where the initial frame bounds are disturbed by T one minus T one plus T. If T is getting small, then this enters here squared, it's getting also large. Is squared, it's getting also large, it blows up. So, but with this random process, we can reduce a given subframe to m log m elements. But this is not enough for us. We want to reduce it even further. Okay. Yeah, it's an easy consequence of Trop's channel bound. Actually, I decided to put this Trop bound because we had a discussion right before the talk on that. The basis of this result, so it's a very straightforward consequence of this bound here, is that we can control the smallest and the largest eigenvalue of a sum of random matrices. So, this can be controlled by such an exponential bound here, m is the dimension, and applying this to this initial frame with the chosen. Frame with the chosen discrete density, how to choose the lines of this matrix, we end up actually with this bound here. So we have this logarithmic oversampling. So now the question is, can we get even further? Can we make it even smaller? Okay. And yeah, this is actually possible by paying a certain price. So there is a nice Price. So there is a nice paper from 2009 from Betsen Speemann Srivastava. The last two are actually also the authors of the solution of the Katterson-Singer problem, which we heard this morning. But this paper was much earlier. So before the Katterson-Singer problem was solved, that result appeared. And actually, for our purposes, it is enough. So what does it say? If we have a frame, but here in RM, not in CM, M elements. Yeah, M elements, then we can constructively select a sub-frame. Think of a matrix, we can constructively select B times M elements. Say this B is like two, so it's almost quadratic, what would we get then? But also weights in order to achieve this new frame here, which is much smaller. Which is much smaller, but involves this additional weights here. And we also pay on the upper frame bound by this oversampling factor B. So if B is getting close to one, if we reduce the very high matrix to a quadratic matrix, then we pay here in this constant. A and B do not play a role because we start with a one tight frame. A and B is one. A and B is one. Okay, this approach is constructive. The over sampling factor B can be made to one. We further incorporate weights as i and so on. So the question is, is there a constructive non-weighted complex version that preserves at least the lower frame bound? As I said, the lower frame bound is what we want to preserve, what we want to keep. Yeah, so this is important. And these weights are somehow annoying. Because what we have in mind for the reconstruction of functions, these weights might blow up or might get, we do not have this under control. The algorithm, the Betsin Spiemann Schliver-Sava algorithm, BSS, does not give a control on the weights. So in order to preserve at least the lower frame bound without any weights, Frame bound without any weights, we need to do something, but it works, uh, it works out very well. But let me first give you two very hard to digest slides. I know it's the last talk for today. I promise I will show many, many pictures very soon. So you can relax and just watch the pictures. But the two slides here give you an impression how that this algorithm is. Algorithm is constructive and it is implementable. So we actually implemented that and tried it on a computer and got some numerical results, which I'll show at some point. But this algorithm does the following. It constructs a matrix. So we start with the zero matrix and it constructs a matrix from bottom to top. We select from the initial. We select from the initial matrix lines. And then, in order to get this good subframe, we take proper lines and add it up with certain weights. So, this is what we do this in the first loop until m times b. We compute certain, say, scores. This is very, you do not have to just the slides show you, it is possible to put this in a computer. This is. To put this in a computer, this is the message of this slide. Okay, and here is the second loop which runs through the lines of the matrix. We again compute certain quantities, and if these quantities satisfy something, we take it to our new sub-sembled matrix. And this works out. We get a new smaller frame, which has, yeah, which has. Yeah, which has this property what we want, and we modified a bit this initial BSS algorithm. And as I said, it is implementable. You can put this into a computer. And the picture behind is that you need to control the spectrum of the matrix, which you build up from bottom to top. So the dots here is the spectrum of this matrix where you take in any step, you take one more. In any step, you take one more line of the initial matrix and you try to control this spectrum by these barrier spectral barriers here. And yeah, we need barrier shifts to shift the spectrum of this matrix, what we build up to the right. It should go away from the zero. We need an edge as large as possible, smallest eigenvalue. Smallest eigenvalue, and oh, yeah, as large as possible, it should be away from zero. So, this is in a sense the idea: this algorithm, how this algorithm works. It's a deterministic, it does not contain any randomness, it's deterministic. But as I said, there are two loops, and in any of these loops, you have to compute eigenvalues, so you have to do a lot of computations. At the end, it's a polynomial time algorithm, but it scales like. Algorithm, but it scales like m to the third at the end, yeah. So, but this is what you do just once, it's an offline step, it's a pre-computing step, and then you have your sub-sampled frame, and with this you work. With this, you reconstruct your function. Okay, so um, this algorithm might take time, but uh still it works. And as I said, I wanted to get rid of um of the weights in order to keep. Of the weights in order to keep the lower frame bound. And this is actually the first theorem I want to show here. We modified this subsampling algorithm and called it plane BSS because we do not have any weights here, but we were not able to preserve the upper frame bound, only the lower one. So, but this theorem is actually linear algebra. You can teach this to say, This is to a student in the first year. So, what it says is the following: you give me m vectors, and I can construct a subset of b times small m vectors out of these many, many vectors. Capital M is very, very large, such that this square sum of Square sum of the frame coefficients, kind of, yeah, can be bounded by this smaller sum. And in a sense, this preserves the lower frame bound of our initial frame. And what we pay is this constant. C is an absolute constant. And if B is going to one, which is possible here, then this might blow up. But if B is equal to, which is a valid. If b is equal to, which is a valid choice, then this is not so bad. So we can control the initial frame and the initial frame bound by our sub-sampled frame by this constructive and actually elementary algorithm. Here is no Katison-Singer involved or some very involved tools from function analysis. This is linear algebra. So if you compare this to this Weber subsampling, which also was mentioned by Mario this morning. Mentioned by Mario this morning. So here, the very deep Kadison-Singer theorem or proof of the conjecture of the Kadison, the proof of the Kadison Singer conjecture led to this sub-sampling scheme here. You start with an initial frame and you get a smaller frame where you keep both the lower and the upper frame bound. You need this additional condition. Additional condition here, and this is what comes out of the proof: these constants here, which get really, really big. So this C1 is the oversampling factor, so we get it down to like 5000 M. So this is a non-constructive statement. It says there exists a subframe. There exists a subframe, but nobody knows how to construct it. But it's powerful in the sense that you can keep both frame bounds here, whereas in our result, we only can keep the lower frame bound. But this is enough for us. And here it is constructive and does not use that involved techniques here. But nevertheless, this is. This is used in order to get the very end optimal result. But as I commented on Mario's talk, if it's okay to have an additional square root of log term in your estimates, then you might use actually this. Oh, I'm sorry, this is a. What is wrong here? What is wrong here? So then you might actually use this result with much better constants. Okay. So what is the consequence then? And a similar result has been given by Mario already here. If we want to recover functions from a reproducing kernel Hilbert space, then you can estimate actually this sampling. made actually this sampling numbers by this quantity where the sigma m's are the singular numbers of the embedding of the reproducing kernel Hilbert space into L2, kind of the benchmark quantities. Okay, so this morning Mario presented this result here without this additional log here. So he had this result and this was based on this Katterson-Singer proof, whereas this result where we spend this additional lock here. Where we spend this additional log here is based on actually elementary linear algebra and can be implemented. So I promised to show some pictures, but let me first also state this result. So the recovery operator this can be made precise, and we get a result here for recovering functions from the Hilbert space by such a sampling weighted least squares operator with high probability. Operator with high probability. So if this P should be very small, this enters only in the lock here. And as Mario pointed out this morning, if these singular numbers decay fast enough, then actually this one behaves asymptotically as the singular numbers sigma m. This is a simple consequence of Consequence of okay. Um, yeah, there is also a further related uh result. You can relate also these um sampling numbers, so the optimal way of approximating a function by n sampling values by the Kolmogorov numbers. How many time do I have left? Six, what six minutes until six, okay. Yeah, so this result here involves this additional log, and it can be removed by using more involved techniques. But with our elementary techniques, say we get up to this log, okay? What did I want to say here? Ah, yeah. So the sampling numbers. So, the sampling numbers are the optimal way of approximating a function by using just m sample values. And here we have in the index b times m many sample values, and we get this bound here where this b might be very close to one, right? Yeah, and the point is in earlier approaches, this b was like still. Earlier approaches, this B was like 6,000. Okay, and this approach we can really make close to one, and this is also then can be seen in the numerical experiments. There is also another nice connection between this optimal approximation using n function values and the Kolmogorov numbers. So the Kolmogorov numbers have been appeared in several talks by Yuri Maliki. Several talks by Yuri Malikin, by Boris Kashin, and other contributors to this conference. So, the Galfa, the Komograv numbers do the following. You want to approximate a function from a function class, right? What you do is you look for the optimal n-dimensional subspace, and then you fix this subspace and approximate in the worst case. approximate in the worst case all those elements from the five from from the class capital F by elements from the subspace so here you ask for the best approximation for this fixed subspace and here the worst case for the class and this optimal subspace is fixed in advance this is the definition of the Komograv number so and Komograv numbers behave of course if if this here is L2 if this is a Hilbert space where you If this is a Hilbert space where you measure the error, then the Komogorov numbers equal the linear widths and the singular numbers and all that. So this is a less interesting thing. But if you put, say, L infinity here, you have really nice effects. So then the Kolmogorov numbers decay faster as you would expect. This is also an old observation by Boris Kashin when he used here, when he computed Kolmogorov. Used here when he computed Konogo of numbers for Sobolev classes in L infinity. So there were some surprising results. But nevertheless, it has been observed by Volodya, Temiliakov that actually, I know one has to be careful here in understanding this result. If you want to recover functions from a function class f in L2, here's L2. So the error of the recovery from sample values is in. From sample values is in L2, but you can relate this error by the Kolmogorov number where you measure the error in L infinity. Okay? And this is interesting because it gives you in several relevant situations the almost optimal bound. This is an abstract result, but yeah, it has applications, it has some specific situations, it gives you a very It gives you a very optimal bound, namely the one where here? Yeah, on the left, there is a row and super. Oh, yeah, the super index means least squares. That means that if I restrict in my recovery process to least squares methods, then I can bound this by the Kolmogorov numbers. And least squares, in a sense, classical non-definite numbers. Squares, in a sense, classical non-weighted least squares. The first result given by Temiakov was when the general sampling numbers are bounded from above by the Komogov numbers times the constant. And in this new result, we can make this constant here precise. We have here the index set, index shift precise. The oversampling factor B is like two. So this is specified, and so this is a Specified and so this is a sharper version than the initial one. Um, Kohen and Micheliorati already had a similar thing, but the index here was n log n and not c times n. So this was the first version in this direction, was improved by Temiakov by index then without the log. And now we have this precise version. And even we can say that the recovery operator is a classical least squares operator and it's. Classical least squares operator, and it still holds. So, but the take-home message is: if you want to recover from samples in L2, you can bound this by Kolmogorov numbers in L infinity. And there is theory. We know Kolmogorov numbers in L infinity in many situations. After that, I did two papers with Volodya in order to compute in some cases the Kolmogorov numbers, and this worked. The Kolmogorov numbers, and this worked out very well. Okay, let me show some pictures. In the beginning, I talked about a finite dimensional situation of functions. And this finite dimensional space of functions had an orthonormal basis, like Fouri basis and so on, whatsoever. And here we consider trigonometric polynomials. Polynomials in, say, in two variables first. We can make this larger, also in higher variables, in higher dimensions, but in trigonometric polynomials in two variables. So, and the frequencies are on a so-called hyperbolic cross. The hyperbolic cross plays in multivariate approximation a very crucial role. So, you see here the hyperbolic cross. This is actually the frequency. Um, actually, the frequency plane, and here you see how the frequencies are kind of located. Um, this is, in a sense, the replacement of the classical Fourier partial sum in higher dimensions. So the frequencies are located in this hyperbolic cross. And the question is, how can I reconstruct functions from this hyperbolic cross-trigonometric polynomial set? The first attempt, and this was already mentioned by Mario, is to use sparse grids. It is to use sparse grids. It turned out that actually sparse grids are very bad in the sense of condition number. So you have as many points as you have dimension here, but the lower bound, the lower reconstruction, so the lower smallest eigenvalue, the lowest frame bound is very poor, and the right one is. The right one is large. So, and in higher dimension, this gets even worse. So, sparse grids were for a long time the method of choice. What we do now is we use our sub-sampling. We take a full grid in every dimension. And then you might say in high dimensions, this would blow up as hell. But in two dimensions, this is still worth. And what we do is we sub-sample this set of nodes. This set of nodes such that we still have a good frame. So now you see we start here with 26056 frequencies and we generate a set of nodes on the time side where we want to recover all the trigonometric polynomials with frequencies here. So, and we have 384 nodes on the time side, which is where the Is where the oversampling factor is 1.5. So here the situation is much better. B divided by A is not so bad. And here we really have guarantees in a sense. But you might say, why not choose simply n random points? But for n random points, the situation is getting in also in higher dimension. If you just use random points, it is too bad. Points, it is too bad. Remember, for the random points, we need this logarithmic oversampling. We need more, much more points than a dimension of the space. So, if we throw as many points as we have dimension without the logarithmic oversampling, this will be poor. But our deterministic sub-sampling is working good. So, and you see here, if you are in d equal five, I cannot plot this anymore. D equal two. D equal two, I can plot a hyperbolic cross, but at d equal five, I cannot picture this. So let me show diagrams. So we have now a five-dimensional hyperbolic cross. And yeah, it looks like a star in five dimensions. It's an interesting set. We use an initial set of points which we want to shrink. And this actually works. Here in this picture, you see that we blow up. In this picture, you see that we blow up the oversampling factor from up to four, and you see that the lower and the upper frame bound get closer to each other. So we can sub-sample this with our algorithm, with our constructive algorithm, we get a smaller set of nodes which have good stability properties. Good stability properties. So, this is can be really implemented and it works according to the theory we gave. So, and now let me finally comment on some recent work. It's actually also an example for this general theory. As I said, if I go to higher dimensions and I want to recover a function, say, in dimension 10 or 12, then it is not very Then it is not very wise to start with such a dense lattice in every dimension, in every direction. These are too many points. So what people did is they constructed so-called rank one lattices. So you fix a vector and then you add this vector again and again. If you go out of your unit cube, so you start again and you. Again, and you can construct such a rank one lattice of points. These points have a structure, and if you use them for the recovery of functions, you have algorithms available which are good for fast matrix vector multiplication. So, what I mean is fast Fourier transform. If you have a structured set of nodes, then you can. then you can use you can use fast algorithms in order to reconstruct it. So you do not have, you do not only have good error bounds, you also have a fast algorithm. But the problem is that even this construction gives us too many points compared to the dimension of the subspace. It scales quadratic, not longer to the D it scales quadratic. So it's suboptimal in. Quadratic. So it's suboptimal in the complexity bounds. So and what we now propose is we take this nice set of points and perform a two-step sub-sampling procedure. First, we select randomly points out of this. And in the second step, we do this deterministic sub-sampling. So we throw away even more points. So from this nice set of nodes where we want to question. Set of nodes where we want to question the function, we have this subset which is much, much smaller and has a size compared to the dimension of the space. And this actually works well when we want to recover functions from, say, Sobolev spaces with mixed smoothness, where this hyperbolic cross is actually the set of frequencies where most, yeah, this is actually. Where most, yeah, this is actually the is this if you want to do the approximation in terms of the Fourier partial sum, then you would use trigger-man polynomials from the hyperbolic cross. And this is the Soberlev space here with a tensor product weight, and the hyperbolic cross is actually the set of frequencies where this is constant, in a sense. So, this is the ESO lines of this smoothness weight here. Smoothness weight here. So, and what you should get from this slide is actually that in order to approximate the functions with m linear information, this is the best bound you can have. So, and we can recover this optimal bound up to a logarithmic factor by this method of. method of taking such a rank one lattice and subsample is subsample it by this two step procedure to a smaller lattice and from these points we take the function values and recover from those function values the function the error is measured in l2 and what we get is such a bound and here you actually see this optimal bound what i just show you showed you but You showed you, but with this additional logarithm, and this can be made as small as you wish. So, in a sense, we now can really give a good set of points for the reconstruction of functions from certain Sobolev classes in high dimensions. And now you might say, okay, if you do all this sub-sampling procedure any time again and again for any function, Uh, for any function, then this might blow up your computing time. But you do this just once because here on the left-hand side, you consider the worst case error. Before you have function values and recover the function, you do the sub-sampling with the nodes. You do this just once. This is a preconditioning step. And once you have these nodes, you can do, you can treat all the functions in the class. So, and yeah. So, and yeah, this is actually for experts, let's say. If you take such a full lettuce, you always have half the rate, but with our subsampling methods, we can get here the optimal rate. Or in other words, we can precisely give you this set of nodes where you have to question the function in order to reconstruct it. This was a question raised by Mario this morning. So, actually, we have a way of finding. Actually, we have a way of finding good points, but they do not look that beautiful as he might want to have it. But it's a constructive way of finding good sampling nodes for a given class of function. So, and I told you that this can be implemented. So, we did that, or Felix did that mainly. So, he used this frame sub-sample. Used this frame subsampling procedure, gave himself a function in five dimensions where we precisely know the Fourier coefficients. And then we let our routine run on this function. And since we know the Fourier coefficients precisely, we can give here the precise L2 error and compare it for different methods. So the black dots here is actually this full. Here is actually this full lattice. And the blue and the magenta dots here are the sub-sampled lattice. So, what you can see here is that you can achieve the same error here with much less points. And also, the computing time in seconds is, so here's your computing time of computing the approximant is for our subsample. For our sub-sampled method, comparable to this full lattice. The full lattice has this nice structure. You can use fast Fourier transform, but we can still use this here for the sub-sampled lattice since the structure is still there. We only threw away some of the points, but the structure is actually still there. Yeah, and as I said, we have this two-step procedure. Two-step procedure. We first take random subsampling and then this deterministic subsampling. And here you see actually the magenta is the random subsampling and the yellow one is this additional shrinkage with this deterministic algorithm. So it uses much less points for the same error. And yeah, this is what you see here. Yeah, and the L2 error is comparable. Is comparable, but you need less points for it. So, this also works for other subspaces. You could also use wavelets and take wavelet subspaces to think of you have a function which is a linear combination of certain wavelets, also hyperbolic wavelets. You do not know the coefficients, but you can recover it from sample. You can recover it from samples, and here is how the samples look like: we start with a random sample, and then we shrink or we reduce this node set even further. And from this reduced node set, we can recover the functions. And finally, we are also working on the sphere. So, what you could also do is to start with a nice Marcin-Kiewicz-Sigmund inequality. Marcing-Kiewicz-Sigmund inequality on the sphere. So, like this regular thing here, Gauss-Legendre points or whatever. And then, using this frame sub-sampling procedure, you can thin out these nodes significantly, but still have a good node set to reconstruct, say, functions from the polynomial space of order n. Polynomial space of order n. So, in other words, if you give me a Marcin-Kiewicz-Sigmund inequality on the sphere, which might have many, many, many points due to the high-dimensional sphere or due to other constants which might blow up, we could run our sub-sampling procedure and reduce this Marsing-Kewich Sigmund inequality to a much smaller one, which still A much smaller one, which still has good recovery properties, yeah. And to conclude, so um, we have a constructive approach of a subsampling procedure which leads to optimal bounds for function recovery. This approach is implemented in a Julia package. You can find it here, and yeah, it works well for hyperbolic cross spaces, hyperbolic wavelet spaces. Wavelet spaces and say a take-home message could be: if you give me good quadrature rules, I can use sub-sampled version of that for the recovery of the function. So, and with that, I want to conclude. And I hope that it was not too boring for the last talk. So, I'm looking forward to your questions. If somebody wants to ask from home, then you can line also live. Wait, would you have to speak here? Yeah, I just have a question about this mass inquiry. A question about this mass increase sigma inequalities. So, first of all, so do you mean the mass increase inequality in L2, right? Yes, yes. Yeah, also, how about the weights? Are they equal with or with some non-equal weights? Maybe I'll speak. Yeah, yeah. So, so there are also, if you sub-sample, then in general, you produce non-equal weights. Non-equal weights, but with with the strategy to With the strategy to get rid of the weights, you can at least keep the lower bound in the Marcy-Kewich-Sigmund inequality, which is the important one for the recovery. That's right. So, and this always works with non-weighted, so with plane weights. Oh, yeah, this is actually more interesting because, you know, yeah, I will talk with you later. Yeah, this is okay. Yeah. Further questions? Maybe I will ask one thing. In this lemma, the elementary lemma that you have, yeah, why don't you show it? So one strategy could be just to take the largest coefficients. Which one here? Yes. I mean, what is behind here is this BSS, but what we do is we blow up the frame. Up the frame, so we uh did these yi are vectors from cm. Yes, we blow them up to vectors from c2m and these new vectors have a l2 bound, a common l2 bound from below. And this gives us the uh the the the possibility to throw away the weights. So, taking only the largest does not work because you see here, this is re-weighted. See, here this is re-weighted. So, um, in the on the left-hand side, you have this one over m and the sum up to m, and on the right-hand side, you have this one over small m and uh right, but yeah, yeah, there are big m over little m. So it's kind of an average, right, right. But yeah, yeah, okay, but it doesn't work. So, so, yeah, it's something a little more complicated where you have to use the structure actually. So, so it is stated in a very simple way. You can explain this to first students. This is to first-year students, but the proof behind is rather involved because you use this PSS algorithm in a modified way. Yes. Okay. Some more question? So if not, we thank Tino and all the speakers from the afternoon session. 