Hear you really well. I can actually see the screen on the projector. So it's yeah, it's really well working well. All right. Representations of Boolean functions by real polynomials play a central role in theoretical computer science. And today I'll talk about what I consider to be a particularly natural complexity measure called approximate degree. Approximate degree. The notion of approximate degree was introduced 30 years ago in the seminal paper of Nissan and Segedi. For a given function f, the epsilon approximate degree is defined as the minimum degree of a real polynomial p that approximates f pointwise within epsilon. All right, so this is approximation in the infinity. Approximation in the infinity norm. And this complexity measure is denoted deg sub epsilon of f. Now, as the definition implies, it is always an integer between zero and n, and the standard setting. Your screen is not advanced. Okay, okay, yeah, yeah. All right. Thanks. I think it fell asleep there for a little bit. All right. All right. So, once again, for a given function f, the epsilon approximate degree is defined as the minimum degree of a real polynomial p that approximates f pointwise within epsilon. Once again, we're talking about infinity norm approximation. This complexity measure is denoted deg sub epsilon of f. And as the definition implies, the implies the complexity measure is always an integer between zero and n. The standard setting of the error parameter, as usual in complexity theory, is a third, and that is without loss of generality. You can boost the accuracy fairly straightforwardly. All right, over the past three decades, approximate degree has proven to be a powerful tool in theoretical computer science. In theoretical computer science, lower bounds on approximate degree are widely used in complexity theory, whereas upper bounds have various algorithmic applications, respectively. So on this slide in particular, you can see that approximate degree has produced breakthroughs in quantum query complexity, communication complexity, and circuit complexity. On the algorithmic side, On the algorithmic side, approximate degree underlies many of the strongest results obtained to date in computational learning and differentially private data release. There are applications to algorithm design in general as well. We'll discuss some of these results later on in the talk when we move to the communication complexity part of it. Complexity part of it. All right, approximate degree has been particularly valuable in the study of the class AC0 in circuit complexity. An AC0 circuit pictured here on the slide is any circuit of polynomial size and constant depth made up of and or not gates of unbounded famine. All right, if we go. All right, if we go back to the motivation slide and highlight the AC0 results in yellow, they'll make up approximately 50% of all applications. And once again, these include breakthroughs in quantum query complexity, communication complexity, and computational learning, such as the resolution of the quantum. The resolution of the quantum query complexity of element distinctness, the determination of the communication complexity of set disjointness in the two-party quantum model and number on the forehead multi-party model, and the fastest known algorithm for PAC learning DNF formulas under arbitrary distributions, to mention just a few. In view of these complex In view of these complexity theoretic and algorithmic applications, it has been an important open problem to determine the maximum approximate degree of an AC0 circuit. For circuits of depth one, the question is very tractable and the answer was given actually by Nissan and Segeri themselves way back in their seminal work introducing this complexity measure. This complexity measure. And they proved a tight lower bound of root n. Now, this was the best bound for a while until the breakthrough work of Aronson and Xi, who gave a lower bound of n to the two-thirds in their paper on the element distinctness function, motivated by quantum query complexity. Bun, Kothari, and Thaler should be alphabetized. Should be alphabetized the other way. Banquotari and Taylor considered a generalization of this function called k-element distinctness, for which they proved a lower bound of roughly n to the 3 fourths. A big step in this line of work was taken five years ago by Bonn and Thaler in a beautiful paper where they proved a lower bound of n to the one minus delta for any constant delta. For any constant delta. This essentially meets the trivial upper bound of n on the approximate degree of any function. Unfortunately, the depth of Bonn and Taylor's circuit is quite large. It grows with one over delta. If you try to flatten their circuit to depth two or to depth a thousand for that matter, the circuit size blows up. The circuit size blows up. The new size becomes something like 2 to the log of n to the log 1 over delta, which is no longer polynomial. So the flattened circuit is no longer in AC0. So, this table captures the state of the art prior to our work. As you can see, Our work, as you can see, there was a big gap in our understanding of the approximate degree for small depth versus large depth. And this gap persisted for the past five years with no techniques available to address it. In fact, Justin and Mark, in one of their papers, actually conjectured, they seriously entertained the possibility that. Seriously, I entertain the possibility that for every fixed depth, the exponent is going to be bounded away from a constant. So, for say, DNF formulas, the exponent should be one minus delta two, where delta two is some absolute constant bounded away from zero. For circuits of depth three, the exponent should be one minus delta three, where delta three is some constant. Three sum constant bounded away from zero as well. And in this work, we disprove that conjecture. We close the gap completely by proving that AC0 circuits of depth two already achieve essentially the maximum possible approximate degree of n to the one minus delta. Our approximate degree lower bound essentially matches. Degree lower bound essentially matches the trivial upper bound of n that holds for any function. And our depth is optimal as well because AC0 functions of depth one, conjunctions and disjunctions, in other words, have approximate degree only root n. Okay, so let me state the result more formally. Let me state the result more formally. Let delta be arbitrary, then there is an explicit DNF formula f in n variables that has constant width and approximate degree n to the one minus delta. Recall that the width of a DNF formula is the maximum size of its terms. Terms. So, in particular, our DNF formula is essentially as simple as it can possibly be. It has only polynomially many terms, and the size of each term is bounded by a constant. We also prove a refinement of our main result where the error parameter is allowed to be vanishingly close to a half as opposed to being a constant. As opposed to being a constant bounded away from a half. So, this is the more refined statement. Let delta and c be arbitrary constants. Then we construct an explicit DNF formula, F, in n variables that has constant width and approximate degree n to the one minus delta, even for approximation to one half minus something inverse. One half minus something inverse polynomially small. You can ask whether you can further push the error closer to a half, and the answer is no. This is optimal because any polynomial size DNF formula can be approximated straightforwardly by a low degree polynomial to error inverse polynomially close to a half. Now, approximate degree is invariant under. Degree is invariant under function negation. So, all our theorems are valid for CNF formulas as well. You can use them interchangeably. Before we move on to the communication complexity aspect of this talk, I'd like to mention an important extension of this result that we obtain that has to do with one-sided approximation. One-sided One-sided approximation is meaningful for any Boolean function. Specifically, the one-sided approximate degree of f is just the minimum degree of a real polynomial p with the following property. At every point where f is zero, the polynomial is supposed to approximate the function very accurately within epsilon. Accurately within epsilon. And at every point where the function is one, the polynomial is not obligated to be bounded from above. So it needs to be at least one minus epsilon. Now, this is an incredibly natural definition for a complexity theorist because this is essentially the class NP reimagined for polynomial approximation. Approximation. So in NP, we expect the algorithm to be extremely accurate on inputs that are the zeros of the function, right? You're not supposed to make any mistakes on those inputs. That corresponds to the polynomial being very, very close to the function on those inputs. And when the function is one, you are allowed to be arbitrarily high as long as you output one on. As long as you output one on at least one point. Okay, so that's the one-sidedness in this definition. We are going to denote this deg sub epsilon soup plus plus for one-sided approximation. And as before, this complexity measure is between zero and n. Now, obviously, Now, obviously, to approximate something with two-sided approximation, you need to at least be able to approximate it with one-sided approximation. So the two-sided approximate degree is bounded from below by the one-sided approximate degree. And the standard setting of the error parameter is, again, a third, and you can boost that extremely efficiently, just like with two-sided approximation. Now, Now, what we do in this work is obtain the exact same result for one-sided approximation, and that allows us to solve a variety of a number of open problems, both in approximation and in complexity theory. Communication complexity. The three specific problems that concern approximation are as follows. They're very natural, I think. The first problem. I think. The first problem is to determine the one-sided approximate degree of AC0. That was wide open. The second problem is also very natural. It says, are there functions where the gap between one-sided and two-sided approximation is large? You know, we know one such function, the OR function, for example. Function, the OR function, for example, you can come up with slightly more improved examples where the gap is also somewhat large. But what is the optimum gap? Can the gap be arbitrary? Is it one versus n? Similarly, you can say, are there functions where you can approximate the function in a one-sided manner very inexpensively, but the negation. But the negation of that function cannot be approximated in a one-sided manner inexpensively. All right, these definitions roughly correspond to the questions P versus NP and NP versus CONP in the computational world. And in fact, we're going to interpret the answers to these questions in terms of the In terms of the communication classes and P and Co and P later on. All right, so the take-home message is that the strongest answers you could hope for to these questions turn out to be true. All right. The first question has the following answer: Let D be arbitrary, then there is a width, a constant. There is a width, a constant width CNF formula in n variables that has one-sided approximate degree n to the one minus delta. So even if you just wanted a one-sided approximation for this function, even then you would need to essentially have the maximum possible degree. Now, this also holds for error vanishingly close to a half. I'll stop mentioning that. I'll stop mentioning that in all later slides. The depth is best possible for the same reason as for two-sided approximation, and so is the error. The previous best result was an AC0 function of depth that grows with one over delta. So nothing was known for DNF formulas, for depth three formulas, for depth three AC0 circuits, for depth four AC0 circuits. For depth four AC0 circuits in particular, we show that at depth two already you achieve the maximum hardness. Now, you see that in this depth, in this theorem, it says CNF formula. You could not replace this with a DNF formula here because the negation of this function, right? Well, you couldn't replace. Well, you couldn't replace it with a DNF formula of constant width, because in a DNF formula of constant width, what you could do is take each term. The width of each term would be a constant. So you could construct a constant degree polynomial that exactly computes every term of the DNF. And then you would just add those terms. That would give you an excellent. An excellent one-sided approximant for your function. In fact, that one-sided approximant would actually have zero error. It would have zero one-sided error. So the negation of this function actually has one-sided approximate degree bounded by the width of the CNF, bounded by the width in. The width in this theorem. And that solves the second problem on the list. For any delta, there is a function with the maximum possible gap between the one-sided approximate degree of f and the one-sided approximate degree of the negation of f. All right, now what about What about the gap between one-sided approximation and two-sided approximation? That falls out as our second corollary as follows from corollary one for the following reason. If you consider the two-sided approximate degree of the negation of f, that is going to be bounded from below by To be bounded from below by the one-sided approximate degree of, it'll be the same as the approximate degree of F, and that in turn will be bounded from below by the one-sided approximate degree of F, which by the previous corollary we know is m to the one minus delta. All right, so to flip back to the previous slide, as soon as Previous slide, as soon as we know that there is a CNF formula with essentially the maximum possible one-sided approximate degree, that opens the floodgates and we essentially have everything. The strength of this result immediately implies optimal solutions to the other problems. If you try to attack them directly, You know, it's not clear how to even proceed. For corollary two, for example, the best example that I could come up with using earlier methods was something like n to the three-fourths or n to the two-thirds. But it's nowhere near optimal. The question: How does the width depend on delta? Yeah, yeah, the width will depend on. Yeah, yeah, the width will depend on delta. That's true. Yeah. So for any function, I mean, for any delta, you're going to have the size of the formula will depend on delta and the width will depend on delta, but we're interested in constant delta. Yeah, that's a great question. Is it like exponential or like how bad is it? How big is it? Yeah, let me. Yeah, let me think. I think it's yeah, I think it's exponential in one over delta. This dependence is necessary, right? If you have a constant width, so actually, I mean, there are two dependencies. There is a dependence. The width depends on one over delta, and the size of the DNF also depends on. Of the DNF also depends on delta, of the CNF. And I forget what the exact, I mean, which of these parameters has dependence exponentially in one over delta? I think it's the size, actually. I think the, or maybe they both depend exponentially in one over delta. Yeah. But what was it? What's the follow-up question? This depends as an episode. If I give you a width, a constant width, the width is k, then it's approximately necessary. It's not linear. Yes, thank you. Here, yes, thank you, thank you very much. Yes, that's exactly right. Um, if you fix the width to say a million, right, then the largest approximate degree you could attain, even for two-sided approximation, would be n to the one minus one millionth. So the depth absolutely the width absolutely needs to grow for you to approach approximate degree n. Approximate degree n in the limit. That's true. All right, let's keep going. Let me mention some applications of this work. The first one is to quantum communication. Here, the parties communicate by exchanging quantum bits, and they're additionally allowed to share arbitrary prior entanglement before they start communicating. Their goal is to compute F. Their goal is to compute F with error at most a third on every input. Now, it's hard to believe it's 2022, but prior to this work, we didn't know of any strong lower bounds for the quantum communication complexity of DNF formulas. Seems like a fundamental question. Seems like a fundamental question that we should have known an answer to decades ago. In the breakthrough work of Razborov, he obtained a lower bound of root n for the disjointness problem. You can get some improvements on that, but they're less than n. They're polynomially worse than a linear lower bound. And we finally settle. We finally settle the issue in this paper. So let delta be arbitrary, then there is an explicit problem, two-party problem with bounded error quantum communication complexity and to the one minus delta. And it's representable by a constant width DNF formula. In this case, DNF, CNF, it doesn't matter because the model is invariant under negation. The model is invariant under negations. This falls out as a corollary to our main theorem on two-sided approximation via the pattern matrix method. It's essentially optimal and finally settles the quantum communication complexity of AC0. Before, we only had strong lower bounds for AC0 circuits of depth that depends on one over. That depends on one over delta. You could say, well, I'll get to multi-party communication, but basically, even if you extend this to multiple parties, the same result holds. For multiple parties, the standard model is the number on the forehead model. Here we have a function. A function f with k arguments, each of which is a string of n bits, and we have k parties which are numbered one through k. Each party is missing exactly one argument, which makes this model very powerful and difficult to prove lower bounds. And the communication occurs in the form of broadcasts, and you know, that means whenever as soon as You know, that means as soon as someone sends a bit, it becomes available to everyone else. The parties have access to a shared source of randomness in the case of bounded error computation. And they're required to compute F on every input with probability of error at most a third. Again, in the bounded error case. In other models, the corresponding computation. The corresponding computation criterion. The multi-party communication complexity of AC0 has been studied for at least the past 15 years. And in this work, we are finally able to give a fine-grained answer to this question. Let delta and C be arbitrary constants, we construct an explicit. An explicit k-party communication problem with randomized communication complexity at least n over 4 to the k quantity to the 1 minus delta. And it's representable by a polynomial size, a CNF or DNF formula. We once again We once again obtained this result as an immediate corollary to our main theorem on approximate degree by invoking the pattern matrix method. This lower bound is essentially optimal and determines the multi-party communication complexity of AC0. You could combine the power of these last two models that I've These last two models that I've talked about, quantum and multi-party, and you'll obtain the quantum multi-party model. The same lower bound remains valid there as well. You get an additional, I think, penalty that depends on K, but it's very minor. In previous work, In previous work, the best lower bound was the only lower bound that approaches n in the limit was for AC0 circuits of large depth. So even for depth a million, for example, you didn't have anything linear as a communication lower bound. This is more or less representative of the state of the art for arbitrary explicit functions. Explicit functions in multi-party communication complexity. It's a huge, huge open problem to prove a lower bound for any explicit lower bound for more than log n parties. I mean, the barrier where all our methods stop working is k equals log n. This is for randomized communication. Randomized communication. We obtain the exact same result for non-deterministic communication by using the one-sided approximate degree result that we prove in this paper. So the exact same theorem holds if you replace it with non-determinism as opposed to randomness. Of course, in the setting of non-determinism, it's important to be working with CI. It's important to be working with CNF formulas. For randomness, it doesn't matter. You can switch between DNFs and CNFs at will. For non-determinism, it's important to be using CNFs because DNFs are easy for non-determinism. This is a quadratic improvement on the best previous lower bound for AC0, no matter which depth you look at. Okay, so this has a very So, this has a very satisfying consequence for communication complexity classes in communication complexity theory. There are three classes that I particularly like a lot. They're quite natural. It's the class NP, the class BPP, and the class CONP. They correspond to problems that can be Problems that can be efficiently solved by non-deterministic protocols, randomized protocols, and co-non-deterministic protocols. By efficient is meant communication complexity that's polylogarithmic in N. And these classes are very well understood. They have drawn an immense amount of interest over the past 10 years. Over the past 10 years. In particular, it's known that CONP is not contained in NP, it's not contained in BPP, it's not contained in the union of these two classes. Not even contained in MA. What our work contributes, though, is the fact that you can separate CONP from these classes. From these classes, essentially, optimally by an extremely simple function, by a CNF formula of constant width. So specifically, if you look at the previous slide, you have a CNF formula. Therefore, the negation of that function is a DNF formula also of, well, the width doesn't matter here. It's just a Matter here, it's just a DNF formula of polynomial size, and therefore you have a straightforward non-deterministic protocol for it, where the parties just pick one of the terms and evaluate it. All right, output the result. So the negation of our function has non-deterministic complexity log n, co-non-deterministic complexity. Sorry, the function has. Sorry, the function has co-non-deterministic complexity login, whereas the original function has non-deterministic complexity essentially maximal and randomized complexity essentially maximal. So, this puts it in the blue region outside the two yellow regions. So, you get an essentially optimal separation by a ridiculous. Essentially, optimal separation by a ridiculously simple-looking function. This is a polynomial. No, this is a quadratic improvement on the previous explicit separation for these classes. If you were okay with being non-explicit, then Tony has a beautiful paper where. Tony has a beautiful paper where, with co-authors, where she proves a separation of log n versus n, which is straight n, you know, omega n for up to an exponential number of parties. But that proof uses a probabilistic argument and you don't know anything about the complexity of that function. The complexity of that function. The previous explicit separation, which was quadratically weaker for these functions, used the set disjointness function. All right, so this is all I have to say about our main results and our applications. In the rest of the talk, I'll focus solely on the Solely on the proof of our main theorem on approximation. So, this is the theorem that we'll spend the rest of the talk discussing. To set the stage properly, I'll start by reviewing the approach of the previous best result on the approximate degree of AC0 due to Justin and Mark. Justin and Mark that they proved five years ago. Asha, quick question. Did you want to, are you just proving the two-sided error lower bound or are you doing the yeah, yeah. So thanks, Madhu. So we're going to stick to the simplest result in this entire work. So the approximation error is a third. The approximation is two-sided. There's no communication in the picture. There's no communication in the picture. So, yeah, just really strip it to the basics. Yeah. All right. So, the Bunn and Taylor approach. As the majority of other works on approximate degree, Bunn and Taylor's result is a form of hardness approximation, which means you come up with a procedure that takes an arbitrary function f and transforms it. And transforms it into a function with somewhat higher approximate degree. Now, it's important that this transformation preserves membership in AC0. That way, if you start with an AC0 function, you're going to get an AC0 function back with slightly better parameters, slightly higher approximate degree. And then if you apply this transformation repeatedly, you can reach arbitrarily high approximate degree. Can reach arbitrarily high approximate degree. So, specifically, Bonn and Thaylor start with an arbitrary function f in n Boolean variables, and they compose f with a carefully designed AC0 circuit of depth 3 with roughly n input bits. All right, this results in a compose function, which I'm going to denote capital F. To denote capital F. Bunn and Taylor show that if little f has approximate degree at least n to the one minus alpha, then the composed function, capital F, has approximate degree n to the one minus two-thirds alpha. So you see that the exponent gets a little closer to the optimum. And by applying this transformation, By applying this transformation constantly many times, you get an AC0 function with arbitrarily large approximate degree. Now, let's take a look at what happens inside this big yellow box, which from now on, I'm going to call the hardness amplifier. All right. Bunn and Taylor start with an arbitrary function f in n Boolean variables, and they compose. Variables and they compose it with this and/or tree. It's not hard to see using the methods of previous work that the approximate degree of this big composed function is root m times larger than the approximate degree of the original function f. So this seems like good news, but in actuality, you haven't gotten anywhere because. Haven't gotten anywhere because the number of variables has also grown and it has grown by a factor of m. Whereas your approximate degree has only increased by a factor of root m. So you haven't made any progress. You've actually made things worse for yourself. However, and this is the magical part of their proof, Bunn and Taylor prove that this new approximate degree This new approximate degree lower bound remains valid even if the input is restricted to Heming weight roughly n. All right, even for inputs of low Heming weight. This makes it possible to compress the input to the function to an input string of roughly n bits using some additional circuitry to do the compressing, which can be done by a DNF formula. Which can be done by a DNF formula, like so. All right, so this is Bonnet Thayler's hardness amplifier. And there are two major bottlenecks here. The first bottleneck is component-wise composition, where you compose it with this and or tree. And the second is input compression. In both cases, you require expensive circuitry as a result of which. As a result of which, the hardness amplifier is a Dep3 AC0 circuit at best, and it cannot be flattened without the size blowing up. Now, to prove our main result, we cannot afford even a single iteration of the Bond-Taylor hardness amplification. So, this is where things were stuck five years ago, and it was not clear at all if there is any way forward. Clear at all if there is any way forward from here. In fact, like I said, it was conjectured that you couldn't achieve near-linear approximate degree with DNFs. So how do we approach it? And how do we overcome this barrier? Here's a high-level sketch of our work. We actually work in a more general setting. We start with an arbitrary function f. With an arbitrary function f that is allowed to have an astronomical number of input bits, like n to the 1000. And our assumption, our working assumption is that the restriction of f to inputs of weight n has approximate degree at least one n to the one minus alpha. What this basically means, his Basically, it means that F formally has n to the 1000 input bits, but it is defined only if the Hamming weight is at most n. And then the approximate degree of that restricted function is at least n to the one minus alpha. So it's a already a weaker assumption than what you saw on the previous slide. And then we compose F with a carefully designed DNF. With a carefully designed DNF of constant width, which has roughly n to the one plus alpha input bits. If we denote this composition by capital F, then we prove that the approximate degree of capital F is at least n to the one minus two-thirds alpha. Even if you restrict the input to capital F to have Hamming. To capital F to have Hemingweight roughly N. You see the induction coming into play here. Now, there are two very important things that go on that make this work. First, the new function, capital F, has higher approximate degree than the original function. Second, the new function, capital F, is much denser than the original function. Denser than the original function. By that I mean that it takes n to the one plus alpha input bits, whereas the original function was very sparse. It had n to the 1000 input bits. To summarize, what we do is we start with a very weak object, which is a sparse function with mildly large approximate degree. And we create from it a function that is. It is a function that is denser and has much higher approximate degree. And we achieve these two things with a hardness amplifier that is a monotone DNF of constant width. It actually wasn't clear to me until the last moment that this could be done. But a lot of, well, I guess two very Well, I guess two very unlikely things happen, as you'll see in the rest of the talk, and they make this possible. By repeating this process, this hardness amplification process, a constant number of times, we obtain our main result. That's because every time the hardness amplifier is a monotone DNF formula of constant width. So if you compose constantly many of the Compose constantly many of those, you're still going to end up with a monotone DNF of polynomial size. Sasha, I asked a question. So, but how can you iterate the procedure if the composed function is only hard at dynamically like n, because now I have like n variables, right? Didn't you need it to be sparse originally in order for this to work? So, um, I mean, the Spartan, the Spartan. the spartan the spartan sparse sparseness is not a requirement uh it's uh it's an additional feature that we handle okay so i mean if you have a function that is is dense originally uh then that's great right you're going to uh the point is the hardness amplification it introduces a little sparsity in order to increase the approximate degree and then it needs to remove And then it needs to remove that sparsity a little bit so the function becomes denser. So there are these two, it's a push and pull. On the one hand, you want to increase the approximate degree, which causes the function to be sparse, but then you hit it with this densification process that causes the function to be dense again. So that's what happens. You can't increase the approximation. What happens? You can't increase the approximate degree without making the function sparse. So, even if you had a sparse, a dense function to start with, you couldn't do anything with it. I mean, the meat of this work is this big yellow box, right? The big question is, how do we design this hardness amplifier with a constant with DNF? It seems ridiculous. So, how does this work? All right. All right, the original function takes as input a vector of n to the 1000 bits, but at most n of them can be turned on at any given time. All right, this makes it possible to view the input as the disjunction of n vectors, each of which is a coordinate vector or the zero vector. all right so this step is it seems it seems uh it doesn't seem like much much of a big deal but uh it makes things conceptually um totally different because now you have n well-defined inputs each of them is of canonical form it's either a coordinate vector or the zero vector all right in this picture All right, in this picture, to actually make the approximate degree grow, we are going to encode each of these n vectors using a string of n to the alpha bits. All right, in this picture, h corresponds to the decoding function, which takes as input. Which takes as input a string of n to the alpha bits and produces a vector of n to the 1000 bits, which is going to be either a coordinate vector or the all-zeros vector. The challenge is to design h such that h is hard for polynomials, but can still be computed by a DNF formula of constant width. That way, the apprax. That way, the approximate degree is going to grow while the circuit will remain small. All right, let's now take a look at some technical details. From now on, I'll let capital N stand for the total number of bits and let M stand for the input length of the decoding function. The decoding function H needs to produce either the zero vector, which Either the zero vector, which I'm going to denote E sub zero, or one of the coordinate vectors. Maybe it produces some junk, which we hope will happen with small probability. The requirements of H are as follows. Like I said, H needs to be a monotone DNF formula of constant width, and the support of each E sub I needs to have constant. Needs to have constant Hemming weight. This is important because this is needed in order for the hardness amplification to produce a dense function in the end, for you to be able to compress the input. Finally, the encodings of the different E sub-I's need to be indistinguishable by polynomials. Otherwise, you can't force their approximate degree to grow. Grow. Now, unfortunately, these three properties are too ambitious and you they are contradictory. In fact, you cannot implement them literally. What we do instead is achieve the third property with respect to certain probability distributions, lambda 0, lambda 1, dot dot dot, lambda capital N, correspondingly. And likewise, we prove the second property with high probability with respect. With high probability with respect to the same distributions, lambda i. So, what are these distributions? If you, so we're going to let k be a large enough constant, we want the distributions lambda i's to have properties analogous to those of h. Specifically, the lambda i's need to be concentrated on inputs of Hemingweight K. Of Hemingweight K. K being a constant, this works really well for us. Second, so each input of Hammingweight K must belong to the support of exactly one distribution, lambda i. That way you can decode in a unique way. And finally, the lambda i's need to be indistinguishable. Need to be indistinguishable by low-degree polynomials in the sense that you take any low-degree polynomial p, then to that polynomial, the distributions lambda i and lambda j for any i and j look the same. Once we have such lambda i's, we can define H, the hardness amplifier, the decoding function rather, to be the monotone DNF formula. Monotone DNF formula that has width k and just checks so it assumes that its input z has weight exactly k and it just checks to see which lambda i has this input z in its support. Now, there is a small probability with a small probability a lambda i is gonna out is gonna create an Alp is going to create an input of high Hamming weight, in which case H will not output a coordinate vector. It'll output a vector of weight, let's say, 1000 or of weight N over two of high weight. And then the decoding will fail and the construction will not work correctly. But fortunately, that happens with exceedingly small probability, and we can tolerate it. and and we can tolerate it um i think there's a there's a question in the chat uh but um for some reason i i i don't have access to the to the chat box from from this full screen mode um so if you guys uh want to use the audio and just ask your question yeah i can ask i was asking so when you say low degree polynomial so what do you mean how low low degree Low degree. Yeah, great. So polynomial in n, actually, it's a small polynomial in n. Okay, thanks. It's little m to some small beta. All right, so this is your growth. All right, so now we have this purely analytic reformulation, right? We've gone away from combinatorics. Gone away from combinatorics. Now we're in analysis land. But we have our DNF of width K as long as we're able to create these distributions, which are concentrated on inputs of low weight. They are disjointly supported on such inputs, and they're indistinguishable by low-degree polynomials. So, literally, if you can focus on these three points for Three points for the remaining two or three slides. That's all you need to know. So you can forget everything else we've talked about today. Just these three points is what we are looking to achieve. Distributions that are concentrated on inputs of weight k for a constant k, they should be disjoint on those inputs and they should be indistinguishable by low-degree polynomials. Okay. Okay. Our first step in creating these distributions is a notion of discrepancy, which is a measure of pseudo-randomness for any finite set of integers with respect to any given modulus, R. The exact definition of discrepancy is given by this formula, but you do not need to remember it for this talk. It's basically the maximum correlation of the characteristic vector of your set with a character of the discrete Fourier transform on Z sub R. We have capital N plus one outcomes corresponding to the zero vector and the n coordinate vectors. For this reason, we're going to set the modulus to be. Reason we're going to set the modulus to be R. Thankfully, constructions for constructions of sparse sets with low discrepancy are known, and we're going to use one such construction. I mean, pictorially, if you wanted to imagine how discrepancy works, it has nothing to do with communication complexity discrepancy, by the way. So, pictorially, if this is the ring of This is the ring of integers modulo R, and these are the elements of your set. Then, you know, if you plot a character of the discrete Fourier transform, it's real part, then it'll oscillate between minus one and one. So you see that sometimes it'll hit an element of your set with a plus one, sometimes with a minus one, sometimes with something in between. Minus one, sometimes with something in between. You hope, if your set is aperiodic enough, you hope that any given character will have small expectation on your set. And if that happens for every character, for this character and for that character, no matter which character you look at, then you say that the set is aperiodic or has small discrepancy. We use a We use a construction of a low discrepancy integer set in order to create a balanced coloring. Now, a balanced coloring to us is a coloring of all K subsets of 1, 2 through M where we color each such set with one of the colors: 0, 1. One of the colors, 0, 1, 2, through capital N. And by balanced, I mean that given any large enough set A, the fraction of K subsets of A that are colored any given color should be approximately one over capital N plus one. As our next step, we recall that the OR function on m bits. OR function on m bits has approximate degree root m. By linear programming duality, this implies that there exists a certain dual object on the m cube, which is a real function that has a large fraction of its L1 weight on the point zero and is orthogonal to all polynomials of degree at most root m. If you visually root m. If you visualize the m cube in this way, then phi corresponds to this shading, where darker shades correspond to areas where the L1 norm of phi is concentrated. All right. Now let's scale phi down relative to the containing hypercube. The containing hypercube, and then we're going to be able to move it around freely. If we pick a point z in this hypercube and shift phi to that point, we have a new dual object, call it phi sub z. And this new object is again going to be orthogonal to polynomials of degree at most root m, because polynomials work well with shifts. But this time, the L1 norm will be a But this time the L1 norm will be, of course, concentrated at the point Z as opposed to the origin. Now we're down to the last few lines of the construction. We're going to define capital phi sub i to be the average of the phi z's as z ranges over all characteristic vectors of k subsets colored with the ith color. Colored with the ith color. All right. With this definition, every point of Hamming weight K belongs to the support of exactly one capital phi I. All right, so the phi i's are concentrated on the kth level, but it's a fairly mild form of concentration. You have a constant fraction of your weight. Constant fraction of your weight above the kth level. And this is when a really unlikely and magical thing happens. It turns out that for inputs of Hamming weight greater than k, the capital phi i's are almost identical due to the use of the balanced coloring. That's because if you take a large enough set, If you take a large enough set, then the proportion of K subsets that it contains are going to be the same for basically all colors. That's the general idea. You can think of the capital Phi I's now as probability distributions because you can subtract out this common part. So we are going to define. So, we are going to define phi to be this part common to all phi i's. Specifically, we define phi to be the minimum of the capital phi i's multiplied by the characteristic function of strings of Heming weight greater than k. That's it. So now we can subtract it out of each. Of each phi and normalize. And then you have four properties to verify, which I won't do in this talk, but you have non-negativity. You have concentration on inputs of weight K. In fact, the concentration is extremely strong. And you have disjointness by design, and you have indistinguishable. And you have indistinguishability. Okay, this completes the proof sketch of our main result. And let me end with four problems that I think are particularly natural. Some of them have to do with approximation. Some of them have to do with communication. A natural question that many authors have raised is whether Raised is whether this exponent one minus delta, which can be made arbitrarily close to one, can actually be made exactly one. All right, so does AC0 have approximate degree exactly n? And this seems to be a hard problem. If you are content with approximate degree n to the one minus delta for arbitrarily small delta, then this work gives the final word. Gives the final word. We can even achieve it with a DNF formula of constant width. But if you want to get to N literally, then that will require a complete rethinking of basically, I mean, everything we know about approximating AC0 functions. Problem two is something that many people. Many people in quantum query complexity are interested in to determine the approximate degree of k-element distinctness, which is a specific CNF formula, and likewise for triangle finding. Problem four is one of the problems that I'm the most passionate about. In this talk, we saw that DNF and CNF formulas are very complicated. Are very complicated. They're hard for communication. Problem four asks you to prove that they're actually not too hard for communication. Sigma two and pi two are very natural analogs of DNF and CNF formulas for circuits where you replace the input bits with rank one matrices. So it's an incredibly natural model. It's a generalization of P versus N P. It corresponds to the polynomial hierarchy level two in computational complexity. And it's, like I said, an absolutely fascinating problem where you see clearly that an object as simple as a DNF formula, where we should basically understand everything about it, we don't understand anything. It. We don't understand anything about it. So there are lots of open problems, even about DNF and CNF formulas, that are quite hard. Okay, let me end here. I'm out of time, big time. Thank you. Thank you. Yeah, I'll ask a question. Do you think there could be a CNF or DNF formula of even fixed psi, like n cubed or something, that which achieved across maybe a linear n? You know, that's a fantastic question. A fantastic question. Honestly, I have no answer. You know, some days I feel one way, some other days I feel differently. I think this question now, I mean, so many things that are so counterintuitive have happened that at this point, not even the experts know what to conjecture. I really don't know. It could be that there is, let's say, It could be that there is, let's say, a DNF formula with approximate degree, you know, theta n. Or it could be that, you know, that's not the case. I hope that there is a CNF or DNF formula with that approximate degree. But I mean, you know, the thing is, even to construct, even to achieve approximate degree n to the one minus delta, if you try. M to the one minus delta. If you try to do it with a DNF or CNF formula, it's very counterintuitive. It's just not clear how to do it. If you pick the terms at random, then the function will very quickly approach one, the constant one function, and you'll be able to approximate it very efficiently. If you pick terms differently, then you run into other issues. It basically took Basically, it took, I don't know, 10 years to figure out the right way to pick terms for this DMF formula. And to answer this other question for a linear degree, I have no clue. I don't even know if it's true. Can I ask a clarification question? Yeah. So, in your construction, so do you still use the gadgets of? Still use the gadget repeatedly to amplify the yeah, yeah, yeah, yeah, yeah, yeah. We uh we amplify it, except that so what distinguishes our work from the previous work by Bonn and Thaylor is we completely redesign hardness amplification. We make it possible to amplify hardness with a DNF of constant width. DNF of constant width, a monotone DNF of constant width. Whereas in previous work, you needed to have a depth three circuit with a large fan ends. Basically, we rethink hardness amplification from scratch. So you're using the fact that when you compose monotone DNFs, you again get monotone DNFs. Yeah. Now, is your out. Yeah. Now, is your auto function original function F? Is it also a monotone DNF? Yeah, it's a monotone DNF. So everything is a monotone DNF. Everything is a monotone DNF. If you like, you can negate it. And then you can negate the inputs as well. You get a monotone CNF. So everything here is monotone. Everything. Amazing. Thanks. Thank you very much. Thank you. Except that team could be back by two for the group engagement right outside. That would be great. 