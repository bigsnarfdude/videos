Thank you. Thank you. First of all, I would like to thank the organizers for the opportunity to make a talk in this conference. And I will speak about the new bounds in the problem of self-discretization. So, first of all, I recall what is the problem. Problem, assume we have two constants, capital C and small c, and we have a finite-dimensional subspace of continuous function on some domain omega endowed with some probability measure mu and this subspace in LP. So, here is LP norm. We can assume that Omega is compact. That OMID is compact, so this P only to stress that we are interested in discretization of this LP norm. So what is sampling discretization? In this problem, we seek to find the least possible number of points in such a way that we can replace our initial measure with the discrete measure. Discrete measure, uniformly distributed discrete measure on this set of points in such a way that we can bound, that LP norms are almost equal on our subspace L. Obviously, the number of points cannot be less than the dimension, and capital N is the dimension of our subspace. And the question is under which. The question is under which conditions we can take M close to the dimension. So there are two additional settings for the problem of sampling discretization. Firstly, we may be interested not only in the uniform discrete measure, but some discrete measure. But some discrete measure. We call this the simple discretization problem with weights, so weight discretization problem. So in this problem, we seek to find points and weights in such a way that we can replace the LP norm with the discrete one. And another setting is when we are interested in almost isometrical replacement. Almost isometrical replacement of the measure. So we are interested when this small C is one minus epsilon and capital C is one plus epsilon. So they will be almost asymmetrical replacement of Philpin on our subspace A one could choose a probabilistic approach to this problem. So in this probabilistic approach, we choose this probabilistic approach we choose points x1 xm randomly and independently with the distribution mu and we can define the error by the error error of discretization so for some set b in our subspace l the error is just this supremum and when we take b the unit ball lp ball in our The unit ball LP ball in our subspace, and when this error, when B is L P ball, is less than epsilon, this then this is essentially the discretization with epsilon. And when we choose points randomly, we may be interested when this error is small with high probability. This problem is connected. Is connected to the problem of approximation of moments of random vectors. In this problem, we have some random vector, n-dimensional random vector, and we have a set k in a rn. And the question is this: how many independent copies of our vector do we need to make this? This error is small with high probability. And actually, this problem studied by many authors. Here is only some of them. It is also studied by other people. And these two problems are essentially equivalent. The problem of sampling discretization, when we use a probabilistic approach. We use a probabilistic approach and this problem of concerning moments of random vectors. So on the one hand, if we are given with a set K in a random, then we can take set B to be the set of all linear functions enumerated by this set K, and we can take measure mu to be the distribution. Measure Î¼ to be the distribution of random vector u. Then the expectation of error in the sampling discretization problem and the error in the problem concerning moments of random vectors are equal. On the other hand, when we are given a set B in some subspace of L P, then we can take the basis, any basis. The basis, any basis of this subspace, and we can consider these independent copies of one random vector. If we take set K to be the set of all coefficients such that these linear combinations belong to the set B, then again the expectations are equal. So we can use results concerning the problem of moments of random vectors. Of moments of random vectors to obtain results in sampling discretization. For example, we can reformulate the old result of Rodelson, which reformulated actually states the following. This expectation of the error of sampling discretization is bounded by this quantity where A is this number. So when we have control. So, when we have control of this expression in the expectation, we can say something about the number of points m sufficient for sampling discretization of L2 norm. And this is a precise statement. Assume that the supremum of our functions, of each of our function, can be bounded by. Of our function can be bounded by same, some constant multiplied by the square root of the dimension and the L2 norm of our function for each function in our set, our subspace A. Then, according to this bound, we can take M points when M is proportional to N log N, capital N is the dimension of our subspace, such that we have good discretization. We have good discretization of L2 norm. And these points, x1, x, m, can be, we can choose them randomly. So random points with high probability give us such discretization result. It is just a reformulation of Robelson theorem. We note that in this theorem, we actually, the crucial condition Actually, the crucial condition was actually this one. We call this condition equality-type inequality conditional assumption. We say that subspace L satisfies infinity Q, equality type inequality assumption with constant M. If we can bound the L infinity norm by this quantity for each function in our subspace. And we will. Um, and uh, we will mainly consider the case when q is two in this talk. We mainly uh consider this case when q is two. And for q equal to, we point out that this condition can be reformulated in the following manner. So, subspace L satisfies infinity to inequality type inequality condition if for some orthonormal basis we have such point one. We have such point-wise bound. And actually, for every orthonormal basis, this bound is valid. So now we move on to the case when P is between one and two. And again, we discuss now the probabilistic approach. Similarly to the Roderickson result, one can. The Rodebuson result: one can obtain the bound for the expectation of our error when p is between one and two, excluding one. So this is the bound, and this is the number a. So again, yeah, we assume that L satisfies infinite two inequality type inequality assumption. So under this assumption, So, under this assumption, we have this bound, and looking at this bound, we understand that when m is proportional to n log n and l satisfies the infinite equality type and equality assumption, then m random points give us this discretization result. So, here is n log n squared, and the proof is actually based on Lagrange genetic. Based on Lagrange's generic chaining technique to bound this expectation of the supremum. What is known for the case when P is one? When P is one, and actually, for each P smaller than two, P between one and two includes. Uh, p in between one and two, including one, we have uh the result of Feng Dai, uh, Andrei Primak, Alexei Shadrin, Vladimir Nitunikov, and Sergei Chikhunov. So if we again assume that L satisfies infinity to Nikolai type inequality assumption, then if we take m points with n proportional to n log n to the power three, then we again have discretization result. Again, have discretization result. And this is valid also for the case when P is one. The proof do not status, but the result actually, as I understand, also probabilistic. So here M points with such M run M random points with high probability gives us this discretization result. This is This was known. And this was a probabilistic approach to the sampling discretization. What also can be used? There is actually a connection with another old problem, the problem of embedding of finite dimensional subspaces of LP into small LP. Small Lp. So the problem is this one. Assume we are given some n-dimensional subspace of LP, for example, of LP on the interval 0, 1. And the question is, which is the smallest possible dimension of small LP, discrete LP, such that we can find subspace L prime in this discrete LP, such that L and L prime are almost as And L prime are almost asymmetrical. So the Banachmaster distance is smaller than one plus epsilon, where the Banachmaster distance is this infimum, where T is a linear non-degenerate transformation. The connection was brought to our attention firstly by Baris Kashnin, then by Guidon Schechtman. What is known for this problem? This is the known results. So for p bigger than 2, one needs a polynomial number of points. And here is more interesting for our purposes when p is between 1 and 2. between one and two. Excluding one, one need n log n log log n squared number of points. And when p is one, then n log n points is sufficient. N log n dimension n log n is sufficient. In the problem of embedding of fine-dimensional subspaces. So the last two results, both are due to Lagrange. And again, this problem. And again, this problem has been extensively studied. And Guidon Schechtman actually told us that this problem, the proof of this problem, actually gives assembling discretization with weights. So the proof itself, the problem. The problem, uh, in the problem there is no uh statement about the discretization, but the proof gives the discretization with weights. And our question is, uh, was, uh, already was, could we make results for discretization with equal weights under the unequality type assumption? Uh, could we prove uh Could we prove such bounds for this type of discretization under the inequality type assumption with equal weights? And actually, the answer is yes. And this is the main topic of my talk. But firstly, I would like to speak about the scheme of the proof. Actually, this is the scheme of Lagrange's proof. At the first step, This is the scheme of Lagrange's proof of these two bounds. At the first step, we actually have a sampling procedure. We embed our subspace L into some subspace L0 of some discrete LP with big enough M0. In such Role in such a manner. So already this is the discretization. We see here the sampling discretization. The second step, the second step, Lagrange proves that under some assumption on the space L0, you could split the set of indices in such a way that on each for each For each subset of these indices, the LPN norm on our subspace L0 is essentially a half of the initial norm. So this bound is valid. And then you could take the set with the number of points which is with the cardinality less than M0. Less than M0, half of the M0, and you could iterate this procedure. And this tradition gives provides this bound for the embedding problem. We wish to make something similar, but for sampling discretization problem under the inequality type assumption. Firstly, I will discuss the case when P is 2 and then when P is less than 2. So when P is 2, here is the result of Limonovichymlikov. There are three constants such that if the subspace satisfies infinitune equality type and equality assumption, then M points with M proportion. M points with M proportional to N are sufficient for this discretization. But I point out that the constant from the inequality to inequality is also here. So this is good discretization when M is one. And actually it was noted by me that the approved some modifications of the approved Some modifications of their proof gives the result when C2 is 1 minus epsilon, C3 is 1 plus epsilon, and in that case, C1 will be some constant multiplied by epsilon to the power minus 2. But I point out that this is not a discretization with epsilon. This will be the discretization with epsilon only when capital M is one. But as an example, capital M is one. As an example, capital M is one when we deal with trigonometric polynomials. So a corollary is this. Assume we have some set of frequencies and we consider the space of all trigonometric polynomials with frequencies from this set. It is multi-dimensional trigonometric polynomials. trigonometric polynomials. Then we can discretize L2 norm with epsilon with the number of points proportional to the continuality of the set of frequencies. Okay, and how one can obtain this type of result? The proof is based on the powerful Is based on the powerful result of Markus Pilman Sivastava, which I state here. But actually, it is more convenient to reformulate this result, reformulate in such a way that it will be convenient for our purposes of sampling discretization. And here is the reformulation. So assume we deal with some discrete set of points and we consider the usual L2 norm with the counting measure on this space. If n-dimensional subspace of functions defined on this space set X satisfies this assumption, then the set of indices can be splitted into Set of indices can be splitted into two subsets such that on both subsets the L2 norm is essentially a half of the initial L2 norm. And we should think that theta is small because this condition, what this condition means? This condition means that each point, each coordinate, each point have small impact. Each point have small impact in the whole norm when teta is small. And then we could split the atoms in two subsets such that on these subsets we have almost half of the norm. This is essentially a result of Marcus Pierlo-Silvastava. Then we can use this result for discussion. Then we can use this result for discretization purposes. So we assume that initially we have such also an equality type assumption. We control the maximum of our functions by the L2 norm. And assume we find the first subset of our initial set, then that we can replace the initial reutonroom. The initial L to norm with our new L2 norm with the constants A and B, and this bound is valid for every function f in our subspace. Then Markus Pilmo-Srivastov result actually states that we can find another subset Z in our subset Y such that we again can control. We again can control L2 norm on this subset Z by such inequalities. So essentially, this bound, lower bound, provides us with a unique type inequality when theta is replaced by theta divided by square root of A and L to norm on the set X. L2 norm on the set X is replaced by L2 norm on the set Y. And then we apply Markus Pilmo-Srivastava theorem. Theta is replaced by theta divided by square root of A, X replaced by Y. And we obtain such bound. And then we iterate this procedure. I also should comment that we take the two subs. There are two subsets, and we take one with the cardinality which is less than half of the initial cardinality of the set Y. And then we trade this procedure and we iterate it while essentially while this expression in brackets is positive. And if we want discretization with epsilon, we iterate it while this quantity is small enough. This quantity is small enough. So, the initial path is small, and then we iterate while this quantity is also small. And this gives the Limono-Timikov result and actually its reformulation of such type. So, we see that the procedure is essentially the same that it was the The same that it was in Telegram's approach to the embedding problem. Let us now move on to the case when P is between 1 and 2. And these are new results due to Feng Di, myself, and Vladimir Timrikov. It was recently put on archive. So, So, the first result is this for P when P is one. If L satisfies infinity two inequality type inequality assumption, then we can choose M points with M proportional to N log N in such a way that we have simultaneous discretization of both L1 norm and L2 norm. But essentially, this improves the result for L1 norm. So, the previous result was And the previous result was n log n to the power 3. And this result is likened to Lagrange theorem for the embedding problem n log n. But also we have a simultaneous discretization, which also an interesting effect. This was for the case when P is 1 and the next theorem is when P is between 1 and 2. Again, Again, if L satisfies infinity to inequality type and equality assumption, then we can choose m points with n proportional to n log n log n log log n squared, such that we have simultaneous discretization of L P norm and L2 norm. But I point out that these points are not random, this is only the existence result. The previous result was results were for random points. How to approach this type of results, how to prove them. And here I present some sketch, some ideas of the proof. Essentially, there repeats the steps from L2. From L2 case and from the Lagrange approach for the embedding problem. But since we wish the result with equal weights under the Nikolai type inequality assumption, we need somehow to control the L2 norm. This Nikolai type assumption, we need control over it. So we need to discretize not only L1 norm or L2 norm. Norm or L P norm in this theorem, but we also need to control L2 norm. However, since both results for L1 due to Dipri Mark Shadr, Timukov-Tihunov, and for L2 norm due to Rodilitson, are probabilistic random points gives good bounds for expectations with hyper for expectations or give good discretization with hyper. Or give good discretization with high probability. We have this bound with probability greater than one half, and also this bound with the probability greater than one half. Thus, we have a unit set of points which provides both discretization simultaneously with some big M0, big n log n to some power. However, this However, this bound for L2 norm gives us the control over inequality type inequality. So now the maximum of each function is bound by this constant multiplied by discrete Telton norm. And this quantity is small. Now, what we do. We now in discrete situation. This is a discrete ball of our subspace and this is the discrete norm. We are in a situation of inequality type inequality. And what we need? We need a good splitting. Moreover, we need this. Moreover, we need this splitting to be simultaneous for L1 and L2 more. So, what is the idea? This splitting inequality can be formulated in such a way that there is a choice of signs plus one, minus one, such that this supremum is bounded by something dependent. By something dependent on this parameter, theta. And we want this to be small. And we could choose these signs randomly. So each sign independent, we choose them independently and plus one with probability one half and minus one with probability one half. So Bernoulli random variables. And for such type of such type of Type of such type of problem, such type of problem, the expectation of the supremum. There are known bounds. For L2 case, there is a bound of Rodelson. It is essentially the same result that I have already mentioned. And for L1 case, this is some reformulation of Toulagrand's result from. Result from the proof of his result for the embedding problem. So essentially, this is Lagrange's theorem. In Bose's theorem, if we have such bound for the maximum by L2 norm, then we can bound the expectation of the supremum by some constant multiplied by this number theta and multiplied by a square root of. Multiplied by a square root of log n. Since both results are probabilistic, we again can choose signs simultaneous for both norms, FOIL1 norm and 42 norm. And get this splitting, this good splitting. Splitting, this is good splitting simultaneously, FOIL one norm and FOIL two norm. So now we start to make iterations. We are in a situation when we have such a bound on our initial space. Here is it. Initially, we have such bound. Such bound. Assume we somehow found a subset Y of our initial set X such that we have simultaneous control of L1 norm and L2 norm. Then the combination of these two theorems provide us with a subset. Provide us with a subset Z in our subset Y, which is firstly have the cardinality less than the half of the cardinality of the set Y and such that we have control of both L1 norm and L2 norm. And again, we start to iterate this. To iterate this while this quantity in brackets are positive or if we wish result with epsilon, while this quantity is small enough. And these iterations give us good discretization result like this serum for a one case and this serum for a two case. So, this is the main idea. However, the proof is more technically complicated, but the idea is this. For the case when P is between one and two and not one, we again use some reformulation. Of it's not a reformulation, but some analog of T Lagran's result which he has used in the problem of embedding for this case. So, this is the exact result. If we have such inequality, we control the maximum with this constant multiplied by L2 norm, discrete L2 norm. Discrete electron, then we can bound the expectation of the supremum by this quantity. And here is essential that m, our initial m is something like n log n, log n to some power. This actually gives us n log n, log log n squared result. So, here are some main ideas. I point out that the initial, the crucial step was the splitting procedure. So, let us describe what is this procedure. Assume we have a fixed point set and some n-dimensional subspace of functions defined on this set. And let And let epsilon of L be the minimum possible number epsilon such that there is a splitting into two disjoint sets of this set of indices such that each norm on each subset is proportional to one half plus epsilon of the whole norm. Of the whole norm. And the question is when this epsilon of L is small. So this is exactly the crucial part of the proof. The natural assumption for epsilon of L to be small is the assumption that the impact of each point, the impact of which coordinate on our subspace. Coordinate on our subspace is small. And we could compare this with the initial LP norm, or we could compare it with some other LQ norm. So assume we have such bound for every F in our subspace. And let sigma PQ be such a number. This is the definition. Essentially, Definition, essentially, this is what we could get under this assumption for each n-dimensional subspace, satisfying this assumption. And we are interested when this quantity is small. The results of Marcus Pilmon, Sevastovetto, and Lagrange could be reformulated in terms of this quantity. So, So, Markus Pilmo-Srivastava result states that sigma is bounded linearly, the bound depends linearly on the parameter theta, but this bound is independent of the dimension capital N and of the dimension small m. So, capital N is the dimension of subspace and small m is the dimension of the subspace. space and small m is a dimension of small lp in which we are acting. Lagrange result states this bound that sigma one two so we are interesting in splitting of L1 norm and the condition is described in the in terms of L two norm. Then we have such bound. This bound also linear in tet but depends on the dimension capital N. The dimension capital N, but again it is small. But there is a negative result, which I also would like to mention. The result is due to Bariskashan and Iru Limonova. If we are interested in splitting of L1 norm, but this condition also expressed in L1 norm, then we have Then we have a lower bound. So small m is to n and n is some number that is between 1 over square root of n and 1. Then we have this lower bound. And I point out that in some regimes, this is not small. For example, when dimension is big, but tet is something like one divided by Divided by n to the power one over, this quantity is not tending to zero. This is a big quantity. So this is actually negative result. Then that there is no such Marcus-Pilon-Sevastaval bound when we consider sigma 1, 1. And I have for a minute. I have five minutes. I would like. I spoke mainly about the case when P is between one and two, but what is known in the case when P is bigger than two? In this case, we have these two results. When P is between two and three, actually, this result is valid for every P bigger than two. Bigger than two. And L satisfies infinity P. I point out this infinity P inequality type inequality assumption. This assumption is more restrictive than infinity tool assumption. Then we can choose M points with N proportional to N log N to the power P, and these chosen points will be sufficient for good discretization. This is a probabilistic result. X1 XM could be chosen randomly. And for P bigger than 3, there is an improvement of this result. Fengda and Vladimir Chimrikov proved that when P is bigger than 3, one can take, and again, L satisfies infinity P, equality type and equality assumption. Assumption. Then one can take m points with n proportional to n log n to the power 3 and get this good sampling discretization result. But as I understand, this is not probabilistic result. Maybe Feng could tell if I'm mistaken or not. But as I understand, this is not a probabilistic result. probabilistic result. So this is known for P bigger than two. And maybe here is some papers that I have cited throughout my talk. And I think that's all. Thank you for the attention. If you have any questions, I'll be glad to answer them. So thank you very much. Thank you, Jagor, for your very interesting talk. Are there any questions? I have a remark for which may some chance of improving something, a small chance, but maybe I shouldn't say it. Thank you for your talk. Can you go back to the slide? Can you go back to the slide where you had the theorem of Woodlson and the theorem of Talagrand? Ah, yeah. Okay, so let's look at the theorem of Talagrand. So the way he proves this is this is called the supremum of a Bernoulli process. And the way you prove it is by majorize it by the corresponding Gaussian process. Corresponding Gaussian process and then evaluate it. So we replace the epsilon j's by Gaussian independent Gaussian variables. However, there is a more recent result, which was not available at 90, in which there is a better estimate. This is there was some conjecture of Talagrand again called the Bernoulli conjecture, which was solved some five, seven years ago. Some five, seven years ago by Latala, and I don't remember somebody else, I don't remember the name, in which she gives a better bound on supremum of a Bernoulli process. It's a combination of a Gaussian process and something else. So maybe there is a small chance. I don't really believe in it, but one is to check whether maybe one can use this to improve some of the results. Of some of the results. So you should look for the solution of the Bernoulli conjecture. This was a famous conjecture at the time. Talagrande even offered money for it and paid it and after the solution. Okay, and thank you for. Thank you. Thank you very much for your comment. Are there further questions? So maybe I have one. Maybe I have one. In this result for p between one and two, you always assume the infinity two Nikolsky inequality, right? Yes. What happens if you assume infinity P Nikolsky inequality? I don't know. Like in the P larger than two cases. This is the assumption there, right? Yes. But in both cases, the assumption is more restrictive. So when P is less than one. P is less than one, then you call infinitude Nikolsky assumption is more restrictive than that. Yeah, okay, I see, I see. So you have a weaker assumption there if you have. Yes, this is your assumption is weaker. So I don't know. This is a very interesting question, and I don't know the answer to this question. So it might be if you pose a more restrictive or a stronger assumption that you get something better or no, no, no. Or no, no, no, no, we pose more restrictive assumptions. I yeah, your assumption is okay. So the P Nikolsky inequality for P between one and two is less restrictive. Yes, this is less restrictive than the Phoenix two. Yeah, okay. And but your assumption is actually more practical. So because if I understood correctly, if I give you a set of frequencies and you consider trigonometric polynomials of this fixed frequency set. Fixed frequency set, then you can discretize them according to your results here with P between one and two. Yes, yes. So the condition for trigonometric system is valid with M. Did you think about implications of this for the sampling recovery problem? I think there are some results of Vladimir Timlikov about the recovery problem, but I mean specifically. Oh, I mean what I mean specifically is not the L2 sampling recovery problem. This is what what you probably mean. So you study, you have a function class and you want to reconstruct the function in some error norm. And here the question is about the sampling recovery problem in LP, not in L2. Maybe this can be used somehow, but it would be interesting because this is completely open. Interesting because this is completely open, so so far we only noticed recovery problem in L2. I see, yes, this is also a very interesting question. I don't know. Maybe are some results of Vladimir Chemnikov. I don't remember. I don't remember. Are they only foil 2 or was there some foil P? I don't remember. Maybe it is better to ask here. Yeah, okay. I can do that. Okay. Are there further questions? I can do that. Okay. Are there further questions maybe in the Zoom audience? Does not seem to be the case. So thanks again, Igor, for your very good talk. And yeah, we should start with