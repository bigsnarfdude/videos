Seb Ubek and Yeshwan Cherukanjuri, who's a PhD student at Berkeley in Computer Science. So it's about this phenomenon of adversarial examples in deep networks that the practitioners have observed you can take a trained deep network that performs well, let's say at classifying images, provide it with an image it correctly classifies, and by looking at just so you can very easily So, you can very easily find a tiny perturbation of that image that leads the network to incorrectly classify the panda as a given. And you can find that small perturbation just by looking at the gradient of the output of the network with respect to the input. Take a very small step in that direction. So, it's easy to find these adversarial perturbations and kind of strange that these. Of strange that these methods that work so effectively are so brittle in this sense. You mightn't care about pandas versus gibbons, but you know, there are more high-stakes applications where this sort of technology is used, and it's still easy to find adversarial examples. Okay, so the one slight summary of the talk is that when we look at constant depth RELU networks with random weights, you know, there's a kind of very simple explanation in that setting of Simple explanation in that setting of this phenomenon arising. So, you know, if you have a parametrized network, the theta of the parameters, x is our vector, our input vector, you know, think of that as the image. It's in Rd, so the setting here is d is something big. Then, for any input x and most parameter values, we're looking at a Gaussian distribution for the parameters of these networks. Works. There's a step that you can take in the gradient direction so that when you fix this input x, take a step in the gradient direction or span of the gradient direction, and you can flip the sign of the output of the network. And the size of that step is tiny compared to the size of the original input. It's like 1 over square root of the input dimension. And the reason that this occurs. The reason that this occurs in the setting of relative networks is because these networks are computing functions that are actually very close to linear in a large neighborhood of that fixed input point x. So it's kind of high-dimensional phenomenon of random linear functions. So the linear case is easy to understand. We'll talk a little bit about that. That intuition, okay, I'll say a bit about prior work in this direction, then look at the linear case. Then look at the linear case and then why random relevant networks give this sort of essentially linear property over quite a large, I guess surprisingly large region of input space. It's a result about bounded depth RELU networks, so we treat depth as a constant, and I'll say a little bit about why that is necessary for the particular model that we consider. Okay, I have already a question. I have already a question. Yeah. That's right. What is the quantification of X there? I mean, there exists an X. Yeah, so for any X, the high probability statement holds. So I'll be precise in maybe one slide. Yeah, happy to wait? Good. It's nothing, it's nothing. Good, good. Okay, I can't see. Shy, so. Alright, so just first to introduce, you know, be precise about the results. So we're talking about these RELU networks. They're rectified, RELU steps are rectified linear units. It's this nonlinearity that's popular. We've got a parametrized class. I'm dropping the parameters here. We're always understanding that there are these adjustable parameters, these weight matrices. weight matrices W1 through WL plus 1. And you compute that parametrized function by taking the vector x, then this linear transformation parametrized by w1, you take that vector and put it through a nonlinearity, take another linear function, and so on. You know, compositions of these linear functions, then a nonlinearity. The nonlinearity is sort of a vector, a scalar nonlinearity applied to components of the vector. Applied to components of the vector. And the particular one we're considering is this RELU, so piecewise linear function here. We're considering a case where these parameters are chosen randomly, they have a Gaussian distribution, all of the entries in these matrices are independent. This is a kind of standard initialization for these networks, but of course, For these networks, but of course, people use typically some sort of gradient method to adjust the parameters. So you could think of it as a result at initialization, this random initialization. The scaling doesn't really matter. We have independent of the entries, which is crucial, but the scaling isn't so important. We're assuming that the entries of the WI or some other notation that I'm going to use throughout, I should have pointed out. That I'm going to use throughout. I should have pointed out: D is the input dimension, D1 is the width of the first layer all the way through to, you know, DL is the width of this LF layer, and then we have a single scalar output. So we're using these DI's throughout and D's input. So the variance of the entries of the parameter matrix of the ith layer is 1 over the width of the i minus 1th layer. And this is just a sort of convention actually to keep all. Actually, to keep all of the signals throughout all kind of a constant scale. So you can kind of wrap your head around the size of things in this network, but it really doesn't matter because the RELU has this positive homogeneity property. You can take all of the scaling and apply it at the end or wherever you like. Okay, so the main result here, so two other pieces of notation: d min is the min of all of the. Of notation, d min is the min of all of these widths throughout the network and the input dimension, dmax is the max of those. We're fixing a depth. I'm saying constants here, they depend on L. They actually, I've called them constants, they actually depend polylogarithmically on the d min and dmax. I'll tell you a bit more about that later. The widths are just kind of anything reasonable, right? You can't have the You can't have the sizes, the widths in these different layers being too ridiculously large or small compared to each other. For instance, polynomials are bounded above and below by polynomials in the widths is fine. Then, for any fixed input, non-zero input vector x with high probability, you can take this, so the high probability here is over the parameters of this function f. Over the parameters of this function f, you can take f at x and perturb it in the direction of that gradient, or the negative direction of the gradient, and flip the sign. So get a, think of two class classification problem, flip the sign, you get the exact opposite of the previous prediction, and the scale of that perturbation is like one over square root d, as big as the scale of the original input vector. Is the scale of the original input vector? Okay. So that's the result. There's been a lot of interest in this phenomenon. There was a nice observation by Ali Shamir and collaborators that you should expect this sort of thing being able to find some nearby point nearby in Hemming distance, so you don't have to change too many coordinates in a multi-class classification setting. You know, multi-class classification setting, not too many coordinates to lead to this behavior. But you know, of course, it may be that you need to change those coordinates by a very large amount, so it's a bit of an unnatural notion of a small perturbation. Danielle and Chuckham had a result where the size of these perturbations was measured in the Euclidean norm, so much more natural, but their requirement. Their requirement, and actually, you know, this is based on the linear intuition that I'm about to present, but the requirement here was that the widths of the different layers were very rapidly decreasing. You know, each layer had to be tiny in width compared to the previous one. And here, when you say most value, you mean in terms of the randomization? Yeah, most here. That's right. Most here again is put a distribution over the parameters, spherically symmetric distribution over the parameter. The parameters, spherically symmetric distribution over the parameters, and with high probability under that choice. So, again, you can think of it as an initialization, a random initialization of the parameters in the network. Okay, and Bubank, Cheripan, Jerry, Gedell, and Decombe had a result that removed this restriction for two-layer networks. And I'll tell you about the deep case. Okay, so let's talk about the linear case, which I think is very informative. So imagine we've got just a linear function. We choose the parameters according to a standard Gaussian, right, and we're considering a fixed vector x in this input space. Of course, the gradient in this case is just that parameter theta. The scale of the gradient is tightly concentrated. The norm of the gradient. Concentrated, you know, the norm of the gradient is tightly concentrated around root D. So this is, think of that as a large-scale thing. The size of the value of the function, well, the function's value, of course, is also Gaussian. The variance is the norm of x squared. So typical scale here is norm of x, and we would certainly not expect f of x to be too much bigger than the norm of x. Too much bigger than the norm of x. And so, you know, we have the norm of the gradient times the norm of x way bigger than the scale of the output. So, you know, our typical output is small compared to the product of these two, and so it means, you know, we can, in particular, by aligning with this direction, we can get a very big change in the output of our random linear function. Okay, so perturbing x in the direction of the gradient, you know, with an appropriate Direction of the gradient, you know, with an appropriate sine flip, is going to move, you know, we'll get to change the sign of our linear function. Okay, you know, very simple phenomenon that's just relying on the fact that we're in high dimensions here and have this verically symmetric theta. So we can pick the scale of that perturbation to the scale of the output divided by scale of the gradient and flip the sign. And of course, the scale of that thing is really small. Is really small, you know, plugging these values, right? The scale of that thing is like 1 over root d times the scale of the x. So it's not at all a surprising thing in this case. And that's really what's going on in the Rayleigh network with random weights, because in that case, f is the functions that we see are nearly linear in a sense. I'll make precise in a moment. Okay, so I said, okay, so we're thinking about quantum. Okay, so we're thinking about constant depth. I said we had constants here, I kind of lied. They're polylogarithmic in the widths, and you know, the dependence on depth is kind of terrible. Typically, we're seeing things like this. They're sort of exponential in the depth. Okay, so just to fix some things, we're always going to think about the norm of our fixed x as being root d, and as I say, we fix the scaling so that So we fix the scaling so that all of the signals throughout are kind of a constant level. Right, and then there are kind of three steps of the proof, just like the linear case, you know, the output can't be too big, the gradients are nice and big, you know, can't be too small. And the interesting part is showing that the function that you get is very close to linear. So for all y in a neighborhood of this fixed point x, and the neighborhood is sort of order of the square root of the minimum width. Sort of order of the square root of the minimum width of the network. You have the change of gradient when you move from x to y is small compared to the gradient at x. No more than this sort of one over polylogarithmic thing there. And of course, okay, so one thing is we actually only end up needing this thing to be sort of a constant or polylogarithmic. Sort of a constant or polylogarithmic, but we can go right out to square root d min and we still have this linear behavior, which is kind of surprising. You know, this is a pretty big range where things are close to linear. Okay, and then, you know, why is this useful? Well, it really is a near linearity property, right? So if you consider a function in a neighborhood of x, so we're moving up to u away from x and look. Up to u away from x, and look at how that compares to the affine approximation of that function. It's easy to, just by thinking of it as an integral of a gradient, right, to show that you don't, that this approximation is good within, you know, the scale of how far we're moving and the worst case difference across that scale, worst case difference of these variables. Okay, and so you know. Okay, and so you can think of that as a notion of smoothness. Of course, these are not smooth. You have discontinuities in the gradient. What matters is that we're having small changes in the gradient on the scales that we're interested in moving around at x versus y. Okay, so you know, we're certainly not in the, these things are computing piecewise linear functions, we're certainly not in any. Linear functions, we're certainly not in any fixed piece when moving all the way across those pieces. Okay, so the first two steps here are straightforward. You're kind of keeping control of the scale of the output and the scale of the gradients. Maybe just say a little bit about that. So the relu here has this nice property. You can write it as the indicator of the argument being positive. And so then you can write the network in this sort of a way as. The network in this sort of a way as kind of a linear function of x, where the nonlinearities are just represented by these indicators. So this is a matrix that's got diagonal entries that are ones and zeros. It's a function of x, right? So it's the indicator for that component. I guess I've written this as in a vector kind of way, but it's the particular diagonal entry is just the indicator for that component being positive. Of being positive, you know, so being in the linear part of the relic. So we can write things in this sort of a way and keep track of... Okay, so I'll use this notation in a moment. So it's easy to see the expectations are sort of scaling nicely. So the signals throughout are kind of constants. With the nonlinearities, we're sort of losing half of the signal with this RELU in some sense. That's in expectation. Everything's concentrated. That's in expectation, everything's concentrated. So, you know, it's an easy calculation to see that we're unlikely to have our output too big or our gradient too small. Okay, so that's sort of more bookkeeping, and the interesting part is this smoothness property, this near linearity property. And a crucial step here is splitting up the difference in gradient into Splitting up the difference in gradient into a piece for each layer in the network. You're looking at the difference in the gradient at the point X versus a nearby point Y, and it's kind of a mapping from the input, this you should recognize as the mapping from the input to the jth layer up to here. Remember H, these are the non-linearities. And that's the mapping that you get when you're considering the input y. The input y. Here's the difference of the nonlinearities between x and y, and here's the mapping to the output from the jth layer, and in this case with the input x. It's nice decomposition summed up across the L different layers. And so we need to keep control of kind of the scales of these things and how the nonlinearities shift as we move around. And I'll say a little bit about keeping control of the scaling. So, you know, you can imagine the sort of argument. You want to make things uniform across a ball around the point. And easy to do that. You get something that looks like Fi here is the output, is the vector value you've computed after I layers, right? And how does that? Right, and how does that change when you move from x to y? Well, it's it's sort of Lipschitz in the change from x to y at the input, scaled by some square root of di over d min, or actually the minimum width you've seen up to that point. That's too loose to be useful, turns out. And you just need to be a bit more careful in keeping track of how things go on, and there's this sort of notion of bottlenecks. Notion of bottlenecks. So rather than working with being uniform across the y's here in a ball around x, you need to be uniform across a ball in the image of, uniform across the image of that ball at some appropriate layer, and the appropriate layer is the relevant sort of bottleneck layer, the minimum width layer, up to the point. So if this is a network with this is our input of dimension D, and there are 10 layers with all these different dimensions. There are 10 layers with all these different dimensions. For the last two layers, the bottleneck is the ninth layer. It's the smallest thing, and we should be kind of making things uniform across what happens at that layer. And for the previous layers, you can work with, there are different bottleneck layers that are crucial here in keeping control of these scales. So when you do things in this way, you can get control of things in a way that depends on an appropriate ratio of dimensions that Dimensions that makes everything work out. That's sort of a crucial concept in the proof. Okay, so that's the result and we have this small perturbation of a scale. You only need to move from x on a scale that's sort of 1 over square root d times times the norm of x. Remember the norm of x was this. Was this fixed thing? This is like a constant over root d, but actually, you know, we were able to cope with up to square root d min over d, so we had a lot of flexibility there. The near linear property extended over a much, much wider range. Okay, so this is all treating the depth as a constant, and this thing, you know. Constant and this thing is sort of exponential in the depth. So the result gets weaker really quickly with increasing depth, which is kind of disappointing. But it turns out it's necessary to have some dependence on depth. So here's an example. You can have for kind of constant widths, sufficiently large input dimension. The width is polynomial in the input dimension. Input dimension and not too big. Then for all pairs in the input of a certain norm, the outputs are essentially the same. They're within 1 over square root d of each other. So you're computing effectively a constant. Actually, you've got a very complicated way of computing the norm of the input times a random number. So something to fire up tensor float of you. Okay, and the kind of idea behind this is just that, you know, if you think about what you compute up to layer i with input x, and then consider, you know, an element of the vector at layer i plus 1. You've got this, these things are correlated, right? You know, maybe they're at angle theta i from each other. What happens to that angle? From each other, what happens to that angle at the next layer? You know, at least when you consider the expectation over the random choice of the parameters. So, this is one entry that contributes to the cause of that angle, right? And that entry, you know, you can write down, it's just an easy integral to calculate, you can write down what that looks like as a function of this angle between them, d r i. And so you get this sort of recurrence that you apply at each layer, and the cause of the angle is This is the angle itself is very quickly going to zero. Like one over L. That's for the expected expectation up here, but you can make that a little more precise. Okay, I'm going to skip over this. Basically, I'm just saying in the bound, there's an exponential in L, sort of degradation of that result of the sensitivity. On the other hand, As L grows, you know, I mean polynomially, but as L grows, you've got a function that's really far from linear. It's computing like this cone-shaped thing, and you certainly don't have that a serial examples. So, okay, so I think this exponential versus polynomial dependence is a big gap. A more interesting open question is, you know, what happens when you move away from this random initialization, when you take account of training data and Take account of training data and consider a trained network. What goes on there? In particular, do you still see this near linear behaviour? What is it that would imply you still see this near linear behaviour? Very interesting direction. All right, I'll stop there. Thank you. So is it observed in practice that in practice that deeper networks are more robust. So I think what is observed in terms of robustness is that if you increase the over-parametrization, you can see You can see more robustness, less sensitivity. Seb has a paper on this in this upcoming EUREPS, based on sort of simple isopherimetry argument. So yeah, I mean, I think the increasing depth for this particular model, having this phenomenon kind of go away with increasing depth, and it's maybe more a reflection of the model. And it's maybe more a reflection of the model than what you might see in yeah um uh yeah, so you know I guess this is just sort of a converse result that shows you know we'd have to change the model to do any better than that. What happens when you have biases? Yeah, good question. So biases are a little, you know, you get a little complication because you need to be careful about the scale. Need to be careful about the scale of them. So we haven't done it, but I'd be kind of surprised if anything much changed if you were choosing the scale appropriately. You can make silly choices of that. The scale of those biases to have things disappear, of course. But yeah, choose that scale appropriately and you should see the same thing happen. Same thing for other squashing functions. You need to be more careful about the scale. Nice thing about Relu is... thing about Relu is you don't have to care about the scale, right? It's all sort of everything's relative. The not nice thing is you know it has this discontinuity in the gradient, so there's no sort of classical smoothness to exploit. Yes, Shaq. I have a question. Yeah, okay. I have two questions. I mean whenever I hear this I mean about this line of research of this works, I think that two things works. I think that two things bother me and maybe you can explain to me. One of them is why should we care about random neural networks where actually in reality we never you use you predict with a trained neural network not with a random one. And the other big question is why should we care about small perturbation? What we care about when we talk is that you are going to deceive the deep neural networks when the human Networks when the human still classifies it as a panda. So, do we have any better model of capturing? Well, you talk here about, you know, if I took a panda and instead I put a picture of a table and a human will tell me it's a table, I will not consider it an Versailles example. So a Vassal example is about the gap between human perception, which is not exactly what is captured by the perturbations. Captured by the perturbations. So, the two big question here is the train networks and the question of how it relates to what you really want to confuse is your perception. Yeah, yeah. So, for your first question, I agree. I think that's why I put up there as the most interesting open question. And we study random networks because we can. For the second question, yeah, I guess another way of putting what you're saying. Another way of putting what you're saying, expressing what you're saying, is we shouldn't be working with the Euclidean norm, we should be working with some sort of perceptual norm of the distance in this input space. I don't know how to do that. I think that's a very interesting issue. I mean, I think Euclidean norm is a good start, right? It's certainly an extreme kind of brittleness property, and it'd be good to understand. And it'd be good to understand that. But yeah, if you have suggestions for better norms, that would be interesting. You know, I'm better at asking questions than in answering questions. I'm wondering if you have to know the, like, okay, we know adversarial examples exist in train networks empirically. Do you know if empirically, like, randomized ones, do they have, are they empirically observed to have? Are they empirically observed to have more or fewer adversarial examples? Like, oh, yeah, I mean, you see this in, you know, the theorems bear out in randomly initialized networks, yeah. And, you know, that paper that I referred to that looked at the two-layer case, did some experiments on deep networks and found the same solution. Do you know if they're more or less susceptible in trained networks? Or less susceptible to training networks, is there by any chance? I'm just wondering if the training process makes it easier or harder to generate adverse examples. Yeah, I don't know. Good question. I mean, you know, this is a phenomenon that was discovered in train networks, right? Nobody's, as Shai says, nobody cares about train networks. Sure, sure. Okay, thanks a lot. It's dinner time. Uh can we distort it? So tomorrow