So for uh the man of hubby. And uh I exactly today's details. I'm very happy to take five minutes type of meeting and uh and here are many things that uh I didn't know. So what I'm going to speak about is uh it's actually kind of fits into the conference because it it relates to the three last talks. So I will be speaking about the idea in the course of from the in the in the evening. In the evening, I will be given a selection criteria in the act of what Naniuka was speaking about, and then the underlying theme would be a conversation trend. So it does relate to the three things that we saw. And okay, so this is part of a program to see whether we put model. Model instabilities in fluids by from the correction. Okay, so that's it's been going on for some time. So I think let me just mention the group. It's the group in Madrid, the group in the ISIC. The proper live street, and also people in Kysinki, the talk after mine by Saudi, and also the group of the Camillo at the Parsons Institute has been looking at these questions. And so there are many ways to set the instability in fluids, depending on these continuities of the scatters or of the direction of the Or of the direction of the velocity. And typically they behave very body, so it's even difficult to talk about weak solutions. So the problem has three parts. The first is the existence of weak solutions. Value situations which I will be more specific when I go to the Vimer. The second is uh The second is what they mean, the mission about these Compass integration solutions. They exist, you are happy, they are regular, and then you prove typically that your positive result and threshold, but then you cannot go beyond that. The result of SESKI does a U for Navy stocks. But what do you mean? Okay? So macroscopical interpretation. Copycat interpretation, micro interpretation. That's the second part of the program. And so this, at the beginning, these solutions were, I guess, relevant because they show us that there is a non-uniqueness for a variety of problems. Okay? And this is a plus and a minus on what. A plus and a minus. On what happened, you're very happy because you were able to show that there was not enough conditions to prove uniqueness. On the second, then, what do they mean? They are not very unique, then do they have, do they retain any physics? Or some of them retain some physics. So, but I think my point here is that there is no hope to restore the uniqueness at the area of weak solutions. At the evolution solutions, because this is really at a scale so small that it doesn't even make sense to talk of uniqueness. I think the people here which know about the stochastics can agree or disagree with me later on, but I think it's like when you flip your coin and then you have another solution of your stochastic equation, it's a different equation, right? It's a different solution for the books. And I think this is what happens in the conviction degradation solutions. Solutions, but what I think is that we should be able to get a uniqueness at the macroscopical level. So maybe we cannot predict what these particles are doing at this time deterministically. And I think this is part of the the nature of the problem, but we can say something about how they affect globally. And this should be this I think we should be able to I think we should be able to delete deterministic uniqueness at the back of in this way where I connect to the talk of Jan Luca. So there should be a section criteria at least at the macro level. Say action practically. So that's the problem. And this you could do from all the names of instabilities which are so famous, including dynamics, Kesmos, Mathematio, all of them. So I will focus now on the Muscapo, which is the one better. And I think half of the talk is very not related to YPN, so the same kind of things would be true for the OR. Of things would be true for the Euler equation and other coordinates. And then the last part of the talk really is it uses the structure of IPM. Okay? So happy I don't have to record the equation much because it was thoroughly discussed yesterday. What I will be doing is a two-phase Muscat problem. So yesterday we heard a lot of nice results about the one phase. The entire talk is going to be The entire talk is going to be about the two fixed. So, this is the equation, and in case the velocity acts at the level of the force due to these grains of sand which are preventing your fluid from moving faster. And okay, so I'm looking at this two-phase problem. So, there will be some during the talk there will be open and closed cur curves. That's why I have this notation. This notation and they are trying to evolve in time. So you will have these two materials with two fluids with different density and instead of one SEI to make the same viscosity to make things easier. But also they could have different mobility or viscosity depending on your background. Okay, so then the Musca problem tells you that the vehicle That the velocity is going to be the vehicle of the derivative of the density, and the density has this jump at its continuity. So, the derivative would be a derived data at its continuity, which tells me that this is the velocity. And I call B, I don't know why, the Muscat operator. I guess now that I think about it, it would be better that it's called M. It's called B. Okay, and this is the classical Muscat equation. And uh we have a lot of experts here. And somehow when you look at the case of a graph, it's a non-linear parameteric equation, which has this feature, and then the difference of the densities, which is constant, is staying viewed whether it's stable or unstable. Okay, in the case that is In the case that is stable, so the I fluid is on top, then I guess there are a number of papers. I guess there is even more. So short time resistance is understood. I mean, I guess long time resistance, we heard that there is some results yesterday. That's not the problem I'm going to talk about. I'm going to talk about the other situation where the heavy fluid is on top. Let me hear it. Let me copy that. And then this is actually the problem which is, there are many, many papers in the engineering literature. If you put a viscous finger in, then the list of papers hundreds and hundreds and hundreds and even last year, then they have new numerics, try to Trying to explain how these things evolve. But as far as I know, apart from some extract solutions, there is not many mathematical theories. So that's what we look at in the last years. And then what you see in these experiments is that this is the initial configuration, the galaxy, and then as time evolves, the good. As time evolves, there would be this zone. So, for the mixing zone, where I don't know, t equals three, then there would be an upper boundary and a lower boundary. And here we still would have our nice heavy fluid, and here I would have our light fluid. And here, there would be a mixture, of course. That's what you see this these fingers are now. My particles are very big. My particles are very big, but uh somehow minus the theoretical mathematicians, my simulation is really bad. Okay, so there is a mixture of the two materials, and that's what the people see. And I think already, I don't know, it is scary, but 12 years ago or so we started to look at this with Diego and then And then in the end, there exists with Compass integration, you can show that there exists these weak solutions. So this part of the program is more or less understood. And also this extends to other situations, I will say in a minute, like what they see. So this we understand to some extent. And then even if in the cases, the new cases that would In the cases, the new cases that would come, we know what are the difficulties, and it's prominent that we understand to some extent. Okay? And yeah. One thing that we introduce, even if it sounds simpler, I think it's maybe the main uh non-technical idea, is that this uh no, this is okay, this is the mixing song. Let's make it bigger. Let's make it bigger. This is time t equals 5. So the missing sum is greater. Okay? Then what we understood, our answers, was that in fact this zone, where the two materials two fuse mix, it's described like this. There is a self-infection. There is a self-interface, which is a curve evolving in time, like in the host case, and then there is a speed of opening, direction of opening, and the speed of opening, and then the full mixing zone is just an envelope of this interface. Actually, it's important that this width of the interface is not constant. It might vary from Phase is not constant, so it may vary from point to point. So, and that's the kind of things you can see macroscopically. So, maybe you don't know in this experiment, you don't know what is happening really with the particles, but you can really see what is the size and shape of the mixing cell. And that's something that you do your mathematics and then you can test these experiments and see whether they agree or not agree, and it can be seen, and also you can do numerics. And also, you can do numerics. Okay? And another thing that is actually a paper we wrote before, I'm sorry after, is that, okay, I cannot tell you what is happening here or here or here, because these things are mixing, but I can tell you what is happening on average. So if I zoom somewhere, then I have this small ball, and I know that it's a missing, it's a total missing. Mixing, total missing. So there would be here heavy material and light material. But common sense tells you that if I'm very close to the boundary, there should be more of the heavy material than of the light material. I mean, the same here. Here there could be mixing, but common sense tells you that there should be more of the light material. And that's actually true, but you have to be careful with the scale of the material. With the scale of the volume or just. And what is exact is that if I take now the average, so I have this Commission Tracy Commission integration solution, which I don't know how it behaves, but now if I look, this curve was called set, and this is called set lambda, then the average of rho in this set lambda, this is exactly lambda, which thing that lambda was from 0 to 1 here. Think that the anger was from zero to one. Here I am when I am in the top, in zero I am in the zero interface. So here I can tell you that the I cannot tell you what the Commission's integration solution is doing, but I can tell you that this average is what it should be. And this you can also go to your you will do this with the equations, with your wine and water, mix them and see where you can make this average easier. Okay? This figure. Okay? And actually, we'll speak a little bit more about this light, but also you can predict not only the density, but also the macro behavior of weakly continuous quantities, of some non-linear quantities. Like for example this one. So this one you can measure and then in average is going to have the behavior in this case it takes by In this case, it takes my and that was that's what relates to that to that result. So this is a long history. It started with a paper with Diablo Paco, where we just explored this Compress integration scheme in few bits for APM, and then we introduced a new technique. Then we introduce a new technique, these default configurations and prove the lack of uniqueness. Then last week we look at the case where this curve is flat. So in that case, everything is one variable. And I just put it here because it might be relevant at the end of the talk. But the easier situation The easier situation, which was also understood by Soto, is that everything is flat, so this is heavy, and this is flat. And this is the mixing. Good. And then there has been a number of papers improving and in different groups and improving the regularity. Doing the reality, and also it applies to what the seeds or table factors. More or less the same philosophy. And that's what I say, this part, that's why I say that this part is more or less a quote unquote universe. So it really doesn't see much of ideal. It's just a general instruction. Okay. And also look, this is something that Konji explained yesterday. As Conjee explained yesterday, we look also at the case of this vegetable breakdown. We know that there exists this scenario where I have this, I start with this stable situation, this graph situation where I have the iter and behavior, and then this moves, and then the iter is on top of the behavior, and also I use the regularity. And so what we were So what we were asking is, you know, trying to understand, is what happens afterwards. After T2, what can you do? And can we continue your solution after T2? And okay, with this machinery, you can show that there exists this the same type of mixing zone, so 32. And that's complicated because here is sometimes the light fluid is on top. Sometimes the iic fluid is on top, and sometimes the i fluid is below. So sometimes it's stable, and you are in the parabolic regime, and in different regions between here and here, you are in this unstable situation. And I will speak a bit about this. And how to glue these situations is pretty difficult. And in fact, this result is satisfactory, but I will complain at the very end of my talk about our own results. When talk about our own result. Okay? Good. And also, funnily, our motivation was to continue the result, but also this scenario, where I have the heavy fluid is inside the bubble. So here is heavy and outside is light. So sometimes light is on top of heavy and sometimes heavy is on top of of of light. So it's passive and stable. So it's partially unstable. This is heavy and this is light. Then this evolves and it does create a mixing zone in the unstable part. This would be this simple interface that I was speaking about, but here it would be evolved according to the miscarriage. So part of the domain is stable, part of the domain is unstable. Sorry? We use this vortex group for the interface. We have an equation for the server interface, and that's what we evolved. Okay, so in a sense, I hope I have convinced you that we have some answers at least for part one of the program and also for part two. But the main issue of this complex integration. Complex integration, and you repeat again. It's not only that there is non-uniqueness of solutions, but also there is non-uniqueness of this macroscopic evolution. So I did some mixing, but for this area, we have for this mixing zone, we make an ansat. And with this ansat, we create this type of mixing zone, which agrees with experiments more. It agrees with experiments more or less. And then we plug in the convex integration solution, and we have many solutions. But nothing tells you that this handset is the correct one. So my point is that we should restore a uniqueness at least at the level of the macroscopical behavior. So that at least we should say, okay, we don't know what is happening here, but the mixing zone is going to be dismal. Is going to be this one. And there is some criteria to determine that. That's, I think, in my opinion, is the biggest problem in COVID integration, apart from improving the regularity and so on, that this will happen in due time, we get to the threshold. But this question I think is not really understood, and there is very partial results. And my point here is that for IPM, we have Is that for IPM? We have a VC sound criteria which I want to explain. Good. So actually then to explain you that, I need to go back in time many years, late nineties, where Faye Soto was uh a young uh a young postdoc, former student of um Stefanookhaus and Look house and Stefan writes a lot to discretize problems, and he had this idea of discretizing the Stefan problem, probably where you mix water and ice. And Felix studied this idea in the context of IGN, the same problem, the same stable program, and he created what then has been called the JKN. Because of that, everything can be uh attached to the Muscat program and to IPM, and it's not so clear what would be the version for the Euler equation, for example, for the next table. Potential energy, because remember that in the Muscat program, what is one feature which is important is that the role of gravity is crucial in the Glassie's law, so that makes the problem very anisotropic. referred solution. And then he formally computes that at the T C level, then this is a gradient flow and maybe here is enough. And then what he does is discretize in time and this would be So the image is a image of phase H and we have K H and put forward a bit because that would be wrong, but then he conjectures, he describes the problem, and then he reacts. A probium and then he reacts it at the discrete level and creates this theta s rho and creates a sequence which is what is happening at time kh determines what happens at time k plus 1h through this variational program where he's using the and introducing for this type of programs the passage time distance. This became This became so famous, has become so famous later on, but this is really the main starting point of the whole thing. So if you are not very familiar with the Baselstein distance, think that this theta is a generalization of the Lagrangian flow, but it's not longer one-to-one. It's measure-preserved, but no longer one-to-one. So in particular, for our minds, it allows mixing. It allows that two particles belong to one. Two particles could go to the same point. And also, this theta here, which is the density, is no longer one minus one, but it's allowed also to take values in Bison. So he relaxed the problem in this model. And this is at the disk level. And the question that he posed then is what's the image when Tk goes to when H, probably H goes to infinity, not zero. And he visited Madrid once, and he told me that in some curious ideas he has this equation that he conjectured that would be satisfying the image. And indeed, we proved that we follow his inspiration. And you can really show that at least formally, these solutions of this JKA minimizing movement scheme, they took from their Then the skin, they do converge to this as well. Okay? Good. And then the new thing is this part strength. Here, but then it appears this new term. So that's what's there, and not much has happened. In the, okay, maybe I say something. In the flat case, in the case that you start with the curve, then already in the paper, in the or already in the paper by by Razio in 2012, is proven that this equation in this case is one-dimensional, it's just the Vargas equation, and it has an entropy solution which you can recover with combination data. But that's just for the other part, from the exit, was visited by the other year, it's a postdoc, and then we proved And then we prove that if you look at all these COMEX integration sub-solutions, then the one that solves this problem maximizes potential energy dissipation. So therefore, the selection criteria that we suggest for choosing the best macroscopical solution of 5PM is the one who uh participates more potentially. More potential energy, which is consistent with the fact that is a gradient flow for the potential energy. And finally, you arrive to the same equation, in a totally different way. You have a question? Okay. So, on one hand, Otto has these minimizing movement schemes, and then if you do it carefully, you arrive at this equation. On the other hand, I will now go to the river of complex integration. You look at what this matter is. To look at all these macroscopic solutions associated with this program in compass integration, and among them, the one who maximizes potential energy dissipation is going to be also a solution to this question. So then we will have more motivation to study this equation, but this is an active scatter. This point at the state B and the row are connected by the And the rows are connected by the operator, a remote operator. So you cannot apply the classical, at least we didn't know how to apply the classical theory of constitutional laws. But however, we borrow the viewpoint, I think, which is more common in fluids. And at this, if the interface, if the initial interface is anietic, you can prove not only that there is a solution, which is easy, but there are entropy solutions. Easy, but there are entropy solutions. And then according to this criteria, we conjecture that those are the this is the selection title. So this is the first time that there is an answer to point three in our problem. Okay? Have a question now? Okay. Okay. So what I'm going to do now is to expose you a bit is to uh expose you a bit to the rivers of uh determination to the rivers of convention. So I need to explain what is this macro micro connection and then explain you why the why the which is actually quite easy dissipating a potential energy gets you to this equation and then an idea about how this can be solved. Good. So this is a So this is a I was about to remove this transparency because this SEI because probably you understand this much better than I do, but I think then I thought it was good to put it out and I would tell you whether it's nonsense or makes sense. So in a sense, the viewpoint of COMPEX integration is that the macro behavior is captured by Wiki image. Okay? So that's what you compute. And then WikiLimits And then with the image, you have any equation, and then you have the sequences of solutions, and then you want to know what are the wiki images. And these wiki images represent more or less what you can really see, which is an idea borrowed, I think, from homogenization. That's Carter. And in a sense, and here is where I am speculating, it might make sense for turbulence because instead of doing coarse graining, where you choose the answer. You choose the lengthen scale, and you see what happens, we mix all lengthen scales. But it's something they see. Okay, anyhow, that's much more try to steer the conversation for the next three days while we hike. And of course, if you talk about with image, then you need to go back to the theory of Luktartar of compactness, which is at the Compactness, which is at the very root of all the convex integration. And what Luke did in a very astral framework is take any tracing non-linear PDE and decompose it into a linear equation, so satisfy a linear system, and a differential inclusion. Okay, and that's what we call these days the Tartar framework. And once you have that, then there is some Then there is some jargon from composition compactness, which is the web con associated to the operator, these semi-convex files, and these quantities which are quickly continuous. I think in the talk of Saudi this is very relevant in MHD, so I think you will explain more about this. Here, let me go a little bit fast. So, as I said, I define the relaxation as a qualifying of possible weak units. It could be the macrobiotic. It would be the macrobiotic unit. So the linear equation would be dissatisfied at the unit, because it's linear. And then there would be a bigger set, which is called K-Reaks. And this K-Realx is sandwiched between these compensated compactness notions. If you don't know them, that's fine. And then what I call the sub-solution or a macro solution would be a solution of the linear equation which is in K-Rax. And this is what is supposed to represent this macro behavior because the desired meta-theorem is that whenever you find a sub-solution, you run convex integration and you will find these infinitely many solutions which will have the same initial data and the same behavior in these cells. In the sense that if I take average solution, we behave like a sub-solution. So in fact, my view these days, which might change the next few months, sub-solutions are more important than solutions because sub-solution is what you can really see. Okay? But it's good to know that there are solutions in that. Okay, so now this programming in IPM, then you have to run this program in all these PDAs in fluids, and for each of them you have a In fluids, and for each of them, you have a cone and some quantities. It's more or less, as I said, understood, though this requires some work. And so here I recall that in IPM, I have dt rho plus divergence of rho b equals c. So I simply recast IPM saying, okay, let me talk about the new variable m and then let me say, okay, I'm going to look at solutions to this linear system where m is equal to. system where m is equal to rho b. And also, since rho has a values 1 and minus 1, I want that the modulus of rho is equal to. So this is a set. Seems naive, maybe it is, but it's useful for this. Okay, then it can be shown that this K reacts, it has a very close expression, which is this, and maybe just think. And maybe just think that on one hand I have one minus rho. Okay, so one minus rho squared. Remember that if I am in the heavy correct fluid, rho is equal to one or minus one. So if I am being a solution, then this is zero. This is zero, and then m has to be equal to rho b, which was, I recall you, that being a real solution of IPN is when m is equal to rho b. So I can't really. is equal to rho v. So I can react this condition m equal to rho v in a quantitative form. Okay? So how much is given by this expression? This is actually these are beautiful geometric configurations. You really need to find a cal in five dimensions using specific solutions. And this was done by Razzloo and then Frank Menwald did the case of While give the case of different viscosities, which is very interesting because, as Mimi was saying, when you do compress integration, your fear is that nothing in it is visible, which could be the case. But then here we saw something in the hull which is very interesting. So this would be the set, these two lines, in a cartoon of the set. Everything which is here are the macroscopical states from which you could run a convex integration, but there is a Misintegration, but there is a pin singularity. And this pin singularity, when you look at what it is, is exactly where somehow you could create this Thomas integration solutions by this continuity of the density and also by discontinuities of the velocity field. And here the velocity field would be continuous. So the hand somehow since the physics. Some physics. Okay. Okay. So there is a, in this setting there is an age principle. So if there is a system of IPM, if there is a subsolution, there is many solutions with disseminated data and with the same behavior with this power virus, for example. And I think Mimi already uh Mimi and Yayuka they reviewed the history, so I'm gonna skip that. The history, so I'm going to skip that. And so the point is to find subsolutions. So now you know this condition. Let me put it back. And then your entire game is get some initial conditions and create some M, rho, and B which satisfy this inequality. If you create this subsolution, then immediately you plug in your H principle, which you maybe you may not understand very well, but it gives you as a black. Very well, but it gives you as a black box many solutions to this. So the entire game now is to create some solutions. And that's a maybe I go a bit fast. So that requires to look at some averages of this Muscat operator that we were looking before. And the first paper we we did, we found Diego, you have to look at some continuous average and Average and it's interesting because in the end, in generation and so on, you have this type of operator, which is a semi-classical operator with a T here. And then we borrow ideas from, so think that T is H, the Planckosta instead of T. And then you can borrow these ideas about computators, gravitational equity, gravity. Groggy's inequality, growth semi-classical analysis, but at the end of regularity is not enough. So we have to develop that theory. And then just it passed. There is a problem because when you do even a toy problem, then the width of this mixing song is here, it's whatever C. And then when you do, if you have a parcel. Uh partial something which is partial stable, then what would happen is that uh you will have to open the domain in some region. Would be a missing song here and then here you have to uh sort this cut. So at some point this width would be zero. Look uh copious, but okay, we could say I'm uh running a bit uh out of time. I am running a bit out of time. Let me go directly to the let me skip the partial unstable case and just go back to the selection criteria which is the newest and perfect models. More interesting and also it's not that complicated. So we simply wrote the potential energy, integrate by parts, and then from this inequality, from the inequality I was describing for some. Equator I was describing for subsolution, you get that M is equal to V, this is fixed, and then the field you have is this G. Now this is, this energy gets maximized when G is equal to minus E2 so then somehow the sub-solutions, if there exist sub-solutions, there would exist sub-solutions so that G is equal to minus E2, then they would dissipate more potential energy than any other. And then you put this into the questions, and then happy you arrive to the same version of the other question. So now we sort of reconcile these two ways of looking for the unstable Muscat for one the minimizing movement schemes and the other the convex integration. But then you need to find the solutions. And as I said, at least we don't know how to do it with the At least we don't know how to do it with the theory of conservation laws because P is a non-linear operator of rho. So, what we did is say, okay, we know at least that in this case it's possible, right? Because here is linear and it's 1D, and then it's simply the runner sequence. So then we create a change of variables which maps. Variables which maps this configuration that we don't know to that configuration which we know. So here would be this potentially new mixing zone, non-linear mixing zone, and there could be a map which T X by T which maps this situation to that. This situation to that situation. Okay. And it's in the next four feet. Okay, that's the theory. Yes, that's all. Proof. So this is this possibility ifromorphism which maps this situation to that. And this is the situation, the solution, solution to others in the fair case. Okay? So what we want is that the conjecture. The conjecture, our answer, is that the new row, which is here in this non-mixing zone, is going to be the row in the case of the linear sequence, of the linear sequence. And this is okay, and then since we know how to recover from raw velocity, then this leads you to an equation for this set of the change of variable. New base sets of the change of variable. And there is a frontal difficulty because this needs to be at least a different morphism. So it's the terminal better to be positive. And that's not clear that it's positive at all. I mean, totally the inverse appears in the estimates. So what we do is prescribe to first order the evidence, which also then Every sets, so then you have a much better control of what they look like. And um, so this is a just what should be in a the velocity initial interface, this is the initial interface, and then this becomes a problem forta. That problem for eta is here, and you see that this is a this is a this operator is it tends to be the um It tends to be the osabar camera, but it's not. But even if it was, then it would be a. Then what it is, is a curve rate of order minus one, which degenerates when t goes to c. So a priori I using this derivative, one derivative in y, but no derivative in what. That's it's a technical issue, but it's very good because here in the Because here in the when I'm mapping everything back here, it's very rigid. So really this really needs to be between minus two and two. And so if we will be using here another derivative in y2, then I don't know. Maybe there is a way, but not the way we don't. Okay? Anyhow, uh since at this if this was the view so that kernel we will be using just one derivative in y, then One derivative in Y, then it makes sense to try to use this type of cochlear kind of theorems. And that's what we do. That's a variant because you cannot really apply the IPTs. Version of Ni C that you have a T here, which you cannot have, but anyhow, this is and this is the theorem we can prove. And it's, I think it's as I think it's as good as it could get. We have this very good description of variable sets. Then the gradient grows up in like 1 over T, but that's okay, right? Because at the minimum set time is an S V V function, so it has a jump. So it cannot be better. And we have this logarithmic term here for the velocity. And it's an entropy solution. So I think it's what we really like. Because of regularity, then it satisfies this. Then it satisfies this classical entropy-entropy gas condition. Okay? So I guess this is a good problem. Stop. It's you? Okay. It's you? Okay, I was letting somebody else go because my question may not be good. But I mean, so in other examples where we know you have this kind of non-uniqueness, like if you took burgers or if you took a very rough transporter, burgers with rough initial data so that the Lagrangian trajectories aren't unique anymore, right? Does this kind of scheme give back our current entropy condition understanding of those situations? I think the only thing we could say is that if you can find an entropy solution, that would be the good. Financial distribution that would be the requirement. But is that the same understanding? It's the same. It's exactly the same idea. Right. It's the same idea. What we know for workers, but you could do JKO there, that's what I'm saying. Think about does that work there to give that solution? That solution. If I understand correctly, I think it's fun. I don't know. So that was taking So instead of taking Darcy's law, you take Brinkman, so you have discarding for V, a La Pacian for V. Of course you you would have a jump, you would have some kind of diffusing effect. People thought looking at what happened in that case and taking the coefficient. And that could be a selection criteria. Yeah, exactly. Well, not that anywhere. I know that phase is very the face of the. I know that Falix is very Falix also very fond of self-expension units. So that he advocates possible state. But that's easy to say. I was thinking of a different filtration law. Exactly. But that's a good one. 