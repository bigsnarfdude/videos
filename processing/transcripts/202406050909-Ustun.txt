It's kind of work we've been doing over the last two, three years that touches on fairness, privacy, and consent. I'm particularly excited about it today just because I think it touches on a lot of things that we've talked about over the last few days. So just get things started and rounded. There we go. I wanted to start with just this idea of clinical decision support and clinical models in medicine. So here's In medicine. So here's the CHAS2 BAS score. This is a simple scoring system that's used to predict Chad that's used to predict stroke risk in patients with atrial fibrillation. This is a model that is on U.S. board exams. It's used by thousands of doctors every year on maybe tens of thousands of patients. And for the purpose of talk, what I want to point out from this model is not necessarily decisions that it's leading to, but just the fact that it uses two variables, a That it uses two variables, agencies. So these are variables that you'd never ever see maybe in a model that's used for lending. It would lead to a bunch of scrutiny. There is like lucidity prediction. And the reason why, right, is because when we use these variables, predictions change based off of age and sex. That means that models are assigning different predictions to different groups. And that's explicitly discrimination. And in this case, this is the third MD health model that we've seen in this workshop so far. In this workshop so far, and no one really has time. And the reason why, I think, is because this is kind of aligned with the ethical principles of clinical decision support in this area. So this stroke risk prediction task is a task where there's no resource allocation problem. As a doctor, your ethical principles are maybe to do the best and to do no harm. And so what that means from a modeling perspective is that if you want to build a model that predicts stroke risk, you want to assign the most accurate prediction. To assign the most accurate predictions to each large nation group without harming any person. And if, for example, stroke rates differs across groups, then the right thing to do is to just use those variables to explicitly target the groups. So this is kind of the basis for using these group attributes in these models. This is an effect that I will call personalization. It's also called score correction. It's really actually the creative. It's really actually the crudest form of personalization. It's categorization, not individualization. And I think based on this, if you go ahead and look at large repositories like FECalc or FE Fenscape, you'll find thousands of models. A lot of them will use age and sex, but they'll also use a whole host of other variables. So just to give you another example, So, just to give you another example, here's the dead rate ID risk score, which you can think HIV. And this also has age and gender. But in addition, it's using variables like sexual practices and race and ethnicity. And now, all of a sudden, this isn't just demographic information. This is information that you're going to have to report truthfully, that is sensitive, that you might not want to disclose, that you might really want to opt out of. Want to opt out. And the reason why it's being included here is because practitioners have access to this data in a large medical repository, and it's really important to predict HIV infection as accurately as possible. And the assumption is that if you train a model with these attributes, that you're going to end up doing better for each group without harming any group. And so I call this an assumption just because, in practice, no one actually tests this. So, here I want to just show you an example where I'm going to build a clinical prediction model and I'm actually going to go ahead and test if personalization actually ends up making things better. So, here this is a simple classification task where all I've got to do is predict whether or not patients have constructive sleep apnea or not. This is a data set that I worked with in my PhD. It's got about 2,000 data points, maybe 29 features. And it's got two group attributes: age and set. And it's got two group attributes, age and sex. And I've created little dummy variables that basically categorize people based off of age. And in terms, they're sexual. And I'm going to use this to create a model just like Chad's do. This is a logistic regression model that is personalized. And what that means is that the set of features includes dummy variables for age group and for sex. And then just to see if personalization actually makes this better, I'm going to train another model. Better, I'm going to train another model. This is what I call a generic model. This is just logistic regression without agencies. And here I'm just showing you a training error of both these models. And lo and behold, I see that there is this gain from using agencies as well. This is not training error. This is a simple model. It generalizes. It holds on test error. But I'm showing you training error because this is kind of the guarantee I want to stop doing. But what I wanted to notice is: well, okay, this is. But what I wanted to notice is: well, okay, this is on the population as a whole. What about for each group? Here I'm going to break it down by groups. And so here you see this 1% gain is not equally distributed among groups. Some groups gain a lot, others not so much. But these two groups right here, these groups are actually worse off. And so this is an effect that. And so, this is an effect that we can call worst location. And I think that it's kind of one of the biggest blind spots in the development of these models today. And it's not just really this loss in accuracy that's kind of concerning. It's just the fact that somehow this promise that we've given to the patients is not being held. These are two groups who are reporting demographic information, expecting that the model is going to assign. The model is going to assign them a more accurate prediction. And they're getting quite the opposite. And it's not only that their expectations are not met. It's that in this case, you could have actually really just done things better by using the generic model on these two groups. So in this case, there's actually a very simple, less discriminatory alternative. That's going to be the focus of my talk today. This is going to be the focus of my talk today. What I want to do is just kind of quickly go over three papers and just kind of lay it out there and just kind of discuss them. In the first part, I want to just kind of give you a sense of what this effect is, how we should think about it, why it arises. And then in the last two parts, I want to just quickly go through some kind of larger paradigms for how to try to address this practice. So let's jump in. I just want to formalize. Let's just jump in. I just want to formalize things a little bit and give you a little bit of notation to make sure that we're all on the same page. So, I'm going to assume that I'm a practitioner who is trying to develop a model like ChadScoop. I'm going to have access to a classification data set where I have my features, my outcome that I want to predict, that's my Y, and a set of group attributes. So, those are my GI. And my group attributes, what are they? These are just categorical attributes that. Is categorical attributes that can kind of slice and dice the population into different groups. And so, whenever I'm going to be talking about groups, I'm going to be talking about intersectional groups, so just the most granular groups that I can specify in each data. And with that, I mean, my goal as a practitioner is to use this data set to go ahead and train kind of a personalized model. So, this is a model, I'm going to call it HP. The idea is that this takes in some features and then these special group features. Special group features and it predicts my outcome of interest. And so this could be like a logistic progression model with gummy variables, interaction terms, it could be a random forest, the DNN, it could be separate models for separate groups, right? Any kind of procedure that takes the group attributes and uses them to assign different predictions. And when I'm fitting this model, my general goal is to go ahead and optimize some. To go ahead and optimize some kind of aggregate performance metric. So that could be test error, it could be test AUC, but what the groups themselves really care about is how that error affects people like them. So that's this group risk. So this is kind of like the test AUC or the test error for females under 60. All right, with that, Alright, with that, what I want to do is just maybe kind of try to formalize what it is that went wrong in that previous example and what it is that maybe we expect from personalization. So I wanted to show maybe two basic conditions that we should expect from a personalized model. This first condition is a condition that is called rationality, and this basically says that whenever I have a personalized model, that it should assign better, more accurate predictions than a generic model. More accurate predictions than a generic model. And so that's just the error on my group with a personalized model is less than the error on my group with a generic model. This generic model, what is it? It's the best model that you can fit if you didn't have to be done. So this is what was going wrong in that first example. And then the second condition I want to talk about is this condition called MV-Freeness. And so what this says is that this personalized model should assign the best predictions when I actually report. Predictions when I actually report my true boot function. So, this is a little less intuitive. Like, let me give some sense for this. Say, for example, that I was using ChatSupask. I am a male between 30 to 60. What if I went ahead and reported female 30 to 60? In that case, and I found out that I actually got more accurate predictions as a result. It might be that. It might be that I was actually getting pretty decent per predictions with my current group membership, but the fact that I actually got better predictions by misreporting my attributes, that's a little weird. And so that's kind of what MV3 distributing. Environment is makes sure that whenever I report my actual true group attributes, that's the thing that is actually maximizing my accuracy. So this first position just says every group should expect a gain from reporting their personal attributes. Improve personal attributes. The next one says that this gain should be tailored to each group. So one point I want to make about this, these are just formal conditions. They're like based in the economic theory, but these are really testable things. Whenever I have a data set and I'm training a personalized model, I can just go ahead and swap out the group attributes and test every freeness. And then I can just take my training procedure, repeat it without. Procedure, repeat it without the group of attributes, train my personalized model, right? And so I can always kind of test this, given any task where I was creating a personalized model. And so we call these conditions fair use conditions. And the reason why is not because they really have to do with fairness, but it's just because they operate in a way that's similar to fair use of the law. So in copyright law, the idea is, right, like fair use conditions kind of characterize when you can use. Kind of characterize when you can use copyright material without asking for permission from the copyright owner. In this case, these conditions kind of say when you can use, well, we characterize when you can use personal information without asking for consent from the owners of that information. And so to give you some intuition for this, right, let's imagine, like, what if groups could actually consent to personalization at prediction? At prediction time. So here, for example, I have like an individual from a group, and the model that I usually use forces them to report their group attributes, right, and I predict the micro response model. But if they could consent, they might have an option to not report anything, in which case I would use the separate model. Or if I allowed them to kind of report anonymously, I'm expecting them to report truthfully, they might actually even misreport. Face report. And in that case, I would use a personalized model with these swapped attributes. So the idea is when a model obeys those fair use conditions, in this case, group G, if it were rational and everyone was acting in the same way across the group, well, they would report the personalized information in the same way that the models currently enforcing them to do so. And this should is something that I should really italicize. The idea is, right, like this. The idea is, right, like this assuming that people are rational, that things are behaving, that every group member is somewhat uniform and exchangeable. And I think that those are strongest arguments. So the way to use something like this is to flip it around and to look at when these conditions are violated. So whenever a model violates these conditions, you have kind of very strong evidence that under these reasonable assumptions, Reasonable assumptions that a group would really not report their data voluntarily or truthfully. And there is evidence of this least discriminatory alternative in the form of a personalized model with different attributes or as a generic model. So with that, when is it that we should really care about this? I think this touches on some of these themes that Lily talked about yesterday. I think we should talk about them whenever we talk about features that are protected. Whenever we talk about features that are protected, like sex, race, in that case, I think that fair use violation would signal inadvertent disformation. It's like, why is it that somehow everyone is feeding from reporting all the personal data, but somehow my group isn't? They could be used, for example, when attributes are self-reported, things like tricks per week, HIV. In that case, what you're really trying to do is incentivize truthful disclosure, or maybe just rule out this intensify. Rule out this intense privacy. And it could also be used for these features that are associated with some funny costs. These could be like sensitive attributes that you don't want to disclose. Like, why am I reporting this information that you don't need and that's actually making things worse for you? Or why am I going ahead and taking this test, spending some time when it's actually making things worse for my group? So these are all different conditions where I think we should be testing for fair use. For fair use. Some of them, I think, there is some legal basis for doing so, like the FDA, for example, has its call-out to see if there's discrimination going on for prediction models. In other cases, it's sort of law, but I think it's like the best thing or the right thing to do. Just because as a practitioner, what I really care about is predicting accurately as possible. And somehow, I kind of want to be thoughtful about the features that I'm requesting from people for in-person data collection. Data collection and also what I'm making people go through. So, with that, what I want to do is just talk a little bit about why this happens. So, when we. 10 minutes? Okay. So, why this happened, right? I think, you know, when we first came upon this, we had really just stumbled upon it. And then it took us about two years to get this paper published because people kept on thinking that it was some kind of artifact in the way that architecture evolves. But I'm here to say that this is not an artifact. But I'm here to say that this is not an artifact. This is really the norm. And the reason why is because, well, when machine learning works the way it works, in the best possible case, it's going to use personal information in a way that makes things better for the population as a whole, not for each group. And formally, the guarantee is that it's going to use. Formally, the guarantee is that it's going to use personal information in a way that lowers aggregate training loss. So, with that, you're going to get fair use violations because of optimization, like because your training loss is not actually equal to your training performance. You're going to get fair use violations because of generalization, because even when you're doing well in terms of training performance, that might not be test performance. And to be fair, I mean, these are things that we have like two decades of work on. We have like two decades of work on how to develop how to deal with machine learning. But all that machinery that we've developed is meant to improve performance at a population level. So just to give you an example, say I'm building a religious progression model that's personalized and it's overfitting. The standard thing to do is add a regularization penalty, like L1, L2, Elastico, Elasticnet, whatever. And then you have a little parameter, and then you're going to You have a little parameter, and then you're going to tune that parameter to get a model that generalizes better. And the way you tune that parameter is you look at 5CD test error. And there, you have a population-level metric. And it could be, for example, that making things better for one group will make things worse for another. And when that happens, that can happen because there's inherent trade-offs in the groups, right? Or because a procedure works well for one group or not another. The procedure works well for one group or not in another group, right? And sometimes it can actually happen by design to us. So it can happen basically because of misdestination, because the model class that we're fitting on is not rich enough to capture the heterogeneity that's in there. So I just want to give you one quick example of this, just to give some intuition. Here I'm going to have this really simple classification task where I have two groups, no features. And I'm going to have this kind of. This kind of data structure where there are 101 data points, so there should be a 50 right there. And then the idea is that I'm predicting some kind of risky outcome. If I am female, that increases my risk. If I'm old, that increases my risk. But if I am female and old, then my risk is actually important. In this case, I have no futures. I'm going to fit a linear model. That's just going to have an intercept term. What is it going to do? What it's going to do is go ahead and predict majority. So that's just here, there's like 51 negative cases, so it's going to say negative for all. And then if I train a linear classifier that has just a dummy for each of these groups, so like being young or being male. Well, that's going to adjust these predictions. And I'm going to lower my overall error and get a game. So this is an example where. So, this is an example where, for example, where personalization is working as it should. My first thing is completely blind to the groups. By accounting for the groups, I make things better. I get a gain of 26 great ones. But in this case, if you look at the young males, they were actually better off with the character model. So that's what they got the violation. And then these old females, well, nothing changed for them, they were the majority. Well, nothing changed with them. They were the majority group to begin with. And so they're reporting this additional information, but they didn't have to. And so, here, for example, maybe reporting your demographic information doesn't matter. It will matter if this was like a lab test or some costly procedure that they had to go through. And so, you know, in practice, these three reasons really are why you just keep on observing these fair use violations again and again and again. So, in our paper, we'll run, you know. So in our paper we'll run thousands of experiments with logistic regression in random forests, neural nets, with different ways to personalize, dummy variables, interaction terms, different models for different groups. And in all these cases, it's really hard off the shelf to avoid these violations. And to give you a picture of what this looks like, I just want to show you what these gaps that are associated with each condition look like. So here I'm showing you these gaps that are associated with rationality. This is just Rationality. This is just the gains that you would get from recording your group attributes. That's the first thing. And these are the gaps that are associated with every freedom. And here, what I've done is I've just colored each one of these boxes based off of the probability of hair use violation. And so for folks in the back, blue is good, red is bad, yellow is, I don't know. And the idea is you're just happening all over the place. It's happening on training data, it's happening on test data. Happening on training data, it's happening on test data, it's happening from one group or another. This is the case for, I think, all these models that we're developing without checking this. If you're checking this, you can kind of use this as a crutch to kind of iterate on model development to make things better. But if you're training a single model, it's really, really hard to fix things for all rooms on test and earth. Like, you can definitely minimize it, but you can't really fix it. And I think that that also is a little kind of discouraging for people in machine learning. Kind of discouraging from people in machine learning because they're just like, what is this? This condition is too strong. And to be fair, in this case, there's really a simple solution. The idea is just, if you really want this off-the-shelf solution, the idea is just free yourself from this requirement of having to fit a single model for all the groups. So here I'm just going to show you this case where I'm trading this first-bus model and I have fair use violation of two groups. If I went ahead If I went ahead and trained a generic model, like if I went ahead and checked my fair use conditions, well, then I also trained a generic model, right? And what I could do is build this kind of two-classifier system where the people who are getting worse predictions were just given predictions from the generic model that would kind of patch performance for these two groups. Another thing that I could do is just use this as kind of an indicator to say that personalization isn't working for these groups. For these groups, and try to just use the data to fit a completely different model for each group. And so, what this would do is that it would really specialize the whole training pipeline for each group. And that can really lead to big gains. With that, I've got five minutes, so I'm going to quickly kind of go through kind of high-level ideas for how to find a lesson. So, this first kind of strategy that I think we should So, this first kind of strategy that I think we should be using to address this is by trying to account for how much data we really need to personalize effectively. So, here I think, in the previous slides, you saw that there are a lot of these cases where you can't even test to see if personalization makes things better for groups. And the core issue is that, well, you know, in practice, whenever I'm developing a model, I have a finite set of data points. So I might have like 16 data points. As soon as I personalize with age, right, the number of samples per group decreases. If I had sex, now I have two samples per group. If I add hair color, then I'm at like one sample per group. And each time as the samples per group decreases, so does the reliability of my tests. And so that's kind of like the shade of these colors. And so in this case, personalization might be working, but I can't tell. In this other case, In this other case, maybe I can tell for some groups, but not others. And so a strategy here is to kind of determine this regime where we can test reliably without making any kinds of assumptions on what the data distribution is. And that's something that we call an epistemic limit in our paper. And the goal is really to come up with a theory to kind of characterize this limit and to say, look, if you have this much data per group, then at least you can test reliably. Then at least you can test reliably. But if you're above this, then you're in this unsafe regime. Danger lives here. And okay, so just like in three slides, the idea here is to kind of design this most powerful test that you can kind of use to detect personalization violations. The test that we use is just to look at any rationality violation for any group. We frame this as a hypothesis test to say, hey, can I show that personalization increases? Show that personalization increases accuracy for each group by at least epsilon. And the final data you want to look at is this thing that's called the probability of error. That's just the type 1 or type 2 error hypothesis test. With that, you can use fancy information theory to come up with lower bounds like this. Something that says that for any hypothesis test, the worst case probability of error on any distribution has to be greater than this whole round here. To be greater than small around here. And so this is something that will characterize the relationship between samples per group and the number of group attributes. And it's interesting and it's pretty, but what I think is valuable in something like this is that you can actually invert it and get some kind of epistemic limit that is really valuable in practice. So just to give you an example, say that I was trying to test that personalization, trying to make things better for each one. I have 8 billion samples, one for each person in the world. Billion samples, one for each person in the world. I'm looking for a 1% gain. If I invert that bound, I can say that, for example, if I have more than 19 binary attributes, and that's what I need to personalize, then any kind of test has a probability of error that's greater than 1 half. And I really can't test for personalization library. There is some chance that the data is distributed in a way that is. Is really going to screw me over, and yeah, there's a 50% chance that at least one of the groups is going to experience visualization. And practice, you know, this is kind of like a generic kind of rule of thumb. Like, I presented this in XTA, and a bank called it 20 More Questions, which I think is a good way to think about it. The general thing you can kind of do is use the kind of thing to give maybe the same personalization. To give maybe like the same personalization budget, where you show, okay, given your sample size and reliability of the test, here's how many group attributes you can use in the first place. So, I don't know, if I have 50,000 samples and I want to gain 3%, then I need, I can use atmospheric three attributes, 50 groups, right? So, this is kind of just, I think, again, like a high-level framework. Framework you developed into something more meaningful by specializing? So a question, but also give me more. Second big strategy, right, is how do I actually fix this model development? And what I want to do is just say one kind of strategy to do this is to go ahead and fit the model that has fair use conditions as constraints. So let me fit the most accurate personalized model that ensures fair use. That ensures fair use. That is, I think, kind of like the generic thing that you do in machine learning. It doesn't work so well because of specification. But the other reason why it's not really a great idea is just because fair use itself is kind of a low bar. Fair use just guarantees that you're not going to be harmed. But you could say, for example, this group, we're going to ask you to report sexual practices, race ethnicity, and give you a gain of 0.1%. And give you a gain of 0.5%. And so the principal idea, I think, here is to just create systems where people can consent. Where people can consent to personalization at prediction time. So the idea is, what if each person could actually choose to report a generic model, a personalized model? In that case, you can kind of resolve this naturally, improve overall performance, reduce data collection. And the kind of paradigm by which this works is instead of forcing people to report through group attributes, what you do is kind of show them the information that they need to decide on whether to report, and they'll let them choose. And the reason why I think this is interesting is because the moderator kind of operates as a market. So, when, for example, individuals experience commercialization, they're going to opt out. If they get insufficient gains, they're going to opt out. If personalization is working, they're going to opt in. And so, in some sense, consumers themselves are going to be protecting themselves from personalization. And now, your incentive as a practitioner is to develop systems that really are forced to give you gains for each group. Because as this happens, it adds up. Like, you also care about aggregate performance. You also care about aggregate performance, rear incentives that are line. So, with that, I think the main idea in our paper is a good way to do this. There's different kinds of strategies. They all kind of try to say, give me a pool, a personalized model, and they stack them up in a way so that people can consent reporting attributes one at a time. They'll improve overall performance, kind of give people the ability to consent. That's kind of what they look like. And that's it. And then let me just end by. And that's it. And then let me just invite my collaborators and my students. Thank you so much. We have time for one question while the next speaker sets up. Yes, hey, thanks. So the word personalization I think is interesting here because you're talking about the experience of individuals. You're describing this based on who you're