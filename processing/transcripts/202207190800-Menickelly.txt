Hi, like Anna said, I'm Matt Menikelli. I'm going to talk about stochastic average model methods, and I'd like to thank the organizers for inviting me. All right, so let's start with the cute overshare. Today is the two-month birthday of my new addition to my family. I gave her a mathematician name. Her name's Teresa Everest from Everest-Galois. And I just absolutely love her. She's my favorite person. I mean, she reminds me. And she reminds me that all human beings are just a collection of gross bodily functions, and everything else we do, like intellectual or research pursuits, is just the gravy on top of it. So on that note, intellectual and research pursuits. All right. So the problem I want to solve is just this one exactly. It's just another finite sum minimization talk. But, you know, while I at least have the problem written down, for the sake of this talk, each of the component functions, I have P many of them in this. Functions, I have P many of them in this sum. It's going to have a Lipschitz continuous gradient and is bounded below. And a quick aside for people that, I mean, especially in this context, you're probably used to seeing stochastic gradient type methods applied to this problem. It's worthwhile to remember that stochastic gradient descent being applied to this problem is an instance of a randomized method being applied to a deterministic problem. This is a completely deterministic problem because I can just evaluate it like this. And there's no expectation or anything. I mean, I just can if I choose to. I just can if I choose to evaluate all p-component functions. Just a reminder of this very basic fact. So, what's the usual motivation for finite sum minimization problems? It is, of course, machine learning, where gradient descent or stochastic gradient descent methods have kind of taken over the field. More on that in a second. Our motivation is different. We're looking at expensive calibration problems. Typically, at least what really motivated this project, we're looking at calibrations of various nuclear models, where each component function is going to be an observable. Component function is going to be an observable derived from a nuclear model parameterized by variables x. Computing a component function or an observable is going to involve a typically computationally expensive computer code. And we usually don't have gradients of these component functions or observables, unless in one project I'm working on right now, our friends in algorithmic differentiation have time to spend on us, which is not always. So there's some very key differences in motivations between these two problems. In motivations between these two problems. So, in machine learning, usually accuracy is not so important for the sake of getting good generalization bounds. That's not the subject of this talk, but just bear in mind that they don't need to solve these problems very accurately. We, on the other hand, in order to have nuclear models that make physically relevant predictions, we typically need our models to be calibrated at fairly good accuracies. Otherwise, they're not going to be totally relevant. Another key difference in machine learning, usually we're talking about huge scale. Usually, we're talking about huge scale problems. Like the number of component functions p is huge. The number of parameters is also like huge, for instance, in training some of those gigantic like GANs or just huge many layered deep neural networks. But in our setting, I mean, most of the problems that I've worked on anyway, these nuclear models have maybe 10 parameters and hundreds of observables. So it's a very different setting in that sense. In machine learning, usually each component function is cheap, it takes maybe order of n. It takes maybe order of n, where n is the dimension of the problem, sorry, dimension of the variables, many things to evaluate a loss function. While meanwhile, obviously, running these computer codes, each of our component functions is expensive and it's going to dominate the overhead that is the linear algebra of et cetera, cost of any optimization method you choose to apply to it. Now, like I said, stochastic gradient methods have kind of rightfully taken over these problems in machine learning. Why do I say that? Well, I mean, this is a, I'm very. Well, I mean, this is, I'm very, very coarsely summarizing this great work from this 2007 paper by Leon Bateau and Bosquet. But they show, to paraphrase their results, every iteration of a gradient descent method, if I want to evaluate the full deterministic sum, that's p mini component functions times order of n for the complexity to evaluate that function, p n per iteration, and you gradient descent, you probably need, in worst case, one over epsilon squared to achieve. Case one over epsilon squared to achieve this kind of gradient stationarity. So the total time as a function of iteration times iterations to reach that level of accuracy is given in the right-hand column. Stochastic gradient set, on the other hand, we usually only evaluate a few component functions per iteration to get a stochastic estimator. In that case, it's only going to be the cost of evaluating one of those component functions. So order n. But you need a lot more iterations. So the conclusion here, comparing the ratio. Conclusion here: comparing the ratio of these two columns in the column in the right, you can kind of conclude that, at least from this worst-case complexity analysis, stochastic gradient descent is a better algorithm than gradient descent for finite sum minimization, provided p is much larger than one over epsilon squared. And remember what I said about the motivation for machine learning? P is huge. Epsilon squared is not very small. In most cases, it get good generalization bounds. So, in most machine learning applications, this p much greater than over epsilon squared is. This P much greater than 1 over epsilon squared is satisfied. Stochastic gradient descent, from worst-case complexity point of view, it makes sense. It does not make sense in our kind of expensive DFO calibration setting. In other words, and keep this in the back of your mind, the faster iteration complexity of one over epsilon squared for stationarity that you get in gradient descent is noble. It's something worth trying to strive for when you're trying to minimize the total time to a good solution. Now, the other well-known Now, the other well-known issue of stochastic gradient descent is certifying that you have an epsilon-accurate solution unless you're willing to go and evaluate the full component of the full sum function. It's a practical nightmare. I mean, this is a well-known folklore result. If you've taken a modern course in optimization, you've probably proven this at some point. In order, if you run a circuit-gradient descent with a small enough step size for capital K many iterations, you can get the stationarity below a certain level, but there is this unavoidable. Level, but there is this unavoidable plus MO plus something that depends on M, which is a bound on the second moment of the gradient, that is not going to go away. It's a function of K. It's just there. And this is always realized in practice. I mean, here I'm just showing you if I have a stochastic gradient estimator applied to a simple quadratic function. If I for different levels of noise in my gradient estimator, if I have a deterministic, of course, it converges to the minimum that you'd expect. A little bit more noise in my gradient estimators. There's this little cloud. My gradient estimators, there's this little cloud, and if I have a lot of noise, there's a bigger cloud. And the cloud, the size of that cloud we're going to converge to, is totally a function of that variance. So in stochastic optimization, you somehow have to control variance if you want to guarantee a particular level of accuracy. So how do they do it in stochastic gradient methods? Well, the classical way that this oldest Robinson Monroe, the 1951 seminal paper that introduced stochastic gradient methods, Introduce to cast gradient methods is just have a decaying sequence of step sizes, something that's going to be not summable but square summable. Another thing you do is mini-batch the component functions. This one's obviously very popular, and there's also been some good work in using gradient, adaptive gradient variance estimates to choose adaptive mini-batch sizes to scale with the optimization. There's momentum-based methods. Think like Atom is the big one in the stochastic gradient community. But the one I want to focus on in this talk is variance-reduced methods. On this talk is variance-reduced methods. And we're going to do something that looks a lot like the first two in this list. This list is obviously not exhaustive called SAG and SAGA. SAG stands for stochastic average gradient. SAGA stands for, and this is really funny. If you go and read the paper that introduced the SAGA algorithm, I don't know if it's a misprint or what. There's all, there's legends surrounding it, and the authors never confirmed it. They never say what the A stands for in saga. But most people say the A stands for in French am√©liore. I probably butchered that. I probably butchered that, which in English is just ameliorated, and I'm going to use that language. So, only one slide to talk about something that was, for instance, mentioned in Francesco's talk, STORM, which is something I worked on my PhD thesis. All you need to know about STORM, and this is my one slide elevator pitch, is that it's just a derivative free trust region method. The only difference is on every iteration, your model and your function value estimates satisfy some probabilistic error bounds. And just paraphrase. Error bounds. And I'm just paraphrasing these conditions here. With some level of probability, I need a capital-fully linear model. And with some level of probability in every iteration, I need my estimates to satisfy a similar bound. And like I said, to keep in your back of your mind about how gradient descent's worst case complexity is a noble goal, you can show an almost sure convergence result for storm that it does have that one over epsilon squared kind of stationarity result in the worst case at the obvious expense of needing more expensive estimators. More expensive estimators, like in these two boxes, than stochastic gradient descent. So I want to make it clear that in this paper, I'm not proposing a new method. The method is storm. I am proposing a new model. The big idea is we're going to use models that look like SAG or SAGA, in particular Saga. So each component function, I'm going to say, is going to be modeled independently by something that has a center point, which I'll denote CIK. K means it's the iteration count, I means it's the component function it's modeling. It's the component function it's modeling. So there's always going to be a center point associated with the model. I'll define the average model in the kth iteration as the sum of all of these models, which might have stale center points. And you can see ik is probably not going to equal xk on those iterations, or xk is your incumbent point. And I want to further distinguish the average model from the expected model. And you'll see why I call it the expected model later. So the key idea here, and this is the model we're going to use. Here, and this is the model we're going to use. It's called the ameliorated model. On the kth iteration, I'm going to choose a set of my component functions indexed by ik. And then when I choose that set, I'm going to update my center points. So now all the ciks become xk's and the model is updated. Let me denote the probability that on the kth iteration, I've selected the ith component function to belong in ik, as pi ik. The ameliorated model is now just a weighted sum of the average model from the last iteration. Of the average model from the last iteration and the differences between the two models evaluated at a point given the two different centers of the new incumbent point XK and the old center Cik. And the weight is prescribed by the probability. So where are these terms coming from? Ameliorated, expected. All right, well, here's why it's expected. The expected value over IK of the ameliorated model is the expected model, which means the model centered at xk. It's very straightforward to see. It's very straightforward to see. If I just take the definition, rewrite it with indicator functions, I see that well, the expected value of the indicator function of i belong to ik by my definition is pi ik. Pi ik is cancel. The center model is defined precisely as the sum over all of the centers in the previous iteration. So those last two terms cancel. Anyways, get back the average model. This is not a new idea. This is just an extension of the stochastic average gradient ameliorated model. So let me illustrate. I'm just going to So let me illustrate. I'm just going to generate a bunch of quartic functions to be each of my component functions. I'm going to model each of them with a second-order Taylor model. Don't worry too much about the details. The point is that it's a quartic model being modeled by a bunch of second-order Taylors. So as you would expect, because sum of Taylors is Taylor of sum, vice versa, I get that the second order model of the second order model of the quartic function is precisely at the Is precisely at the old point, the center point C, is just going to be the average model. But the expected model is going to be the model at X K, the second Or Taylor model at X K. No big deal. What does the ameliorated model look like? All right, let all the probability parameters be 0.05. So there's a 5% probability of grabbing each of those functions on a given iteration. For a bunch of realizations, so I just independently flip a coin that is weighted 0.5.95. And if success, I'm going to put it in my set IK. If success, I'm going to put it in my set IK. Here's a bunch of those realizations. Here's what the model looks like. So you see that it has some properties where the model still kind of resembles the average model, the model centered at C, but it's also doing a pretty good job of capturing something at XK, the point where I'm trying to, where I want this thing to be the expectation of. What is this model not? Here's a mini-batch model. I'm gonna take the exact same realizations and I'll show you what happens if I. Same realizations, and I'll show you what happens if I only sample and then re-weight all of the models that are centered at XK for this new batch. You can see that at XK, there's quite a bit more variance in the mini-batch model than there is in the ameliorated model. It's an important observation. And I'm not using the average model. I'm using the ameliorated model. Here, if I just show you the average model, you'll see that even after I updated, so I'm only going to just take the So, I'm only going to just take the average model, which is the sum of all the centers. If I look over here, it's still a pretty good model of the old center C, but it didn't do much to update XK. This is actually a well-known property of the stochastic average gradient, non-ameliorated model method as well. It's kind of slow to update if you only use the average model. So we're using the ameliorated model because it has these just generally good properties. He's 100. Yeah. Sorry. So you would expect then, with probability being five, I'm sorry, 0.05, there should be, in expectation, five in the set IK. All right, so what's the pointwise variance of this model? It's a reasonable question. Let me denote two additional things. I'm going to denote pi ij, which is now the probability that both i and j component functions belong to ik. And let me denote this quantity dik. And let me denote this quantity dik as the difference between the model predictions at the point x between the model centered at xk and the model centered at its old center point. Now, it's not hard to prove, but it's way too tedious for this talk. It's about a page-long proof, but it's really simple steps. The variance of this model is just given as this guy, which is still not easy to unpack. But I'll get to unpacking this in a second. I do want to point out there's an obvious problem where if I try to write down the exact expression for the variance, Down the exact expression for the variance. It depends on totally unknowable quantity because the whole point of this was trying not to, first of all, evaluate any or update the center so I have to do any new evaluations at the new point xk, but also I have to know this model difference at any given point x to compute the variance at this point. But you know, we're DFO people, and all model-based DFO is, is about bounding the quality of estimates. So we'll come back to this, but pretend for a hot set. Come back to this, but pretend for a hot second that we know how to evaluate DIK, that is the difference between these model predictions. So, we want a estimator that minimizes the variance. So, we're going to consider the regime of independent sampling. That is kind of just what I showed before. When you draw IK, I'm going to realize P independent Bernoulli with success parameter given by pi ik. And if i is in ik, then it belongs in the realization. So, under independent sampling, So, under independent sampling, pi ii is just pi i, and the product, the probability that both i and j are in the set is just the product of those probabilities because they're independent. So, you can easily see that the variance expression I gave, the complicated one, just simplifies to this guy, which is now only a sum over p many functions. So, a few quick sanity checks about this simplified expression. We're always going to assume the probability is greater than zero, so this is always a well-defined thing. The variant. The variance is always non-negative because probabilities are always properly less than one. So you'll notice that one over pi minus one has to be bigger than one. And notice that if pi ik is one, that is, the probability is one for all the component functions, and this variance is just zero, which makes sense because that means it's a deterministic model. So, how do I get a minimum variance estimator? Well, I gave you an expression for minimum variance. I can Variance. I can trivially express, like I said in my comment to Warren a second ago, the expected batch size of IK under independent sampling. It's just going to be the sum of the probabilities. So subject, the sum of the pi IK parameters. So subject to a fixed expected batch size B, I'm going to minimize my variance expression. So I just get this simple program. And just by using straightforward KKT conditions, you can get the analytic expression for the optimal solution to this program in terms of the probability. Solution to this program in terms of the probabilities. So the intuition is there's going to be some cutoff point C, which is determined just by evaluating this formula. If after I order the errors dik, if the i is bigger than c, it deterministically goes into my set. Otherwise, the smaller probabilities are going to be less than one, are all proportional to the size of the error at that point x. So a couple frustrations. First of all, So, a couple frustrations. First of all, expected batch size is terrible in practice because if you actually want a batch size of B so you don't over or underutilize computational resources, I mean, you can derive some tailbounds for how large your set IK is going to be a given realization, but that's not really satisfying. So we have some heuristics to basically say, and they're exactly the heuristics you'd predict, to say if the set is too small or too big, we're going to either add some additional components or subtract some. That's not. Components or subtract some. That's not really want to focus on. The bigger, glaringly obvious frustration is I still have to worry about this thing. But like I suggested earlier, I'm going to just replace it with computable upper bound. So it's easy to motivate this with just first order Taylor models. So in general, I mean, just using triangle inequality, I can upper bound that quantity using this guy, this E function that I have here, which is parametrized by both the incumbent, sorry. The incumbent, sorry, the current iterate and the center point. And if I just use first-order Taylor models, I know that as a function of Lipschitz constants, Li, I can just upper bound it like so. So I get these nice bounds that seem to roughly scale with Li, the Lipschitz gradient constant, multiplied by the trust region squared. Probably not too unexpected. And I can similarly do the same thing for point estimates. Point estimates. So if I want to evaluate the maximum of this expression over a trust region, I can do that, like I just said. And similarly, if I want to evaluate just two incumbent points, I can do it like this. So, I mean, that's referred to our Taylor models is easy to motivate. We can do the same thing with derivative-free models, obviously, or else I wouldn't be talking about it here. So we can drive some approximate bounds for derivative-free least squares minimization, like pounders, as well. And so we derive a resulting method. And so we derive a resulting method, Sampounders. The idea is that if you sampled i and ik an iteration, you're going to perform your geometry updates or whatever to get your new to get your new model. So you only update models for a given component function on the iterations where i belongs to ik. Now there's a lot of tedious notation. It's all in the paper, obviously, but the upper bounds are functions of what you'd expect. A function of interpolation point geometry, trust region radius, center point, incumbent point, and the trial. Center point, incumbent point, and the trial point for your estimates, and the radius on which your old model with the old center point was constructed. And of course, LI, the Lipschitz gradient constant, which, yes, it's problematic to assume you could possibly know Lipschitz constants in DFO, let alone even some derivative base settings, but we can dynamically approximate it and it's not a big deal. So we can construct models and estimates. This is just me trying to use what we've developed so far to pack this into the STORM framework. To pack this into the STORM framework. If you give me a computational resource size R and some algorithmic parameters C, which dictates accuracy and alpha, which dictates the probability, and you give me an initial realization of IK that has size R. I'm just going to grow IKR component functions at a time. And between each augmentation, I'm just going to check this very simple formula. And if this estimate of an upper bound of the variance model V is sufficiently small, I'll stop because I'll say the variance. Sufficiently small, I'll stop because I'll say the variance is low enough that I trust my model given the current parameters. All right, so let me just show how this works on a couple test functions, and that's actually how I'll conclude this talk. So first, I know this is a derivative free conference, but let's start with a derivative base just because it's simple to show. Let's look at logistic loss functions. So if I use gradient-based first-order Taylor models as the model class, I'm just going to generate a particular data set just with minus. Particular data set of just with minus one, one labels. And then I have this logistic loss function with the regularizer. Now I'm going to generate data according to three modes because I want to explore the behavior of this algorithm. First is the imbalanced mode, where all of the coefficients inside my logistic loss model are going to be from a mean with it's going to have mean zero and variance one. All right, I have a progressive mode where it's the same basic thing. Where it's the same basic thing, but I'm going to multiply that entry by i. So as you go from i ranges from one to p, you get progressively linearly larger entries in your logistic loss function, which is going to affect, obviously, the gradient Lipschitz constant. And I'm going to have an imbalanced mode where I'm going to take the very, everything's going to have the correct the random initialization. Oh, I lost one of the screens. Everything's going to have the correct initialization, but. Everyone's going to have the correct initialization, but the very last entry is going to multiply by 100. So you're going to have these wildly, the Larry Latt component is going to have a wildly different gradient Lipschitz constant than everything that came before it. And I'm just going to choose 128 to be the number of component functions and the dimension of the problem. So here's a couple sample runs. Now you'll notice that in the balanced generation case, what I'm showing in these plots is there's two different axes on the K, sorry, the K. On the K, sorry, the K, the X-axis is just showing the iteration count. The B's on the left-hand side is showing the number of the component function of 1 through 256. Oh, I just realized on the last slide I wrote 128, I meant there's 256 component functions here. And if there's a blue dot, then it means that in that iteration, I belong to IK, according to our dynamic prescribing algorithm. On the right-hand side, I'm just showing you the optimality gap. This is the difference between the function at the xk minus. The function at the xk minus the optimal solution value. So you'll notice there's kind of a sweet spot right around here between the 30th and 42nd iteration where the trust region is such a certain size that I don't need too many samples to get the variance where I want it. And I also don't need, and also the trust region hasn't gotten so small that I need to be very, very, I need to have extremely accurate models in order to satisfy these bounds. To satisfy these bounds, so in there, I get very sparse solutions that everywhere else, at least in the balanced mode, I'm going to have to sample a lot. I'm essentially sampling all 250 component functions on every iteration, except in the sweet spot. In progressive, I mean, what the on each of these pairs of plots that I'm showing in the right-hand side is essentially just the sum across the rows of these plots. You'll notice that this actually does scale somewhat. Scale somewhat. There's obviously some correspondence to the growth in the gradient Lipschitz constants going down. So, for instance, you sample the 256th much more than you sampled the first across the run of the algorithm. And you get somewhat more sparsity in what belonged to IK in each iteration. And then imbalanced is where you really start to see a lot of practical gains, because especially in the entire beginning run of the algorithm, the first 90 or so iterations, I very, very sparse. I very, very sparsely sample anything of i belonging to ik. And by the time I actually start having to sample all the component functions, I've already achieved like 10 to the minus six level accuracy. So of course in this paper, we had to compare to existing things. The closest comparative we could find that was going to be fair was to use chastic average gradient that can exploit Lipschitz constants and involves a line search. It's not It's not, it's still a stochastic gradient method. It still has bad expected worst case complexity. And these results also just show that, you know, this method just blows it out of the water. So the question that you really want to ask is that the dynamic sampling we're actually doing, does it actually accomplish its goals at all? So here, for given resource size, meaning the number of things I add to IK at once, I compare in the solid lines uniform methods versus uniform methods versus in the dotted lines, our dynamic method of generation. And you can see that for every pair of R across all types of data sets, we always do better by generating things dynamically than just by uniformly sampling things into IK on each iteration. So it's just some justification that what we're doing is same. What if you know your valid Lipschitz constants? Should you expect any improvement? At least in this balanced data set, no. Balanced data set? No. With progressive data sets, you actually see that if you don't have Lipschitz constants, you for these lower computational resource sizes, you do better, which maybe isn't surprising because if you use the globally valid gradient Lipschitz constant, that's obviously a very large overestimation of any real second derivative behavior. So it's probably actually harmful. It's probably actually harmful to use that information. And this is going to, so it's actually better to try to dynamically estimate this more of a local approximation. And in the imbalanced case, we observe some similar things where at least by the end of these optimization runs, they're roughly converging at the same rate. And I will get to this in a second. I'll show it. I want to at least show this Pounder's results and I'll discuss more what these plots mean. So for just the sake of the So, for just the sake of this presentation, I'm only going to focus on one class of functions. I'm going to show you generalized Rose and Brock functions, which are given by this expression. And the important thing to notice here is that in all the even components, the Lipschitz gradient constant of the inner function, not including the square, is zero. But I'm going to use the same kind of generation schemes of imbalanced, progressive, and balanced to try to manipulate the Lipschitz gradient constants to get interesting problems to look at. And you notice that then the leading, the gradient Lipschitz constant. The gradient Lipschitz constant scales with those coefficients. So we observe some similar things, and these should not be surprising that because in all of the even components, we very rarely need to sample it, right? Because it has a gradient Lipschitz constant of zero. It does not make sense to try to update the model because if you have a fully linear model, or actually use a linear interpolation model, I should say, it's going to keep being accurate everywhere. So there's never really a reason to update it in this algorithm. Really, reason to update it, and this algorithm realizes that again, we said we know it's the exact same trend: that the dynamic sampling makes way more sense than doing uniform random sampling. In every computational resource size, we're going to perform slightly better, actually significantly better on these. If you know valid Lipschitz constants, well, actually, it turns out that with the derivative free case, if you could magically know your Lipschitz constants, so we'd fed it to this algorithm instead of using the update rule. Algorithm instead of using the update rule, you do slightly better. So it's interesting to question why that's true for derivative free problems and not derivative base problems. And this is a general trend notice across problems. And this is the very last slide that I'm going to show. I want to dig a bit more into this because this is also an open conversation that I want to keep having because I'm trying to grow this idea. We use this in a recent paper to benchmark various nuclear calibration methods. Methods. And there's this concept that we call resource utilization plots. So, what we ask is: if you give me a machine of size mu, how many computational rounds, that is batches of size r, so I'm going, how many batches of size r does it take a problem, does it take to solve a problem to a desired accuracy here indicated by tau on that machine? So, obviously, so there's a couple problems. So, there's a couple problems with this setup that I would like to address in future work, which is this obviously assumes that you have a fully parallelizable thing, which means that the cost of performing any given R functions in parallel, they're all going to finish at the same time, which of course does not happen in practice. But let's just go with it. So, you'll notice that when R equals mu, that is the lower x-axis mu is equal to r, which is indicated in the left. Is equal to R, which is indicated in the legend, that's where a flat line is going to occur. Because if your machine size grows, but you're only willing to do R in your batch, there's obviously no benefit to doing a smaller computational batch than the size of the machine, which is why I get these flatline effects. On the left-hand axis, we're showing the number of rounds. So, first of all, it's worth noting that on this problem, Sam pounders with a batch size of 16, that's just deterministic pounders because you're updating all 16 models. Because you're updating all 16 models in every iteration. So, this green line with the stars is just deterministic pounders. And you'll see that on any given machine size, it is generally doing worse for the smaller batch methods of SAM pounders on that fixed size. And I mean, this is justification that, especially for this particular problem class we evaluated, I'll talk more about that in a second. I'll talk more about that in a second. This held on other classes of problems too. There is seriously a justification for using SAM pounders over a method like deterministic pounders. So some future directions, and this is where I'll end the talk. We tested SAM pounders and other functions and got similarly good results. Now, like we said, our motivation here really was nuclear calibration problems. And it is sometimes the case, actually it's frequently the case, that some observables are more sensitive to changes in parameters than others. In parameters than others, which you could translate to very different sizes of li. So it's not unreasonable to look at, for instance, that Rosenbrock function. I mean, maybe things aren't going to be totally linear, but they might be far more globally linear than other component functions. So this is not a crazy idea to try to use this kind of bounding procedure in a stochastic average gradient, or sorry, an ameliorated model setting to try to ameliorate that. That. So we are. I mean, I only just put this on archive last week. We do have some kind of testing framework ready to go to look at nuclear model calibration problems. I plan on applying these very soon because I really do want to explore these problems. This will also tell us something about the structure of the problems in terms of better quantifying really how sensitive the separate observables are to parameter changes. And another future direction, and I hope to get something out in a few months. I hope to get something out in a few months. I hope people in this room will hold me to that for some accountability. I've realized I can apply a very similar idea to ameliorated models to coordinates and arbitrary bases to get what I call a stochastic average subspace method. So this is like a coordinate descent method where I'm going to take averages of old models of the null space of the subspace that I'm looking in to be averaged with an updated model of a reduced subspace that I'm currently going to recenter. Subspace that I'm currently going to recenter in an ameliorated setting. We gratefully acknowledge our nuclei funding and thank you very much for listening.