And this log complexity is easily seen to imply unimodality if we have a positive numbers. Okay, and an even stronger property is the Newton inequalities. And this is stronger because the binomial And this is stronger because the binomial numbers themselves are log-concave. So, if we divide by them, then we get a stronger property. So, this Newton's inequalities implied log concavity. And what Newton proved was that, well, if this is a real rooted polynomial, then these are the elementary symmetric polynomials in the, well, they are always, but elementary symmetric polynomials of real. Symmetric polynomials of real inputs, and he proved that then the Newton inequality solved. So, the strongest properties of these are then real-rooted polynomials. So, being real-rooted then implies all these set of inequalities. So, knowing real roots, we know some things about the shape of the sequence. But then if we want to work with real-routed polynomials and see how we may deform them and whatnot, then we quickly fall out of univariate polynomials and we want to look at multivariate analogs of rebootedness. And this is what stability is. So we now take a polynomial with complex coefficients. With complex coefficients, commuting variables. And we look at the upper half pane. So when the imaginary parts are positive, then P is called stable if whenever all variables are in the upper half plane, then P is non-zero. So P is non-zero whenever all the variables are in the upper half plane. Upper half plane. And of course, well, if we have complex coefficients, then we sort of it doesn't matter much if we look at some other half plane. So we could rotate this half plane. So we could have the right half plane or the left half plane. And this is where control theory comes in. So in control theory, you're interested in when the half plane is the right half plane, I guess. I guess. So, for example, then this would be a stable polynomial because, well, both stability is, of course, since it's defined by being non-zero, then products of stable polynomials are stable again. And the product of two numbers in the upper half plane can never be a positive real number. Real number. So this has to be stable. And also, this is stable because if we plug in numbers in the upper half plane, then the imaginary part of this is positive. And we add to that something which has positive imaginary part again. So the imaginary part of the whole this thing is positive. So this is non-zero. And then And then the reason for why it's a multivariate analog of rerouting is well, if you take a real polynomial in one variable, then since the non-real zeros come in conjugate pairs, so if it's supposed to be non-zero here, then it's better be non-zero downstairs also. So all the zeros have to lie on the read line. So for one variable, So, for one variable and real coefficients, stability is equivalent to real rootedness, meaning that all the zeros are real. So, this is sort of the how I got into stable polynomials, being a multivariate analog of pre-rooted polynomials. And of course, if you sort of want to work, if you your motivation is to work with univariate polynomials, To work with univariate polynomials, then if you sort of extended your problem to multivariate polynomials and you want to go back again to real-routed polynomials, you do this by diagonalization. So if you have a stable polynomial with real coefficients, then if you set all the variables equal, then it better be stable again. And it's univariate, so it better be rerooted. So, it's going to be rerooted. And by convention, also, the identically zero polynomial is stable. And this convention is convenient because, well, then it makes the space of stable polynomials of bounded degree, then it makes this space closed. So it's a closed space by this. This Hurwitz theorem on the continuity of series. And this space can also be seen to have non-empty interior, so it has a full dimension. So the space of stable polynomial is a full dimensional space. Okay. Okay, so what other stable polynomials are? Well, for many purposes, this the class of determinantal polynomials are sort of sufficient. So if we take n plus one Hermitian, complex Hermitian M times N matrices, or we could take symmetric Real symmetric two, of course. So if A1 through AN are positive semi-definite, then we can form this polynomial, which is the determinant of a pencil of matrices. And this turns out to be stable. And maybe it could be instructive to have a short proof of that. So this by this So, this by this. So, we noticed on the previous slide that the space of stable polynomials is closed. So, we may assume by a little perturbation that A1 is positive definite. So, what do we prove? Well, we need to prove by definition now: if you take anything in HN, then we need to prove that. Then we need to prove that P evaluates to something non-zero. So we plug this in and we just divide this up in the real, well, in the yeah. We write this matrix as a red part and a blue part. And we see that this, since this is a sum of positive. Since this is a sum of positive, well, positive semi-definite matrices, and A1 is positive definite, then this B better be positive definite. So we can sort of conjugate by the B to the negative halves. And why do we want to do that? Well, so this is a Hermitian, this is a Hermitian matrix, so it has Hermitian matrix, so it has only real eigenvalues, but yeah, so negative i cannot be an eigenvalue, which means that this cannot be zero. Okay, so this simple observation just means that we can enter a large and interesting class of polynomials. So, all of these determinants of pencils, of Fermi matrices, are Fermion matrices are stable if we assume that these A1 through AN are positive semi-definite. Okay, so this then leads to the question, well, what about is the converse true? Well, so for n equals true, two, it's true. True, too, it's true. So if we just have two variables, then the stronger converse that we can actually choose the AIs to be positive, the AIs, the matrices to be symmetric is true. So this result of essentially Helton and Vinnikov, this proved the Lacks conjecture from 1958. And what it says is that, well, And what it says is that, well, it's the best of what you could ask for. So if you have a real polynomial of a degree at most d, then the following are equivalent. So the first statement is that P is stable. And the other one is that we can find three real symmetric D by D matrices, A, B, and C, with A and B positive semi-definite, so that P is the determinant. So, that P is the determinant of this pencil. So, in N, in two variables, we just have determinantal polynomials, but sort of the exact converse of this fails for more than three variables. And it's just to count the number of parameters. Well, since we have D times D matrices, this means that we just That we just have a certain number of parameters for this determinant. But we said that the space of stable polynomials has non-empty interior. So this has full dimension. And this means that we just don't have enough parameters for this to hold for more than two variables. Okay, so we. Okay, so we when n is greater than two, then we leave the world of determinantal polynomials and we yeah and yeah, so so so the question is then how you work with stable polynomials so you want to know how you know if you have you want to know what you can do with this with the Can do with this with the polynomial, and it still is stable. So, this was the motivating question for people that studied the Riemann hypothesis in the first half of the last century. So the Riemann hypothesis may be phrased as this: entire function can be approximated uniformly on compacts by real-rooted polynomials. Polynomics. So this is the gamma function, and this is the Riemann Z function. And this then motivated Hermit, Laguerre, Jensen, and Polyan Schur, and many others to study then linear operators that preserve B routines. Because then you can sort of then you can approach this entire function with elementary means and maybe you can sort of. Elementary means, and maybe you can sort of deform your, yeah. You study what you can do with these rerouted polynomials and also with entire functions that can be uniformly approximated by re-routed polynomials. So, I want to explain well one of the fundamental results of Polio and Sure, and this is. And this is the classification of so-called multiplier sequences. So, a sequence of real numbers is a multiplier sequence if this operator, just diagonal operator that maps x to the k to lambda k x to the k, if this operator preserves real rootedness. So for example, if lambda k is k, then what is then this operation is just taking the derivative and then multiplying by x. So it's x times d dx. And this is a multiply sequence because, well, it's easy to see by the sort of intermediate value theorem that dd. Theorem that d dx in between every two zeros of f is a zero of d dx. We have this interlacing. So this ddx preserves real rootedness and multiplying by x preserves real rootedness. So this lambda k equals k is then a multiplying sequence. So what did So, what did this polyunshore theorem say? Well, it says they gave three or two equivalent statements that characterizes the multiply sequence system. So, the first one is more of an algebraic form. It says that it suffices to check these particular polynomials. So suffice it to check that one plus x to the n is mapped to a real-rooted polynomial. And so this is in some form, well, this is then an algebraic characterization. And then the transcendental characterization is. Characterization is that if you if you instead so if you look at e to the x well e to the x is a limit of real rooted polynomials. Well, sorry. So since e to the x is the limit The limit of a scaled version of these ones. So the exponential, well, the transcendental characterization says that it's enough to check e to the x, that this is mapped to some rerooted entire function. So it says that this, well, a priori is just a formal power. Is just a formal power series. So, this formal power series should define an entire function, which is the limit uniform on compact sets of re-routed polynomials. So, this is then sort of a full characterization of diagonal operators preserving free rootedness. So, it says, so it gives you sort of a toolkit. So it gives you sort of a toolkit, what well, a partial toolkit on how you may deform rerooted polynomials. So now we want to look at stability preservers instead. So then we move to several variables and also for well. Also, for well, so we first describe sort of an algebraic algebraic characterization of stability preserves. So if we want to fix first the degrees, so we look at this linear space, which consists for then a given kappa of non-negative integers. We look at this finite-dimensional linear space of all complex. Complex all polynomials with complex coefficients in n variables for which the degree in xj is less than or equal to kappa j for all j. So this is a finite dimensional linear space. And we want to look at what, well, we want to look at linear operators on the space. So it's linear operators that map. Linear operators that map this finite dimensional space to well into some into any polynomial space. And we want to ask ourselves, when does this preserve stability? So to answer this, we look at sort of the multivariate analog of this algebraic symbol that Polly and Schur looked at. So, but we sort of But we sort of homogenize it or we add some extra variables yi's. So this is then the symbol. So it's just letting T act on this specific test polynomial. And if you don't like that definition, well, if you expand this, this is what you get. So you can take this as a definition. This is a definition, and here I use multivariate notation. So, this is just xi to the alpha i, the product of that. Okay. And then the characterization of stability per service that. Burcia and myself achieved in 2009 was then an analog of the Polyo-Schur, the algebraic version of Polyashur's characterization, but now for complex coefficients and for any number of variables. So it says that, well, we assume that t has rank at least two. There is a, you can consider rank less than rank less than two less than at least three I think no at least two yeah so otherwise t is just going to be sort of a the the the range of t is otherwise going to be just a constant polynomial and this has to in order for that to preserve stability this one polynomial has to be stable so this is not interesting but so we assume that the rank has at least two And then we could prove that T preserves stability if and only if this symbol is stable. So, what does this mean? Well, it means to preserve stability in n variables, you just have to check that your polynomial in, well, n plus n variables is itself stable. So you just have to see. Stable. So you just have to check the stability of one single polynomial. But that could be hard too. But it still gives us some sort of way of trying to work with stable polynomials. So I thought I should give an example that we will use later. So if we look at some So, if we look at some directional derivative where these coefficients are non-negative, then we want to see what we want to apply this theorem. So, we look at this GT and this is T of This is what we want to do. And if we do this correctly, so this we abbreviate this by x plus y to the kappa. Well, if we take the derivative of this and expand it, what do we get? Well, we get x plus y to the kappa sum. Okay, but if the imaginary part, so assume then now that the imaginary part of the X i's are positive and the imaginary part of the Y i's are positive, then the imaginary part of this guy Guy this is negative. So, so the so this is then a convex sum of of numbers which are in the in the lower half plane. So, this is in particular in the lower half plane again. So, this is going to be non-zero. So, g is stable. So Gt is stable, which then implies that T preserves stability. And this could, of course, be proved by other means, maybe by the Gauss-Lucas theorem or whatnot, but this is sort of just to illustrate how this theorem works. You just have to check one polynomial and see if it's stable. And this is what we This is what we did here. So these directional derivatives turn out to preserve stability. But then we can also look at real polynomials and see when they when a linear operator on them preserve stability. And then the theorem is. The theorem is very simple. So, again, T preserves real stability if and only if the symbol is stable or the sort of mirrored symbol. So we just have negative y's here instead is still. And this that we have two cases is the reason for that is that, well. If you have a sort of, well, it is exactly this that we have this negative x goes to negative x that is maps the upper half plane to the lower half plane and leaves the reline intact. So, then generally, so could in, for example, the Li Yang program, one is interested in other domains than just product of half-planes. So if we take some other domain, we can say that omega, well, a polynomial is omega-stable if it's non-zero whenever the variables are in omega. Variables are an omega. Sorry, Peter, there was a quick question coming up. So, can you explain one more time what real stability means? Real stability is just that the coefficients are real. It's stable, but we just look at real polynomials. So, all the coefficients are real. So, that's a real stable polynomial. Thank you. So then, but turns out that this classification extends to Cartesian products of half planes combined with disks or exterior of disks. So these are then the images of Merbius transformations. But other than that, it's sort of. It's sort of almost well. We can also, together with Matthew Chassie, we could extend the characterization to the exterior of strips. And this is, well, this is a topic that has been studied by, for example, Debru. Well, we should know. Sorry. So he was interested in, well, the Riemann hypothesis, you know, you wanted. Is, you know, you want to look at horizontal strips and see when the zeros are in such strips. So this means exactly that you're non-vanishing in exterior strips. So the Bruce studied linear operators preserving, having zeros, having zeros in a strip. So in several variables, this means that we're stable. We are stable with respect to the exterior of disks. So one can also extend the characterization to such domains, but other than that, it's widely open. And I guess, yeah. I think Tosha. I think Tosten Theobald will talk more about these omega-stable polynomials for certain, well, for omega is almost like the upper half plane, but instead of looking at the real upper half plane, you look at any cone, which is then conic stability and conic hyperbolicity. And he will tell you more about that. So, as we heard before my talk, that stability is connected to macro theory, and this is what I would like to talk about now. So So we first want to describe a notion of discrete convexity. So first we want to, well, so what is a step from if we take two, so we are now in the lattice C to the N. So if we have two elements of this lattice, then we want to talk about a step from alpha to beta. And this just means, well, And this just means, well, that you have, say, alpha here and beta there, and then you just move one unit step towards beta. And in words or in typing, this is what it is. So it's just a coordinate vector, plus minus a coordinate vector that you move in the direction from alpha to beta. Then a jump system is so again it's easier to draw a picture. So if we have so if you have alpha here, which is in so we want to describe when a subset of Zn is what we call a jump system and a jump system is then a discrete convex set. Set. So if we take alpha to be in J and then take beta to be in J2 and then say that we take a step from we take a step from alpha. I didn't want to do that. We take a step from alpha to beta. So a step was just something like, well, to here. And now And now, assume now that we have jumped out of the system. So we're no longer in the set. So we've taken a jump and this is then alpha plus S and this is not in J anymore. So say that now we're taking a step from alpha towards beta and now we know From alpha towards beta, and now we're not in the set anymore. Then we should find in some other direction. Say we should take a step t and then we should be able to jump into the system again. So this is then alpha plus s plus t. So it says that whenever you've you've moved from alpha to beta in such a way that you've In such a way that you've jumped out of the system, then you should be able to jump in again with a unit step towards beta. So recursively now, we can find sort of a path, a discrete path from alpha to beta. So this is some sort of discrete convexity which is studied in, well, Studied in well in discrete mathematics. And for the special case where J only consists of 0, 1 vectors and where the sum of the coefficients is constant equal to R then J is a jump system if and only if J is a set of only if J is a set of basis of a matroid. So this being a jump system is a generalization of matroids to other lattices, well to lattices and to more complex situations. So what's the So, what's the connection? Well, we can get a set of vectors in a lattice if we take the support of the polynomial. So if we have a polynomial, then the support of the polynomial is just the alphas which have non-zero Taylor coefficient. Sorry, Peta, again, a question on the last slide. The S and the T that you showed in the The S and the T that you showed in the picture, are they meant to be distinct? Um, or can S be equal to T? S can be T, so we could have moved here. Okay. That's so, yeah, so it could be the same. Thanks. And this is for univariate polynomials, this is it's going to be sort of. It's going to be sort of it has to be the same, or it could be the negative two. Yes, so now we want to relate polynomials to these jump systems. And we do that by taking the support of a polynomial. And the support of the polynomial is just the indices here or the exponent. Here, or the exponent for which we have a non-zero Taylor coefficient. And here is sort of the connection was first made by Joey, Oxley, Sokol, and Wagner in 2004. And they proved this: that if the support norm is inside the discrete simplex, and the discrete simplex was just these. These where we have zero one vectors, right? And where these where they sum to R. So this is a chance to be a matrix. So then they prove this that if this support of a polynomial is in the discrete sink. Discrete simplex, and this polynomial is stable, then the support is actually the set of bases of matroid. So, this then opened up this connection between matroid theory and stability, which has ever since been studied. And we call such macros hyperbolic, or about in this paper by Joey Oakes. This paper by Choi Oaks, the Sokol and Wagner, they talked about the weak half-pane property. But it will be hard to say that every time, so we call them hyperbolic. And I saw in the that Mario Kumer will later talk about the half-plane property on Wednesday or Thursday, I think. He has some news to tell us about that. So So we have this hyperbolic matrodes then, which are the ones that are the support of stable polynomials. I mean, well, you can also prove this. So this was not, this phenomenon didn't have anything to do with the discrete simplex. It holds in general that if we have a stable polynomial, then the support is always a jump system. It is always a jump system. So there was a question again in the chat. So exactly the discrete R simplex. So it means F is homogeneous and exactly what is homogeneous and multi-fi. So it actually is of degree at most one in each variable. So this condition is the same as saying that f is homogeneous. As saying that f is homogeneous of degree r and it's multi-affine, meaning that the degree of each variable occurs at most, well, is at most one. But the big question is then which matroids are hyperbolic? Hyperbolic for this connection and it yeah so first so one of the motivating examples of matroids is the so-called k-linear matroids and these are the ones that that come from a config so if we have say if we have So, if we have, say, if we have a matrix with columns V1 through Vn with entries in some field, so we suppose that this matrix has full rank, then we can construct a matroid out of that. Namely, we take all subsets B for which the corresponding vectors is a basis, and this is a set of basis of a k. And this is a set of bases of a k-linear matrix, and this is sort of the motivating example for matroids or one of them. So whenever we have a bunch of vectors in a linear space, then we have a k-linear metron. But then one can see sort of using these determinantal polynomials, we can see directly that if we're C linear. If we're C linear, meaning that this A is has it entries in C. Then I can use the Cushy-Binet theorem to expand this. So this is a these are rank one positive semi-definite matrices, right? So, we had this on one of the first slides, we had this theorem that said that determinants of this form are always stable. But we can read off the support of this. So, the support of these are exactly the ones that have non-zero determinants, meaning that Zero determinant, meaning that they are linearly independent, meaning that they are basis, right? So the support of this guy is this K-lin C-linear matroid that defines this matrix. So this says that any C linear matroid is hyperbolic. So this hyperbolic matrix is a generalization of C linearity. C linearity, but they are a nice class in several ways. So this class of hyperbolic matroids is closed under most matroid operations that you can consider. So duality and taking minors and direct sums and bridges and whatnot. And then one can prove that a binary matroid, so meaning binary matroid is that, so it's Z2 linear. So a binary matroid turns out to be hyperbolic if and only if it's regular, meaning that it's k linear for all k. And this is regular matroids is a well-studied class. So, this in particular means that we have a so not all binary matroids are regular. So, we have a large class of binary matroids which are not hyperbolic. So, a large class of matroids are not hyperbolic. And similarly, one can prove that. So, if we take any projective geometry, so this means that we look at all the vectors of a certain length with entries in k, where k is a finite field. And one can prove that they are never projected, they are never hyperbolic. Projected. They are never hyperbolic. So, for example, the Sephano plane consists of all vectors in Z2 with three in C2 to the three, I guess. So they are never hyperbolic. And then we have this famous Varmos. We have this famous Vamos metroid, which is famous for being a counterexample to several sort of natural questions in matroid theory. And well, so the definition is, so the basis of this Vamos metroid are all the four tuples of things here, which do not lie on one of these half-planes. So this is a basis for it. So, this is a basis, for example, but this is not a basis. So, this is the BAMOS matroid. And one can prove that this is not k-linear for any k. But nonetheless, Wagner and Wei proved that this is a hyperbolic matroid, meaning that it is the support of a hyperbolic or is the support of a stable polynomial. The support of a stable polynomial. And this fact was then used to come up with counterexamples to one of a question in real algebraic geometry called one of the generalized Lex conjectures, which was around that. So this sort of one. So, this sort of one can use this fact that this Vamos matroid V8 is not k linear for any k, but it's still hyperbolic to find sort of a hyperbolic polynomials for which no power of it is a determinantal polynomial. But I won't talk too much about that. Maybe Tosh Toshten will talk about spectra hegress, maybe he'll mention. He just and maybe he'll mention more about that. Yes, yes, I will. And I will also talk about the conics that you asked, you mentioned earlier that I will talk about this. The answer is yes. So now I determine what you will talk about. So, yeah, so this. Yeah, so this motivated us to, since we had this, these sort of gave us counterexamples to questions about representability of hyperbolic polynomials by determinants. Then, you know, it's interesting to find similar families of matroids which are not linear but hyperbolic. And together with Nima Mini, we Nimamini, we came up with a large class of matroids, one for any for any, what is it, one for any simple graph and one for any actually multi-graph too. So this is one example, then a general generalized Vamos matroid, which has this property that it's Which has this property that it's not linear but hyperbolic. Okay. But so what so I guess I don't have very much time, right? Much more time. That's true. We are coming to an end. I mean, there were a couple of questions in between, so why don't you take a few more minutes, maybe? But yeah. Few more minutes, maybe, but uh, yeah, I'll just uh mention quickly how sort of so what we've seen so far about the relationship between matroids and stable polynomials is that stable polynomials is a way of representing matroids. So, certain matroids could be seen as supports of stable polynomials, but we saw that not all of them are. But if we if we relax this property of being stable, This property of being stable to something called that we call Lorentzian polynomials. And the definition is in terms, we give the definition here in terms of stable polynomials. So say that we have a polynomial P, which is homogeneous and has non-negative coefficients. Then we say that this is Lorentzian if either the degree is less than one or less than or equal to one. Or less than or equal to one, or we get a stable polynomial whenever we differentiate down to quadratic with these directional derivatives, which we talked about earlier. So these are supposed to have positive coefficients. So, as I told you before, if p is stable itself and homogeneous. itself and homogeneous than these these so so all the these directional derivatives right the these preserved then the directional derivatives are stable and homogeneous so so this if you're stable then this yeah all these polynomials are again stable Are again stable. So stable polynomials are a subclass of Lorentzian polynomials. And they're sort of designed to be that. So homogeneous and stable polynomials are Lorentzian, but we have more like Minkowski volume polynomials and volume polynomials of projected varieties. But the thing is that they are characterized in terms Characterized in terms of jump system. So one can prove that a polynomial is Lorentzian if and only if the support is a jump system and whenever you differentiate down the coordinate axis, you get a stable polynomial or an identically zero polynomial. And there's a converse to this, namely that if we have a jump system and that this generating polynomial is homogeneous, then Homogeneous, then the polynomial is itself Lorentzian. So if we summarize these two theorems, it says that, well, for the case of matroids now, all matroids come are the supports of Lorentzian polynomials. This is what this says. So these Lorentzian polynomials, they characterize Um this matroids. So by extending the definition of stable polynomials to Lorentzian polynomials, we can in fact get all matroids, but nothing more. So I guess I'll stop here. And sorry for the last slides was a little quick. Last slides was a little quick. Yeah, so thank you so much for this wonderful talk. And let me give the opportunity to the audience to ask a couple of questions. You can use the chat, or I think now also you can just use your microphone if you would like to ask a direct question to Peter. Yeah, please, please go ahead. No, no, no, please. Yeah, so maybe Alicia, you had a question. I was just going to say that I enjoyed a lot the talk, so I will let Alex ask questions. Okay, so the determinant of a linear combination of good matrices is Is stable and the determinant is just the resultant of linear polynomials. So, can we somehow extend this fact to resultants of polynomials of higher degree? I would think not, but maybe someone in the audience has a more okay, so at least it is not obvious, right? No. Yeah, but but if it was to I'm sort of skeptical that you would get something new but let me say a stupid question that maybe is unrelated, but when you have one of these multi-affine Multi-affine homogeneous polynomials. Every monomial is a vertex of the Newton of the polynomial. And if you start putting polynomials of higher degrees, then this is not longer the case. And I think you will lose at least the combinatorics. No, is this unrelated to your question? Well, I don't. Well, I don't know for the one and but so both of these are so so the um the the supports that you get from uh from non-multi-affine polynomials is uh is called polymetrodes or M-convex set. And these the neutron polytopes of them are well studied in combinatorics too. So these are well studied. Too. So these are well-studied objects. And they sort of you don't lose much when going to. There's no difference in the picture between multi-affine polynomials and non-multi-affine polynomials. You can sort of go in between the two. But maybe this was not the question.