He didn't ask me to send them. I could send him a descriptive title or an alpha title, so he asked me to send them a so random RMT, random matrix theory, NLA. There's not going to be much NLA, but people, some more than others, than me know about how RMT relates to NLA, and MML, so modern machine learning. So one of the things I was struck by, sort of as randomized linear algebra progress, is how little random matrix theory you could get out of. Little random matrix theory you could get away with knowing. Right? So, okay, early on you need either to know enough to figure out how to use Rudolph Machine and then these entropy concentrations was a mess. And then, you know, fast forward a couple years later, and everything's easy. You just call a few black boxes that are making these concentration results, and again, you don't need to know anything. So you can modularize everything. And you can always do better on log factors and stuff if you tear them apart. And stuff that you tear apart, you know, stuff apart. But some people like that, and some people don't. But you could get away with not knowing much. And it was a lot of linear algebra, actually. And you used linear algebra in ways differently than the numerical analysis. You might ask, you know, types of bounds, multiplicative, et cetera. So I didn't do much random matrix theory. And then people in machine learning recently have been looking at it. And sort of roughly what they're doing is saying, you know, say something's Gaussian. Say something's Gaussian, which if you check it's sort of not, and then get a Marchenko-Prussure result, and then take some limit, you know, make your matrix infinitely wide, and then you know, feed it into something. So, which I found a little dissatisfying, but again, you don't need to know that. You could do a little stuff. And then there was a broad class of results. So, I'm going to talk the last little bit of a longer talk, but I wanted to sort of give you a sort of a practice of theory and more practice on sort of why some of the things will be. Why some of the things we'll be talking about here would be useful more generally, in particular, in the context of going beyond eigenvalue distributions to eigenvalues and eigenvectors and a range of other things people care about. All right. So, okay, so if you're a statistician, you might say, I'm going to hypothesize that there's one signal in the data, the rest is. Signal and the data, the rest is noise. And it's not uncommon in that case to get an eigenvalue distribution. It's an ESD, eigenvalue spectral distribution that looks something like this. That's your Marchenko-Pasteur thing, plus a spike. And the spike, you think, a signal and theorem under some scaling limit and so on. Pull off that spike, cut, I find the first, a sweep, I get the first. Cut, I find the first, a sweep, I get the first cluster. There's a lot of work sort of of that flavor. And sort of a baseline for that is: well, if there's no signal, if the signal is too weak, you know, you have Vigner semicircle or the rectangular versions of Marchenco Pasteur. There's different ways to quantify this. It could be a non-asymptotic version or an asymptotic version. I'll get to that a little bit. So, you could do two things. You could say, I'm going to posit a model. I'm going to think of the model as having that distribution, and then go. And then go find that thing. You could also, just, if you're a modern machine learner, just get a big system with lots of knobs and hyperparameters and press the button and optimize everything. And then you could, after the fact, go and look at the matrices you computed and you could say what's their eigenvalue distributions. Nothing looks like that. State-of-the-art pre-2013 oftentimes looked like this. It's a little bit more subtle than that, but let's look. It's a little bit more subtle than that, but let's just be generous and call it that. So, this will be a sort of an empirical claim about tens of thousands of models and layers and so on. So, there's exceptions, but the leading of the thing is on log-log scale, you're heavy-tailed. Think of a power law. There's a lot of details about power law, truncated parallel, but extremely heavy-tailed. If you look at the weights of this thing, the weights are roughly mean-centered, and they're statistically different than being Gaussian. They're statistically different than being Gaussian, but they're not wildly heavy-tailed. But the correlations are heavy-tailed, the eigenvalues. And they're correlations because the atrices are mean-centered. So, this got us me wondering, can you go beyond sort of what people are doing in the Gaussian universality class? So, I'm not going to answer that question in the next 20 minutes, but I think some of the tools we're talking about here capture concentration in a final way, so maybe we can. All right, so here's another sort of question that sort of was always bothering me, and it's spectral clustering. It's spectral clustering. So, spectral clustering says take the data, construct a matrix, adjacency, Laplacian, this, that. And what you have in the back of your mind is that the data looks something like this. And you say, if I give you a new data point, you know, there, it's got to be a minus unless you're reverse. There, you don't know. There, it's a plus, right? So, spectral clustering says construct a matrix from that, an adjacency matrix, compute the leading eigenvector, sweep along the leading eigenvector, you find Sweep along the leading eigenvector, you find that cluster. All right, so you probably know, so Joseph started yesterday saying, I'm going to talk about two dimensions, and three dimensions is very different than two. I'm in a million dimensions, and a million ones not so different than a million. But both are very different than two or three. So, question: Does the data look like that in a billion dimensions? So, that's an empirical question. So, you could go to a, if there's a practical answer, the answer is no. Oftentimes, there's a Answer, the answer is no. Oftentimes, there's a theoretical question, and you could set up your model such that basically the mean difference between two clusters scales faster than the variance grows. In which case, then you could say I have two clusters. I'm going to look at the distribution of distances between the clusters. So this is a histogram, and this is a distance, pairwise distance of data points. In truck cluster, it looks something like this. You could scale it and say, inter cluster, it looks something like this. Did I get that right? Something like this, did I get that right? Inters farther apart, cut, and that's sort of what this is. Inter is pretty close, inters pretty far. Of course, if you do that and you do the sweep cut, it works. Why? Basically, because you're in this situation. You could tweak things just a little bit, go to a different scaling limit, roughly n and d diverge together. If you're going to have the parameters of the size of your matrix, in which case, histogram inter looks like that. Like that. In intra looks like that, and that's me being a bad artist. They're just right on top of each other. Still do spectral clustering, go to million metrics, compute dragon vector sweep, perfectly separate. So is that a good or bad thing? So what's going on here? So, what you would say is: I mean, here the idea is the picture people would draw as well. I have a set of data points. Inter is very well connected. Well, connected. Intra is very weakly connected. What I'm saying is, in a lot of cases, that's false. This is what the data will look like. And yet, still the spectral clustering algorithm works. Why? And it can't be because you go to this scaling limit that shows that these two things separate, because they don't. They end up on top of each other like that. So I'm not going to solve the harder problem, I'll do the easier problem. But I think the tools, you know, the concentration tools that are under the hood, you know, That are under the hood, you know, maybe more general. So, this is a baby thing. There's going to be a whole bunch of other versions. What if you have a, you know, not just N and D, but N and D and M, some other parameter, because you have two layers together, something else scale. So it's going to be more general than just the setup. But as a very simple thing, think of the simplest thing in the world, and this breaks, and a lot of other things will break too. All right. Theory, practice, more practice, some comments. Okay, theories. Comments. Okay, theories. Large data, you want N and P and capital N. You know, if you're doing least square linear, it's your number of rows and columns. Large models have a million moving parts, but usually there's some effective dimensionality parameter and some effective model capacity parameter, but you may have intermediate layers. So say there's some other parameter, just the structure of the model itself may have something that diverges, and so you've got that too. Basically, at a high level, At a high level, if you're just doing least squares and your matrix, it becomes very rectangular very fast. So the number of rows is the number of columns squared, or number of columns times log number of columns or something. Something's going to concentrate in a much cleaner way. And so you'd be able to apply random matrix theory. And if it doesn't concentrate in that clean way, you're here. You still may build a little cluster. But then you get into, are we dealing with Johnson-Lindenstraw? So large numbers, a central limit theorem, or whatever. And I think there's going to be a lot of. Theorem, or whatever. And I think there's going to be a bunch of counterintuitive results. Sort of perversely, some of the algorithms work in this regime when you analyze them in this regime, which is either a bug or a feature, but it sort of gives a big theory of practice disconnect. And understanding the fine-scale structure of a lot of machine learning methods, if you do random matrix theory and get eigenvector information, which a lot of classical techniques do, you've got to get eigenvalue information, which a lot of classical techniques do. Then if you want to get an eigenvector and do a sweep cut. If you want to get an eigenvector and do a sweep cut, it's a whole one with a can of worms and so on. So it'd be nice to handle all this sort of in it in a single way. All right. Here's a picture of what I'm talking about. You can call it low dimension versus high dimension. Think of two regimes in high dimensions. One is where one dimension is much higher than the other. And for least squares, it could be that the number of rows grows much faster than the number of columns, or vice versa. I mean, or vice versa. A lot of times, if you read A lot of times, if you read about large language models in the New York Times, they're essentially in a regime we have many more parameters because parameters in compute are relatively inexpensive, and so you are in that regime, in which case the model is underdetermined, but you can control capacity the other way. So one of the two parameters is much larger than the other. And the hard regime maybe is where the two parameters scale together. This is the picture I had before. Maybe we'll do the normalization on that's better than this or something. But this is a picture we had before. Than this or something, but this is a picture we had before. And so, in a lot of cases, the data points are not sort of classifiable pairwise because pairwise information, they're all up on this, I had one on the previous slide, they're all up on that high-dimensional thing. And so not only are they the same distance from the origin, but all the pairwise distances are the same fluctuations. So they're not pairwise classifiable. The Euclidean distance is roughly the same. There's going to be an issue about the scaling. Are you going to allow The scaling, are you going to allow a large number of central limit theorems? A lot of the details will depend on that. But classification will remain possible. So, why is that? And so, can you get a general approach that can get eigenvalues as well as eigenvectors and a bunch of things? So, here's the picture. Here's that matrix. That's black. That's not. You know, the projector is not great, but it's good. This, you know, so we're a 500 by 5 matrix. That's pretty rectangular. Here's 500 by 250. That's almost square, yeah. That's almost square. Yeah. I do a bit of a question about this, which is like how much of these modern machine learning stuff actually is still running on eigenvectors? How much is running on eigenvectors? Just like I've seen stuff like rubber embeddings, which they just still quickly move. So just so fresh. I don't even know how that'll come forward. I mean, roughly what they do, what you do in state-of-the-art thing is you put down some transform model and pound out of the GPUs. You get. What they get is they just get straight straight level. They get you, they just get straight labeled, they really have a lot of distance. Well, so then you can ask: what problem did you solve? What I said here is pre-2013, the problem you solved was a spike. You may not have known it, but you just pressed the button with all the hyperparameters. Now you're solving something where the weights have a scale, but the correlations don't. So they're capturing this eigenvector information. They may not call LA pack as a black box, but can you understand something about the linear algebra that's going on here with a random matrix theory? Here with a random matrix theory. So they're certainly doing that. And they're doing it at scale and ways that stress test things analogously, maybe, to scientific computing, but certainly different detail. But they don't modularize it, and it's not explicitly called out as, well, I want numerical stability on the side index. That's not. At some point, I was looking at performance of these models with lib SDM. I turned out how libSDM does their regularizer as well. Has a huge effect on which one's the view better. So that's a bugger feature. There's a lot, a lot of hyperparameters. Every hyperparameter is an algorithmic parameter. It affects runtime. Every algorithm hyperparameters is a regulization parameter. It implicitly or explicitly gives you regularization. So teasing these things apart is something some people do and some things some people ignore. But certainly those things will have a big effect. But you still think that there is an eigenvector somewhere. I mean, either you close your eyes and can't figure out anything about what's going on, or you say there's some structure. Or you say there's some structure, these things are matrices, they're transforming, you pound on these things, you could ask what's going on here using eigenvector insight. I mean, what Wigner did when he introduced random matrix theory was to say, I want to understand the low-lying energy states of a nucleon with 200 nucleons, with a nucleon with 200 nucleons. Who knows what's going on here? I can barely go beyond hydrogen. I'll understand the low-lying energy levels. Take this plot, turn it on its side. Bullen has an energy level, boom there's an energy level, there's a lot of garbage. So it's just the same playbook using random matrix theory. The same playbook using random matrix theory to treat certain things as random and explicitly call out structure. I mean, our difference here is that there is clearly structure here. So, here using random matrix theory to model signal, not noise, but it's the same playbook as before. All right. So, think about it as a sample covariance matrix. I'll describe it in the covariance matrix. And this will highlight a few things. These slides are about a little bit old. I was hoping, we're running up a sort of Little bit older, I'm hoping because we're running up sort of an overview. I thought we were going to have it done by now, but we didn't, so I just went back to the old slides. So if you're interested in some details, the ideas are sort of captured in here. So think of three steps. One will be just covariance, which I'm calling XX transpose. One will be linear models. And so you can apply various techniques: churnoff, you know, what do you call it? You know, lump uh what do you call it? Marchenko pressure, variants of techniques here, asymptotic or not, this limit, this thermodynamic light limit where N and P diverge together, or or this the one where one grows larger than the other. For a lot of applications, you might want a black box, just a crank you can turn. You can think of x, you know, it goes to whatever, ax plus b. This is a general structure of linear models and affine transformation. You can do something analogous to the Steeltes transform. Analogous to just the Steeltes transform, but other things here potentially and get eigenvectors. It'll work directly with the resolvent, not necessarily the Steeltes transform. Get a bunch of other information for linear models. And then the question about non-linear models. And I'll say a little bit about that at the end, just depending on what Gunner says, the times stuff. But you can think of doing linearizations in high dimensions, not Taylor series expansions. Expansions. Okay, so what's going on here essentially is the following. You know, maximum likelihood sample covariance entry-wise, you just take the plug-in estimator if things converge entry-wise. Just let n go to say they converge entry-wise. Does that convert to a norm bound? Basically, it's going to be the question. So you get a bunch of results entry-wise in the matrix. Does that convert to a norm bound? The answer is yes, if you eat log factors. If something grows bigger, because you get Johnson. Something grows bigger because you get Johnson-Lindenstras, you get worst case over problem instances. The notion of a subspace embedding in randomized linear algebra basically says I'm just going to acute perturbation from high dimension to low. I'll eat those lock factors, do JL, and then that's just a sort of a black box that's turning parents. So that's false here because N and P are diverging together. You don't have enough concentration for that to converge. I mean, it might be the case that when you feed that into an iterative algorithm, you fail to get two dimensions, but the iterative algorithm could pick that up two steps later. But it's not a modularized black box that But it's not a modulized black box. And basically, it's just because norms are not equivalent. Norms are equivalent up to an n factor, but that n will kill this. So they're not algorithmically equivalent in that sense. So you're not going to get this norm concentration. All right. Probably all know this or have seen these sort of letters, you know, Marchenko-Pescher and just concentration of coherence matrices. Am I talking about a corner case or not? I mean, this is sort of a fair question. Maybe I'm just using this as an excuse to derive some sort of neat results. And maybe I'll justify and say it's not a pointer case, but I'll still use it to justify neat results. I think you're almost always in this case, right? Even if your aspect ratio is 100, square root of 100 is 10. 10 means that on this Marchenko Pestriski, you're not concentrated at 1, you're plus or minus 0.1, right? So you're always here. And so never mind. And so, never mind if you have a big training machine learning pipeline, you know, no engineer is going to want to eat a factor of 100 in compute time. These models cost $100 million to train. So you're always in this regime. A lot of old school stuff takes a resolvent, a regularized version of the inverse, and chews on it with a Steeltz transform. You get eigenvalues. If you want eigenvectors, you just have to do different things. Work directly with the resolvent. When you're in this resolution, The resolvent. When you're in this regime, I said you failed to concentrate, which is the case. On the other hand, you could say I concentrate just in a weaker sense. Meaning the matrix doesn't concentrate, but it concentrates if I promise to ask only certain questions of it. Basically, questions that map to a scalar, scales of transform, quadratic form questions. If I only ask quadratic form questions, which are eigenvectors and eigenvalues in linear algebra, then even though the object doesn't concentrate, that functional of it concentrates. So you can get eigenvectors, eigenvalues, inverses, trace. Eigen vectors, eigenvalues, inverses, trace, bilinear forms, integrations, a whole bunch of other stuff. Namely, the bread and butter of classification and regression and a range of other things people do. All right, so Juna's going to signal to me. So this is, I think, important because if you're in this regime and you say, I have a theorem and it works, but you're really in this regime, what you're asking is a worst case bound over problem instances. Machine learners ask for worst case over problem classes. The worst case over problem classes. What do you lose by eating that factor? And oftentimes you have phenomena that behave in very complicated ways, but the thing you used to bound it behaves trivially. And so VC type arguments get that. Johnson-Lindenstrust hides a lot. And so if you look at the internal structure, what's going on, depending on regularization parameters and aspect ratios, you can have very different trade-off points. I'm not going to explain all the x and y's, except to say here's a mean squared error. This is really a phase transition in terms of the trade-off between variance and body. The trade-off between variance and bias, and you get qualitatively different properties over here. Very easy to wash it out if you have a single regularization parameter and change it. Never mind if you have a thousand regulatation parameters. So it's easy to wash that out. But I think this finer scale analysis, which addresses this question, sort of is irrelevant for a lot of very practical things here. And I'm going to be at dinner at the next coffee break. So if you have a matrix and you feed it through a whole bunch of element y's nonlinearities. Of element-wise non-linearities. Is there nothing you can say, or is there something you can say? And if Gunnar changes his mind and gives me 10 more minutes, I'll tell you what you can say, but otherwise, get me at dinner or something. I don't care before dinner. All right. So, you can do a lot of nonlinear versions. Sorry, could you say one more time why the regularization of the last slide that impacted the double deception shape? How that explains the spectral gap between the bimodal and the. No, it doesn't. I'm in a bad regime. Okay. You know, it's easy. This is sort of a fact of the world. And if you've done random matrix theory theorem, proven, you know, or if you ever drank a glass of water with ice in it, Glass of water with ice in it, you know that you've got different phases. And so, what do you do with it? You know, do whatever you want. So, one thing people do in practice is regular eyes in lots of different ways, and it's easy to hide that. But a lot of stuff in machine learning says do such and such because I have some theory from BC theory that says such and such, and that's a little analogous to saying, you know, get a better, worst case bound, which may be good, and it pushes area forward, but you can over optimize to that, or say, well, I'm going to feed it into a linear algebra pipeline and do a little bit better. A pipeline and do a little bit better. So when you start to stress test things, it's easy to break it or it's easy to make it disappear. And just put a little bit of regularization and wash that out. Okay, anybody else? Mike, thank you very much again. 