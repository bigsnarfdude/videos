Yes. Okay. Okay. Yeah. So good afternoon, everyone. Yeah, thanks organizers for inviting me. So I wish I was in Oaxaca. So I really enjoyed my Oaxaca time last time when I attended this meeting. But I wasn't there, but I'm still happy to share my research. So today I will present one kind of like relative application. Relative application, like area I'm working on now, it's about the precision medicine in HIV. So, I will show how we can utilize the tools in Bayesian statistics and also data science to facilitate precision medicine in this specific disease area. Okay, and first I would like to thank all my collaborators, you know, my students, postdoc, and collaborators on this project, and also because we analyze data from MaxWhite's cohort studies, so I also would like to thank cohort study. So I also would like to thank all the participants on this study. Okay, so we know to determine the optimal sequence of treatment decisions or like any sequential decision making, conducting clinical trials is always a good standard. So however, it's often costly and may not be feasible in many scenarios. So luckily for us, the existence of large and diverse electronic And diverse electronic medical records or electronic health record data makes development of this data-driven clinical decisions possible. And what it's lacking now, I think it's like accurate and reliable models that can give us like this reliable sequential decisions. So my team has been developing these novel methods for optimizing sequential decisions in a wide range of medical applications. Medical applications. So today I will focus on HIV. Okay, so we know HIV stands for human immunodeficiency virus. So this is a virus that attacks humans immune cells such as T cell. So there's a pathway on how the virus attacks human cells. For example, it first, so you see here the number means the steps. So there's, so the first it bends to the host cell and then enter the host cell. cell and then enter the host cell. So after entering the host cell, the RNA is reverse transcribed to DNA and then integrate to host DNA and then replicate and make this viral in human cells. So luckily for us, we already have the antiretroviral therapy. We call it ART or ART. So these are medications to treat HIV. And currently there are more than like 30 FDA approved drugs for treating HIV. Drugs for treating HIV. And there are different types of drugs and that they target different mechanisms. So, for example, here, there is one famous type of the art drug. It's called nucleotide reverse transitase inhibitor or NRTS. So, it specifically prohibits the second step. So, basically, it's so, you know, literally from the name, you know, it inhibits the nucleoside reverse transparency step. That reverse trips pay step. And here I list all the drugs and their drug classes. So, like I said, you know, drugs belonging to the same drug class, they have similar mechanism for treating HIV. And their common drug classes are these five classes. So, NRTI, NRTI, and also the EI is entry inhibitors, and PI is proteins inhibitors, and also II is instead inhibitors. ACNC inhibitors. So, here is just some commonly used drugs. And interestingly, these two drugs were actually because they are all antiviral drugs. So, at the beginning of COVID-19, and they were used to treat COVID-19 in some clinical trials. Okay. And we know that this emergence of antiretroviral therapy or ART has transformed HIV infection from a fetal disease to a chronic disease, and they significantly reduce HIV-related mortality. Reduce HIV-related mortality. And among the ART drugs, they adopt this cocktail treatment approach because different drugs target from different classes, they target different, they have different targets. So people find this cocktail approach, so the combination of ART, they're mostly effective. So for example, an ART regimen may be two different NRTI drugs plus one instead drug. Or that could be two NRTI drugs. That could be two NRTI drugs plus one NRTI drug and plus PI. So that's a typical ART regimen. So we know the regimen for treating HIV people is a combination of drugs. So people leave every they are recommended to follow up with their physicians every half year. And at each visit, there are social demographics, lab tests, behavioral characteristics like smoking, substance use, or clinical vectors like environmental. Or clinical vectors like very low CD4 count, and they were all collected. And then physicians look at these measurements and then assign the ARQ regimen based on these clinical observations. And this process repeats every half year. And the U.S. Department of Health and Human Services, they provides a general guideline for assigning ART treatments. However, these guidelines are usually applied to treatment naive. Applied to treatment naive patients. So that means these were people who were newly diagnosed with HIV and never received any treatment before. However, for pre-treated people, so for example, the data we analyzed in this project is people who have been taking HIV drugs for like over a decade or even two decades. And the guideline, there's no clear consensus on how to treat these people. And also, the guideline, they mainly focus on the various They mainly focus on the variable suppression because you know, for the treating HIV, the major goal is suppressing the variance. But now we know they are most, you know, if this cocktail approach is very effective. And the people who are living with HIV, they basically have the same life expenses as the normal people. So aging is also a popular problem for HIV people. But these guidelines, they didn't consider these long-term side effects. So for example, they could have. Sad effects. So, for example, they could have sad effects on people's mental health or cognitive impairment, or could, you know, they're also like some life quality. And, you know, for example, like here I said, like sweating or, you know, serious infection and also kidney failure and the liver problem or lipid. So there are a lot of these long-term adverse effects of ART drugs may pause. And our goal is to determine the personalized ART regimen to optimize. ART regimen to optimize the long-term health in a data-driven manner because we have collected this data over decades and we want to utilize the information from the data to accompany these guidelines and to help physicians to treat patients. And also we want to develop a software, a platform, so that enables clinicians to, for example, when they input patient information, and then we can give recommendations. And also here I quote, you know, from I quote from the academy executive director. So he said, Our patient population is growing, HIV population, but the number of medical professionals specializing in HIV care is not keeping pace. So we found out, you know, for example, like the Baltimore area, we have a lot of physicians, their expertise is in HIV, but in suburb area, also like Middle, like Middle West, Middle East, so these parts of the United States. Parts of the United States, and people with HIV, they are usually treated by their primary physician, but their primary physician may not be expert in HIV, not to mention the people in developing countries like Africa countries. So we want to develop this software so people can kind of use this to, we're not, you know, replacing clinicians, but give them suggestions, you know, what could be their good combination for treating patients. Yeah. Okay. So before. Okay, so before I talk about the methodology, I would like to give a brief introduction about the data. So the data is called MAX-Wise Combine Cohort Study. So the MAX was a HIV cohort study that's mainly focused on men since 1984. And the WISE is a cohort study for focusing on women with HIV. And they recently just combined. So it's called the Max WISE Combined Cohort Study. And people there by average have. And people there by average have semi-annual visits. And here is one example. So, here we can see the x-axis is a calendar date of their visits, and the y-axis is the drugs they used. So, we can see over the time and the drug combination this person, the drug combination this person has been taking. And also we can see the trend, like at the beginning, people only use monotherapy back in 1996, but after that, people find out combination is most effective. Find out combination is most effective, so they start to use a combination of drugs from different drug classes. So, here, different colors represent different drug classes. And also, at each visit, we have recorded their health-related environments. So, for example, here, depression, very load, EGFR representing the kidney function, and here BMI. Of course, we have missing data. Okay. So, here we show these health environments and their ARP. Health environments and their ART use over time for these specific patients. And we have thousands of patients like this. Okay, so to optimize the sequential ART regimens in a data-driven manner, there are many challenges. So first, we need to estimate the effects of ART regimens from a high-dimensional and unbalanced space. So here, when I say high-dimensional, it means because we have more than 30 ART drugs on the market, and there are a lot of On the market, and there are a large number of possible drug combinations. Like in the data, we have thousands of possible drug combinations people have used. And on balance means here, some drug combinations, they are frequently used, but others are not. So for example, this drug combination is default T plus L A L. So they are all like a short name for the drugs, but I will just skip the full name. Okay, so these two RNTI drugs plus one PR drug, it was used almost 1000 times. Used almost 1,000 times. However, another similar drug combination, so two same ARPI drugs, but just a different PI drug, it was only used 12 times. So it's very unbalanced. And we want to estimate the effects of different ART regimens from this high-dimensional unbalanced space. And the second challenge is if we want to assign the regimen to patients, we need a way to generate a realistic ART regime. A realistic ART regimen from a large discrete space. So, because like let's say we have n possible ART regimens, so it's about like how to how we represent an ART regimen. So, assume we have n ART drugs, single drugs, and a natural way or straightforward way people would think is we can just use an n-dimensional binary vector to represent, right? So, if this specific combination have this drug, then we denote it as one. Otherwise, it's zero. Otherwise, it's zero. However, it doesn't work in practice. So, the reason is that if each, if it's a binary vector, and then there are two to the n possible values. So, think about two to the 30. There are too many possible combinations, and many of them will never be prescribed in practice. Because, like I said, we only have like it's a large number, but we like we have thousands of possible combinations. So, a lot of these boundary vectors. This battery vector representation will not actually show up in our data. So that's the second challenge. And the last challenge is about optimized treatments from observational data. So because you know the observational data and they generated from we call the behavior policy. However, we want to optimize a policy that may different from the behavior policy. So the data distribution would be different. And in machine learning language, people call it. in machine learning language people call it distribution shift so it basically is a distribution of data under the behavior policy that generates the data and the optimum policy they are different okay so those are the challenges so to solve the this problem of precision medicine HIV what we would like to do is we want to understand the ART effect and then we want to optimize the ART sequentially and then we need some ways to And then we need some ways to incorporate domain knowledge because there are guidelines there and we don't want our suggestions to be very different from guidelines. So our clinical collaborators has been telling us that we need to somehow incorporate the domain knowledge, like clinical guidelines, and also the literature into our model so that our recommendation is guideline compliant. And then our holy group will be, you know, if we have a very nice method that gives their good suggestions to planning nations and we build this solution. To clinicians, and we build this software, like user-friendly software, and then that can help you know help physicians and the patients in practice. Okay, so now let's go to the methodology part, like how we optimize this sequential decision. Okay, so remember we have these semi-annual visits. And to optimize the sequential decisions based on the observational data and in statistics, And in statistics, there's an area called dynamic treatment regimen. And all in reinforced learning is called offline reinforced learning. And offline reinforced learning, we know it has been super popular in the last few years. And it has been applied in different applications, like in medical applications, also like self-driving cars applications. So, you know, there are a lot of successes about using reflex learning. And there are two types of reflex learning methods people usually use. People usually use. So, one is model-based methods, and another one is model-free methods. And today, I will focus on the model-based methods here we developed because we want to see like you know how Bayesian statistics and other data science can be used. So, I will focus on model-based methods. But at the end, if we have time, I will briefly introduce another method our group is developing now, which is model-free methods. Okay, so here is an overview of the methods. Of the methods. So we have a two, we propose a two-step approach. And the first step is we develop a Bayesian dynamics model to model the dynamics of these health outcomes based on all the environments and people's ARP information, like treatment history. And then after we learn the dynamics with uncertainty quantification, and we propose a pessimistic environment with uncertainty penalization. With uncertain penalization to optimize the ART drug. So it's a two-step approach. Basically, the first step is we learn that we learn the environment, and the second step is we optimize the policy. But I think the key novelty part about our methodology is that we incorporate the uncertainty from the first step into our policy learning. Okay, so let's go to the details of our Let's go to the details of our methods. So, for the problem formulation, we assume. So, here are some notations. We assume xi0 to denotes to denote the baseline covariance for each person. And also, because we have a lot of kinetic data, so we have TI to represent TI1 to TIJI. So, that means patient I has JI visits. And also, we have a bunch of health-related variables. We assume it's an M-dimensional variable, so it's YI. Dimensional variable, so it's YI1 to YIJI. So at each visit, we have the observations. And also, we have the ZI to denote the treatment. We call here a combination ART regimen used by individual during the time period from the previous visit until the current visit. Okay, so that's the notation. And here is our data, assuming we have capital I patients, so here is all the data. All the data. And also, we use this bar. So we follow the convention of the notation. So we have Yij bar to represent all the history, the measurement history before the G's visit, and the Zij bar to denote the treatment history before the visit J. And also we assume the health-related environments, they follow a dynamic model. This dynamic model is conditional on all past history of environments and also. Past the history of measurements and also the treatment. And it's parameters by phi. And our goal is to optimize ART assignments to maximize the individual's long-term health outcomes. And we know it's actually an optimization problem, right? So we want to find the sequential actions that can maximize certain rewards. And here, let's formalize this into optimization like using methodology. This into optimization, like using mathematical language to formulate this problem. So assume for any individual I, she already has JI visits, and then let the YINU and the ZINU be the future longitudinal states and ART regimens. And then assume our policy, we are going to optimize, we have a way to parameterize that, a parameterize by theta. So it's Zij plus one, means the treatment. IG plus one means the treatment for the next visit conditions on the whole past history of health-related environments and the past history. And we use this part in our policy function. And also assume everyone receives a stochastic reward function, RI. So it's a function of the YI mu, because YI remember is health-related environments. So we use a function of these health measurements as a way to quantify these people's overall health. These people's overall health. Okay, and then the expected reward for any individual I can be represented at these forms. So Ri theta, because here the policy function is parametered by theta. So we're going to kind of like find the optimal theta that can maximize the reward. Okay, so here's R i theta. It's the expectation of y nu z i knew. And here, assume we have a probability function. Assume we have a probability function, we know the distribution of y nu, z i nu, and also here's the reward function. And here, p and here the p phi condition on d is uncertainty. Remember, our dynamics model is parametrized by phi, right? So we assume we can learn some joint distribution of y and u, z and nu from the data, and then it's parametrized by phi. And by the Bayesian model, we know the uncertainty of phi. So here, Of five. So here is how we define the R data. So basically, you see the expectation is respect to the new policy and the environments, but it integrates out the uncertainty from the dynamics model. So that's the way how we incorporate the uncertainty in the environment to the policy optimization. And our goal is to find a theta i star that can maximize r i theta. And also remember here, the note here r is indexed by i. Note here, R is indexed by I. So that's how we achieve the precision medicine because for different person, and this data I star may be different. Okay, so that's kind of like the overall framework. So that's how we formalize the problem into an optimization problem. Okay, so we want to maximize this expected reward. We want to find the theta to maximize the expected reward. And a typical way to solve this problem is policy gradient. Is policy gradient. And because we have a way to parametrize the policy, so we can use a policy gradient. And for the policy gradient, we can use the stochastic gradient descent, right? So for example, if we can calculate the derivative of Ri theta, with respect to theta, and then we have the ways to update the theta. Okay, so here the key problem is how to calculate the derivative of expected reward with respect to theta. With respect to theta. And luckily for us, we have this r i theta. So we see it's a formular, and we can actually derive the derivative of r i theta by the you know this policy gradient method. It's very simple derivation. But I know here it's a long formula, but we can decompose that into different parts. For example, here, this part is a log policy. So remember, we have the parametrized policy function, so it's pi, z. So it's pi z conditional y z theta. So it's a log policy function and respect to theta. And also, yeah, here, if we can parameterize the policy function, then we can calculate the derivative. And another part is this joint distribution or the predictive distribution of y and u z i nu. So as long as we have a way to sample future states, and then we can also get this part. And the last part is the RIY nu. Is the RIY mu. So we need to define reward function. It's like clinically meaningful reward function. So essentially, we decompose our task of calculating the derivatives at these three parts. So as R as we have the ways to parameterize the policy function, to define the reward function, and the way to sample future states, and then we can plug in them and calculate derivative, and then we can just directly use the stochastic degraded descent to get the optimal theta. So, that is the optimal theta. Okay, so in the next part of the talk, I will introduce each of these three elements. Okay, so for the first part, how to sample the future states? And we propose, so remember we have this multiple longitudinal health measurements. So, here we propose to use multivariate Gaussian process. Okay, so the reason why we use the multivariate Gaussian process is because it's a popular choice for modeling this irregularly. This irregularly spaced multivariate large tunnel data. And also, because we have missing data, and the multivariate Gaussian process has, you know, because we have this MCMC, so even you have missing data, but all the posterior updates are still in closed form. Of course, you can use other any, like, because we have this two-step approach. So, even if you don't like the multivariate Gaussian process, you can still use other models to model this large tunnel data. So, it's just our choice. No data, so it's just our choice, okay. Uh, so for the marked vertical Gaussian process, uh, here is not exactly the model we use, but here I just I kind of choose a simplified version to like demonstrate our method. Uh, but uh, yeah, the details are in the paper, so because it's the actual model is a little bit more complex, so I just show an illustration. So, essentially, assume like we have yij, so and we have m. So, and we have M health measurements. So, we basically model that by different components. And the first component is non-ART effect. So, we include all the demographics, behavioral characteristics, and the clinical variables into this term. And the difficult part is how we model the ART combination effect. Because remember, the combination effect basically is like a combinatory of different drugs, like A plus B plus C. Drugs like A plus B plus C. So here is a key. And to model this AR key combination effects, this term needs to have two properties. The first one is sharing of information. So meaning if two drug combinations, they are similar, they should have similar effects. So we need, so if we can borrow information from other similar drug combinations, that would improve the efficiency of the model. And another property. And another property is to reduce the high-dimensionality because we have thousands of possible drug combinations and we need ways to reduce the dimension. And here is our method. So we basically propose reduce dimension using, it's like the splan or kernel idea. And basically, we introduce a set of, we call the representative ART regimens. And here we find, we We define these capital D representative ART regimens. So define it's kind of like the knots, right? So by defining these representative ART regimens, so we reduce the whole dimension of all the drug combinations to only capital D. And also here, we calculate the regimen similarity between any drug regimen with Z D. And I will introduce the similarity function later. And then this gamma ID. And then this gamma ID is just the parameters. Okay, so that's the way how we kind of like propose to borrow information from other drug combinations and also how we reduce the dimensionality. And the last two parts is simple. So like here is a Gaussian process, like we consider, you know, along the time and also the dependence among different measurements. And here's just the independent noise. Okay, so we see the first. Okay, so we see the first green parts and these pink parts, they are all standard statistical models, and it is focused on how we model the second part. Okay, yeah, here it is normal and here's gotten process. So for the regiment similarity, so the key part how we define the regimen similarity. Okay, so a straightforward way would be a linear kernel. So the linear kernel means we just compute the proportion of common drugs that two regimens Of common drugs that two regimens share for the similarity. So, for example, for these three drug combinations, we can, we know like here, they have three drugs and they share the two common drugs, people, t plus LAM. And then by linear kernel, the similarity will be two over three, right? However, the linear kernel has disadvantage or limitations. So, limitation is that we can see the first here, this default T plus L M plus N F base. D4T plus LOM plus NFV. So this 2NRKI plus 1PI. And the third drug is also 2NRKI plus 1PI. And but for the second one, this EFV, it belongs to an NRKI drug class. So like I said at the beginning, the drugs belonging to the same drug class, they share the similar mechanisms. So we would expect the two drugs on the left, they should share a higher similarity compared to them. Compared to them between them with the second one, this EF is the right one. And also, for example, for these two, for these two drug combinations, if we use linear kernel, the similarity would be zero. However, both of them, they have two NRTI drugs and one PR drug. And because they share similar mechanisms, and we would expect they have certain similarity, even small, but they still. Even small, but they still share similarity. But the linear kernel cannot capture that. Okay, so to model this kind of like to capture these properties, we propose to use subset tree kernel. And the subset tree kernel was initially developed in natural language processing to represent sequence structures. And here we borrow the idea to represent the ART drug. So here we can see we have a tree step. So, here we can see we have a tree structure for each ART regimen. So, the first level is just ART, and the second level is the different drug classes appearing in this ART regimen. And the third level is a number of drugs in each drug class. And the last level is the detailed drug in this drug class. Okay, so we can see by this tree representation, like the blue color, we can see that the similarity between regimen A and B. Between regimen A and B. And for the yellow part, like regimen A and C, they don't share any single drug, but the yellow part, these two drug combinations, they all have two NRTI drugs. So the tree kernel can capture these kind of similarities. Okay, so here we show the similarity score based on this regimen A, B, C. So we can see this A and the B has high similarity compared to A and C. And also Compared to A and C. And also, the soft similarity of C is larger than the soft similarity A and B because the regular C has four drugs, and we know it's more difficult for the four drugs to be the same. So this is a similarity. Okay, so now we have the first part, like we have the, we propose a model and then we know how to sample future states. And now let's talk about the second part: how can we define a reward function. We define the reward function. And in HIV clinical practice, so many factors contribute to long-term health and quality of life. So, of course, the suppressing the virulo is a primary goal. And also, they are also at increased risk of many competitives such as kidney disease and lipid disease. And also, there are mental health adverse effects. So, we define our reward function based on very low kidney function and the depression in the next. Kidney function and the depression in the next two years. So, here we choose next two years because we don't want to focus on too long from now because new drugs are coming out and then we want to focus on a future but you know near future. But other factors can be easily incorporated. Okay, and here is a reward function. So we can see here is GF plus one to GF plus four because they have semi-annual visits, so like every half year. So, like every half year, and this depression, very loaded at the EGFR, and the depression is smaller the better. But for very loaded at the EGFR, as long as they are in like normal range, we don't penalize. And if they are outside of normal range, we penalize. So we can say here, we use this as abnormal thresholds to penalize these two measurements are beyond the normal range. And also the weight determines the relative importance. Relative importance. So, for example, if these people, this person has the kidney function doesn't, you know, isn't really well good, and then we can have higher weights on the EGFR and relatively lower weights on very low depression. Okay, and also because we have this distribution shift issue in the offline reinforcing learning, and also we have this probability model in the first step, and we propose this uncertainty penalizer rule. Propose this uncertainty penalizer reward. So essentially, the idea is like this. So we define this Ri tilde as a new reward function. It's Riy nu minus this lambda times the uncertainty of the y i nu, z i nu. So this uncertainty, basically you can think about it, it's like if some drug combinations, they were rarely observed in the data, then they would have high uncertainty from the multivariate Gaussian process, and then we will penalize the reward. And then we will penalize the reward. So that's kind of like the motivation. And lambda is a Toulon parameter. And then, because we have the multi-barter-Gaussian process, so we can just write down this uncertainty and calculate this one from the posterior predictive distribution. Okay, so now we have ways to sample future states and we have defined a reward function. The only step that is left is how to parametrize the policy function. Is how to parametrize the policy function. Okay, so the prior to parameterize the policy function, we have this, again, it's like the tree type of structure, but it's like we mimic the, you know, in clinical practice how physicians treat people. So basically, when a new person comes in, they will decide, okay, they ask what regimen you are using now, and then decide whether you need to switch the regimen or not. Because if the current regimen, everything is normal, you can just Regimen, everything is normal, you can just keep using the current regimen. So we basically have this way to model. So, like, if everything is a normal range, you just keep using that. Otherwise, we need to switch. So, if we need to switch a new regimen, then we need a way to generate a new drug combination. You know, how can we generate a new drug combination? And that's how we kind of like, again, utilize our tree structure. So, essentially, we We generate that by two steps. The first step is: we first determine which drug classes in each, which drug classes are used in each regimen and how many drugs in that drug class. And it's basically a multiple class logistic regression model. And then we decide the last level, like which individual drugs within each class. So it's basically a non-central hypergeometric distribution. And of course, this basically models our actions probability control. The actions probability conditional of the past history. Yeah, I will skip the formula. Okay, so now we have done the three parts. So we know how to sample future states and we have defined rule function and we have ways to parametrize the policy function. And then, you know, we can just plug in these terms into the derivative of R theta and then plug it into the stochastic gradient descent. And we just perform the SGD and then get the optimal theta. And then get the optimal theta. Okay, so I know the model is complex, but essentially, you know, the way I want to emphasize is like the key part I would say is like, you know, we can copy this uncertainty in the sampling model into the policy optimization. Okay, so now let me show the results. So here is one example about like how we apply our method to the data. So of course we did a simulation, but I skipped that part because it's expected. Because you know, it's expected our method performs better, and we compare to several alternative methods. I'll just skip that part just to show one real example. And for this person, she already had 31 visits. So, here is the drug history this person has been using. And this person's health environment is relatively, I would say, balanced. So, we give one-third, one-third, one-third weight on the three parts. So, here is So, here is the SGD iteration results. So, you see, around like 1000, it's kind of like a convergence. And then we can also output the optimal regimen for the next four visits. So, we can see we first propose to switch this one, and then for the next three visits, we suggest to add Kobe. So, that's a new regimen. And also, we test how our method performs in, like, for example, if we used this regime. We used this regimen and what would happen? So, here we calculate the predictive depression score under the estimated optimal regimens, and we found out, for example, like depression if under the under our recommended regimen compared to if this person keeps using the current regimen, and that would be 23% improvement. Okay, yeah, maybe I just have two minutes to introduce. Minutes to introduce some, like you know, the work we are currently doing now. So, we developed that method and we test the method on our data and we build a software, but we found out it's super slow. I think you can imagine, right? We have a multivariate Daussian process in the first step and we do the MSMC and the second step, we need to do this SJD calculation. So, this derivative calculation. So, it's very slow. So, MAC group is like we want to develop a relatively fast map group. Relatively fast method. So, you know, actually, the clinician doesn't need to wait like half an hour to wait for the recommendation. And here is our method we call the HIV AIPR. So it's a model-free method because we found out, you know, this multi-party Dawson process is the most time consuming part. So we propose this model-free method. Essentially, again, we still have all the data here, and we, again, we use sub-trick. And we again use subtree kernel representation to represent each drug regimen. But now we have this history dependent because traditional reforms and learning methods are all based on MDP, Markov decision process assumption. However, our process is not Markov because every step depends on all the histories. So we propose a history-dependent reinforcing learning method and then we model like we develop this method and a new patient comes in and we can apply this method. comes in and we can apply this method and then outputs the optimal ART regimen. And we found this one is like very fast. Okay and also there's one extension I would like to talk about and we're currently working on because like you know now we talk about how to understand the ART effect and also how can we optimize the ART, but how to effectively incorporate the domain knowledge, like everything, the guideline, there's still like we haven't achieved that part. Haven't achieved that part. So, another project we're currently working on is like we want to use like the you know, the recently large language model is very popular. So, we want to combine the large language model and the reinforced learning and the human feedback to improve our reinforced learning. Basically, it's like we have the clinicians and we have our reinforced learning algorithm. So, after we have, you know, after we train our model and, you know, and this reward. Model, and you know, and this reward the model, and we get the output, and then we will send the output to the clinicians, and the clinicians can help us to rank our recommendations. And then based on the ranking data, and then it will again use in their like our reward model and then further improve. So, you can think about it's like kind of like feedback loop. Uh, so that's also a part of what we are currently working on. Okay, yeah, that's all. Thank you. Thank you so much, Yanshun. So I want to open the floor up for any questions in the audience. Sorry, apparently this was not on. So, what I was saying is on the part where you said that you replaced the question process because it was very costly, and then you costly and then you did it model free but what exactly I mean because you still have the space right of the you still need to you still need to have something there that cannot to tie in the covariates with the reward but what do you replace the Gaussian process with you mean they're so like for their you mean like You mean like for the reward function part? Like for the how it when we define the reward, how we incorporate the information from the multi-variety Gaussian process? Yeah, exactly. Because you said that, so when you say it's model, the part where you say you modified it to be faster, but so it was model free. If you eliminate the multivariate Gaussian process, I mean, there's still like you still need something there, but what do you use? Oh, yes. Oh, yeah, so for the SEO problem. So for the model three part, so like here, yeah, we get rid of the environment model. So like, you know, in general, we have, we can use model-based and the model-free method. For the model free, essentially, you directly model the Q function. So the Q function is a function of the state and action. So essentially, you can think about we directly model the Q H A. So H is. Q H A, so H is denotes all the history. And we use some deep learning, like neural network models for this QSA, so for this action value function. But then the covariate information disappears in there. Oh, yeah, so all the covariates would be also in the state. So you can think about the state. So S here, like at each time point, S will have all the covariates. Have all the covariates and also all the health-related environments. So everything will be in S, the state. Okay, I see. Thank you. I think we have time for one more question. Hi. Can you hear me okay? Yep. Thank you for the wonderful talk. Oh, this is Armin, by the way. I don't know. Oh, this is Armitage, by the way. I don't know. I think we may have met Lisba at some point years ago. So, this idea of the similarity of the treatment. So, essentially, for my causal brain, you're trying to get around a positivity violation, right? For some treatment assignments, there's zero probability of getting, or near zero, or near positivity violation. Oh, sorry, I could you could you speak a little bit loudly? Could you speak a little bit loudly? I could hear you at the end. Yeah. Can you hear me okay? Yeah, right now he's okay. Yeah. So my understanding is that you're that some of these treatment combinations are sparse, which amounts to near violation of positivity. Yes. So is the sim, you explain how you're using that U function to penalize sort of treatment combinations that are on the boundary? But it's sort of that are on the bounds of those positivity violations that you might be more certain about. Does it depend a lot on the choice of the sorry, it's fitting again? Okay, how about this? Is this yeah, yeah, now it works again. It's kind of like the, I feel like the voice is fading. All right, yeah. Yeah, so um. Yeah, so is the U function to penalize for the sparsity of the treatment combinations? There's the choice of that U, like how do you choose this U function in the reward? Yeah, so U function is a very good question. Yeah, so U function, we have many ways to choose. So here, because we have a Martiverte-Gaussian process at the, you know, for the model. At the, you know, for the model, so for this one, I think it's kind of like the relatively, I would say it's straightforward choice because we can calculate the variance because we have this posterior predict distribution from multi-variety-Gaussian process. So we just get this as, you know, square root of variance, kind of like standard deviation. So it's similar to the idea of, you know, in reinforcement learning, people just, when they do the pessimistic reflex of learning, you penalize certain, you know, some. Certain, you know, some curing parameter time standard deviation. But I think that's a very good question because I think that should be a more elegant way to incorporate the whole uncertainty. Because here it's kind of like still, so even though we have the uncertainty quantification from the multi-barter-Gaussian process, but it's kind of like still a kind of point estimate, right? So like you said, it's just a choice. But I think we're currently working on another project. Another project is similar here, but we are trying to see the way how we can actually, for example, here it's like kind of like an integrate out the distribution of the y nu, z i nu with respect to the theta. I think that would be a more elegant way to encourage the uncertainty. So, here it's just a point estimate, so just one choice. But you can have other choice, for example, like some upper bounds or you know, like 75% quantile. Like 75% quantile. So there are a lot of choices here. Yeah. It seems like the uncertainty that's coming from this is uncertainty that's resulting from a positivity violation in your data. In which case, a lot of the uncertainties come from model extrapolation. You just don't have enough data for certain treatment combinations. Yes. But at the end of the day, you're just getting a point estimate. You're getting a single optimal reward, right? Optimal reward, right? You're taking the whole posterior distribution, you're saying this is the optimal. And in reality, we have uncertainty about which treatment combination is optimal. So in some sense, another way around it could be to maybe get a distribution. I don't know. Yeah, I think that's a good question. I think another way make our so here we're not being giving one deterministic recommendation because remember we are optimizing theta. So theta, we Theta. So, theta, we still have a policy function, right? So, that we're plugging in theta, it still gives uncertainty of the regimens. So, in the results, what I presented is just like, I would say MAP estimates. It's like we're plugging in theta and you still get the probability of each regimen. I see. Yeah, but I wouldn't see here, you know, the clinicians, they don't. Talk to the clinicians, they don't really like this kind of stochastic part because you know they're not sure which one they're going to use. Yeah, I feel like there's probably a balance of between uncertainty of the actions versus recommendation, you know, we actually want to give in practice. Yeah. Yeah, thanks so much. I'll stop hogging the microphone now. Thank you. So let's thank Yanshun again for the great presentation. Thank you. Right, so for further discussion, we can take it offline on Slack. So let's move on to the next presentation of the day.