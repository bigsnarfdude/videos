I think we let's wait for about an hour with a short break at the half record. And I would like to remind you that this lecture is being recorded. And so please turn off your camera if you don't wish to be seen. Yeah, let's begin. Thanks. So let me share my screen. Thanks. So let me share my screen. So to remind you what we were doing last time, so we're interested in understanding the past of the uniform spanning forest. So remember, we have this oriented version of the spanning forest. So you should be able to see this here, where every vertex has one arrow pointing out of it and some random number pointing in. And the future of the And the future of the vertex is when you follow those arrows forward, starting from the vertex, and the past is everything that you are in the future of. So the past of the origin is the set of V such that the origin is in the future of V. Okay, and one-endedness is equivalent to the past being finite. So what we saw last time is that the past of the origin is always finite when you're in at least three dimensions. In fact, it's true in two as well, but the proof is different. True and two as well, but the proof is different. And we also proved a quantitative bound saying that the probability that the past survives to n generations, so that you have a path of length n, simple path of length n in the past, is polynomially small in n, with some power that depends on the dimension. Okay, and the way we did this is with the Insulation Alice-Proder algorithm. So remember the Alice-Broder algorithm. So remember, the interlacements is this Poisson process of double infinite random walk trajectories that we defined as a kind of a limit of random walk excursions on these finite graphs with wide boundary conditions after a Poissonian time change. And the way you compute the forest from the interlacements is for each time t, which you can choose. Of which you can choose. You look at the first time each vertex is visited by some trajectory, and then you look at the first edge that that trajectory took to get into the vertex, and you include that in the forest, but backwards. Okay, and the theorem says that for each tea that you do this with, you get a copy of the wide uniform spanning forest. So, in particular, we can change tea and we get a dynamical version of the wide spanning forest. And these dynamics And these dynamics are helpful for analyzing the questions about the past, even though those are static questions. Introducing the dynamics actually makes them easier to understand. And in particular, we had this picture that the past of the origin sort of can jump when the origin is itself hit by a trajectory, but in between times, as we run time backwards, it gets It gets monotonically smaller, so it can jump up when the origins hit, otherwise, it's getting monotonically smaller. And we kind of understand exactly how this works. When a trajectory hits part of your past, the part of the past that it hits, it kind of steals away from you. So everything below there gets cut off. And so we use this to prove this quantitative version of the one-ometimes theorem. One endness theorem, where we had this dynamical thing where either the origin gets hit within time epsilon, which has probability epsilon, or it doesn't. And then basically you have to have one of these paths that's there at time epsilon, and it survives between time zero and epsilon. So it doesn't get hit. And this hitting is controlled by the capacity of the path. And just using this sort of deterministic lower bounds on the capacity, we got this end. This n to the minus d minus 2 over d bound. Okay, now what we're going to want to do today is to try to improve this and actually get sharp balance on the same probability. And you can see that the place where we're really lose, or at least one place where we're really losing something in this argument, is that we use this deterministic bound saying that the capacity of any n vertex set is at least n. set is at least n to the n to the d minus 2 over d. And this is a sharp boundary, but it's sharp when the set is a bulb. And it's not sharp in the case that the set is a loop raised random walk, which is really the relevant thing. So that's one thing we'll want to revisit and then see how we can take improved control of the capacity of a loop raised random walk and use it to improve our bounds in this there. Okay, so before I go Before I go further into that, let me take a step back and talk about the big picture for high-dimensional critical exponents and so on. So very broadly, the picture is as follows. So if we have some critical statistical mechanics model on a lattice, we generally expect that the sort of interesting large-scale features of it will be described by some critical expert. By some critical experiments. Okay, so for example, if we consider instead of the spanning forest a model that will probably be more familiar to many of you, which is percolation, so we expect that in critical percolation, there should be some exponent delta such that the probability that the cluster at the origin has size at least n is roughly n to the minus 1 over delta, up to perhaps some sort of sub-polynomial corrections. Now, these critical exponents, they're expected to depend on the dimension, but not the fine details of the model. This is the prediction of universality. But there's actually more that you can say than that, which is that these exponents delta, these critical exponents, so an example being delta, are supposed to stop changing. Stop changing, stop depending on the dimension once you go above some special dimension called the upper critical dimension. Okay, so for percolation, this is six, for the uniform spanning forest, it's four. So for example, for percolation, you know, it's a big theorem of Harrow and Slade and follow-up work that this exponent delta is always two in high dimensions. But in low dimensions, somehow the lattice is constrained. Constrained enough. There's not enough space for the model to kind of spread out and do what it would really like to. It's being constrained by the lattice in a non-trivial way, and this ends up in these X buttons being different. And this is a subtle effect, but it turns out to be the case. So for example, in percolation, this delta is supposed to be 2 above 6, and then in 2. And then in two dimensions, you get these weird rational exponents. This is quite typical. In these intermediate dimensions, we really don't know what it is or that it exists or anything like that. These are all big problems. And then you also get this additional effect, which is that when you're at this uppercritical dimension, so six in the case of percolation, you expect to get basically the same thing as the high-dimensional case, but with some kind of log correction. And these phenomena are all expected to hold very generally, maybe different values of the exponents, but for all sorts of models, so percolation, easing, random cluster, uniforms boundary, etc. As long as the model is undergoing some kind of continuous phase transition, or the uniforms boundary is sort of the self-critical model, you should kind of always expect roughly this picture. Except that the optimal Except that the upper critical dimension depends on the model. So for the spanning forest, it's four, not six. Okay, so the result that I want to tell you about today is a confirmation of the high-dimensional part of this picture for the uniform spanning parts. Okay, so in particular, I'm going to focus on this exponent that we've been looking at, but you can look at other things as well. So remember, this is the probability that the past survives for n generations. Okay, so you could imagine that in general in general dimension, this should be like n to the minus 1 over rho plus letter of 1. Say where rho is this exponent that you'd like to compute. Okay? And so this, the first theorem. And so, this, the first theorem, which is already published, says that you have the mean field behavior that you expect in high dimensions. Okay, so you should compare this to a critical branching process. So the probability that a critical branching process survives for n generations is also of order one over n. So it's saying that in high dimensions, the past of the uniform spanning forest basically behaves like a critical branching process, which actually. Critical branching process, which actually, if you do the wire spanning forest on a tree, say on the three-regular tree, the past is actually exactly distributed as a critical branching forest. So it's saying the high-dimensional lattices, the model behaves in approximately the same way that it does on the tree. Okay. And this more recent ongoing work with Palasusi that will hopefully appear fairly soon. Appear fairly soon, says that we're also able to compute the logarithmic corrections for the same thing in the upper critical notion when D is equal to four. So I'll be able to tell you a bit about how this goes. Although there are a lot of this is much more technical actually to get it to work properly than the high-dimensional case, and there are a lot of details that I'll have to brush over. So, you know, here I'm focusing on this one thing, but there are many other exponents you could look at. For example, the volume of the past or the probability that the past reaches some distance in ZD. And these are all things that you can understand. Once you understand this, you can kind of derive the other exponents from it. Although in four dimensions, it's quite hard. But in high dimensions, it's not so hard. Knowing one exponent considered enough to know all of it. So, a few other things to mention before we get stuck in. So, one is that there are some related results about scaling limits of uniform spine trees. So, Perez and Revelle in 2004, in a paper that was actually never published for some reason, but I Some reason, but I think it's correct anyway. That proved that if you take the uniform spanning tree of, for example, a big torus in high dimensions, then it converges to the continuum random tree, which is the same thing as the limit of a uniform random tree. And so this is another instance where you get mean field behavior in high dimensions. The model is sort of in a higher dimensional setting, the geometry kind of trivializes. The geometry kind of trivializes, and the model behaves the same way it does in this geometrically more trivial set of a tree or a complete graph. Okay, and this result was also extended to the four-dimensional case by Schweinsberg. So that's much more difficult. And you also have to, there is a difference between the four and five-dimensional case, but it's that the scaling that you have to do is different. You get sort of log corrections, but they don't. Sort of log corrections, but they don't appear in the scaling limit, they appear in the scaling that you have to do to get that limit. Okay, so these results are morally closely related to our results, but they're quite different in the detail. Our results sort of apply on a different scale. I mean, our results are sort of more like things about mesoscopic scales on the Taurus, whereas there's really. Whereas theirs is really, it's a very global result. So, although these two results are morally related to each other, the proofs are very different. Although I should say that ideas from our proof can be relevant for these scaling limit considerations. So, in fact, there's a paper plus I think ongoing work by Michael Macmias and Shalev. Mekmius and Shalev, where they prove they studied the diameter of the uniform spanning trade in high dimensions using some of the same ideas we'll be looking at. And this is relevant, for example, if you want to improve the topology with respect to which this convergence holds. So anyway, so there is also stuff about scaling limits, but we won't talk more about that. I'll also say that our analysis in the four-dimensional case. Analysis in the four-dimensional case is going to rely heavily on analysis of the four-dimensional loop rise round walk that was done by Greg Lawler in the 80s and 90s mostly. So I'll talk more about that when we get to it. So as I said, we're going to want to eventually come back to this proof that we did and see how we can improve upon it once we know something. Improve upon it once we know something about the capacities of loop-based renterbox. What I want to do before that, though, is to come from the other side and talk about how we can prove lower bounds on the probability that the past survives fair generations. Okay, and in five dimensions and higher, this is actually very easy using this dynamical picture of this mountain forest that we've developed. Okay, so let's try and prove this theorem that says that the Try and prove this theorem that says that in high dimensions, d at least five, the probability of surviving, the past surviving to at least ten generations, is at least constant over n. Okay, so here's the idea. We want to say that roughly having this past reach generation n, statically, it's kind of the same thing as the dynamical event that the origin is hit in some small window of time. Window of time. So, first of all, let's just notice that the probability that the origin is hit by a unique trajectory in this window of time is of order one over. So, this is just by the definition of possible. Now, we also know that from the definitions of displacements that conditioned on this event that we get a unique trajectory, let's call it W, it basically looks like a double infinite random block, right? So it has. Infinite random walk, right? So it has this forward part, which is just a random walk, and then it has this backwards part, which is a random walk condition not to return to the object. Okay? Now, we know that in high dimensions, d at least five, by the theorem of Erdős and Taylor, which scroll back to, couldn't prove it before, that with With constant probability, these two parts of the walk of W don't intersect after time zero, right? Because they're just walks in high dimensions. They might just not intersect each other. Okay, so this happens with constant probability. Okay, now imagine that, so of course, when I compute the forest from the interlacements, I don't just have this trajectory w, I have all the other trajectories as well, right? But the way the algorithm works is that trajectories that arrive earlier have precedence in the algorithm. So that means any trajectory that arrives after time w is we're going to be able to safely ignore it, basically. So in particular, if w was the only thing in our intelligence process, which of course it isn't, then the past of the origin would be infinite, right? Because we can apply the Because we can apply the ultraspregon just to w. It doesn't give us a spanning forest, but it gives us a tree, right? We just every vertex that is hit, we take the first entry edge and we put it backwards, same as before. And if we apply this algorithm just to W, we in fact get a two-ended tree. Yeah, so Mark asked a question: does the The question: Does the conditioning make the continent larger? Yes, it does by a coupling argument because you can the warp conditioned not to return, you can write it as the, or a walk that isn't conditioned not to return, you can write it as a concatenation of a random loop, and then a walk that's conditioned to not to return. So it's a stochastic domination. But anyway, we don't. But anyway, we don't need to know that, we just need to know that this is some positive constant that this happens. So if we just applied the algorithm to W, we get actually a double infinite trail. So in particular, let's take eta to be an infinite simple path that's in the past of the origin in this tree that we get by a path. That we get by applying all this product to W. Okay, and just to say eta is going to come from the forward part of W because there's this edge flipping thing in the definition, so it's slightly confusing, but this negative part of W will become the future of the origin in this tree, and the positive part will become the past. And in fact, one can prove that eta is distributed as a kind of conditioned blueprint and walk, but we Conditioned loop race random walk, but we don't actually need that for this proof. It's just some infinite simple part. Now, what happens is that the forest F0 will, it includes all of ETA up until the first time that you're hit by some earlier trajectory in the interlacement process. Right? Because W has precedence over anything that arrives after it. Rise after it. So in fact, this will be exactly, you'll get exactly this part of eta up until the first time that you'll hit before. Now, by the splitting property of Parson processes, if I just look at the first n steps of eta, then the probability that they're not hit by any earlier trajectory is at least this exponential thing, just because all I've Um, you know, all I've conditioned on are the trajectories that hit the origin in this window. So I then have a bunch of other trajectories, but they're just independent of those ones by splitting. So the probability that none of them hit these first n steps, it's just this Poisson thing. So it's a Poisson. The number of them that hit this first n steps is a Poisson process with intensity given by the capacity. I'm looking at it. Capacity, and I'm looking at it on this integral of length constant over. Okay, now on the other hand, the capacity of any set, so remember this had this definition that it's the sum of the degrees times the escape probability. So in particular, it's always at most the sum of the degrees. So in our setting, degrees are bounded, they're constant. So the capacity of any set is at most a constant times its volume. Okay, so if I just plug that estimate in here, Just plug that estimate in here, I get that this is bounded by a positive constant. Yeah, so what we've shown is that in high dimensions, if I condition on this event that the origin is hit by a unique trajectory in this interval, then the conditional probability that the path reaches level n is actually a positive constant. level n is actually a positive constant. And this event that we're conditioning on has probability of order 1 over n, so we get that indeed this probability of surviving to level n is at least 1 over n. Now it turns out that the same strategy actually works to give the correct answer in dimensions three and four as well. Mentions three and four as well. Except that it's much harder to analyze because there's a few extra things going on. So, you know, this is no longer true. We don't get non-intersection with constant probability, but we know how this probability depends on n. If I want this, the first n steps of eta not to be hit by the other part of w. Be hit by the other parts of W, I know how that behaves. And also, this bounding the capacity linearly is not going to be correct. So it turns out this is actually sharp in high dimensions. The capacity of an n-step loop-brace random walk really is of order n. In low dimensions, that's not true anymore. But again, we can understand how it does grow. And if you put all these ingredients together, this bound will actually give you the correct. This bound will actually give you the correct lower bound. And it's not too difficult to do this. An interesting thing to note, though, is that actually these two different effects that I've mentioned, they're actually competing with each other. So one is trying to make the probability smaller and one is trying to make the probability larger. So the fact that in low dimensions the capacity of the n-step glue parasarand walk is growing sublinearly, it basically means I can take epsilon to be means I can take epsilon to be larger in this proof. And that's going to help make this probability actually be bigger. On the other hand, this thing that this non-intersection probability now is decaying with n is making it smaller. So you have these two competing effects. And for example, what happens in four dimensions, and we'll talk a bit more about this later. Talk a bit more about this later, is that the capacity is really of order n over log n to the two-thirds. But this non-intersection thing is like one over log n to the one-third. So this lower capacity means we can take epsilon a bit bigger in this argument. And what we get is that the probability is at least this term, which is epsilon, times this non-intersection term. Section time. Okay, and it turns out that the larger capacity thing wins, and what you get in the end is bigger than in high dimensions. But it's a pro, it's not clear why it should be bigger. Why does this effect beat this effect? And I'll come back to this later because it's a very interesting problem to do this in three-dimensional. Of course, to do this argument properly, there are a lot. Properly, there are a lot more detail that I'm not showing you. In high dimensions, the argument I showed you is really a foolproof. Getting this to work, even given these two ingredients, you need to do some technical stuff to get it to work in four dimensions. So, maybe if anyone has any questions, it would be a good time before we start talking about upper bounds.  So I guess I can continue. Okay, so what we're going to want to do is to try to take this argument that we had last time and then understand more accurately how the capacity is written. The capacity is really behaving and try to improve the arguments. So, what I'll do to stop writing this probability over and over again, I'm going to define Q of n to be the probability that the past survives to generation n. Okay, and what I want to do is that to prove that in high dimensions this is bounded by a constant over n. And the way that we'll do this. And the way that we'll do this is so this is kind of a way of avoiding picking up these kind of log terms that we had before that shouldn't really be there, is we'll prove an inductive inequality like this. So prove that the probability of surviving to two n generations is at most constant over n plus, let's say, a quarter times the probability of surviving to generation n. And this factor of a quarter is not particularly important, but it just has to. Not particularly important, but it just has to be, this number has to be a smaller than a half. Right? And if you can prove a bound of this form, then it just follows by induction that you have this bound. At least you prove it for n of the form due to decay, but then this is a decreasing function of n, so you have it for all n. So let's start by just trying to do Just trying to do what we did before. So, again, the idea is going to be that this first term, this constant over n, is going to come from this dynamical event that the origin is hit within a small window. Whereas this inductive term is going to come from the other thing where we try to say, well, if we didn't get hit in the small window, what's the probability that? Window, what's the probability that, nevertheless, we survive to generation two and okay, so we can do exactly the same thing as before. So we'll do a union bound. So either we get hit in this small window, or the following happens. So what we did last time is we just said, or there exists a path, there exists a vertex u, which is a distance level n at the past, at time epsilon, and the path between u. And the path between u and the origin is not here in between. What we want to do now is slightly more complicated. So we'll look at the number of u that are at level n in the past, but they also lie on a path that goes to level 2n in the past. So which is the same as saying that their path survives to generation n. Okay, so we want that u is one of these vertices and the portion And the portion, this path between the origin and u is not hit by the interlacement process between time zero and m. Okay, so it's clear that if I survive to generation 2n and I'm not hit in this small window, then I must have at least one vertex like this, and I can bound the probability that such a vertex exists by the expected vertex. Such a vertex exists by the expected number. Okay, so what we'll do next is we'll try to bound this expectation in terms of, so we'll split it into sort of a term where these parts have the capacity of the order that we think that they should, and another term where they don't. So a high capacity term and a low capacity term. So this guy here. Okay, so this guy here, which I've just copied here, so this is just the same thing that was written before. So we can bound this by, first of all, if this path has capacity at least delta n, then we know that, again, these trajectories that arrive between time zero and epsilon, they're independent of the forest at time epsilon, right? And the hitting probabilities. Right, and the hitting probabilities are given by the capacity. So, just like before, if the capacity is bigger than delta n, then this probability that they're not hit is at most e to the minus epsilon delta. So, epsilon is the time interval, this is the intensity of the plus and process. Okay, and then we can then we have to found the number of points that we have like now, what we can do. Now, what we can do like before is we can use the mass transport principle. So here we have the expected number of vertices u that are in generation n of the paths of the origin and themselves have paths reaching generation n. But this is in fact just equal to the probability that the path survives to generation n by mass transport. So remember this mass transport before. And it's basically the same arguments as here. You just say, well, Well, I transport mass, so I take like f xy is the probability that x is past by generation n, and x is in the past. Past of wire generation M. And again, it's the same thing where the mass in is exactly this quantity that we have here, but the mass out is this probability because everything has exactly one thing as uncertain in the future. Okay, so we get that this expectation is exactly the future. Okay, so then there's this other term, which is where the sort of the bad thing happens, and this doesn't have the capacity that we think it should. So again, you can do a mass transport, and you get that this is the same as the probability that the past survives to generation n, and the first n steps of the future have capacity less at most delta. Okay. Now it's going to be non-trivial to upper bounds, but let's just suppose that we can. So what I'd like to say is that if I pick some small delta, so I kind of expect the capacity is going to be linear in high dimensions. So maybe if I pick delta to be very small, then I know that if I don't have this capacity constraint, if I just have this constraint, I get Q. Have this constraint, I get q of n. So maybe if I take that event and I intersect it with this low probability event, I hopefully get an event that has probability much smaller than the original event. This is not at all trivial because these events could potentially be very highly correlated with each other. I don't a priori have much way of controlling that. But let's just suppose I could. So, in particular, let's imagine I could prove that if delta is sufficiently small, If delta is sufficiently small, I get a bound of this one. Okay, then if I put all these things together, I'd get this constant times epsilon term. That's just from the origin being hit at a small time. And then I'd get this high capacity term with this thing coming from the path not being hit in between times. And then I'd finally get this term with the bound that I've just That I've just been with my wishful thinking about. Now, if I have this, I can conclude because all I do is I take delta to be small enough that this bound holds, and then I just have to take epsilon to be a big constant over n. In fact, this works, log a over delta. So if I take that, I'll get another one over eight here. So I'll get a quarter in total, which is what I wanted. And then here I'll have some maybe. And then here I'll have some maybe really huge constants over n, but it doesn't matter if this constant is very huge. Okay, so if I can get a bound of this form for this term, then I'll be done. Okay, now of course, you know, I've been telling you that high-dimensional lip phrase random walls have linear capacity, but I haven't proved that, right? So certainly the first thing we'll have to do, if I were Have to do if I want to try to complete this proof, is I'll have to actually prove something to you that boot phrase random walks have high capacity. So let's do that. Okay, so remember again this definition of the capacity, and that as we said, there are also many variational formula. Many variational formulae for computing the capacity as well. Okay, so we really want to understand capacities of loop raised renovals, but let's start with the easier problem of trying to understand the capacity of a simple random walk. Okay, so I'll write x superscript n for the first for the random walk stopped at time n, so the first n steps of the random walk. You know, how does this capacity grow? Well, let's try to. Let's try to see what the capacity is in expectation. So, this turns out to be very closely related to the non-intersection corporate. So, if I take the random walk up to n steps, the capacity will be the sum over the vertices in the random walk, which once you're at least in three dimensions, it's basically going to be the same thing as summing over times, but it's. The same thing as summing over times because each vertex is not visited very many times of the probability that if I start another random walk y then it doesn't hit the random walk that this other random walk after time zero okay so in particular in high dimensions this very easily leads to a proof that the capacity grows linearly in expectation by the By the Taylor theorem again. Because we can write, this is actually not really inequality because there's this issue that really I should sum over the vertices that have been visited by the random walk. And here I'm summing over times. But let's ignore this because it's not very important. So we get that the capacity of the random walk. The capacity of the random walk is this in expectation. I can think of it if instead of thinking of this one random walk and then this other random walk coming off it, I can think instead that this is the origin. And I have one random walk X coming off this way and another random walk Z coming off this way. So I have these three random walks X, Y and Z and I want Y to not hit the union of these two parts, X and Z. Of these two paths x and z after time zero. So up to some fudging, the expected capacity is the same as this sum. Okay, where x is run for time m and z is run for time n and n minus m and then we sum over the values of n. Now, in high dimensions by Edisch Kehler, I know that this probability is. This probability is bounded from below by something positive. So I get that the capacity in expectation is growing linearly. Now, in four dimensions, you can also understand what's going on. And it turns out, so this was done by Lawler, and you find that the capacity gets this n over log n thing. In fact, the lower bound of this is quite easy. The lower bound of this is quite easy to understand, and I'll come back to this in a minute. In fact, unlike many other things in three dimensions where we can't compute exactly what the exponents are, the capacity exponent is actually something we can understand in three dimensions and it's uh square root of And in fact, you know, here these results are all just about expectations, but these are really the typical values of the capacity as well. And there are many very strong and precise results to this effect in a series of works by Asila, Shapira, and Susik. Okay, so let's see an easy argument showing that the lower bounds, in fact, for all In fact, for all, for both three, four, and greater than five dimensions. At least a sketch of it. Okay, so remember, we saw before that there's this variational formula for the capacity, where the reciprocal is this infimum, where I have to take a probability measure supported on the set, and then I have this Green's function. Okay, so as we So, as before, I can always get an upper bound on the reciprocal, which of course is a lower bound on the actual thing, by just taking μ to be the countermeasure. So you get that this is always a, this squiggly thing means up to constant because I'm ignoring this degree. So, in particular, up to some five. Up to some fudging again about the fact that really, well, no, this is exactly true. So the capacity of the random walk is at least the number of vertices it visits squared divided by the sum of Green's functions over pairs of times. Now again, we know that, in fact, We know that, in fact, as soon as we're in three dimensions, because of transients, this the range of the random walk grows linearly. So this numerator here is always going to be of order constant times n squared when we're in at least three dimensions. And now for the denominator, you can just expand everything out. Everything out. I mean, this expectation, you write it, you just write out all of what it is in terms of transition probabilities, and you get this huge, messy expression. But it's completely elementary. And you can eventually simplify it to get this precise round. It says that the expectation of this quantity is always at least 2n plus 1 times this sum up to n of n times the. Of m times the return probability, and then plus this other tail time. Okay, and so this is similar to some of these calculations we did when we did the proof of the Eddish Taylor thing. Okay, and now we know that the return probabilities are like m to the minus d over two. Okay, so if we just plug that in here, we can get an upper bound on this expectation, and you know, we compute that it's linear when, so in high dimensions, these sums are both, this sum is going to be convergent, so we just get this linear thing, and this tail thing is negligible, and then in four dimensions, we get a log coming from this sum, and in three dimensions, we get square root. So, these are these are just very simple. These are just very simple, explicit calculations you can do. Okay, but if we take this and then we plug it in here, we see that indeed the capacity, because this we have some control of the expectation of this, we can use Jensen and we get that the capacity of the random walk is at least what I claimed in these three cases with good probability. Okay. Well, that's nice. And as I said, you know, this gives you some quite weak control, but you can prove a lot more about the capacity of single run and walk if you want to. But really, we're interested in the capacity of the loop radio. So, how might we understand that? Well, it turns out that actually, in expectation, there's something very There's something very nice that happens. So, if you'll remember when we were discussing P-Mantel's theorem, I mentioned this theorem of lines, Perez, and Schrann, which says that on any graph, the probability that an independent random walk and loop race random walk intersect is of the same order with some universal constant as just the two simple random walks. Not intersecting. Not in setting. Now it turns out you can take basically exactly the same proof and use it to prove a related boundary for the capacity. So this is something Padder and I are doing in our paper that's to appear. So it says that it works on any graph, take any transient graph, and you take a random walk, start from some vertex, you run for n steps, and then you look for. And then you loop raise, then in expectation, the capacity of what you get is at least some universal constant times the capacity without the loop pressures. And of course, the other way around, the loop pressure is smaller, so it's actually not obvious from the definition I gave, but the capacity is a monotone function. So we trivially have the low. So we trivially have that the left-hand side is bigger than the right-hand side without the constant, but in fact, they're always the same up to this factor of 1 over 2. Okay, so this is a very nice thing. So at least in expectation, we reduce the problem of computing capacities of loop-based running walls to the simpler problem of simpler animals. So, of course, there is this issue, though, that the time parameterization changes. So, we're really, in what we were doing before, we're really interested in the first n steps of a loop raised round of wall, which is not the same thing as the loop raiser of an n-step random walk, right? There's some kind of time change. And in high dimensions, this time change is just going to be kind of multiplication by a constant, but in low dimensions, Multiplication by a constant, but in low dimensions, this is going to be a real serious time triple. So, for example, in four dimensions, it's a result of Laura that if you run a random walk for n steps, then its loop ratio will have length typically of order n over log n to the one-third. And with some concentration, that there's some specific constant times there that it likes to. Times those that it likes to do. So that means that if we want to compute the capacity of the first n steps of a loop raised random walk, that should roughly be the same thing as the capacity of the loop ratio of a simple random walk, but run to time n log n to the one-third. Okay, so if you plug in what we know, you get that this should be of order log n to the two-thirds over. And this is not a rigorous argument, but this does actually give the right answer. And it's one of the things we have to do in our paper is to actually make this, to do a precise argument to this fact. And in fact, we also proved that this really holds not just an expectation, but with high probability. Again, these things get kind of technical, but intuitively, everything. But intuitively, everything that you want to work works. So, okay, now so let's say we now understand that listing high dimensions, the capacity of the n-step loop raised random walk is order n with high probability. There's still a problem because this thing. There's still a problem because this thing that we wanted to analyze before, see if I can get the other page of the profile. We had this last term coming from when the capacity was smaller than we wanted to be. So we've argued at least vaguely that this second event that we're intersecting with should have. We're intersecting with should have low probability when delta is small. Okay, but we still have a problem that we have we want to estimate the probability of an intersection of two events and they're not independent. If they were, we could just take delta to be sufficiently small that this second event had probability at most one-eighth, and then we'd be done. Okay, but they're not. Now, one situation in which this wouldn't be a One situation in which this wouldn't be a problem is if we just had some kind of strong concentration for the capacity of the n-step blue phrase renbolt. So for example, if we knew there was some constant delta such that the probability of this having small capacity was much smaller than one over n, say, then we could just ignore this first constraint and just use this and we'd be fine. Okay, and this should actually work in very high-dimensional. Work in very high dimensions. It might work in five dimensions. I'm not really sure. I seem to remember convincing myself years ago that it doesn't work in five dimensions, but I'm no longer convinced. I forgot how I convinced myself. So, in any case, this should work in very high dimensions. It's unclear whether it's going to work in five dimensions because although this is sort of concentrated. Although this is sort of concentrated, the bounds tend to be not super great because basically the loop race random walk is not so well behaved. Okay, so how are we going to get around this? So, one way we could try to argue, which is actually what we have to do in four dimensions, is To do in four dimensions is just to try to argue that these two events are approximately independent. And intuitively, it's plausible that they are because there is some interaction certainly between the past and the future, but the interaction probably happens kind of near the origin, whereas you can get all the capacity you want just by looking far away from the origin. Looking far away from the origin, say between times n over 2 and n, and this should be roughly independent of what's going on in the past. And in four dimensions, we did have to do it this way, and you can make it work, but it's not that easy. In high dimensions, though, there's a trick that gets around this, and you actually learn more by doing it this way as well. It this way as well. So, and this trick is using what's called the WSFO or what I've been calling the V-wired uniform sphere. So, doing it this way will actually give us stronger results than doing it the other way. So, this is a variation on the wide uniform spanning forest model that was introduced by Yarai and Redik. And they introduced it in the context of their work on the abelian sandpile. Context of their work on the abelian sandpile model. And actually, if you want to understand the sandpiles, this other model is often more relevant than the original, the normal white uniform spanning forest model. So the way it works, remember before that when we defined the spanning forest, we had this graph gn star right away, where we see if I can find it, where we took everything outside. Outside, we have an exhaustion of our vertex set by finite sets, and then at each stage, we take everything outside of Vn and contract it down to this one big vertex, delta. Okay, so in this alternative model, the V-wired uniform time for us, we fix some vertex V and we kind of wire V to infinity. So at every stage of the exhaustion, we identify this special vertex V with the boundary vertex. Okay, and then as before, we take a weak limit of this should have V though of these uniform spanning trees on these funny graphs, orient it towards the special boundary vertex, which is now identified with B. Okay, so fine. Why is this useful to us? Well, it turns out that there's this very useful stochastic domination property between the Domination property between the usual wide spanning forest and the V wide spanning forest. So this is essentially due to Lines Morrison Tram. So what this says is, suppose I take the wired spanning forest, in fact it works for this Lee wired spanning forest as well, and I take some list of vertex u1 to uk, and imagine that I condition on the future. That I conditioned on the features of all of these vets. So maybe U1 and U2 are in the same component here, U3 is in the same component. And now, given all this information that I've conditioned on, I pick some other vertex, say X, which could be the same as one of these size. Then what this statement says is that conditional on all this blue stuff that I revealed, the part of the past of X which does not belong to the revealed. Belong to the revealed tree is stochastically dominated by the component of X in this XY spine forest. Okay, so here that thing is here, so this part of the past which is kind of hanging off the revealed part of the tree. Okay, so in particular, the V Y forest will have this special component at the Have this special component at V. It's not a translation invariant model. And this special component will actually be finite. So the whole component of V in the V wide spanning forest is sort of like the past of V in the normal spanning forest. And this stochastic domination property is very useful. Roughly speaking, we often use it in a similar way to how one would use the BK inequality. One used the BK inequality in percolation, so if you're familiar with percolation, that's often how the stochastic domination will be used. So one consequence of the stochastic domination property is that the so just a very simple case of the thing, is that the past of the origin in the widespread forest is stochastically dominated by the Is stochastically dominated by the entire component of the origin in the origin, in this modified model. So, in particular, if we define Q0 of n to be the probability of surviving to generation n in the modified model, then it's bigger than the original q of n, which was the probability of surviving to generation n in the original model. Okay, now you know, this is all well and good, but if maybe this new model I've introduced, I don't have any tools for it anymore. I have all these special tools available to me. That's not the case. So, in fact, everything that we know about the usual model has an analog for this other model. So, for example, Wilson's algorithm is basically exactly the same, except you root it at v, so but v. So, but V and infinity are the same. So, you start the forest with just the vertex V and no edges, and then you run your random walks, and maybe they hit V in case, which case you stop them, otherwise they run forever, and otherwise you just run walks of the algorithm exactly the same way, but you have this special kind of sync vertex V. Similarly, you can define a V-wired version of the interlacements. Again, you can define it. It through this limiting procedure where you consider random walks on this graph g n star v and you do your Poisson time changes and so on and you get something which is very similar to the normal interlacement process except it has some extra things so instead of all the trajectories being doubly infinite instead you'll get some doubly infinite trajectories that don't have V otherwise you get singly infinite trajectories of both kinds so either start Trajectories of both kinds, so either starting at infinity and hitting V, or starting at V and going to infinity, or you can have finite trajectories that both start and end up with special vertex v. And again, all these things are sort of the same if you're willing to say V is extensive. But otherwise, you know, these are actually quite small differences, and generally things work in basically the same way as before. So, what you can try to do is to directly analyze this modified model instead of the original one. And that will give you a stronger result because of the stochastic domination. But we'll be able to use this stochastic domination property, similarly to the B-cane quality, to decorrelate things for frequency. So, if we try and do it before, now I'm using all these modified interlacement processes and so on. Interplacement processes and so on, but it's all basically the same. So I get this first term from the origin being hit at a small time. Then I get this other term, which is the same as before, except everything here is now this new modified version of the model. So I need to have a vertex U that's on the path to generation N, and then you can have another vertex generation N into its path. And I also need it to be not hit between. So as before. Pointing. So, as before, I can split this into these two terms: the high capacity term and the low capacity term. And the point is, whenever I see this event that U has a large past, well, I know that conditioned on the future of U, the past is stochastically dominated by this U-wide uniform spanning process, right? So, it means I can just, using stochastic domination, I can pull out facts. I can pull out factor q0. Again, I can do this in both the high-capacity terms and the low-capacity terms. So if I pull it out here, I keep this capacity coming from the dynamics, and this term coming from the dynamics, and I get the expected number of vertices in generation n of the path of the origin. And similarly, I can do the same thing here, but I get this other term. Here, but I get this other term with the small capacity constraint. Now, if I had this bound in the original model, I'd be good because I could do mass transport and I could turn this into a question about the phrase random walk that I already understand. But unfortunately, there's a new problem that emerges: is that this new model that we've introduced is not translation invariant, so we can't use the mass transport principle. Or at least, not in the Or, at least, not in the way that we'd like to. So I'm out of time to explain how to do it, but this turns out to be a tractable problem. So these two things, terms that we have here, although they're defined in terms of this funny new model, you can reduce them to questions about loop brace running walk using glosses algorithm. And again, And again, it will just boil down to the fact that in high dimensions, an n-step loop resurrect walk came from a round walk that also had length order. So I don't have time to get into details, but these are problems that can be overcome without too much difficulty. And we get that this expectation, just the number of vertices that are in generation n of the past. That are in generation n of the past. In the original model, this was exactly equal to one. In the modified model, in high dimensions, it's at most a constant. Similarly, this low capacity term, in the original model, it was just the probability that the loop raised around walk had low capacity, which is decaying. Here, it's not, but you can make it work. And you get that it's a small constant. So, if you put all these things together, you get exactly. Together, you get exactly this inductive inequality that we wanted to study. So, I'm already slightly over time, but let me talk a bit about some problems so that you'll maybe be able to get something out of all the things you've learned. So, as we saw in this proof, passing to this modified model, the B-wired uniform standpoint gave us. Uniform spank rice. It gave us this stochastic domination. It let us basically decorrelate things with crit. Now you can still do that in low dimensions, but the problem is that it's not going to be sharp anymore. And the reason is that in high dimensions, these two probabilities, the probability of reaching the two models, the probability of reaching generation N, is of the same order in both of them. So bounding one by the other, I Bounding one by the other, I didn't lose anything. But in fact, this is not true anymore in four dimensions and below. So, for example, what happens in four dimensions, it's not so bad. They're only different by a log. But the logarithmic correction is different. So, in the real model, you get log n to the one-third, but in this modified model, you get a bigger power of log. But this problem actually becomes much worse as you go into low. Actually, it becomes much worse as you go into lower dimensions. So, in three dimensions, these two different models actually have different critical exponents associated with them. And in two dimensions, this wide model just becomes completely degenerate, and it's actually just the whole, this probability is actually one in two dimensions for all, and it doesn't decay. So that's very bad. So this. This technique sort of breaks down more and more as you go into lower dimensions, and you have to really be able to understand how these things do correlate instead of using the stochastic domination property. One interesting thing, though, is that, as I said, Yerai and Reddick, they introduced this modified model in their work on the abelian sandbar model. So I don't have much time to say what that is. Don't have much time to say what that is, but it's a model where you have these random avalanches, and they showed that the distribution of the avalanche of the origin is not quite stochastic domination, in fact, that it basically is stochastic domination, so let's just pretend that it is. It's stochastically dominated from above by the component of the origin in this modified model, the the origin wide spanning forest, and from below by the past of the origin in the wide spanning forest. Of the origin in the widespread part. So, what happens is that in high dimensions, these two things actually have the same behavior. So, if you don't care about constants and you just want to determine critical exponents, you can actually get exactly the right, you can exactly understand what the exponents are for the sound part by understanding both of these models. What happens, though, is that if, for example, in four dimensions, we don't Example: In four dimensions, we don't know how to compute the logarithmic correction for the soundpath model because we just know that it's bounded between the log correction for this model and the log correction for this model, but they're not the same. So a new idea is needed to get the correct thing. And in three dimensions, the problem becomes even worse, and that these two things have different different critical exponents. So you no longer can get the exponents. You get the exponents. And in two dimensions, this upper bound becomes completely degenerate and you don't burn anything. So, figuring out how to close this gap would be very interesting. In four dimensions, you only have to close the gap by a cube root of log seems like something that might be attainable. I mean, it's quite a small thing. A related problem is that I. That I think is super interesting is whether you always have this mean field lower bound. So if I ask what's the probability that the path survives to generation n, my conjecture is that this is at least a constant over n for Z D in every dimension. And in fact, it's only really open in three dimensions, but the hope would be. The hope would be that maybe there's a general argument that shows that this is always true, and then you would learn some interesting things about three-dimensional loop raised random walls and spanning forests without actually having to do things that are specific to three-dimensions and therefore might be very hard. So if you were able to prove this conjecture, you would learn that the Hausdorff dimension beta of the 3D loop raised random walk is at least three over two. Least 3 over 2, and numerically it's about 1.6. And the rigorous upper bound of Lauder is quite good, but the lower bound is not. And there's not any specific number bigger than 1, which is the argument is ineffective. It doesn't give any specific number bigger than 1. So if you prove this conjecture, you get 3 over 2, which is actually quite good. And just to say that what is true. What is true is that this origin-wired version is always at least constant of row. And this actually follows by basically the same thing I showed you before, the proof of the lower bar. Here you don't have this issue with the non-intersection because you just have one random block. Okay, so I'm out of time, so let me end there. Thanks, everyone. Let me end there, and thanks everyone for sticking all the way to the end. Right, thanks, Tom. Since this is the final lecture of OOPS, I would like to take some time to mention the people and organizers who supported us. Specifically, we received support from CRM at Montreal, BURS, PIMPS, SMS. SMS at Montreal and MSRI. And we would also like to thank all the other organizers and the speakers who gave really wonderful lectures and participants who participated enthusiastically. I would also like to thank Ricardo from CRM and Brent Jake from Buzz who provided technical support. Provided technical support with Zoom and also making the recorded lectures available online. Okay, so let us also thank Tom Hatchcrox for his lectures. Sarai, can you unmute us so that we could give a round of applause for Tom? Alright, I think we should yeah, we should be able to Thanks again to all the organizers for inviting me to give this course as well. It's been very nice. So I think we can stop the recording and open for questions. 