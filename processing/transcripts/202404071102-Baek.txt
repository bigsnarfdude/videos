To the organizers again, Mika, Omar, Ajman, and Shrikat, for this great, productive, and fun weekend. So, and thank you for inviting me. So, I'm very excited to present this work, which is about RL in a behavioral health application, which is an application that's a bit different from what we've seen this weekend. Actually, we'll see that actually the techniques that we'll use for the theory, we actually borrow from the reusable. We actually borrow from the reusable resources literature from RM that worked on, which I thought was kind of neat. So I'm excited to show you guys here. Okay, so let me start with the motivating application, which is actually has to do with tuberculosis, or TB for short. Okay, so basically tuberculosis is an infectious disease, so kind of like COVID. But unlike COVID, TB is curable via a six-month course of antibiotics. Antibiotics. Okay, but TB is still a big problem. And one of the reasons why it is still a big problem, especially in kind of developing countries, is that treatment adherence is a big problem, right? So the treatment is the six-month course of antibiotics. You have to take this treatment every day. There's a long duration, there's unpleasant side effects, and then people start feeling better after a month or two, and then they kind of stop taking their treatments. Okay, and it's unclear how to address this problem. And even kind of collecting data on this is very costly, right? So it's hard to collect a large-scale data set on treatment adherence. So here we partner with this organization that was working in Kenya, and their goal is to improve treatment adherence for tuberculosis. And sort of, I'll tell you sort of how this platform works. Basically, you can think of it as an online You can think of it as an online platform or like an app on your phone. Patients enroll in this platform and they interact with Hila via their phone. And this is the main interaction. There's two main things. One is that the patients are asked every day to verify that they took their treatment. So they get a notification, like, did you take your treatment today? And then you have to respond. And, okay, one thing that this gives us is this kind of large-scale data set on Large-scale data set on adherence for the kind of thousands of patients that was enrolled in Keykila. Of course, self-reported adherence is not the same as adherence, but given that there's no large-scale data set on adherence, kind of this is what we'll be kind of sticking with self-reported adherence, but of course there's a gap between that and real adherence, which we'll be kind of assuming. So we'll just be kind of optimizing for self-reported adherence. Okay, the second thing is. Okay, the second thing is that Kihila has a bunch of interventions to help people who are kind of not on track. And you can tell who's not on track because they're people who haven't been verifying on their phone. So there's a bunch of interventions. The main one that Kihila kind of markets itself as being the most significant is what's called personal outreach from peer sponsors. So peer sponsors are people that are hired by the Kikila organization. They're locals that actually have gone through TB and they're Have gone through TB and their goal is to message the patients, either texting or calling them, and sort of encouraging them to keep on taking their medication and giving some information and helping them to do so. Okay, so this is the one that we'll be focusing on. This is kind of the main one that Kehila thinks about. But this one is kind of budget constrained because we need people. These are only a couple people that are hiring by Tehila, but we have thousands of patients, so we can't reach out to everyone. So, we can't reach out to everyone, so this intervention is costly. So, we're going to be thinking about how to optimize this intervention. So, there was an RCT that was ran, and in the RCT there was some kind of heuristic baseline that was used to do these interventions. And so, the high-level question is, okay, so we have some data on reported adherence and some policy that was running before. Can they design a better intervention policy to maximize the impact of these interventions? Such a patch. Two questions. So, the data, what is the depth, longitudinal depth, how many observations you have per patient? And second question, what do you randomize at the RCT? What is randomize? Oh, okay, good question. So 5,000 patients. So the treatment duration is six months on average. So it'll be daily data for six months for 5,000 patients, which is a pretty reasonable size data set. And healthcare, the RCT was actually just like. The RCT was actually just like people who are on Kihila versus not on Kihila. So it was just a matter of, okay, is this platform help me in general? And we're trying to figure out, okay, can we optimize the policy? What is randomized? The assignment to the. Yeah, of whether you are on the KBLA platform. Oh, actually, I think there were a couple of different arguments. One was people who didn't even have this intervention at all, access to this intervention at all. So this was just testing, okay, is this platform helping in general? And then we're asking, can we improve the policy of the platform? Improve the policy of the module. Okay? Okay, so let me go to the model. So here we're first going to formalize a pretty generic model of how patients behave. So here's a general model. This is just an MVP model. So every patient, we assume that they have some state, SIT, state of the patient at SIT. At every time step, we have to decide on whether we give them the intervention or not. We give them the intervention or not, and we'll call that action one, otherwise, zero. And then, okay, there's some transition probability from one state to another, given the state before and the action. And then there's some reward associated with the state. And this happens for like the six months that the patient is in the platform. Okay, so here, this is just a very generic MVP for one patient. And then, of course, when we have n patients, where n is like 5,000. For n is like 5,000, and these patients are linked through the budget constraint of at every time step, we can only say that we can only intervene on B patients, where B is much smaller than the number of patients. And our objective is to find the policy that maximizes the average reward. Okay, so, okay, but then, okay, one thing to note is that, okay, the problem that we're trying to solve is that we actually have. That we're trying to solve is that we actually have some data already collected from this RCT with 5,000 patients of these kinds of trajectories given some PRC policy that they already used. So that will be important later on. Okay, so this is the high-level problem. So, of course, this is kind of a generic problem, and so there has been obviously firework that we can use to address this. So, I would say that for Address this. So, I would say that prior work kind of falls into one of two buckets. The first bucket is where you assume you know all the parameters of the MDP, like the transmission probabilities. So then this is like an MDP planning problem. So here, you know, if you can use generic MDP methods, like maybe policy duration or value duration. But then, of course, we have the specific structure with the end patients and the budget constraints. That's the restless bandit problem, which is actually a hard problem. So, even if you know that you're Hard problem. So, even if you know the transition probabilities, it's not clear what the optimal solution is, and like the Widow's index is a common policy. Okay, so this is if you know the transition probabilities. If you don't know the transition probabilities, this is basically an RL problem, right? And so you can use one of many RL methods, or maybe you can use band-aid methods, which are kind of a more myopic approach. And the policies in this camp are kind of, usually they're Usually they're formulated as like a long, like an online learning problem where you want to try to get subliminal regret over time. But for us, we have access to this medium-sized data set. And so it's not clear that we have enough data to really fully learn the transition probabilities that we can just use this camp. But also, we don't necessarily want to use an online approach. Use an online approach. And so, like, the problem that we're going to tackle is: okay, can we just come up with a reasonable pretty good policy? Just given this data set, let's just try to come up with a good policy that just uses this data set. So, doesn't kind of rely on online ideas. Yeah. So I want to ask about the relationship to off-policy uh off-policy evaluation. So let's say idea of Pi zero is the uh they call behavioural. You want to test a new policy. You want to test a new policy with offline data. So, I think the budget constraint is definitely a different thing, but is there, like, outside the budget constraint, like, what's the main difference between them? Well, all policy optimization is very difficult. It requires some kind of exponential number samples. And they also need the Pi zero to be randomized in some way. Yes, yes. Although we will actually kind of rely on this assumption that the Pi Zero is randomized, but still, like, in general, doing auth policy. In general, doing off-policy, I mean, even just evaluation is hard, and then optimization is very required as that. Is there a forming panel to this also that there will be like latent states which you don't talk, latent part of the state that you don't observe? What of us in other state? Yeah. Yeah, so I'll tell you how we think about the state space. Okay. Okay, but basically, okay, so I'll tell you. Okay, but basically, okay, so I'll tell you about the policy that we come up with, which actually kind of leverages existing algorithmic ideas. The main algorithmic idea we're going to leverage is actually just policy iteration. So how policy iteration works in general is that you iterate over these couple steps. You start with some policy, you compute this q function, and then you define a new policy, pi prime, where each state you just take the action with the highest. The action with the highest Q value, and then you get this new policy pi prime, and then you start over beat. Okay, so that's a policy iteration. So the step that's hard is this evaluation if this pie is different from the one, the data that you have. That's all policy evaluation, and that's kind of what's hard, and that's why you can't really. I mean, yeah, so if you knew, if you could do this, then you will eventually converge to policy, but it's not. Q policy, but it's not easy to evaluate this Q function for an arbitrary pi. But okay, so I told you that we have data generated by some heuristic policy. So we could estimate Q for this, the heuristic policy. And that's just supervised learning prediction. Okay, so basically we can, if we start with Q0, we can do this once, and then we can do this improvement. And then we're just gonna stop. We're just gonna do one step of policy iteration. A policy version. Okay, that's our policy. Okay, there's one other thing we need to do is because we have an exponential state space because we have these MDPs. So we do this other thing. So the size of the state space is still exponential in N. So then we have to do this decomposition in Q. So instead of these big Q functions, we make these little Q functions, which is, this I is specific to a particular. This I is specific to a particular patient. And we compute these Q, we predict these Q, this little Q function, which is now a function of just this one patient state and that action that they got. And then, so this QI represents the expected future reward from patient I when running policy pi, condition on being state S when they were given action A, that time T. So it's kind of the natural kind of decomposition of this. Decomposition of this big Q function, although we are kind of losing information by doing this decomposition. Okay, so then our policy is just we learn this and then we compute this, what we call the intervention value, which is the benefit of giving them an intervention at state S minus, or basically this is kind of like the benefit of giving them an intervention versus not giving them an intervention. Do an intervention. And then we are going to choose our policy. It's called Decomp PIs and Decomposed policy iteration is just to choose the B patient, so this is the budget with the highest values of the CIT pi. You run one iteration for this iteration. Is it because you don't overly rely on your data? Is it because computation needs? Yeah, it's because I don't want to do off-policy evaluation. It's 'cause doing another step I would have to evaluate the Q functions for a different policy. Functions for different policies. Want to maintain the stationary distribution used by balance in the distribution? Um, and not necessarily. No, so it's just because I can't do the second step because it's the M1 component is too large. Only through the budget, right? Right. So that would be, if there's no budget, that would be there wouldn't be a real outcome. Right, yeah, yeah, yeah. So so you model each patient as an individual MDP. So I wonder if it's efficient, data efficient MD. Is it possible to consider the patient in some way? Uh why? Because now you you have to estimate, let's say, the transition probability of each patient individually. Uh-huh. Yeah. No, but the way we do this is going to be through. I mean, we're going to kind of regularize how we do this. Okay, so actually let me just tell you how I define the space. State space. Okay, so here's the summary of the algorithm. We have data. We estimate these q functions. We estimate these q functions and then we just take the at every step we'll just compute this I mean we estimate this offline and we'll just compute this pick the people with the highest intervention value. Okay, so how do we what is S and how do we learn this? So we right so a Q we can just think of as so the reward we're just going to model as whether they verify at a particular time step or not. So we're just maximizing the number of average number of verifies. Maximizing the number of average number of verifications. So Q is kind of the expected number of future verifications. And so our state space, we kind of just put in, we have a pretty, we just kind of design a state space where it's, we put in the static covariates, we put in kind of a history of observations for that patient, so kind of, and then we kind of make a lower dimensional state space kind of like, we just, we just try to put in all the relevant information. We just try to put in all the relevant information, like how many times have they verified the past week and past month, like what is their long history, and so on. And then we're going to just assume a linear function for, we're doing a kind of a linear function across patients for the queue. So we're pooling all data together. We're not learning for each patient separately. So we are pooling it that way. I think this is also important to continue. Okay. Okay, so that's the policy. That's it. So it just requires this. Policy, that's it. So it just requires this prediction, and then we just sort by this. Okay, so great, it's a simple policy, but okay, is this policy any good? So in general, you're not going to get any performance guarantees with this policy. So in general, doing one step of policy iteration, in general, it improves from the existing policy, but the improvement, how much you improve, there isn't much guarantees on that. There isn't much guarantees on that. But also, the fact because we're doing this decomposition, there's actually no guarantees that you can get a distance. You can construct instances where you don't need to prove. So, okay, in general, you're not going to be able to prove results about this policy. But basically, we're going to posit a stylized model that kind of captures some of the salient features of Kehila and just proves some guarantees in the stylized model. Guarantees in the stylize model. But then later on, I'll show you some kind of simulation results, kind of put the flow model up. But the stylize model is supposed to give theoretical motivation for why we think this policy should work well in our study. Okay, so here's the stylized model. So we think of each patient as just having two states, a two-state MDP. There's a kind of a bad state, S equals zero, and a good state. So this is the state. So, this is the state where they're not verifying, and then this is the state where they are verifying that they took their treatment to. The motivation for this particular structure is because in the Kihila data, we see that one of the most prevalent features of the data is that this kind of habitual behavior is very prevalent, right? So, the the biggest predictor of whether someone's gonna verify tomorrow is just whether they verify today. Like eighty percent of the time, that's that's the same thing. 80% of the counts as the same thing. Okay, so it's modeled by, this kind of captures this one kind of streaky behavior that we see in the data. So we assume that every patient will have kind of different parameters for when they transition from one to the other. And then we posit that if we give them an intervention, the probability that they'll transition from here to here will increase by a factor of 10. And so, okay, so our goal is just to get the most number of people in this state, or like the average number of people in this state. And so we'll apply DCOMP PI to this specific model, also using a specific policy, and we'll derive results in this stylized setting. Okay, so what are the results? So, okay, here are a couple different policies. couple different policies. So first we define the null policy as the do nothing policy which just gives no interventions. OT is the optimal policy. Optimal policy given that you know all the parameters. Okay. So we're going to prove a guarantee with respect to DCOMPI, DCOMPI based on the null policy as the kind of starting policy. This is, so I'm going to show a kind of simplified result. In the paper, we a kind of simplified result. In the paper we kind of generalized this null policy to be a it can be a policy that gives interventions at random with some small probability. But let me just tell you the result with the null policy. Okay, the main result is the following. So this is our policy. So this is an approximation ratio of one half with respect to the difference of our policy compared to the null policy. Compared to the null policy. So we're setting kind of null as the baseline. So this is the difference between calc and null. And then we're saying our policy is going to get half of the difference from null to calt. Okay? So this result is much stronger than the usual one-half approximation, which would be something like this. And this is significant because, in our setting, even if you don't give any intervention. Like, even if you don't give any interventions, this null policy, you can actually get a pretty big fraction of opt. So, in our case, actually, even the null policy would be actually more kind of opt because you don't necessarily need this intervention to move to this state because this PI can be positive, can be zero, or it can be positive. So, I mean, it depends on the parameters, but so often this result can be stronger than this one. Sorry. Any questions on Any questions on the results? Okay, so as I promised, I'm going to tell you how this relates to reusable resources. Okay, so you can think about one patient as one item, one reusable resource. And then you can think about them being in this state as we have the item in stock. And then if they're like, they go to this state, this item is sold, it's unavailable, it's being used. Unavailable is being used, but then they might return, right? So the item comes back. So like giving an intervention to a patient, so like choosing that patient you can think about as like offering that item in an assortment. And the item returns after some time and the reward is like in our patient example, we want people to be in the state. And then in the absorbent setting, we want to sell this item. There's one key difference though from our setting to the reusable resources setting is that items can kind of spontaneously become unavailable without having to have operated in the assignment. Without us picking this patient, they can move to this state because of this PI. But otherwise, it's almost the same thing mathematically. It's almost the same thing mathematically. Okay, okay, so then actually we can leverage some of the existing reasonable resources results into the setting. So in particular, we use some of the proof techniques from this paper by Shaoyer, Benit, Garou, David, Rajant, and Xiaoyu. So this is why I'm anyone. So this is why I'm analyzing greedy for reasonable resources where you can get greedy is greater than half opts. So you can get exactly this results in this setting or you have to kind of suddenly change the proof. Okay, but this is not the result that we want. So this greedy is not the same as our policy. And so this greedy is a myopic policy. But this greedy kind of doesn't correctly take into account that we don't want this result, we want a stronger result. Stronger result. This doesn't take into account this difference because the squeeze is kind of myopic. It doesn't take into account that items can kind of spontaneously move to the state one. But we leverage kind of this technique to show our main result. And because basically our policy can be thought of as a greedy policy with respect to our intervention values. So before the greedy wasn't Values. So before the greedy was something different, myopic, but here we do have a greedy policy with respect to this particular intervention value. So the, so, and this ZIT kind of correctly takes into account, because it's defined based on the Q values, it does kind of take this kind of condition into account. And so one, I won't go into the proof, but one key lemma that we use is that we're able to write for any For any policy alg, we're able to write alg minus null as the sum of the intervention values of everyone that we choose. Okay, so basically maximizing L minus null is equal to maximizing the intervention value of people that we chose. So this is kind of the correct metric to optimize. And since we are greedy based on, we're going to be greedy with respect We're going to be greedy with respect to this, these values, that's why we get the half approximation. Like this is greedy with respect to these values, and so we kind of use the same kind of techniques to show that we get this one half approximation. So what is the naive greedy algorithm in this case? It doesn't take into account that naive greedy is actually the one who maximizes tau divided by Divided by Q, which is kind of how long they would stay there if they. If you touch on the network version of this, where you don't have one intervention, you have multiple different types of budget interventions in what you're um content. But have you thought about that? Hm, I I haven't thought deeply about that. Yeah, it's a good it's a good question, but yeah. It's a good question. Yeah, I'm not deeply on that. Okay, so basically showing this lemma, like do we need to use a more complex sample path coupling argument that kind of couples these three different policy to show this lemma, and then we can show that's greedy. And then we're just saying, okay, greed, that's one half of those. The budget doesn't matter for the, because like they don't have a budget, and they're greedy outward. Um. Yeah, the budget doesn't matter, but like the, I mean, aught also kind of respects the budget constraints, so it actually, like, I mean, it's kind of like, yeah, I mean, I think it's kind of like, you know, maybe the vanilla setting is like you only offer one item, but then here we're saying we can offer multiple. But like, it's not exactly the same as the sort of because, like, It's not exactly the same as the sorbent because, like, every patient is kind of independent, so it's not like there's a choice one. Like, every patient will be sold with some kind of problem, so like, multiply it's kick as it, yeah, that looks kind of not better. Yeah. Like, when I'll index literature, there's some like maybe look at the indexability of the items, right? And it looks to me it's similar to the like what you're getting here, because indexability means that you can index each. Can eat that each individual and just from top to bottom. So the ingredient is similar to that. Yeah, good question. I'm not sure about that. If you find a composable activity, it's basically You mean like the Russell's mandate? For example, is the option policy like Whittlow's index? Oh, like Whittle's index is actually mostly a heuristic, and there aren't really guarantees on it. Well, if you have to have specific, very specific assumptions. So actually, like, yeah, in the Restless Panda setting, getting guarantees is actually not. Okay, so let me just finish off with just some simulation results, right? So, okay, so what is this thing? We're just saying that in this dialyze model that captures, we think captures the kind of first order phenomena of how patients behave. It seems like we have a reasonable theoretical guarantee about our algorithm. But of course, patients don't behave according to this. So, right, so okay, so then. So, okay, so then I'll show you some empirical results. And so, the empirical results aren't going to assume this particular structure, right? So, we set up a very generic simulation setup where we learn a simulator where the probability that someone verifies the next step is some complicated function of their history and their state and their action. So, we kind of learn a simulator using just like kind of black box RL methods. We check the calibration of the simulator. Check the calibration of the simulator, and then we learn decomp, our policy decomp PI based on a more complex state space using the method that we described, kind of just like a linear function of the state. But basically, we show that our policy, so the x-axis here is just like the budget, so we're just running simulations via our simulation model. This is the overall verification rate, and so this flat. And so, this black line was kind of this baseline heuristic. And our policy, we get substantially higher. And one way to read the graph is that we can get the same performance as before, only using about half the budget as before. Okay, so that's basically it. So, you know, we, I think the main thing, you know, so we're studying this kind of healthcare application. I think the main thing I wanted to convey for this audience was that we have a kind of For this audience, was that we have a kind of neat application of reusable resources too that we can call how simple. Yeah, that's it. Thanks. So, in your simulation, did you