Workshop and let me present my recent work with Guy, which is on the low-degree coefficients from geometric graphs. I'll be giving some really quick intro and motivation. So, CT, I think, already motivated this problem yesterday, but very briefly. So, oftentimes in the real world, we just encounter networks and they are formed based on some similarity of feature factors. So, for example, think of friendships, right? People are friends because they are the same age, went to the same school, came to the same workshop in Banner. Came to the same workshop in Banf and we scare all using this. Right, so you want to study this mathematically, and you need some model for this. And one very popular model is that of random geometric graphs. So each vertex has some random feature vector in a metric space, and then the probability of two people, or like the probability of an edge between two vertices, just depends on the distance condition. Just depends on the distance conditions on these vectors. So, this is the same model that we saw yesterday. Now, oftentimes, these feature vectors are partially or fully hidden, and this could be due to different constraints, like, for example, privacy in the Facebook graph, and so on. This gives rise to a lot of different statistical tasks. So, for example, one may try to embed the vertices in the metric space with no or very little information about these hidden vectors. Or maybe one wants to. Or maybe one wants to match two unlabeled copies of the same graph. For example, think of we have Twitter and Facebook, but we don't know who is who in these graphs and we want to match them. Or for example, one wants to infer some parameters of the underlying space, such as, let's say, dimension. Alright, so these are all wonderful questions. I will be talking about something else. So I'll be talking about the problem which is testing against Erdushraini. In the Erdus Reni graph, I briefly. In the erstrani graph, every edge appears in the panel with the ATP. So, formally, this hypothesis testing question. Now, you can ask, why would I do this? That's literally the one task that you would never want to solve in practice. Like, if I give you the Facebook graph, you immediately tell me, well, that's not there in the training, obviously. But I would like to argue that there are good reasons to study this. So, first, the simplest reason is that this is just the theoretically simplest task. So, just the binary hypothesis testing. Just the binary hypothesis testing question. Testing as Erd≈ës Ready is somewhat natural because that's a model in which there is no correlation between the ITs. So at least intuitively and heuristically, this means that also this is somehow a prerequisite for other tasks. So if you cannot even distinguish your model from something where edges appear independently and there are absolutely no dependencies, then okay, probably you can't really learn anything about the information in this graph. And I claim that there is yet another And I claim that there is yet another reason, which, at least for this talk and for me, is perhaps the most important reason, which is that developing and refuting testing algorithms usually reveals some structure. And I mean, I'll show you some harness results and so on, but I think really the main thing I want to focus on is the structure that we will show random geometric graphs in this event. Alright, so specifically we'll focus again on the spherical random geometric graph about which we heard yesterday, and again, just its defined following. And again, just it's defined as follows. First one draws uniformly distributed independent vectors on this sphere, and then connects them, connects psi and j whenever the inner product is sufficiently large. Alright, so what do we know about the testing task in this setup? There have been a few works in the past almost 15 years, and the state of the art is as follows. So we have some regime where the dimension is less than 8 cube p cube. Dimension is less than n cube p cube, and then we can test counting signed triangles. Let's hold this up to log factors. And why is this natural, forgetting the signs for a second? Well, it's simply because it captures the axiomatic triangle inequality. So let's say if people A and B are friends, then their vectors are closed, and B and C are friends, their vectors are closed. And by triangle inequality, we expect vectors of A and C to also be closed. Now, on the information-theoretic LORBA. On the information theoretic lower bound side, so for generic P, we have this lower bound by Sikit, Selil, Sid, and Elizabeth Yang, which says that if the dimension is more than Qp squared, this distribution is close to every strand in total variation, which was known in the very dense case from this 2014 paper by Bobeck, Kraz, Ding, and Eldon. And only in the very sparse case, we have a much better lower. For a much better lower bound again in the same paper, which says that we need essentially a logarithmic dimension to converge treasure straining in this parsed case, which we heard about yesterday. So in this talk, as you can see, there is still some gap. So between what we can achieve by counting sign triangles and what is information theoretically now. And I mean, I think that's a hard problem. Problem. And today I'll take a slightly different perspective to it, which is by allowing the group polynomials. So I'm not attributing any conjectures to anyone right now. But yeah, my question is, okay, can I test, for example, using some other low-degree polynomial, not necessarily counting sine triangles? So is there some other simple test and even information theoretically that's interesting, right? Like we want to know if. Interesting, right? Like, we want to, if we have this gap and we don't know what the information theoretic answer is, showing that there are no low-degree polynomials means that this certain test cannot work. So, we need to, if we want to look for a test, it needs to capture some other structure. As we all know, to show that there is no low-degree polynomial test, one simply needs to compute some low-degree chi-square advantage and it needs to advantage and it needs to show that this advantage is small and it's essentially the sum of the squares of Fourier coefficients of this distribution. And specifically all one needs to do to show such hardness is to compute this expected sign summer of counts or Fourier coefficients. And this has been pretty powerful in large in quite a few contexts related to A few contexts related to random graph distributions like planted cliques, stochastic block models, and so on. The difficulty here is, however, that for random geometric graphs, such a calculation is, I think, this much, much harder, and I will show you in just a minute why. Specifically, even to make things simpler, no, this will be the main question for most of the remainder of the talk, how to calculate this? And why is it hard? And I'll make a few. And why is it hard? And I'll make a few simplifying assumptions. So I'll focus on the density one-half case in which the threshold is zero. Why is it zero? Well, when you take two uniform vectors on the sphere, with probability half, the inner product is positive, and with probability half, it's negative. And in this case, I'll actually also assume that the vectors are actually not on the sphere, but they are isotropic Gaussians. This will make things slightly simpler. Again, it doesn't really matter because, again, the sign of the inner product doesn't depend on the norms. Norms. I also think of the subgraph H on which we are computing the Fourier coefficient as connected. And on at most, let's say, log square edges, because this is the most relevant setting to what we are doing. And finally, I will assume that the degree is at least polynomial. All of these assumptions can be significant. Sorry, the dimension. Yeah, right. The dimension is at least polynomial. All of these assumptions can be. Polynomial. All of these assumptions can be relaxed pretty easily using more or less the same arguments. So, not fully, but like we can definitely get a dimension that's smaller than polynomial. We can definitely get essentially any density and h also can be on polynomial in many vertices, or at least number of vertices that approaches the dimension. Okay, so what are some approaches? Are some approaches for computing this Fourier coefficient? Is the punchline going to be that counting triangles is the best you can do? We'll see. Related to my example. So this sine triangle count thing, you can interpret it then as like the degree 3 or something, low degree likelihood ratio, or it's like a subset of that, but that would also test successfully, this like very, very low degree trial. Successfully, this like very, very low-degree tremendation is sort of part of that. Yeah, yeah, yeah, exactly. Yeah, and that makes more sense. Well, to me, that makes more sense than just the magic. That was done as well. I just wanted to clarify. Yeah, we can chat about this later. In terms of it for very similar models, for example, like counting things from speaking about later exactly. So, related question: I mean, counting K4s, for instance, is also a good. K4s, for instance, is also a good idea, but is that not as sensitive as counting triangles? Does it fail at a lower dimension? Yeah, exactly. So it turns out that for this specific model, it fails at dimension n square. However, there are other models where it turns out that actually counting four cycles is a stronger test than counting triangles. But yeah, for this specific model, it turns out that four cycles fail in the end square. Okay, so how do we know? How can we hope to compute this for a coefficient? So I think the first most naive approach is direct integration, right? We just have some integral with respect to the Gaussian measure, and we know so many things about Gaussian integrals that when I first thought about it, I was like, this should be true. But actually, it's pretty hard to do even for triangles. It's very global. But essentially, anything that's slightly more complicated, so anything that's not just a tree of cycles, is Just the three of cycles is like really, really hard, and I challenge you to do it if you don't know what else to do. All right, so now actually plausible approaches. So there is this one approach which has been used over and over again in stochastic block modes, planted click, and so on, which is conditioning on the latent vectors. And I will illustrate with the planted click. So again, we all know what planted click is. We sample a click set as by including huge vertex with some probability. Including each vertex with some probability k over n. Then, whenever we have two vertices which are not both in the clip, we independently form an agent in them with probability half. Then that's exactly calculating the Fourier coefficients in this model is very simple, right? So, suppose that we have some connected graph H, unless all of its vertices are leaked vertices, there is a Bernoulli one-half H and its zero-soulby Fourier coefficient. So, specifically, yeah, unless all. Specifically, yeah, unless all the, yeah, exactly what we said, and we get this very convenient expression. Now, sometimes one needs more sophisticated versions of this, so for example, conditioning on a typical behavior of S or something like this, additionally, to conditioning on latent vectors. But I claim that all of these approaches are doomed to fail for random geometric graphs because we have this hard thresholds function. So, specifically, if you condition on z1, z2 up to zn, so the latent vectors, there is no randomness left. Latent vectors, there is no randomness left. So we'll never get such a, no, we'll never get this cancellation, which happens when not all vertices of H are in S. And I'll briefly also mention a third approach. So it turns out that in a very similar high-dimensional model, we can do some perturbative analysis. So over the torus, we tell the infinity metric, and there the idea is that the graph essentially decomposes over the one-dimensional random geometric graphs. One-dimensional random geometric graphs, and then you do some computations over them. And then, using something like the cluster expansion of this spirit ray, you can patch up these different components and you get some estimation for the Fourier coefficients. But again, this doesn't fully work here because the way different coordinates interact is much, much more complicated. So, here we have some threshold function instead of just them. Alright, so what do we do? Alright, so what do we do? We showed three approaches, and all of them seem to be bad. Well, as at least Dinon likes to say, some bad ideas are better than other bad ideas. And I think that here actually the first bad idea can be, so we show that the first bad idea, conditioning on the latent vectors, and at least this comparison with planted techniques can help, but in a slightly different form. So what will be our dream? Our dream will be to represent random geometric graphs. Be to represent random geometric graphs as a planted sub-graph distribution. So, the same way that we have the planted click, you just take first sample of Herdushrani graph and plant a click. We want to represent the random geometric graph as something similar. So you sample an Erdus Schreny, then somehow plant a few dependent edges. And if we manage to do so, this is really good news, because as soon as the planted edges, so as soon as H meets a non-planted edge, H meets a non-planted H, it will just zero out the Fourier coefficient and we'll get these some decay of Fourier coefficients, which we hope for. Okay, so the question is how do we do this? And I'll go back to the definition of random geometric graphs. So what determines the edge 1, 2, 1? So by definition, this is just the threshold of the inner product of the latent vectors. In other words, this edge is determined by the In other words, this edge is determined by the two latent vectors. But actually, there is just a slightly more subtle version of it. It actually only matters what the projection of Z2 is on the direction Z1. So we don't really care about the rest of Z2. Alright, so let's formally write this down. Suppose that we have Z1 in some basis, it's just written in that way. And then what's the projection of Z2 on Z1? Well, it will look like this. So we'll first project. Like this. So we'll first project on Z1 and then we'll have some other part. Right, now what if we want to see what happens with the third, the edge from vertex 3 to vertices 1 and 2, we can just do the same projection. And we can repeat this for all of the vertices of our graph H. Now, since these vectors were Gaussian, what we obtained is actually not something new. It's been known for a hundred years. It's known as the Barfett decomposition. At the composition. And this lower triangular matrix has some very special properties. So, first, it turns out that all the random variables here are independent. And one can show this using the isotropic nature of the Gaussian distribution. Then also we know what the distributions are. So essentially, everything that's below the diagonal is a standard Gaussian with variance one over d. And everything that's on the diagonal is essentially a square root of a chi-square distribution. Square root of a chi-square distribution. And for the rest of the talk, I'll actually forget about this diagonal. I'll just pretend it's one. If you carry out the calculations using its concentration, everything works out. So let's just say it's one. Okay, so now we... Yeah, this is kind of just a result of Gram-Schmidt, right? Yeah, just Gram-Schmidt, yeah, sorry. Yeah, you just apply Gram-Schmidt and you obtain this decomposition and it has some name. And again, what do we care about? We care about inner products. Gram-Schmidt came first. First. Hey, me. Did you ever see a fancy name for an obvious idea? Oh my god. I also have the explicit gram schmick, the words better. So we direct knowledge, gramm and schmit. So what's the inner product of two latent vectors? Of two latent vectors, Zi and Zj. It has two parts. It has Zji, which is some independent Gaussian random variable of variance 1 over D, so we expect it to be on the order of 1 over square root D. Then we have some cross terms. So in the regime where we are, where k is of size polylog, because we took our subgraph to be on polylog vertices, we have polylog cross terms, and each of them is a forger one over u with very high probability, which means that this. Probability, which means that this second part, the cross-terms, is of much, much smaller order, which suggests that, well, maybe really these independent ZGIs really determine whether there is an edge or not. So, more concretely, let's make the following observation. Each inner product is Zji plus minus something, that's some delta which is on the order of polylog over d. In particular, as soon as Zji is not in this interval. Is not in this interval negative delta delta, then the sine of Zji and the inner product of Zi Zj is the same, which means that the sine of Zji determines whether there is an edge between I and J. This is really incredible news because ZGIs are independent and we want to find independent edges. Okay, so let's call all the other pairs fragile. So whenever ZGI is in this interval, we call them fragile. In this interval, we call them fragile pairs. Let's find some properties of this ZGI. So, first, fragility is rare. AGI is fragile only with probability around 1 over square root D. Why? Because this interval has length 1 over D and Z Gi is normal with variance 1 over D. Then also, different pairs are fragile independently, simply because the ZJIs are Simply because the ZJIs are independent and those and fragility is determined by the ZJIs. And finally, everything else. So all the other edges in the graph, as we said, are independent because they're independent Bernoulli has because they're determined by ZGIs which are not fragile. So actually, we have, in some way at least, shown that we have such a picture. So we have some fragile edges and everything else is Kelsey is Bernard. So, yeah, let's. There is still some caveat, but let's try to calculate the concrete Fourier coefficient and we'll see what the caveat is. So, suppose that we have this graph H on six vertices and we want to calculate the Fourier coefficient. Suppose that we have drawn the latent vectors and it so happened that the things in blue are fragile and everything else are the fragile edges. So, does this automatically mean that So, does this automatically mean that conditioned on this being the fragile edges, the Fourier coefficient is zero? It seems like it should, right? Because in planted click, as soon as you have a Bernoulli one half edge, this Fourier coefficient is automatically zero. This, however, is not the case here. And it is not the case for the following subtle reason. So, think about this inner product Z5, Z6, which is fragile. The inner property in Z5 and Z6 also includes Z6Z4 as Z64 as a cross term, which means that, well, maybe all of the green edges are only one half. However, the blue edges are a function of the green edges. So if we simply re-randomize the green edges, we'll change what happens on these blue edges, and the Fourier coefficient potentially will not end up being zero. All right, so however, it turns out that these, no, blue edges are. No. Blue edges are a function of a very small portion of the Bernoulli one-half edges, which we call the boundary. And specifically, those are the things that come from the cross-terms of fragile edges. So in this picture here in gray. So as long as your subgraph contains things that aren't on the boundary? Exactly, yeah, exactly what Anchor said. As long as not all of your edges are fragile or on the boundary of the fragile edges, then the free coefficients. The Fourier coefficient becomes zero. And yeah, here is just some definition of the boundary. So, up to this ordering, according to the Gram-Schmidt, if you have an edge BA, which is not fragile and its larger vertex, so an edge that's not fragile, if its larger vertex is incident to a fragile edge, then it's on the boundary. And how would we use this? Well, exactly. Yeah, so going back to our picture. Going back to our picture, this was our leaf planted subgraph representation, but what we actually get is something slightly more complicated with this boundary. As I said, again, everything, all the green edges plus the boundary are Bernoulli halves. However, the planted dependent edges depend on the boundary. And yeah, I mean, essentially, here are this. And I think if you are to remember one thing from this talk, I think it should be this equivalent representation. It should be this equivalent representation of random geometric graphs because it's, I think, just a very different way of viewing random geometric graphs, and I believe it's also more broadly useful. For example, if you're in the okay, I guess it's like a little bit more painful if you're insisting on being able to be doing the special value, but I guess it's yeah, they're not quite independent, so you won't get that. You can still work with Gaussians, but you have to like divide by the norm and now like you multiply the normal. None of the angles change. None of the angles changing. Yes, so actually, that's exactly what you do. If you want to do the same argument for the spherical case, that's exactly what you do. You first draw your vectors as Gaussian, then you divide by the norm. And then some terms, so these cross terms become slightly more complicated, but they depend on essentially the same edges. So you actually get the exact same picture. Yeah. The real caveat. The real caveat is that here you need the dimension to be, well, I guess it's the exactly opposite inequality, but the dimension needs to be much larger than the number of vertices because you want to run Gram Schmidt. So I think that's the caveat. It's not really whether you're spherical or Gaussian. Okay, now going back to Anker's observation, as long as the union of the fragile edges and the boundary Edges and the boundary is not everything, then the Fourier coefficient zeroes out. So, in particular, if we define psi to be the minimal size of a fragile set, such that the fragile set, and so define psi that way, so the minimal size such that fragile set in its boundary cap or so. Edges, you need at least that many fragile edges to get a zero-free coefficient. However, each edge is fragile with probability one over square would be essentially, which nearly gives you the. Essentially, which immediately gives us the following bound. Okay, so we obtained this bound and then we tried to use it, and somehow it looked very weird, so it didn't immediately give anything super useful. Then there is something very interesting about it. So this bound depends on the ordering of the vertices in Gramschmid. Because the boundary, if I go back, the boundary is defined by this ordering. So the So the boundary depends on which, so whether something is in the boundary depends on an event that's related to the larger vertex. However, what's larger and what's smaller is purely artificial because we just chose some ordering for the Gram Schmidt. So okay, is this problematic? It seems like there is something very non-intrinsic happening here. And yes, but also this allows us to just choose the optimal ordering. So we essentially do exactly what. So we essentially do exactly what I said. So let's call this side the ordered edge independence number with respect to the ordering phi. And we can just optimize over pi and we get this ordered edge independence number. Sorry, what's in the expression? What's capital Phi? Psi? Yeah, Psi. Capital Psi. So minimal set such that the set and its boundary cover all edges. Yeah, and with respect to this ordered edge independence number, which we defined, we essentially get the same. Find essentially get the same bound, and here I've even stated it for all densities. Okay, now we actually want to use this bound, right? So, to actually use this, we need to learn something about this ordered H independence number, because otherwise, great, what is that? And okay, let's go to the first example that we had, which is testing against Erdogan. Turns out it's somewhat easy to show that the order to H independence number is at least the number of vertices divided. At least the number of vertices divided by two to this like fractional part. Actually, this immediately gives you the low-degree hardness picture. So, there is some regime in which you can test by signed triangles, and otherwise you cannot test by any degree, I don't know, log square, or you know. But the punchline on for games? Sorry? Yeah, now it turns out that this phenomenon is even stronger. So, let's think about this model, which Alex told us about a couple of days ago, where Told us about a couple of days ago, where instead of observing a random geometric graph, the full random geometric graph, you can choose a more structured subset of edges and only query those edges. Can you do something better? Well, it turns out that again, random geometric graph just completely dominate the picture. So either there's a regime in which you just need to see the entire graph and you can detect by common sign triangles, or no, you cannot really do that. Or, no, you cannot really do anything with low-degree polynomials. Now, here there is some slight subtlety in the proof that we use. So, suppose that you want to prove this for a given mask M, and then you carry out this low-degree chi-square calculation. In particular, you need to understand how many copies of a given graph H appear in M when M has a bounded number of edges. And luckily, this was resolved, luckily for us, was resolved by Us was resolved by No Gallon in the 80s. So it turns out that the maximum number of subgraphs a graph on M edges can have is up to some constant, which doesn't really matter. m to the power the number of vertices divided by 2 plus some other combinatorial quantity divided by 2. This combinatorial quantity is this delta H, which measures the maximal difference between a set and its neighborhood set. And here I think Guy and I really surprised us, because we surprised ourselves. us because we surprised ourselves because we managed to prove that the shortage independence number is actually also lower bounded by this combinatorial quantity plus one. And essentially this was like the key part of this proof. Otherwise it's again just the standard low degree chi-squared calculation. Okay so so far we've shown two different ways in which sine triangles really seem to be by far the most informative low-degree test for random geometric graphs. This gives the following conception This has the following conceptual implications. So, suppose that you take some completely different model. It has nothing to do with random geometry graphs, except that also it's like the dominant low-degree free coefficient is also signed triangles. And somehow the side triangle counts in both models much. Then, heuristically, you shouldn't be able to distinguish these models, because their most powerful low degree tests match. Well, it turns out that this is exactly what happens, and we illustrate this with the planted coloring model. With the planted coloring model. So, in the planted coloring, we have essentially like the planted click, except that each vertex appears independently one of two clicks, and then everything else is still air-double-shriny. We use slightly lower density than one-half simply to match the edge density. There is also some so this model was studied by in a paper by Provesh, Alex Wine, Sandlarge Vampaline. Uh, San Laj Vampala and Jeff Shup. Jeff Shield, thank you. Uh, yeah, and they study almost the same model, except that for them, the ambient air Gushrain is a density one. So, what we obtain is that there is some non-trivial regime, such that testing between random geometric graphs and this model is slow degree hard. And this is a regime in which both of them, in particular, are distinguishable from Ergus Ready. From Ergus Schreini. And so let's make two observations. First, at least for pretty much in the entire regime, there is an information computation there. Because this planted coloring model has a click of polynomial size. In fact, most clicks are... There are so many clicks of polynomial size. Well, I only need a couple of minutes at most. Well, random geometric graphs, the largest click. Geometric graphs, the largest clique is of logarithmic size. Also, another comment is that actually this, when you translate this relationship between D and the number of colors, testing between, like the harness we obtain is actually in the exact same regime where in their paper they obtain harness for refuting Q-colorability, but by some other Q-colorable distributions versus surgical rain. And so, yeah, let me end with some open problems and directions. First, obviously, you know, there is the elephant in the room, which is the information-theoretic convergence between random geometric graphs and ergonomic shredding. But I would like to propose a few maybe more modest goals towards it. So, first, I think that this planned subgraph view of random geometric graphs, which I presented, has other applications. In fact, I mean, we even know how to use it. You know, know how to use it in the context of information-theoretic convergence, but you know, not yet to close this gap or even to prove anything new. Then there's also defining these fragile pairs, which are essential for our planet subgraph representation, without reference to Gram-Schmidt. And this is for two reasons. First, I think it's a purely aesthetic reason that I mean, even though we optimize our orderings, I think still this entire business with orderings. Still, this entire business with ordering is slightly artificial. It's very convenient to analyze, but there is something new about it. But also, potentially, we want to do it when n is much larger than d. And then probably just this Gramschmid. Yeah, you just cannot run Gramschmid. Then I also think it could be cool to find lower bounds and precise estimates on Fourier coefficients, because that's what actually gives you an algorithm. So if we show that Fourier coefficients are small, sure, we have Carmen's, but if we want actual algorithms, If we want actual algorithms, let's say to estimate the dimension or to match two noisy copies of random geometric graphs, I think actually estimating the freight coefficients is much, much more important. And the last thing is, so with the bounds we currently have, even if we hope to improve them a little bit, there is a barrier at showing currents against polynomials of degree log square. So can we go even to log square plus epsilon? I think this would already be interesting and maybe even degree. Interesting and maybe even degree polynomial or n, or this should be viewed as some intermediate regime towards proving the information theoretic arguments. Why can't you go higher? Yeah, I cannot go higher because the Fourier coefficients, the bounds that we get decay exponentially in the number of vertices. But then the number of subgraphs grows exponentially in the number of edges. So essentially, when the number of it's the same thing that happens with planted thing. There it has to happen. There it has to happen. So for planet click, even if you plant every vertex with probability 1 over n, it still blows above the log squared. So you need some conditional chi-square. And I assume that here, probably also if you use some conditional chi-square, maybe you can blow better than log square. maybe you can better nam square but they have no clue how what to conditional how to compute square coefficients and so on. Look at change in a simple soft geometric model where the probability at tent is Ahead of the head is some other function besides a threshold function of yeah, well, it will change. Yeah, I mean, that's something that I've been thinking for and Kevin. I don't think I have a very good answer. But a lot of things change. So, for example, if obviously if the function is very flat, you know, we will converge much faster to Herr Schray simply because everything, you know. And I mean, there are also some gaps. So there is. I mean there are also some gaps. So there is this paper of Suchimo and Kirazu first studied some like Lipschitz connections. Then yeah, Kevisara paper on which he studied Lipschitz connections. But yeah, I think a lot of things change, but yeah, I don't know what the simple answer is. Thanks, yeah. Thanks again. Hi there, everybody. For what I presented, you know, actually it seems like the scratch one is the only thing is that in the one. So the only thing is that in the one-half case, you can actually prove something stronger. So what I presented, we can exactly prove for every density. For the special case of one catch, we know how to prove a slightly stronger efficiency. But it's only marginally stronger. Like the most exponent is twice as big, which gives you something, for example, the sex writing value, but also doesn't give you current slope squared for sex loss. So in the one, okay, in the ST1 half there is a symmetry around zero, which actually allows you to define a slightly smaller boundary. Like this is the pessimistic boundary I define, which works for low densities, but for the very, very special. But for the very, very special case of one web, I think you find a slightly smaller one. But I can't, I don't even know how to do it for one-third. So, do you have a guess is like it's very important. Yeah, it's like blocks squared because it sort of well I don't know, I mean like hints of the hints of the probability identified 16 inches small and just front. I'm trying to guess, like you could try to let it take a large solution, which you like. It's likely long man. Yeah, our yeah, also like, wait, you guys, so you need do you need at least the following login and uh like can you just take the union round over the edges for uh fragility or um  But the reason you're going to break down today is that you can use the first dimension. No, you're not the first person. By the way, I found some of my shelves, so you found some of the stuff. So you need the dimension. Yeah, dimensional shelves. For this, we need to make sure that Not a non-graphical, just okay. I see, I see. Okay, I mean, it's not any questions. That's not bad. It's like five minutes. I can join you. I am not joking. David's got an infinite partial. Yeah, David has also been to Mars. Actually, he's very, very educated. I really care about the participation separately. 