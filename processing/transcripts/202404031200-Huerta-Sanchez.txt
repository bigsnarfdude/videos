Great. Welcome, everyone. My name is Emilia Huerta Sanchez, and Mark Broome, who's here, and I are co-organizing the Early Career Research Series, which, as you know, is part of the hybrid thematic program on modeling and theory in population biology. And of course, all of this is made possible by the bound. Course, all of this is made possible by the Banff International Research Station. And just as a reminder, next week we have an event. Alex Diaz Popkovich, who is a postdoc at Brown University, will be discussing population genetics and Wikipedia. And so that's next Tuesday. Okay, so today I am delighted to introduce our two speakers. So first we will hear from Lenore Pipes. We'll hear from Lenore Pipes, who is a postdoc at UC Berkeley, and she's interested in developing phylogeny-based methods for metagenomic data sets. And after that, we'll hear from Dr. Maria Martignoni, who is at Hebrew University of Jerusalem. And her research interests are in host-microbe interactions and in developing new theory that accounts for their significance in ecology and evolution. And so, first, we're Evolution. And so, first, we're going to hear from Lenore, and then we'll have time for a few questions, and then Dr. Maria Martignoni will go next. Okay, so with that, I'm going to give you the floor, Lenore. So welcome. Hi, so I'm really excited to be here today to be able to talk to you about some of the work I've been doing, my postdoc, and it's really And it's really a pleasure for me to tell you how I'm developing these methods within an evolutionary context to study biodiversity. And so I'm developing these computational methods that are really ideally for these massive environmental DNA data sets that people are producing now. And so eDNA has so many applications that can answer questions not only about present. Not only about present day, but also the past. You can start to, with these data sets, you can start to answer questions about pathogen disease monitoring, such as how to detect and monitor spread of pathogens in the environment, how to detect, assess biodiversity, including detecting rare or elusive species. Even with counterterrorism, we can start to sample suspects' clothing or shoes to. Clothing or shoes to trace their recent travel history or international locations they've visited. With climate change, we can start to understand the effects of climate change on different species and their habitats. With community assembly, we can start to understand how abiotic factors such as temperature or pH affect the availability of influence of the assembly of communities. And with ancient environmental samples, we can start to reconstruct these ancient ecosystems and understand these life forms that existed in the past. So what exactly do I mean when I say eDNA? Well, eDNA is DNA shed in the water, soil, or even the air by all of the organisms. And we can sequence all of the eDNA in a sample to identify. In a sample to identify everyone who's been there. So it's this really easy, economical, and non-invasive method that results in this comprehensive biodiversity assessment. And because of the rapidly falling cost of sequencing technology, it's being used at a scale that we've never seen before. But it's been blocked from its potential because it's been limited by the methods. Limited by the methods. And today, hopefully, I'll show you what that means. So, the fundamental challenge in eDNA research is being able to assign a taxonomy to a read that's been sampled from the environment. And this is a really difficult problem because this relies on a reference database where certain regions of species are sequenced. And these regions ideally evolve just enough to be different from another. Enough to be different from another species to serve as these barcodes. And I became interested in this problem when I was a grad student at Cornell because my lab was involved in a study called the PathoMap project. And the goal of the Pathomap was to sample, sequence, and identify all the organisms living in the New York City subway system. And my PhD advisor, that's him there, he did not, in fact, go out and collect. Not in fact go out and collect all the samples. He recruited about 20 Cornell undergrads one summer, and they went out and sampled every single subway station, of which there's over 200 of them in triplicate. And one of the main findings of the study was that most of the DNA belonged to rats. But the other main finding was that they had claimed to identify anthrax and the bacteria that. Anthrax and the bacteria that caused the bubonic plague. And so, as you can imagine, this claim got a huge amount of attention, not just the New York Times and Wall Street Journal, but also from the CDC. And the CDC ended up writing a rebuttal to their paper criticizing their complete analysis of the samples. And this really got me interested in this question about why are analyzing these environmental samples. Are analyzing these environmental samples so problematic, and why are we limited here? How can we have these really like very big public health implications when the analysis is not robust? And so I started thinking about this within a phylogenetic context, and I developed a phylogenetic placement for. Phylogenetic placement for analyses of these types of samples. So, in de novo phylogenetic reconstruction, which most people are familiar with, you have a set of aligned sequences, you do some inference, and then you construct a tree. With phylogenic placement, it's a similar problem, but you have two sets of sequences: you have your reference sequences and the queries. And for your reference sequences, you spend a lot of time. Sequences, you spent a lot of time inferring the tree, you're happy with the tree, and you just want to know where in the tree these new sequences belong. And so, file placement is the addition of these new sequences onto the existing tree. And so, it's not a new problem. Traditionally, people have been using phylogenic placement as part of a greedy algorithm. You start with a small tree and you add a Tree and you add queries one by one. But it really had this revival in about 2007-2008 because it had this new application. And the new application was with microbiome data. And so in microbiome analysis, you have a set of unknown sequences from unknown organisms, and you want to know what they are. And you can look for the best placement of these sequences onto your reference tree. Onto your reference tree to tell you something about who's making up your sample. And so, right now, phylogenic placement is used pervasively in all types of application. You might want to know what species you have in a sample. You can use it for functional analysis. And if you have two different conditions, you can compare the placements across conditions to tell you something about how these. About how these change, how the conditions change. And recently, people have been thinking about how to use phylogenetic placement to understand COVID. You might want to understand who transferred the virus to whom. Well, you might want to know who infected the president of your country. So you can get the sequence and you can place it in the tree and it'll tell you something about the transmission. And so. And so, since this revival with microbiome analysis, there was a lot of development in this area, specifically with maximum likelihood-based methods early on in Eric Mattson's group with P. Placer, and Alex Damutaka's group with EPA. And more recently, people have. And more recently, people have shifted away from maximum likelihood-based methods because of the scalability issues, which I'll discuss. But at this point, you might be thinking about why do we need to do placement at all? Why can't we just compare one query to all the sequence in the reference database? Why can't we just blast it? And so I'll show you a couple examples of why that's problematic. Examples of why that's problematic. In this one case, where we have the true placement of the query is in red and it's equidistant to a large sister taxa. Well, with closest match, with BLAST, you will just arbitrarily pick one of these sequences that are equine distance, which would be incorrect. Another issue. Another issue with blasting is that there's not uniform, the rates of evolution are not uniform across branches. And so in this case, your true placement of your query is in red. And its closest match is really R1 in terms of the sequence distance. But in terms of the phylogenetic distance, it's really closer to R3. So in this case, you would. So, in this case, you would get a misassignment. And so, in practice, what does this mean? In 2020, Balaban did this analysis, which they started with a very large tree that had like over 100,000 leaf nodes, and they just downsample this tree down to 500 leaf sequences. So, this is just the same. Sequences. So this is just the same tree, just down sampled down to 500. And so they did the missing out test where they left out some of these references as queries and they tried to insert them back in just by finding the closest match. So this alignment-based approach like BLAST. And what you see here is that 50 to 60% of the time, you find the correct place. Time you find the correct placement, but the rest of the time you are misled. And then you might be asking yourself, well, maybe you can't do any better. This is a really huge, huge tree. And the answer is you can. And so the two methods that I mentioned earlier, P placer and EPA, do far better than just placing by closest match. And P placer is actually quite a bit more accurate than EPA. Quite a bit more accurate than EPA. But the issue is that when the reference tree keeps growing, these methods stop to be applicable for various reasons, maybe numerical issues or memory or running time. But the thing is, you can't run these methods when your backbone tree is like 100,000 sequences. With more recent disk. With more recent distance-based measures like apples, we can extend the scalability of the phylogenetic placement, but it still doesn't achieve the level of accuracy that, for example, P placer has. And so how slow exactly are these methods? So in P placer, if we were just to do a back-of-the-envelope calculation of placing a Of placing a single query on a reference tree that had over a million and a half taxa, it would take about 20.9 hours just to place this single query. And so I made this schematic here of the current methods. And where we really want to be is in this top left corner. So Apple improves on the scalability in terms. The scalability in terms of speed between P placer, but we wanted to know, we wanted to build a method that would make this even more scalable. And so what's the current problem? These methods are really slow and memory intensive, and they can't handle the types of data sets that people want to analyze, which are both they're generating millions of reads of queries, and they're also Of queries, and they also have a reference database that has millions of sequences. And so, we developed this method called Tronco. And Tronco is just the Chamorro word for tree. Chamorro is the native language of the island that I'm from. And we built this phylogenetment method where underlying it, we have this multiple sequence alignment, and we estimate this tree. We estimate this tree from the multiple sequence alignment. The tips of the tree are linked to this multiple sequence alignment. And just taking the first column and the different character states in the nodes of the tree, we know the probabilities for each character straight at the tips of the tree. And we can just use Felsenstein's algorithm to calculate the To calculate the conditional likelihoods in the internal nodes of the tree. So we're just using this dynamic programming, message passing algorithm to estimate the ancestral states of internal nodes. And so we do this, we calculate the likelihoods for each character state in the internal nodes. And then we move on to the second. Move on to the second site, for example. So, this is just using Felsenstein's algorithm to compute these conditional likelihoods. And then we add this query into the alignment. And so when we add this query into the alignment, we can calculate a score calculation using these conditional likelihoods. And we just add in. We just add in a few other terms for sequencing error rate and for handling gaps both in the query and the reference sequence. And so we can calculate these scores for different nodes in the tree. And so the score really maximizes the probability of observing the query sequence if it's placed exactly in that node. Exactly in that node. And so we can get different scores for every node in the tree. And then we can simply find the best placement with the highest scoring node. And the difference, and you might be thinking, well, what is the difference between previous maximum likelihood-based phylogenetic placement method and this method? One of the issues we One of the differences we use in my method is that we're actually doing a node placement instead of an edge placement, which really improves the scalability of the method. And so once we have a placement, we actually apply what's a lowest common ancestor algorithm. And we use this because we don't want Because we don't want multiple placements in one tree. With the lowest common ancestor algorithm, we can just have one placement in the tree. And users can increase or decrease this cutoff to have a more aggressive or conservative assignment. And so we wanted to. And so we wanted to assess the accuracy of this method by taking a data set of shorebirds, where shorebirds are often misidentified by the wrong species. And so we do this missing out test where we take out a species from the reference sequence, simulate sequences from that missing species, and then try to assign this back into the assign this back into the Back into the assigned placement of this using our method. And so we had these two measures that we are looking at, the misclassification rate, which you can think of as the false positive rate, how many mistakes we're making, and the recall, which is how many reads we're assigning to a certain taxonomic rank. And we wanted to compare our method. And we wanted to compare our method Tronco against, you know, obviously the other phylogenic placements, P, Placer, and APPLES. But our main motivation in developing Tronco was to actually test our method against the methods that people are routinely using for these large eDNA and microbiome projects, such as the Pathomap project. The Pathomap project used Metaflan, for example. K-mer Kraken 2 is another very popular. Another very popular K-mer-based method, and Megan is a BLAST-based method. So, in a perfect method, you'd have a 0% misclassification rate and 100% recall. And because we can adjust our LCA cutoff to have a more conservative, aggressive assignment, we can build this rock-like curve where we know that methods we can take. Methods, we can tell if the method that we're comparing to is better or worse. So, with our non-phylogenic placement methods, we see a great deal of misassignments at the genus level. And with Megan, it's much more conservative. There are fewer mistakes, but they're still pretty significant, have a pretty large misclassification rate. Misclassification rate. With phylogenetic placement methods, we're making fewer misassignments. And if we add in our method, we can see that the other methods are in fact lying within the convex hull of our method. And we're doing about as good as P placer, which is what we'd hope. But at the family level, we're doing even better. So we've developed this method. Developed this method that we like to think is lying in this top left corner here of the schematic. And this worked really well in practice where you have a single order of birds that align really well to each other. But as you start to include more and more diverse species, the quality of this multiple sequence alignment breaks down and it starts to not, the tree starts to be cut. It starts to become more problematic. And because of this, we developed this method called Ancestral Clust, which just allows this, is a phylogenetic-based clustering method for diverse sequences and allows us to break down these very large data sets that have millions of sequences. And because we're allowed, we break down this. Down this, all the sequences into different groups. We have clusters for different diverse taxonomic groups, and we can just score each most likely tree in these databases, which also improves the scalability. And so we tested our method Tronco against mock community data sets that. Mock community data sets that people had generated. Mock communities are just data sets where they know the bacteria that's included. They know what bacteria and archaea species are being sequenced and sampled. And we showed that we have very few misassignments with these much more popular Kraken and Popular Kraken and non-phylogenic placement methods. And so, going back to the Balaban data set, we showed that we have actually, in fact, proved that we're scaling at the P placer level accuracy and really improving on the accuracy such that as you get to 100,000 tips, Tips that you're getting a placement accuracy close to 100%. And in terms of speed, how do we do? We can actually assign a million queries on a database with over a million and a half sequences on a single core in about two to three hours. And so, some of the key takeaways: we developed this phylogenic placement method that just scales at P placement. At peak placer accuracy and can be applied to these very large data sets. So, what are we? Are we? We built this cool method. Are we actually doing using it for anything? So, yes. Last November, we launched this eDNA Explorer, where Tronco is the core engine behind this website. And this was made possible by a million-dollar grant by the OceanKind Foundation. And these are just And these are just some of the partners using the eDNA Explorer. And the motivation behind EDNA Expo was to have people who have very limited computational skills, you know, give them access to these more complicated, like command line programs that they wouldn't otherwise be able to access. And with EDN. And with eDNA Explore, I got involved with the LA River project, where in the LA River project, Rachel Meyer from UCSC, she's interested in studying the effects of urbanization of communities in the LA River. For example, how does laying down concrete in the riverbed affect community assemblages? And previously, they had used an alignment-based method called ANA-Kappa, but switching Call Ana Kappa, but switching to Tranco, they saw this huge increase in the amount and diversity of the attacks that they identified. And they were also interested in investigating how human behavior affects environment, and they use this metric called global human modification. And GHM just reflects the proportion of the landscape that's been modified by human activities. So you'd expect to see a correlation between human activities. See a correlation between human activity and species composition. And with the previous method, they weren't able to find that correlation. But switching to Tronco, they found this robust association between these two measures. With the Pacific Shellfish Institute, they're interested in how these cultures of eelgrass and oysters increase the diversity of fish. And previous to eDNA methods, they used these seam. They use these scene and video capture. So they know actually what species are in the bay. And previous to Tronco, they weren't able to actually detect the species that they knew were there. So that was an exciting improvement that we've been involved with. With a grad student, Maya Lemonkeishi, in the Nielsen lab, we're working on using Tronco to do. On using Tronco to do some molecular dating. I'm running out of time here, so I don't really have that much time to go into this. But the goal of this is, you know, carbon dating can't date past 50,000 years ago. So there's a huge motivation to be able to date samples that are past this point. And one path forward is through phylogenetic dating. And so we're And so we're developing this method called rate placer to date these ancient samples. And some of the simulations I wanted to show here was that with using simulations where we know the true age, which just represents an age estimation based on the molecular clock. We've done simulations mimicking cytochrome oxase 1, which is a meta-barcode, next generation sequence screen reads. And we tested this on different true, different age estimates. And with different age estimates, we're doing pretty well. Estimates, we're doing pretty well in context where we can test ages that are millions of years old. Of course, the further you go back, the more difficult it is to estimate these ages. But a recent study from Esko Willers' group from Willers' group from taking ice cores in Greenland, they dated these samples back to 2 million years ago. And so, we're actually in that range where we can have a very good estimate, phylogenetic estimate of these types of ages. And finally, with what I wanted to just mention, as I mentioned, as placement gets more accurate, trees get larger. So, we actually use. So, we actually used Tronco in this tree imputation approach, which we showed was orders of magnitude better than a naive approach. And in doing this, we identified a site where there was very high error in our tree imputation approach. And this turned out to be a site where there was a sequence. Where there was a problematic sequencing site. So, because of that, that led to this application of using Tronco to leverage these highly accurate phylogenetic priors to correct this low-quality sequencing data. And that's together with Monica Arniello, who's a grad student in the Nielsen group. Sorry, I went over time, but I'd like to thank Rasmus and Rasmus and Maya and Monica, who are, and Rachel. Yeah, and happy to answer any question. Thank you, Lenore. Sounds like Toronco has a lot of really interesting applications. Why don't we take just one? Does anybody have any? We have time for maybe one question. Please speak up if you have a question. Florence, go ahead. Yeah, thank you. That was super interesting. I was wondering what kind of input do you use for your method? Can you use it for untargeted sequencing, for instance? Yeah, so we've started to do, we started to use it for SARS-CoV-2 wastewater shotgun sequencing data. shotgun sequencing data so we can do we can use tronco on um on genomes that are small enough um uh we started to use it on mitochondrial whole genomes we're we're building a data database now for that