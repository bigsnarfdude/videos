Okay. Hello, good morning, everyone. So thank you again to the organizers for this nice event. So I'm going to talk about a specific application of machine learning models for the reduction of noise events in the search of gravitational waves. Of gravitational waves from core collapse supernova using the coherence waveboard method. Again, I'm Mauricio Antelis, and this is a collaboration with other colleagues. Okay. Some moment, something is happening here. Okay, I want to start remembering that up to today, there are about or a little. About or a little bit more than 100 gravitational waves detected, all of them from binary systems and mostly from binary black holes. And one important characteristic here is that the search of these gravitational waves benefits from having highly accurate signal models. And therefore, the search is based on much filtering, the strain. Based on much filtering the strain data with available template signals. There are, however, other astrophysical sources for gravitational waves, ascorcollapse supernovae, which are also a detection target in the upcoming observing runs with LIGO, Virgo, and CAGRA. And the importance of CCSN is that Is that the electromagnetic radiation with the emission of neutrinos and also with the gravitational waves will provide new information to the multi-center astronomy. However, the morphology of gravitational waves from this time of source is highly stochastic. And therefore, unmodeled and modelled searches are the approaches available for this kind of demand. For this kind of gravitational waves. And the coherent waveboard is the standard method for the search of this kind of birth transients. CWB does not impose any assumption or at least minimal assumptions about the morphology of the signals. Very briefly, it is based on excess power analysis. Is based on excess power analysis, on wavelet analysis, and on coincident tests, obviously to detect gravitational waves, but also to reconstruct the waveform and to estimate some signal parameters. CWB has played an important role in the previously detected gravitational waves, but importantly, CWB is going to be essential in the next upcoming Upcoming observing runs for the potential detection of gravitational waves from supernova. Okay, there are, however, other difficulties in addition to the stochasticity of these gravitational wave signals. First of all, the collected string data contains a lot of bleaches. And the critical part is that the morphology Part is that the morphology of those glitches is similar to the morphology of the gravitational wave transients from supernovae and also from other unknown sources. The consequence is that there is an increase in the false alarm rate and a reduction in the statistical significance. And this happens despite rejection tests when we have multiple detectors. Detectors. It is necessary then to detect and to remove those non-astrophysical events, those noises. The other complication here is that there are periods of time where only one detector is in operational conditions. That means recording science quality data. Indeed, Gabriele Vagentes mentioned something related to that, if I remember. Something related to that. If I remember correctly, she said that about 12% of the data is lost or there is only one detector available. And the critical part is that that represents about 11 events that were lost. So basically, when we have period of time with only one detector available, it is not possible to do. It is not possible to do the coincident test, and therefore, there is a reduction in the statistical significance. The question is then how to improve the detection significance when we are searching for gravitational waves from corcollac supernovae with the coherence wave or method. In other words, how can we reduce the number of noise events? Well, machine learning. Well, machine learning methods offer to us new opportunities, new alternatives to address this kind of problems. And I have to say that machine learning and also the special kind of deep learning models have been used extensively for gravitational wave astronomy in the recent years. For instance, those methods have been used to discrete. Has been used to discriminate between noise and gravitational waves embedded in the noise. And also, they have been used to identify and to remove glitches. They have been used to enhance signals and thus to achieve higher sensitivities. Importantly, they have been used also to enhance the performance of Enhance the performance of CWB. There are also review of applications, and during these days, we have attended to many other important works related to machine learning and deep learning for gravitational waves astronomy. But I want to highlight here a recent work from Marco and others, where they use genetic programming arguments. Use genetic programming algorithms to discriminate between signal and noise from CWD with single detectors. So this is an important work that demonstrated that it is possible to reduce the background, to reduce the distribution of noises in CWB when we are operating with CWB. With CWB. So, in that line of research, here we are using machine learning models as a follow-up method for CWB to reduce the background. And we want to extend the previous study not only with one interferometer, but also with a network of interferometer. We want to use our idea here was to use other classification models that do not rely on random stochastic initiatives. Random stochastic initializations. And most importantly, we wanted to perform studies to assess the improvement in the statistical significance. So in simple words, basically what we want to do is to recognize using a machine learning model whether an event produced by CWB belongs to non- Belongs to noise or to signal given a set of reconstruction parameters from CW. So that's basically the idea of this work. And I want to illustrate a little bit more the importance of noise reduction and the use of machine learning as a follow-up method for CWB. So let's consider that we have a CWB. Have a CWB analysis where we have the background and we have the simulation with the background in the bottom plot. In the top third plot, we can see the distribution of noise events, which is used to compute the false alarm rate. In the bottom, we have the distribution of signal events, which is used to compute sensitivities, detection efficiencies. Detection efficiencies basically. So, our idea is somehow we have a machine learning model which is able to discriminate between those two places. And the idea is that we want to detect as many as possible noise triggers in order to reduce the distribution. We have a distribution of noise triggers there, and we have to reduce that distribution over there. Distribution over there to reduce the number of noise. On the other hand, another important aspect of our model is that we want to keep as many as possible signal triggers. So that's basically the idea. And if we are able to do that, then we can compute the false alarm rate before and after the application of the model. We can compute also for a specific Also, for specific also unsource windows, the false alarm probability before and after the application of the machine learning model. So, this is what we want to do to reduce the false alarm rate and the false alarm probability. But at the same time, we want to maintain, to keep, to do not affect the sensitivity of the search. So that's the idea of machine learning model. machine learning model as a follow-up method for CW for CWB. Okay, so with all these ideas, let's move then to the specific work we have been doing. The first thing is that we carry out a lot of analysis in CWB. We basically we are working with strained data from the zero-observing ROM. The theater observing run, we selected two different strain data setments. Well, to be honest, we have carried out a lot of analysis with different strain data setments, different time windows, but in this presentation, I'm going to show you only the results from two time windows. And all the CWB analysis are done with one detector, Livingston, one detector, and for all with. Or with two detectors in the network. The idea here is that we have to do the background analysis in order to compute the false alarm probability, in order to compute, sorry, first of all, the false alarm probability rate and then the false alarm probability. Then we have to do simulation analysis in order to compute efficiency versus the distance of the injected. Distance of the injected gravitational waves, because here we have to inject gravitational waves at different distances in order to compute the proportion of correctly detected signals by CP. Okay, but this background and simulation analysis are not only important because they are an essential part of CWD, but they are as important because they provide us with They provide us with distribution of noise and signal triggers, and we can use that data to train and to test our machine learning models. So that's another important aspect about the analysis. I want to mention here that our simulation analysis in CWB considered several gravitational wave signals from 3G numerical simulations, and we selected And we selected some families that were used in the all-sky search of short gravitational waves during the auto run. And also, we selected some families of gravitational waves that were part of a retina study devoted to quantify the detectability of gravitational waves from supernovae in the upcoming runs with the LKK. Upcoming runs with the LDK network. Specifically, we use, for instance, seven gravitational waves from O'Connor 18, five gravitational waves, yes, five gravitational waves from Powell 19 and 20, one gravitational wave from Mesacapa, the name of the waveform are there, are the same that they are reporting in the... In the seminal paper of those signals. And we also included a little bit older gravitational wave, family of gravitational wave from supernovae, Scheidegger 2010. Basically, we selected three waveforms from that big set. One with or generated by a slowly rotating progenitor, the other from a moderate. from a moderate rotating progenitor and the last one from a rapidly rotating progenitor. Basically, our idea here was to have a family, a family of gravitational wave signals with different characteristics in the progenitor. And as an example, we can see here the time domain evolution of the gravitational wave, plus and colours polarization. This is from Polarization. This is from Mexica, I think, yeah, Mexica. And here at the bottom, we can see the spectrogram of the plus polarization, where basically we can observe the run-up in the frequency of the signal, which represents the G-mode of the gravitational wave. So that's a little bit the description of the gravitational waves that The gravitational waves that we use. Now, I want to share with you one example of our CWB results. In the left, we have the sensitivity result, the result of the sensitivity study, the simulation. We have efficiency versus distance of the injected gravitational wave. Basically, we obtain the We obtain there the typical reduction in the efficiency as the distance of the gravitational wave increases. But most important of that is that the efficiency are similar for detector networks with one interferometer or with two interferometers. So the efficiency is quite silly. So that's a good observation. Now I want to switch to the background analysis. To switch to the background analysis. In the middle, we have the false alarm rate versus the kissing row. And the important observation there is that the proportion of noise events is higher for networks with one detector. And basically, the reason is because it is not possible with one detector to do the Detectors to do the time-shifting data across detectors to increase the lifetime, and also it's not possible to carry out the consistency tests. And because of that, the proportion of noise triggers is bigger. And also, I want to show you here the false alarm rate versus the false alarm probability for a on-source window. A on-source window of one second. And basically, the false alarm probability is about three sigma, a little bit less than three sigma for one detector and is between four and five sigma for two detectors. So this is important to show that we want to move these operation points to the left and down. And down. That's what we want to do to improve the statistical significance. So there is a need of a follow-up method, CWB analysis, to identify and to discard noise events in order to improve the statistical significance. But at the same time, we want to keep, we want to maintain, we don't want to impact our sensitivity. In fact, our sensitivity. So that's the idea of the machine learning method as a follow-up. So what we are using as features to recognize between noise and signals is basically a set of parameters from the reconstructed signal provided by CWB. So we are using parameters such as the volume. Such as the volume of the time frequency representation, the number of significant pixels in the scalogram, the duration of the signal, the spinal frequency, the bandwidth, and others. So our feature vector is basically an n-dimensional representation where n is the number of features, it's 11 for one interferometer, and each vector belongs to signal. vector belongs to signal or to noise for the case of a training set. Or in operational conditions, we want to infer, we want to estimate, given a machine learning model, whether that feature vector belongs to signal or to not. So that's a little bit the idea. So our machine learning model or classifier is basically a computational model. computational model where the input is the feature vector and the output is a label indicating nodes or signals. That's basically the idea. In this study, we decided to work with linear or non-linear classifiers that are based on the construction or destination of a separation hyperframe. Separation hyper frame. So that's basically what we wanted to do. Instead of using genetic programming, we decided for this other type of classifiers. In those classifiers, the class for a new feature vector depends on which region the feature vector is located. That's basically the idea. So we have a discriminant function where x there is a feature vector and vector and w is a vector of weights while b is a biased term and we need to estimate to compute those those two parameters the vector of weights and the bias to do that we use a training set and after that we are able to incorporate the model in in in in in situ In CWB. So, I'm not going into more detail, just to mention that we are working here with linear scheming and analysis and also with super vector machines, one linear and one with regional basis function. Okay, so now what we did, we did two studies. I'm gonna show you now the first study. What we did here was to What we did here was to train and to test our machine learning model using data from the same strain data segment. And basically, the goal was to assess the discriminability between noise and signal. That was the idea there. So the methodology is here in this illustration. We have a data set of A data set of noise and signal triggers, background, and simulation. Using this data set, we apply a scale for cross-validation, where we train and evaluate our models several times. Important to mention, training sets and testing sets are always mutual exclusive. That's important in machine learning. And we can compute then performance metrics. Let me just say for Let me just say for the moment the confusion matrix with the percentage of correctly identified noise and signals and also the false positives, false negatives. Now, this methodology was carried out several times with several combinations, for instance, of signal triggers. So this was applied using signal triggers from a specific distance. Specific distance, I have to say, I forgot to say this before. We use signal triggers from 0.1 kiloparsecs up to 10 kiloparsecs. And also, we apply this methodologing in the data set signal triggers from all distances combined. So that means signal triggers with different signal to. With different signal-to-noise ratios. We also considered in our methodology, each family of supernovae waveform or families together. And in the bottom, I think there is something about, yeah, this methodology was applied for each classification algorithm independently. So basically, this means a lot of executions in our computer with In our computer with different conditions. So, results. We have here one of the results of our analysis in this study. This is for the first strain data set. We are using here signal triggers from Power 2020 at one kilo parsec. And basically, in the upper plots, we have the distribution. We have the distribution of currently detected noise and signal triggers for each detector network. One interferometer and two interferometers. So the observation is that the distribution of noises correctly identified is very high, is almost close to one. And the distribution of signal trigger is also very, very high. and trigger is also very very high although a little bit lower than the accuracy for for noise so that's one observation there another observation is important to mention is much better to recognize between noise and signal when we are operating with two interference anyway our confusion matrix shows that there is a high noise reduction about 100 percent of the noise triggers Of the noise triggers are correctly identified. So that's good because we want to recognize noise and to remove them. What else? Yes, and there is also a low signal reduction, especially for the case of two interferometers. Only 0.2% of the signal triggers interface are misclassified. So this is important because we don't want to remove signal triggers. Move signals. So, this is a summary of those results. Obviously, we have this result for other families, for other distances, but I'm not going to go into all the details. Then, we also wanted to study what's the effect in the classification across the different networks. So, here we have the distribution of Here we have the distribution of noise reduction and signal reduction. So we want no reduction as close as one, and we want a very low signal reduction. So basically, this shows again, it is better the classification, the discrimination for two interferometers. And in any case, the noise reduction is very high, the signal reduction is. Is slow. So that's the case when we are using signal triggers from all the distances, which is the more complicated case for a classification algorithm, because the signal to noise ratio is different across distance. And also, we are using signal triggers from all the families that we considered in the study. And now we have here the results for each distance. For each distance. So that means results of the cross-validation when we are using signal triggers at one kiloparsec, 3.16 kiloparsecs, up to 10 kiloparsecs. So these results shows that the noise reduction decreases as the distance increases, which is expected because again, the signal to noise ratio. The signal-to-noise ratio reduces. And also, the signal reduction increases as the distance also increases. So, basically, this shows that there is a expected behavior in the classification of our noise and signal events. Okay, so here is another analysis. Basically, we wanted to explore if To explore if there were differences in the recognition across the different families of supernovae waveforms, there are differences, a little bit of differences, variability, and basically this is because there is variability of physical phenomena, modeling methods to obtain the gravitational waves from the three-dimensional simulations. But the interesting observation is that the distribution of non- Distribution of noise reduction and signal reduction is a little bit in between. And in this case, this group considers signal triggers from all the families together. And now, what about the classification algorithm? I have to say, sorry, all the previous results were for one classifier. I think it's the support vector machine linear, I think. And now we want to explore. Now we want to explore the differences across different classification algorithms. We could employ artificial neural networks, other kind of models, yes, but we decided to work only with LBA, SBM linear, SBM nonlinear. So in the upper plots, we have the distribution of noise detection, which is very height. Detection, which is very high in all the cases. One observation is, for instance, for the LDA is a little bit lower in the Hanford interferometer. Let me move this here, sorry. So the classification, the observation is the classification of noise triggers is similar across the classifiers and also across detectors, except for Detectors, except for HAMPOR. And what about now? About the classification of signal triggers. The observation is that signal triggers are always better recognized using the non-linear super vector machine. And this is confirmed here in the area under the cube of the receiver operating characteristic. Characteristic across classifier. So the conclusion here is that we decided to continue working only with the non-linear supervector machine. Okay, so now we have here a summary of results of this first study. Percentage of noise and signal reduction. Remember, we want to reduce noise as maximum as possible. Maximum as possible, close to 100, and to minimize signal reduction as close as zero as possible. So, all these results are for different distances of the signal triggers or all combined. This is the most important case for each network of detectors and for each family of supernovae. Just a quick summary. This is the most difficult condition for the classification. Difficult condition for the classification algorithm. Sorry for this. It's the information is here, but not here in the screen. Sorry. So basically, noise reduction is very high, about 100%. Signal reduction is low, especially for the two detectors, 1.7 here, and it's a little bit higher for the handful. So this is a summary of results. Now, second study. In the second study. In the second study, I think is the most interesting, exciting, because we are training and testing our machine learning model using different strain data segments. Basically, we want to evaluate the performance of the machine learning model as a follow-up method in offline surface of gravitational. surface of gravitational waves with CWB. And the idea is here. Basically, we use the full data set from one CWB analysis to train our model. And then that computational model is used in a different CWB analysis, in another stretch of data. And we can compute now the false alarm rate and the sensitivity. And the sensitivity before and after the application of the machine learning model. And obviously, we want to reduce the corporate algorithm. We want to recognize and to remove noises after the machine learning model. So that's basically the idea. So in addition to the Confucian matrix, we can compute distribution of noise triggers before and after, the Pulsar and Ray, the sensitivity, and so on. Again, this was done. Again, this was done considering different situations, signal triggered in specific distances, each family of supernovae waveforms. But I have to clarify here. Our machine learning model is used with a data set of noise and signal triggers, specific signal triggers, with a family of supernova. A family of supernovae waveforms that was not used during the training of the algorithm. So, this is critical and important because this is the way in which we can test our model to our known gravitational wave signals. So, that's basically the idea, and this is the quote over there. Okay, what about the results? So, for one of those analysis. Of those analyses. Here, for instance, we are training with data from the first strain data set. We are testing with the other data setment. And the testing is done, was done with gravitational weight from Powell 2019. That means that the analysis in the strain data set was done with all the Was done with all the other gravitational waves. That's a little bit the idea. So, in the Campusian matrix, we can see a very high accuracy in the recognition of noise. For this example, look 100% recognition of the noise triggers. And in the bottom, we have the distribution of noise triggers before and after the application of the noise. Of the model. So, this is good, highly good, almost 100%. Now, we have the case of signal reduction. Remember, we want to keep all the signal triggers. So, signal reduction is about eight percent for Livingston, six percent for Hanford, and zero point three percent for the two network of two detectors. To network of two detectors. Again, we want to keep all the signal triggers. We want to maintain the distribution of signal triggers. And this is a summary of our results here for the different analysis that we did. In specific, we have the percentage of noise and signal reduction. And here, it is important to mention the family of supernovae waveforms refers to Refers to the signal triggers in the testing set, not in the training. Just to give a summary, for instance, for two detectors, all of the noise triggers are recognized and then they can be discarded. Only 0.3% of the signals are lost. 96, 98, 5% of signal lost, 8% of signal loss. Of signal loss. That's a summary. But now we mentioned, or I mentioned at the beginning, that we wanted to reduce noise, we wanted to improve the false alarm rate. And here we have those results for this analysis. So in red, we have the false alarm rate versus the kitten row before the machine learning. In green, we have it after the machine learning. After the machine learning, so we can appreciate, we can observe a reduction of the false alarm rate. And for this case, 100% noise reduction. And also the sensitivity. We mentioned, I say that we wanted to do not impact the sensitivity. I have to say, there is a little bit of impact for one interferometer or very little impact for. Very little impact for two interferometers. And now, I also mentioned about these plots, false alarm rate versus false alarm probability. And these operation points, the ones in red, are for the strain data setment, for the second strain data set, before the application of the model and after the application. Model and after the application of the machine learning model. So there is a reduction in the false alarm rate, there is also a reduction in the false alarm probability when the on-source window is one second, which represents a neutrino-driven search of gravitational weights from supernovae. And for the case of an on-source window of one day, which represents optical integrated search, there is a There is a reduction in the false alarm rate, but not in the false alarm probability. Okay, so a summary here. Yes, so we can compute for the operating point of the search the false alarm rate before and after. Then we can compute the orders of magnitude in improvement of how much the far Much the far is reduced. That's basically the idea. For this case, for two interferometers, important to mention, we are discarding all the noise triggers. So the false alarm rate after the machine learning model, the only thing we can say is that is smaller than one over the lifetime of the search. And basically, the improvement. The improvement in the false alarm probability about one sigma for detectors for networks with one detector. Okay, then I have also a summary key, a summary here about the improvement factor. For instance, when we are testing our model with gravitational waves, Gravitational waves from one of the families. The reduction is about 60, 26, and 300 for Livingston, Hanford, or they both combined. Okay, so that's a summary. I hope I in time, not fast, no slow. So in conclusion, we have presented a follow-up method based on machine learning. methods based on machine learning to improve to enhance CWP based sources of gravitational weights from supernovae but this can be also applied for any other type of gravitational weight it's only a matter of the training of the system and indeed it can also be applied for different type of gravitational wave signals we have demonstrated the use with one or with multiple interferometers Interferometers. In general, there is very high percentage of correctly identified noise triggers, low percentage of signal misclassification. And also, it's important to remark here that the machine learning model has to be learned with a data set from a specific analysis, and then we can use it to identify. Then we can use it to identify and to discard noises in another different analysis. That's the idea. So we showed a reduction in the false alarm rate and in the and therefore an improvement in statistical significance with a small impact with one interferometer and almost no impact in the sensitivity with two interferometers. With two interferometers. So I think that's all. Yes. Thank you very much for listening and open to questions. So do we have any questions here first? Do we understand the difference between L1 and H1? Just due to the sensitivity? I have to say that for this specific result that I'm showing here, Results that I'm showing here, which were the first thing that I did. The final time of available time within the initial and the end time that I selected was lower for the Livingstone interferometer. I don't know if there is an impact there. I remember also that I computed the sensitivity curves. Sensitivity curves for those periods. Remember that there were differences, but nothing conclusive there. So, my hypothesis at the beginning was that the noise level was higher in that period of time for the Hamford interferometers. But for our new analysis in different time windows of data, The final results are with a time window of three days. And for some reason, in those cases, the total lifetime is similar across the vectors. Do we have any questions on Zoom? Don't see anyone's hands raised. Oh, yeah, yeah, go. I was wondering, you use the feature vector to discriminate between signal and noise, right? So approximately, what is the size of the feature? What is the size of the vector for the size? 11 dimensions for one interferometer in the epoch. And if I remember correctly, 18 dimensions for two interferometers. Yeah, it was not 22 because some of the features were the same. You can reduce the feature. But what I'm sure is that it was a little bit. Uh, I'm sure is that it was a little bit bigger for two interference. Okay, yeah, and the second question just to see if I understand clearly is you first use cross-validation in the first segment and then you test it with the second independent. Yeah, and that's something that is very, very used in any, any anything you take data. You first cross-validate with the machine learning model if the machine learning model shows discrimination. Learning model shows discrimination so that you use all the data to train the model and to use it online or in your application, whatever you want. Yeah, yeah. Thank you. This in the worst case of the final reduction, this 8%, 6%, what could imply in the final? I don't know if you have talked with some experts in waveforms. Yeah, basically, what I'm saying is... Yeah, basically what I'm saying is that there are signal triggers with loud signal to noise ratio, either because of the distance of the injection or because at those specific times the noise is bigger. So the signal to noise ratio is very low. And then it is Low, and then it is more complicated for the model to recognize between a real noise and that signal because that specific signal or those small set of signals they are like noise. This could also be used for not only for strength but for different kind of data as well or elaborate a little bit more, sorry? A little bit more correct, sorry. Besides the strain, you can use like this approach to different data from the auxiliary channels. Oh, yes, of course, yeah. And there are a lot of work in that front. However, here, oh, yes, maybe it's possible to use to incorporate in those specific time of the trigger of the event to incorporate auscidari channels determined to help. Oscillation is determined to help the classification, the recognition. Yeah, it can also be used here. I was thinking that it was not possible, but yes, it exists. Yeah, yeah. More questions? Yeah, I was just wondering if people have started to think about what the glitch situation will be like with Einstein Telescope or Cosmic Explorer. For Cosmic Explorer? Like, will they be glitchier than LIGO? So, you mean if this idea or this kind of ideas can be? Or I'm kind of wondering, like, how much people expect glitches will impact data at future interferometers, or if you just have to build the interferometer and see. To be honest, I yeah, yeah, I have been thinking about that. About that. Yeah, yeah, about the only thing I can say is that this kind of machinery can be used to help to improve many, many of the problems. And obviously, there will be another different kind of problem situation. Okay, cool. I think that is all for questions. So thanks again. Uh, and next up, we have uh,