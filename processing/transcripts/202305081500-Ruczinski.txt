For coming back. So it's my pleasure to introduce here Ingo from John Hopkins University. He will. Oh, I think I will. Sorry, Ingo. You have a. That's okay. Yeah. So, yeah, so he will be talking about quantification and detection of antibody. Detection of antibody reactivity and looking forward to hear more from you and this FIP-SIG data. FIP-SIG, yes, yes. It's a new technology. So that makes it interesting. So one of my collaborators is actually Ben Larman. He's here at Hopkins. So he's a co-inventor of that technology, which has been around for 10 years, but it's really sort of starting to gain traction. And we were lucky enough that we were. And we were lucky enough that we were among the first ones in the biostack crowd to get the hands on these kinds of data. In particular, I highlighted here Athena Chen, she was a PhD student in the shop who did most of the lifting of the methods and the software. So what is TIP-seq? Let's see. I think after this pandemic, I don't need much effort to convince people that the human immune response is kind of important. So if you're interested in characterizing In characterizing peptides, for example, antibody reactivity to antigens. So, for example, you might be interested in all sorts of allergens, you might be interested in the viral proteins. One way of doing this, this FIPSIC technology, is to use bacteriophages, which are viruses that infect bacteria. So, the idea is that you create first a library of DNA sequences that sort of span and tile the proteins that you're interested in. Proteins that you're interested in inquiring as your antigens, and then you clone them into the bacteriophages. And when they invade the bacteria, they replicate and they also express the antigen on their surfaces. So you can see that in the schematic here. And the idea is then once you have sort of a phage display, you can mix it with serum from a participant. And in that serum, you have all sorts of antibodies that you'd like to find and quantify. That you'd like to find and quantify. And so the idea is the antibodies find their respective antigen and bind to it. And then you capture these guys, these bound phage, using magnetic beads. And then you use a magnet to pull them off and all the unbound phages get washed away. So basically, what you have by the end of the day are a bunch of phages that are bound to the respective antibodies. And then you have sort of the, you can quantify. sort of the you can quantify actually the the amount of of antibody that way that you find in the serum then it goes its usual way that it gets pcr amplified and run through a sequencer and then by the end of the day you just use your your favorite alignment method like bowtie or some exact match approach to map your reads to the uh to the reference genome and you get what's called a read count matrix just like you have in RNA-seq where you have instead of transcripts you have Instead of transcripts, you have peptide read counts and you have a bunch of samples. So, what we were interested in is basically how do these data look like? What's the data generating mechanism? There's a huge repertoire of RNA-seq method. Can they be used for the analysis? Can they be improved? So that all was part of the PhD thesis of Athena. And the main paper that I'll talk about today appeared in BMC Genomics last year. So, that basically summarizes our approach and sort of gives an insight into the data and how we model the data and compare them to the existing RNA-seq toolkit. So, in particular, we compared it to EdgeR, which is one of the most popular RNA-seq approaches. What I'll discuss a little bit is how these data are similar and how these data differ from RNA-seq data. So, that's the next few slides. So that's the next few slides. If you're interested in the technology, I should also mention you can just go to YouTube and put in FIPSIG into the search query, and you'll get a video that Ben has made that sort of in a few minutes explains the technology in more detail. So the outcome is a read count matrix that quantifies how much antibody respectively we've seen. And what we want to do is sort of create some inference which peptides are reactive. So what's important So, what's important and what's a big difference compared to the RNA-seq experiment is that for FIP-Seq, you have to include some negative controls. They're called mock IPs or beads-only samples, but you don't load any material. And the reason for that is there's a strong technical artifact due to the beads. And you see sort of a skew in the library. And what you see on the left hand is just when I compare two empty samples, two mock IPs. So you have a 96-well plate. Typically, six to eight of these wells are. Typically, six to eight of these wells are reserved for these Mach IP for the beads only samples. And we look at the read count matrix and we compare the reads per million from two samples that are sitting on different plates. You see a very strong correlation. So that's sort of a strong technical artifact, a very strong peptide effect. Certain peptides always capture more weeds than others. So there's no material here in that channel, and you can still see such a strong correlation. In the middle panel, you see, In the middle panel, you see if I take the average of the six or eight Mach IP samples and I compare the averages from two different plates, I get an even tighter correlation. So on the right-hand side, I sort of quantified sort of what kind of correlations, Spearman correlations, would you get when you compare pairwise samples and when you compare averages? So the more depth you have in your library size, if you have a few million reads, you can get very, very concordant data when you. Very concordant data when you compare these peptide counts. So, this is something we need to take into account in our data analysis, but it's something we want to take into account. What we're going to do is we're going to look at peptides in a serum sample and compare the read counts that we've seen for that particular peptide in our serum sample to the ones to that peptide in the mock IP samples we have. And that's where the inference is based on. So, in RNA-seq, you typically look at differential expression. You have a bunch of samples in the cases. You have a bunch of samples in the cases, a bunch of samples in the controls, and you try to infer a differential expression. This is something completely different here. You have a single sample, and you use these beads-only samples as sort of the negative control to sort of see if you have an enrichment of peptide counts. So, in RNA-seq and FIP-seq, you have to deal with what we call competing resources. So, that simply means if you have a highly abundant gene in RNA-seq, it pulls. In RNA-seq, it pulls a lot of reads. That means you expect to see fewer reads at other transcripts than you would if there was no strong expression. So that's like the TMM in Edge R, for example. And the same is true for our beads only. So if you have a super strongly reactive peptide, it pulls a lot of reads and that diminishes the read counts that you see for some other peptides. So this is a typical plot that you would see. If I use the beads only and I plot the The beats only, and I plot the expected read counts on the x-axis. I take my serum sample and I look at the observed read counts on the y-axis. You can see there are some super reactive peptides up here. I truncated the read counts at 100. It can go into the thousands. And you can see these are all clearly a bunch of reactive peptide. And then each of them pulls a bunch of reads. And the dashed line here, that's the 45-degree line. So you can see here this computer. Here, this competing resources effect that the actual observed read counts are sort of greatly diminished if you look at that red line here. But it doesn't mean these peptides are negatively reactive. They can't be, right? So they're just background. But you have to deal with these competing resources in the same similar way as you have to do in RNA-seq. Also, when you do RNA-seq data, you typically filter your genes that you say, like, if it's super lowly abundant. If it's super lowly abundant, we filtered out using CPM or similar, in part because we think it might not be biologically relevant. Here, peptides that have low read counts are of biological interest. So it could well be that if you see 10 reads for a certain peptide that you never see in the mock IP samples, that this is actually a real reactivity signal. So we have to take these small abundant guys into account too. Then, this is sort of a little bit about the data structure. On the left-hand side, I made a histogram where I take one read-only sample and I look at the distribution of read frequencies in the sample. So if I have like a few million reads total and I have 100,000 peptides here in that viral library, I can make a histogram of what kind of frequencies do I see. And there are a couple fits you see here. They're both beta distributions fit in different ways. So, overall, the distribution. So, overall, the distribution that's just the take-home message, the read frequencies do follow reasonably well a beta distribution. Now, if I pick a particular peptide among those guys and I compare that peptide across the eight beads only samples, I get the figure on the right-hand side where I use some empirical base methods sort of to estimate what is sort of the peptide read probability across these eight Mach IP samples. And these are the thin lines. The thin lines. So you can see there is a little bit of a random effect. If you think about the probability here as the probability to pull any particular read from our library, what you see here is that you have sort of a random effect between the beads only samples. And you can use that data again sort of to estimate what that random effect might be. So that big, that thick black line here represents sort of the dispersion that we expect to see of that actual probability. Of that actual probability across these eight bead samples. So, here the green ticks are the medians of these individual distributions, and that black line sort of quantifies the distribution across those guys. So, these are all beta distributions, and all of that serves to moderate the approach that we wanted to use because the way we sort of think about the data generating mechanism is sort of in a hierarchical kind of framework that we say, okay, what That we say, okay, what are these probabilities? What do these probabilities depend on? What do our observed read counts look like? So, our method, drum roll, is called beer, Bayesian enrichment estimation in R. We have a Bioconductor package. So, this is how that logo looks like. And as you might expect, now we are about to commit some Bayesian acts. So, this is the overarching view of that DM. Overarching view of that beer model without going into too much detail. So, what we observe are the read counts, these are the y's, and the library size, the total number of reads, these are the ends. And we're indexing here the peptide with the letter I and the sample with the index J. So, what we're doing is we always compare a serum sample to a bunch of these mock IPs. And so, what we imagine that the read counts we observe, the yij's, follow a certain binomial distribution. So, in truth, they're multiple. So, in truth, they're multinormal, but if you have 100,000 peptides, a multinormal n equals 100,000 is a bit hard to work with. And also, the binomial for these kind of n's is perfectly okay for an approximation. So, we imagine that our read counts depend on some parameter theta, and that theta is sort of the probability of this particular peptide pulling a read from our total library size. So, this is our nj is the total number of reads. j is the total number of reads in our sample j and theta i j is for that particular peptide i the probability of pulling such a read. Now, what does that theta depend on? A whole bunch of things. So we have, for example, the attenuation constant, C, that is the competing resources parameter that we need to model. It depends on the actual fold change, the phi. If phi, if the peptide is not reactive, phi is equal one. If the peptide is reactive, it's bigger than. If the peptide is reactive, it's bigger than one. And the A's and the B's, they sort of can think about these are sort of my hyperparameters for that thick black line that you saw in the previous plot. And these are our sort of hyperparameters that we need to model the beta distribution. And there are various ways you can think about going about estimating these parameters using method of moments or maximum likelihood, or even better, borrow strength across all peptides to get a more stable estimate of what these. To get a more stable estimate of what these parameters might be, so what we're doing is basically we're saying this theta, this probability, follows a beta distribution, and that beta distribution has some parameters A and B that we also in part estimate from the data and or put prior distributions over. So the C and the phi have priors, and the A's and the B's get estimated from the data. And oops, here we go. The C. Oops, here we go. The C follows a beta distribution where A C and B C can be chosen by the user a priori. We have some default values for those based on real observed data. And I'll show you in a second how that looks like, but that is all changeable. And then the Z is actually our main parameter of interest. The Z is the indicator whether or not we think the peptide is enriched or not. So if Z is equal one, it's enriched. If Z is equal zero, it's not enriched. So if it's not enriched, we imagine. So, if it's not enriched, we imagine that the fold change is equal to one. If it is enriched, then we model the enrichment, the fold change, as a shifted gamma distribution. And again, this is sort of based on observed data. And so then the question is like, how do we know about the Z in a Bayesian framework? Well, we imagine here the Z follows a Bernoulli distribution where this pi j is simply the proportion of enriched peptides in a sample, also that all of this. In a sample, also that all of this changeable, and the pi j follow a beta distribution with a pi and bπ. So, here's how these priors look in practice. So, here's that model again. The a and the b we use, we estimate from the data. Okay. The c, our enrichment, our attenuation constant, uh, by default follow this beta distribution with these hyperparameters. And so, typically, we see sort of an attenuation in the neighborhood of 80%. In the neighborhood of 80%, but could be between 70-90%, depending how much reactivity you have. So, this is a fairly wide prior distribution. But again, if you have different beliefs, you can easily change those in the code as well. Our shifted gamma distribution, these are the fold changes we model. I haven't mentioned yet what we do a priori when you look at the peptides. There are some peptides there so clearly in which we have a first step where we use maximum likelihood. Step where we use maximum likelihood to remove the most blatant enriched reactive peptides, and that just makes the entire operation a bit more stable. So, if we get a full change estimate using maximum likelihood, that's above 15, we'll take that peptide out, recalibrate the library, and then run this machinery. That just has to do with numeric stability and mixing and run types. And here's our typical beta prior for the pie chain, the proportion of reactive peptide, but again, Portion of reactive peptide. But again, if you have different beliefs, that can all be easily changed. So far, so good. Then this is how we run beer. It's implemented in R. It uses JAX, just another GIPS sampler. And we run these Markov chains. And what we're interested in is for most the proportion of times that our chain is in state Z, right? Our posterior probabilities are based on how often are we in. Are based on how often are we in the state reactive versus the state not reactive? So that Z is the underlying sort of key ingredient to generate these posterior distributions. We run each sample individually rather than by plate. We explored this a little bit. So you might think maybe you can borrow strength across samples. If you have a bunch of samples that all have the same reactive peptide, you might be able to learn a bit more. There was very little, if any, gain if that was the case in our simulation. That was the case in our simulations. If we simulated the same peptides being reactive, but there was actually real detriment if you had samples that didn't have the same reactive peptides. So for simplicity and for runtime, for scalability, we are running one sample at a time against all the mock IP on that particular plate. And as I mentioned before, the clearly enriched peptides are included force. This is again for scalability and sort of to make the entire operation more. To make the entire operation more robust. So, how do you know that your model is doing what it's supposed to do? One way of doing this is to look at posterior predictive distributions. So, we took 3,000 HIV peptides in an HIV sample, and we ran our sampler and we created these posterior predictive distributions. We looked randomly at every 50th peptide, sorted by total number of read counts. And what you can see here are our sort of. And what you can see here are our sort of medians and our 95% credible intervals for the read counts we should be observing. And the blue dot is the actual observed read count. So we are fairly confident that the model is actually doing what we think it's supposed to do. So the model was specifically designed in mind how we think these read count matrices arise. When you want to compare it to another method, then of course the oldest Then, of course, the oldest trick in the book is to simulate data from your model. And then, of course, your model does better. But at the same time, you're sort of in a bind if you really think this is the data generating mechanism. How do you prove it that you're not just like picking up what you simulated from? So if you go through that paper, we went through great length to sort of try to convince the readership that indeed our model does something meaningful. And so we compared it to EdgeR, which is Compared it to edge R. We just picked that. There are many other methods you can think of. So you can run edge R by sort of tracking it a little bit by running this one sample versus the beats only. You can run edge R with one versus eight. If you implement it that way, the one thing you have to do is you have to change the two-sided p-values into one-sided p-values because there's only one side. If the peptide is reactive, the fold change has to go up. Change has to go up. The BIR framework is a Bayesian framework. So it's based on that assumed data generating mechanism. Of course, the runtime is a bit longer than for these HR methods. They're almost instantaneous. So here's the take-home message. And again, I refer you to the paper to sort of convince yourself that I'm not making this up. So first of all, the finding we were totally surprised is that HR does incredibly well for FIPSIC data. Incredibly well for FIPSIC data. It was not designed for these kinds of data, but it runs remarkably fast and it runs remarkably efficient and picks up peptides. Now, it's in part also ow to, if you look at these reactive peptides, some peptides are super reactive, they're impossible to miss. Some peptides are clearly not enriched. They're also not, they're kind of hard to call reactive. And there's really sort of some in between with the smaller photo changes. This is where the difference is. Where the difference is. So, what we found is that for these weakly reactive peptides that are sort of somewhere sitting in the gray zone, this is where the Bayesian model really shines, where it really does better than HR. But again, this is not a big surprise because we really designed it for that, and HR was not designed for that purpose. What we also found is for our hyper, for the betas, the A and B, the boring strength across peptides is super important. So, we looked at methods of momentum. So we looked at methods of moments, maximum likelihood methods, and other methods to estimate these beta prior parameters. And what we ended up doing is we're using HR to do this empirical base step to estimate these parameters to borrow strength across peptides. And that turned out to be much better than method of moments or maximum likelihood. And we also found that you do need a certain number of beads only samples, so between four and eight. Only samples, so between four and eight is a good number because there after that, there's diminishing return. If you have fewer than this, you don't have enough information to really pull it off consistently to find the reactive peptides. So, here is an example where I think we can show without simulated data that beer does something better than HR. So, what we have here is these are HIV samples. These are HIV samples. So they're elite controllers. These are people who get infected with HIV, but they never develop the disease. So without any viral suppression, they do this, they call it elite controllers because they naturally suppress the viral load. So they don't get any R treatment or anything like this, but they suppress naturally the viral load. And so these are blend Altman plots of B and HR, where we have each, the numbers here are the proportion of pepper. Here are the proportion of peptides that are called reactive. So each dot represents a protein. The number of peptides in each protein is given by the scale down there. And we knew these people were infected with an HIV subtype B. So we were expecting more red dots than we were expecting black dots to be called reactive. And you can see that consistently so we have like more red dots going in the right direction for BR than for HR. You might well say there is a black dot. You might well say there is a black dot that is not a subtype B that shouldn't be up there, that is absolutely correct. But we looked at these guys, and this is all owed to cross-reactivity when you have so much sequence similarity that for other subtypes too, you show reactivity. That's called cross-reactivity. So that's expected. So this is an example where we think we can show that this beer pulls out a bit more information than HR. And again, this is sort of HR does really fantastic. HR does really fantastic by default, but you can pull out a bit more information if you spend the computational time and run the Spayesian chain. So this is sort of part of our simulation that I hope highlights exactly where the gains are. We did a bunch of simulation where we simulated different fold changes. And you can see here as a function of the true fold change, we simulated the probability of being classified as enriched or reactive. Enriched or reactive. And you can really see sort of in that neighborhood where the reactivity is weak. So between two and eight fold changes, this is where you see a true difference. So if the fold change is very large, both methods easily pick them out. If it's very small, both methods have a tough time picking it up. But it's in between in this twilight zone where it is worthwhile to spend the time and run the Bayesian machinery, we think. So, for example, here, if you look at a full change of Example, here, if you look at a fold change of four, the probability of being called reactive in that simulation was 53% for beer and 21% for HR. Or if you want to look at the 50 percentile here, what is the 50-50 chance of being called reactive? That was for a full change of 3.7 in beer and 5.5 for HR. So again, if you spend a lot of time and money in the lab to generate these experiments, it's a worthwhile endeavor to just throw on the laptop or your local cluster. The laptop or your local cluster and run the MCMC as well. So we have, of course, a bioconductor, or maybe there's a type bioconductor package that first creates a container to hold this data, to store the data. It's an extension of the summarized experiment class. Then you can run your beer analysis. The main function is called bro, as you would expect, and that does all this posterior probability of enrichment, estimates the log2. Enrichment estimates the log2 fold changes, et cetera. And we have some additional features in part owed to the request by the referees. We have this sort of slightly tweaked pipeline that you can run on this FIP data object, just sort of so you don't have to run it outside of that package. And we also have some helper plots like where you compare the expected to the observed, and you can color this by posterior probability, or you can make plots of. Or you can make plots of base factors, these kinds of things are all available. So, just sort of to summarize, to finish up, the couple collaborations I'm currently involved in. One is with Craig Hirsch up at MGH at Harvard, up in Boston. So, he's a member of the COP gene cohort, COPD gene cohort. They're interested in COPD, in lung function, and there is a huge literature out there speculating that. Huge literature out there speculating that viral infection, in particular respiratory viral infections, play a key role in lung function decline. So we just submitted a grant and we generated some pilot data. So this is just A data from one participant of COPD gene. COPD gene brings participants back every five years and gets a new blood draw. So these are from phase one and phase two, five years apart. And it's just sort of to Apart, and it's just sort of to highlight how stable these reactivities are. So, what we have here in this participant up here, these are all peptides that have four changes across both phases. And again, they're five years apart. And I just highlighted here in green, these are all peptides from an Epstein-Barr virus. And the blue ones are peptides from a rhinovirus. And you can really appreciate how stable these reactivities are over a long period of time, five years. And here on the lower end, And here on the lower end, I have a couple human coronaviruses, not SARS-CoV-2, but a different coronavirus. It's a beta coronavirus. And you can see a whole bunch of peptides. They were sort of in the neighborhood of a full chain of one, so not reactive. But here, we have a whole bunch of peptides that all of a sudden have much higher reactivity in phase two. And these peptides are sort of tiling, so they sit next to each other. So basically, this is the epitope that's really the reactive part of that probe. Really, the reactive part of that protein. So, this technology is very good at measuring reactivity over long periods of time, and it's also easily able to detect new infections. So, our grant sort of centers around lung function decline in the presence of existing or new viral infection. Last but not least, an example. I just refer you to the paper. The idea was if you want to do cross-sectional incidence estimation. To cross-sectional incidence estimation for HIV. The typical way of doing this is to measure a certain protein, the LAC, and measure viral load. But with viral load becoming less and less useful to measure the infection status, as a proof of principle, we built a four top-scoring pair classifier that actually does better in classifying people and discriminating people into recent versus non-recent infections than the current status quota. So I'll without going Status quota. So, I'll, without going into much detail, I'll just give you the reference. And so, this is just the proof of principle how informative these peptides are. You have these peptides pair in HIV. Some go up over time, some go down over time. So, what you're doing is you compare the relative abundance of certain peptide pairs, and that informs you about when did these infections occur. So, with that, I'll say muchas garcias, and I'm happy to take any questions. Thank you so much, Ingo. I don't know if you could hear the claps, but there was enthusiasm in the room. So, any question for Ingo here? Are you in the chat? And people in Zoom, you can add your questions in the chat as well. Yeah. Yeah, thanks, Engel. Great talk. I mean, is there something in tiling along the sequence of the virus that would help you sort of identify regions that were more likely to face an immune response? And sort of, how do you guys think about sort of variation in the viral sequence? And then not end up with a library with just gazillions of things in it? Very great question, Robert. You get an assist. So we're currently working with the postdoc on exactly this. So the initial libraries were just sort of tiles of 56 amino acids. They were overlapping by 28 amino acids. But as you say, there's plenty of information like what are the epitopes, what are the reactive parts. So the postdoc, Anna, is her name. She is currently using a couple sort of machine learning, mainly random. Sort of machine learning, mainly random forests to sort of better predict what are the reactive parts, what are the less reactive parts. So we end up, so we're currently writing this paper up, we currently end up with a much smaller library size, much more compressed, much cheaper that has a much higher proportion of reactive peptides when you sort of try it out in a general library. Once it's done, Robert, I'm more than happy to send you a draft. Thank you. And I think you recognize Robert's voice because I didn't introduce him. I will say, I will do that next time. Thanks. Any other question? No. I don't think there is another question, Ingo. So thank you so much for joining. And keep in touch. Bye. Thanks. 