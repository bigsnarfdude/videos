We're very happy to welcome Tim Trojan. I'm glad you could be here. And please, I'll just let you know. Thanks very much, Greg. I'm delighted to see you all. So as Greg said, my name is Tim Trudge and I'm from University of New South Wales, Canberra, at the Australian Defence Force Academy. Please do ask lots of questions during the talk, not too dissimilar to Greg's opening remark. His opening remark: A former student of mine, he's just graduated, but an Indian student of mine said to me with an undue amount of confidence, I thought, from a student, he said, Tim, with those ridiculous sideburns of yours, you look like a cavalry colonel in a Bollywood film. And I said, that's both true and also very presumptuous of you. But he graduated. I have no further power over him now. So he can slander me as much as he likes. So do please ask questions throughout the talk. Please ask questions throughout the talk, particularly those on the junior side. I think it's a really good idea to have all these people here at this conference where we can have people talk in the first week, but also in the second week, people can actually work on some problems. Often, as I was saying to some of the organizers, often you go to a conference and someone who's very well credentialed talks more or less at you for a couple of hours and then you say, uh-huh, uh-huh, uh-huh, and then you fly back home and that's the end of it. So I think it's a really great initiative that the organizer. I think it's a really great initiative that the organizers have done. And I won't be here in the second week. Unfortunately, I have to fly to England, but I'd like to take the prerogative of being one of the first non-organizer speakers to thank the organizers. So could you join me in giving them a big round of applause? Certainly, organizing conferences takes a lot of work, and I think many of us who have gone through the last few years of not being able to travel so far have really appreciated. Travel so far, have really appreciated it. Another point I should point out there by way of an advertisement: if you get tired during this talk, if you could Google for me a local barber. My wife said to me, I must get a haircut before I left. And in my video diary to my wife and my sons, I said, day one, I took a picture and said, no success thus far. But if you can find one for me, that'd be great. Now, before I start the actual part of the talk, I want to draw a little scale here. So here's zero. So here's zero and here's infinity, not to scale, you might say. Now I want to write up two problems. And while we're talking about the problems, I'll give you some background on them. We've got two problems I want to share with you. We'll call them problems A and problem B. So what I'm about to write up, I'll tell some of the details on momentarily. Let me give you these problems here. So A and A primed, B and B primed. So n odd, n perfect implies n bigger than 10 to the 1500. Now, I didn't write up the title of my talk. The title would have been something crazy like, oh no, not another explicit result, same. But I'll talk a little bit about why I'm trying to give the talk at the start of the conference in particular. Start of the conference, in particular, to motivate why one should be interested in explicit results, what explicit results are interesting, and what we can look out for in terms of pitfalls for publishing and for collaborating. So here's one result. My idea with this scale is to get some voting going on. I have no learning activities, but I can do electoral activities. I can do voting activities. And voting may not be fun, but where I'm from, Australia, it is compulsory. So we'll do some voting in a minute. So my question is, which of these two results, this one here? Which of these two results, this one here and I'll tell you what these things mean in a moment, but here are two results. I'm going to ask some key people in the audience which of A primed and B primed they think is more interesting. So this is a scale of interest here. So, I'll tell you about these problems in a minute, but I give you problem A, I give you problem B. And now the question is: how are we going to evaluate which of these improvements is so-called better, of more interest to the community? So, suppose this result, and I'll walk you through the words in a minute, were improved in this way. Same preamble, but say n to the 10. So, A is a result, A is a theorem. So A is a result, A is a theorem, and B is a result, B is a theorem. I'm saying perhaps we could improve A to A prime. This whole thing is true for some larger n now. And B prime, same thing. These are the two kth moments of the Riemann zeta function. Again, I'll define those in a minute, but whatever they are for now. They're known for k up to 2, same preamble, but for k less than or equal to 3. So A and B are theorems. So A and B are theorems. And suppose tomorrow someone proves A prime and someone proves B prime. I'm wanting to gauge the interest of some key people here about what they find more interesting or less interesting. So Nicol, if you could give me a hand with this voting procedure. I have some students with me, but two of them are graduated. They're beyond my power. It is only Nicol whom I can control. So, Nicol, if you could grab my hat that's over there. So I wanted six people for the six organizers. I'm going to ask the Organizers. I'm going to ask the four organizers who are here, and I'm also going to ask Olivier Ramaray can fill in for one of the organizers, and Liam, you can fill in for one of the organizers. Yeah, so the four organizers plus Olivier plus Liam, you have to write on a scrap of paper and give it to Nickel, anonymous secret ballot. Australia also pioneered the secret ballot. With a trudge and talk, you get all the side information you could possibly want, and then some. Want and then some. So, a secret ballot, you write down secretly now, you draw this line and you make mark two points, A primed and B primed, what you think is more interesting. If A is improved to A prime and B is improved to B prime. Once you've got that, Nicol will take your votes in there and he will then give them to me. So, while our panel of experts is doing that, let me tell you about these problems. Greg, question. Oh, it was such a simple exercise, Greg. I thought my instructions were quite clear. No, no. If you draw this and then put an A prime somewhere and a B prime somewhere on such a scale, if you go out here or down here, then you fail the exercise. Okay, so while they're doing that, and when you have organizers plus proxy organizers, raise your hand and nickel will collect it in my. Organizers, raise your hand, and nickel will collect it in my hat. So, while this is going on, let me tell you a bit about what these things are. So, A is a theorem. It's a theorem by Occam and Rao in 2012. Let me tell you a bit about A here. So, we're talking about perfect numbers. Here, perfect numbers are ones for which, okay, sigma of n counts the number of divisors of n equals twice n. So, for example, sigma of Twice n. So for example, sigma of six, the divisors of six are one, two, three, and six. And if I add them up, I get 12. Oh, I've gone below the line. I can't do that. Sigma of 5, for example, if I add the divisors, 1 plus 5 plus 10, this is 16. It's not the same as 2 times 5. Thank you, Nicol. Dot done? Yeah. So the idea is. So, the idea is that there are perfect numbers. Six is the first perfect number, 28 is the next one. And if you get tired of this talk, you can calculate the next one and the one after that. I should say, one of the first times I saw perfect numbers was in Shanks' book, something like Problems in Number Theory or something, a very underrated book, I think. And he talks about interesting conjectures and dull conjectures. And he said, Here's one. The first few perfect numbers are six, 28, something else, and something else. But the last digits go six. Else, but the last digits go six, eight, six, eight. And he says, okay, maybe they all go six, eight, six, eight. And he said, that would be a dull conjecture. And then he explains why it's actually not true. So, anyway, these go back to the ancient Greeks, these perfect numbers. We assume there's infinitely many of them. We assume they're all even. There's some quote by Sylvester: this guy had a massive beard, so you know he's trustworthy. So Sylvester has some quote: I read it on Wikipedia, so you know it's true. And he said something like, There are Something like: There are so many conditions that box in there being an odd perfect number that it's almost impossible to believe that one exists. Something like that. So all the perfect numbers we know are even. And we believe, everyone in the universe believes that they're all even, that there are no odd ones. So one way of measuring how close we are, very difficult in this game to give metrics, but one way to measure how close we are is to say if we can show that there were an odd perfect number. There were an odd perfect number, it has to be huge. Then that might give you some evidence to say, well, perhaps there aren't any. And there are other results along this line. If there were an odd perfect number, then it would have to have, I don't know, 5,000 distinct prime divisors, say. So going from A to A prime is to say if there were some odd perfect number lurking out there, it has to be huge. And A prime would be saying it has to be even huger. So let's go over to B. To be actually, Nicola, I might make you sweat a bit more for your supper, mate. Could you please put these up on the interest line as I'm talking? Right, so let's let me tell you about B. So, B, these toothcase moments of the zeta function, I'm sure that there'll be other people at this conference who talk about that. I'm sure. I guarantee there will be. Now, one way of trying to get at the Riemann zeta function, to get at the Riemann hypothesis, is to understand the growth of the zeta function. To understand the growth of the zeta function, how it's distributed. That's a very hard problem. A poor man's way of doing that is to ask how these moments are distributed. After all, if you know the moments of a function, you know something about its value distribution. So what we'd like to know, what I mean by these two kth moments, is I take the integral. The details don't really concern us. But zeta of a half plus it. I take the two kth power and I integrate. And maybe I divide by t, so I'm computing an average. By t, so I'm computing an average. Now we know these estimates asymptotically, bang on the money, for k1 and k2. That's in the bank. That's problem B. The question of B primed is what if someone were to come along and say what we know for the first and second moment, or if you like the second and fourth, if we could improve that up to k equals three. So on the surface, you have. On the surface, you have an improvement here of this size, an improvement here of this size. Now, part of why I want to give this talk, and while Nickel's writing it up, what I hope to draw out from the voting, if and when the results are finalized, is to say that it's not immediately clear. Certainly it's not clear to me what the outcome will be. And I honestly, I won't look. It's not clear to me what the outcome will be, though I can predict. If I had to lay money on it, I think I'd lay money on it. To lay money on it, I think I'd lay money on which one is more important to people and which isn't. So, but why give the talk in general? Well, part of why I think the talk needs to be given is that, yes, it's true, we shouldn't just be writing mathematics in order to prove papers, prove theorems and write papers. There's more to mathematics than that. But part of any academic's career, particularly to those junior members of the audience, is that you will have to write papers, and those papers will have to. Papers and those papers will have to be accepted. And the more papers that get accepted in better journals, whatever that means, the better for you. And so thanks, Nicole. Something that I've oh, oh, I see. I see. It's very clever. It's very clever. It's very clever. He preserved anonymity. That's very good. Yeah. Okay. All right. Okay. Oh, yeah. Well, this is very interesting. Well, no one should reveal how. No one should reveal how they voted. This looks highly suspect, that one up there. But anyway, if we look at this, if we look at this, and for those at the back, or for those online who can't quite see, these are the red strokes for A prime. Here, here, here, here, one there, and one there. And the blue ones were in the middle and in the top half. So, on average, I mean, full disclosure, I never took a statistics course, but on average, it looks as though blue has outperformed red. It looks Outperformed red. It looks as though, from our six experts, I mean, two actual experts and two proxy experts, but for the purposes of this talk, you two fellows are experts. It looks like B primed has outperformed A prime. Now, if you had asked me, I would have said the same thing. I would have said B prime outperforms A prime. Now, one question is: how are you supposed to know that? Suppose you're fresh off the PhD chain or something, you're thinking about. Chain or something, you're thinking about papers to write. How are you supposed to know that one of these problems is quote unquote more interesting than the other one? I mean, from this meagre data, one of them is more interesting than the other one. One may object and say, I'm not interested in the opinion of some people, but when we apply for grants, when we write papers, we have to be taking the interests of other people on. Taking the interests of other people on board. When you submit a paper to a journal, it will be the interests of the editorial board, the likely referees, who will determine whether a paper is accepted or not. So my question to the junior members of the audience is, how are you supposed to know just by looking at these two problems, which one is supposed to be better, whatever that means than the other? So, part of what I'm talking about today is to lay down some principles, some Principles, some broad ideas that might be useful in order for people to figure out which are quote unquote the good explicit results to look at and which are the ones that you might have a harder time selling. And again, you may object. What a mercenary attitude you have, Trudgeon. You with your sidebirds and your army cufflinks, et cetera. What a terrible attitude you would have that you would sully pure mathematics in such a way. Well, that's true, but I have a wife and two kids to feed. But I have a wife and two kids to feed, so I need to write papers in order to sustain my livelihood. So I think it's reasonable for us to say: why is it that some results are better than others? And how can we spot this in advance? Now, it could well be that after reading pages and pages and pages of this result and pages and pages of that result, you can come to your own conclusion. But that's a lot of investment you may have to give in this problem and that problem. To give in this problem and that problem in order to figure out which one to attack. So I say here in my notes: the purpose of this talk is partly career advice, partly rant, and partly alliteration, but mainly rant. So why, even before we start, why even do explicit estimates? But I think if that's the first question, in a way, we have to ask a zeroth question. Why do we need to ask the question? Need to ask the question, why do we need to do explicit estimates? It's pure mathematics, after all. I've never met anyone in topology who pauses to ask the question, why should we be doing topology? I mean, you do topology, you write a paper, job done. So I think the reason that it's important for us to justify our existence in explicit number theory is that it's not the easiest field in which to get published. In which to get published. So, one question to ask before I lay down these general principles is: why do explicit stuff? Why even do it? Well, hopefully, my principles will answer that. But even before then, why even ask that question? I think we need to ask that question. I think we need to be aware that there are some biases that exist. That exist. I think there are some biases that exist, and the three main ones that I wrote down, these are biases that one might find from editorial staff, from colleagues, from grant bodies. And that's not to say, oh, what a world, what a terrible world we live in. But I think one has to be open and honest that there are some biases out there, and then we can figure out how best to combat them. Figure out how best to combat them. So, I think there are biases out there, and the three ones that I thought about were: one, a sort of anti-calculator bias. I'll explain what I mean by these in a minute. Two, an anti-increment bias, and three, an anti-details bias. So, again, this is not to say there's no way we could ever publish any. There's no way we could ever publish any paper in explicit number theory ever. Of course, we can, but I've noticed in my own work and by talking with colleagues, many of whom are in this room, that many people have experienced these sorts of biases against their work. So let me walk you through what I mean by these. So when we're doing explicit number theory, very often there has to be some computation involved, some calculation. Maybe you have to cover the first thousand cases. Maybe you have to cover the first thousand cases or something by computer. So at some point, you have to hit go on a computer, or you have to solve something not just analytically, but with some kind of calculation, some sort of computation. To get numerical improvements in something, after all, you are comparing this decimal with that decimal, 10 to the 1500 or 10 to the 15,000. And I think some of this anti-calculator bias goes back to the pride that some of us have. Pride that some of us have when we graduate from mathematics and we say, We're not like those engineers. We don't need a calculator. We can do stuff in our head or analytically, whatever. So I think many of our colleagues sometimes do look at explicit number theory as nothing more than hitting go on a computer. And again, I'm not saying they're wrong with the terrible people. I'm just saying that exists. Hence, if we lay down some principles, we may be able to figure out a way of circumventing that. A way of circumventing that. Another one is anti-increment. I've written beside it in my notes here: find details from my own rejections. So, over the years, I've been rejected from more grants than you've had hot dinners. It doesn't matter how many hot dinners you've had. I've been rejected more times than that. And the problem is, there were too many rejections to sift through. I didn't get time to zero in on any one example. But anti-increment, so many grant rejections, so many paper rejections. So many paper rejections I've received have the phrase, this is just an incremental improvement on blah. And I say, guilty is charged. This is a problem. This is why it was difficult. I improved it. If I knew how to prove the Riemann hypothesis, I would. So sue me. But many people will say, this is just an incremental improvement, therefore, why do we bother? I had a grant report that I got a couple of weeks ago, which said something. Weeks ago, which said something like, Well, this grant is just going to widen the zero-free region, it won't even prove the Riemann hypothesis, so why bother with it? And I say, Well, what are you supposed to do with that? And again, the point of giving this talk is not to say, well, therefore, the world's against us and there's nothing we could do. The point of giving the talk is to say the world is against us, but there are some things we can do. And the third one, anti-details. All right, for those of All right. For those of us who've written papers in the area, we know this. For those who see them for the first time, I think everyone also knows this. It's a lot of hard work making explicit improvements in number theory. A lot of work. It's not just here's a nice proof and you're done. There's a lot of hard grafting that goes on. There's lots of gaps which appear. You may be able to prove something for all integers up to a thousand or positive integers up to a thousand. Maybe for all things from 10 to the 20 onwards. There's a lot of hard Onwards, there's a lot of hard slog that needs to go on in that gap in the middle. It ain't just hitting execute on a computer, it's a lot of work that needs to go on, and the papers can run to 10, 20, 30, 40, 50, 60 pages. There's an excellent paper by Mike Bennett, Greg Martin and Co. in the Illinois Journal, very much like a modern version of Rosseru and Schoenfeld's work. And I'll discuss these papers later on. The paper runs into the hundreds of pages. You need that kind of Right, you need that kind of detail in order to make these sorts of advances, and some people will say, detail, length, ah, it's terrible, there's no beauty, there's nothing involved anymore, and they'll be against it from the start. So, given those biases, given that hopefully there's a way around it, what are some of the principles that I think one could lay down? And this gets back to that question with A and B and A primed and B prime. With A and B and A primed and B primed, how are we supposed to know that A primed is, at least according to some experts, or the majority of experts whom we surveyed, that B prime was more interesting than A prime. And hence we should develop our work on those problems that are more interesting, say. I should also say that not only are there biases, I'm happy to admit that I have my own biases. I think the more that people can admit that they are biased in their own way and can deal with that, the better. Biased in their own way and can deal with that, the better. So, for example, when I sit on promotion panels and hiring boards and whatever else, I absolutely have my own biases. I'm happy to tell you, I'm never interested in papers in fancy journals. So, someone has a journal in, someone has an article in Compositio or Duke or Inventiones or Annals, and they make a big deal out of it. I'm not interested. Maybe they'll be a good colleague or not, but whether they have a paper in there doesn't interest me. Now, I know that other people. Now, I know that other people have a bias the other way. That's fine. I think so long as we're bold enough to admit that I find this good and you find that good, and we can deal with it somehow, the better. I find it exceptionally difficult to admit that there's any such thing as objectively good mathematics. I just don't know what that statement even means. So, when someone says this is objectively better than that, I tell them to jump in a lake. As we saw with our experts, there was not uniform agreement. Even among the six, there was some bouncing. Even among the six, there was some bouncing back and forth. And I think so long as we can be bold and upright about it and say, sure, this you've asked me to referee the paper, therefore you've asked me to tackle it with my biases and prejudices. I think that this paper is good for these reasons, therefore publish it or not. Okay, so these five criteria that I thought I'd write down to give us a way of trying to figure out. Of trying to figure out what are quote-unquote good explicit results and what are ones that perhaps there's better value elsewhere. After all, there's only finite time we can spend on trying to prove these things. Okay, so the five principles that I like to lay down, and for each one, I have a sort of a champion example, an example of an explicit paper in number theory, which I'll Explicit paper and number theory, which I'll go through in a bit more detail. So I'll write down the principles, I'll write down the champion example, I'll talk about the principles, and then I'll talk about these examples in some more depth. So the first one, the first guiding principle as to what makes a good explicit result, I think is to resolve something, resolve something completely, solve a conjecture completely. Completely. Maybe a conjecture was only known for everything sufficiently large, and then you came along and said, Never mind, it's now known all the time. So resolve. And my go-to example, which I'll talk about a bit later, is the goldbark problems. The ternary goldbark problem and the old school goldbark problem. So I think one strategy is to resolve something completely. I should say these criteria. I should say these criteria, well, one, they're highly subjective, of course, but two, it's not saying that every paper must have all of these. My idea with drawing these out was to say probably most papers that are good, whatever that means, have one of these things. And I don't have any learning activities assigned to me, but what I think would be good for people to think about at some point is maybe some of these criteria are terrible. Probably they are. Maybe I've missed some, almost certainly. So maybe you could ditch some of these. So, maybe you could ditch some of these and replace them with better ones. So, resolve something completely. So, require here meaning something is required for some application. We need to have this estimate in prime number theory, say, because this person in computer science needs it for some application. So, some kind of requirement. And my go-to paper, which I'll talk about in a minute. My go-to paper, which I'll talk about in a minute, is one of and several of the papers by Rosser and Schoenfeld. I think a third criterion I've called here refocus. What do I mean by that? I mean, sometimes problems have lots of different aspects feeding into them, and it's not clear to and out. And it's not clear to an outsider where one should spend one's effort. If this problem requires 10 bits of theory coming in, I would not sell my second son or maybe my first son, whichever's the naughtiest at the time. I'll sell one of my sons if someone can tell me where I should place my effort. Don't bother placing your effort in that one or that one or that one because I've just finished them completely. Refocus your energy somewhere. Refocus your energies somewhere else. So, by refocus, I mean often saying, don't waste your time on this area, direct your energies somewhere else. And that's very helpful, particularly when these problems, some of which I'll outline later, have five or six or more components. We want someone to be able to tell us, work on this one, don't work on that one. So, my go-to example here is Watkins' paper. Hopkins' paper on class numbers. The fourth one I have written down is rally, meaning to rally the troops, a call to arms. I'm from the Defense Force Academy after all. So rally, a call to arms. Try to inspire people, you clever people in the room, work on this. Work on this now. This is something to work on. Get stuck in. This is something to work on. Get stuck into it right now. So, this often sets a challenge, sets a challenge to say, We've done X, we think X plus a little bit more is possible. You, young folk in the room, go forth and do it. So, my go-to example for a rallying cry is Lehman on SKUs number. And again, all these terms. And again, all these terms, I'll go through them one at a time, but this is just the go-to example that I have in mind for the rallying crime. And the third one, I can't even count. What a terrible mathematician. But the final one is respect. I couldn't really think of a better way of framing it, but respect, as in getting some street cred, getting a nice, clean-cut result, a clean-cut, direct, quote-unquote, beautiful result. Quote unquote beautiful result. I don't know what it means when people say something's beautiful in mathematics. When my sons smile at me, that's beautiful. When I see a sunset, that's beautiful. I don't know about beautiful mathematics, but whatever. What I mean by respect is that you may be able to take something that was long and horrible and in a nice, explicit way, wrap it up very neatly. And my go-to example here is Bertrand's postulate. So, once more, given the finite board space, I'll have to rub out some of them as I make progress on the others. But once more, these again were some kind of principles I came up with: of a good, whatever that means, explicit paper or a good project. Or a good project in explicit number theory ought to have one of these, I mean, the more the merrier, but one of these aspects. It should solve something completely. That's a resolve. It should give us something we require for something else. And often this one here will be something outside of number theory or outside of the main area of number theory. Someone else working on elliptic curves or whatever needs this result about character sums, same. Refocus, it should refocus. Refocus, it should refocus our energies. We have finite energy as a group of mathematicians, and we can work on the problems to get the most bang for our buck. Some kind of call to arms, a rallying cry, let's see if we together can meet this challenge. And finally, respect, giving a nice, clean-cut, beautiful, if you will, proof of something, extra clarity to a problem. Okay, so my goal now for the next. My goal now for the next little bit of the talk is to go through these one at a time. So I'll try to rub out the least amount of material possible. I'll go through these resolve, require, refocus, rally, and respect. I should say, before I start rubbing stuff out, does anyone wish to ask any questions so far? It's perfectly okay if the answer is no, but I'll have some water while you think about that. Some water while you think about that. Okay. So I'll talk about the resolve first. So the classic example here is the Goldbach problems. So we have the old Goldbach, the binary Goldbach problem. I can write every even. Problem, I can write every even number, I guess, bigger than two, as the sum of two primes. And the odd Goldbach problem, or the three-term Goldbach problem, or the weak Goldbach problem, or the ternary Goldbach problem, they all mean the same thing. I can write every sufficiently large number, so it's got to be bigger than five, as the sum of three primes. So even numbers, I should be able to write as a sum of two primes, odd numbers, the sum of three primes. Now, we know that the Goldbach conjecture is still an open problem, but the ternary one, the three prime one, has Ternary one, the three prime one, has been solved. So I'll talk mainly about that. So often we have these gaps in a proof. We have these gaps in a proof where someone proves something for sufficiently large numbers, someone proves something all the way up to blah, and there's a big gap to fill in the middle. So let me give you some examples of just the Goldbach one for now. So in 1923, 23 Hardy and Littlewood said on GRH the weak problem is true. I'll say weak goldbach. True for n large. What do I mean by weak goldbach? I can write n as the sum of three primes, where here n is odd. Okay, n's got to be bigger than five, so Hardwood would first cab off the ranks saying that under GRH, this is true provided n is sufficiently large. And then you had a whole stream of improvements. So 1937, Vinogradov removed the GRH condition. Weak gold bar. Weak gold bach true for n large, but unconditional, unconditional, that doesn't need GRH. And then there's a whole bunch of improvements about saying, well, this could be as small as three to the three to the whatever. All right, then we fast forward a wee bit more. Oh, I still mean the weak gold bug is what I mean. Yeah, weak gold bug. So we fast forward to 2002. So we fast forward to 2002. So Liu and Wang. I mean, there are many other results in here. I'm just giving the Cliff Notes version. All this stuff, unconditional again. So weak, gold bark, true, 4n bigger than 10 to the 13, 47. And my younger son, Monty, when he saw these. Younger son Monty, when he saw these notes on the desk before I flew out, he said, Dad, did anything special happen in the year 1347? I said, I don't know, but now that's much more exciting than writing my talk. So let's figure that out together. So apparently, this was the year that the Black Death started in Europe. So sorry to give a bit of a macabre note to the talk, but that's apparently that happened in 1347. We figured that out together, intergenerational mathematics. Yes. So we go back true for NC. We go back true for insufficiently large. We have to go back in time to die. I know it's terrible, isn't it? Yeah. Yes, possibly. Yeah. Now, here's a paper that is somewhat underrated in my mind. It often gets missed when, at first glance, one goes through these catalog of results. If I jump, I won't. Results. If I jump, I won't write it down, but if I jumped to the most modern one, 2012 and 2013, plus Harold Helfcott has a thousand and one proofs of this on the archive, for example. So weak goldbuck conjecture, true all the time. And those papers are, there's a lot of mathematics in them. So 2012, 2013, 2014, and then some. So it is true, right? So this gap, which is large, was whittled down all the way to no gap. So the weak goalback conjecture is true. So, the weak goal by conjecture is true. But I'm not as interested in that for the purposes of this talk. What I'm interested in is a paper that fits in between these two, which sometimes flies under the radar. And I think it might be a bit undervalued. So in 1997. Now, you might say, well, hang on, on GRH, it's true when n is large. Unconditionally, it's true when n is large. Unconditionally, it's true when n is this big. Well, if unconditionally, it's true when n is this big. Additionally, it's true when n is this big. Maybe under GRH, which is certainly true for n no bigger than this, maybe we can bring this down a little bit more. So, sure enough, there's four people writing this paper, Dewey, Efinga, Tarila, and Zinoviev in 1997. So actually, they say They say that under GRH, it's true for all n bigger than 5. So this paper's saying that under GRH, not only is it true for n some crazy large, you know, but actually it's true all the time. So a lot of mathematics has to go into such A lot of mathematics has to go into such a result. Now, yeah, the last one, the actual one that resolves it, is Helfgot's paper, which is very interesting to read. It's certainly the first few sections of that paper, there's also good mathematics in there for sure. Now, this paper here, I think it's a bit undervalued because some of the material in there could and should be used elsewhere. So even though I'm in the bracket of resolve, I wanted to give a rallying cry. I wanted to give a rallying cry to people to try to use techniques from that paper in order to prove something else. So the talk has now intersected with itself in some meta version of something. And now I'm giving a rallying cry to a resolve. And don't worry, there will be a test on this at the end. So, what I'd like to say is, could one use the methods in this paper? I think the answer is yes, one could. Could one use the methods in this paper to prove? This paper to prove an explicit version of one of Hua's results. So, Hua in 1938 says n congruent to 5 mod 24. Never mind why that for a moment. And n large means that I can write n as the sum of five squares. Squares of primes, p1 squared, p2 squared. P2 squared plus P5 squared. I mean, why does N have to be congruent to 5 mod 24? Otherwise, there's some silly obstruction with quadratic residues or something. Now, so that's true when n is sufficiently large. I'm going to wager good money, the Greg Martin-style wager of $3 Canadian. I'm going to wager $3 Canadian or Australian. Oh, Australian's worth less than Canadian, right? I'm going to wager $3 Canadian. That's how confident I am that. How confident I am that someone ought to be able to use these techniques from this paper to show that this result here holds under GRH if you have to, for all and bigger than something, 10 to the 20, say. I wage actually even more money that one could use this method to say this holds all the time. Okay, maybe there's some counterexample around 50 or something. So this stuff here uses methods that could be similar, but are similar to those employed here. But they are similar to those employed here and could be useful in proving something like this for an explicit result down here. But the Wheat Goldbach is such a good example of a lot of people working independently and in concert to try to not just bring down some bound, but resolve something completely. I mean, we could talk about the strong Goldbach conjecture as well. I don't have the time today, and one could talk about the advanced. And one could talk about the advances made there as well. But in terms of resolve, this gives you something which has been resolved completely, and in a sense, also has a little bit of this about it, in that there's challenges to try to overcome to bring something down. But certainly the strength in one of these resolutions is saying it's not just true for n sufficiently large, but for all n. And we've done it all in one go. Okay, so I want to move on to remote. Okay, so I want to move on to require. This is the Rosser and Schoenfeld one. So Russer and Schoenfeld in 1962, in 1975, and we also have And we also have Rosser solo in 1939-1941, Schoenfeld solo in 1976, and Hahn solo a long, long time ago. So these papers here all revolve around Around the same idea. If we know from the prime number theorem that the number of primes up to x is asymptotically blah, doesn't matter what blah is, can we get very, very good explicit estimates on that? Can we get good explicit estimates on error terms in the prime number theorem? Can we get good explicit estimates on all On all or almost all the normal arithmetic functions that we like to play with in analytic number theory. So, for example, in these papers here, even in Hahn's paper, why not? In these papers here, we have things like pi of x. Okay, that's just the number of primes up to x. So, how many primes are there up to 10? Well, two, three, five, and seven are all prime. Three, five, and seven are all prime. They're the only primes up to 10. So pi of 10 is 4. Now we know from the prime number theorem that pi of x is asymptotic to x over log x, meaning the quotient of the two sides tends to one as x tends to infinity. Meaning what? If the quotient of the two sides tends to one as x tends to infinity, then I can say the following. You give me any epsilon you like, like 0.001. Epsilon you like, like 0.001, say. If I go out far enough, like x beyond 10 to the 10 to the 10 to the 10 or something, then I can always ensure that this is trapped between one minus this, sorry, so sorry, one minus epsilon times this and one plus epsilon times this. I can always make sure it's trapped in a very small interval. What do these fellows say? These fellows say, for example, one of their proofs is pi of x is less than 1.26. Less than 1.26x over log x for all x bigger than 1. Now, certainly, when x is very, very, very large, this is pretty bad. When x is, say, bigger than 10 to the 10 to the 10 to the 10, I can go and do some stuff. I can follow the methods in their paper. I can go and do some stuff, and I can improve this. If this is 10 to the 10 to the 10 to the 10, this might be, I don't know, 1.00001. Be, I don't know, 1.00001 or something. But the point is, they give a result that works for all x bigger than one. And often you look at the, what do you call it, citations on Google Scholar or MathSciNet or whatever. Plenty. I don't know what the number is, but think of a big number. It's that. Now, many of those citations aren't even in number theory. This is the requiring bracket. This is some people, it doesn't matter what they're doing. This is some people, it doesn't matter what they're doing, group theory, computer science, graph theory, something, whatever it is they're doing, they don't want to muck around with Zeta zeros and explicit zero free regions and all that. They just want something off the shelf that they can use and fair play to them. So they only need something in here. They don't care that when X is bigger than 10 million, you can improve this, but this is fine for what they need. They need. So these papers are just full, packed to the gunnels, with examples of asymptotic formulae, which you know, sorry, explicit formulae, which you know are weak when things are very, very large. But the fact that you can prove it for all X here is very nice. It means then that you can apply it when you require it in other examples. So, another one that they give is give is p n this is the nth prime so i guess p1 is two uh p2 is three and so on i can never remember how the proof goes uh but there's something where if i tell you this you rearrange it and you get an asymptotic version for p n i i'm not going to try to do it because i know i'll screw it up but when you take this and you muck around with it this should also go like n log n So, what these guys prove in this series of papers, not only is that true, but in fact, so that's true on the prime number theorem. It's not what they prove, that's true, yeah, I don't know, pick someone, Landau, one of those old timey fellows. Someone would have proved that way back in the day. Right, what these guys prove is in fact Pn is bigger than n log n. n is bigger than n log n for again for all n bigger than one and again it's that kind of thing that people require out sometimes in number theory for sure but sometimes outside of number theory i mean if you give someone this someone's in graph theory or something and they want to construct some graph for example uh it's the sort of thing they would do yeah um and and they need to know something And they need to know something about this. They gave me the PhD. They can't take it away. Don't worry about it. Yeah. So they want something that tells them that the nth prime is bigger than something like n log n. They don't want to go through all the guff to figure out that they could make this constant two provided n was large or something. They want this to insert straight away. Now, there's a bit of rallying, refocusing, and resolve in these as well. And resolve in these as well. In that, this here, I don't have the time to write it up. I guess if at the end we have time for questions, I can do it. We can write up some of the advances that other people have made on this estimate here. And there are some curious side issues with what's going on with this estimate here. Something which when I first read it a few years ago, I said, this looks interesting. And I still don't know to this day why some of the lower order terms in this. Some of the lower order terms in this expansion give the problems that they give. But again, that's a kind of a teaser. If you can't think of any other questions to ask, or you haven't had time to look up where our local barber is, then you can ask a question about that, and I'll get to it later. I also should say, and again, if we had time, how are we doing for time? Well, maybe we do it later if there's time. These guys also have a rallying cry, they compete. Rallying cry. They compute some things up to 10 to the 8. Now, I should also say, when these fellows were doing computing, it was quite clever, the things that they were doing. They weren't just doing what I would do and write some terrible code and hit go and then swear at the computer. They may have done that as well. But they compute stuff up to 10 to the 8 and they show that something, some quantity, is always positive. And they say, could this continue indefinitely? In brackets, probably not. And turns out the answer is no. Not and turns out the answer is no, but they raise these questions. You wouldn't really want to call it a conjecture, but they raise these questions like this is what we've observed. Now, you other fellows go out there and investigate. Does this change sign infinitely often? Okay, so that was require. So now I want to look at refocus and And there were many papers I had in mind for the refocusing. And the reason I chose Watkins' paper on the class number, on computation of class numbers, is that it, in some sense, goes to the heart of saying there's no need for other folk to work on this specific problem. I haven't told you what the problem is yet. I haven't told you what the problem is yet, but the point of someone doing computational work on class numbers, still haven't told you what they are, what a terrible lecturer this guy is. He's not telling us anything. The whole point of people doing computational work on the class number was to gather some evidence about possible low-lying zeros of Dirichlet L functions. That was the whole point of computational work. And he was saying, I've not only done a little bit more, but I've done more than you could possibly imagine. There is no more. There is no more data you could possibly need for this specific problem. So we'll close that off. That's a cul-de-sac now. One could improve his work, sure, but there's no more need for any extra evidence to be done. Shift to something else, right? Move to a different part of the problem. So, okay, so Watkins is my go-to example. So the problem goes way back to the Gauss class number problem. Number problem. So I know that Kanika is giving a primer on algebraic number theory tomorrow, and I'm sure she will include a thorough bibliography of the class number problem. She only has an hour, but you know, thank you, by the way. Yeah, yeah. So the class number problem goes back to Gauss. So this paper by Watkins is called something like class numbers of imaginary quadratic fields. Quadratic fields. So the Gauss class number problem is the following. You need to list all the imaginary quadratic fields with class number one. Now, in these kind of problems, if I give you the discriminant of a number field, you can just go ahead and compute the class number. The difficulty is, well, how The difficulty is, well, how many discriminants do you need to compute? How many do you need to compute to figure out that you have a solid answer? So the Gauss problem is to list all imaginary quadratic fields with class number one. And I should say, if, like me, you say, oh, yeah, some of these words are a little bit hazy. The actual paper by Watkins itself is very good in that in the first chapter or so, it gives you all the history of everyone who's ever done anything in this problem. And it gives you three or four different ways of visualizing the problem. So, if like me, you once did algebraic number theory 15 years ago and you've forgotten everything, then you read the introduction. Everything, then you read the introduction to Watkins' paper and he walks you through it nice and simple. So, list them all. Now, it's not well, one, I don't know, and two, I don't really care. It's not immediately obvious to me who put the final nail in the coffin for this problem. So, lots of people worked on it. There were holes in proofs. The holes were plugged, they were re-holed, re-plugged, and so on. But Higner, Baker, and Stark, a lot of papers written in 1971, you could certainly say that by 1972, the thing was solved. That by 1972, the thing was solved. But in terms of who listed them all and whose proof was correct or whatever, I'm not interested in. But the problem of listing them all up to class number one, this was done certainly before 1972. Now, what you can do, you can ask the same problem, and you can say, why don't you list all the imaginary quadratic fields with class number up to n. Fields with class number up to n, where n is two or three or four or five, say. And you might say, hang on, why do I care? Well, we're talking about, I always put the thing in the wrong spot. So we're taking D to B positive and square free. We're taking Q, adjoined this fellow here. And there's a connection, which I'm sure someone at this conference will talk about. Well, right off the bat, everyone. Will talk about. Well, right off the bat, everyone who's not me knows more about this than I do. So there's that, but someone will talk about it, I'm sure. The class number h of d can be connected. I'm not going to write up all the factors here, but it can be connected to the value of the associated L function of one. And there's other stuff in here, which I've left out about fundamental units or whatever. I'm not interested in that. What I want to draw out from this is to say hd is the class number. Now, what I want to draw out of this is to say, What I want to draw out of this is to say there's a connection between the size of the class number and the size of the L function at one. That's all we need to know from this. The motivation for these problems is to say, if I have an abnormally large or small class number, then I get an abnormally large or small value of L1 chi. So we could rephrase all this. I mean, there's algebraic stuff which I don't understand, but we could rephrase all this is to say this is a way of is to say this is a way of our looking for abnormally large or small values of Dirisa L functions. So because of this connection and stuff that Habiba talked about today, low-lying zeros, later on people will talk about exceptional zeros, Siegel zeros, all the rest of it. Now, all that stuff is a way of trying to get to grip with how L behaves near the point one. So why I put this under the research. So, why I put this under the refocus bracket is to say there are many ways for us to examine abnormally large or small values of L1 chi, many ways. This is one such way. What does Watkins do? He takes this problem here and he lists them all up to with class number less than or equal to 100. So, when he writes it, he says, This is when two was done. Was done. This is people thought three could be done, but there were some problems. People thought maybe four could be done, but there were some problems. So now we'll just push all the way through, combining these ideas and say, therefore, we've done almost everything you could possibly believe. We've gathered what little evidence that gives us about this. And now, if you, folk, want to learn more about the size of this thing, you should look elsewhere. You shouldn't try to push these methods much further. Push these methods much further because it's highly unlikely you can improve that by much. So it's a refocusing of energy. If what you want, if like me, you don't understand anything algebraic, if like me, what you want to do is examine how this fellow grows, do something else. Do some zero-free region work or something else, but don't pin your hat on this problem. Now, that's a very unfair take on this paper. The paper is excellent, and the paper gives a huge, detailed history. A huge detailed history of the problem. And it may well be that more work can be done and should be done. So don't take my word for it, but I still think this is good under the refocus bracket. Not to say don't do this work anymore, but if what you want is more evidence, look somewhere else. Right, so I want to now move on to the rallying cry. And for rallying, I had Lehman. Lehman, sorry, mate. Uh, yeah, uh, so I had very good, yeah, and we didn't even work that out in advance. That's the chemistry we've got going on here, yeah. So, Lehman, uh, looking at SKUs number. Now, I haven't told you who Lehman is. I haven't told you who SKUs is, but we all know what a number is, so you're welcome. Do we? Yeah, yeah, yeah, yeah, yeah. It's day one of the conference, yeah, yeah. No one's defined it yet. All right, so what did I say? All right. So, what did I say by rally? I said, a call to arms, a way of saying, I would like it if people could work on this. Sometimes I say to my own students, when one's giving a talk or at a conference or something, it can be useful to come up with a wish list. You're saying something like, I've got as far as I can on this problem. I really wish that. I really wish that someone could do one of these four things. The more discrete and detached they are from each other, the better. I wish someone could improve on A, B, C, or D, Y, because that will help me with this problem, say. So you're asking people, can they chip in in some way? Now, Lehman's paper is one of my all-time favorites. It's only about 11 pages or so, but it's a very nice paper. It's 1960, 70, somewhere around there. But it's Lehman's paper on SKUs number. So let me tell you a bit about that. So let me tell you a bit about that. So, going way back to paper number zero, or paper number one, at least for me, this is Riemann's paper on the zeta function. You find this business of pi of x, number of primes up to x, being less than lie of x, where lie of x. Lie of x is the integral from 2 to x of dt over log t. This is the so-called logarithmic integral. You could write it from the integral from 0 to x, but I have to deal with all this business of what happens when log t is 1. And then someone's going to say, Cauchy principal value. I'm going to say, I don't know what that means. And then we're going to be here for an hour. So I think if we just put it like that, we'll be fine. Now, I'll tell you for free. You can put your wallet away. I'll tell you for free. You can put your wallet away. I'll tell you for free that pi of x is asymptotic to lie of x. That's true. And if I do that thing, integration by parts, then I get this is x over log x. Okay, I get some low order terms, say. So this is true. Put that in the bank. Now, many times it can be interesting. Sometimes it can be dull. If only there were some principles by which we could judge whether it was interesting or dull. Sometimes it can. Was interesting or dull. Sometimes it can be interesting to say: if A is asymptotic to B, how does A approach B? If A is asymptotic to B, then A over B tends to one. How does it tend to one? Is A always less than B? Is A always bigger than B? Does A minus B change significantly often? If so, how often? If so, on what set of density, etc. These are good questions to ask. Sometimes the answer is dull and simple. Sometimes it's very difficult. So, this and this get very close together. This star here, I don't want to say it's a conjecture. This star here is a statement. Possibly, pi of x is always less than lie of x. Okay, maybe we have to be careful about x being near 2 or something. So we'll say for all x bigger than 2. I mean, it's going to be a bit awkward x is less than 2, but it doesn't matter. Now, Now, when we go back to Riemann's paper, Riemann has this problem. The problem itself goes back to Gauss, but Riemann makes this claim. I shouldn't say claim. Riemann says, if this thing were to be true, then the Riemann hypothesis would follow. So if this were to approach this, which it does, from below, then the Riemann hypothesis would be true. So what I've written up there, the state. So, what I've written up there, the star, it is false, but the study on how and where it is false is very interesting. So, to save some space, I won't necessarily run up everything here, but as I'm erasing, I'll tell you that Littlewood, back in 1914 or so, proved that this star is false infinitely often. So, meaning. So, meaning infinitely often this is not true. Now, it is true for all x that we know about. So, star is true for x between 2 and 10 to the 19. And the 10 to the 19 is due to Jarn Bertha in some year. Some year, 2014, 2015. I've done no research, but a terrible lecturer. Yeah. But the hit of the 19 is true. Now, Littlewood shows that it's false infinitely often, meaning what? Well, it's false infinitely often, then there's got to be a smallest x for which this is false. The x can't be less than 10 to the 19 because I said so. So there's got to be some x bigger than 10 to the 19, which is the smallest x for which this is false. Which is the smallest x for which this is false. So, a little with student skews. SKUs were two different papers, and sometimes his result is misquoted from one to the other. In one of the papers in the 30s, it's assuming RH or something sort of a quasi-RH, quasi-Wihamen hypothesis. In the second paper, it's completely unconditional. So, in SKU's papers from the 1930s and 1930s, And 1950s, he's got two of them. He says that pi of x will overtake lie of x for some x less than four of them. One, two, three, four, seven point seven zero five. So this is E as in whatever 2.71, something or other. And I honestly forgot whether this is. I honestly forgot whether this is the quasi-RH one or the unconditional one. I also don't care. But this is some very, very large number. So it does overtake this infinitely often. And you certainly don't have to go beyond this one to find a counterexample. So from now on, when I say counterexample, I mean an x for which is just flipped around the other way, an x for which pi of x exceeds li of x. So now the question is. Now, the question is, I still think it's fair to ascribe this rallying cry to Lehman and not to Skews. Skews' paper is very interesting in that Littlewood's original proof, it wasn't even obvious that you could make it explicit. So there's a paper by Ingham, I think it's a paper by Ingham, where he makes a point. Not only does he not see that it could be, but he doesn't see that it ever could be made explicit. Pretty high praise, then, for someone to come along and do it. It so what does Lehman do? And again, just as we saw in the start of the talk, an improvement just with some numbers might not necessarily be useful. Improving odd perfect numbers from 10 to the 1500, 10 to the 15,000, okay, that might be interesting, it might not be interesting. But what does Lehman do? Well, he improves on this substantially, but that's not the main reason why it's exciting. But that's not the main reason why it's exciting. But he does improve on this. He says pi of x overtakes li of x for some x less than 10 to the 1166. And according to the yes, good, yeah. Exactly. And it was a good and it was a good thing that I prepared this with Archie. Prepared this with Archie. Apparently, William the Wicked of Sicily dies in 1166. So, hopefully, we've all learned at least something today. Yeah. William the Wicked. I don't know why it was called William the Wicked, but William the Wicked of Sicily dies. So, not of the Black Death, though, because that came later, as you saw. So, he improves this. Well, good for him. This number is a hell of a lot less than that number. Is a hell of a lot less than that number. But the reason I put this under the rallying bracket is the method of proof is fascinating. The method of proof says, well, it says two things. I'll just give a summary of the first one for now. It gives you a heuristic as to why this could be true, in case it's not true, but it gives you a heuristic as to why this could be true. Why this could be true, or if you like, it gives you a heuristic of why you would expect these numbers to be very large. It gives you, it says you wouldn't expect to find a counterexample around 10 to the five or something because you need a lot of conspiracies to happen amongst the zeros. And he writes down what that is. So, if we have time at the end, I can write down that form. It's only a couple of lines, but in interest of just getting on with stuff, I'll say that. Getting on with stuff, I'll say that he gives you a nice heuristic as to why the first counterexample shouldn't be expected to be low down. So that's good. There is no such heuristic, maybe implicitly, certainly none given in Littlewood, and I don't think any given in Schweser's papers either. So that's good. It gives you a reason why these numbers are crazy large. But the main reason I put it in the rallying bracket is. Is that he doesn't just say there's a counterexample less than that, but he gives you a method to say where you should be looking for counterexamples. So he has a method. Don't worry too much what this is. He's looking for a region in which pi of x. region in which pi of x is bigger than li of x. Therefore, this just goes like x over log x. If this is bigger than it should be, then I guess we're going to have a whole bunch of primes clustering together all of a sudden. At least informally, that makes sense to me. This is of size that, if we're going to make it exceed this, there must be a whole bunch of primes nearby. And so he looks at the following. So think here of. So, think here of t being a number that's not too large, like 100. You're going to be computing zeros of the Riemann zeta function up to height t. And you're going to be computing them with, say, 20 decimal places or something. So think of t is not too large. So here I'm writing zeros as rho, and rho is beta plus i gamma. Beta plus I gamma. I'm not interested in the trivial zeros of e to the i gamma u over rho. So again, think of t as being something like 20. There's only one zero between zero and 20. It's the first one, 14 point whatever. So if t was, say, 20, you just got one zero in the sum. Now the idea is. Idea is that if you can find some u for which this is large, you've got a very good chance of finding a counterexample. I haven't showed you where that comes from. Lehman's paper is very good. You can read this section in 10 minutes. So where does this come from? This comes from the explicit formula, writing psi of x take x is a summer over the zeros. That's where this is coming from. Now, what he wants to do. Now, what he wants to do is take t large, like say 100. I don't know how many zeros there are up to 100. Let's just say 15 or something. So, if we know all those zeros up to 100, what he wants to do is to say, can I now pick some U's such that I can make this sum large? Now, he doesn't guarantee anything. What he says, though, is if, again, take your favorite T like 100, if you can find If you can find some U, this U, that U, some other U, for which this sum is large, then you have excellent fighting chances of finding a counterexample. So when he plays this game, he identifies hunting grounds, places where you ought to hang around and try to look for a counterexample. So he says when he does this, and he does it for different values of t, sometimes a thousand. Different values of t, sometimes a thousand, sometimes five thousand. You, for you to do this, you have to know gamma to a lot of decimal places. You have to line these things up. Hope this sum is large. So what does he do? He identifies these kind of regions. U727.952, 853, 2682. What is he saying? He's saying, here are some values of you. Saying, here are some values of you, and there's some more decimals, but who cares? Here are some values of u that I found. And I think around these values of u, because this sum is large, we have excellent fighting chances of finding a counterexample. Now, it is the case here, e to the 2682 is this 10 to the 1166. So here. So, his counterexample that he finds with William the Wicked of Sicily dying, his counterexample that he finds is one from one of his hunting grounds. He says, these are good regions that you should look in. I've found that here we have excellent fighting chances of getting a counterexample. Then he goes and works hard and says, yes, there is a counterexample there. Example here. Then he says, I would love it if someone could find counterexamples here and here. We have excellent fighting chances, but when I go and do the extra hard work, I can't make it all work. I've got a main term, I've got error terms, I can't quite make the whole thing positive, saying. So, why I put it under the rallying bucket is to say, he's now saying to people, go forth and see if you can do. North and see if you can do this one and this one. That's the areas in which you should search. And this was done by Tarella. I don't have the year on me. So Lehman's statement is saying what? Pi exceeds lie for some x less than this. Terela uses these hunting grounds. Uses these hunting grounds and says, Actually, I'm going to look around here and work a little bit harder, compute some more zeros and so on. And sure enough, there's now a counterexample no bigger than e to the 853. And then this one here was done by Chow and Plymouth. It's around the 1990s. And now, every single advance we have on SKU's number since that paper on the 1990s is still in this hunting ground. So Liam, all Liam's, yeah, yeah. So close to your name, too, yeah. Lehman's method is saying, here are some regions, this is the easiest. Here are some regions. This is the easiest to go at, so I'll do that. Hopefully, someone in a few years' time can do that. That's in the 80s. Someone else in a few years' time can do that. Now, there's a very interesting paper by Bayes and Hudson, which gets more hunting grounds. I don't have time to go through it, but smaller values of you, where one should look. Now, the trouble with that is that when you try to do that, you work really hard, and even with 10 to the 10. Work really hard, and even with 10 to the 10 zeros or something, you still can't make it all work. But not only should Lehman's paper go in this bucket for the reasons I've just given, but if I can put a rallying cry on top of his rallying cry, look at the paper of Bayes and Hudson and see why is it the case that they can get more hunting grounds? And why is it so difficult that they can't show? That they can't show counterexamples lower down. So, this method of Lehman is quite fascinating. And it took me a little while when I was reading this paper as a PhD student to get the fact that all subsequent improvements from Lehman onwards were using Lehman's paper. It wasn't as if they used this and discarded it. Every single one of them is using his method with the same regions and getting better and better stuff. Regions and getting better and better stuff. And this paper was done in the 60s. It was quite impressive, the computation done in there. Okay, so we have time ever so slightly to move to respect. Find out what it means to me, etc. Okay, so under respect, I put Bertrand's postulate. I was trying to think of where I had first seen this. It was not in Hardy and Wright. But if you wish, you can look at it in Hardy and Wright. I've got the sixth edition, but any additional work, and there it's around. Work, and there it's around pages 455 to 457. But even though he's not here, I wanted to give a shout out to Ram Murdy, because in his book, sections two to three, even if I had seen it before I had read his, this is problem with analytic number theory. Even if I had seen Bertrand's postulate before I read Murdy's book, it was only when I read it in Murdy's book. When I read it in Murdy's book, that I got an appreciation for how clean-cut it is and how quote-unquote easy it is to prove if you know what you're doing. Again, haven't even told you what Bertrand's postulate is. Terrible. So I don't know who this fellow was, but 1845 is what the internet tells me. It was when Burton was making his postulate saying. So, what's the statement? So, what's the statement? The statement you can write it in lots of different ways. And the postulate is the statement that for all n bigger than 1, there is a prime in the interval n to 2n. Now, you could say for all n bigger than or equal to 1, and you could have endpoints or something, whatever. So, when n is 2, the prime 3 is inside there. When n is 3, the prime 5 is inside there. The prime five is inside there, and so on. Now, in a way, you could take this statement from 1845 and you could say, well, maybe it's true, maybe it isn't true. If I know the prime number theorem, I should be able to prove that pretty quickly. So by the prime number theorem, which was proved a good 50 years ago. Was proved a good 50 years after Bertrand's postule, I should point out. But by the prime number theorem, we know that pi of x x over log x plus O of x over log squared x. And now for any c bigger than one, you do pi of c x. And now I'll leave it to you. You push those things together. Pi of X grows like this. Pick your favorite C bigger than one, like 1.01. That'll do. 1.01, you do this thing as well. You do this, subtract that. You have to be a bit careful because you have to expand this. And if you're like me, you forget how Taylor series work, but hopefully you're cleverer than me and you can do that. You subtract this, and then you end up. This and then you end up showing that this take this is positive for all x large because this take this has a term that's going like c minus 1 x over log x plus an error term. And even if c is really, really small, like 1.001, the main term swamps the error term, provided we're out large enough. So by the prime number theorem, which came 50 years later, by the prime number theorem, this statement not only is true, Not only is true, but is true if you replace this by 1.0001. You might have to go out very, very far. So, do you want a prime between n and 1.001n? If that's what you want, yeah, you can go out to n bigger than 10 to the 10 to the 10, and that'll be fine. So, the prime number theorem gives you Bertrand plus more. But it's kind of ugly in that it doesn't tell you how big n has to be, it relies on a result that's quite. It relies on a result that's quite strong. You're going to have to have an explicit version of the prime number theorem, and even then, it might not be obvious how to do that. So, Bertrand's postulate is asking for a lot less. But it's nice and it's neat. Respect in the sense that these proofs, this one here, something boils down to checking that the numbers, I don't know, I don't know, two up to 400, all the primes from two up to 400 lie in suitable intervals. And that's as much of computation as there is in there. And this one here is very clever, uses ideas from Ramanujan. And when you package these things together, you see that this falls out without recourse to zero free regions, zero density estimates, finite verification, Riemann hypothesis, et cetera. Verification, the Riemann hypothesis, etc. Those are all good things. They're my bread and butter things. That's how I feed my family. But this result here gives some respect, gives some clarity. Says if you want this short, clean-cut result, you can get it and you don't have to feed in complicated stuff. No one likes to see how the sausage is made in explicit number theory. But here there is no sausage. It's like a matrix quote, but worse. Here there are no complicated things that go into Bertrand's postulate. Into Bertrand's postulate. There's some very clever, elementary, and ingenious arguments. Now, as a side note on Bertrand's postulate, we can ask similar questions. We can say, well, if there's a prime in this interval, is it possible for me to show in a similarly elementary way, maybe there's a prime in interval n to 1.5n? And a lot of people have done that. That at some point it becomes a case of how interesting does that result become? Can you show in an elementary way for all n bigger than a thousand, say there's a prime between n and 1.01 n? Maybe the answer is yes, maybe the answer is no. But as we move further and further away from Bertrand's postulate, the clarity, the respect that one gets from the proof, in my mind anyway, tends to diminish. Why? Because here we didn't have to figure. Because here we didn't have to feed much in. We had this nice combinatorial argument. We have this one here from Chebyshev. We have these arguments put together to give us a nice, clear-cut result. And again, this being the last in the bucket of principles, I think that if one can establish a result that's nice and clear-cut, that doesn't use a cannon to kill a mosquito, we shouldn't need to rely on the Rely on the latest verification of the Riemann hypothesis in finite intervals and the latest whatever. We shouldn't have to rely on that, hopefully, to prove a result like this, and we don't. So rather than use a strong result here, we can get by with much more elementary arguments. And that I think encapsulates some of the ideas a little bit better. So I have some miscellaneous examples that I've written down. But in the interest of time, I just wanted to write up the five R's again. Five R's again. So again, these criteria you may well object to, possibly on good grounds. It may well be that some of them overlap, some of them are redundant, some of them are silly, maybe that I've missed some good ones. But the five ones I had down were resolve a conjecture completely. Use an explicit result because you require it somewhere else. Refocus attention. Focus attention, meaning people should put their efforts into something different. Rally the troops, try to have some challenge, some discrete challenge for future people to tackle, and the Aretha Franklin Award, some respect where you can underline the exact idea that's going on in a proof without extra machinery. Thank you very much. Thank you very much. That's a very good question. It's a very good question. Let me dig my notes up here. So, the question was: what about the sort of intuition heuristic that Lehman gives for not expecting a counterexample to be very low down. So, I'll I'm sure someone else will write this down. The standard way that one sees the so-called explicit formula is to see the connection between primes on the left and zeros on the right. What's the left? The left is counting the sum of the von Mangold function. Roughly speaking, it's counting primes with logarithmic. It's counting primes with logarithmic weight plus some other prime powers. Don't worry about those. It's counting primes in some sense. This is looking at sums over the zeros, and the error is small enough. We don't have to worry about it. Now, you don't see it very often, but you can convert all that into an explicit formula. The prime number theorem means that this is asymptotic to this, meaning this here should be little l of x. You can also convert this into an asymptotic, into an explicit formula for pi. So pi of x goes. So pi of x goes like lie of x. Take x to the half over log x. Take a sum over the zeros. Lie of x to the row plus an error. So if pi is asymptotic to lie, it should be that this and this and this is all small. Little o of x over log x same. So what does Lehman do? He says, on our So, what does Lehman do? He says, on RH, I can add these guys together: lie of x to the row plus lie of x to the row conjugate. I'm going to add the zeros in conjugate pairs. So, again, I'm sure someone will talk at length about the explicit formula. When you have to add these together, you have to add zeros in conjugate pairs for convergence thingies. So, Lehman says on RH, I can add these things together, and what do I get? Things together, and what do I get? Now, the thing I'm about to write in the braces is very important. If we look at this, pi take lie goes like minus this term, minus this term. So the x to the half log x, we get another one over here. So this is x to the half log x times one. And the stuff in here is going like x to the half log x times. Here is going like x to the half log x times something else. So, what is the something else? You might say, Well, what's that about? The only thing you need to know from the thing in the braces, sign of anything, I don't care what it is, it's bounded by one in size. Is it's bounded by one in size because of anything, I don't care what it is, it's bounded by one in size. The numerator, therefore, is bounded by a constant times gamma. The denominator is going like gamma squared. So the thing inside here is going like one over gamma squared. Now that means in the sum, as the gammas get further and further out, I'm getting a smaller and smaller contribution of them. Now, what do I need? Now, what do I need for pi of x to be bigger than li of x? Pi of x minus li of x is equal to this factor here, x to the half over log x outside of a minus one minus this term in the braces. So, if I want pi to be bigger than lie, I need this thing to be bigger than zero. Minus one is minus one, and that's true. For this thing to be bigger than zero, this thing here, or this thing in here, has to be less than minus one. That means this thing here has to be less than minus one. Now, this thing here, the summons are going like one over gamma. Are going like one over gamma. So the smallest gamma is 14 or whatever. So when I look at the size of these things, these things are small. So I need two things to happen. I need to add a whole bunch of these things up. The size is small, O of one over gamma. But also, there's going to be some cancellation and stuff going on in the numerator. I need to add these up to make the thing bigger or less than minus one. So I'm going to need a lot. So, I'm going to need a lot of gammas in there. I'm going to need an awful lot of gammas to add together. One, so that the absolute size of the thing is big enough, but also to avoid the cancellation going on in the numerator. So as Lehman says in his paper, because the smallest gamma is around 14, these things are going like O of one over gamma. It means we need an awful lot of these zeros to be in with a fighting chance of this thing here to be less than negative one. Less than negative one. And that's the only way we can hope for this whole thing to be positive. So that's his explanation as to why one ought not to expect there to be counterexamples low down, whatever low means. Does anyone have any other questions? That's right, yeah. Okay, so if you assume geo-rich enough Yeah, that's a very good question. I'm inclined to say no, at least not obviously. Well, I mean, sure. Um, well, I mean, sure, someone could do it, but um, Bertha's proof is completely computational. Um, what could be done, I suppose, is going the other way. So he also proved things like, you know, Riemann hypothesis true up to height x implies things like pi of x, take lie of. Take li of x. I can remember what this is supposed to be, whatever it is. The thing you get on Rh, log x or something, or maybe this is supposed to be psi of x, or whatever it is. You get good error terms in the prime number theorem up to height roughly x squared. So if you can prove the Riemann hypothesis to some crazy high height, you get very good estimates on the error term up to some bigger height. term up to some bigger height. The problem is that still won't get you that this take this is less than zero. And I think it would be interesting to look at whether one could use RH or GRH. I can't quite see how GRH is going to come to the table, but how RH or GRH could allow you to extend it. But certainly in his proof, no, there's no obvious way, I think, to insert that. I should also say that if you force me to put money on That if you forced me to put money on it, I would say that that e to the 727 blah blah blah. I would say that probably the first counter example is around that height. There are these other hunting grounds of Bays and Hudson and so on. But having worked on the problem myself for a couple of years, I would say that the first example probably is around that height. But who knows? Hopefully, someone here will prove it and then take my $3 Canadian off me. $3 Canadian off me. But it won't be corrected with inflation, though. So do it now. Do it quickly.