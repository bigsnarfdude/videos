Low level and short as much as I can. I will talk about the so-called Winsburg-Landor equation, joint work with Benjamin Yuri from Karskur. And the basic application that we are having in mind are superconductors, and I don't think I have to explain much to your audience like you. So, superconductors of course materials that allow to conduct electricity with no electrical resistance, they have a lot of interesting properties, and the one that will be in the And the one that will be in the focus of my talk are vortices. So you can imagine when you have a sufficiently strong magnetic field, then it can penetrate the superconducting material in these isolated points, which are exactly the vortices of the quantized amount of magnetic flux around it. Okay, and now the task is to simulate these kinds of scenarios. So for that, I will use the Ginzburg. I will use the Ginzburg-Landau model. So in this case, omega is my domain that describes the superconducting material. The superconductor itself is modeled by the water parameter, as usual. That's a complex value wave function. And once you have the water parameter, you get your physical observable, which is the density of superconducting electron pairs, by simply taking the absolute value of u squared. Of u squared. And this is a number between 0 and 1. So 0, no superconducting electron pairs, and 1 means 100% superconductor. Okay, and now in order to find the correct order parameter for our setting, we have to find a minimizer of the so-called Gibbs free energy. This is this functional here, Gibbs free energy. Mathematically, we minimize this functional. minimize this function or overall functions in H1. And this Gibbs free energy contains two crucial quantities. The first one is the magnetic potential, which I will assume is in L infinity. And I will also assume that it's divergence free and that it has a vanishing normal trace. And I will in particular, for simplicity, assume that this is given. I know in practice you also have to. I know in practice you also have to compute it, but to keep things simple, I will just assume it's given. These are typical assumptions in this context. Now, the important parameter is this material parameter, kappa, that appears here, which loosely speaking correlates with the temperature. And the very interesting case is when this material parameter is getting bigger and bigger and bigger. And why is that the case? And why is that the case? Well, maybe you have seen that before, but the material parameter is crucial for the appearance of the vortices. So, if you fix your magnetic field and you increase kappa, then you will get more and more vortices, and the vortices, they will get smaller and smaller and smaller and smaller. Okay, so this is the model, and let's say you want to compute these minimum items, to simulate this, for a given kappa. Like this, or a given kappa. And the question, the basic question that I would like to investigate in my talk is the following. So, obviously, the larger kappa, the more vortices, the smaller the vortices. Okay, now you want to do a practical computation. You go for, let's say, finite elements. You select the mesh, and the question is: is your mesh fine enough to capture the correct cortex pattern? Because obviously, if you use a coarse mesh, then there is no change. You use a coarse mesh, then there is no chance that you capture the correct vortex point. So, how does kappa translate into some kind of mesh resolution condition? You give me kappa, I give you a mesh size, then you run your simulations on that mesh and you will observe the correct vortex pattern. So this is the basic question that I would like to answer. And of course, the first thing that you should understand in this context is how do Is how do different norms of your minimizer scale with kappa? That's not so difficult to answer. I have already shown you, so the density is always between 0 and 1. So you can bound it pointwise by 1, which gives you an order 1 estimate for L two. You can also prove that the gradient scales with kappa, second derivative scale with kappa squared. Derivatives, scale of kappa squared, and so on, and so on, and so on. So, the more derivatives you have, the higher the dependency of kappa. But all we need for this talk is this one and this one. Gradient, scale of kappa, second derivatives, scale of kappa squared. And I will show you some estimates, and in order to have an appropriate measure for errors, I will go for these kinds of weighted norms. Of weighted norms. So I will always scale the gradient contribution with 1 over kappa, because in this case, if you now would investigate this more for your minimizer, then both terms would be of the same order. Remember, this is of order 1, this is of order kappa, scaled with 1 over kappa, then both of them are of order 1. This is very natural, and it's also a very natural norm for the error analysis. Okay, and what do we know? Of course, the Ginzburg-Landau equation is around for quite some time, so there are, of course, analytical results in this case. So let me briefly describe the most popular setting in this context for the other analysis, where you compare your exact minimizer of the energy with some appropriate minimizer in your. Minimizer in for simplicity P1 finite element space. So you somehow computed your discrete minimizer and you somehow identified a real minimizer to which you can compare it. And then the question, how big is the error? What do we know? Well, there are basically this question was investigated by a series of papers by John Duhl, Max Glinsberger and Janet Peterson at the beginning of the nineteen nineties. Of the 1990s, so quite some time ago, but they only proved optimal order error estimates. They did not really go that deep to work out all the Kappa dependencies, and they also made some simplifying assumptions in the error analysis. And that's basically where it stopped when it comes down to the minimization problem. So, the theory that we have is 30 years old. But of course, there's also the time-dependent principle-Glando equation. Equation, which has been studied a lot, especially in recent years with major contributions by Wuyang. But in this case, the problem is structurally very different, and also the focus of what they were looking at, namely non-smooth, non-convex domains, is a very different setting. They did not go after the kappa dependency, so there is nothing here that we could use for the minimization problem. Okay, so Okay, so um keep it short, basically we do not know what to expect because there is no error analysis. But um we could approach the problem naively to give a very quick answer in half a slide. It's very simple. We could say right we we now know how different norms of U scale with Kappa and then we can say okay the best approximation of our unknown minimizer in the finite element space In the finite element space, must behave like H times kappa. This is a standard estimate. You just bound this by U minus the interpolation or quasi-interpolation of U. You get some H, you get some H2 norm of U. You look at the kappas and you will find out that you get H times Kappa. Okay, very simple, very boring. And if you see that result, then you would say, okay. Then you would say, okay, so probably if the mesh size is smaller than 1 over kappa, you should capture the correct vortex platter. At least that's what we would get for the best approximation. And so now the question is, can we get the same one for the actual minimizers? And spoiler alert, it will be a little bit different. Okay, so there will be a surprise. Alright, so this is what we get for the best approximation. We get for the best approximation, and now I'll show you at least some math. I could directly skip to the main result, but I at least show you some math to give you an explanation why something is breaking. Okay, the first thing that you should be aware of in this context is that this is a non-convex energy minimization problem. The minimizers are not unique, and they are not even locally unique in the sense. Are not even locally unique in the sense that we have this Lauder invariance property. So, in this case, you can multiply any given minimizer with a complex number that is one in modulus, and then the energy level will be the same, which means if u is a minimizer, then alpha times u is a minimizer as well for any alpha that you can pick. Okay, so this is something that you have to be aware of and that you have to account in your error analysis. In your error analysis. And usually, when you want to derive error estimates between exact minimizers and discrete minimizers, you have to look into the derivatives of your energy because you exploit the classical properties. You have a minimizer of an energy, which means the derivative of your energy in the minimizer is zero. Second-order condition. Second order condition, and you have this first order condition, sorry, and second order condition. The second derivative evaluated in the minimizer has a non-negative spectrum. So these are the classical conditions, and that's typically what you explored. So the first order condition, derivative of the energy in U is equal to zero. You don't have to process this, okay? I just computed it here for you. It's known as the Ginsburg. Here for you is known as the Gins-Pog-Lando equation, the stationary one. Okay, so from the energy minimization problem, this is the Gins-Pog-Lando equation. And then you have a complicated structure for the second derivative, which looks like this. And what you usually do at this point is you exploit the positivity of the spectrum of that operator to construct some kind of rich projection, and that's in the core of your error estimates. But here we have this problem of the But here we have this problem of the missing uniqueness, uh which means that this must be a singular operator. It's not a doesn't have a positive spectrum and it's also easy to identify why this is happening. So just a quick graphic. So let's say you have your minimizer U, and these are, so on this on the circle line, you have all the other minimizers that you obtain by multiplying. That you obtain by multiplying with a complex number that is one in modulus. And since the energy is invariant under these kinds of phase shifts, every point on the circle line is a minimizer, which means that if you follow a path on the circle line, the energy is constant, which means the first and the second derivative of your energy must be zero. And then the question is: okay, so what is the direction that keeps you on the circle line? Well, that's either I times U or minus. Well, that's either I times U or minus I times U, depending on if you pass it clockwise or counterclockwise. And the direct conclusion is that this direction here, this prohibitive direction, is the one where the second derivative becomes negative. Or in other words, i times u is an eigenfunction to the eigenvalues. Alright, you can also show this. You can also show this with a direct calculation, it's not very difficult. So this is the singular direction, but all the other eigenvalues should be positive, and the positivity of the spectrum, except for this eigenvalue zero, gives you in substability on the orthogonal complement of the first eigenspace. Complement of the first eigenspace. So Ru, the span of Ru is the first eigenspace. So the eigenspace belonging to the eigengard is zero. We go to the orthogonal complement of that space. Now the spectrum is positive. This operator is invertible and you have substability. Okay, now we have something that we can use, but there is something that is really important here is you get the existence of an inf sub constant. Existence of an inf sub constant which is positive, but you do not know how it depends on kappa. Okay, so there is some unknown dependency of kappa entering here. And that's also the reason why you should not start to identify some kind of risk projection based on this operator here. Because that would imply that at the end of the day you get a pollution in your error estimates depending on that in subcontract. Estimates depending on that in sub constant. So, what we did instead was the following: we took the linear part of that second derivative, that's the first part, a linear part of the second derivative of the energy, and then we stabilized it by sufficiently large L2 contribution so that this is an elliptic and coercive bilinear form on the space. Bilinear form on the space H4 kata. So you can prove that this is continuous and coercive with constants that are independent of kappa. And now we can define the Ritz projection with respect to this bilinear form. And yeah, since this is a standard Ritz projection in the usual way you get that the projection of any function in the orthodox complement of IQ. The orthodox complement of IQ is a best approximation. This is a quasi-best approximation of that function. This is a standard result. And now in one slide, the core of the argument. So this is what you would like to compare. You have to make sure that they are in the same phase so that you can compare them. Remember, you don't want uniqueness. So these being in the same phase means Being in the same phase means UH is in the orthogonal complement of IU, which you can guarantee. And then you bound this by the error of the ritz projection. That's fine, that gives you an optimal order error estimate, right? We can bound this without constants by the best approximation. And then we have this difference between the exact uh the minimizer that you have and the rich projection. Okay, and here is where something happens. And here is where something happens. So, this is now a discrete element in the orthogonal complement of IU. So, we can use the IMSOP stability, but in this point we get this IMSOP constant that we do not know. Right? Okay. Then we still have the second term that we have to estimate and there we can split it, we can add and subtract artificially u. Subtract artificially u and we get two types of contributions. For the first contribution, you exploit that this operator here was based on the linear part of that second derivative, which means all the higher order terms are cancelled out here, and all you get is a lower order contribution in L2, which converges faster. And for this term here at the end, you expect At the end, you exploit the Ginz-Bog-Landau equation. So, the condition for first derivative in U is zero, and first derivative of the energy in UH is zero. And then again, at the end of the day, there is some technical estimates happening. You also get some higher-order terms. Okay, so what do you see? You see, you have an optimal order term, and then you have a higher-order term, which is polluted by the instop constants. And if you play now an absorption argument where you Now, an absorption argument where you are hiding these higher order terms here and here. You, at the end of the day, you end up with an estimate like this, which tells you that, yes, you get basically as good as the best approximation, but you have a pollution of mesh size times kappa times the infoc constant. And what does it mean? It means it is not sufficient. It means it is not sufficient if your mesh size just compensates the size of kappa. It also needs to compensate the size of the MSUP constant in order to get this term here small and to get accurate approximations. And this is not something that you just you could say, okay, maybe the error estimates were not optimal, right? I mean, maybe the strategy was not a good strategy. The strategy was not a good strategy. But here is what you see in practice. This is just a reminder. This is how the minimizers look like. We use the discrete gradient flow to compute things. Now, interesting are the pictures. What do you see? The dotted lines, maybe hard to see, are the best approximations in the finite element space. And they directly follow the expected line H times kappa. The error behaves as expected. The error behaves as expected like h times kappa. But the actual minimizer that you compute in the first regime is just not meaningful. You get something, but it's not a correct minimizer. You have a pre-asymptotic regime, and only at some point you go down to the correct line, and you're basically as good as the best approximation. So you clearly see, also numerically, that you first have to compensate some unknown constant. Compensate some unknown constant. And you could also ask: what is the energy of the good approximations? Why don't we find them? So I computed here the energy of the best approximations, and you see they have a very high energy. So they are simply not found in your final element displays because their energy is too high. And the bad approximations, they have a low energy. That's why you find them. Okay, so clearly you see this, and we also Clearly, you see this, and we also computed the spectrum of that operator, and we found zero as expected. And from looking at the spectrum, it's hard to make a clear prediction, but our conjecture is that this sub constant scales with one over kappa, which means at the end of the day, your effective resolution condition is not h should be smaller than kappa, and no, h should be smaller. h should be smaller than 1 over kappa, but h should be smaller than 1 over kappa squared. And this is a significantly stronger resolution condition than what you would expect if you just go for the best approximations. Okay, so there is really something happening here. There is a pollution effect which you clearly see numerically and which is part of the error estimates. And we also came up with a Came up with a suggestion of how to get rid of this, and I'll keep it short. But what we proposed were to go to multi-scale spaces or generalized finite element spaces based on a construction called localized osmium decomposition, which goes back to a work by Daniel and Axon Morphus. And in our case, we did the following: we used the linear part of the second derivative of The linear part of the second derivative of the energy and define the differential operator for this bilinear form. And that's possibly indefinite. It might be even singular. We don't stabilize it. First, we made a mistake to stabilize it, but then it doesn't work. So you should not stabilize it. And then you look at the image of applying the inverse of the differential operator to finite element functions. To finite element functions. I'll not go into details. You can compute this space practically. It has a quasi-local basis. You can do this, so this is not an issue from a practical point of view. To compute this space, all that you have to know is if h times kappa is smaller than 1, which is what we need anyway, this is a well-defined space, and it has the same dimension as our P1 finite element space. So same dimension. So, same dimension as VH. And in that space, we did an error analysis, and we came up with a very different type of estimate. So, first of all, the nominating error is not kappa times h, but kappa times h to the power 3. So, we get a third-order convergence in that space instead of a linear-order convergence, which is much better, of course. Convergence, which is much better, of course, third order instead of linear order, even though the spaces have the same dimension. But what is even more crucial is that this pollution term here that we had before now behaves like, well, kappa to the power 4, h times to the power 4, times this problematic in-sub constant. And that means, at the end of the day, we don't have to compensate the in-sub constant, but only But only the M sub constant to 1 over 4, which is, of course, a significant improvement if this scales like kappa. That's not kappa squared times h, but kappa to the power one plus one water. And if you look into the error estimates at some experiments, you still see You still see, there is still some pre-asymptotic regime, which we expect, right? Because we still have some kind of pollution term. It's not totally gone, but it's a very short regime. And then we almost right away follow the convergence of the best approximation. The curves are flattening out because we reach the accuracy of the reference solutions. So we ignore these parts. We basically have this third-order convergence. And it becomes even more striking if you have. If you have a look at the actual approximations, so this is the approximation that you would get in the standard finite element space. This is the approximation that you get in the multi-scale space of the same dimension. And you see this space, this is the mesh size, 2 to the minus 2. The dimension of the space is 9. You have 9 basis functions, and you basically already get the correct vortex pattern. The correct vortex pattern. It's still not yet clean, but the number of vortices and the position is roughly correct. And then here, you still have nothing in the standard finite element space, but in that, I think the dimension is around 60. So 60 basis functions, you almost perfectly capture the correct vertex pattern. And then it doesn't get better very much. Whereas here, you need to go to this fine mesh, and I don't know if you can see it, but there is. I don't know if you can see it, but there is still some roughness here. So it's still not as good as in this picture. So these spaces, you really seem to get rid of this resolution condition for the spaces, and you can actually compute your minimizers much faster than you can do with the standard spaces. Alright, and that was actually all that I wanted to say. Here is a list of references to introduction. A list of references if you're interested. The paper is not yet online, but it will be hopefully one or two weeks. Thank you. Questions? Yes. I think that when it comes to the local non-uniqueness of the solution, I think that can even I think there can even be. Okay, so if you have, for example, if you have a circular domain and your curve of A was also circular and symmetric, then I think you could look at the solution with vertexes like this, but that you could also rotate it. Yes, we had actually the same idea, but there is a it's it's if you do the calculations and you would try to figure out And you would try to figure out what is the direction that you should take, what is the tangential direction that gives you a problematic direction that essentially reproduces this behavior. Because you can sometimes rotate it. And then we figured out that this direction is something like, I don't remember a curl of U, I don't remember in certain direction. And that was not a space, that was not a function in the finite element space, H1. So there is no continuous path that gives you. No continuous path that gets you there, and it will not appear as eigenvalue zero. It's a very subtle thing, and at first it was exactly our idea that we will get the second eigenvalue zero if everything is circular and rotational invariant. But at the end of the day, it's not a very good command. Very nice, my mark. Uh the multi-scale functions we were computing in the end, they have the flavor of a cubic spline. The flavor of the cubic spline in some sense, so they that explains also the higher divergence. My question is: in your error analysis, is the order more important or is it more like the right type of regularity of the basis function in the sense that fluxes are continuous and stuff like that? So, what is more important? Yeah, I mean, I I cannot give you a definite answer. We are at the moment implementing higher order finite elements to make uh some comparison. Um at the moment I would say it's At the moment I would say it's really important to get rid of this pre-asymptotic regime, that this is reduced. And we believe that it will not work so well with higher-order finite elements, but I cannot give you a definite answer yet. Using P1 like non-finite elements? Yes. To impose A.n equal to zero on the border? No, we don't impose it. At the moment, we. This is the column gauge unit. Yeah, yeah, yeah, yes, yes, yes. So, the natural balance is exactly. But so, we assume that the magnetic field is given, so we just pick something. I know it's part of the analysis, it's almost worked out even for the general case. But even in the general case, you don't really impose the boundary condition because when you look at the the Ginsburg-Lando equation, um the natural boundary condition falls. Condition the natural boundary condition is red U plus I kappa A times N is equal to zero. And this is the natural boundary condition that you will get, right, when you go for a minimizer of your energy. You don't have to impose it. If you have sufficient regularity, this is what you observe in the weak formulation. In the wheel formulation, yes. Because we're never exploiting this. We just have the energy, we don't prescribe any boundary condition because it's a natural one, and then we take the gradient to go down to the minimizer. But you're right in the sense that if you have a very rough domain, you run into problems. I mean, then even having a divergence regorge is. Yes, right, I know. Boo young worked a lot in this. Buyang worked a lot in this direction. It's a proper, but it's a start. We just want to understand the problem. I just ask also a similar question. That if you solve the equation with A as an unknown function, then actually A dot A is zero unknown. So do you choose such a given given A which set satisfies this condition? Yes. Yes, so in this experiment, I was using an A which fulfills it. It's an analytical function, you can have it on the slide. So it was. Yeah, this one. Okay, just something simple. At the moment, we're doing the analysis for the case that A is not given. We started with the case that A was given to start with something. This is a smooth thing, and we already noticed that when you do the calculations and also you go to different spaces, multi-scale space, you need to take a different approach for A. You should not take a multi-scale approach for A. Because very quickly, when you use a gradient flow to compute your minimizer simultaneously for A and for the order parameter, it's super quickly you have convergence in A. You have convergence in A. So the state for A is reached within, I don't know, four or five iterations, nothing changes. But then it works hard to find the order power meter because that has all the vertices and things like this. I did forget about your condition RV. What's the regularity condition RV? So here I assumed that it's divergence free, L infinity, and has a vanishing normal trace. That that's all that we need for this analysis. All that we need for this analysis. But of course, we would like to reduce it more, but we haven't managed it yet.     Okay, so you have to have a data.    