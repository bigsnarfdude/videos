Thank you, Alina, and thank all the organizers for the nice invitation. I'm sorry I couldn't make it in person because I have one meeting last week and another meeting in the following week. And given the length of the flight, I won't be able to find a proper flight to actually handle three meetings. Okay, so I'm going to talk about a recent work that's collaborated with. Work that's collaborated with V1, and this is to show a certain numerical scheme is asymptotic preserving. What I'm going to do is the brief outline is that I will explain to you what the AP method is. I think many of you probably know this. And then I'll explain the equation that we're going to look at. And then I'll show you the scheme. The scheme is designed by Lee and her collaboration. By Li and her collaborator, she. And our job is to show that their scheme is AP, is asymptotic preserving. So, this is a numerical analysis talk. And okay, so first of all, what is an AP method? So, this is a very brief introduction, and we know that a lot of people have worked on the AP method. Here, I especially mention Shijin's group and his collaborators. So, what basically people want to do is you have a more Basically, people want to do it is you have a multi-scale, let's say kinetic equation. It doesn't have to be kinetic, but since we are the kinetic workshop, so kinetic equation, multi-scale. Now, if you just do a direct discretization, taking epsilon like a constant, then and the direct error estimate, usually what you get is the error estimate, the error bound will depend on one over epsilon, which is not good. Or put another way, if you want a bounded error bound in epsilon, you will have Error bound in epsilon, you will have to kind of take delta t to be dependent on epsilon. Again, that is not good because in multi-scale computation, when epsilon is very small, you don't want to take your delta t to be too small. So the goal of an AP method is to say that let's design a scheme so that we can show the error actually is independent of epsilon. So how is this durable? Now, the main idea is by using an additional information of this equation. So what we let's say. So, what we let's say that in general, if you have this epsilon, this multi-scale equation, you believe there is a certain kind of continuous asymptotic limit. For kinetic equation, that's not surprising at all. We usually expect some kind of macroscopic equation asymptotic limit. So, let's say we do know that, then, this is an extra piece of information that can help us to get a uniforming epsilon estimate. So, the idea is now. So the idea is now our goal is to estimate the discretized multi-scale equation or solution, which I denoted as F epsilon delta, compare it with the true solution to the multi-scale equation, F epsilon. So if you don't do anything, that's basically the estimate along the green line. You do a direct estimate. But sorry, as we said, usually that is that can that area will depend on when we. Can that area will depend on one over epsilon in addition to the delta t. Now, with the additional information of the asthmatic limit, we can actually compare these two along the red route. So we make a detour to say that let's instead of compare it with what's happening on the continuous level. So I put the error estimate here to say the first way is to do a direct estimate. And the second way is that we're going to compare the solution, the discourse. The solution, the discretized solution for both the discrete level and the continuous level with what's happening in the asymptotic limit. And then plus the error that is the numerical error for solving the asymptotic limiting equation. So you add up those three errors, basically those three line segment errors along those three roots, and then you minimize. So in that case, you can. Minimize. So, in that case, you can minimize and then get that your error can be independent of epsilon. So, this is the main idea behind the AP method. And the equation that we are going to look at is the Levy-Foker-Planck equation. So, we want to numerically compute this one. Afterwards, we will introduce the scale. So, this one is unscaled yet, just to show you the operator. So, the Levy Foker Planck operator is basically to replace the Laplacian in the Fokker-Planck by a fraction. In the focal plane, by a fraction of the plastic. So that's what we're getting. And what we do know is it has an equilibrium state, and its equilibrium state is algebraically decaying. It can decay very, very slowly. So this is the main property of the equilibrium state, that for S between 0 and 1, you always get an infinite second moment. And for S that is very small, you actually don't even have a finite first moment. So that really introduces quite a bit of difficulty when we are trying. A bit of difficulty when we were trying to do analysis or numerical analysis. Okay, so this is the equation that we will be looking at later on with the scaling. And let me just quickly say what happens for the regular Foucault-Planck. For regular Foucault-Planck, what we know is with the Laplacian here, what we know is the equilibrium state is Gaussian, so it has any order of moments, especially it has the second-order moment. And why does that help? Because in that case, if you now introduce the parabolic. Now, introduce the parabolic scaling into the regular focal planck. What you can prove, for example, formally using the Hilbert expansion, you do order by order, then you can see that the limiting equation is going to be a diffusion equation. And the important thing is the diffusion coefficient relies on the second moment of the equilibrium state. So, that basically says for the Levi-Franco-Planck, you do not expect that you are going to get a diffusion equation as a limit. Instead, what you have is a fraction. Instead, what you have is a fractional diffusion equation in the limit. So, this is the scaling that we introduce. This is the proper scaling that we expect to get something meaningful in the limit. And again, the limiting equation is for the averaged F averaged in V. And then the limiting equation is the fractional diffusion equation. You notice that we started with a fractional diffusion in V, and in the limit, it becomes a fractional diffusion in X. diffusion in x okay and it is this equation that we really are going to perform the numerical uh computation and um and we hope that the numerical scheme uh that we have would be a p so that's the justification that i'm going to show you okay and let me say that um if you remember the three roots basically if you want to do an ap you'll have to kind of understand how your multi-scale equation converges to the limiting asymptotic equation To the limiting asymptotic equation, right? And well, for this equation, if you forget about numerical schemes, just ask that if I pass epsilon to zero, how do I prove that it converges to the fractional diffusion? Well, there are multiple ways that you can do that. You can also do a Hilbert expansion, but it's much trickier. So you will have to pick up the correct terms to balance on the same scale. But there is a very nice way that was done by Sespurana Malay and Trivi. Malay and Trivisa, that what they did is they actually used a specific special type of test function to say that I building the transport phenomenon into the test function. And if you do this kind of test function, now why the transport term is important? Because whether it's diffusion or fractional diffusion, at the end of the day, the Laplacian in X or the fractional Laplacian in X, they all come from the transport term. So that's why the The transport term. So that's why the transport really captures the diffusion or fractional diffusion part. So if you use this kind of test function, the important thing is it can actually transfer between the x-derivative and v derivative. Especially, I have two equations here to show you that for this type of test functions, for example, if you do fractional in v, then it becomes fractional in x. And the v dot grad, which is part of The V dot grad, which is part of the V Foucault Planck, becomes the transport index. Okay, so if you use all this information and the weak formulation of the equation, you put all the derivative to the test functions, what you see nicely is the middle terms are canceled out and the fractional diffusion in V becomes fractional diffusion in X. So this is how you basically get the weak formulation of the limiting equation. Okay, so the numerical scheme will be built upon The numerical scheme will be built upon this idea of picking up a test function that is of this specific format. But it's not exactly by using a test function. Instead, what the numerical scheme would look like is to say, if I want to do an asymptotic limit, then I would usually decompose my f as the limiting row, which is independent of v times n. It's independent of V times M plus G, the correction term. But here, the idea is borrowed from the previous analysis to say that here, I actually do not do just rho of tx. Let's build in this x plus epsilon v into the rho. And this is what is called as the eta sub-epsilon term. So the decomposition is slightly different from, oh, I missed the s here, the fractional, the less. The Levy Fokker Planck is 2s, absolute 2s here. And you do the decomposition in this specific way. So even the leading order still depends on the V, but you see that it's a very small dependence on V. Because if my row satisfies the, I think, yes, this sign is correct. If my row satisfies the fractional diffusion equation. Satisfies the fractional diffusion equation, then my eta will also satisfy the same equation. The only difference is the initial data is shifted a little bit. So one can actually prove that rho and eta are close by epsilon to a certain power in the proper norms. Okay, so this is the starting point of constructing the numerical scheme. And based on this, we split the equation into the equation for the epsilon eta and the equation for the remainder term g. And that would be looking like this. And that would be looking like this. Eta will satisfy the fractional diffusion equation. Eventually, it will become the row equation. Then the rest of the things will go to the g equation. Now, depending on how small or how big your epsilon is, it's not necessary that your g is close to zero, the correction term, because if epsilon is not that small, then g, of course, is of order one. So there is some complicated commutator terms involved here just because we were. Involved here, just because we were splitting the equation. Let me just say that here we did not do any approximation. If you add back the combination of eta and g, you get exactly the f back, which is the solution to the original Levy-Fouca Planck. But then afterwards, we're going to introduce the actual numerical scheme because this comes from an operator splitting scheme, and this is semi-discrete because the X and V are continuous, the only discretized part. Continuous, the only discretized part is in time. So we just discretize the eta equation in finite difference. That's very straightforward. And for the G equation, what's done is that we separate the transport term from the rest of the terms. There is a gamma that's manually add in. This is to stabilize the system because this L operator, it has a null space, right? It has an equilibrium state. It does have a null space. So it will be easier. No space. So it would be easier if we have a gamma here. And later on, you will see in the statement that gamma, the choice of gamma, actually depends on epsilon and delta t. And then after this operative splitting, our approximate solution will be the combination of eta n and g n in this way. By doing this, we can actually write down the difference equation, the error equation. So the error equation would be that So the error equation would be that the error will be the true solution eta evaluated at time tn minus the discrete solution. Same with g. And then you do proper the same combination, you get the area equation for the total density function f. And we denote it as f tilde. And one can also work out the equation for f tilde. So it has a large part, which is basically the Levy-Foker-Planck part. Lavi-Fouca-Planck part. And then it had some error terms. The E1 and E2, these are much easier errors. These are discretization errors coming from discretizing the differentiation in time. The harder error to deal with later on is the error that comes from the operator splitting. So this is basically the main equation that we'll be looking at. And we ask how big this F tilde is. And the statement of the theorem basically says if I allow enough. And if I allow enough smoothness, so the for initial data, if we have enough smoothness up to second derivative in a certain space, then outside an initial layer, so we do skip an initial layer. In this way, we do not require our initial data to be, for example, close to the equilibrium state. We can start with things that are far away from equilibrium, but our analysis is outside. But our analysis is outside of an initial layer. So, outside of the initial layer, at the end of the day, what we can prove is a weighted norm of the error is going to be bounded by a C independent of epsilon times delta T to a certain power. So all these parameters, the power alpha and the beta, the B power, they are all explicit. So that's the main statement. And I'm going to briefly explain how the proof is. How the proof is done. So the proof is slightly different from the usual AP method. So we view the proof as a slight generalization of the usual AP method. On the right-hand side is the usual AP that basically says no matter what epsilon, what delta T you have, you can always do the estimates in two ways: the green way and the red way. Then you minimize. So this is the usual one. This is too strong. We were not able to get. Is too strong. We were not able to get such a strong statement. Instead, what we do is for a certain range of epsilon and delta t, we can do the green. For a certain range, we can do the red. And then in this case, we basically estimate, for example, for the green one, this is when epsilon is relatively large compared to delta t. And we call this region as the kinetic region, which is not very precise, but you get the idea. Is not very precise, but you get the idea that epsilon is not too small compared with delta t. So, when epsilon is not too small, we can do a direct as a direct comparison. When epsilon is relatively small, we make use of the fractional diffusion limit. So, this is what we view as a slight generalization of the usual AP method. And here is also to say that the power beta here, which is the interface between the two. Which is the interface between the two regions that we're going to use different methods to do the estimate. This beta is explicitly computable, it belongs to a certain range, and kinetic region is when the epsilon is relatively large with respect to delta t and the other one is what we call as diffusive and intermediate regions. Okay, so let me show you how the analysis are done in these two regions separately. Done in these two regions separately. For the diffusive and intermediate regions, let me see that this is the region that we're going to make use of the fractional diffusion equation at the end. But this also means something that we have to get the convergence rate when we are taking the asymptotic limit. The work by Trevisa's group, they derived that the fractional diffusion is the right asymptotic limit, but their analysis is through the weak formulation and there was no obvious conclusion. And there was no obvious convergence rate. So we have to make, we have to find a stronger statement. At the end of the day, what we did is, because these two equations are both linear equations and they're posed over the whole domain, we actually just solved both equations and then directly compute the difference between the two solutions. So the idea is what I wrote down here by using Fourier transform, can see that we can get the row solution, how it depends. Solution: How it depends on the initial data, we can get the f solution, which is rather complicated. But at the end of the day, after a very lengthy calculation, we can actually get that by directly comparing the two solution formulas, we get their difference is of order epsilon to the 2s. So, that is a way to show that if epsilon is really small, how we make use of the diffusion limit. Diffusion limit. Okay, so based on this, we then get the error estimate. Now, the error estimate, you remember that is F tilde. F tilde will be the part that comes from eta tilde. That is the easy part. Eta tilde is just a macroscopic equation. So that's very easy to control. And then we have the correction term, the difference between the discrete one minus the continuous one. Now, the previous statement using the previous statement using the Fourier transform basically tells us that the continuous one is actually small. It is small, the bound is in terms of epsilon. Now, because we're looking at the region where epsilon is smaller than delta t to some beta, that basically tells us that g itself at evaluated at tn is of order delta t to some power. So this term is good. At the end of the day, we just need to control the g n. So we actually did. Control the GN. So we actually did not take their difference. We controlled both of them separately in the diffusive and intermediate regions. And let me just say that in this region, our choice of the gamma depends on also the relative size of delta t and epsilon for epsilon not very, very small. The gamma can be fixed as a constant. Otherwise, gamma itself needs to be chosen as fairly large. Beta here is less than. Fairly large. Beta here is less than one. And then by choosing this gamma, we can prove that the L2 norm of this G n is of order delta T. The idea of proving it is actually just to solve this Gn from the numerical scheme and we can get iterative equation and then we control the size of the G n. The choice of the gamma comes from that when you are controlling the When you are controlling the norm of the operator, the operator norm would depend on this gamma for enough convergence. We would introduce certain constraints for this gamma. Let me say that this doesn't seem to be very analytical, the choice of gamma. In fact, Li tested that if we are in this region and we just randomly choose gamma, for some gamma, it's not going to work well. For example, if we are in this region and we just fix gamma as order one, the Gamma as order one, the convergence seems to be very bad. So it seems to suggest that you have to kind of tune your gamma with respect to the parameters. So this is a part for the diffusion and intermediate regions where epsilon is relatively small. Now, when epsilon is relatively large, this is what we call as a kinetic region. This is where the harder analysis go. And we worked with the equation for We worked with the equation for F tilde directly. And what we can prove is in this region, the choice of gamma is basically also satisfy a certain range dependent on epsilon and delta t. And we can explicitly choose some beta and alpha to show that the norm is bounded by delta t. Okay, so let me just show you some steps of proof here. Some steps of proof here. The equation for the error, this is what I copied down. So the Levy-FoucaPlanck part, the discretization error part, and the operator splitting error part. And one of the difficulties is really because this M decays too slowly. So for example, if S is very small, M does not even have a first moment. And if you recall in this L. And if you recall, in this L equation, in this L operator, L composed of terms like gradient V times F, which basically means even for that V times F is not in L1. So we had to do some kind of negative weight to compensate that. But the choice of the negative weight took us quite a long time to figure out. We have to carefully pick the coefficients here to make sure that all, because we have a lot of commutator estimates with this L, which has a fractional. With this L, which has a fractional diffusion. And at the end of the day, in any case, what we figured out is if we do a negative weight with a certain B power, which was also determined later on, and with a small enough delta, which depends on epsilon and delta T, and then our energy, if we're using this Fn for the energy, you can actually directly compute the evolution of this Fn iteratively, and the iteration is given here. Iteration is given here with the difference between the adjacent two is controlled by an A, which is here, which is very complicated, and then an R, which is even more complicated. But the important thing is for the R, it is, for example, in every term you have a delta T. So R over delta T, first of all, is going to be bounded because if you and the delta appears in a positive part in this term. In a positive part, in this term, in a negative part in the second term, and nothing is in the third one. Third one is well controlled, so we just minimize this delta. This is how we choose the delta. We minimize over delta and then play with the coefficients to guarantee that this iteration converges well. And that's the to give you one example of what kind of parameters you might choose. For example, you can choose this b as one plus. You can choose this B as 1 plus 2s, and you can choose beta 0 as something that will depend on S and D. And the R file would be, this is all very complicated formulas. It's just an example to show you that, yes, indeed, we have a certain range of the parameters that depends on the dimension and depends on the S. And at the end of the day, let me just show you a quick numerical computation, which shows that it does look Which shows that it does look, it does show an AP and it has a better rate as our analysis. Our analysis has a rate that is too complicated. Here, the two pictures are for S that is less than 0.1 half. That's what we call as weak singularity and strong singularity. Both of them basically show a rate of order one, which we did not get that good rate. So we were just happy enough to have something that is AP. And that's all I want to say. Thank you. And that's all I want to say. Thank you. Thank you very much. Thank you. Questions? Yeah, just a minute. Hey, Wayran, this is Bertram. Hi. Thanks for a nice talk. Can you elaborate a bit on this initial layer, how this comes up in the second part of the proof? And does this not also depend on epsilon? Is not also depend on epsilon then? Yes, yes. The initial layer does depend on epsilon. Actually, let me show you one picture. Well, sorry, one formula, then you can see what the initial layer is. It's right here in the solution of the F, which is F epsilon. If you do the Fourier transform, you can see there is epsilon to the minus 2s times T. The initial layer is of size epsilon to the 2s. So in our proof, for example, we always want to, we want, eventually we want. To we want eventually, we want this term in many places to be uh uh to be controlled or well controlled. In that case, we want t over epsilon to the minus 2s to be well controlled. So, this is why we take t to be larger than some fixed t naught. And epsilon usually expected to be not too big, right? So, once we pick the t out away from zero, then t over epsilon to the 2s will be well controlled. That was the idea. Okay, see. Was the idea okay? Cheers, thank you. I actually have a question about the same slide here on this formula for the error. It really depends on S what is the order you have, right? Sometimes it will be epsilon squared, sometimes it will be if S is small, so this term, right, it will be order one or even less, right? Yes, so this is the part when S is less than delta T to When s is less than delta t to some beta, uh, so in that case, like delta t we expect it usually to be small, maybe not too small, but small, right? So, so here what we have is we have three terms. One is epsilon to the 2s, epsilon squared, epsilon to the 4s. And the worst would be epsilon to the 2s because s is between 0 and 1. So, in that case, we basically get an order epsilon to the 2s. And if you combine it. And if you combine it with this one, that's like delta t to the two or two delta t to the two s beta. Okay. And I have another question where you have these two options for computing epsilon like two graphs, a standard one and another one. Yeah, here. Is it on the left side? Is it both times better? Like you make epsilon less than delta t better and bigger. So I'm a little bit confused here. So, I'm a little bit confused here. Oh, so in our proof, what happens is, for example, if epsilon is relatively large, then we were not, if epsilon is relatively large, that basically means the diffusion approximation is not a good one. And in that case, like going along the red line seems to be not so good. But the main problem is actually here when epsilon is really small. If we want to do a direct computation, there are iterations. Computation, there are iteration coefficients. When you do a lot of iterations, it doesn't converge. So we were not able to get it converged. Thank you. Any other questions online or here in the audience? No? Okay. Thank you very much, Veron.