He's going to speak to us about causality, interventions, and counterfactuals in structural causal models. Go for it. All right, thanks. Yeah, so how this goes is my research initially focused on concurrency in pre-sheaf models a while back. But what happened is I discovered dependency relations in these models back then that felt like causal forms of dependency, as in the field of causal inference. So I felt the In the field of causal inference, so I felt the need to connect my work with causal models in that field. And well, so the main model of causal inference in the literature today is Julia Pearl's structural causal model, whose theory centralizes around interventions and counterfactuals. Yet a problem with this model is that it's a bit too specific and it doesn't seem to provide the right level of abstraction for the arsenal of causal relations that Pearl defines over them. And actually, it seems clear that this. And actually, it seems clear that these Causal models could benefit from generalization to the categorical setting and dependency structures on categories. So, this is what I'm working on, and so I will talk about that. So, to get that going, well, first of all, what do we mean by causality intuitively? So, causality is just the sense or attributed measure of responsibility of an event, a cause in the occurrence of another, an effect, or in between entities that drive those events. So, variables, for example. Those events, so variables, for example. That has a rather general connotation, so we expect that there are several ways in which we can provide meaning for that. But the most important point to remember is that ultimately what we really want is to understand how the rules of a system or nature operates. And the purpose of any form of causal discourse is really just to understand how the rules of a system force the occurrence of certain events under the occurrence of other events and how this happens all along a certain order, like time. All along a certain order, like time. And secondly, there are various degrees at which we can exploit causal inference. What I mean here is that causal models can be endowed with several layers of structure and causal relations that enrich the causal discourse, and they each add to the strength of that discourse. So as a basic example, the statement: the event of missing the bus this morning is the cause of me arriving late at work. So that's a statement that underlines several causal notions. Underlines several causal notions. So there's a sense of causal sufficiency. That is, if the event of missing the bus occurs, then I will arrive late at work. And it might just be that in this situation, we're actually thinking that it's causally necessary so that if I don't miss the bus, then most definitely I will be on time. And of course, we can't expect causal relations to just come down to double implications. So it's understood in this context that arriving late at work cannot be a cause of missing the bus. Cannot be a cause of missing the bus because it can only occur after the event of missing the bus. So there's this sense of order that's actually a primitive notion, or at least we should be given some guarantee that such a causal or order on events arises one way or another consistently. So that is, in general, causal relations appear as an attribute of some underlying universe of events ordered in the manner in which they occur. And I refer to these as evolving systems. So an evolving system. So, an evolving system is a collection of interconnected events or actions that basically describe how a system changes along the order of time. So, examples of that are Winscales event structures, Bayesian networks, quantum computer systems, Markov chains, structural causal models, register machines, Petri nets, etc. So, basically, anything in which we can get a sense of traces that unfold. And so, causality then manifests as various cause-effect relationships on events along. Cause-effect relationships on events along the order of such systems, where an effect is a result of or is influenced in an instrumental manner by the events that cause it. And actually, all of the above examples mentioned embody one or several forms of causal relations there. So the question is thus, what are these different types of cause-effect relationships and which ones are really central to general causal models? And how do we classify the strength of a model? And to begin addressing these questions properly, since the most intricate Properly, since the most intricate theories of causality have been developed in the field of causal inference and probability and statistics, we have to start there. And the first thing we should note is that there are various schools of thought that debate there as to what constitutes a proper foundation for causality, such as whether deterministic mechanisms are required or not at the base of causal models. So there isn't a general consensus about that. And though initially, causal models seem to have Causal models seem to have emphasized Bayesian networks and probabilistic distributions over graphs, satisfying such conditions as the Markov condition, along with two other conditions called minimality and faithfulness. But there were important cases of causal models where we had neither minimality nor faithfulness, so it was difficult to say whether these attributes were truly essential characteristics of causality. And now, newer trends of research in causal inference actually tend to focus more. In France, actually, they tend to focus more on models that can interpret Pearl's ladder of causation, where interventions and counterfactuals are used as a basis of causal discourse. So, Pearl's structural causal model, which I will talk about, is the main model of these aspects under investigation in the field of causal inference. So, what is this Perl's ladder of causation? How does it work? Well, Perl, again, like many others, initially emphasized aspects. Like many others, initially emphasized aspects of minimality and faithfulness in causal models, but his perspective seems to have changed over time and shifted towards interventions and counterfactuals as the more central notions. And he eventually proposed this hierarchy of causal models based on their ability to move beyond simple associations and correlations. And that's when we can give an interpretation of interventions. And then the highest form is with counterfactual reasoning. So the lowest type of causal discourse is when we make causal judgments. Is when we make causal judgments based on simple association. For example, so suppose there's a specific program on integer-valued registers that I run a few times, and there's this register R at time T1, where whenever I observe a certain value, say the number six, I then observe that same value in another register, S, later at time T2. So there are values in RT1. So the question is: are the values in RT1 causally responsible for those? Responsible for those of ST2 as our program runs? Well, not necessarily. And it is interventions that would let us know if this is the case. It's just an association. So the way interventions work is we come in the middle of a process, we force a value, and then see how things unfold after that. So we disturb the natural development of a process as it runs and observe what happens next. So for that same example, if I were to run the program again and the moment it produces a value six, And the moment it produces a value 6 in RT1, say, I come in and intervene and force a value of 2, and then I let the program continue its course. Will I observe the original value that ST2 would have had without the intervention or something else, like the one that I just forced in? So if I observe that intervention in RT1 changes the natural outcome in ST2, this means that the value of RT1 was indeed used in the computation of ST2's value. That is, RT1 is causally relevant. RT1 is causally relevant to the variable st2. Otherwise, it just means that RT1 and ST2 have a confounded dependency, a result of a common cause existing somewhere else in the program. And finally, counterfactuals in our causal discourse allow us to focus on a specific situation and ask if an event X in that situation was indeed the cause of an event Y. So, if X had not occurred in that situation, would Y have been different? So, for example, So, for example, suppose that I need to drink a lot of water and take a pill to put an end to my headache. And suppose that first I'm in a situation where I just drink a lot of water and I don't take the pill. So I observe that my headache is not gone. In that specific situation, if I had taken the pill, then I would have relieved my headache. So taking a pill is causally relevant to providing relief in this specific situation. However, if I find myself in a situation where I If I find myself in a situation where I did not drink water and I did not take a pill, then in that situation, if I had taken the pill, that would have changed nothing. That is, taking a pill is causally irrelevant to providing relief in that second situation. So the main feature of counterfactual is that they allow causal queries under specific situations and conditions for a system. And so this gives sort of the broad idea of what Pearl's ladder of causation is. So we can Ladder of causation is so we can sort of now take a look at the canonical model for interventions and counterfactuals that you propose. So, a structural causal model, and I will focus on the acyclic type here, just because it's a bit simpler. So, these consist of a simple directed acyclic graph with set of vertices x representing the variables of the system and a set of arrows A that we model as pairs of vertices following our assumption of simplicity, so no loops allowed. And an arrow xy represents the xy represents the potential of x to directly influence y. And given a vertex x, we can define its parents to be the set of vertices that have an arrow into x. And the background variables of the system are represented by vertices with no parents. And endogeneous variables are just the variables that are not background variables. So these are the ones that work internally to our system, and the background variables are the ones that are exogenous on the boundary. On the boundary. And then we have a family dx of non-empty finite sets indexed by the variables, where dx represents the domain of the variable x. And we can extend that. So for a subset of variables y of x, we can define a domain for sets of variables as the product of the domains of each variable in that set. And finally, we have a set of functions indexed by endogenous variables where each fv represents an internal local rule that determines an outcome in the Local rule that determines an outcome in the domain of V solely in terms of input on the domain of V's parents. So the outcome in V is determined by its parents. And so these essentially look like these networks of operations. And I have an example here where variables have integer domains, so not finite domains, but that's okay. And with three background variables u1 to u3, endogenous variables v1 to v3 determined by addition and multiplications, multiplications on pre- Multiplications on previous variables in the graph. So, just as a remark here, I just happened to put one multiplication with three inputs here. So, okay, fine. And so, given any input on background variables, we can fire up these operations concurrently to fill the graph with a global outcome. And so, this is the main thing in SCM in general, in structural causal models in general, for every input U on the background variables, you have this network that you can fire to properly. You have this network that you can fire to propagate outputs to create a unique global outcome X for the system. That is, we obtain a global section whose restriction to the background variables is the input value U, and restrictions on the endogeneous variables commute with the application of the local equations set up by the FV maps on the parent nodes. And then if we write just notation here, x tilde u for this unique outcome given input u. Outcome given input u, we get a goal global solution map from background variable input to global outcomes over our network. And also we get one for any subset of variables y by just restricting this global solution map. And we get these y solution maps in terms of background variables input. So for example, here, if I take the set of variables y that consists of v1 and v3, then the y solution map on the input given here just outputs a value. Given here, just outputs a value of 4 in V1 and a value of 7E2 in V3. And then we can also add probabilistic structure to an SCM. And the idea is that we simply add a distribution, say, P on the background variables, and then pairing the structural causal model with P forms a probabilistic causal model. And the idea is that there's a unique extension of P to all vertices as a joint distribution simply by pushing out along the solution map. That is, the solution map becomes a random. Is the solution map becomes a random variable, and the probability of an outcome x, say, globally is just the sum of all the probabilities of the background input that can produce x as a solution. So that defines the basic idea of an SCM and probabilistic SEM. So let us now see how interventions work in structural Causal models. So suppose we have a process on four zero, one valued variables, x, y, z, w, where we know that the outcomes on our variables are ordered in. We know that the outcomes on our variables are ordered in time with the following diagram. Suppose for this process, we observe global outcomes that are either all zero values half of the time or all one values for the other half. Then we might have distinct structural causal models to explain this situation. So for example, we could construct three processes that consist of three steps. The first two steps will be shared. So the step one, the background variable X receives a random input. Step two, Y and Z receive a copy of. two, y and z receive a copy of x's value, so we just copy the values of x. And finally, the third variable could be different for three processes. First process could simply copy the value of y onto w. Process two will let w receive a copy of z's value, and process three will let w receive the product of y and z's value. And then we can add a probability distribution on the background variable, say 50% probability of a zero input and 50% probability for a one input. Probability for a one input. And the important thing to notice here is that we have three entirely different processes that are completely indistinguishable under global outcomes. And in fact, it does not matter what probability distribution you put on the background variables here, we get indistinguishable global distributions. And that's an important fact that if you don't exploit that in your model properly, you get causally ambiguous systems. And that's why we had an order at Priory to make sense of that. Or at least we need to guarantee that it exists uniquely. Or at least we need to guarantee that it exists uniquely. And local structure really is the key here to telling these models apart and identifying causal relations properly. And in practice, we can expose this local structure by intervening in the middle of a process. So in process one, for example, where w receives a copy of y's value, interventions in y do have an impact on w on its response. So if we start with a zero value in x and we intervene with a value in y that matches the value the system would have produced by itself, then of course The system would have produced by itself, then of course, this changes nothing. But if we intervene by forcing a one value, that value gets copied into W and we see that Y is indeed causally relevant to W. Same thing if we force a zero value when input is one. On the other hand, interventions in Z have no impact on W. So regardless of the value we force in Z, the outcome in W is always the same, is the same as when no intervention is involved. No intervention is involved. So, Z is causally irrelevant to W in that case. And in this manner, interventions actually allow us to expose a relations on the variables of a system called causal relevance, and we get distinct causal diagrams as follows for the processes we identified. And so, basically, we can think of our causal model as sound if the exposed diagram of causal relevance with interventions, for example, has no loops. For example, has no loops, or otherwise we might question the orderliness of the causal influence on the variables. And so we can give a formal setting to interventions. It's not too complicated. So basically, whenever we have a structural causal model, we can form a new one. Whenever we have a set of endogenous variables y and any realization or outcome small y for those variables, the y intervention submodel m y of m is the structural causal model whose graph has the same vertices. Causal model whose graph has the same vertices, but remove all the arrows that go directly into a vertex in the set Y. And we also remove the associated F V with a vertex V in Y. So remove all of that. And then we set the domains to be the same for non-intervene variables. And we set intervene variable domains to be a singleton that consists of the intervention value at that variable. So these intervene variables become a background variable. But since domains are singlotons, we can basically Since domains are singlotons, we can basically ignore them when computing solution maps. And that is what matters: that all models and intervene submodels share a common set of background solutions, share a common set of background variables for their solution maps when disregarding singleton domain background variables. And this is an essential aspect for counterfactuals to work, actually. So, for example, with the STM here, if I do an intervention with a value of 3 in a variable v2, we remove all We remove all errors from parents of v2 into v2 in the corresponding multiplication operation, and we set v2 to a value of 3 in the background. And then the way counterfactuals work is that given any set of variables w, the potential response of, there's a w solution map in the y-intervention model, which we write w tilde y script, and it's called the potential response of w to the y-intervention. W to the y-intervention. And then, given input u and background variable, the solution w following the y-intervention on that input represents the counterfactual value that w would have had in situation u if y had been y. So this is how it works. And the idea of counterfactual as before is that we can observe a situation like this one here and ask if the v3 equals 72 response at the end would be different if v2 was in fact 3. V2 was in fact 3. And since actually, it would, since the potential response in V3 following this intervention changes the outcome in V3 to 36. So this means that the original event V2 was causally relevant to the outcome V3 equals 72 in this specific situation. But in another situation with a zero value in the background variable U2 here, the event V2 equals zero is not causally relevant to the outcome V3 equals zero at the end because the counterfactual At the end, because the counterfactual value here does not change based on any interventions in v2, so it's always the same. And so, counterfactuals give a case-by-case type of causal relevance by pivoting a situation around common background variables and running, letting the process run again from there with subsequent interventions. But these are just limited examples of the different forms of causal relations that exist in these models, and there are other important relations. And there are other important relations with probabilistic counterfactuals and actual causation that we could talk about, but this would require a bit more time to go over, so I'll leave that for another time. Suffice it to say that right now, what I'm doing is I'm experimenting a bit with different ways of providing categorical models for structural causal models. And my recent focus has been on giving a representation as pre-series on the freely generated category, CJ of the graph G of the model. And what you can do is you can try. Model. And what you can do is you can treat the vertices as a discrete category. And we can think of the variable domains simply as a discrete functor that can be extended to the category CG using the right-can extension. And so we get this functor Cf. Then there's an obvious topology on CG where parents of a nod form a cover. And what this comes down to is that you is this topology generated by the following sieves on the endogeneous nods of V, where we take all paths into V except the identity. And so we're looking. And so we're looking at the topology generated by these things. And F then satisfies a form of separatedness property with respect to this topology, in the sense that every matching family has at least one amalgamation. Yet, each local behavior of the function STM tells us that there's a specific way in which we can glue matching families on SV to a matching family on V. So anything that is not on the identity extends. That is not on the identity extends, and you can add this. You can add this value on top with the domain of V, where you apply the initial F V on it, and you get a new matching family for V. And so let's call that map bar F V here. And what it does is it provides sections of the restriction map the other way around. And so it's just this specific model that I'm experimenting with now. Experimenting with now. And it so happens that this categorical SCM representation of a structural causal model gives a notion of morphism for SCM models. And they're just natural transformations on F that preserve the gluing operations. And so we get a notion of isomorphism as well. And I think what I should point out is that structural causal models were not really studied as part of a category. So our first step was really to make sense of that. And now we have at least some idea of what morphisms could be like. Could be like. And in fact, each SCM gives rise to a categorical SCM in this way, in such a way that distinct structural causal models give non-isomorphic categorical SCMs. So the passage that we've constructed from structural causal models to categorical structural models in this manner doesn't lose information. And what it means is that it just justifies that a categorical analysis of structural causal models is possible. Causal models is possible. And so we just intend to investigate how notions of interventions and counterfactuals together with other causal relations can be promoted to the categorical setting in this sense. So yeah, that's basically what I've been working on for now. And yeah, thank you. Yeah, I don't know if I was right on time or was there more or less. More or less. I didn't mute you. Yep. So that's on time. Any questions for Simon? All right. Oh, yes. I like the applied nature. Yes, I like the applied nature of this talk, Simone. So I'll be watching the talk again. I didn't quite catch everything. Right. Sure. Yeah. Thanks. Yeah. All right. Well, in that case, let's next email again. Oh, sorry. Oh. Premature.