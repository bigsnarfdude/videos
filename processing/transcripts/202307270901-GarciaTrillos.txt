I'm motivated and pushed by things like this. I come to places like this, meetings like this, and I learn a lot from a lot of people, and then I try to do my research based on what I learned in settings like this. And in fact, so part of the talk, or half of the talk, has a story similar to what I'm saying. A couple of years ago, maybe two years ago, we were at Simon's in Mercury and it was a really great setting. Really great studying. Keating was one of the organizers. And it was about geometric methods. And I don't remember what was the other topic, but we were there for a semester and it was a fantastic place to get ideas and just bring together knowledge from very different people. So, in some sense, actually, the two papers that these talks are based on have stem from that time. So, I'm hopefully. So hopefully we'll find more connections and learn more things to keep doing my own research. Okay, so now what is this research that I'm interested in? So you've heard about success stories of AI, so in image recognition, natural language processing, data generation, cancer detection, et cetera, et cetera. So it's out there everywhere. And so there's also like this kind of very recent applications where, for example, you can say there's this app value or this web page value by OpenAI, the same guys from ChatGPT. You can just insert queries and then basically this thing is going to generate some image for you. So I actually inserted the query generate an image representing the success stories of AI and it produced it. So this thing is producing some imagery. Producing some imagery, some creative work. Okay, so creative in quotes because they can debate whether it is creative or not. But anyway, so it can produce things. So there's plenty of things that AI can do. You just have to watch the news to see this or just read anything pretty much these days. But of course, there's also, you know, like the widespread use of AI tools, there's also some concerns. And this is some concerns that maybe stem more from. Concerns that maybe stem more from kind of like our own humanity, right? We want this to be a tool for us, not something that controls us. And so, you know, you go to, for example, philosophy departments and law departments, and this one is University of House, and then talk about how are we going to regulate things? What are some of those values that we should impose to certify an AI system? So, philosophers and lawmakers are concerned about these things. And even very recently, And even very recently, this was last week, July 21, Biden somehow pressures, well, this is what the headline says here, pressures AI companies to put guardrails on this headline. And so he gives this speech. He talks about three values. He talks about safety, security, trust. And actually, I think this article is kind of interesting. It's also funny. Of course, Funny. Of course, my impression is these companies to agree on guardrails, but there is no discussion about how to enforce those guardrails, right? So at this moment, the seven companies, the seven biggest companies of AI are saying like, sure, we're going to take care of these things, but lawmakers are so far behind. And in this article, the last two paragraphs are just very funny how they describe what is the attitude of Congress in the US, for example, about this. They're completely clueless about what's going on. So, anyways, but this is something that is important, right? But this is something that is important, right? I mean, it's not so much about this being the president, or it is both about this guy being the president of the US and not about that. But the point is that at all levels of society, you realize that this is something that people are concerned about. What are some of the drawbacks that can come from AI? So let me then stop for a moment to discuss those values that are mentioned in that article and in that and of course. Kind of video of the person of the US. They talk about safety, security, and trust. So, just to give you some very brief examples of what these things can look like. So, safety could be because you want systems to be robust perturbations of data, and I will tell you more explicitly what I mean by that. Security could be just that your systems are vulnerable to the intervention of adversarial agents. I will also mention something about that. Net trust could be. That net trust could be, you know, like is your data private really? Are these AI systems making fair decisions, or is it like some sort of bias is guiding some of the decisions that these AI systems are making? And also, when you have objects or things that are generated by some of these systems, can you tell that they were generated by these systems and not, I don't know, maybe be portrayed as something green, for example? So, some sort of watermarks that can have. Some sort of watermarks that can help be transparent. This was generated by me. So, those are things that are important. So, let me just be more concrete about two very specific examples of that. How does this look like in some applications? So, let me tell you a little bit about adversarial machine learning. So, success story, well, neural networks, maybe you've heard a lot about those. They are supposed to be kind of the best tools that we have for. That we have for in many ways for classification, especially for in imaging, for example. They're being used everywhere. But the problem that they have, and this was kind of like recognized more or less at the same time when these systems became the dominant ones, they are very sensitive to adversarial attacks. So what that means in this supervised setting is that you could have data, in this case images with some labels, and you could have one of these systems trained on clean data. Systems trained on clean data, and you could achieve very high accuracy on these systems. But when you take inputs that are very small but crafted perturbations of the original clean data, you can basically manage to completely change the prediction that the AI system or the machine learning system makes. So, for example, this is a very famous image from kind of one of the papers that talked about these things. So, you input About these things. So, you input this much per panda, the neural network would classify this correctly with pretty high confidence given the number of classes that there are there. But then you add the tiny perturbation. It doesn't matter what this is. It's just the important thing is that it's a structured way of producing modifications of this original image. It's a small perturbation. Small in the sense that you cannot tell the difference. And yet, when you input the new image in the Input the new image in the neural network, now the neural network is very confident, extremely confident that this is something else, it's not a pattern. So, this is, of course, an issue, especially when you deploy these things. So, you could ask, well, is this something artificial? Or can those attacks arise in the real world? And so, maybe a few years later, they say, let's try to do this. You put some patches. These are physical patches on stop size. These are physical patches on stop signs, and then you wonder whether by placing these patches in a clever way, you could fool the system to think that that stop sign is not a stop sign, but say a 30 miles an hour sign or something like that. So, of course, if, I mean, part of the point in this paper that I mentioned here, but it's all by this guy and his collaborators is that the adversarial text can appear in real life, in the physical. In real life, you know, in the physical world, somehow. And so, and then you can do all sorts of things. What if you start putting some of these perturbations on physical objects like cars? And if you have a self-driven car and you're, well, if you have a self-driven car and the kind of image recognition system somehow is sus susceptible to these attacks, it could, for example, think that things that are supposed to be cars maybe are something else. And again, this is an advantage. And again, this is okay. So, here, of course, what I'm emphasizing is that this is something that one should definitely be concerned about. Are these systems robust to this type of things? Okay, so let me give you another example of things that are in the realm of making, putting guardrails in AI on machine learning systems. So, let's talk a little bit about federated learning. So, federated learning is this framework where instead of like Instead of, like, you know, do you, for example, you having your phone, and your phone, you collect your data with your phone, and somehow have a certain learning system trained on just your data. You somehow want to communicate with the world, take advantage of what others have learned. But one of the things that you would like to make sure is that your data somehow stays private. Maybe you don't want to reveal your data, and still you would like to learn from others, for example. To learn from others, for example. So, this is, maybe this example of phones could be maybe a bit irrelevant because at the end of the day, people share their images all the time. So it's not that they're thinking about keeping their images private. I mean, at least in a general sense. But for example, for hospitals, this is important. You don't want to disclose the data of your patients. By law, you cannot. And so you still want, as a hospital, if you want to make As a hospital, if you want to make prediction for diseases for your patients, you would like to use what other hospitals are doing without somehow revealing data. So you would like to have this interaction of agents, and now you can start seeing, at least in this setting, the connection with some sort of collective dynamics. You would like to have some interactions while protecting your data and somehow get benefited by that interaction. And benefited meaning that you want to make good predictions based on the data that you have. That you have. Sometimes there could be a central server mediating this interaction, sometimes it could be just completely decentralized. Okay, so that's the introduction. So that's the setting I'm interested in and those problems. I gave those two examples, but overall I think it's important to think about how to make machine learning ass. And of course, many people are thinking about this, but what I'll talk about today is how could you think about these problems for a machine learning? How can you think about these problems from a mathematical perspective and specifically from a case collective perspective? So that's kind of the goal for the two tasks that I mentioned before. Okay, so now we're going to be a little bit more specific about how we turn those rough ideas into math and what can we say in some sense. Okay, so there are a few words here. I'm going to try to make sense of all of them. So, you know, adversarial training in itself is Adversarial training in itself is a term, so I will define that, and eventually I will describe this kind of neural network settings, which maybe some of you know, maybe not all of you. So let's just get it started. So what would be adversarial training? So keep in mind that example of what you would like to do is train models that are robust to perturbations of data. So typically what you would do, if you don't care about being robust to perturbations of data, you would try to solve a minimization. You would try to solve a minimization problem like this. This data here would be the parameters of your learning model. This move here would be your data distribution. And this L is somehow some loss function that tells you how your learning model is performing at predicting for this input x, this output y. Okay, so this is what you would do if you don't care about robustness. But now, for robustness, one of the things that one can do is say, well, how about we imagine that Imagine that we anticipate that potentially there could be some adversary that can take a clean data point X and perturb it and change it to a new data point X tilde that is close enough. So there is some budget epsilon that the adversary can spend to modify the original point X and move it to a point X tilde. And we somehow would like to, and in this model at least, we would like to choose the best model that makes on average this worst case Worst case kind of effect of the attacker, the spouse. So the difference is we have this supremum over here, and geometrically, you can think about what that means. So in a two classes setting, well, before you would say if you have this data set with the colors here represent labels, what you want to build is a classifier that can match the colors of the data that you observe. So, of course. Data that you observe. So, of course, for the classifier that I have represented here, which is just the background color, you know, like the risk or the error would be just a mismatch between colors. This red point should have been, or this red point is a mismatch with the blue background, so you account for that, etc. But now, when you care about the robust risk, well, even if you correctly Even if you correctly predict colors, so this blue point agrees with the blue background, well, that point actually now contributes to the error because within the ball of radius epsilon, an adversary could move this blue point to the other side of the decision boundary and therefore achieve like a wrong prediction. So, then in that case, that blue point before was not penalized, now it's going to be penalized, let's say. So, you can see more or less here. Say. So you can see more or less here that there is something geometric going on here in terms of decision boundaries, because now you should care about how big your decision boundaries are. And this is somehow what the adversary would do. Now, I just wanted to mention this briefly, but that's not the kind of the perspective that I'm going to take today. So anyway, so this model was introduced by computer scientists, or I mean, written explicitly in this way by computer scientists at MIT. By computer scientists in MIT, so a person called last name Madri and his team, he has a big group. He also has like a nice kind of repository online where he keeps track of all algorithms that people use to train models for us. So I think they're doing a good job at somehow keeping track of what people are doing. So anyways, now, mathematically, you can generalize this formulation a little bit. You can say, well, instead of thinking about perturbation happening, Thinking about perturbation happening point by point, you can think about distribution of perturbation. So, what that means is you initially have a data distribution moo as before, but now you think that what the adversary can do is change the entire data distribution. So, take it from Mu into Mu tilde, as long as it stays within a certain budget. And of course, implicitly here we have a way to measure the discrepancy between these measures. So, you know, like this is again a general This is again a generalization of what we have there. And there's, of course, you can imagine there's other generalizations, like instead of putting an explicit constraint here, you can pose like a penalization term where you say now, well, the adversary can essentially do anything, but in order to do that, it would have to pay costs. And presumably, the farther it tries to perturb the data, the more expensive that perturbation should be. Okay, so different formulations. And of course, the natural questions are, you know, how do you do that? Of course, the natural questions are: you know, how do you find solutions to this problem? Written in this form, it looks more like a min-max game of two players, one learner, one adversary. And sometimes, like, if you're not interested in the solution of the specific problem that you have here, maybe at least you can try to get upper and lower bounds to get a sense of what's going on and maybe get some batch bars. You could also ask, well, when you do something like this, how is that related to explicit regularization? Related to explicit regularization methods. You try to impose generalization on your models explicitly as opposed to mediated by some adverse actors. So you can ask a bunch of different questions. Now, what I'm going to do, so this is basically my description of what is adversarial training. I will be working in these two settings, so the one with the explicit penalization here, but I will do it in a setting where my family of classifiers is very special. Is very special that will give us some structure that will also suggest some methods to try to solve this mean-minded scheme. Okay, and the strategy will be what you would naively do, which is some sort of gradient ascent descent in some sense. This is what you would try to do. Okay, so what is this family of models? And let me be more explicit about what I want to consider there. Okay, so here is kind of like a summary of my kind of like parameters for this. Kind of like parameters for this problem. So I'm going to be considering a regression problem that means my data is pairs features x and labels in the real line. So these are not hard classifiers. This is just like, you know, regression problem. Maybe one can generalize a few things, but let's just stick with this simple setting. And now the functions, the classifiers that I'm going to be interested in is what one would call Main field neural net. Okay, so a one-layer mean field neural net. So the idea is you have a function that takes an input x, but somehow this function is parameterized by a measure over parameters. So parameters here are pairs AB is going to be a scalar, B is going to be a vector in RT, and this H is going to be a non-inert. And so maybe just maybe just to write something to keep in mind. So we'll say one specific In mind, so to say, one specific example of that could be something that looks like this, AIH dot X, which is what one would represent as a neural network in this form, where the X is Where the x is input here, here are the linear functions b1, 2, n, let's say. Here is the nonlinearity. And then there is the linear mayor of at the end, a1 and 2, a n. Okay, so what I'm saying is that this is a generalization of having a neural network with capital N neurons and just one hidden link. So this is what people typically So, this is what people typically refer to as the mean field region for depth. And you can see, okay, here I'm looking at an empirical measure, but all I'm saying is, well, in general, just take an arbitrary measure on the space of parameters. Okay, finally, so with that definition of the space f, you know, the loss function is, well, it's a regression problem, so let's just do some basic square loss. So, you know, the f is going to predict something for this x tilde, and we just want to see the least match for this y tilde. Just want to see the least much with this white tail. That's the setting we're interested in. I also have to tell you what is the cost for the adversary. And again, for simplicity, let me just imagine that the cost that the adversary has to pay is it tries to move the data distribution mu into a new mu tilde, and in order to do that, it will have to pay the faster standards. So I'm putting a coefficient in front that will essentially say how much budget we're going to do. How much budget were given to the adversary? So, the smaller this CA is, the larger the budget. There's going to be a smaller cost. And if the CA is much larger, then that means that we're really forcing the adversary to move data very close by. So, this is the problem. Now, what is interesting about this formulation is that now, because of my parametrization of my learning functions in terms of Learning functions in terms of measures, now you can see that this is a game in the spaces of probability measures. So, this is maybe one of the nice things about this formulation. And now our game is just a game in the space of probability measures. One is over the, let's say, the data space, the other one is over the parameter space. And now, well, naively, what we would do is, well, let's just try to do some sort of gradient ascend-descend relative to some metric. And now, Some matrix. And now, the philosophy of what I will describe in this first part of the talk will also match the same philosophy in the second part of the talk, and also the philosophy that many people have been using in general when studying learning systems, especially in regimes where you're sending parameters to infinity or the data to infinity or something like that. The perspective is there's going to be some sort of algorithm that happens at the discrete level, finite level, finite dimensional level. You may not be able to. Level, you may not be able to say everything you would like to say for that system, but you can say, What if we scale up that system? Can we recover some sort of equation? And when we recover that equation, can we say some qualitative properties of that? Is that thing doing what I want it to do or not? Because you will see that philosophy appear here. Okay, now this is our problem again. You can rewrite it in this form. In this form over here. So now, notice that, you know, so when we look at this term, the blue is fixed. So I'm not changing the clean data, that is always fixed. So I can actually just use the Kantorovich trick to write the Buster Stand distance in terms of couplings and say I'm going to treat this problem as one where I actually have a minimization problem, still over measures, but now over product measures. Product measures. And this pie is essentially going to keep track of the coupling between the minimum material that I'm considering. I'm not going to be tracking whether it's optimal or not. I'm just going to say, let me just take all the couplings. And of course, I have to add the constraint that the first marginal has to be the clean data. So one can rewrite this problem in this form if we just take this view in that form over here. So now in this formulation, now I can say, well, let's just do some sort of grading asynthesis. Some sort of grading ascent. So I'm writing this equation here, this set of equations over here, and what is happening here is really I'm saying pi t is evolving by following the gradient ascent of the payoff function u. But then the metric that I'm using is some sort of Fischer-Rau metric. So it's a modification of the standard vasters and distance where you The standard Vasters and distance, where you allow mass to potentially decrease or increase. But I have to project. So this pi T has to live in this space, right? So it has to have the first marginal move. So I'm going to make sure that that constraint is satisfied. So then that is achieved by subtracting this term over here. And you can see that this pi T, I mean, one can do that computation. It's going to be the dispi T as Me that this pi T along the flow would respect the constraint that the first margin was moved. Here, this is the conditional distribution of, you can think of this as at the clean data point C, the adversary is thinking about attacking it and moving it to C tilde prime, and well, the adversary is giving some weight to that decision somehow. Likewise for the parameter space, now you're doing gradient descent of the payoff, but now, you know, okay, in this case, I don't have concerns. You know, in this case, I don't have marginal constraints. The only thing I have is the total mass constraint because I'm moving in the space of probability measures. So I have just to subtract the full average of this. Okay, so this u pi and u nu, as is the standard in when you're doing gradient flows in these spaces like Wasterstein or Fischer-Raw Wasterstein, this U pi and U nu are the first variations of U. And well, this is the way that those sentence-sent equations. Ascent-descent equations would look like. And okay, so this is what I was saying. So this is some sort of projected ascent-descent with respect to Fischer-Rau. And now what you can say is, well, that's what you would do in general if you could also think about, you know, from the perspective of particle dynamics. And this is, in fact, what you will do in practice. This is the thing that you have to evolve. What is the adversary want to do? Is the adversary want to do? So it's what is represented here is saying the clean data point C i is, well, this is going to be fixed, nothing is going to happen there, but it's going to propose to change to a collection of C tildes, and it's going to give some weight to those points. And likewise, for the neural network, it's going to choose some parameters and it's going to provide some of those points. Provide some of those ways. So, this has to do a little bit with the formulation that I have over here. So, you have a system of ODs. Do you mind going back two slides? Sorry, I just wanted to one more. Yeah. Okay, so just to make sure I'm following. New, always a probability measure. High, always a probability measure. If you're letting them evolve, you're doing it with respect to this phosphostype fishing route. But this is still conserving. But this is still conserving the fact that there are always probability measures. Yes, okay, good. Because you're subtracting this thing here and this thing. In fact, here there is that extra constraint that the first marginal is the clean date. But yes, so now the thing is that, so the reason why we can talk about particles is in the end because this Fischer-Rau-Wasserstein gradient flows, you have Lagrangian description, because essentially Fischer-Rau-Wassersten is nothing but some sort of lifted version. But some sort of lifted version that gives you the standard Busser distance in a space where you keep track of both location and mass, essentially. So anyway, so one can then ask, well, if we track this dynamics, these ODEs, and we track the empirical measures, is it possible to recover some sort of mean field behavior? And of course, under some And of course, under assumptions, and of course, one can, I mean, here we put the most general, the most, the simplest assumptions that would allow us to say something meaningful. So, you know, if you assume that the parameter space is bounded, the data space itself is bounded, you, of course, there's always going to be this condition about this initial positions of these our ecosystems have to be well prepared. And in fact, I will make emphasis in this what this means for our problem in a moment. Problem in a moment, then you would expect that as the number of particles goes to infinity, in our case, what that means is as the number of neurons in your neural network goes to infinity, and also as the number of somehow, as you kind of like explore the space of potential attacks also corroborates, then these dynamics will, you know, the particle dynamics will come. The particle dynamics will converge to some mean field limit, and that mean field limit would look pretty much the same, except that the initial conditions are not empirical measures, but now are going to be general measures that presumably are being approximated by the empirical measures at times here. For, yeah, so for at times here. Okay, so this is maybe not so, you know, I mean, a theorem like that you would expect, but Theorem like that, you would expect, but let me just stop for a moment in what it means to be well prepared in this setting. So, what it means to be well prepared in this setting is maybe not the standard kind of like vaster stand perturbation thing, at least not for the attacker, maybe for the learner is the same. This is what we're imposing, but for the attacker, you actually have to work with a different quantity to control everything, to essentially carry out the Carry out the standard arguments in this mean field here. So, the quantity that you care about is this sort of kind of noted optimal transport, which is some notion of comparison of, let's say, measures over a product space where you first align the marginals, and once you have aligned the marginals, you try to align the conditions. So, this would look something like this. The reason why we need Like this. The reason why we need, I mean, ultimately, one needs to work with this quantity is because, in our equations, if you remember, in the equations for the attacker, we are keeping track of conditional distributions. We want to preserve a certain marginal, and so the equations involve the conditional distributions themselves. And this is not a quantity that can be controlled by the Vasters and this, the opposite. So, the Vasters and distance of pi 0 and pi 0 can be controlled by this, but not vice versa. So, it's important to that your initial conditions are Conditions are well prepared in this set, not in a different set. Okay, so you can, of course, you can describe ways to initialize in that way. You can do some sort of random sampling, but you have to do random sampling kind of in a rectangular way. You cannot just do IID on the product. So you really need to be able to approximate conditional. So that means slightly different ways to initialize. To initialize. Okay, I'm going to skip this part. And so I was telling you the first story, right? So the first part of the philosophy is to say, you know, I cannot understand maybe for finite and what's going on. Let's just send them to infinity. And now once we are in the main field setting, what we ask is: if I follow those dynamics that I propose, are these things doing what I want them to do? In this case, what I want them to do is to hopefully reach. Is to hopefully reach some sort of Nash equilibrium or at least concentrate around something that is close to a Nash equilibrium. So, this is somehow what we say in this other result. So, we say, well, if you, and this strategy is standard, like you, you have to average your dynamics. There is no strong convexity or anything like that imposed here. So you have to do some sort of maybe averaging or pick the best kind of iteration. Kind of iteration in an interval, something like that. You cannot just take the dynamics. But the idea is that if you track this dynamics, you would expect that eventually you're close to a Nash equilibrium, or you enter a regime where essentially you are a delta inashi equilibrium, which would mean for the learner, which is the agent that maybe we're more interested in, it essentially is achieving, you know, or is constructing a model. Or is constructing a model that would be essentially robust to adversarial perturbations. Now, of course, you can imagine behind this there is some assumptions, some strong assumptions on initialization. In this setting, and as well as in other settings that I study in film of neural networks, you typically need to assume that your initial measures of distribution somehow have to be well spread out. And so once they Spread out. And so once you have well-spread kind of particles, then you would indeed enter into the region where you can get close to this nashvalu. So this is not unlike what she said about. For us, it's in both the attacker and the learner. In the standard settings without adversaries, it's just on the learner because there's simply no attacker. Now, I just want to mention very briefly one small thing: that is that in the case of Is that in the case of our payoff function like this? When you enforce that the adversary has actually a very small budget, so in other words, the CA is very large, you can actually get some sort of PL condition for the variable that the attacker has to maximize in this case. And what that means is essentially that you have gained strong concality. So you would expect that you can now. So you would expect that you can now relax, at least in that setting, when the address doesn't have a big budget, you would enter the regime where essentially you can initialize the attacker any way you want, and you would have some sort of convergence. So this is kind of like the second result that we have, under the assumption that the attacker doesn't have a very big budget, which is in any case reasonable. Remember, if you just want small perturbations of data. Small perturbations of data, you would see that these dynamics, regardless of what Pi CO is doing, if you modify them a little bit so that you could move faster in the variable where you have the strong complexity, you would get, again, a similar result, but now under no assumptions on the initializations of the attacker. Yes. So for the one with before you had the strong concavity, for the long time behavior, and maybe you were the You for the, I'm not sure if I'm remembering the right Shizabach paper that you're citing, but my vague memory of some of their results on the long-time behavior for these models of neural networks with a single hidden layer is that they can identify the long-time behavior, but somehow only under the assumption that as T goes to infinity, the gradient descent dynamic converges to something. Is there a similar, is this this, am I remembering right? Here I'm not saying that this is. Here, I'm not saying that this is converging. Well, I'm saying that after a long time, all the things that it's going to explore are Nashville after some time. Almost Nashville. Yes, exactly. Yeah. Thank you. Right, so there are some I just want to mention so that I can move on to the second task, which I mean, I have maybe, I still have enough time. But anyway, so I just want to mention that there is some interest in this type of problem. There are some two papers that. The problem. There are some two papers that I mentioned here that also start thinking about this CRSOM games, you know, players in spaces of measures. But the settings are different. I am in particular interested in the adversarial training problem. For example, in this paper, there is no adversary. It's more like a linear payoff setting. There is no description of what is the adversarial machine learning or anything like that. And the results are also much more limited. There is no gain of strong complexity, for example, that we do get given that. For example, that we do get given that we're restricting an attacker and so on. And then there's also this paper that, again, maybe it's in the same setting as this one, the results are stronger, but for example, the examples that are given for the adversarial model are just simply not really their setting, the way the different label problem is simply not able to capture what we would like to do. I could have shown you some numerical results that we have, but honestly, that wouldn't tell us much. It works. Much, it works, it works fine, you know, it can improve robustness, etc. But I don't want to, you know, maybe I want to focus on kind of the collective dynamics and so on. So let me skip that for now. Okay, so now I have time to tell you a little bit about the other task that I was mentioning in my beginning. So this is not going to be about adversarial training, this is going to be about federating learning. And in a specific set, that's something that is called cluster federated learning. So as before, Cluster federator. So, as before, I'm going to tell you what is the setting, what are all those words, what is CVO, if you are not familiar with, and I'll show you what we are doing, connecting these things. So, okay, so remember, this is the setting where you have users. And now users are actual users. These are agents that have their own data. And we're going to assume that we have capital N of them. And then we're assuming that agent I has a certain loss function that is essentially dictated by the data set. That is essentially dictated by the data set that agent I has. Okay, so of course, agent I wants to minimize this loss function. It wants to find the learning model that best predicts their data set at DI. So this is the setting. And now, so what could these agents do? So of course they could do essentially nothing, not talk to anybody, and just try to do local training. This is the standard thing that people would do. This is the standard thing that people would do. And those dynamics would look more or less like this: some sort of gradient descent, plus some sort of roughly speaking, this is what an individual agent would do. Now, there is an algorithm, very famous, one of those papers that have tens of thousands of citations. They propose federated averaging. So, what is federated averaging? I don't think that for you this would be very surprising. What you would do is, well, instead of taking the gradient, Taking the gradient, each agent taking the gradient at the individual point, what they do is they take the gradient at kind of some sort of average point, and that average is essentially just taking the combination of all the parameters of all the agents. And this is what you do. So now, of course, this can be mediated by a server, in the sense that each agent could be sending theta ti to the server. The server averages and then just propagates. The server averages and then just propagates in that way. Or it could even be decentralized. Like, if everybody can talk to everybody, then each agent individually could be computing this as well. This is also possible. One thing that I would observe is that if you think about the privacy of this mechanism, so in this case, the agents, the only thing that they are sharing is their parameters. And you could say, well, but And you could say, well, but is that potentially dangerous in the sense that could someone try to use those parameters to recover the data? People have tried, like, you know, numerically, how difficult it is that inversion problem, like from parameters, especially if your model is kind of sophisticated, trying to recover the data. And in fact, it's difficult. Something that is easy is if agents were sharing their gradients, that would be super easy. And in fact, people have inverse. And in fact, people have inversion attacks, so that you imagine that you provide to some malicious agent, I don't know why they would do that, but suppose that you provide to a malicious agent a bunch of gradients from your computations, the attacker could essentially invert and recover your data points, especially for some imaging tasks. This can be shown that it's possible. But my observation is that this doesn't have a problem because, again, the only thing that is needed. Problem because again, the only thing that is needed is parameters, and parameters are much more difficult to invert. Okay, now one of the issues that FedAverage has is that it probably is not going to perform very well when agents are heterogeneous. So what I mean by that is you can see that if all the distributions, if all distributions of all the agents were the same, then essentially you are aggregating gradients. Creating gradients, and those gradients would have essentially more and more data points. So, essentially, everybody would totally be benefited from communication with each other. So, that definitely is the case. But if for some reason the data sets are quite different, it's not clear that talking to everybody and taking like a full average is going to benefit me. In fact, it could send me into regions that I do not want to explore. So, then, of course, you can say, well, what would be a realistic setting? Be a realistic setting. So, this is what I want to discuss now. So, this cluster federal learning. So, cluster federal learning is a setting where you still have agents, but now we are imagining that these agents are kind of clustered in groups. So, this is, of course, a very simplifying assumption, but the idea is, suppose that there are k clusters, k much, much smaller than n. Think of it as space, whereas n may be being very, very large. And now, what we're assuming is that all agents that All agents that belong to the same cluster are going to have the same data distribution. So, in reality, it's not going to be exactly like that. You would imagine that maybe people have different data sets, but what I'm trying to say is that they're roughly the same data sets, roughly the same loss functions, is what I'm saying here. That's a simplifying assumption, but I think is reasonable, or at least much more reasonable, than the previous segment. Okay, so again, there are agents. Agents that share things with others. Now, of course, we want to have a private protocol where I don't know who the agents that are like me. And I can interact with everybody, perhaps, but I don't know who are like me. So ideally, I want to talk to those out there who are more similar to me than to those who are not too similar to me, at least for the purposes of minimizing my objective function. So let me tell you one. Let me tell you one very popular algorithm to achieve that. Let me tell you some of the drawbacks, and then I'll tell you what we do. So, you could consider like a slight modification of the federated averaging setting, which works more or less like this. So, imagine that there is, I'm going to state it assuming that there is a server. So, there's a server. The server needs to know how many clusters are out there, which may be a little bit unrealistic, but making A little bit unrealistic, but making some science is okay. But in this framework, the server knows that there are two types, in this case, orange phones and blue phones, for example. So then the server is going to propose two values of parameters. It's going to send both of them to all the agents. The agents are essentially going to say they're going to evaluate the performance of these parameters with their own data set. Then there's Set. Then they're going to choose the parameter that best achieves a better value of their objective function. And they're going to update that parameter to obtain a new value theta j tilde. And that agent is going to return to the server two pieces of information. One, the updated parameter. And two, which one of the two parameters that received was the one that gave him or her a Him or her a better loss function. That's what we need share. Then the server averages, separates the blues from the oranges, averages, and repeats. So you can see that this is reasonable, it's kind of an extension of federated averaging, but somehow agents are providing the information which cluster I belong to, more or less, based on their own loss function. So, okay, so this is what one does, and some dropbox, of course, is that. Dropbox, of course, is that the server needs to know the number of clusters to be able to propose this different parameters. And if you think about it, there is also some sort of privacy violation because you have to state I am blue, I am orange. This is needed in this framework. And also, of course, well, there is not so much theory about what happens with this method. So, what do we do? So, what we do is say, well, let's just try to take care of. Do we say, well, let's just try to take care at least of some of those drops. So we take inspiration in something that was introduced not so long ago that is called consensus-based optimization. So in consensus-based optimization, the goal is to say, I want to do global optimization of some loss function that could be pretty rough. This was the motivation. Potentially also a setting where it's expensive to compute the grade. So this was the initial motivation for these methods. And then what people do. And then what people do is say, well, let me fix a few parameters, lambda, sigma, and alpha. And what I'm going to consider is just pouring over the kind of a search space a bunch of particles, and I'm going to let them interact following these rules over here. Essentially, every particle is, you know, it has some drift term, it's trying to align to some consensus point, and that consensus point is some weighted average of the location of everybody else. The location of everybody else, but those weights are determined by the value of the objective function at each of those points. So, the idea, you can see in this formula, this of course is resemblant to Laplace's method. The idea is that points that have a smaller value of the loss function are going to receive more weights. And so, the idea is that the particles are trying to align to, you know, they're trying to align with other points that achieve. Align with other points that achieve better values of the objective function. And of course, you have to control and play with those parameters to be able to achieve consensus and somehow hope that that consensus is reached at the global solution of your objective function. This was the motivation that people had. And this was the original paper. And since then, there's been a lot of different settings where people have either extended this framework or studied this practice. Or study this predator. So, okay, so from mean fields to just the behavior in time of the mean field, also different settings, like you can generalize this from global optimization to, say, some sort of multi-objective optimization, even sampling, et cetera, et cetera. And you can see also that all these papers are relatively recent, so this is something that is being developed quite a lot. What do we do? We say, well, maybe. We do? We say, well, maybe this system has some nice features that actually we can exploit for further APLs. So, this is our method, so we call it FedCDO. And let me just describe what it's doing for the case where there's only two classes, two types of agents. Of course, the generalization will be completely straightforward, so with two is enough. So, it's again an interacting particle system. In principle, I'm splitting into agents of time. Splitting into agents of type I, agents of type J, but in fact, there is, you know, it doesn't matter. The algorithm will not know this. Presumably, each agent will be doing, computing these things on their own local device. And this is the important part. So you can see it looks a lot similar except for maybe these two terms. You know, you can ignore this gradient term here if you don't like to think about them. Everything else is the same. The important The same, the important observation is that here, this averaging is, of course, if you could just do the averaging over agents that are of the same type, that would be great. But you cannot know that. That's the point. We don't know who are blue, who are orange. And so then you have to take averages over the entire collection of parameters. You don't distinguish between them. The main difference between what particles of type 1 and particles of type 2 will be doing is that when Type 2 will be doing is that when they look at the way that they give weights to these parameters, they will consider different loss functions. Those of type 1 will be using their loss function, L1, to evaluate which parameters are good, which parameters are bad for them. And those agents of type 2 will be doing the same, but with their own loss function. So in this way, they don't need to know what are the parameters that are coming to them. They can determine that by themselves. They can determine that by themselves with their own loss function. They don't need to know any other information. So, in a sense, it's very similar to the CVO system, but again, the framework, the structure allows us to at least propose something like this. So now we try to consider this philosophy that I was saying before. What happens when we scale up the number of particles? So, essentially, what we're saying, what is the proportion of particles of type 1, you know, Of type 1, you know, converges to all W1. The same thing for particles of type 2. You initialize your parameters in some, you know, this is maybe not so relevant to say everybody initializes uniformly independently. And so, you know, you would expect, and there are some assumptions that I don't want to enter into details of, you know, loss functions, but some things that are quite reasonable, you would have a mean q limit. In this case, we're writing it in this form. We're writing it in this form essentially in the Levi-Prokorov metric for each uniformly in time for a fixed time interval. Your each of the kind of empirical measures for type 1 and type 2 particles converge to corresponding type 1, type 2 distributions. And those things, of course, will have some description. What is the mean field? Well, it's just the law. Well, this is just the law of some mean field SDE that looks pretty much like this. Each of these guys is like a representative particle for type 1 and type 2. And so again, here, the averaging is happening over the entire distribution row team, or just the individual ones. And the difference is that particles of type 1 do L1, the other ones do L2. And then there is a second particle. You have the mean field. Is this thing doing what you're supposed to do? This thing is doing what you're supposed to do? So, again, and this is just almost borrows the literature of CBO that is out there. We can say, like, if we initialize correctly, so at least you have to give some chance to explore the global minimizer, just even a tiny bit would be fine. You can tune the parameters so that at least within some time interval that you can essentially determine, you would reach essentially those. That essentially those distributions would be concentrating in terms of kind of this square error around the global solutions of their corresponding plus functions. So, what this is saying is that by interaction with this FedCBO system, particles will start converging towards their corresponding global minimizer without knowing that when they were interacting with other agents, which ones were blue, which ones were orange. Like all of them naturally converge. Like, naturally converge to each other. Notice also that I don't need to assume what is the number of clusters. You can run the algorithm, and in principle, if there were 10 clusters, in principle, you would find those 10 clusters. I don't need to specify that. And again, the only thing that is shared is the parameter. So then this would not be susceptible to inversion attacks, or at least the risk is pretty small. At least empirically, what it is. Small, at least empirically, it's not that sick. This is the algorithm. I just want to mention: you know, when we, when you do, maybe you have not seen this example, this is the reason why I'm presenting it here. When you want to test this federated learning settings, this is the MNIS of federated learning, which is rotated MNIS. So you have your data set consists of four types of data sets. So it's MLIS, MNIS rotated by 90 degrees, MNIS by 180, and M is by 270. Those are the four classes, let's say. The four classes, let's say. So, in this example, you would have a bunch of agents that have MNIST data, a bunch of agents that have 90-degree MNIST data, etc. So, you test with these things, and one can compare with the other methods, the ones that do not consider any clustering. You can see that those ones are not as good as the ones that do consider clustering. We are essentially the same. I wouldn't highlight that this is extremely better than this. Extremely better than this. But the point that I would highlight is the qualitative properties that these guys have: that you don't need to know the number of clusters, that you don't need to reveal your identity, and that somehow there is the. And well, definitely all of them are much better than if everybody was doing their thing on their own. Yeah, so just one minute to wrap up this last part. So, you know, one big open question is: can one show that FedCVO probably? That FedCBO probably outperforms local trade. So that means that, you know, can one say that there is kind of data aggregation in a sense, like that it's much better to communicate. We see that numerically, but is there a theorem that one can prove where one can achieve that? And it's not so easy because what is being exchanged is parameters. And we don't have kind of this linear structure where we would see easily that, oh, yeah, you're just doing data aggregation. That in itself is a very big and not so easy. Very big and not so easy questions. Can one better capture the effect of communication? Because I didn't emphasize too much on this. I have to be very transparent here. In fact, this is one of the main things and concerns in fairy learning, how expensive it is for people to be exchanging things. And of course, when one compares methods, one has to make sure that the cost is kind of the same. This is the variable that has to stay fixed to compare which method is bad, or one of the variables that has to be. Or one of the variables that has to be kind of fixed. How to modify the system to make it robust to backdoor attacks? What that means is this is assuming that all agents are behaving in good faith, that they're not sending trash, but they could be sending trash. And if there is time later, I can describe what are some of the settings that we are interested in in this context. And well, the other thing is related to furnace, like by interaction, you could ask the question, You could ask the question: What if some agents were enforcing some fairness constraints? What this means really is that they're trying to solve some optimization problems with some constraints, but maybe some other agents are not. And somehow, you know, maybe this is a little bit realistic, but still, the question could be, is there some sort of way in which you can propagate those constraints? So, if not everybody is following a certain policy, can you correct for that in a kind of like private way as much as possible? A lot of open questions. Well, a lot of open questions. Thank you so much for your attention. Thank you for the presentation. Are there comments, questions? So in the first half of your talk, you were you introduced this function like C mu mu tilde, the cost. In the specific case that you looked at was the cost of standard. Looked at was the loss of standard. Are there other reasonable choices of cost to look at there? Or have you thought about other types of costs? So for example, yeah, so first of all, like that cost makes sense when you have maybe regression because you can perturb the y variable. I mean, that's kind of like what the model allows. But I think it's more reasonable for, I mean, the adversarial training framework, the The adversarial training framework, the very original one. So I guess I have to go a little bit less. But let me just show you. The very, very first formulation implicitly is assuming that, so these are data pairs X, Y, but somehow the adversary cannot touch the Y. So that means that Y is fixed. So what that means is, for example, cost that you could consider. So, costs that you could consider are costs that only penalize the x-coordinate and they don't allow for y. Like W infinity on the X marginal, which is a thing that could. That could be, but maybe that's just like a locality for X, but I mean that it would cost infinite amounts for the adversary to change a label, for example. But then you can relax that. And this goes to what we have been discussing, like that. What if you have a mechanism that tells you what? Mechanism that tells you: well, I mean, it's not so expensive to go from a guitar class to violin class.