Yeah. Yeah. Okay, great. I wish I could be there. But I think he had some single wall. So it's my great pleasure to present our original work to this group. It's an SRTY semester. And I'll switch the introductions. Here's basically the two-stage transformer association study. And this is And this is the formula that we read on for the first stage of training the transfer communication model and the second stage of pressing the association between the chemotype and the genetically regulated transpression component. And here, this is the formulas that's been widely used to cover the TWAS testing by using the TWAS summary data. And as you can see, basically, when the gene Basically, when the gene-based association testing was doing, it's doing a weight-tape, these four tests. And these are the GWAS summary statistics and the dot views for the EGPR infrastructures estimated from the gene traction perfection model. So I would just echo Harpy's presentation that the information in the team wants to result is really not due to the error. The error is, and is more due to the LP, and for the genes that we tested. When we have the linear 5 genes, we test the scopes, and those are testing those genes, and those will cause inflation. And also, when we have the highly coordinated expression, those will cause inflation. And what we see with the test statistic is just a waiting of this word statistic. So, the topic that I want to present today is: for now, we have a multiple reference panel, like the JTAS and the other study code for finding the on your C data. We have multiple t-shirt tabs, and then we have multiple statistical models for training the gene expression into data models. And right here, we have the RAS map and the G-text all profiled at gene expression. All profiles actually threshold data for the brain prefunctual cortex. And the GTAS has more than 48 human tissues. And they exist in multiple t-block tools like the Codescan, Filter, Tiber, and all other methods all use different statistical models to fit the gene spectrum prediction model. And the approach that we want to propose is how we can deploy all of those. We can then reach all those gene special imputation models in 20 using different reference panels, different tissue panel, and different distance models. And I think a code that we can do is just do an element of the E2TL weights, and that would be an asset of the E2TL weights for the P1. And what we want to propose is the aspect of version. In fact, in our example, based on an example, machine learning method is just text or question. And what you ask for is our validation data, that need individual level expression data, and use this data to learn the best ways to aggregate the base models. And by this approach, we have the advantages of increasing improvement on Increasing training sample sizes and also improve the accurate prediction accuracy, and then increase the TWAS power. So then here is the input that all we need is a train base modules. Those can be trained by any statistical methods, like the pretty scan, it's a typo tool, across multiple tissues. Tissues and the idea is we need to solve for a set of optimal rates. Here it deals by the data pay, and we need to maximize our direct prediction offering in the validation data set. So here is the E as the expression or a G in the validation data set. And in the previous head, this is the imputed expression by each test model. by each base model. And the data k is a weight for each base model. And we are averaging to this weighted average of the base model prediction. And we are minimizing the loss, the L2 loss here in the validation data. And we subject to the sum of the ways equal to 1. After we solve all those data, then we have a set of optimal ways to combine. More ways to combining the base models, and we just do the weighting of the EQTL X sizes of the base model and take this W theoda as our EQTL weights, as the next level weights that will be used in the stage 2 association testing. And so, here's how we conducted our issue: how we can best use the validation data. And the model. Data. And the model we train for is particularly for the teaching of the validation data because we are maximizing the R-squared in the validation data. So to basically leverage the validation data, we just take the simple average of code. So we do take an average of the E2 calves are in the base models that trend from the individual data and the E2 calibrates that we are. To tell the rate that we learned from the statue function. And turn out this out range of the base validation model and the S-ball regression estimate gives us the best cover. And we showed that by simulation studies, and the simulation study, we considered two focus. We simulated from the genotype, the real genotype data from the ROSMAP in the GPAC version 8. Version 8, and we simulated the validation data and test data. And here, the validation data and the test data are all simulated onto the same model as the loss and method training. And it becomes the scenario where the two cores of the QPL, the mixture of the reference cohort, have only had 50% of overlap. And we are considering the variates are total proportion of the two quality of. Proportion of the two quality, our various range of the expression heritability, our various range of the genotype heritability. And we assume the total expression heritability is the same for the two training forwards. So here we can the validation test and data assumption of the same model as used for the training roadsman paper. RoxNap paper, and you will see essentially you're going to see the best performance for the RoxNap-based model. And we do one-sided repeated conditions for the power, and we compare the performance to each of the individual-based models. And the usual approach are to like average the scatter regression and the average of the validation-based model and the scatter regression model. Scaffold ratio model. And here we just show the estimated weights for the models because in the simulation study, you only consider two base models, so there's only one set of the data. And this is for different scenarios. Different proportions of the portal are seen on the robes, on the columns, and the different expression are held abilities of the rows here. Of the rows here. And here is a forecast. You consider three different scenarios. One is when the credibility is incording to a reference panel and 50% overlap of two portal schemes, and the same are two portal schemes, and only half of the herdability in the GPACS are known. And you can see from the And you can see from the data estimates. So the data estimate to be at the outpost to 1, this is the data estimate, but you can look at the ROSNAP model. So you'll see when the expression of probability is high, the proportion of the cortisol, the ROSNAP has been selected more and more is a higher rate close to one. And for some Close to one, and for some scenarios, we see balanced contributions from two-base models. And when we compared the fact for cross-validation hardware in the validation data between the SRTOS models and the Naive models, and we'll see the SRTOS models to have improved the prediction of our square. And similarly, when you look at the power, and when you look at the power, let's see the purple one. The purple one is the average of the validation base model and just that progression model, which means it has the highest power across all scenarios. Here's the highest prediction operator and here's the highest power across all scenarios. We also did the long simulations and we showed that. And we showed that all individual-based models and the nine-board approach, our status model and average model, they all control for the PAP1 arrow pretty well. And in our real application studies, and we first did our navigation study to show the prediction accuracy in a real test data. What we did is we considered three, big two real data. Two real data-based models. We split the real map data into half of the validation data and half of the test data. And we compared the median R-squared, mean R-squared, and the number of genes with R-squared greater than 0.5%. And the last one is the SRTOS. And if we participate, each of the individual models. Each of the individual models and just one with the naive approach. In our real application studies of the algorithm, we consider four-based models. Two trained from each of the reference panels. The GPEX pre-found the cortex tissue, and we trained two models back to statistical methods. To statistical measure, the predicted scan and the paper, and the symbol for the ROS map. And here we consider a validation data in the supplemental motor arrow. It's still in the brain, but it's a different tissue from the frontal cortex. And we look at the G1 state file of the 2021 version of the AD summary data. So this is the results, summary results of the individual base models and it's a sample size, median cross-degation R-squared, mean cross-degenation R-squared, number of genes, with cross-regated population R-squared greater than 0.5%. And here is the two population-based models, and this work is the and this one is the stat professional models you see that the highest median cross-validation prospect and the last one is average of the validation based models and they saw our regression model which gave out testing the number of tested genes had a cross-validation of square and 1007 and when we look at the data to be considered four Four base models now. So, this is the data, it's the one of the data for each of the base models. And we can see for different genes, yeah, like different tissues and different oscillator methods could contribute most to the gene. And we look at the magneto plot of the GWAS of AB. This is the five or four individual phase models. Individual base models, and here is the two of the validation-based models. And this one is the SRQA result, and the last one is the average of the validation-based models, and the SRQ was all regression models. And this one helps us to have the highest power also in the real data, it identifies the most the number of independent significant genes. Secondary pensions. And we look at the list of independent stagnatories. And we look at their FRQ disorders. And here at the individual, this is the validation base model results. And then you can see the average validation. The average validation process gives us the highest power here. And we also look at the genes and most of the results are in our related. We also applied to the parking cell disease. In the parking cell disease, we considered base models from six different tissues in the GPAT. Tissues in the GPEX core. They're all from the GPEX reference panel, but there are five brain tissues and one whole blood tissue. We include the whole blood tissues because they have the largest sample size. And we consider the magnification data from the GPAT4. And this is a tissue known to be relevant to the Parkinson's disease. And we use the most Use most of the recent GMOS summary data for the Partisan data. And we looked at the overview of the train models and this is the base models, the sample sizes, median cross-validation R-squared, mean cross-validation R-squared, number of regimes, with cross-validation RSQ with 10.5%. We still see basically those two is the validation. The first two is the validation models, and this is the SR regression models. And we see the static regression models here at the highest median CDR square, not mean R square. And we then look at the combined, the average of the validation-based models and the SDR models. And here is the data estimated weights are being estimated at more than quarter trees. I told all the genes and for each of the tissue, each of the base models, the tissues, and the data, and we can see they all contributed to the final separation model. And we look at the RTOS by each of the individual tissues, and using the language and RS models, the spatial aggression models, and then the last. Models, and then the last one is the average validation-based model and the step progression models. And then also find the most map-purpose independent teams by the average validation and the separate lateral models. And we looked, this is a list of the independent genes that we curated on the results. And we also found they are like related. they are like related to the neurodegeneration disorders or ALT related. So to summarize, for the advantages, we can never reach base models, trainer reads, like multiple reference panels. We don't need to have individual level J expression data and all we need is train the models by our devices, by any statistical microservice. By any statistical masters and from our any teachers, whether they are relevant or not relevant, it will be fine because we are learning the weight for all those basic groups. And the step for regression are the optimal prediction models based on the validation data. So the results should be interpreted specifically for the tissue of the validation data. Validation data. And we found the average validation-based model and the standard pressure model gives us the best prediction accuracy and the highest CTWS power. And for the predictions, we do ask for an additional variation expression data. This has to be individual level data. And the GRX prediction accuracy and T1 power would be determined by the choice of the aggregation data set. As that. And last week, I just want to share our tour. It's available on the Data Hub, and our manuscript is Docrint available on this iPad, and it's accepted by the internet communications. That's just going to acknowledge the first counselor with the PhD students at Emory, Randy and Dr. Michael Edstein, who has been helping the project, and our And our data was posted, and the company was hosted. And thank you all for being here, being here, hearing my time for a couple of questions. Okay, so one quick question I have for Gigi is that, so right now you are using a validation set to combine the ideas of multiple references. I was just wondering whether there are sort of basic fronts where you can automatically infer those weights or use a more general sort of assumption on state effects that need to bring you to sort of bypass this validation standard, whether that's feasible at all. Sorry, sorry, your voice is not the better creator. I didn't hear your question very well. Okay, so we want to know where's the election should I face? So, yeah, I was just wondering whether. So yeah, I was just wondering whether there are like basic inversion stuff you can think of where so right now the main limitation as you mentioned is validation data. So because the validation data you have to split the training data into a training, extra training and validation, which somewhat limits a power. So if you take like Bayesian type approach, if you infer the ways to benefit from these different models, or use a more flexible like effect size assumptions for effect size expression, whether that would allow you to bypass the validation data to Validation data to so that it can maximize the sample size and get trained data for its music. Yeah, I think limitation, I think the limitation is not asking for validation. It's more asking for an individual level validation data. If we can use that summary-level data, like also for the validation data, then we have Valentination data, then they have plenty of choice that is testing for any diseases, there will be a relevant issue which has some level expected UPR data available. But because for now, the stack of regression asking for individual level validation data, but that would be a limitation. I think we don't need to split. Don't need to split the validation data because it can demonstrate other existing train models from different resources and in the end because we have the average of the validation based model. So we don't lose, actually we don't lose much of the information after we take the average. So we still take the base model. Here, that take the base module to prompt the navigation. Wait, thanks. And I think there's one question from Eric. Yeah, Eric, thanks for the fantastic talk. So I was wondering, what happens if, for example, in the scenario where one of your trained models has SISIQTLs, and then the second trained model has transient QTLs. Can you talk through that scenario for us? So I think since the trans-CQTL versus CISEQTL. Yeah, CisEQTL-based model. Trans-CQTL-based model. What happens under that scene then? Yeah, yeah. I think it's a great question. And so I think it generalizes to the trends or each of the models. Times our issue here of bad models as well. And I think it is that when that can be considered united in our religious, we don't limit it. It doesn't have to be limited to the same states. It can be for the same, it can be both city trans. And it's being, like what we're doing is attributing the models. If you have a modular transition to the model or part of the cities and part of the cis and the transit. CIS and the tracking cupidal models. I think they would do the same way of aggro regime. Okay, great. Thanks. Oh, another question. Have a question, please. Oh, yeah, yeah, sure. Yeah. Think any other question for Ging before we sort of close the recording here? So then we'll just close the recording and stop recording and and stop recording and