Okay, thank you very much, Samni, for the introduction. Can people at the back of the room hear me? I know usually my voice is not very loud. It's okay. Okay. So I'm very happy to visit Oaxaca again. It's my second time here. It's a very beautiful place. Actually, it's my second time invited by Fabrice and Sammy. Sami. So I'm very happy to see all of you in person after so many COVID years. So today I'll talk about some recent work with Vang Joon Yuan, who was my postdoc in Ottawa last year and who is currently postdoc at University of Luxembourg with Mark Kodolski. So let's start. So I will start with the introduction and then And then the main result is quantitative central limit theorem. And finally, I will try to give some ingredients for the proof. So the model that we considered is a hyperbolic counterpart of the model that we have seen in the morning in Davar's talk. So that was for the heat equation. So here the difference is that we have a second order derivative in time. A second-order derivative in time. And moreover, we are not considering a sigma function applied to u, it's just the linear function sigma of u equal to u. And another difference compared to that model is that the noise that we consider depends only on the space variable, not on time. Initial conditions are very simple. So at time zero, the solution is one and the velocity is zero. It is possible to It is possible to consider a more complex model, but we wanted to focus on this simple situation. Now, how do we define a solution for something like that? So this is a Walsh approach, the random field approach to SPDEs. So basically, the solution is a stochastic process, which depends on time and on space. So here we consider only dimension one for the space. And it will satisfy this integral equation, which is Integral equation, which is derived basically by analogy with Duhamel's principle. So, this is again something that was mentioned in Davar's talk in the morning. So, basically, if we are replacing this component u of tx w dot x by a function, and we are looking at this formulation, what we will have here would be exactly the convolution between the fundamental solution g of the wave operator with that function. And in our case, our function is more complex. It involves the solution and also the noise. We will have to give a precise meaning for this stochastic integral. So this is something that I will discuss later. First of all, I want to remind you what is the fundamental solution of the wave equation in dimension one on the entire domain. So this is a very simple function. It's something which depends only on the indicator of the ball. The indicator of the ball. So here we have actually the interval around the origin. Its Fourier transform is the sign, and this plays an important role in a lot of calculations. A few words about the noise. So the noise is something that we will call a rough noise. It's basically arising from fractional Brownian motion of index h. We are considering the case when h is less than a half. The case when h is less than a half. The reason this is called rough is basically when h is one half, this is Brownian motion. And as Celine was mentioning in the morning, the fractional Brownian motion has a modification which has paths of Holder continuity less than H. So basically, if H is less than a half, we have paths which are rougher than Brownian paths. More precisely, the noise is basically a collection. Is basically a collection of, so it's a Gaussian process indexed by test functions. And we have the covariance, which is expressed in this spectral form. So here we are looking at Fourier transform of a function phi, Fourier transform of a function xi with a conjugate, and this Reed's kernel, which appears in the integral. We will not use this form of the inner product, so the covariance of the noise. So, the covariance of the noise, we will use something else which is called Galiado representation. And this is something very useful because it allows us to go back in the original space domains of spatial variables instead of working in the spectral domain. So this formula involves now the difference or the increment of the function phi at time at the space location x and y. And we have this. This kernel x minus y power 2h minus 2. And if we are just focusing on w of the indicator of the box 0x, then this is a fractional Brownian motion. First question is, can we prove that this equation has a solution? It turns out that the answer is yes if h is greater than one quarter. Basically, the existence of the solution is proved by iteration. This gives us By iteration. This gives us also the so-called chaos decomposition of the solution. So we are looking at this series. So I n is a multiple integral of W. We have this kernel Fn, which is given by this representation. So we have a chain product of the functions G, and they are integrated on the simplex. And moreover, we have a way of controlling the moments, at least very roughly. So we have an upper bound for the P moment. So they grow. The p-moment, so they grow exponentially, and here we have the correct power for the exponent of t in time. And one of the open problems that you might want to consider is to look at the exact asymptotic. So instead of the limb soup, to see if we can prove something about the limit. But this would be something which is very different than what I'm going to talk about today. Yes, yes. So I can say that that's something that. Sorry, I keep going forward. I should go backwards. Yeah, so this is something that I don't think it's solved in the literature. Nobody looked at something like that. But it's interesting because it tells us a lot about the behavior of the solution, the exact asymptotic. Okay, so the main result is something about a different property of the solution. So, here, this is a problem which appeared first time, I guess, in a preprint in 2018. It was, I think, in the fall of 2018. So, there is this very important paper by Juan Niualart and Vita Sari. And the idea is we noticed that the Is we notice that the solution is a stationary process in the space variable and can also be proved that it's ergodic. So, therefore, by the ergodic theorem, if we are looking at the spatial integral of the solution centered, of course, I forgot to say that the mean of u of dx is one because of our initial condition, and we normalize by one over two r, which is the length of this interval here, this quantity will converge almost surely to zero. So, we have some information about the So we have some information about the behavior of this integral when r is very large. And the next question was: what about the central limit theorem? Is this something that we can also prove? And this is a question that, as I mentioned, was addressed for the first time in this important paper for the parabolic Andersor model driven by space-time white noise. So we had the same question for our noise and Noise and we are able to prove something more. So, not only that we have convergence to the normal distribution, so here Z is the standard normal random variable, but we are able to say how fast it converges in the total variation distance. So, just a quick reminder, the total variation distance is defined in this way. So, if you are comparing this ratio with a standard normal distribution in total variation distance, this will be less than. This will be less than r to the minus one half. So, this was our goal from the beginning to see if we can prove something like that. And the rest of the talk will be to tell you a little bit about how we achieved this goal. H in between one quarter and one half. But for H larger than one half, is it? Uh, for age larger than one-half, we prove it before, and that wasn't so hard. Yeah, so we have another paper for the regular case, yes, yeah. Okay, so the idea is to use Maliavin calculus. I wasn't sure if a lot of the people here will be familiar with Maliaving calculus, so I have prepared several slides about this. So, we are extending this process to an isonormal Gaussian process. Process to an isonormal Gaussian process. So, this would be indexed by a Hilber space. And just a brief review: so, we have the Wiener-Caus expansion. So, every random variable which has a second moment and it's measured with respect to the W can be written as this series. Moreover, we can calculate the variance. Another object that we use is the Ornstein-Wollenbeck semigroup. So, this is the TTF and the property of the Ornstein-Wollenbeck. Of the Orange-Timolenbeck semi-group, which is called hypercontractivity. So, this is a semigroup of contractions on L2, but we can actually control the pth moment. So, the Q of T moment of T T of F is bounded by the moment of F with a proper choice of Q. Maliya-Vin derivatives. So, if we have this type of cow. If we have this type of chaos expansion with a kernel fn symmetric, then we can define the Malayalam derivative in this way. And the next thing would be the divergence operator. So we would say that process u of x belongs to the domain of the divergence if this series converges in L2 of omega. So maybe I should go a little faster over these things. Turns out that delta is a joint of D, and we have this duality relationship. Have this duality relationship. If h is one-half, fractional Brownian motion is just Brownian motion, and delta coincides with a scorah integral. So now back to our problem in terms of total variation distance, there is this fundamental estimate of Stein. So if you have any integrable random variable and we want to see how far it is from a standard normal, then we have this important inequality. And in our case, And in our case, we are trying to connect this with Maliven calculus. So, more precisely, if this sorry. So, if you are now looking at the function which is actually the score of the integral or the divergence operator of u, and this function happens. And this function happens to have variance one, then we can calculate exactly this moment which appears here using Maliavin derivative. So that is using duality and chain rule. And therefore, using Cauchy-Schwartz, we have this new estimate for the total variation distance. Moreover, we can take U to be of this form where the L inverse is the pseudo generator of the Oriststein-Ullenbeck process. So a lot of new. So, a lot of new concepts that I'm introducing here. Second-order Poincaré inequality. So, this is an inequality due to Anafidotto 2020. So, basically, continuing on the lines from the previous slide, if we are looking now at this random variable f prime, which is defined in this way, the classical Poincaré inequality allows us to estimate the variance of F prime using this quantity. This quantity, and then from here we can go a little bit farther, and this makes us use the second Malayvin derivative, which is denoted by D2. So the application for our problem, so returning again to how do we estimate this total variation distance. So in our particular case, implementing this procedure takes us to Takes us to this new quantity. And if we look at this more carefully, we see that we have to find some upper bound for the increments of Maliavin derivatives and also some rectangular increments of the second Maliyvin derivative. And everything has to be integrated in a proper manner. And this result appeared in an earlier paper by Enuel Arshia and Zenk. And moreover, we can push this spatial integral inside. So, Maliavin derivative, the fourth moment of the Maliavin derivative of F can be estimated using the Maliavin derivative of our solution. And similarly for the rectangular increments, so basically the task which remains is to see if we can control these increments for d and for d square. And for d square. So all of this was pretty much standard, what I was explaining so far. So this is where the real work begins. So the first problem that we had was that we had to tackle moments of order four. And this is not so easy unless you look at this. Look at this moment comparison. So, let me try to explain what's happening. So, this is a result which I have stated here in a much more general context. It's valid for noise, which can be time-dependent. It can be colored in time, in space, it can be white, it doesn't matter. So let's look at this and the operator L can be, in fact, any second-order pseudo-differential operator. So, if we denote by u the solution of this equation, and we put this square. equation and we put this square root theta in front of u with arbitrary initial condition, then it can be proved that the pth moment of u theta is actually bounded by the second moment of the solution of a new equation which has a new parameter p minus 1 theta. And this is a consequence of the hypercontractivity of the Orange Dein-Willemette process. It's something which was proved by Koale in 2016. And a key fact for his proof was For his proof, was this observation. So he noticed that if you apply the Orange-Anwell of X-M group to U, theta, basically you get another U, but with a rescaled value of the parameter. And then in our case, this allowed us to basically control the moments of any order of the difference of Malayavin derivatives. So here we are going from moments of order P to moments of order 2. To moments of order 2. And the small price to pay is instead of having the original theta, we are now looking at the solution with p minus 1 theta. So remember, our goal was just to treat the equation with theta equal to 1, but now we introduce this theta artificially, and second moments are much easier to handle than higher moments. So that was the first tool. The second tool is a Kaus expansion of the Malievin derivative. So here we see. So here we started with the random variable uθ, which has this chaos decomposition. So remember, this is the multiple integral of order n. The fn are the kernels, so they are very explicit. And we can calculate the Maliavin derivative simply by shifting the integral. So now it's of order n minus one. And then this point z appears also in the kernel, but we have to make sure that we are using the symmetrization. And we have an And we have an extra factor of n. So the symmetrization of f tilde can be written further using some additional kernels. So these are the fjn. So all together, we obtain a very explicit expression for the Malayevin derivative of the solution. And in fact, these kernels that we have, Fjn are not so bad. So we can write them using the original kernels and something new, which is here denoted by. Is here denoted by G. And the G has an expression which is now an integral on a different simplex starting from R and going all the way to T. So this G is in fact related to a different equation. So this is a hyperbolic Anderson model with delta initial condition. So we are trying to understand that. So it turns out that if we are focusing on initial conditions Focusing on initial condition zero. So at time zero, the wave displacement is just zero, and the velocity is given by the Dirac delta Z. Then the solution of this equation will have a chaos expansion, which will start with a Gt minus R. And then from that point on, it will continue adding multiple integrals. And the kernels that we have are precisely the kernels Gn from the previous slide. Slide. Now, unfortunately, we were not able to prove, and I believe this is not true, that this integral of order n minus one can be written as this type of product. So something like that is true in the case of the white noise in time, but not in our case. Nevertheless, we are still able to exploit this connection between Maliavan derivative of u and the solution v, and this played a very important role. Important role. So, the increments of the Malayavin derivative of U, putting together all these ingredients that I mentioned before, they are estimated by this integral from 0 to T of this quantity I1. And if we examine closely this quantity, it's a quantity which contains increments of U at evaluated at a new parameter eta. So, eta is 4. So eta is 4p minus 1 theta. And then we also have the increments of v, but this time they are increments in terms of the z parameter. So that would be the solution with initial velocity z. And here we have the solution with initial velocity z plus z prime. So the increment in that sense. And then there are the other two terms which are easy to control. So that would be the first bound. So remember, we need bounds of this form for. We need bounds of this form for our total variation distance estimate. And the goal would be to show that something like that is smaller than a constant times r to the power minus one half. So we don't see the r here yet, but it is going to appear. And I didn't want to make this slide too complicated, but there is a similar inequality for the rectangular increments. For the rectangular increments. So remember, we had to look not only at this type of increments, but also those which contain four terms. And in that case, we encounter rectangular increments of v and a different parameter. So that would be a 6p minus 1 t. In terms of increments of u, there is this type of result. So when we integrate the increment of u against this The increment of u against this kernel, we obtain something which is uniformly bounded. Another property was some translation invariance. So remember, I was telling you at the beginning that this process is strictly stationary in the spatial variable. So not only that, but we have more stationarity properties. For instance, for V, if we are looking at here, we are looking at an increment in the Here we are looking at an increment in the Z variable. It turns out that Z can be eliminated. If you're looking at an increment in the X variable, it turns out that we will have to subtract the Z. So all of these properties are proved using chaos expansions. And they are important when we start to do the calculations. And I think this will be probably the last slide. What we managed to show was. We managed to show was finally something about the increments of v in various forms. So, if you look at the first line, so here we have the increment in the z variable. We integrate in x, we take the square all together, and then we take a supremo in z. So, something like that is uniformly bounded. We can do something similar where we now integrate dx. We also have the rectangle. We also have the rectangular type of increments, and there are three more inequalities of this format where we are replacing the soup in z by a soup in x and the dx integral by the dz integral. And the way to prove something like that was basically very interesting. We couldn't prove it directly. We discover a connection between this model and a similar model, hyperbolic Anderson model, which had which is driven by Which is driven by rough noise in space, but white in time. So we had to introduce the temporal component. And there is a comparison between the two solutions. And moreover, the model with time, noise, which is white in time, it was a little bit easier to handle. So that was sort of the idea. So all of these ingredients. So, all of these ingredients went into the proof, and these are the references. Thank you. At the beginning, you put the query transform of the kernel, which is this sign. Yes. Yes. This is the reproducing kernel of a Hilbert space. Is it relevant? We didn't use that property. No. No, we just used this for... Actually, we didn't use the Fourier transform a lot. We use it for the existence, but not for the CLT. So basically we have two ways of calculating the H norm. So this would be in terms of Fourier transform. So, this would be in terms of Fourier transform and the Gagliardo representation. So, for the majority of the result, we use this part, not the Fourier transform. The Fourier transform, I think, we only use it for the existence right here. So we calculated, so we showed that this series converges using the Fourier transform. Yeah, but not the reproducing kernel Hilbert space. Thanks. Right, I think there are more questions. You said you had to use the solution, I mean, yeah, the solution for a white noise design. Yes. Yeah, we had a connection with that. And then the type of increments that we needed, it turned out they were easier to control for the white. Easier to control for the white noising time. Now, the white noising time has other problems on its own. So, we had another project which involves more people, and this is work in progress, the white in time. Same idea. So, QCLT for that model. So, hyperbolic underson model. So, there are some steps for which it's easy to have. Yeah, there are some steps, and then there are some steps which we don't know how to do for the white noise in time. To do for the white noise in time. Why is it called hyperbolic? I guess it's because of the second derivative in time. Yeah. I've heard some people saying that it's not a good name, but we kept using it. I built my career basically in the last few years on this name, so I wouldn't be happy if we have to eliminate it altogether. Okay, yeah, it's by analogy with a parabolic Anderson model. Yeah, yes, that's right. Is there any intuition that 2H plus 2 over 2H plus 4? No, we don't have any intuition where these powers come from, but we suspect that they are the right powers. The reason I'm saying that is to prove something like. To prove something like that, we use something very basic which held work for other models. And in the case of those models, using this very elementary type of calculation gave the right powers. So I suspect here too. But it's a lot more work that is required to get the exact symbolic. Even the lower bound, like the limb inf, it would require new ideas. And we don't have them on cut. And we don't have name and cuts as representations of this. I mean, there could be some, but not for this time-independent model, for the time-dependent. Well, five minutes we have. 