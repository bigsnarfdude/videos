distribution of resources and how to find any free distributions on the substances. But the and possibly uh the most related talk we've seen so far is the first talk in the morning today. But the formulation of this result is inspired in some results in combinatorial geometry. And that's something that maybe doesn't show up in the archive paper, so I want to tell you a bit about those, about variations of Feli and Keratodori, and how from those How from those the formulation of these results comes about, and then some ideas behind the proof. So that's mostly the plan for this talk. So let me start with the following theorem. And this is a formulation of the colorful Hayley theorem. Okay? So I will say that in terms of that given D finite families Finite families of convex sets in RD if each has any infusion. So I'll think of each family as a different color class: the blue sets, the red sets, the green sets, and so on. Red sets, the green sets, and so on. So every time I pick all the sets of one color, they don't have a point in common. And then we can pick one set from each family such that we have again an empty intersection. Anti-intersection in the family which is. So usually we see the Yutoboro ground. If every colorful family has a non-empty intersection, then one monochromatic family has a non-intersection. I want the monochromatic form to be in the conditions here. So if all the families turn out to be the same family, we just get Heli theorem. And so we have sort of the history here. Helli from 1921, 1924, 1913, depending on the count. And then there's Colorful Helli, which is by Lovas in 1982. And then there have been extensions and variations of this one, right? One of the biggest ones, I would say, is the one from Kalai. Kalai and Meshulan from 2005, where they did quite a few things. They got a topological colorful heading. But I won't focus on the topological part here, I'll focus on the part that they changed the coloring condition by an arbitrary matrix, and somehow the independent sets of this matrix told you how the sets intersected, and you wanted to get a conclusion with a set of large rank that had edge intersection. Frank that had edge intersection. So, this is sort of a big jump from what I want is to get something that's sort of between this one and this one. Between this one, oh yes, plus two. Thank you. And what I want to see is what happens if we have more finite families of convex sets like this. Okay? So what I want to do is, suppose I give you now n finite families of convex sets. Finite families of convex cells here in n in this t plus one. And of course, if this happens, then we can just pick t plus one of them, apply column for Heli. And if we just increase the size of our column for Hamilton, it will still have M interception, right? So if this is easier, maybe we can ask a bit less from our families here. And instead of asking that each of them has M-interception, what I'm going to ask is that every n minus d of them have m intersection. So if I take any n minus d colors and get all of them together, then they have an m intersection. So if I give you more sets, it's easier to get an X intersection. And so we have more fabulous, but we have a weak condition, but we still get this condition. So this is sort of what I would call a sports colourful Sports colourful helicopter. And it sort of lies between this two here. And this one is fine. Well, that's not such a big surprise here. If you wanted to go ahead and prove this one, it turns out you don't have to do much work. And I won't do this now, but you just take Lois' proof. And with essentially minimal, like almost doing nothing, just applying this idea, you get this here. But the main moral is that if I increase the size, the number of colors, I can weaken the conditions here. Something like this also happens with the colorful characteristic. So the colorful characteristics is that given So the colourful characteristics are given D plus 1 families of points in all D, if each of them capture the origin. And again, I'm going to think of them as a blue set, a red set, a green set, and so on. And then we can We can find a colorful set that captures the edge. Captures when the convex? Exactly. So by capturing, I mean the origin isn't but the convex holo isn't. Captures the edge. And so again, we can wonder: what if I give you more colour classes here? So this is by So, this is by Inverbaria from 1982. If we now have n classes here, and each of them captures the origin, then I can just pick D plus one of them, apply colorful clatorid. And if I pick a colorful set, I pick that is plus one topple, and I can just add text, right? So I can weaken the condition that each captures the origin. And here it will be that the union of any n minus d captures your t. And so this also follows mostly directly from the original perfect colour criteria. And that's what is a bit funny. You can get away with a plasma here. A plus one here. So think of the case when n is equal to d plus one. It tells you not that you needed every single set, a single colour class to capture the origin. It was sufficient that the union of any two set colors captured the origin. And the version worked with n equals d plus 1, that was proven independently by Arocia, Braccio, Barron, Fabila, and Motjano, and by Holmes and Pach et al. Pach at all. But if you have more families, then you can get away with something like this. Again, you don't have to do much here, a bit more than with this one, but here you can use essentially the same proof that Arocho-Racho Parani Fabina Motehama used. Of paramilitarian handler used, and you get this result. But sort of the moral of the story so far is if I give you a colorful result and I give you more color classes, then you can specify this one. You weaken the conditions by choosing something like this. And also in that one, you have the the version by Cratodori, the colourful version by Baron, you have this sparse colorful, and you also have a matrix version by Holmesen. Majoring version by Holmes and I think 2016. So, sort of the same scope of things happens here. So, going from here, what I was really looking for is a theorem where we have a color proversion in which we try to sparsify it this way, but we actually have to do some work and not get it free from preparing. Okay? Questions so far? So, let's then jump into the work of cake cutting and fair distributions. And this is where. Distributions. And this is where you actually get interesting results of this thing. Cake partitions. And this is not me coming up with a foreign name. People in the area just call this family of cake partition products. The idea is: I'm going to have the cake, which I think I've Cake, which I'll think of as a rectangle. And suppose I'm going to get k people coming to a party, I want to divide the cake into k pieces and give each of them one piece. Okay? So I'm only going to allow you to do vertical cuts in this cake. Now suppose that k is equal to 3, so I'm going to do there's no zoo. Oh I don't know. Did it go off for the last stroke as well? Did it go off with the last stroke as well? It's working for the last minute, I think. It's on? It goes on the last uh last time. No, I'll let me try it right now. You're not supposed to touch this top there a button, isn't it? A monitor. Hopefully. Okay, well how you go? How are monitoring now? But again, it's just for speakers to see themselves. The Zoom probably isn't working anyway. He was on the Zoom. He said no. Oh, the Zoom is not working. No, no, it isn't. It's working. It's working. Okay. Sorry about that, Dad. So I will. So, I will suppose I have three guests, I'm going to do two cuts and try to give each of them a piece here. So, what we want is to find conditions on the preferences of these people so that we can do an MV-free distribution. If I make a cutting and I ask a person which piece do you prefer, they will tell me I want maybe the first one, maybe the second one, maybe the third one, and I want to be able to distribute so that everyone gets their favorite piece. Their favorite piece. They are allowed to select more than one piece as their favorite, but I want to give one to each of them so that each receives their favorite piece. So given k guests, can we find an ND fend ND free rotation? Rotation among them. Okay, so we're going to divide it into pieces. And so the bulk of the work in this area is to find the conditions on the preference of our guests so that we can guarantee something like this. This was first, the first result of this type was proven by Stramquist and Boodle. And Boodel independently, only 1981. And Lendercy resolved by KL in 1984 with weaker conditions on the preference of our guests, which is the one we'll use here. So what conditions do we need on our guest? We need, the first one is that the preferences are closed. So if I give you So, if I give you a sequence of partitions, so I'm going to be subdividing this, and suppose that I give you a sequence of partitions which is converging, meaning that the first cut converges, the sequence of the second cut converges, and so on. And every time you told me that you wanted the second piece, then in the limit you also want the second piece. You don't get to change your mind in the last one. You might, in the limit, like the second and the third piece, but you still like this one. Okay? So we get this. This function. Okay? So we get this condition here. And the second condition is that for any partition and any guest, they want at least one IT piece. So, our guests for hungry. This is what we call the hunger guest condition. Okay, so whatever. If I make the first cut here, the second cut here, I have two non-empty pieces. The middle one would be squished. So, you would prefer either the left one or the right one. So, we're not allowed to, they're not allowed to prefer empty pieces. And with those two preferences, you can show that you can always find in every frequency. Show that you can always find in a different distribution. Okay? So now, what if we get more guests than before? What if we have n return guests? So I'm a lousy host. I'm only going to divide the cake between k of my invites, okay? Even if we have one. And if k of them were hungry, I can just divide the cake between them. So as I'm a lousy host, So, as I am a lousy host, I'm also allowed to have petty guests who will reject cake completely. If I propose this partition, maybe there's a broccoli here and a broccoli here, and they say, you know what, I don't want any of those pieces. So the question is, what type of conditions can I impose on my guests so that I can guarantee an end-to-free distribution with k of those end guests? And so this is the term here. Suppose that for any partition of the cake and any n minus k plus one guests at least one. At least one once and an empty piece. The preferences are also going to be closed. Then I can find k guess and a partition so I can find an NV3 partition about them. Okay? Then we have. That's the same thing as K plus 1, guess 1, and that. This is the same thing as what? I'm saying it's among n minus k there is at least one which has an MTPs. It's the same in total the number of guests who want n MTPs is at least k equals k. At least k. But as we're changing the partition we just don't know which k are going to want a piece. No, it's slightly different for the partition. And for good reason, right? Because if I get n minus k plus 1 guess who never wants it, then trans I might only have. Cake, then trans I might only have k times one people interested. If I cut the cake into k pieces, one of them will not be liked by any of those people. I have to give one of the pieces to one of the heads and not. Then we can find an NB3. I'm assuming that we're always cutting into A pieces. So I want to tell you a bit. I want to tell you a bit the main tools used in this, in the proof of this. And in this one, we actually have to do some additional work or whatever. So what is the usual way we approach cake counting problems? The first thing we do is we parametrize this place of partitions. So let's look at our plate here. We can then off by x1, the length of the first piece, by x2, the length of the second piece, and so on, up to xk. And so we get k numbers. xi is at least 0 and the sum of the xi's Of the xi's is 1. We have k numbers here. And this is really parameterizing a simplex, right? So the space of partitions, I can view it as a k minus 1 dimensional simplex. Okay, so let's analyze what does the preference of a harmon guest actually mean. Of a humble guest actually means. If I pick any point in the simplex, this corresponds to a partition, and so our guest is going to prefer one actually the vertices here correspond to, say, 1, 0, 0, 0, 1, 0, 0, 0, 1. So this is a partition where the left part is the whole cake and the other two are next zero. The middle part is the whole cake. The right part is the whole cake, right? The right part is a whole thing, right? So if I ask one of the persons what do they want, they will tell me, well, maybe I want the green part, maybe I want the second part here. And so for each point they're going to give me a color, it's going to be a coloring of the simplex with K color, right? And here I am forced to want this color, here I'm forced to want This colour here, I'm forced to what? This, and I'm forced to this. So, what we'll get basically is a cover of the simplex with k sets. So, if this corresponds to port 1, port 2, and port 3, then I can then obviously satisfy. A1, A2, and to AK. A1 is the set of points where you prefer the first piece, we prefer the second piece, and so on. So the conditions translate to AI is closed. The union of the AIs offers the simplex. And then the hungry, so this is meaning that the set is hungry. Not wanting empty pieces is sometimes written as slightly differently. What I'll use here is that the intersection, or AI intersection FI, this is the facet opposite to the vertex I, is empty. So we never want a T that is empty. Usually you have slightly different conditions. But results are equivalent with those conditions too. The results are recorded with this condition scale, so I won't worry about that too much. This is what we call a KKM concept. And just as with KPSRM, there's a history of interesting results about KPM covers, which translate to these things here. In 1929, Nastur, Glatowski, and Nasr Kiewicz proved that If A1 up to AK is a KKM cover, then the intersection of the AIs is not empty. So if you're a hungry guess, there's always going to be at least one point where you don't care which piece you get, you're happy with that. 1984, Dale choose what we call a colorful paper. Choose what we call a colorful KKM theorem, which now is basically that you can always find average partitions this way. In terms of KKM covers, this tells me that given K different KKM covers, so the preferences of the first person There exists a fermentation such that the intersection of A I times I is not empty. So this is telling me who So, this is telling me who gets the first piece, who gets the second piece, who gets the third piece, and so on. This is what we call the colorful KKM theorem. So, this is the one we want to sportsify and actually get our results here. And so now, if a person doesn't want cake, I mean, let's look at this person here. And what they want is, so this is. So this is play the cards. So if we analyze this preferences, this is basically a person that will only want cake if they get 60% of the cake. Okay? They are really, really hungry. If you offer them less than 60% of the cake, it's not worth their time. And this. Will maybe be a person that is not something that they only want. Diet, they only want a relatively small piece, but they will not have a piece if someone gets too much cake. If someone gets more than 70% of the cake, they say, Yeah, you know what? This is something I won't participate. So near all this is a KKM cover of a sequence. But if we put one on top of the other, then of course we do get one. And this is sort of what we wanted when we asked for these two people to For these two people, do any of you want a piece of the cake? Maybe they won't agree every time who wants a piece, but together they behave as one hungry person. Okay? Actually, this is the main result here. So we want false KPM covers of our syntax. So we're going to get n families of sets A11, A1K up to A and 1 up to A and K. Okay? So what do I want here? So for a set of indices in N, In n with at least n minus k plus 1 indices, what I want is I take the union of the ai1 with i and i k i and i. So this is now the family of sets. So, this is now the family of sets, like when we put everything together. We want this to be a KKM corporate. So the take condition is if I give you a family like this, we should be able to find something like this, right? We should be able to give each piece to a different person and have non-net intersection. So if I give you a A for scale cover, what we want to find, we want an objective function from k to n, telling us who gets which piece, such that the intersection of a i I of i from i equals 1 up to k is not active. So this would be the spores color for KKN theorem that's been inspired by this. I have about three minutes to give you the main ideas behind the proof. So how do we go ahead and prove something like this? So let's look at Gale's original proof and what goes in there. So we have, say, the preference of three people, three KKM colours. What he did was to pick a point X and to make a matrix, a K by K matrix. It's going to have a column for each person and it's going to have a row for each column. Okay? So what I write here is, so second row, first column, in this person, the second colour is say this. I'm going to put in the distance between x And the complement of A12. So he was doing it for open KKM covers. I'm going to not notice for the moment that we took this. And so you populate the matrix this way. So each column has at least one non-zero entry because each of those is a KKM cover. And then we normalize columns. Columns to sum to one. So I divide each column by its sum, so we have a nice stochastic matrix. So there are two things that come into play here. One is for some x, mx is doubly stochastic. This is where the topology comes in. The topology comes in. So as I'm moving this point x, this matrix is changing, and at some point the rows have the same sum. And so we have a double-stochastic matrix because every time the columns all add it to one. And then we use Birkhoff's theorem for doubly stochastic matrices. Well, finishing. Well, finishing just one minute. Meaning, so if I give you a double stochastic matrix, I can find A entries, one image, not repeating growth and not repeating columns, which are positive. And if this is positive, it means that the distance to the complement was positive, so it was in that column. And so this is the distribution we want. So what happens if we do this? So, what happens if we do this with this X here? We're going to have n people, so the natural thing is to make a k by n matrix. We're going to have more columns and the same. We're going to have more columns and the same number of rows. Now, the issue I have here is I cannot normalize as I was doing before, because if I have a point that's in the complement of each of the colors, so this person didn't want k here, I have zeros in that column, and so I cannot divide by something to make the sum equal to one. Okay? So instead of normalizing as before, if I have here y1 up to yk, we're going to divide, so we're going. We're going to divide, so we're going to introduce an epsilon bigger than 0 and to replace yi by 1 over the maximum of epsilon and the sum of the yi's by json. So don't take incessant y i. So if we can normalize uh dividing by something sm uh bigger than capsula, we do that. But if we would need to divide by something smaller, we just divide by capsula, okay? Just divide by epsilon. And so this is going to be our matrix M x epsilon. So for every value of epsilon, we can use the same arguments as before to find a matrix where the sum of the rows is the same. So in this case, we have a matrix n x epsilon etc. And then we take epsilon to 0. So this sequence of matrices, we can assume we got loss of. sequence of matrices we can assume we got the loss of charge that it converges and this is going to be the partition that actually works. We have to be a bit more worked to make sure that this is actually what we want. We need a slightly different version of Birkhoff'sorem which turns out to be a consequence of Birkhoff's theorem so it's not that surprising. But this is the matrix that we have to do to change things. And since over time I think this works out. What's the topological argument? So if I look at the sums of the rows and divide by that sum, that's a function from the simplex, because it depends on text, to the simplex. And the conditions on the preference tells you that this function sends faces to faces. So the boundary is homotopy to the identity. So the boundary is homotopic to the identity, and so it has to be surjective, and at some point you get the barycenter of a sign. It's the exact thing that happens here, so that, but if you don't do this, you lose the continuity, and this is why the strict is important. And there's a lot, like in this setting, a lot of equivalences between the open case and the closed case, or this version of PKF, where you don't hit the opposite facet and the usual one. And the usual one, those are easy to do. In this one, you have to revisit them without the org. But it's basically a degree organized on public. We will continue at 5:30 with the study group with the