So take it away. Okay, well, thanks very much. Thanks to the organizers for inviting me. And I'm sorry I couldn't be there in person. So the general setting of what I want to talk about is the following type of problem. So there's an unknown distribution out there. We'll call it D. It's a discrete distribution. It's going to take K different values with K different probabilities potentially. So throughout Pi is going to be the probability of the ith outcome. The ith outcome, and we want to estimate some function of these probabilities p1 through to pk. So there's two notions of complexity here. There's the standard one that statisticians think about, which is the sample complexity, like kind of how many IID samples do you need from this unknown distribution D in order to estimate your function, let's say additively up to plus or minus epsilon with probability one minus delta. But there's Um, but there's the also the space complexity, which I guess we're more used to thinking about in kind of streaming or like by analogy in terms of communication, which is like kind of we're taking these samples from the unknown distribution, but do we need to store all the samples before we compute our estimator? More generally, how much memory do we need as we process the stream of IID samples? So, this leads us to general questions like kind of what sort of functions. Sort of functions can we estimate with Limbton memory? And are there trade-offs between the number of samples we take from the NN distribution and the amount of space we're using to process these samples? So over the last 15 years, there's been kind of a growing body of work related to this general setting on things like learning parity, learning distributions, subspace dimension, collision probabilities, robust statistics. Collision probabilities, robust statistics, bias of coins. Okay, so in today's talk, I want to talk about estimating entropy. And the title was estimating entropy for the impatient, in the sense that we don't want to wait for too many samples before we get our estimate of entropy. And for the forgetful, i.e., we've got small space. We can't remember everything about the samples we're getting. Okay, so entropy, I don't think I need to define. I have pulled a fast one here. I have pulled a fast one here. I defined entropy with a natural log rather than a log base two. That doesn't really make a difference. It's just at various points, I'm going to take some Taylor expansions and I wanted things to be slightly cleaner. Okay, so our goal is we want to take samples from our unknown distribution D, and we want to return an additive epsilon approximation of the entropy of D with probability at least two-thirds A. So, one caveat: this talk is mainly about. Talk is mainly about upper bounds. I did clear this with the organizers. But wait to the end, and there's some natural lower bound questions, as there often are at the end of upper bound talks. And also, if there's one concept that firmly sits in the intersection of people, of things people who do lower bounds are interested in and people who do upper bounds are interested in, I think that should be entropy. Okay, so in terms of what's known so far, if there's absolutely no If there's absolutely no space constraints and we're just looking at the statistical question of how many samples do we need for an ad-sepsilon approximation, this is finally fully understood. We can say that the number of samples you require is this expression here. So it's basically kind of almost linear in k. And there's these dependences on the as sort of approximation factor epsilon. Okay. Now if we If we allow ourselves a kind of sort of amount of space which is normally allowed in the streaming context, so kind of polylog k, kind of poly1 epsilon, kind of words of space. Then we can, the number of samples that are sufficient is like kind of k over epsilon plus one over epsilon squared log squared k. And this basically just kind of follows from the fact that there exist the main focus of streaming. The main focus of streaming algorithms for the longest time was like kind of estimating something that was in the stream. So, like if you have a stream of a certain length with elements in the range one to k, you can talk about the empirical entropy of the stream. So, you can talk about the kind of the relative frequency of all the elements in the stream. And there's a bunch of algorithms out there that'll estimate the empirical entropy of the stream. But if you take this number of samples, then the empirical entropy of the stream Then the empirical entropy of the stream is within epsilon of the entropy of the source with high probability. So, we get this result from previous work. So, the main focus of our work and the most closely related work that appeared a couple of years ago in Europe is in the constant word regime. So, the question is like, kind of, is it possible to return an absolute approximation if we only have Approximation if we only have a constant number of words. And so I should say what a word is in this context. Like kind of a word is basically enough space to store a single element of the stream. So kind of log K. And also, so it's basically kind of log K plus log one Rhubson. Okay. So our result is that if you only allowed order log K plus log one psalm space, then Then, as long as you have order k over epsilon squared, some polylog and one of epsilon samples, then you're good. Okay, so we conjecture this is optimal up to log-rompsilon factors. I'll talk a little bit about that at the end if I have time. And this improves over the best previous results in this regime, which was by Acharya et al. in Europe's 2019. So, they had an argument that's roughly used. An argument that's roughly used k over epsilon cubed, and this epsilon cubed really bugged us, and so we managed to get it down to epsilon squared. We think this is the right dependence on epsilon, but we could be wrong. Okay, so that's the problem, that's the results. Any questions before I go on? Okay, so I first I want to go through kind of First, I want to go through our first approach, which is going to kind of set the scene for what we're actually going to do, but is suboptimal in various ways. Okay, so the upper bound is going to work as follows. So we're going to define a random variable x based on, it's going to be a random variable x that we can draw a sample from in small space. So you're right to process these streaks. So, you're right to process these stream of ID samples. Look at the first sample you get, and that's going to be a number between one and k. We'll call that value that you get a. And now what you do is you just keep on looking at samples until you see t more copies of a observed. Okay, and then x is just that total amount of time you had to wait divided by t. So, if you think about what x is, x is technically a mixture distribution. It's the average of t geometric distributions, each with parameter p A. But of course, A was I with probability Pi. So X is a mixture distribution. For the sake of analysis, I'm going to introduce another random variable, X superscript I, to be X conditioned on. be x conditioned on the event that a equals i. Okay, so x superscript i is just the average of t geometric distributions with parameter p i. Well, each of those geometric distributions with parameter p i has expectation one over p i. So the average will be one of a p i. Okay. Now, if x i concentrates around its expectations sufficiently well, then the expectation Then the expectation of the log of this random variable that we've defined will be pi times the expectation of the log of x superscript i. Okay? Because first of all, we're taking the probability that a equals one, a equals two, a equals three, et cetera. So we end up with this expression. Now, as I say, if xi concentrates around this expectation, well, the expectation of log. Well, the expectation of log of xi, um, and this xi is just one over pi, and so we get back something that looks like expectation. Sorry, it looks like okay. So does xi concentrate? Well, it depends how big we set t to be. Remember, x was the average of a bunch of geometry distributions. We all know that the more you take of something, the more concentration. You take of something, the more concentration you can hope for. So, you can, if you wish, show that this approximately equals to is approximately equal to with a plus minus epsilon error as long as you set t to be roughly one or epsilon. And then the variance of log of x ends up just being something like log squared k. And this follows from the fact that when you sample A, you could get an A could equal I, where I equals something like where P of I equals a half. P of I could be very small as well, but it's unlikely to be much smaller than one over K, because there's only three possible items. Okay, so you can work these things out and you can argue that averaging A bunch of copies of your basic estimator taken in series gives you this plus minus epsilon approximation here Chebichev. Okay. I should say, like when I said I define entropy in terms of natural logs rather than kind of log base two, I didn't intend to rewrite all my order notation in natural log, but this is what happens when you search and replace in the document. Okay, so sample complexity. So, sample complexity, because of the way we've set up this basic estimator, the sample complexity is something we can actually kind of compute exactly just because the number of samples we need to take is exactly x. And we already have a good handle on what the expectation of x is. So the sample complexity ends up being: well, there's one sample to draw A. There's the total number of times we need to wait until we get X is just T times X. We get x is just t times x, and we need to replace the whole thing r times. So, substitute in those r, those t values, and we get one epsilon cubed times k times log squared k. Okay. So two things annoy me here with that expression, that sample complexity. Well, let's talk about the thing that I like. I like the fact that sample complexity is small. Like, you know, to do this, we really only need three words to compute the basic. Really, only needed three words to compute the basic estimator. We needed a word of memory to remember what A was. We needed to count up until we've seen T copies of A. And I guess we need to repeat this whole thing kind of R time. So that's another basic estimator, another counter. Okay, but in terms of what annoyed me was that sampling complexity ends up having Sampling complexity ends up having this, like a one-up selling cubes, ends up having this kind of log squared k. Whereas we had reason to believe that the log squared k didn't have to be there. And as with many of these estimation things, like whenever you get something that isn't one or epsilon squared, you question, is this an artifact of how you're proving something? Or is it, could it be something about the problem? So we set about trying to get rid of this log squared k and get rid of this one or epsilon. And get rid of this one or epsilon. Okay. So the basic approach is something that seems somewhat natural, at least in retrospect. So we have a choice, like, you know, we needed to set t to be large such that our estimator had small bias. Okay. And when we set t to be one over psi one, that's T to be one epsilon that was incurring one of the one epsilon terms here. Okay, so the idea was why don't we stop worrying? I want to paraphrase how I, yeah, how I stopped worrying about the, I really can't remember the movie now. What was that movie? How I can talk to streams. Yeah, and what's the line? Yeah, and what's the line? How I learned to stop worrying about problems. Yeah, how I stop worrying and love something like that. But anyway, this would have been a nice pithy comment if I could have remembered that that movie kind of went inside the slide. So in our context, it's like, you know, I'd like to stop worrying about the bias and actually start loving the bias because it turns out the bias is something we can estimate on its own because of some nice properties the bias has. So, in particular, Bias has. So, in particular, let's split up the Shannon entropy into something based on our random variable x. We'll call this the fake entropy. And then we're going to write it as the fake entropy plus this bias term, which is the difference between the fake entropy and what we're trying to estimate. Okay. So, let me talk. So, basically, the talk is going to be divided into kind of estimating and kind of computing these two terms sufficiently well. Computing these two terms sufficiently well. Estimating the bias term is going to basically revolve around explore, investigating this term and showing that it can be, that estimating this term can be reduced to the problem of estimating a polynomial in these PI terms. And that's something we can do pretty space efficiently and sample efficiently. And estimating the fake entropy, the basic idea is going to be rather than kind of sampling from X and return. Kind of sampling from x and return log of x and repeat it a whole bunch of times and that had a fairly high variance. We're going to write the expectation of log x in terms of the following linear combination of conditional expectations and where E1, E2, etc. are going to be partitioning events carefully chosen such that estimating each of these things is not too hard because it has small variance. Okay. Okay, so estimating bias then. So we want to estimate the bias up to plus minus epsilon. So let's first of all kind of rewrite the bias into a form where we can start thinking about Taylor expansions. So the bias was the expectation that was basically the fake entropy subtracting off the thing we actually wanted, the standard entropy. Entropy. If we define yi to be xi times di, remember xi was the average amount of time we need to wait for another copy of i. And then so the expectation of xi was one over pi. So xi times pi is going to have expectation one. So I'm going to introduce this new random variable yi based on xi. Yi based on xi. And then I can write this bias as by just doing some algebra here and combining some terms as the bias is the sum of the pi's times the expectation of log of yi. And this yi I said is expectation of one. Let's pretend for a second that yi is always somewhat close to one. And particularly, it's strictly bigger than zero, strictly less than two. And then we can take the Taylor expansion. And then we can take the Taylor expansion of this log to get a bunch of terms like so. This is the familiar Taylor expansion. It wouldn't be allowed if xi was much bigger than two, but unfortunately for us, that's not going to be a problem. Okay. So notice what these terms are. These terms look very much like central moments. This is the kind of the first central moment, the second central moment, the third central moment. So the hope would be. So, the hope would be that if we could split this expectation of this infinite sum into a sum of expectations, each of these expectations would have some nice form that we could hope to kind of approximate. And that's actually what happens. So this is something we're used to in familiar random variables, like the central moments of the binomial random variable, for example, are polynomials n and p. But if you think about it, there's no, it's, it's, it's, it's not. There's no, it's not clear why this should always be the case. Fortunately, for our random variables, these kind of sum of geometric random variables, or equivalently, negative binomials, it is the case, that each of these central moments end up being polynomials in these unknown pi's and these t's, which is something like we had control of. So, the approach is to estimate, um, is to estimate this term by first. Is to estimate this term by first or first of all truncating this Taylor expansion and then show that each of these terms, like kind of here anecdotally, it seems to be true that each of the central moments is polynomials in PI, but we actually want to prove this. And then once we've done this, this expectation of log of yi will be some polynomial in PI. This thing will be some expression of the form of... Sum of a sum of rhyme of pi tends to poly pi. Okay. Okay, so three quick steps. I need to argue that we don't need to take too many terms in this table expansion if we want to get small bias. I then need to show that this is generally true, that the central moments of a negative binomial are polynomials in PI. And then I need to talk about estimating the functions of that polynomial form. Polynomial form. So let's do the truncation step first. So I'm going to consider the degree d approximation of the natural log of y. And so that's an expression like this. And then yi will be the natural log of y subtracted off our approximation. And this will be related to the error of our estimate of the bias. So remember, like we normally think about the bias being the error, but now we're trying to estimate the bias. Error, but now we're trying to estimate the bias, so we're worried about the error of estimating the bias. Okay, um, so we can show that in expectation, this error term is small, it's less than epsilon, as long as r is theta log of one or epsilon and t is roughly r squared. And I'm not going to go through proof of any of these statements, but the proof ideas I think are fairly. Fairly useful to see. That first of all, we apply Taylor's theorem so that when yi is concentrated about its expectation, then we know that the error, we can quantify the error of the Taylor expansion in terms of the first term in Taylor expansion that we omit, which is something like that. Okay. Then we can analyze central moments. Then we can analyze central moments of scaled negative binomial random variables to argue that the expectation of this quantity can be written in terms of this kind of r and this t. And then if we substitute in t to be r squared, well, this thing ends up being, could be a small constant, if you like. R being natural log of one epsilon could end up, this whole thing we can make as small as epsilon. So that's the proof ideas. Epsilon. So that's the proof ideas. Again, we have to take a little bit more care, like, you know, when since these yi random variables could be large, the Taylor expansion isn't valid when yi is very large. But then that happens with so small probability, there's other ways to kind of quantify the error there. So the crawlery of all this, the crawler of the lemma is that we can We can that the pi times the expected value of this function of yi equals the bias with some small error. Okay, so we next need to argue that the expectation of the function of yi is actually a polynomial in pi. So we showed this anecdotally for some small moments. Why is it generally true? Why is it true that for this? Um, that for this random variable and this function, should we get this? Why is this a polynomial in the pi's? So given the form of this expectation, it's actually sufficient to show just the jth central moments for all the different values of j are just polynomials and pi. There's two possible ways of doing this. If you're me, you do the kind of classical way, and you kind of find the moment generating function. You kind of find the moment generating function of your random variable. You want to find the date moments. You differentiate it, differentiate j times. You try and do a lot of algebra to find some like common expression. You do some more algebra. You stop every five minutes thinking this should be known. This should be on the web somewhere. I should be able to look this up. That slows you down. You keep on going. And eventually you find a nice expression. If you're my co-author, you find a much kind of cleverer, kind of quicker hack. Clever kind of quicker hack, which involves relating the central moments of the negative binomial to the polylogarithmic function and appeal to known properties of the poly logarithmic function. You learn a lot from co-authors, I guess. Okay, so what we've done so far, we've said that we're trying to estimate the expectation of the log of a random variable. random variable we say well we take a certain number of terms of the test mention we can get a small error small additive error so the but our estimator of the bias has small bias and we reduce and our estimator itself is now just a polynomial in these p i's so we need to estimate some polynomial of this form for some kind of coefficients alpha naught alpha one up to alpha r for example One of the alpha r, for example. A simple way of doing this is if we want to kind of, is that in our setting where we're taking a stream of samples, if we want to estimate something like the sum of pi squared, what we can do is we can just take the next two samples. Sorry, take the next yeah, take the next two samples. If they're both equal, output one, an expectation. In expectation, um, so this is me describing the running variable z1. In expectation, z1 is the second moment of the pi's, z2 is the third moment of the pi's, etc. And so the learning combination of these gives you an unbiased estimator of this thing. The variance depends on the size of those coefficients, but you can argue that those coefficients are small. Argue that those coefficients are small using a nice trick of relating the coefficients. If you have a polynomial, a TV n polynomial that is bounded, say for all x between 0 and 1, then you can argue that the coefficients of that polynomial are only so big. And so you can use that trick to argue that this unbiased estimator for this polynomial. Bias estimator for this polynomial has small variance. Okay, so that's estimating the bias. Let me talk briefly about estimating the fake entropy. So the idea here was we were taking the fake entropy and we're splitting it up into a linear combination of conditional expectations. The conditions for the conditional expectations are going to be that x line. Our expectations are going to be that x lies in certain ranges. So from 1 up to some value b1, then between b1 and b2, all the way up to b L minus 1 up to B L, where B L is going to be K roots L. Okay. Notice as I define this, like B1 up to B L aren't quite fully partitioning because X could be infinite. We'll just do a little hack. We'll redefine X to be at most K over epsilon. K over epsilon. And this actually only changes the expectation by a small amount, by epsilon again. Okay. So why do we want to do this? So the main idea is that each of these conditional expectations is x lying in a small range, like kind of lying in the range, say between bi and vi plus one. Okay. So we can show that the very Okay, so we can show that the variance of log of x conditioned on this on x being in that range is at most log squared bi plus one divided by b i. Just some algebra. So if bi plus one is not much larger than multiple, much larger, if it's not much larger in a multiplicative sense than bi, then these things have small variance. The other nice thing about decomposing the fake entropy in this About decomposing the fake entropy in this way is that remember, like, you know, like originally, all we do, originally in the streaming algorithm, we're trying to sample from X. The larger X is, like, you know, the more samples we take. If we're estimating one of these terms and X exceeds some point, so we know that it's no longer in the bucket Bi, then we can stop. Okay, so in particular, estimating whether X is in BI requires at most X is in PI requires at most T B I stream samples. Okay. So how do we estimate a conditional? How do we estimate some of these of this form? The basic idea in the stream settings, we're going to go from like kind of B1, we're going to deal with that term and then B2, then go through the buckets one at a time, adding up what we find out. Now, if we want to estimate the probability that random variable Random variable lies within a certain range, how would we do that? Well, we do that in a natural way. We just take kind of a bunch of independent copies of that random variable and see what fraction of those lie in the range. So that's all we do here. So we take a bunch of independent copies of our random variable. We see of those copies, how many lie in the range? And like, we're going to estimate this probability as just the number of copies we have. The number of copies we had on the denominator and the number of copies actually lay in the range on the numerator. How do we estimate a term like this? Very similarly, like, you know, we take our independent copies of X and we look at all those for which X lay in the appropriate range and we take the average of log of X of those. Okay, so that would give us. Okay, so that would give us this term. Unfortunately, we never ever see a copy in that bucket. We don't want to be dividing through by zero. So there's some special cases that we do when Ci is zero. We've taken these independent copies and we've never ever seen X lying in the range. So then we're going to return basically our estimates of this times the estimates of that summed over I. Times decimates of that summed over i with a little bit of cleverness involved. Okay, so I'm Andrew. Um I'm gonna have to ask you to maybe start cluing up a little bit. Okay, sure. Okay. I think, yeah, so all the ideas are basically there. This is the algorithm. So what we need to argue is that we return this, it has small variance. Now, because of various Because of very careful algorithmic choices we made, we can actually estimate this error. I'm quite pleased with the slide, but it's really not worth going through at this point. We can estimate our error. We can work out how many copies of X we need and we can define these BI values such that we get our required sample complexity. So let me just So, let me just mention this lower bound and then conclude. So, is the algorithm optimal? We think it is. And the example that we think is hard is consider the following setting. We have X, we have K labeled coins. K over two of them are fair coins, K over four of them are heads biased, K over four of them are tails bias in case one, where heads bias means this three-quarter probability of head, one quarter probability of tails, and tails bias means the flip. In case two, means the flip. In case two, we have slightly more fair coins and slightly less heads bias and slightly less tails bias coins. So we imagine a stream like in either case one or case two where we draw a random coin, we flip it and you see either heads or tails. The entropy in case one is this expression. The entropy in case two is this expression. The difference is epsilon. So these are two cases with an absolute epsilon difference in their entropy. We have reasoned We have reason to believe, but we haven't yet proved it, that distinguishing between these two cases would require k or epsilon squared, thus proving our bound is our algorithm is optimal. Okay, thanks very much. Are there any Andrew? A quick question: If you have more samples, can you get better? Samples. Can you get better?