driven sample average approximation with covariate information. This is integrating machine learning or statistical learning together with sample average approximation. And this is joint work with Rohid Khanan, who is right now at postdoc at Los Alamos National Lab, and Jim Lutge from the University of Wisconsin-Madison. And I would like to gratefully acknowledge the To gratefully acknowledge the Department of Energy grant that allowed us to do this research, it's a Maxer grant, and also thank you for the invitation to be part of this wonderful workshop. So let me start by, so I'm going to start by introduction and motivation, the reason why we're studying this problem and what we're doing. And I'm going to talk about what exactly we are doing in more detail here, which we call empirical. Detail here, which we call empirical residual sample average approximation, followed by the theoretical analysis of this method. And next, I'm going to present computational results and discuss some future work. Okay, so the what we are going to consider. So, this for those of you who are in stochastic programming, just a little bit more. Little bit more different notation than you might be used to. But basically, we are reserving the capital X and the capital Y as the, you know, more statistical fashion as random variables. So here the Z is going to become our decisions. So for example, if I need to make some inventory decisions and but I don't know what the demand is, which is this capital. I don't know what the demand is, which is this capital Y, then you know, maybe I'm going to do my inventory decisions in an expected fashion, minimizing some expected cost where I have some feasible region for my decision C. So this is a traditional stochastic program. And suppose we have access to, let's say, independent, identically distributed, but it doesn't have to be, right, observations. So typically we cannot solve this problem. Typically, we cannot solve this problem, even if we know the distribution of y, right? Because calculating this expectation is a multi-dimensional integral that is typically not possible because this function c could include optimization or nested a bunch of optimizations in it, actually. Very difficult. And so, or we might not know the distribution of y, we just have access to data. In either case, right, instead of solving. case, right, instead of solving this true problem, what we do is a common intuitive method is to use sample average approximation, which basically takes the expectation that you see here and replaces it by a sample average. So given n data, then the expectation is turned into sample average for each data point yi and take 1 over n and minimize this quantity instead, right? So it's a very well established theory. So it's a very well established theory where people have studied consistency rates of convergence, many, many results. And if you want to really learn about this topic, Professor Shapiro and Dancheva and Ruzhinsky, Andre Zhrzhinsky, they have this really nice book on stochastic programming, if you're not familiar. So that has really nice chapters on this and results. So and for a less detailed, more survey type of introduction, here's a Survey type of introduction. Here's a paper with Tito. But yeah, if you want to really learn it, I recommend the book. So our setup is slightly different, right? So this is a traditional stochastic program. Now we are going to, we're going to enter some kind of machine learning into it, but the way we do it is through covariates. So I don't know, for instance, my random demand, but I might have some other realizations, right? So for example, So, for example, this is introducing a completely new product, but I might have data from similar products, you know, historical data. I might use those. I might also, you know, in an, like especially in electronic stuff, you introduce it at these conventions and things like that and give samples to some social influence. Samples to some social influencers, for example, and then they will review it. And you can actually track data of, let's say, Twitter mentions, news, media mentions, and so on and so forth, and use those data, which we call covariates, but it can be called also side information, auxiliary variables, features in machine learning. So, like you collect data on Twitter, Chatter, or some other things, plus historical data from Plus, historical data from similar products. And then you use this information to estimate what the actual demand is going to be, right? So you use some kind of learning. And given what you have observed, so we turn the traditional stochastic program into a conditional version. So given this data I've collected, I'm going to minimize the conditional expectation of my cost for my demand here. My demand here, making some inventory decision Z within my feasible reach. All right, so this is the one we're going to use. And Bertsimus and Kellus, basically we're using their notation. That's why I put that, but they also obviously worked on this problem as well. All right. So suppose we have data again, right, which are not necessarily independent and identically distributed. For instance, For instance, you know, and data, but instead of just the demand, now I have observations from, you know, this Twitter chatter or something else, right? I might actually use past demand, so like some of this XI covariates here could be demand I observed in the history, so I can also use those and other things, right? Maybe a certain market that you're A certain market that you're selling it to, let's say North America versus Central America versus South America, you know, the location could be one of the covariates. So you observe these data and throw in some others you want to. And now, when making the decision, we have a new covariate. This is historical and this is what we observe next. Now, how are we going to create a sample average approximation in this situation? In this situation, right? And there will be two components. One, I have this historical data and maybe, you know, my new covariate. First, I need to do some kind of learning. I need to first predict my random vector or random variables, just demand, from the covariates that I have, right? Especially the current covariates. So I'm going to use historical data, but also use the current observations I have, and I need to predict what the demand for my product is going to be. The demand for my product is going to be. Next, I need to optimize my model, the inventory decisions, right? So, two things. And this can be done in a variety of ways. And this is a hot topic that basically have now a lot of literature, recent literature coming up. And so, this is a review of that, but not complete. So, just to make it clear that this is definitely not a complete. Clear that this is definitely not a complete review, but just to give you an overall idea. So, one of the approaches is to do empirical, which is called empirical risk minimization. This can also be considered as sort of solution learning, learning how the solution is going to be, let's say by machine learning or statistical learning. So, given the covariates, can we map the covariates to a solution Z? For example, Bonn and Ruthen looked at this in the Within looked at this in the like a news vendor type of inventory problems. So, in this case, this basically says Z follows, for example, some kind of a, let's say, a linear function like the linear regression, and it needs to map from the X's to the to this, you know, my cost, for example, is going to be. My cost, for example, is going to be like let's say I have data one over n, and then I have c of some kind of z, and then the x, like I think this will be that. And here I need to minimize the z from this function, right? So this basically tries to take the covariate and let's say a linear regression and tries to learn or come up with some decisions. So it is, first of all, handling constraints can be a problem. You can get infeasible. Constraints can be a problem. You can get infeasibilities. This is close to affine or some kind of decision rule, right? Non-linear, linear decision rules, because you're forcing it to be a function of the covariates. And also Baximus and Cullis show that this type of model can have other problems and in large dimensions is challenging. So it may not be the preferred method, but something like specific models with simple constraints, it could work, but otherwise it has some problems. It has some problems. A second way of approaching this problem is to basically our work, but it has been studied before. For example, a paper by Ben et al. or Sue Richardson and Deng also proposed a model that we are going to study in more detail today. And Barsima Sankellis has a weighted sample average approximation. So here, the learning step and the optimization step are somewhat similar. The optimization step are somewhat separate, right? There's a learning, especially, that is done in a more traditional fashion. But the idea is to keep the learning part as it is, but embed it somehow into the optimization. So the optimization step is modified to take care of the machine learning part. Okay, and this is what this talk is doing. A third approach, which came actually a little bit earlier, right, is this smart. Just smart predict optimize is to say, okay, if I'm trying to learn, you know, for the specific optimization model, why am I wasting my energy to do the best learning model? Perhaps I should only learn the parts that are relevant to my optimization. So, this one combines and modifies the learning step in order to fit the optimization, right? So, this changes. This changes, for example, the loss function that is used to reflect that the optimization model. So, this is more advanced, but because this is more advanced, it results in a definitely more challenging optimization problem, harder to solve. So the complexity increases, can also be less modular. But on the other hand, because it's modifying the machine learning to do the optimization, it could actually lead to better, you know, lower cost, for example. You know, lower cost, for example. So it's more difficult, it could be better in certain settings. So this is easier and modular, and this has some problems. All right, so we're going to focus on approach. Some applications. So I have been giving the application of production inventory planning, which have been studied, for example, in both of these papers. As I said, Y would be, for example, the product demand. My covariates could be seasonality. Be seasonality, the web search results, the Twitter chatter that I mentioned, historical data, and then these are the production and inventory decisions. Another nice application has been used in the energy area. So, for example, in power grid scheduling. So, the random vector would be in this case the load, renewable energy outputs, which can be quite random, like wind and solar. Can be quite random, like wind and solar. So, and to predict those in a better way, right, we use weather forecasts, seasonality, you know, time and day, so and location, there are many other and past load, historical demand data, and so on and so forth. And then we do a machine learning to learn the tomorrow's load. And then, for example, we can do a unit commitment problem where we determine the generator up, down. Up-down with respect to ramping constraints and whatnot. So that's another application, but the applications are numerous, so anything that you want. All right. So here is the approach that I'm going to focus on. So here our setting is this. We have this random vector y, the demands, given the covariate information. So the conditional random variable given the covariates, right? The covariates, right, is defined by some unknown function, but we assume it has this form. So, this f star of x is what's called the regression, the true regression function. So it gives us the conditional expectation of y given x equal to this specific covariate little x. Plus, it has an additive error. So we assume additive error here. And we also say, okay, so this belongs to a known class of functions. A known class of functions. By the way, this is not very restrictive in the sense that I'm not like restricting it to linear regression or something. It can be infinite-dimensional. It could depend on n, the number of sample points we have. One big assumption we make is that the errors are independent of the covariates x, right? Throughout this talk. And of course, you know, this is very classic, the errors in the true model, right? Errors in the true model, right, they have to have an expected value of zero. Okay, so given this setup that we that I just talked about, so conditionally I have some true model that I don't know, but suppose I know this, then my true conditional stochastic program, which is, if you remember, is written like this, right? Given that I have observed this Twitter chatter and this past historical demand, I predict my Y to be that. I predict my y to be that, or the conditional distribution of y is this, then I need to solve this conditional problem. I replace it with essentially this value here to take care of that conditional demand given the covariate x. All right. Now, once again, I'm back to the traditional setting. Given the setup, right, I just have a little bit more complicated function here with the error term. Complicated function here with the error terms, and that's about it. So, I can take this instead of the expectation, I will replace it by its sample average approximation. So, every place I see this, I'm going to use data. All right. Suppose I have this data, like n of them, and again, it doesn't have to be independent and identically distributed. Given this data, I can actually look at the true errors, right? If I know the true regression function, I know. True regression function, I know the true errors. So, what I can do is I can create a sample average approximation going from here to here, basically plug these errors with the true regression function and get these observations, right? So, I have the conditional yi for a given covariate x. All right. The problem is, of course, I do not know this regression function, right? Do not know this regression function, right? If I knew it, everything would be easy. But since I don't know it, we're going to introduce the learning part, and that's going to create some sort of an additional error term that in the theoretical analysis we have to take care of. All right. So because I do not know this true regression function f star, we are going to use our favorite method, whatever you want, machine learning, statistical learning. So here's one example where it's minimizing. Where it's minimizing a loss function, more generally, like M-estimators, but it doesn't have to be parametric, it can be non-parametric methods as well. For the theoretical analysis, we're going to need some results on this part, but it can be quite general. So, given the data, now I'm minimizing some loss functions. So, for example, this could be the squared loss, and I find the best model that Find the best model that minimizes just like a regression model, right? Ordinarily squares. And I will get the best fit for the data. And that will be my estimated regression. All right. Now, given the estimated regression and given my data, I don't know the true errors, but what I can do is I can compute empirical errors. So what we call is the empirical residuals, right? So given the data, So given the data yi and xi for each data point, and given my regression estimator here, f n hat, I calculate the errors for each one of my available data. Now, I'm going to use this as a proxy. So both my estimate of the regression plus my estimate of the residuals, right, as a proxy and plug that in into the sample evaluation. And plug that in into the sample average approximation. So, this is what we call empirical residuals-based sample average approximation, or ERSAA for short. And if you have any questions up to this point, please feel free to unmute and ask. All right. So this method is not new. It's actually been proposed, as I said, by Surijit Sen Dang, also by Banetol. By Bon et al. in their multi-stage setting, actually, in that case, but there's no general analysis so far, it has been analyzed for specific problem or for some parts of it. But what we're going to do is we're going to do a more general analysis of this next. One thing I also want to mention is one problem with this method is, right, or potentially, if we do not have enough data here. Not have enough data here. These residuals are based on the regression model that we estimated, but this regression model used the same data, y and x's, right? That's the historical data. So there's some bias introduced here. What we have done is also we've considered two new variants that are based on, you know, reducing this bias in the empirical residuals. So let me talk about that next. So let me talk about that next, especially when the sample size is small relative to the complexity of the regression method, the dimension of the y and so on and so forth. So we have this bias in our estimator. So we can to mitigate this effect, we can use leave one out residuals. So if you're familiar with jackknife methods that have been used in the statistics literature to deal with this bias issue, right? Bias issue, right? This is what we do. So they're inspired by their jackknife methods, essentially. So every time for each data i, we take that point out and then do the regression on the remaining n minus one data. So we take the point i out, we train the data on the remaining points, right? And we get a regression estimator, which we call negative i, so i removed estimator, and then we And then we, to somewhat reduce this bias, right, for each point i, this regression function didn't use that point, but we calculate its residual based on the point that we discarded. So in some sense, that removes the bias in between the two. And we do it for every point. So we're going to have n regressions here and then n different residuals that do not use that data to remove the bias. To remove the bias, and we can do one of two things. The first thing is we can use the full data and do a full regression, right? This one, plus the leave one-out residuals. So this is what we call jackknife SAA. And the other one is we call Jackknife plus SAA, which for each data point, it also uses the leave one-out regression. The leave one out regression function. In our competitional results, we did not see much difference between the two. So, when I show the competitional results, I'm only going to show the jackknife version. It really didn't make that much of a difference. But as I said, these are inspired by jackknife methods and aim to eliminate the bias. Okay, theoretical analysis. Let me talk about some notation. Me, talk about some notation and like give you the main idea here. So, what's happening here is: if you remember, we've got like a three-sequence of problems. One is the true stochastic, the conditional stochastic program, given our setting, of course, given this true but unknown regression function and the true errors, right? The optimal value of this problem I'm going to denote by v star of x, so given some covariance x. Variants x and the true but unknown objective function is given by this g. What we can do is again, so somewhat intermediate between the two, again, assuming I know the full regression function, the true regression function, I can do its own sample average approximation, right? So, this is nothing between these two is the traditional sample average approximation. Unfortunately, Unfortunately, of course, because I do not know that, I have to estimate it by this regression. Now I have created another error here, right, based on my estimator, plus also the residuals. I don't know the true residuals, so I have the empirical residuals, right? So I want to show this problem five or the ERSAA, how good it's actually estimating the stochastic program. And as I Program, and as I increase the sample size, can I actually get to the true problem? Or, for example, the objective function is a statistical estimator of the true objective function. You know, can I get convergence, for example? So these are the things I want to answer. And the way to do it is basically control the error between the two. So the first one is between like I need to have a good traditional SAL part and then I need to have obviously a good estimate. Have obviously a good estimator. So, if I have those two, then I will get to what I want. And so, that's the whole idea of the theoretical results, just the insight of how to do this. So, okay, so this was the notation. So, every time you see ER, that's the empirical residuals. And this is the sample average approximation, assuming the true, and this is the true but unknown and non-computable. A non-computable function here, G. All right, even more notation. So I have a set of optimal solutions to the true problem, and this is one optimal solution, and this is the set of optimal solutions to the empirical residual. And throughout, I'm going to assume non-empty for almost every x of the covariate so that I can solve these problems. Now, I'm going to show you a set of assumptions such that A set of assumptions such that I will have the desired convergence result. So, what are my set of assumptions? The first one is going to be on the sort of the structure of the problem. Second one is if I know the true function, I need to have good properties of the sample average approximation, otherwise I don't have a chance. Third, of course, I need to have some good properties of my learning part, the regression part. So, the first one on the problem structure, we're going to assume. We're going to assume Lipschitz condition on this cost function and Lipschitz with respect to the demands or the unknown parameters y here. And this Lipschitz constant satisfies this condition. It's going to depend on the for each decision z, but even in the worst case, it's finite. For instance, this can be satisfied by two-state stochastic linear program under mild conditions, two-state stochastic mix. Conditions to state stochastic mixed integer with continuous recourse and stuff like that. So, also, I just want to mention that it can be relaxed to local Lipschitz continuity rather than this global one here. But that will need additional conditions on the regression step. So let's assume this Lipschitz condition on the cost function is satisfied. Next, if we know the regression function, right, we need to We need to have uniform convergence and probability to the true expectation. So we need to have good properties of the SAA. And this typically amounts to, so for example, if I have independent, identically distributed data under well-known, well-studied conditions, or non-IID data, but one needs to be a little bit more careful, right? Some kind of uniform law of large numbers. Uniform law of large numbers type of result is needed. And then the last one, I need some kind of actually two kinds of consistency of my regression. And the first one is, for example, what we call pointwise. So for every covariate information, new covariate here, I need to convergence in probability. And the mean squared error consistency here. And under, again, some. Under again some appropriate and relatively mild conditions, it will hold for ordinary least squares, k-nearest neighbor, random forest, and many other non-parametric or parametric regression models. So once we have all the three, then what we can show is, so we do have the desired convergence of the optimal values. We also have the set of optimal solutions to our empirical residual problem. problem converges this sense right the this distance between these two sets converges to zero and the the the nice convergence results and one thing is that we show our results in convergence and probability but it can be they can be strengthened to almost sure convergence okay we can also do same similar thing on the rate of convergence by strengthening the Rate of convergence by strengthening the assumptions. For instance, instead of saying, oh, well, this converges to that in probability, now I'm going to say, right, I give a convergence rate here for both the mean squared part and the pointwise part. And this alpha is going to change with respect to my regression model. So for example, if I have ordinary least squares and lasso and the model that I'm fitting. And the model that I'm fitting, right, is simple linear, sparse or non-sparse. This is alpha is equal to one, which is good. However, I might not know the function is linear or it may not be linear, right? So k nearest neighbor, random forest, they can handle like a larger class of like maybe non-linear functions, yet again, they have the curse of dimensionality. So as the dimension of the covariates increases, right, this rate. Where it increases, right, this rate actually suffers. How fast we converge to the true regression function is going to suffer with respect to the dimension of the covariance. And under, you know, by the strengthening of that assumption, then we can show plastic continuity, then we have the optimal value, have this rate of convergence to the true optimal value. So, we can also do finite sample guarantees. So, here's what we're going to do here. So, this will be our reliability level, let's say 0.05. And then this kappa will give me some kind of kappa optimality. And this is the solution to the set of optimal solutions to the empirical residual problem. Empirical residual problem at the covariate value x. And this is the set of kappa optimal solutions, right, to the true problem, again at the covariate x. So here is the sample size estimate. If I solve the empirical residual model at the covariate x and I get this set of solutions, right, what is the probability that this, my empirical residual model, gives me a kappa of Model gives me a kappa optimal solution to the true problem, right? And I want that probability to be, let's say, 95%, at least 95%. What is the sample size so that I get at least 95% of the time some really good solution, maybe not optimal, but capable. And this type of thing has been studied in the literature by Shapiro, Titohamandemela, Anton Claywork, for instance. So here we're going to do it for our We're going to do it for our empirical residual version. And there are two effects. The first one is just from the traditional SAA part, right? And this is, by the way, from the classical, this quantity you see here is from the classical SAA analysis. So this O1, I mean, there's some constants and stuff, but I'm going to call it like the N sub C, C coming from the classical SAA, right? Because there's Classical SAA, right? Because there are two effects, if you remember. If I know the regression, then I need to sort of close the gap between the SAA version of my conditional stochastic program and the true one. Plus, I need to close the gap on the machine learning part. So that's the second part. So all of these things are going to include this traditional term plus the part, the sample size, additional sample size required. Sample size required in order to learn the regression function. Okay, so you'll see like all the results are NC, right? Traditional plus, so it's very conservative. So these are not to be used in practice, but they give insight into the problem complexity. So suppose my true regression function is linear, right? And suppose I use ordinary least squares. If the regression function is linear, ordinary least squares also assumes. Ordinary least squares also assumes a linear regression function, so it's fitting the correct model. Then, you know, I'm actually pretty good. So the minimal sample size required is going to depend here linearly on the dimension of the covariates. Just to note, by the way, there's also the dimension of the y, let's say the random dimens here, right? How many products. How many products I have, the dimension of the number of products is the number of dimension of the vector y. And these are the covariates. So, but I'm focusing on the covariates here. Suppose I have an S-sparse linear model instead, right? If I use ordinary OLS regression, ordinary least curves regression on a sparse linear model, I'm going to overfit. Model, I'm going to overfit. But if I fit the correct thing, right, then I can do way better because it's S sparse. You're going to see it depends on it instead of linearly, logarithmically, of course, there's the S there, but that's not a big deal, right? And again, the dimension of the Y shows up here. Similarly, so I can do significantly better in terms of the sample size required. However, the problem is. However, the problem is I don't know if it's linear or S-sparse linear. In more general cases, I can have non-linear functions. If I assume, right, so I, in order, so here are these are parametric models, so I can do non-parametric in those cases because I don't know what the function is, then if I do that, I pay a penalty. So I can do a general class of regression functions without much assumption. But here in this case, if we But here in this case, if we assume, for instance, Lipschitz continuum time, we use k nearest neighbors. In this case, and the k nearest neighbors that I use has to increase in a particular way with the sample size, n, right, in this form. Then we see much more complicated results. So it's going to be like there's a dx and log dx, and then there's terms here, and then by the way, there's an exponential term here. Way there is an exponential term here: dimension of the y to the power dimension of x. So it gets like much harder. So you need more sample size to converge to the same quality of solutions. Significantly higher sample size as the dimension of the covariates increases. All right. So this basically summarizes what I just said. Okay. So competitional results. Let's see. So, this is a two-stage resource allocation problem of one of Jim's really nice papers, somewhat simplified version of that problem. But so we have 30 customers and the dimension of the demand is 30 here, right? So, we need to meet these demands. And we have 20 resources we can use to meet these demands. Resources we can use to meet these demands. And we assume the uncertain demands are generated according to some function like this. So there's some additive error term, some constant terms, plus there's covariance, right? And constant at covariates to the power p. So this power p, for example, if p is equal to 2, this is a quadratic model in the covariates, let's say. In the covariance, let's say. But we're always going to fit a linear model. So, what I'm going to, when I show the results, I will always fit assuming p is equal to 1. So, sometimes this fit will be the correct one if the true p is also 1. However, if the true p is 2 or 0.5, which is a square root, right? So then it's going to be an incorrect fit. And then we have normal errors. I mean, we just Normal errors. I mean, we just generated the data. One other thing to note: we're going to look at covariate dimension to be 3, 10, or 100. However, the true covariate dimension is 3. So even though I might consider 100 covariates, in fact, we generated the data so that only three of these covariates are the true covariates that affect the demands. That affects the demands. The rest of it, like the 97, right, is not important. But of course, our regression might not see that, at least for small sample sizes. So this is the setup. And as I said, we will always fit a linear model. So we will always fit one with p equals one. And let's see. And we estimate the optimality gap of the solution. gap of the solutions that we obtained to our ERSA problem. And that's what I'm going to show the results and we repeatedly experiment a number of times. So this is the effect of varying the covariate dimension. I'm going to show the results with the correct regression fit and the incorrect regression fit. So first of all, so the red ones is the model that we have. Is the model that we have proposed, right? So we're using our empirical residual SAA, and we fit an ordinary least squares to the data. And we assume the model class is also linear. So this ordinary least squares actually fits a correct model. And in these plots, the black ones are the re-weighted SAA with KNN, K nearest neighbors. This is the Bartsimas and Bartsimas and Kellis paper. And the way they do it is they do something like this. For instance, they would minimize, as E, right? Remember, in our case, we have 1 over n and we have our empirical residuals and stuff. So they do, instead of giving each one over n, they do something like this. This, they will have a weight i given the covariate and the cost function at that data point yi. So for example, if we are using k nearest neighbors, this weight will be something like 1 over k if that data point xi is one of the nearest neighbors of the x, the new covariate. So, others will get weight zero. This is a resample, re-weighted sample average approximation that uses non-parametric models. In this case, non-parametric model is the k-nearest neighbor. So, here, this is with dimension three of the covariate 10 or 100. And the y-axis here is the optimality gap of the solutions that we obtained. So, we have box. Solutions that we obtained. So we have box plots, we repeated them a number of times. And here, the lower is better because the optimality gap is lower. And here, this is sort of like 1.2 times the dimension or 1.5 times the dimension and so on and so forth. So the sample size is increasing according to the dimension at the same rate. All right, so remember, lower is better. Lower is better. So, as you can see, our empirical residual SAA is doing significantly better when we fit the correct model at small sample sizes, even at larger sample sizes, like it's doing really, really well. Like here, it's almost like no error. It finds the optimal pretty much all the time, even at high dimensions. And we can also do this with fitting the incorrect model. For instance, this is a quadratic model, right? But we fit a linear regression to it. So, what ends up happening is, even in this case, we see better behavior. The problem is, of course, when we fit a linear model to a quadratic, it converges to the best linear. So, it doesn't find the true optimal. You can see as the sample size increases, there's still some error there. But at small But at small sample sizes, we definitely perform significantly better, even at high sample sizes as the dimension of covariate increases, right, compared to this related SAA with K-nearest neighbors. So even with model MISFIT, we are doing pretty well for this problem. And the jackknife estimators that I talked about, now these I'm going to show only. Now, these, I'm going to show only correct regression fit, but the other ones are similar. It loses some effectiveness and it will converge to a little something different, but it's still better. And here to look at the small sample sizes, right, because this is especially important when you have small sample sizes to reduce the bias. So, this red one is again the ERSAA with the ordinary least squared, and the green one. Squared and the green one is the jackknife version of it. And these are, you can see, and I'm only showing dimension 10 and 100, but the similar result holds. Here, especially at small sample sizes, right, jetknife does better. As we increase the sample size, the two start to become very similar. So if you have small sample size, it is worth Small sample size, it is worth definitely worth doing the jackknife, it looks like. But if you have sufficiently, you know, a good amount of sample size, jackknife is more competitionally burdensome, so it might not be worth doing. Okay, next, I'm going to show sort of overfitting effect, right? So the prediction model, we can also, our method is very modular, so you can pick whatever regression model you want, put it in there, do it with jet knifing or not. Do it with jet knifing or not, right? So there are many variants. But here is to show the effect of the prediction model on overfitting. So especially if I give you three, remember, we have three covariates that are the true covariates, but we put in 100 or 10. The model does not need all 100. As I said, 97 of them are not important. So if you do LASSO, LASO will pick. So if you do LASO, LASO will pick that up, I mean really easily. So as you can see here, the blue is the LASSO and the red one is our empirical residual with ordinary least squares. And this is a true model fit, especially at higher dimensions. I mean, even at sort of somewhat smaller dimensions, Lasso is 2 in 1. Only at higher sample sizes, empirical residual can do better or similar, but yeah. But yeah, higher dimensions, it definitely has impact. Even when we misfit the data, right, we use, for example, linear to a quadratic model, LASSO at small sample sizes is doing better. Okay. Oh, I'm almost done. I think I'm done with the time, but let me just take a few minutes to do the, to show you some. The to show you some extensions, at least mention them. Okay, so this is a summary. Perhaps let me not mention that. So this is, we've done some extensions on this. So another way to deal with limited data is to consider a distributionally robust version, right? So we have our model, so we do our regression fit. Regression fit and put that regression at the new covariate x, plus we have our empirical residual, right? And here it's going to be like at the point i. And we'll have a number of these points, right? n of them. And we give, let's say, equal weight to each one of them. This is, you can think of it as the probability one over n, right? That's the empirical probability of the model with the empirical residuals. model with the empirical residual so if i give a direct direct measure here so i can create an empirical residual probability distribution right with one over n probability for each one of those points now what i can do is i can create an ambiguity set and say okay consider all distributions q that are sufficiently close Sufficiently close to my empirical distribution, right? Even my estimates and empirical residuals. Let's say I can use Wasserstein, I can use phi divergence, I can use other types of, and we looked at a variety of different ways, that is less than or equal to some radius, you know, let's say rho, right? And then I do a min-max in this case. So I minimize my maximum expected cost where the X Expected cost where the expected conditional cost, right? Where the expectation is taken by any distribution Q within that set. And we see that Wasserstein actually performs very well out of sample when we have limited data. And the theoretical analysis and the computational results, you can find it on an archive paper. Another thing we looked at is this big assumptions of the independence of the errors and the covariance. Dependence of the errors and the covariates. So here we have generalized it to a case that has been studied in statistics. So we're maybe let me do this. So here, given the covariate information, we have this f star of x plus instead of the error term only, we have some Some q star of x. So the error here also depends on the covariate. This is important in applications where, for example, some covariate, you know, at some covariate, the variance will be small, at another covariance, variance will be high. So we do this model and we show similar results. I'm going to end in a minute. We show similar results. We show similar results. The only thing I need more assumptions, especially to estimate this square root of the variance-covariance matrix here, the Q star. And that adds a little bit more complexity. And especially this finite sample sizes is harder to drive, and so on and so forth. So it gets more complicated, but it is possible to generalize this analysis to that. And if you're interested. Analysis to that. And if you're interested, there's a note on archive as well. Other extensions that are important, multi-state chance constraints, and many other variants one can think of. So thank you for your attention. I'll be happy to answer any of your questions. And the preprint of this work is also available. Thank you. Thank you so much for the great talk. So much for the great talk. Are there any questions? I have a question. Hi, Gusen. Hi, Dweller. Nice to see you. Thanks for coming and everything. That was such a great talk. Really enjoyed it. Just a few remarks. One thing you mentioned the uniform convergence, like uniform law, large numbers, you can probably. Uniform law of large numbers, you can probably relax that to like a epigraphical law of large numbers, yes, but that's a small thing. But the main thing I was thinking about listening to you that this is this conditional fact that you have x equal x, that's a little bit like a constraint, right? And one could imagine relaxing that constraint, and then you have only y, which is you know kind of the raw data. Which is, you know, kind of the raw data on why it can be used to estimate why. But then you have this constraint. And when you think about it in a constraint way, then you suddenly have opportunities for duality to come in. And you can think of, okay, we relax this constraint, but then we bring it in again through some sort of dual formulation or dualization of those constraints, which of course relates to many other things in stochastic programming. And this was something that we used, I think. I think in a little interesting way in this residual risk paper that we did five or six years ago, where we kind of thought about: okay, if you have such covariate information, how would that kind of reduce the otherwise risk that was in a particular situation, say, based on some sort of risk measure? But if you have this covariant information, then the risk is reduced to what? Well, to what we call the residual risk, and that can exactly. Risk and that came exactly through this kind of you know dual expressions and things like that. You see exactly how much it reduces it. Well, it depends on how much information there is, if you call variance and things like that. I don't know, it could be something that could be useful for you. I think, yeah, that will be really nice, especially to get some error bounds or something, right? Might be useful if I understand it correctly. But I'm very interested in seeing that work. You can send me. You can send me the work. I'm really interested in it. Thank you for bringing that into attention. And also, like, this way you mentioned, like, okay, like, what if we consider it as a constraint and then relax it and then bring it back might be computationally advantageous as well, so that one can devise perhaps some specific algorithm using something like this. That's a very neat idea. Thank you. Thank you for that. Thank you. Thank you for that. Other questions or comments or suggestions? So I have a small comment to make. First of all, my name is Bodhi. I'm a statistician at Columbia. I was trying to follow your rate of convergence results for the key nearest neighbor methods, and you show that it depends badly on the That it depends badly on the dimension of the covariate and so on. I just wanted to basically make sure that, so there has been some work where you can show that this dx need not be the ambient dimension of x, but it could be the intrinsic dimension of x. Effective dimension, I see. Yes, so basically, if you have a 100-dimensional covariate space, but you're Covariate space, but your covariates are on, of, say, a small-dimensional manifold, then you expect a faster rate of convergence because the dx that may appear would only be the intrinsic dimension as opposed to the ambient dimension. But even in that case, yeah, I mean, I would be, please, if you could forward that to me, that would be very nice. You might have missed that because there's so many things coming up in these areas. But just to make a comment, Just to make a comment in our numerical results, right? So, I mean, let me show the misspecified ones once again. Here, I mean, if you remember, the black is the KNN, although it's a little different method with the re-weighting here, right? But the KNN here is just the rate of convergence is slower. Like you can, even with the dimension 100, right? So, here, Uh, so here uh, and the effective dimension that you mentioned here is three, uh, but uh, so I don't think okay. So, one comment is that I think the effective dimension that you are talking about is the relationship between y and x. That is three. But when I say intrinsic dimension, I mean just the dimension of x. So, there is a slight difference because when you say effective dimension, you are looking at the relationship of. Dimension, you are looking at the relationship of y and x, and you are saying it depends on only three coordinates. But if I have said data that is a multivariate normal, the intrinsic dimension is always the exact dimension. When I say intrinsic dimensionality, I mean the X is lying on a, say, on a line.