I'm interested, my lab and myself are interested in studying how we can learn to understand sequence function from evolution. And thankfully, Anchul gave a wonderful talk earlier this morning, very much explaining some of the tasks that I'm interested in, which are to understand gene regulation, be it transcriptional gene regulation or post-transcriptional regulation. And these are processes that are mediated. And these are processes that are mediated by proteins recognizing certain sequences in the DNA or in an RNA molecule and interacting with one another and with other machineries in order to impact transcription or post-transcriptional events. Of course, there's been a lot of data generated by major projects such as the D-ENCODE project, which has fueled a lot of Project which has fueled a lot of the machine learning approaches that Anschul and many others have developed. But clearly, there's a need for a computational understanding of these processes, because if we have to generate a ChIP-seq data set for every transcription factor in every cell type in every possible condition in every possible individual, we need something that will scale up. Need to something that will scale up better, and that's obviously going to be in the form of computational models. So, what we're interested in is a prediction task where the goal is to learn a certain function f that takes as input a sequence, whether it's DNA or RNA, of whatever size is deemed relevant, and then predicts whether this sequence has a certain function we're interested in. Is it going to be bound by a certain transcript? Is it going to be bound by a certain transcription factor, or is it going to contribute to the localization of a certain RNA? So on and so on. And she'll really introduce this family of problem extremely nicely. If we were able to learn such a function, we would be able to predict the impact of mutations. We would be able to maybe understand or learn a little bit more about. Understand or learn a little bit more about the mechanisms at play. And we might even be able to use this function as an oracle to design new sequences that would have enhanced properties. But learning these functions is challenging in large part because of the limited amount of training data we have. Here are a few examples of such types of functions that we might be interested in learning. Some of them are expressed. Learning, some of them are expressed as a binary prediction task. Will this particular exon be skipped in neurons or not? Or will this protein be localized to the membrane? We can often simplify things and think of these properties as being binary properties. And in other cases, we might really want to think more quantitatively, where the goal might be to predict the half-life of a transcript or Half-life of a transcript or the expression level triggered by some enhancer. And for every version of these problems, there's been lots and lots of papers, Anschul's papers often being the nicest ones, I would say, that have introduced different types of models to make predictions about those from existing experimental data. And in some cases, these approaches work really well, and those tend to be cases. Really well, and those tend to be cases where the amount of training data is very large. In other cases, when the amount of training data is much less, obviously the performance suffers. And there are a lot of interesting biological processes that are really represented or are only happening in cells. Represented are only happening in cells at a few loci. And so these are cases where it's not a matter of trying harder to generate more data. It's just there's a finite amount of number of examples of this type. And so we have to see what we can do with that. I'm going to be focusing on two problems today, as I mentioned earlier, and you've heard about them already, the binding of transcription factors. The binding of transcription factors are RNA-binding proteins to DNA or RNA. This binding depends on the sequence of the binding sites itself, but also on the presence of other binding sites for partnering transcription factors or RBPs in the vicinity of those. And the model that Anshul introduced is one where we have from experiments generated by a bunch of positive. Experiments generated a bunch of positive and negative examples. From this, we want to learn a predictor. This is possibly one of the oldest problems in bioinformatics with approaches dating back to my birth date, 1975, and continuing into today's deep learning approaches. And the same thing for our And the same thing for RBP, it's a little bit more recent, but the same sort of story. Despite all of this, there's still very much a need to improve those approaches, in particular to reduce the false positive rate, which for most practical purposes remains too high to really replace, let's say, a lab experiment. A lab experiment. So, the rest of my talk is kind of summarized by this picture. This is a static picture of some sort of underwater coral reef. And if I ask you, okay, what do you see here? Can you predict what is the function of this image? What's going on in here? Well, you can see some things and you can make some predictions. However, if I show you that actually If I show you that actually, this is a movie, and what we were seeing was just one frame of that movie, but now I'm playing the movie. Seeing this data in motion gives you a lot more information about what's going on. And there were some things that you did not suspect were present that when you see it in motion, actually becomes much easier to see. And so that's what we're going to do, except that we're going to do it in really slow motion. Really slow motion. And we're going to use evolution to play that movie for us. And then we're going to try to learn from that movie how better to understand this function, this function that maps sequence to activity. Before going in any further, I'll just give a very, very brief primer on evolution. Prime muron evolution, although it's so basic that I'm sure everybody knows this already. So, if we think of this space here as the space of every possible sequences of whatever length we are interested in, or of all lengths, if you want. Now, part, so each of the dots here is a different sequence in that space, and there's a certain portion of that space that corresponds to the sequences that have a certain property that we're interested in, property P. We're interested in property P, and then there's another portion, the rest, that doesn't have that property. When a sequence evolves, it moves around this space through substitutions, insertions, and deletions, and others. And so it just wanders around, but doesn't do that entirely randomly. Although mutations are happening randomly, their consequences are not random. And so, oops. And so, oops, for example, if a sequence has that important property, but then a mutation makes it lose that property, then natural selection would kick in and would reduce the likelihood that this individual who bears that mutation survives or has children. And so, there's going to be a bias against losing that property, or also a bias against. Property, or also a bias against gaining that property because that is generally not a good thing to create, to acquire a new function when that's not needed. So often we're going to have in the genome a large number of regions that are evolving under that same type of selective pressure. Maybe these are all enhancers that need to be activated. Uh, are need to be activated, uh, let's say, during uh myelination of uh oligodendrocytes. Okay, and we have maybe hundreds of those, and we can, so these would be the black one, the black lines here, tracing the evolution of each of those enhancers. And then we also have the evolution traces of sequences that don't have that function, and we have actually lots and lots of those. There's no shortage of negative examples. There's no shortage of negative examples in general. But the point here is that these trajectories will rarely cross the boundary of P versus non-P, as I've shown here. They will sometimes cross that boundary. It does happen that the sequence loses a function, and that somehow that's not such a big deal for the individual who has this, or maybe it's even advantageous. But these are going to be rare events that we. Events that we'll need to factor in, but that are still not going to mess up our argument too much. Okay, now of course, I've drawn these evolutionary trajectories as a single evolutionary process, but it's really a branching process. So if I'm looking at one particular enhancer, we might have the ancestral version of that sequence evolving in the dark. Of that sequence evolving, and then our speciation events, and so on. And so, really, what we have for each sequence is a tree. But I got tired of drawing all these little trees everywhere, so I drew it more like a single trajectory, but it doesn't change the core of the argument here. Now, very often, the function that matters is really not a binary function, it's more of a quantitative thing, and in that case, And in that case, we might be looking at a function that assigns a certain activity value to every point in our sequence space. At some places, the activity is low, at others, it's high. And in that case, the evolutionary trajectories would be usually more or less following the contour lines of that function. Why? Because if you have a certain enhancer, let's say, that has a certain activity. Cancer, let's say, that has a certain activity, meaning it, let's say, it activates transcription at a certain level in a certain cell type, then most of the time, the best thing is to keep it as what it is, not to reduce its activity, not to increase its activity. And so that means that although those trajectories would be random, there's a selective pressure to make them more or less follow the contour lines of that activity. Of that activity function. Now, of course, the selective pressure is applied on sequences not on the per-cell type manner, but really what determines the fitness of an individual is how proper the sequence is in every cell type of that organism. So, really, the phenotype that we're interested in is not a scalar like I'm drawing. A scalar like I'm drawing here, but it's a high-dimensional thing, but we can still think of this of selective pressure as imposing some sort of contour-hugging trajectories. And so if we were able to observe those trajectories, we would maybe see something like this. Those are the same trajectories as what I showed you earlier, except that here we don't know what the function is. And one of the questions we're going to be asking is, can we use those trajectories? Use those trajectories to learn something about that function, the one that I was drew, the one whose contour lines I was drawing here. Now, in general, we will have a little bit more information than what is shown here. We will have experimental data that tells us what is the activity level of one of the endpoints or one of the leaves of that tree. For example, it might be the result of an experiment done on a human sequence. Sequence. And so we would have data that would tell us that those are low activity human sequences and those are high activity human sequences. So can we learn from this better than we can learn from only looking at human sequences, for example? That's the topic of the next 12 minutes of my talk. My talk. Now it turns out that there's a whole lot of evolutionary data we can take advantage of. And there are something like 66,000 different vertebrate species that live on Earth. About 600 of those are species whose genome has been sequenced. In what I'll be presenting, we'll be focusing mostly on a data set of 59 mammals that are listed in the tree on the right here. And very conveniently, And very conveniently, those genomes are not just sequenced, but they can be aligned to one another. And from this alignment, we can actually infer ancestral sequences. And so we can really start seeing the movie of evolution, of some ancestral sequence evolving into maybe what it is today in human, but also what it is today in mouse and so on. And the question is: can we take advantage of that movie to improve our ability? That movie to improve our ability to make certain function predictions. So, the data that we're going to be looking at looks something like this. At the top, here you have a certain human sequence, and you might be asking, what is the function of that sequence? And now we're complementing that with the sequences, the orthologous sequences of a bunch of different species, and not shown here, but we also have the predicted ancestral sequences at every node of the tree. So, what can we do with this? So, what can we do with this? There's three types of approaches that I'll briefly talk about today. The first one aims at taking a predictive model that is already there, already trained, and just using it better to hopefully improve its predictions. The second approach is going to be to take a previously defined architecture, but to retrain it in an evolutionary aware manner. Aware manner. So, again, as to improve the predictions. And then, in the third part of my talk, I will briefly talk about how what happens when the function that links sequence to activity, when that function actually changes between species. So, the first part of what I want to talk about is called phylo-PGM. It's an approach that actually is going to be presented at ISMB. Presented at ISMB in a couple of months. And here, the idea is: I give you a transcription factor and a certain cell type, and we have some sort of trained predictor that is given to us. Maybe it's Anschul's, maybe it's some other predictor that's built and trained previously. It's a predictor that is able to take a sequence and predict a certain, let's say, probability that this sequence would be by. Uh, see that the sequence would be bound by that transcription factor, and then we're asked, uh, we're given a certain human sequence, and we're asked, will that transcription factor bind that sequence or not? And of course, we can apply the pre-trained predictor F to that sequence and get the prediction. But can we do better than that? Well, we can do better than that if, in addition to applying what I'll call the base predictor to that sequence in order to get a prediction, we actually apply. We actually apply, we first of all extract the orthologous and ancestral sequences for the human sequence on which we want to make prediction. We apply the predictor to each of those sequences to get prediction scores, and then we combine those scores somehow in order to hopefully get a revised prediction score about the activity of that human sequence. And the way in which we're going to do that is that In which we're going to do that is that we're going to look at how the predicted scores change along the branches of my phylogenetic tree. Remember that we know what the ancestral sequences are, so we can apply the predictor to those as well. And then we're going to ask whether the patterns of changes of the predicted scores is more consistent with this human sequence having the function we're interested in versus not having it. So there's going to be a little bit. Having it. So there's going to be a little small machine learning approach that is going to learn these probability distribution functions here that describe how the score assigned to a certain node, given the score of its parent, how what is the distribution, this conditional distribution here under the situation where the function inhuman is actually functional versus under the situation where it's not. And we're going to simply combine all of the To simply combine all of these scores, which are log likelihood scores, in order to get a revised prediction score that hopefully, everything goes well, would be a score that would be a more accurate prediction of the human, of this function of that human sequence. Long story short, this actually works remarkably well. We've applied this to several data sets of RBP or transcription factor binding. RBP or transcription factor binding. And in this particular case, for example, we can boost the accuracy of a top predictor RNA tracker in 29 of the 31 RBP data sets. And this boosting is particularly strong in cases where we are seeking predictors that have a low false discovery rate, where this is really where this sort of approach shines. So, in brief, here, the really nice thing about Phyllo PGM is that it's a very simple approach. It can be applied to any sequence-to-function task for which any predictor exists. It's not dependent on the architecture of that predictor. And it also works, the stronger selective pressure is, the better it's going to work. The second type of approach that I want. The second type of approach that I want to briefly talk about is called phyloreg. It's phylogenetic regularization. And this is when we're actually going to retrain a predictor. So the idea of a typical predictor is that we want to train some neural network, for example, a CNN or LSDM or some combination of this, to predict to learn this function. Usually that's going to be done by minimizing some sort of loss function. Some sort of loss function that compares the predictions to the actual labels. We're going to complement this with what we call the phylogenetic inconsistency loss, which is going to encourage models whose predictions, when applied to orthologous sequences and their ancestors, these predictions should be as similar to each other as possible. So we're going to minimize, aim to minimize the difference. To minimize the difference of the differences of the predicted functions along all of the branches of our tree under the assumption that function rarely changes. And so if we have a predictor whose functions to whose predicted function is relatively smooth along the branches of the tree, that would be presumably a better predictor than one that where the predictions jump up and down. So by adding this simple term to the Adding this simple term to the loss function and then retraining whatever architecture we started from, we can actually again boost the accuracy of the predictor very substantially, whether it's for transcription factor prediction or for RBP. Now, the last point that I'm going to make, and very briefly, is what happens if the function that we're trying to learn is not the same. Is not the same across the different species that we're looking at. So, for example, there's maybe something that changed in a particular species that maybe a mutation in a transcription factor in the protein that alters the sequence motif that it recognizes. So, how can we deal with this sort of situation? Well, in this case, what we're going to see. In this case, what we're going to seek is not just one predictor, but a family of predictors. We're going to have actually a different predictor for every species and every ancestor. But we don't expect that those predictors should be very different from one another because those changes are expected to be rare. So we're going to have some sort of regularization here where we're going to say we really want a separate predictor for every A separate predictor for every species in our tree, but those predictors need to be similar to one another. For example, the predictor that will be applied to species A is going to be whatever root predictor we had, and then added to the weights of that predictor some delta, some change to the weights of the network in such a way that this is more adapted to that species. But we will want to minimize. But we will want to minimize the total size of those changes. So we enable change, but we penalize it so that we take advantage of the fact that we have actually multiple species to learn from here. So we've applied this to a variety of data sets. Here I'm showing an example where the goal is to predict whether a particular bacterium is going to be resistant to a Is going to be resistant to a certain antibiotic. And there are simple ways to do that that work relatively well, but they don't work well across the entire tree of because certain things have changed. Certain gene families are associated with that trait in some part of the tree and not in others. And so we need to account for this in our model. That's what this dendronet approach does. This dendronet approach does. And actually, because of this, we can improve over the state-of-the-art predictions, which are actually very simple predictors, simple logistic regression predictors, but nonetheless significantly improve over many of those for a large number of antibiotics. And so, there's a lot of interesting things in that direction. In particular, there's also the idea that this might be an approach that allows us to improve fairness in the Allows us to improve fairness in the sense that this predictor would not be biased towards, let's say, species or groups of human individuals, for example, that would be overrepresented in our training data. It also seems like it enables by taking advantage of this evolutionary processes, we can actually disentangle. Can actually disentangle the networks that we're learning and make it easier to extract meaningful information out of them. I'll be happy to discuss that some more later. So, in brief, I think of evolution as a process that is really like a massive experiment that's been going on for 3 billion years, massively parallel. And we get to basically read out the or see what the readout of the experiment is. What is the readout of the experiment by sequencing genomes and maybe inferring ancestral ones? So, if we can learn to interpret that data, we'll be much better off at understanding function. And the steps we've taken in that direction are just small steps. And I think there's a lot more interesting stuff to do. So, the work that I've presented was primarily the work of Faizi Assan, Elliot Lane, Don Junglim, and Isai Camilla. It's been done. Camilla. It's been done in collaboration with a large number of researchers who are listed here. Thank you, everyone.