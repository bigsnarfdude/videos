Okay, that takes it like that. That's the biggest deal. I mean, it's just getting some kind of market.  So now we continue with John Farers for the green tips. Thank you very much for the invitation and thanks to the organizers for spending five years of their life organizing this continent. It is a world record, I'm sure. So I wasn't sure if we should start with team anecdotes. However, I am sure that the meeting is recorded, so I won't say anything. But, well, I will say something. It's impossible not to say anything. But I will start about the this three K step. Um I will start very slow defining the process, then I will go faster. If I don't reach the cruise at the end, I didn't reach the cruise at the end, it's fine. The end, I didn't reach the first at the end, it's fine. This is joint work with Nina Gantel from Technical University of Munich and Dominic Schnitt, who at the time when we started, he was Nina's student and now he's doing a random walk around the world and he's currently in Born and flying let's see what one oh I guess I need to be here. Oh no it's not here. Okay. I will still go back and forth. Flying against tradition. I'll start with the bibliography because, in fact, this is pretty much a complete list of what we used for this paper. We wanted to start this process just because it originated from some engineering and physics papers, but it was in a very particular case and there was no really mathematical papers about it. Mathematical papers about it. You could find something about invariant measures for exclusions in general graphs, so also on trees, from papers of Liggett and Branson and Ligget, which we use to find invariant measures for this process, which I haven't defined yet. You can study SEP on it. In fact, this is another paper of Nina and Dominic, and Nina and Dominique and Chelsea. Then these are the two physics papers that one was the first one, this 2013 one, and this one came in 2020, just after we put the paper on archive. And finally, this is our paper here. So if you have a student who wants to start on something and that's only four papers to read, this is a good process to start with. And yeah, I can give you a list of open questions. Actually, I can give you an extra. Open questions. Actually, I can give you an encyclopedia of open questions. So, this is a bibliography, and I think this is still complete, even though I wrote this talk, did this slide a few months back. Let's go the other way. There we go. So, Tim always complained that he doesn't like too much graphics on my talks. And um, yeah. So uh so let me let me just uh define the the space first. Find the space first. So we will have this task process on a rooted Gafton-Watson tree. Usually people start with deregular trees or something similar. So if you want to think deregular, go for it. We start with the root, that's the initial vertex, and then we create an offspring distribution that creates the tree. There are two caveats: for sure, we cannot have leaves, and for sure... Have leaves, and for sure, we cannot have a one-dimensional space. So, we need a tree. You can still have long stretches that have no branches, that's not a problem, but definitely this is the we need some fatness to our tree to begin with. And then we will put a random environment on this. Actually, there is no distribution on what I'm going to define as the rates, they are just going to be deterministic from the beginning with. With some of it. It's gonna get worse. Just letting you know now. So on each edge, we assign a positive rate that is fixed throughout, and we denote those rates as Rxy, going from parent to child. At the so each of rates corresponds to some Poisson process, so we can do graphical construction here. So, we can do graphical construction here, I will explain in a second. Those are Poisson processes associated with the links, and the little diamonds are where the events are happening. And we also have a reservoir at the root where particles drop in at rate length. Now, I want to define the dynamics. Essentially, if a particle is at location X, it checks all possible edges and jumps from the link to that links first. Jumps from the link that rings first. So you will be like that when they are the link on that edge jumped first, ran first, so the particle just moved down. Now we need an asymmetry, and I put a total asymmetry on this. Essentially, all the particles will be moving away from the root at any given time. So it's not a reversible process. So this guy decides to move. If the root is empty, then particles will just Then particles will just come in and drop down, appear at the root, and then start their own little walk down the tree, and then they keep going. It's going to go for a while or just disappear. Just in choice. And I hope it stops. So this process was down the tree. And so far, I didn't say anything about exclusion, I will in a moment, but jump attempts happen in the But jump attempts happen independently. There is no visible order to jumps unless you know all the Poisson processes, which is possible. And it is very problematic, but unfortunately true, that a particle that comes much later might find itself deeper into the tree than the particle that came in first, for example. Because it all matters in which kind of sub-tree or sub-branch they go down, and the rates are still not determined. Determine. So now I also want to put an exclusion dynamic. So all particles and the erratic jumps are subjected to the exclusion rule. So for example, let's focus on that sub-branch layer. Here we have a possible arrangement, right? If an edge rings while the parent is occupied but the child is empty, the jump is performed. The jump is performed. So, for example, that classical there has no problem going on on that vertex. On the other hand, if an edge rings while the parent and the child are both occupied, like that one over there, then the jump will be suppressed. And in this arrangement that I have here, actually there are two attempts going on the occupied vertex, but in the end, in this particular arrangement, that has to go will go down the other way. So blocking here can actually spread the particles. Can actually spread the particles and it can create something like the arrangement we see over here. Everybody okay with the process? Okay. So I want to put some assumptions on the rates because first of all I want to make sure the process exists. But let me explain all the problems that we had and what assumptions we needed to put. So first we needed to make So, first, we needed to make sure that the tree got visited, so there are no undesirable roots. And that came in the form of a uniform ellipticity. These are both rates going from X into its children, and essentially the smallest of those ratios, no matter how you take the ratio, needs to be just larger than some epsilon that will appear in all theorems, essentially. Then we also need to make sure, since we are looking for current, we need to make sure that actually particles go through at the time scales we are interested in. If you think of a Galton-Watson tree, the nth generation is actually quite chubby. It's exponentially large. So we need to guarantee some current going through, at least in exponential time scales, and that means we need to bound our... That means we need to bound our rates from below so they cannot go to zero fast. Well, at least they cannot go to zero faster than some exponential. C low is up to us, kappa is up to us, but they cannot go down more. And finally, we do need the process to be well defined. And in fact, in this case, we can actually have it feller. And the assumption is that the maximum possible rate out is bounded. So I'm not going to have increasing rates, I don't want my particles to split. Increasing rates, I don't want my particles to speed up. I actually want them to have a constant speed or stay or get slower as we go down the tree. So these are the three assumptions on the rates. And away from this, the sky is the limit. There is absolutely no problem what kind of rates you want to decide to use, which we really wanted because there was a drive at Sussex to do stuff with applications, and I can sell this, that I'm doing traffic. Um you know, nobody's buying, but it's okay. So the thing here is to have four examples in mind. These are the examples that we actually can do a lot of computations and say something very specific about currents and invariant distributions. So I want a deregular tree. So d means the degree is d. This is a binary tree. So actually the children are d minus. So, actually, the children are d minus one. And this is the very good example where we have what we call a homogeneous flow. Essentially, the sum of the outflow is the same as the flow inside, that's the rates. And as you go down the tree, they go down exponentially fast. Then it could be the constant rates, rate one everywhere, which is a pretty fast tree. Somehow particles jump a lot. Somehow, particles jump a lot. And then there is the polynomial decay. So there is decay as you go down, but it's not as hardcore as exponential. And they go down like, let's say, n to the p. And finally, there is what we call slow, which is the same flow as before, so the 1 over 2 to the n, for example, but we multiply with an extra function g, and this function actually goes to 0 as you go down the trick. Goes to zero as you go down the tree. So it's less optimal than flow. It slows back down. And in all these four examples, invariant distributions are different. We can say stuff in all these examples about them. And actually, I know this is not really a Dustin-Watson tree, it's just the regular tree, but it's already complicated enough. So what I'm going to try and say here is about currents down this tree, but I understand people like a lot of the environment. I understand people like a lot the invariant distribution bits, so we can talk about that at any time since we're here. Ah, you want to know that? That's fine. So the first thing I want to say is about what we call a disentanglement of this process. And as an initial motivation, this started as a curiosity, right? We were like, but isn't this supposed to be true? But in the end, it was really important that we obtained this result. Than that we obtained this result. If we completely ignore the exclusion rule and we just let the particles go down, essentially they are doing random walks down a tree. And if you are doing random walks, you sort of expect that the first n particles will be in separate sub-branches by the time they reach a generational constant logarithm of n. They should spread out. If you think that this might be true for exclusion, That this might be true for exclusion, it's actually quite important because the moment these first n particles are in separate sub-trees, there are going to be leaders there, and there is nothing in front of them that does blocking. So you start from TASEP, then you spread out, and then you just do random walks, which are easy to manage, if this is true. And this is the question that we have. What happens when we actually have the explosion? Will the particles push themselves out? Push themselves out, or can they create fabric corridors because they are faster compared to the other raids, and then the blocking will persist deep down the tree. And deep changes based on the use, essentially. So let me explain this a bit. If I am in a situation that I don't have too many particles inside the tree, the probability that actually two particles follow each other That actually two particles follow each other for a while becomes exponentially small, so they will disentangle. But if we are in a situation like this, here and here you have blocks, this rate and this rate can be small, so nothing moves, and then this little guy will be essentially forced to come here, but here you have a stretch with no branches, and it will follow that guy over there. So somehow exclusion will persist a bit. Will persist a bit longer, and then you can have these geometric length traps that you cannot escape until you reach an exit somehow. So on the other hand, if we are in a situation as you see in the shape there, already there is some disentanglement happening. This guy is the leader in its sub-branch, saying here, here, and there. And now we will completely have separated the particles, decoupled. Separated the particles, decoupled the trajectories, if that guy over there decides to jump to the correct spot. So if it goes there, then we completely separated all the particles. None of them is blocking each other at all, and they just go down like random walls. So if I call the generation M tilde N the generation that I can guarantee that my particles are leaders, all of them, the first n particles. All of them, the first ten particles, in this picture it will be three for the first six particles. Now, this is a random generation, it depends on the tree, it depends on the rates, so that's less than optimal, but in fact we can find deterministic upper bounds for this, which we call Mn. Those are P almost surely for the first n particles. First empathy goals went to be a generation that they are already leaders in their respective sub-trees. Now, this almost surely here, it really means with respect to the tree measure and with respect to the tasia, and these are product. So, let me show you the theorem. It's not pretty, but it's true. You know, it depends on your personal viewpoint. Let me try and explain. Let me try and explain. First of all, there is this magical generation dn that guarantees that after it the rates are small enough. Now, dn could be infinity, right? There is no reason that the rates behave the way we want to. However, we can decide what dn is just by properties of the tree and the rates. The C low there comes from the rates, the c mu is something about the tree. The logarithm Something about the tree. The logarithm cube over there is just so that the estimates are Borel-Cantelli ready, but there is no reason for the logarithm cube, really. So that generation can be computed just by the tree. And now we need to decide what this DN is and how it grows. And there are two regimes that are of interest. If Dn doesn't grow more than constant times logarithm, then the particles have already set. Then the particles have already separated by dn, but dn is like log, and this is another log. So essentially, particles decouple by generation log, like they would do in random walk. But on the other hand, dn can grow much faster than logarithm, depending on what rates you put, like polynomial, for example. And then the separation generation is going to be the minimum between d and n times some constant that again depends. Times some constant that again depends on the tree. And there is, okay, you guys notice that there is a lim soup there and a limit here, there's something in the middle. And the thing that is missing says that on subsequences it will do that, and on other sub-sequences it will do that. However, this we can compute, this generation MN exists and particles after it will do random walks. And this is important for bounds because we cannot use state set to bound things if the rates become very, very small. If the rates become very, very small. It is just fine. Okay, questions so far? Okay? But there is some randomness in the end after which generation it actually works. Right, so in fact, yeah, that's correct. So this is for n large enough. But well, originally we wanted limits, so we thought it doesn't matter. But yeah, you're right. Anything else? Okay. So let me let me talk a bit about currents. Um I'll state the theorem here. This is not the true theorem. This is just looks compact and pretty. But essentially what it says is that if I want to do an aggregated current across a generation, I can actually find the time window T low T up so that before T low Before T low, no particle went through that generation. So it looks like the shape over here: the green is where the particles are, Ln is the generation that I care about to find the current. At T low, no particle passed. But at T up, we have essentially a linear order current across that generation. There is nothing special about linear, especially since this time won't necessarily be linear. Won't necessarily be linear, but at least we have some measurable current across LN. And the delta matters, depending what delta you want, there changes the T outp. So what's the true theorem? I need to define five different functions of the rates, and they are all very special. So there is the R mean, capital R mean, R max, and rho. Dalar mean, R max, and rho L. This I can at least give you a heuristic what each one are. So, in expectation, that would be the slowest possible trajectory on the tree. Essentially, at each generation, you find out what is the smallest possible rate. You use that, and then you add the map up to level M. So, in expectation, that's how long it takes a single particle to move, and that's the slowest possible trajectory. And the next one. Trajectory, and the next one is the fastest possible trajectory if there is no blocking and you have a single particle. And this one here is the smallest, fastest node because it can create delays, especially if you want your nodes to output particles fast to make some counting. So that also comes up. And then there is this more esoteric function. So theta is just that link inf, and theta n is And theta n is any sequence that goes to zero as long as this limin is actually infinity. So that's the actual theorem. What is the time window? And again, it doesn't look great because we needed to keep the tree as generic as possible. But in the end, t low is going to be the minimum of two times, and tab is going to be the sum of a bunch of times, depending if theta is infinity or not. If theta is infinity or less than infinity. And you can see that in these times here, the generations of interest appear. Ln shows up there and also implicitly in there. And here you see the disentanglement generation mn. And again, these bounds are for n large enough so that mn is disentanglement. But we can actually put, we can work on the event that this is true when the probability is true. That this is true when the probability is anyway quite high. So let me tell you what happens at least in the deregular trees with those symmetric rates that I said earlier. If we just have a flow and the outflow is the sum of the inflow and Ln is just a logarithm multiplied by a constant as long as it's just larger. Constant as long as it's just larger than the disentanglement generation. Then Tf and T low look like this. It's essentially some power of n. And while this is the case that we can say the most about the equilibrium, actually the T up and the T low don't match at all in order. So even though we think they should, and actually we think they should be the correct one somehow. For constant rates, so rate one everywhere, if we, so mn is going to be of order n. If we look at ln a bit later, both t up and t low are of order mn, which is order n multiplied with some constants. So at least there we have the correct order for the time window. It matches at least in order one. The stronger results come when you have polynomially decaying rates, plus this two. Plus, these two extra limits existing, they don't need to exist, right? LN can be anything, but if you choose LN so that those limits exist, there are two regimes. If B is zero and A is in zero, one, then T up and T low match precisely, at least in the first order. So it's going to be just the generation to the power T plus one. And if B is not zero, then they match, but up to constants. So T up. Up to constants. So T up will look something like this, which is a bit annoying, but T law will also look something like that, and they are comparable, at least in the first order of magnitude. So somehow the time windows are good in the sense that we can match at least leading orders, but we cannot match constants necessarily yet, and we cannot even guess what happens in lower orders. Okay, so let's see. So I think maybe I have like ten minutes. Six minutes. Okay, let's see how far we go. It's okay. You can stop me anytime. So I'm going to tell you the couplings that we are using to find these bounds. And in all systems on the trees that I'm saying, all the trajectories are coupled from the beginning. And all the Coisson processes on the edges are coupled. Processes on the edges are the same in each particle system discussed. So for an upper bound, here you see three t-seps, that's TT and random walk RW. And what you see in these J's over here is actually currents across generations. So if we start from this initial condition, for example, this is the current that we have, essentially, you count the particles in front of you. And I want to make a faster system to check the current. And the faster system I'm doing is just removing the exclusion rule and just doing random work. I need to alter the rates a bit to have inequalities in distribution, but in the end, particles in both systems are coupled via canonical coupling, and they are moving one One with the exclusion rule, the other without, and at any given time, at any given jump, the currents in the random walk is larger. So we have that the random walk current will always be faster than the TASEP current in this case. And because that's an upper bound, if it's zero, then we can actually say that the current in TASEF is zero. So that's what we are using for T-low. That's what we are using for T low. So our m is going to be the ln that we care about, and t will be t low. And we want to try the maximum possible time that the random walks don't reach generation ln. And here's just the technical lemma, which is not horrible. Essentially, I want to bound the probability that 1 over Ci omega, if I add all this If I add all these exponentials, it is less than t. Now, that bit is actually not used for the upper bound for t low, it's used for the other one. So, don't worry about it. There, the omega i's are going to be iid exponential one, and the c i's play the role of their rates. The c that you see on the right-hand side is just the minimum of all possible rates, and the s is just the sum of the inverses, so it's just an expectation. The inverses, so it's just an expectation, and that is true for n delta in 0, 1. But this is an upper bound essentially for just sums of independent random variables. And the proof either comes from Chebyshev or from Chernov. And then to go to T low, we just use this bound and we count how many particles are in the system by seeing how many times there was a chance. Seeing how many times there was a chance that the particle was put at the root, and then we do a union bound over all particles. So it's pretty coarse, but it actually works. And for the lower bound, I want to create a slower process. So the original idea was, well, let's ignore the tree completely and let's pretend that we have a 1D taser. Except we needed to make sure that particles wouldn't bypass. To make sure that particles wouldn't bypass each other inside the tree. So we enforce an extra condition if we want to use the 1D KSEP that you cannot go to a generation if there is a particle in front of you. So it's really a one-dimensional KSEF if you cannot go to a generation unless you have space. But that makes the system remarkably slow. So that's where this disentanglement generation comes in. This disentanglement generation comes in. If we work on the event that they disentangled by that generation, then we can do tase up to this point, then have the particles chill out here until everybody reaches this location, and then have them do random walks. And this is a slower process, but not a horribly slower process, because here it's actually a pretty decent estimate. Here it's horrible. But we can connect to a quarter growth model where on each diagonal line are the slowest rates in each generation. And then we can try and understand that polar growth model and the current in this particle system is actually less than the current in a tree taser. So if this is of order n, then this will be of order n. Then this will be a water. So here is hopefully my last slide. I want g to be the last passage time in that parallelogram, but this is an inhomogeneous passage time, and these can be very, very small. So if you're trying to find out the maximal path, for example, it could look something like this. But if you have exponential decay on the rates, it most likely will look like something like this. These are the heaviest one. If this is about logarithm size, Is about logarithm size, this will be of order and they are very heavy rate, so that's where all the delay happens. We can do just a little estimate and say that anyway, in this arrangement, the passage time will not exceed a certain value. And after that, we reach the good location where everything is disentangled, and then we just do random walk. So, T app we just set to be the passage time plus the random walk time to reach the. Plus, there are no work time to reach the generation we care about. And that's how we find it, actually. And I understand they both look very coarse, the lower and the upper bound, but they agree in so many cases, so it's actually not wrong. And I guess with this, I will stop. Thank you very much. Thank you very much for a nice talk. Nice talk? Is there any question? I can give you a lot if you want, but yeah. What if you let the particles run all the way to the boundary of the tree? Do you see any interesting mass distribution forming there or something under the? The tree is infinite. Sorry? The tree is infinite. Yeah, yeah, I know. But if you let them down. So, yes, in fact, you do. You do. So, for example, in the slow rates, you have a complete blockage in the limit. So, the invariant measure will actually be delta one. In the case of low, you have IID Bernoulli, and those are extreme up. In the case of the fast tree, you can prove that the invariant measure has no density, essentially. So, they they really go down and they disappear. Like a tractor, you understand? Yeah, so it depends on the rates you choose, but all the cases are there. You can have all of them. There, you can have all of them. How long did it take you to make these slides? Actually, not that bad because I go to schools a lot, so I need to make these slides to keep children entertained. And then, after you know how to do them, it's really fast. Now, keynote takes latex, it comes directly from papers, it's it's really fun. Why are you asking? You don't like graphics. Yeah. Yeah. Any other questions? You mentioned this uniform heliocity assumption that you had, and I saw that this epsilon appeared in some of your quantities. Do you have any indication as to whether or not there's a way to weaken this? Okay, there is a way to weaken this because all the estimates are in finite n essentially. So you can actually tweak it and let it go to zero with some speed g of n. With some speed g of n. I don't know what happens there. It's quite possible that you actually kill parts of your tree, but that would just create invariant measures that have regions that are zero. I don't think that's bad in any way. So yes, you can actually try and follow all the estimates, and it can lead to some interesting and homogeneous behaviors, I think. You cannot completely ignore it because if you do. Ignore it because if you do, then you can actually put zero somewhere and that's cutting off the whole sub-tree. And then you don't really need to do that. But yeah, it's possible. Yes. Are there special cases where you have explicit invariant measures of except the flow, you mean? Except the flow. So in the flow case where the inflow is the same as the outflow, I see. Bernoulli. I see. But in the other ones, if it's delta 0 or delta 1, we can still get it. And anything else in between, we don't know. We cannot construct the other ones. Just then what? When you have this saturation, the holes applied anyway. Correct. Okay, so that is the law of dots. So if you have if you have a lot of. So if you have if you want to put holes at the at your t is infinite so you have holes at infinite coming down. You mean uh moving up. Yeah, yeah, that's right. So well, okay. So we actually use this fact to always find how many particles you have in the tree. Now the tree is very fat and particles come in at a linear speed. So if you Linear sphere. So if you actually want to see the holes disappear, you need to wait really at time infinity. There is no other way. Because, yes, otherwise you will always have collapse. So when you actually start having blockage, they just come up slower and slower. Now that's what happens. But we didn't prove that, right? It would be interesting to prove that. Actually, following the host backwards, I don't think it's an easy process. It's an interesting process, but I don't think it's easy. Because you create a bottle of text. Thank you. More questions? So we have five minutes no, we have more and like the shows. Sure.