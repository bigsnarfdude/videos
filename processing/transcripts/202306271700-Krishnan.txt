This connection between Thomson Monoid F and Markovianevi, Slide Better. And this is joint work with Klaus Koessler and Stephen Wills. The talk is on the following connection. We look at the connection between representations of the Thomson monoid F, which is given here in its infinite presentation. So it has generators G0, G1, and so on. And they satisfy the relations. And we satisfy the relations gkl is equal to gl plus 1gk whenever k is less than l. And if some of you have seen the Thompson monoid or the Thompson group before, you might notice that these are the relations satisfied by the inverses that is usually given in the presentation that we're usually used to, but this is what we've involved with. And we look at the connection between representations of F plus in non-commutative probability spaces. Non-commutative probability spaces with unilateral non-commutative stationary market processes. And that's a mouthful, but I'll try and explain what that is: what a non-commutative stationary process is and what it means for it to be market. And the connection is via something known as partial spedability, which I'll talk about at the very end. So the reason we started looking at this was that there was this other monoid, which is sometimes Monoid, which is sometimes known as a partial shifts monoid. I write that here as S. This also has this infinite presentation with generators H0, H1, and so on. And the relations are really similar. We have HK HL equal to HL plus 1HK, except that these relations also hold when K is equal to L. So there's an additional set of relations. It holds whenever K is less than or equal to L. And so this partial Schipzmonide is just a quotient of Is just a quotient of the forms of monide that these additional relations. And in work by Kusla and Evans, Gohm, and Kusla, they found this connection between representations of the partial shift monide with unilateral non-commutative Bernoulli shifts using this distributional symmetry called spreadability. And that's where the motivation to define partial spreadability and look for this connection came from. This connection. The Thompson group F was one of three groups introduced by Richard Thompson in 1965. F was described then as a certain subgroup of piecewise linear homeomorphisms on the interval 0, 1 that satisfy some additional conditions. But we're really only working with the presentation where we look at the relations satisfied by the generators. And as I said, the relations are these. I said the relations are these. GKGL is equal to GL plus 1GK. So the generators don't commute, but you have an increase in the index of the larger generator. So GKGL becomes GL plus 1GK when K is less than L. So just for example, G1, G4, would be G5G1. And what we found was that these relations seem to encode Markovianity in the context of non-commutative probability. In the context of non-commutative probability spaces. And conversely, when you have Markov processes in certain contexts, we're able to get representations of F. So I'll start by defining some of the things that were mentioned there. We'll be working in the setting of a non-commutative probability space, which we sometimes write here as NCPS. We'll always have a pair m psi. pair m psi m is the neumann algebra and psi is a faithful norman state on m and whenever we consider maps endomorphisms and homomorphisms on m psi or between non-commutative probability spaces we want that they are state preserving so we really want stationarity and i'll i'll talk about that when i define what an endomorphism is and what a random length is but just to put uh The context of a classical probability space into this setting. If we have a probability space given by omega, a non-empty set, a sigma algebra sigma, and mu a probability measure, we can algebraize this probability space to get L, L to be L infinity of omega sigma mu. And then L is a commutative for Neumann algebra. And we can define a tracial faithful normal state on L given by Normal state on L given by the integral with respect to the probability measure. And then this pair L trace mu is a non-commutative, well, it's a commutative non-commutative probability space, but it's algebraized to fit this definition. And when we say an endomorphism alpha of a non-commutative probability space m psi, we mean a unital star homomorphism on m, which satisfies two additional conditions. We have scale. We have stationarity, so the state psi composed with alpha is just psi. And we also have that alpha commutes with the modular automorphism group sigma psi t. This is a condition known as modular conditioning, and the idea is that this will ensure that certain conditional expectations exist. And often we'll just work with a traitial state, so this condition two will be satisfied with it. Can I ask, is this guaranteeing that you're dealing with a normal star? That you're dealing with a normal star monomorphism? The one that preserves the prima of projections. Alpha, is alpha normal? Yeah, so are you assuming that your endomorphisms are going to be normal star homomorphisms? Yeah, I think these two conditions are not. It's normal. There are things more likely over the time for extreme fat for time because it wouldn't be a federal. Yeah, sorry. But for the second point, issues because you think more for type three parts or you wouldn't need it for one part. Yeah, it's just sort of a technical condition to ensure that we can talk about conditional expectations, which is what we use to define what we're talking about. One, the only one first. And also the stationality condition and the fact that Psi is faithful. That psi is faithful will give that the endomorphism alpha is injective. That will also follow, it's always injective. And we just write the group of endomorphisms of m psi as end m psi. And I'll define what a random variable is in this context. This is also just a suitable algebraization of a random variable as we have in measurable classical probability theory. probability theory. We have A Phi and M, two non-commutative probability spaces. A random variable will be an injective star homomorphism iota zero from A to M that again satisfies stationarity. So psi composed with iota zero is equal to phi. And we want the subalgebra iota zero of A to be psi condition. So that is the unique normal conditional expectation from phenomenal algebra M onto the subalgebra iota 0. Onto the sub-algebra, iota 3 of a exists and it is also state resolved. So, this random variable in the commutative context will correspond to composing with a random variable in the classical context. A few more definitions, and we'll get to what you result. So, a unilateral non-commutative stationary process. We want to define what a Markov process is, but we start with the stationary process first. Spatially processed first. We have a pair, a non-commutative probability space m psi. Then we have this generating subalgebra A0 on which we're going to act an endomorphism alpha. So the stationary process will be this whole quadruple M psi alpha A0, where M psi is a non-commutative property space, so phenomenal algebra with a faithful normal state. A0 is a psi condition sub-algebra of M, and alpha is an endomorphic. And alpha as an endomorphism on end cell. You require that alpha of this time enough which leave A ring variant or it won't require job? Do we need that alpha is leave the H0 in variant? No, no, in general it won't. In fact, we could have that this process, we call it minimal if the action of alpha on A0 fills up the whole space. It doesn't have to restrict to A0. To restrict to exit, and there's always this: there are two ways of thinking about a stationary process. You can think about it in terms of just quadruple or in terms of a sequence of random variables that is stationary. I'll just say something about stationary in a minute. But what we can do is to get a sequence of random variables, iota n, going from A0, the subalgebra, to m, where iota n comes from powers of alpha acting on iota 0. On out of zero, and next draw m psi, and then the endomorphism alpha, and then we have zero, just take psi restricted to zero, this color, and then we have just an inclusion level, i zero. And then I take powers of alpha, let's have Of alpha, let's share iota. So iota n will then be a stationary sequence, and this has a meaning if you look at phi, the state phi here, or psi, sorry, state psi, really encodes what joint distributions are. So stationarity has, you can, there's an equivalent definition of a stationary sequence that can be given in terms of joint distributions, which essentially says that the joint. Essentially says that the joint distribution of a sequence is unaffected if you move the entire sequence by some fixed quantity n. So that's a stationary sequence and that's a stationary process and you can go back and forth between them depending on what context you want to work in. And okay then now we can say when a stationary process is a marked process. So quadruple m psi alpha a0, which is a unilateral Which is a unilateral. So, the fact that it's called a unilateral stationary process just means that alpha is an endomorphism and not necessarily an automorphism, which you can make the exact definition for a bilateral non-commutative stationary process where alpha is an automorphism. And in that case, you can get away with defining an automorphism because you have nicer things. You have the existence of an adjoint, for example. So the modular conditioning can be described in a different way. Described in a different way. But in this case, we're working with a unilateral setting. So we have this particle m psi alpha a0, and we say it's a stationary Markov process if the following is satisfied. So this is the Markov property that we want satisfied. So what we do is to look at the sub-algebra, the phenomenon sub-algebra A0N. And this is the phoneme subalgebra generated by the action of alpha on A0 with powers of alpha acting on A0. So alpha to the i, but So alpha to the i, but i lies between 0 and n. And we think of this as the past, so the time before n, time n. And then this is the future Fonnet sub-algebra. So a n infinity is the Fonuman sub-algebra then related by alpha to the I acting on A0, where I is greater than or equal to n. And then we have the present, which is just the action of alpha to the n on A0. And because of all And because of all the modular conditioning and everything we've assumed, we have the existence of these conditional expectation EI onto each of the AIs. The what, sorry? Yeah, it's just some, yeah, big V on data. The phenomenal algebra generator habitat is what I mean. I'm not sure that that's super standard, but it's what we used. But yeah, we're always talking about. But yeah, we're always talking about these are all for Neumann subalgebras generated by those by alpha to the i of A0. And so the condition we want satisfied for Markovianity is the condition as we know from classical Markov chains, which is that, well, in some form of what we know in the classical setting, which is that the conditional expectation E and infinity, which represents the algebra of the future acting on Acting on the algebra of the past is the same as this conditional expectation E and infinity acting on the present. And this slide was meant to say. So I could add here a composition with E and infinity, and it would be the same. So the future only depends on the present and not on the past. So that's what a stationary Markov process is in this algebraic setting of Algebraic setting of non-commutative probability spaces. I'll say one last thing about it, and I'll fill in this picture. So, if you have a stationary Markov process, in general, by that I mean a non-commutative stationary Markov process, m psi alpha A0. Iota 0 is the inclusion map of A0 into M. We can talk about the transition operative T. I'll just fill that in here. We always guarantee the existence of. Is guaranteed the existence of the adjoint of the injective star homomorphism iota zero. And then what we get here is zero. And well, I'll start with saying that t is iota zero star composed with alpha composed with iota zero. So it actually turns out to be a dilation in the sense that this diagram commutes for all n. For all n. And so, can you clarify a little more what the map ion stars, ion zero stars, is doing? Well, it's the what we have is that so iota zero scale, iota zero would just be the identity. So, is it like a conditional objection? Yeah, so iota zero, iota zero seven would be the conditional expectation on the Expectation on the image of five years ago. And so this is a transition operator as we know it as, you know, we have the transition matrix in the case of a classical Markov sequence. And the Markov condition will actually guarantee that alpha is a dilation of t in the sense that iota 0 star, alpha to the n. That iota 0 star alpha to the n iota 0 is p to the n for every n in n 0. So it really is a dilation. And in this context, also, we can talk about the associated stationary sequence of this stationary process. So we get this sequence of random variables defined the exact same way as before. iota n is alpha to the n of iota zero, and that would be called a stationary marker sequence. So, really, what is so? I should say, I should have said at the beginning that. Say, I should have said at the beginning that all of this terminology comes from humoral. So, I think there are different ways of talking about malco dilations. And Radharambhat, for example, uses something else I'm not very familiar with. But all of this is the terminology as used by Kimeral. And classical model sequences will fit into the picture by algebraizing as shown before. Anyway, so that was to get some of Get some of the definitions under the way that we'll see. But the main thing to take away is this conditional expectation of the one which is set is conditional expectation on so the first thing that we have is that if we have representations of Representations of F in a non-commutative probability space, then we get a whole family of unilateral non-commutative stationary Markov processes as you were just defined. And so the way we arrive at these Markov processes is by looking at fixed point algebras. Suppose we have a representation rho of F plus into the endomorphisms of a non-commutative problem space. Then we just write rho of g n is alpha n and we look at the fixed. And we look at the fixed point algebras m alpha n of each of these endomorphisms. M alpha n is x and m such that alpha n of x is equal to x. And then we look at the intersections of fixed point algebra. So mn would be the intersection of m alpha k probably greater than or equal to n plus 1. In many cases, mn will just be fixed point algebra of alpha to the n plus 1 in some nice cases. This gives us an increasing tower of the Neumann sub-algebras at zeros. And I mean subalgebras, m0 is contained in m1, mean m2, and so on. And what we get is the following: suppose we have this representation row, which with, as I said, alpha m is rho of gm. And if you look at the intersection of fixed point algebras m0, then the quadruple m sine alpha 0 m 0 is a unilateral station Markov process. And there's nothing special about zero here. We get We get a whole family of stationary Markov processes. So, in the same context where rho is a representation, you get the morphisms of m psi. Mn is the intersection of fixed point algebra, m alpha, k for k greater than input n plus 1. Then this quadruple m psi alpha mn is a stationary Markov process whenever n is less than or input. And the way we arrive, we're able to show this Markov property, so we're using the relations of. Property. So we're using the relations of the generators of F plus in combination with a form of the mean and product theorem. So we're able to put these together to get this Markov condition that we're looking for. So this is in one direction. This is for a general non-commutative probability space. We're able to arrive at starting with the representation of F plus, we get stationary Monco processes. And then the question is, if you have a stationary Markov process, And then the question is: if you have a stationary marker process, can you always get a representation of F? And we have a partial converse, but before that, I'll go over an example which is perhaps more important than the converse because it's really illustrative of what happens there. So we start with two non-commutative probability spaces, A Phi and C chi. And then from them, we can build a larger non-commutative probability space. So M is the phenomenon algebraic dense product. The phenomenon algebraic dense product of A, with C, with C, and so on, and which is which comes from the product state psi, which is phi with chi and chi, but these are the states corresponding to A and C. And what we do first is define these partial shifts, beta, k, and we define them on elementary tensors as follows. So the zeroth partial shift, we leave A as it is, and then we start moving to the right. And then we start moving to the right. So we start moving x0, x1, and so on to the right. And where there's a gap, we fill in the identity. And in general, the kth partial shift, we leave everything fixed until k minus 1, put a 1 in there, and shift everything else to write. So these are just known as partial shifts. And then one to check that these beta k's will satisfy the relation beta k to l as beta l plus 1 times k. This is actually true when k is less than or equal to l, including k equal to l. Including k equal to l. So define the representation of f plus, but also representation of monoid x plus. It defines more. But what we can do is to aware that I only have four minutes left, so I'll probably stop with this example. But what we can do is to perturb these shifts. So by that, I mean the following. We have random variables which are just injector star homomorphisms. So C goes from A phi to this tensor product and B goes from C column. Tensor product and B goes from C chi to this tensor product. They're injective star homomorphisms that are stationary and so on. And then we define alpha n's on the endomorphisms of xi as follows. So it's like the partial shift, but where we have a little perturbation in the beginning. So where we had in the previous case just A tensor one, we have C of A, which could be anything. So we're perturbing the beta zero from earlier and then shifting everything else to the right. And similarly with Right. And similarly, with n for any n larger than or equal to one, we're shifting everything after perturbing by d. And despite this perturbation, it's still true that alpha k alpha l is equal to alpha l plus one. And in general, this will not be true when k is equal to l. So in full generality, that may not be true. So this gives a representation of f plus in the endomorphisms of m s. side and what we can do is to write alpha n as this perturbation multiplied by beta n so in that sense it perturbs the partial shift beta n and this is known as a coupling to a shift okay beta n is a shift and this is known as a coupling that is that these joint endomorphisms beta n exist again as a consequence of having that modular condition so uh the thing about this example is that it really gives us That it really gives us the converse in the sense that you do get a representation of f from a stationary Markov process if you have that denser product construction. And it turns out that that's always true in the classical case, which where we have this function converse. So if we have a stationary Markov process, but in the classical setting, so the phenomenal algebras are all commutative, then we get representations of F plus. One last picture of a dilation, and that essentially comes from. Dilation and that essentially comes from this result that says that have the transitional operator corresponding to some Markov process here. You can dilate this to the tenth product of A with an L infinity space. This would be the trace coming from the data metro. Coming from today, so there's always an alpha there which dilates in the sense that we roll. So pay tensor L. So you can always lift the Markov operator to a dilation on a tensor product. And this is something that we have in the classical case, and it's really not clear whether that should hold the problem of general case. The result we get by using this is that if you start with a probability If you start with a probability space where A is commutative and R is a transition operator on A coming from a Markov process, then there is a probability space m psi. And that m psi would really be a tensor product construction like we saw in the example. So we start with this and then enlarge it to that infinite phenomena by tensor product we had there. And from there, we get a representation rho of F plus into the endomorphisms of M psi such that the image of The image of A will be the fixed point algebra of rho of g1, and rho of g0 dilates the Markov operator with which we start. And what this gives us is that in the classical case, a transition operator coming from a Markov process can be written as a compression of a represented generator of F plus. So I'll stop by saying what we have all together. So we started with a non-commutative probability scale. A non-commutative probability space where the probability space is commutative, but in that asteroid form. And we apply this result as we have partial converse. So from here, we would get a representation rule. And then if we use theorem one to take that representation and construct the stationary Markov processes that we got there, we would arrive back at the Markov process with which we started. So in the classical case, the result is really complete. And in the general non-commutative case, we have General non-commutative case, we have one direction, and in very specific cases where such a tensor dilation is possible, we can talk about the compass. That's 18 or 23, but I will stop here. Is there any questions? Do something similar with the other Thompson groups and monoids. I haven't seen them at all. I haven't seen them at all. I mean, I know there's B and D, but I don't know how they're represented as. Do you know how you write them? But generators of relationships are, you know, I know that they're given by homeomorphism as well in the circle, but I'm not sure how they're written as generators of relation. And that's really what's used yet. So not something I've looked at yet. So, sort of almost to a follow-up, because I was curious about that too. But from your results, at least it seems to be. At least it seems to be, at least in certain special cases, these Mountain Mockov processes, they really are, in a sense, coming from representations of the Thompson monoid F. So in a sense, if the sort of converse extends more generally, then it becomes a moot point to ask about the other monoids, does it? Because this would completely encode one reality, perhaps. Mark of reality, but perhaps one needs to look at a different property of the sequence of running variables. So, here they seem to encode markov sequences. Also, the converse is really, it's not really clear what you would do to get the stensor product kind of construction. There is some sort of idea that you can use a three-product construction, but I'm not really sure. We haven't really progressed much with that. Yeah, and anyway, I also wanted to say that everything here was about F plus, but a lot of the results will carry forward to the thought. But a lot of the results will carry forward during the Thompson group and bilateral model processes as well. The strength of product construction will change a little bit, the one that we saw in the example, but at least broadly, the results are also true for F and connecting them with bilateral non-core processes. But B and D are something I really should look at. I really don't know anything about them. The interest here was the fact that they have these, that there is this presentation with generators in relations and satisfying these really nice looking relations, very easy to work with. Of innovation is very easy to work with at some sense. So, can I? You took what I would call the usual Thompson monoid and you took the inverses. Yes. Why is that more natural? Well, it was really to fit in with the partial shifts monoid because that so there is this notion of sequence of random variables being spreadable. Being spreadable. Just means that if you have, yeah, I didn't say anything about partial spreadable, but you have a sequence of unknown variables. Then you can talk about what spreadability means in terms of the distribution, the functional side, the state side. It means that This joint distribution is the same as, well, I just use the different three and one where we say that and some subsequence. So just as a distribution is unchanged, if you pass through a subsequent. Is unchanged if you pass to a subsequence of the sequence that we started with. And what Klaus and Rohlf and Greon found is that this distribution of symmetry, which just describes, it just describes a sequence in terms of these linear functions being equal to each other, that is the same as having a representation from the partial shifts monoid S plus satisfying a bunch of conditions. So the idea was to just use S plus and you. idea was to just use S plus and embed it into F, but that required the inverses to be used. So it was meant to be in parallel with this. And all of this comes from Spread of Hand. And yeah, there are these theorems in classical probability, the Finette-type theorems, which relate various kinds of distribution symmetries. So spreadable sequences, exchangeable sequences. Exchangeable sequences come from representations of the infinite. Sequences come from representations of the infinite symmetric growth and so on. So, the idea was to use that kind of definition, but by replacing the partial switch monoid with the Thompson. But what they didn't know was something they noticed that they didn't know it as Thompson monoid, but then the relations are very similar to those of the Thompson monoid, and the leader is going to be true to UZ versus C. So, that was why. So, can you move back to this slide a while ago? This sequence. Slide value this sequence of m 0 and 1 and 2 and the sequence of phenomena are trust. Yes. That corresponds to the Jones construction. The Jones construction of when you have an inclusion and you get power. Not sure to be honest. Sure, to be honest, yeah, it gives an increasing power of these of these intersection of fixed point algebra, but I'm not sure how it connects there. But you do end up with a lot of commuting squares that's very very corresponding. Yeah, I mean the fixed condition is the joint construction with that fixed point out of that. I will ask you about those after. I will ask you about those after. Any question? Let's thank our speaker. So for the three afternoon tomorrow, it will be 