I just wanted to thank the conference organizers and the workshop organizers, in particular Marco and Jade for inviting me. And I'm really sorry I couldn't be there. I'm currently up in La Palma in the Canary Islands building a telescope, which I'll be talking about shortly in preparation for the upcoming O4 run, where we're doing some major hardware upgrades. And I'll talk about the types of And I'll talk about the types of hardware upgrades that we expect and how we're thinking about automating the electromagnetic follow-up for different types of gravitational wave events. But just a little bit about me. So I'm part of the Gravitational Wave Optical Transient Observer Network. It's a consortium involving a lot of different universities around the world. I'm currently at the University of Warwick, but previous to this, I was at Monash University working with the Osgraft group. Working with the Osgraft group for electromagnetic follow-up in Australia. Okay, so just a brief overview of where I think the general time scales for electromagnetic follow-up lie, where, you know, basically we have a gravitational wave trigger event, and typically these come with transients. And by typically, I mean those with compact objects like a neutron star, things with a lot of mass that can be disrupted. So if it is a So, if it is a binary neutron star or a source that has a neutron star with it, we try to go and try to track that down. And so, we have a survey mode of an instrument here. So, this is just one of the go-to facilities that we have on sky. It's a survey-style instrument, and we go and try to track the transient on the sky. And timescales for at least the gravitation wave trigger for the appropriate sky maps to come out, those can be minutes to, you know, if we have a full five. To, you know, if we have a full finalized sky map that can be days, where the survey to observe the training on sky, once we have that sky map, that can be on the order of minutes. Another part of this process is that once we take our image, we have to have an appropriate reference image where we can create what's called a difference image to try to find any sort of transient-like event that has come out that has been new within this timeframe. And of course, what I'll mention later on is that cadence is a huge. Mentioned later on, is that cadence is a huge issue for this full automation. And then within hours to days, once the pipeline has created this difference image, a human will then decide if the trans in itself is interesting. And then, of course, within the hours to days, we expect to schedule follow-up observations. And so, this is the typical time scales of the very basic bare bones. And I'll just showcase where. Bare bones, and I'll just showcase where automation and machine learning helps to speed these time frames up. So, going back to, of course, the 1708-17 sky map. So, of course, we all know is very well localized and about 28 square degrees at the final sky map when we were looking for this. But when we're searching for these, at least in the optical sky, we're ideally looking for only one particular transient in this field of view, that is the counterpart to the gravity. That is the counterpart to the gravitational wave events. But just looking within this 28 square degrees, just by pure sky density, we're already down to 16th magnitude, not even reaching what was found with the kilonova in G magnitudes. It's about 50,000 sources and more, just in pure sky density. Now, if we want to get down to below the magnitude that was actually picked up from the kilonova, we're looking at probably about a half a million sources. And again, the number of sources that get picked up in this is. Number of sources that get picked up in this is again cadence dependent. So, of course, that was a very well localized sky map that was only 28 square degrees. But if we look back at some of the sky maps that we had for the first half of 03A, we can see that the sky maps were not only quite varied, but they were also much larger than we expected from 170817. And so this challenge only starts to compound as we get larger and larger. As we get larger and larger sky maps available to us. And so, of course, even then, I think at the start of the advanced LIGO era and the advanced LIGO Virgo era, we expected that as we improved the detector's sensitivity and improved the instrument itself, we would try to bring down and reduce the localization error regions, the 90% credible area. Area. But there was a publication, I think, this past year that says, well, based on the O3 runs, we may be expecting similar credible areas from O3. So we actually might not get that gain. And that, of course, is coming from the third and fourth detectors that will come online, like COGRA and sensitivities of Hanford, LIGO, Virgo, and all these combinations. Combinations just very briefly. So, again, just looking at the BNS expectations for the binary neutron star systems as we go move from O3 to O4 to O5, we don't see as much of a decrease in the gain in the extent that we maybe potentially expected. But of course, this is to be seen. And not only that, but again, the luminosity distance. I think this, if I remember correctly, I think this, if I remember correctly, this black area is what was expected, I think, if I have that right, where the blue curve is the, oh no, sorry, the thinner curve, I think, is the expected from the, what is it, the catalog? And I think the darker blue line was from this particular simulation run. But of course, that's to be expected and wait to be seen. And wait to be seen. Again, now just looking at the challenge of optical follow-up and why we need to be careful about our choice of instruments, is that time on these facilities is oversubscribed. And of course, for particularly exciting events like this 1904-25Z and 1904-26C, these were both very exciting events in and of themselves where we had a lot of participation from facilities around the world. But you can see that a majority of these facilities. You can see that a majority of these facilities, while they are oversubscribed, they do tend to work together effectively to try to scan over these regions. And a lot of this is because positional dependencies, they happen to be on sky at the time, or latitude dependencies, et cetera. But you can still see the challenge of trying to cover these regions where here we have facilities like Kate and KMTNet trying to get the lower half. Trying to get the lower half of a particularly complicated sky area and still attempting, but these areas are quite challenging in themselves to cover. So that kind of motivated the whole idea of the gravitation wave optical transient observer itself. So specifically designed and optimized only for electromagnetic follow-up searches to gravitation wave events. And so the philosophy behind And so, the philosophy behind it is that it's cost-effective, it's scalable. So, here we have eight unit telescopes located on a single mount. And so we can scale our needs as needed. And it's also adaptable, where here we can see the full design is not only one dome filled with eight telescopes, but then a second dome, which is actually what we're building currently, where we're filling in and repairing. Filling in and repairing and building from scratch this second dome here. And of course, there's always going to be a trade-off when you're designing and building these instruments where you're either going to try to maximize or minimize your cadence area flux. And there's always going to be a trade-off here. And so for our needs, what we did was we chose cadence and area. So we wanted a very wide field instrument to be as fast as possible to moderate depth and flux. Depth and flux. But of course, with this strategy of pointing where we have two domes, we can actually either co-point to get deeper limits and go further into, I guess, the background, or we can spread them apart and look at separate parts of the sky and scan more area in less time. So, again, this is just dedicated to purely dedicated to following up gravitation wave events, and in particular, And in particular, because we don't have that time constraint that a lot of other facilities may have, we're allowed to chase the less favorable events or the sub-threshold events, or we can choose to follow up a low-threshold binary neutron star event and then try to look for a binary black hole counterpart that may be co-occurring and sort of do these trade-offs. So, on each single mount, it's 40-centimeter telescopes with F-SOP. Telescopes with FSOP of 2.5. And the final design is, of course, these eight UTs, which gives about a 40-square-degree instantaneous field of view, which you can see in one of these boxes here. And with the second dome, which will hopefully be completed in the next week or so, we'll be out to 80 square degrees. Now, going back to the astrophysical transient landscape, and what I mean by this is more of contaminants. And these are the things that we have. And these are the things that we have to filter through to try to find that one particular counterpart to the gravitation wave event. And so, what these can look like, if we look at the peak absolute magnitude versus characteristic time scale, we can see we're littered with a lot of different types of sources. We have classical novae, we have calcium-rich types of transients, type 1A explosions, superluminous supernovae, but in particular, this area here that I've highlighted. Area here that I've highlighted in this dark green, this would be the short gamma-ray burst orphan afterglows or the short gamma-ray burst afterglows themselves. And so you can see within this region, they're going to be quite crowded with a variety of other types of instruments like, sorry, transients like the supernovae, thermonuclear supernovae, calcium-rich, et cetera. But not only that, but if we want to look long-term, right? So if necessarily we don't get on sky immediately at these early times. Sky immediately at these early time scales to get this short gamma-ray burst afterglow, which is the counterpart to a gravitational wave event, a binary neutron star. We also want to try to look for the kilonova, which may accompany these sources. And these would then be, of course, much fainter with much shorter durations. And these also are occurring with white dwarf and NOVI type explosions as well. And not only that, but again, based on cadence. So, and what I mean by cadence is. So, and what I mean by cadence is how often you return to the same field to reimage that, you're gonna have different types of sources that come up. So, let's say you look at one particular field and then you come back 30 days later, the types of sources that may be in your image are going to be more of the type 1A, the long GRB events which aren't associated. You might start getting into these odd sub. These odd subtypes of supernovae, like the 1BCs, type 2Ps, etc. So, again, it depends on the cadence and how often you can revisit the field. And so, having a regularly steady cadence at every given time helps to eliminate transients that eventually pop up in your field of view. And again, it goes more than that, right? Because these aren't just transients, but there's also a high level of variability. So things that are just turning on and off in the field. Things that are just turning on and off in the field and don't necessarily are novel in the sense. They just may be novel depending on your cadence. And these can be eclipsing stars or rotating stars, asteroids that will just fly by once, cataclysmic variables, pulsating variables. It can be a wide mix. And so to try to optimize any kind of machine learning process to try to identify all of these different subtypes is a challenge in and of itself. A challenge in and of itself, but it's something that a lot of people are working very hard on. And I'll talk about a few different ways that we're trying to address this issue. And again, this is the photometry for 170817. So this is taken from the Kilanovodop Space website, which has collated a lot of the data that had occurred. And so we can see it's actually quite, you know, it was a very massive campaign, a global campaign, which enabled this kind of data coverage. This kind of data coverage. But still, there's a latency that cannot be ignored, which is the first seven hours of this event. And so, of course, that's where my particular interest lies is in that first eight hour span. And where, in my opinion, a lot of interesting physics lies, in particular, the neutron star spin-down limits, things like that. So, the physics that happens. Things like that. So, the physics that happens right after the merger. And of course, that takes automated processes to get sky maps early enough for dedicated facilities to go and find them. And so the early types of physics that can come out is actually, in my opinion, quite interesting. So we have a black hole engine. So right after the merger, a neutron star can collapse to a black hole. Shortly after that, we have emission along these relativistic jets. Relativistic jets. And so, some of the first emission that'll come out is low-energy gamma rays, where so this is the first emission, and then the emission starts to go out almost like shells, where you'll have a faster shell and slower shells, and they start to collide. And these collisions of these internal shock waves are propagating out emission. And so, when we hit this internal shockwave, when it hits actually the external media. Shockwave when it hits actually the external media of the environment of a merging binary neutron star system, what happens is a reverse shock can happen. So it can hit the shock front and reverberate back. And that actually gives you a lot of information about the jet structure itself, which is an unknown even today, right? So a lot of speculation is going on, but having information within the first hundred seconds of that reverse shock to give you information about the propagating jet. Information about the propagating jet is basically a smoking gun to your jet emission mechanisms. Okay, so now I've kind of tried to bring about the problem of what we expect and typical timelines, and then areas where I think electromagnetic follow-up can really benefit from having automated classification and machine learning algorithms in place here. And so, one of these is, of course, is once your pipeline. Is once your pipeline creates a difference image and you have realistic potential transients, how do you tell what's real from what's bogus? And then going from that, from your sources that have been identified as real, how do you classify those sources? Meaning, if you have only a limited amount of time to do follow-up observations, you want to be very sure that the sources that are being classified as potentially interesting are worth that time to do your follow-up observations. Observations. Okay, so going back to the real bogus classification now, again, this is a pure source density issue. So the deeper you go, the more sources you're going to pick up in your image. And so some of the very early machine learning applications, and one of the very first things you want to do is just pure volume reduction. And so this is some of the early real bogus classifications where you have either feature representation, so the pixel counts themselves. So, the pixel counts themselves. Does it look real? Does it look like a stellar-type source? Does it look like a transient? Or contextual information? Is it nearby a galaxy? Is it in the galactic plane where it's most likely to be a variable star? Things like that. But the very first step is to do volume reduction. And so, a trick of the trade, that's what I call it, is so-called difference imaging, where here we have our observation image. So, let's say we Observation image. So let's say we just had a trigger, a gravitational wave trigger. We go and we take an immediate image and we go back and we say, okay, we visited that field a week ago and we have a reference image. We perform a convolution to match these two images together and subtract them. And where we have our kernel K is defined to appropriately match these two images together to hopefully at the end of the day be left. Hopefully, at the end of the day, be left with the transient that wasn't there that's potentially interesting. Now, of course, the quality of this image that comes out is affected by a lot of different variables. So this comes from the precision of your reference frame. So if they're not appropriately matched up, that can lead to a lot of issues like you might see down in here. Imaging noise, so not effectively accounting for the different levels of noise can tend to mask. Can tend to mask your transients within that noise if you have atmospheric disturbances like clouds, turbulence, or if you have saturated pixels. But what we've seen so far in this early time scale is that machine learning techniques for pure volume reduction can reduce the set of transients that you get can be on the order of 10,000 to only a handful, meaning a human can go at the end of the night and to bet them. But we can still do better. Can still do better. And again, here's a typical example of volume reduction where we have a few examples of some transients that have made it into the actual image that look real, but we have a lot of junk. And the wider your image is, the worse this gets. And we can see it's highly variable. Where here we have sources that aren't matched in terms of what's called seeing. So your point spread function or the shape of your star in the image is. Shape of your star in the image isn't matched appropriately. It's more spread out. Or down here, we also have parts of the image which have been, it's called coma, where we have the image is actually spread out and not matched in this case, where we have dipoles and we have a lot of different things that are very complicated to assess with extremely wide field images. And of course, this is in terms of the real bogus machine learning. Machine learning algorithms, the real difference imaging is the main bottleneck. And this, of course, comes by quality issues for each exposure. And so just a little bit more of a few examples, we have issues like backgrounds. We might have the telescope itself not tracking properly. It might track very much improperly. Here's an example where the seeing is an issue, meaning that the Seeing is an issue, meaning that the point spread function or the shape of the star itself has almost blown up in size and doesn't look like a star at all. Or we have a reasonable image here, which we've classified as good. And all of this, again, comes at cadence, right? So also it depends on your exposure time. So the longer you're on sky, the more likely it is that these issues can come in and make an appearance in your image. So you have to. Um, so you have to try your best to try to maximize your depth with the exposure time for wide-field images where these issues can arrive. Um, so two things come in here. We can't change the wide field nature of our images, but you can do better at modeling your print spread function to try to correct for these. You can also pre-clean your image where you can either say, well, this image is too far gone, like this one, which is really bad tracking, it's basically unused. Which is really bad tracking. It's basically unusable. Or you can try to denoise or artificially correct something like this. And you can do these in, in my opinion, very interesting ways. And I'll talk about those towards the end. And again, yes, so some of the early machine learning applications would be things like variability metrics. So is it variable? Then it's probably bogus. You can do things based on the shape itself. So is the object skewed? Is the object skewed? Is it actual Gaussian or is it some odd shape? Is there periodic metrics to it? So, do you notice any dominant frequencies when you perform long-scorgal analysis where you see multiple periodic measurements? Or you can just tell a lot by the context itself. So, is it close to a galaxy? Is it completely far off? Is it somewhere completely unexpected? You can provide a lot from the context itself. A lot from context itself. And I just want to briefly talk about some of the early shape analysis that I've done. And this was all purely based on trying to reduce the volume as much as possible without knowing anything else about the data. So this was an unsupervised shape analysis technique where here on the left-hand image, we have a typical point spread function where we have the highest intensity. Where we have the highest intensity here. So, this scale is by pixel value. We have the highest intensity at the center, and then it falls off as an approximate Gaussian. So then what we do is we project that point spread function onto a set of basis functions. And in this particular case, we use a form of shapelets called Zernike polynomials, but you can use Gauss-Laguerre polynomials, Hermite polynomials, any set of orthonormal polynomials. Orthonormal polynomials will work here, as long as we're orthonormal polynomials. So, what you do is you decompose it using singular value decomposition. You obtain your coefficient values for each index of your basis function, and then you do this for each source in your image. So you continue to run this, you do your decomposition, and you get a range and an ensemble spread and an ensemble average of all of the stars in. Average of all of the stars in your image. And then for each order, you can fit it to find a median. Now, I went over that, I think, quite rapidly, but the idea is to say then you want a definition for if you have a new transient that has come into your image, does it look like a transient based off the shape itself of other objects and other stars in particular in your image? Transients, by definition, will look stellar. Will look stellar like a star. And so here I've just called this the shapelet score, where it's a criterion for how point-like any new object in your image will look. And so this was just purely to say if you have a transient that looks this bright all the way down to just above your noise limit, does it still look like a transient? And based off of this unsupervised score, here we've called it the Zernik. Score here. We've called it the Zernike distance because I've used Zernike polynomials. We can see that a lot of the residuals tend to be at much higher distances, meaning they don't look like point sources at all, where your reasonably bright sources will tend to cluster at very low distances, meaning they very much look like point spread functions or like a transient, and they have a typical spread. Of course, this was just one way to do pure volume reduction. You can see the Your volume reduction, you can see the number of residuals that don't pass through. But since then, we can improve on this method to do more fine-tuning of volume reduction. Yeah, and so this is again, just coming back to this idea where I just wanted to say that many of these machine learning methods, things like neural networks, classifications, regression trees, et cetera, they all could just be. Et cetera. They all could just be seen as just different ways of estimating basis functions from data. But I think in all cases, the amount of data that you have is important. And of course, that comes in terms of not biasing your results or your training set or the outputs that you get. And so this is just briefly coming back to GoTo now. So when we try to go after different electromagnetic Different electromagnetic counterparts or candidates. When we started scanning the sky map, what we have here is an output where we have the science image, a reference image, this difference image that I've talked about before, and we have a lot of other contextual information, such as how far away this object is from a galaxy in a known galaxy catalog. Has it been found in what's called the transient name server, which is a known transient object? Transient object and other contextual information, including a score, as well as a light curve and a photo. And this particular source came about when we were searching for GW190901AP. And so this is just, we want to improve the every aspect that goes into this, including contextual information, light curve analysis, probability of the The classification of this type of transient. And so I'll try to go through more go-to specific ways and that we're improving our snapshot, so to speak, right? So we only get one page where we have to go through and we have to look in one snapshot. Is this interesting? If it is, trigger an external facility. And if not, keep going and move it to junk. So you get basically one chance to do this. So it has to be right. One chance to do this. So it has to be right. So, going back to the idea that the amount of data that you have is most important, what we want to do is we want to build the source list as fast as possible. And of course, this was tricky when GoTo was built. We had our first light in 2017, but we were still in commissioning for a lot of that time. And so we still needed a lot of data very quickly. So one of the things that we did was So, one of the things that we did was we just thought, let's try a minimal sampling technique, right? So, let's say every image that comes into the science image is going to be real. It's made its way into the image. We call it real. The number of contamination of actual fake objects or cosmic ray hits or actual junk was relatively low for what we needed to do. So, we just let's just try to build as much. We just, let's just try to build as much data as possible. Eventually, we ended up using a number of minor planets as actual sources that make it through, where minor planets are asteroids that have come into the field. And when they come in and we take a snapshot, they'll look just like a real transient. But you can see the actual pure amount of data that can be generated with relatively similar responses. Responses for the fraction of events that we can recover. And so I'll just mention this. Eventually, this was just one way that we were taking to approach it, to do real bogus classification. And I'll leave the link here. So this is a paper from a student in our collaboration, Meng et al. Another way that we approached this was from our colleague Tom Killerstein. Was from our colleague Tom Killerstein, which is that eventually we wanted to assign confidence to each candidate, transient candidate that came in. And so he developed a model called a transient optimized source recovery using a Bayesian convolutional neural network. And so here is a snapshot of his model itself, where it's basically an add-on to a typical convolutional neural network, but with Bayesian posterior. Bayesian posterior probability distributions that can come out. And so he's called it a nuanced uncertainty-aware classification, where the number of posterior samples basically improves the class-balanced accuracy that comes out. And just looking at the types of sources he was investigating, we have galaxy residuals in blue, which were clustered in this particular area. We have sources from the Marshall, which I've mentioned. The Marshall, which I've mentioned just below, that was the snapshot web page. We have a list of minor planets that have come in, which are basically asteroids, near-Earth objects. We have random junk, which tends to be the most of the sources that come in from pure image attractions. And we have synthetic transients as well. And so, what we want to recover are the orange objects or the Marshall. And here we can see the output, which is the confidence score, which roughly. Confidence score, which roughly matches up to the objects that were making it into the Marshall. But there was a lot that went in behind this, I guess. Okay, so then this is what it looks like at the end. So here we have our science image, our template image, or the reference, and difference. And so by eye, it can be quite tricky to make an assessment, especially for this one. If you just see the difference. Especially for this one, if you just see the difference image here, you think that looks quite point-like. But then you take a look at the science image, and it's quite noisy, and there's probably something wrong with the image itself. And so the confidence that has returned for this is quite low, as it should be. Whereas something that actually looks much like a transient, potentially there's a source there that might be a very faint host galaxy. It's hard to tell at this scale, but something has clearly happened. Has clearly happened in this field, and in response has a very high confidence score. And so, what we can see is across all magnitudes that GO2 can reach, this is the L-band magnitude. So, I think the equivalent first kilonova detection would be somewhere around here, so about 18th magnitude-ish. We can see that we can actually go much. We can see that we can actually go much deeper than if we don't use the method that he prescribed, which tends to cut off at around 20th magnitude, but we can go down for about half magnitude deeper. It also helps with having different host offsets. And so, this is in units of arc seconds. So, the further we get away from the galaxy, of course, the better any trans. The better any transient will do because you have less galaxy residuals to deal with, and you have less, you know, morphology of the galaxy to contaminate your transient object. But as you get closer, it tends to perform worse. But Tom's method shows that we have relatively stable recovery across all of the nominal host offsets that we may or may not expect. Okay, so then I just wanted to touch on kind of one last issue that I see is somewhat impertinent, which is data imbalance. And this is meaning that one or more classes are typically overrepresented. And the cadence observing strategy that GoTo follows, we tend to get a lot of variable star sources. We do get some supernovae candidates, some active galactic nuclei, AGN, but we tend to be over-representing. Be over-representing of variable star sources, which are not interesting when we're looking for electromagnetic follow-up gravitation wave counterparts. And so also what we have when we're doing light curve classifications for these types of objects is a lot of sky survey, well, I guess a lot of sky surveys tend to bias their observations depending on the source target they're looking for. The source target they're looking for. So, for example, a lot of supernovae may or may not come up by nightly visits. AGNs will tend to populate over year-long or month-long visits, and variable stars will tend to come up nightly. And so the, again, this is what I mentioned earlier was that the cadence is a huge factor in the data representation that you're using. Data representation that you're using for your machine learning algorithms. And so, for an example for this method, that Umar, again, one of our PhD students in the collaboration, he looked at around 99,000 objects where 99% of them are going to be variable stars. And so he obviously saw this as an issue and wanted to investigate it. And so, one of the first things he did was, how do you split the sources? And again, this comes back to that tree that I've done. That tree that I showed, where you have different physical mechanisms itself, which will split your variable stars, right? So you can either have physical means, so a source actually moving, you can have things exploding, you can have Cepheid variability, or you can even split your supernovae that you have by spectroscopic features. And so there's a lot of different areas that you can split your data. And that you can split your data. Do you want to just classify all variable stars all together, all supernovae together? And if you do that, it's going to be very challenging to actually have full confidence in the results that you obtain. Okay. Yeah, so then I just wanted to kind of bring about some caveats of things that when we were looking at this issue were kind of brought about. And so dealing with data imbalance. And so, dealing with data imbalance, should this rely on purely astrophysical rates when weighting? Whereas the algorithms themselves that work, like the RNNs, for example, or the convolutional neural networks or autoencoders, they all assume roughly even distributions of classes. So, one idea was that, okay, maybe you can train all your classifiers together, but that basically blows up the time of your computation. The time of your computation. And so there's other ways you can do this. You can do this at a data level, meaning you can change different aspects of the data itself. Or you can do this at an algorithm level, meaning you can downweight things that tend to be overrepresented. And I think for what Umar had chosen to do was some hybrid method where he used both data level and algorithm level methods to And algorithm level methods to try to bring down the overrepresented sources for his method. So this is what he did. He did light curve classification, particularly using imbalanced data. And so this is what his algorithm looked like: where we have a light curve. It's not regularly sampled. You can have clusters. The cadence is completely off. It may not even be in the, you know, in similar filters, so in different colors. So it can be a mixed bag oftentimes when you get these light curves. And so what he had was he had a regular RNN where he had two RNN layers, where at the end of this, he appended a contextual information at the end, which then he put in again into a dense layer. Again, into a dense layer as the merged branch, where the output layer is a list of class probabilities where they sum to one. And so for his, it was particularly the three, variable star, the AGNs, or the active galactic nuclei, and the supernova. Okay, and I've just put some information about some of the other models here. One is called the long short-term memory that he used, and another one was the That he used, and another one was the gated recurrent unit that he used. And that's just different types of gatings for looking at the data. But I'll just leave that there. Okay, and so here are his results. So what he saw was that the long, short-term memory with a weighted focal loss tends to outperform the one with the GRU. GRU with weighted focal loss. And where here focal loss is defined as an improvement on the cross-entropy. So it's a way to downweight the probabilities that are for multi-class systems or sources that have been overrepresented. And so here we have a modulation factor, 1 minus p, where p is the probability of a class of an object i. Class of an object i belonging in class j and we have this gamma term which is the rate at which you downweight over represented classes and so one interesting thing that what he saw in this au curve was that the the gated recurrent unit with focal loss without having that contextual information was the worst of all of the methods that he tried which goes to show the basically that the more information you can provide to your model the better it will perform Divide your model, the better it will perform. Okay. And just one final thing, which I think is particularly interesting, is that it's not about detections on the final image, but there's also an aspect to preparing your images before you even throw it into a machine learning algorithm. And by this, I mean doing proper image quality assessment. And so there was actually a recent paper that came out by Tave Mori Inia. Tabe Mori Inia, my apologies, called Deeply Embedded SOMs, where here we have our CCD images, we can pre-process the data, and we provide it into a deeply embedded SOM here. So I'm just throwing up the algorithm, which is called DESOM1, where here again we go back to the images I showed earlier, where we have Earlier, where we have potential issues on the image, like background problems where your telescope isn't tracking properly, or it's really not tracking properly, or wind shake, the wind has actually physically shaken the instrument, which will leave these trails all over. Seeing is an issue. Maybe you have clouds in the frame, or you have good. And so, this is actually this deeply embedded SOM method is actually shows promise. Actually, shows promise for helping to pre-process your data before you even go to input it into your algorithms. And so, here's one example of the output from the DESOM-1 itself. And so, you can see there's a lot of different types of data that has been represented here. So, here we have what are called donuts, meaning that these are out-of-focus images or sources. Here we have Or sources. Here we have saturated stars that have made it in, things that are highly wind-shaken, cosmic rays will just hit single pixels in your detector and those will be represented. And so basically what you can do from here is you can say, if I have an image, what does it look like on this SOM map? And so here you have a density map of all of all of the different features. All of the different features that are represented on this grid. And so here you can see for really bad tracking, it tends to dominate this lower corner here, which looks like what you would expect. Bad focus would be these donuts. And again, these are represented by the doughnuts. Or background problems, which tend to be single stars with a lot of gradiations, which tend to be kind of raw on tier, I guess. Kind of a round here, I guess. But the main point of this is once you have this density map, you can convert this into what's called a histo vector, where you have spikes of your cell index, which then you can feed into a second algorithm for a dense autoencoder. That's what they're called. And so you can, once you input these, this histo vector, you can cluster them together. Vector, you can cluster them together and normalize them and then input into this dense autoencoder. And at the end, what you get is a map of almost like a phase space of different types of images that can come in. So your background images with background issues can be clustered here or down in here. Good images will tend to be much broader in the types of sources that you get, seeing issues, tracking issues. Issues, tracking issues, so they all can be represented with a certain probability in this case. Um, okay, so um, I just wanted to kind of mention some areas where I think, well, at least go to specific. This is, of course, not the end all for SkyServe facilities or even EM follow-up facilities and the progresses that other instruments are making, but just different areas where I think doing this. Doing this electromagnetic follow-up is benefited by machine learning, and that includes having a model decide if a transient is interesting and having a model decide if a source is deemed ready for follow-up observations, which are typically oversubscribed at these stages to vet them. Is this a real source? Is this a minor planet flare or an indoor flare? Is this a Or indoor flare, is this a supernovae to make those decisions for us? Where I see other benefits is taking all of this from minutes to days, minutes, moving this, I think, I don't know if there's bars here, being able to reduce this from hours to days down to seconds. And again, the follow-up observations to have the schedules down from hours to days, which is typically from human interaction. That would be. That would be reduced down to minutes. But of course, there's a lot of effort going on on this side of producing posterior distributions from gravitation wave parameter estimation pipelines and producing the sky maps for the surveys to start to track them. Okay, and then I think just one final slide of areas where I think great gains can be made. Think great gains can be made that I didn't have time to focus on. One would be scheduling gravitational wave follow-up strategies. This includes: do we want to look for every event that comes through the LIGO-Virgo-Cagga pipeline? Do we want to only look for sources that have a probability of having a neutron star associated with it? How do we, if we have multiples coming in at the same time, how do we have a signal? If we have multiples coming in at the same time, how do we want to look at that? And we want to do that in an automated way and using as much information as possible. Also, photometric redshifts, and this could be from using probabilistic methods. This is gaining traction at the moment where now we can probe our volume, right? So if we can perform photometric redshifts on any transient, we can actually probe a volume and see if it fits within the 3D space. People also look. People also looking at denoising of images where we can go further into the volume, so going down into the noise to try to dig out sources. Other forms of light curve analysis where people are looking in kind of a fun way of almost like a spectrogram where you have change in magnitude versus change in time instead of just a light curve. We can deal with atmospheric turbulence, like deep blurring, using blind deconvolution. Using blind deconvolution. So that's also picking up traction. That's a very interesting field. And of course, we have no idea what we may expect from every gravitational wave event. We have models and predictions, but we really don't know. And so we want to help with anomaly detection or outlier detections or things that don't fit any model that we have or that are just slightly, you know, slightly anomalous to some extent. Extent. And yeah, so I think that's all of my time. And so I just want to thank you for allowing me to be your final speaker for the conference, and I'll take any questions. Thank you. Thanks, Kendall. Okay, so do we have any questions here? Okay. Yeah. Hi. I was wondering, Kendall, hopefully, I'm coming loud, coming clear because I'm in a cafe. I was wondering. I think we have two people speaking at once. Oh, I'm so sorry. Sorry. Steve, could you go ahead because we had the speaker up? Okay, cool. Yeah, I was wondering about the classic. Cool. Yeah, I was wondering about the classification code that you showed. What is the difference between, say, what you're proposing versus tools like Rapid, which are already out there and used by broker teams and they provide, say, real-time classification of different supernova subtypes. There may be other tools as well, like Super Rain, maybe. Yeah, absolutely. Yeah. Yeah, absolutely. Yeah. And yeah, so I think that the methods that I've proposed here, this is almost our in-house, you know, based on our own data, but there are absolutely other tools that may or may not be more suitable for the data sets that we can look into. Yeah, so I know that we have several talented PhD students who are probably more aware of this than I am for tools like RAPID. Tools like Rapid. So I'll have to ask them. I'll defer and ask them. All right, yeah, sounds good. Thanks. Okay, Marco, if you can go ahead. I can't hear you, Marco, if you're saying something. I don't think we can hear Marco. Yeah, and I lost my Zoom window. Hey, Kendall, did the question go through? No. No, we'll try again, just a sec. Okay, yeah, it's okay. All right, uh, Kendall, just a quick question. You mentioned the class imbalance when you train it and a way of fixing that by re-weighting the neural net. Is that something uh is that method something easy to implement uh in the in in the code? In the code, I mean, there are already prescriptions or is something more complicated? I'm not too familiar with that. Yeah, I think I have to double check and look up again his. Yeah, so I think what he did was he just used this equation. So let me go back to it. Where'd it go? Do the imbalance. What's this one? Yeah. Nope. It was this one, yeah. Nope. Yeah, so I think a lot of this comes down to it was in this method here. So it was the difference in here. And then I think, where'd it go? Actually, it was this one here. So this would be the cross entropy. So instead of using the typical cross entropy for this multi-class loss function, multi-classification, the loss function for a multi-class problem, he Multi-class problem, he used this focal loss instead, and so that's how he actually downwigged it. So, I think it's just this extra term. Well, there's this alpha term, which is the weight, and then he just added this one minus p to the gamma function. So, I'd have to dig around in the code, but I think it was fairly simple to implement, but I'd have to double-check. Okay, so yeah, modifying the cross-center. Okay, thank you. Yes, Yes. Okay, any other questions for Kendall out there? No? Okay, well, let's thank Kendall again. Thank you very much.