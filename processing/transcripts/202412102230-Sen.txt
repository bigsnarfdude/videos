Okay, so good morning, everyone. I was asked to give a tutorial on quantum Shannon theory. There are others in this workshop who are probably better qualified than me to give this tutorial. But anyway, yeah. So take the brick bats. Okay, so quantum Shannon theory, quantum information theory. Shannon theory, quantum information theory. So today I will concentrate on what's called Shannon theory, which is a subset of information theory. So by Shannon theory, we mean the kind of problems Shannon talked about in his old 1948 paper. So it's basically message compression and then coding for a noisy channel. Okay. So, but I'll take a contemporary viewpoint. So let's start off with a few basic problems. So I would Few basic problems. So, I would like to say that quantum Shannon theory contains three broad classes of problems. So, this is my own idiosyncratic way of looking at it, but at the cost of maybe a little oversimplification, it clears up the field a lot. So, there is the packing type of problems. Okay. People who have studied classical information theory may be familiar with some of these terms. Then there is. Of these terms, then there is the covering type of problems. Okay, these two terms exist in classical information theory, but the third one is unique to quantum. It is the decoupling type of problems. Okay, so I would like to say that almost all problems that you would study in quantum Shannon theory can be Can be handled by using a combination of techniques from packing, covering, and/or decoupling. Okay, so for the packing problem, let's look at a simple thing. So, this goes back to the good old problem of sending messages through a noisy channel. Okay, so let's. Okay, so let's say there is this quantum channel, and for simplicity, let us say it takes only classical inputs, but it gives quantum outputs. So this is what's called a CQ channel. CQ channel. Okay. Classical input, quantum output. Let's give some names to it. So the classical input Classical input inputs are symbols from an alphabet X. And if let us little X is a input 3 mark like     Gunil, this is not open. Okay. Okay. Now, what is the best I can do in this scenario? So, let me come to this. So there's a message M that I want to send. So the message M is coming from an alphabet. Okay. Let us say the alphabet is of size 2 to the power r. So r will be called the rate of transmitting messages successfully. So the message is any number from 1 to 2 to the power r. Now this message M will put We'll go through an encoder, okay? So, maybe so there is the encoder, okay. E now what comes out of the encoder is a probability distribution on these symbols x. So, I will call it so let us say Let us say let's give a name to this. Maybe subscript is better. Yeah. So it's a probability distribution over the alphabet x corresponding to the input message M. Okay. And when I'm talking of encoders, in the final expression, I'll choice, I will optimize over all such mappings from M to probably such kinds of probability distributions. Okay. And now what comes out here is of And now what comes out here is of course some things which I can now write down properly by linearity. So I'll call it a density matrix supported in Hilbert space Y, but this is what you get if M is input. What is this? This is summation over all symbols X. So I will take okay. So basically choose a So basically choose a symbol x according to the probability distribution p subscript m and if the symbol x is input, the output is this density matrix rho sub x. I'm taking the corresponding linear combination. So this is the meaning of this notation rho subscript m okay fair enough. Now Okay. Yeah. Now, of course, the receiver here, the sender has done her job, okay? Encoded M into this probability distribution, sent it to the channel. The channel output is this. So this has to go through a decoder. Okay, we'll see what the decoder does. So this is a decoder. And the decoder will output a guess for M. A guess for M. Okay. This whole thing is probabilistic because already probabilities are introduced here. Even for a fixed m, there is some probabilities over symbols x introduced here. Then the channel is a CQ channel, so there are inherent probabilities in these density matrices. Then the decoder will probably use a measurement, so there are even more probabilities. So what comes out here is basically a probability distribution. So let's give a name. Let's give a name for this alphabet. This is the space of all messages, capital M. So I'll get a probability distribution over capital M okay, corresponding to the message small m out here. Okay. And what is the correctness of this entire procedure? Okay. So the probability that so let's say So let's say that we're going to have to do it. So let me explain what this is. So require okay, so it's a slightly complicated looking expression, but let's say the decoder has done something, and of course, the outcome will be. And of course, the outcome will be probability distributions over this message set. Let's consider one particular outcome to be m hat. Okay? Yeah, just to distinguish from what was transmitted. So the message M was transmitted here. What we get is a probability distribution over the set of messages. A particular instance I'll call m hat. This expression is nothing but the probability m equal to m hat. M equal to m hat. Okay. Summation M 1 by 2 to D R. Okay. This equal. So I'm just written in this slightly more messier, but if you stare at it for a few seconds, it realizes the same thing. Okay. So this is my requirement. The requirement is the channel is given to me. That means the characteristics are given or the mapping from x to rho sub x is given. Okay. The aim is. The aim is to send messages of a certain size. So that's where the rate comes in, such that there are encoding and decoding procedures which achieve an error of less than epsilon. So epsilon is also one of the parameters. Okay. So let's see what we can do. Okay, so let's understand what the decoder will do. The decoder, of course, will consist of some measurement. So, how are we going to model the measurement? So, the decoder is a POVM. Okay. That means it's a set of It's a set of positive semi-definite matrices lying between zero and identity. So zero is less than identity. So this matrix on which Hilbert space? The output Hilbert space y, because that's what I'm going to measure. Okay, so this is 0 on y, identity on y. So I have a set of this, okay, corresponding to every possible. Corresponding to every possible outcome, which I'm now calling m hat. Okay, that's the decoder. And the aim of finding a good decoder is basically trying to find the set of matrices. There's, of course, the condition that sum over m hat, the POVM condition. Okay, now let's No, let me question. Okay, so P subscript M is the output probability distribution on the message space when little m is input. Okay, so this is the probability distribution. Now what I want is if little m is input, the correctness will occur only if m hat is equal to m. So this is the mass, this probability distribution plays on the same m. Lays on the same M. Okay, because M is being input, the output, of course, is a probability distribution over the entire message set, but the correctness will occur only if M is seen at the output. So that's what this quantity is. So maybe it's easier to follow that M equal to M hat. It's the same thing. Which one? This one. Oh, this is the probability over the entire process. Over the entire process, that's the probability over the encoding, okay? The channel and the decoding. Okay. Oh, maybe you think the outer P is superfluous. Okay, yeah. If you want that, let me remove that. Okay. Yeah, that might be better. Yeah. Okay, cool. In this expression, there's no m hat, but but here there is. Yeah. Yeah. Okay. Fine. Yeah. So maybe this is better now. Okay. Okay. So we'll set it up, but what kind of rates you get? So let me introduce the first entropy quantity for the packing problems. This is what is called the smooth The smooth IPO this is resting entropy. Okay, so let me write. So let's say there's a density matrix row on some. Some Hilbert space X tensor Y and okay, so H is for hypothesis testing, epsilon is what's called a smoothing parameter, it will be related to that error probability epsilon for our problem of actually actually yeah, so let me do it a little more generally. Let me do it a little more generally. Okay, so there are two density matrices, rho x and sigma x. Let me do the most general thing, okay? On the same Hilbert space x. Okay, so this x is, don't think of it as that. Okay. So I'm looking at the smooth hypothesis testing. I should probably call it relative entropy. Yeah. Yeah, okay. I'm looking at the relative entropy of rho with respect to sigma. This definition is asymmetric. Okay, if I interchange sigma and rho, you'll see that you'll typically get different numbers. Okay, so this is the relative entropy of rho with respect to sigma. They are density matrix in the same Hilbert space. So the definition of it is this. So this is negative log. Well, I'll break it down slowly. Okay, it's not as scary as it looks. Okay, so two density matrix are given, rho and sigma, on the same Hilbert space. I'm defining a certain quantity. So, and you'll see why the word hypothesis testing comes in. So, I'll think of rho as what is called the positive hypothesis. That's the ideal state. ideal state the system X should be in and sigma is the imposter hypothesis okay so I want to distinguish between one single positive hypothesis versus the imposter hypothesis so how will I distinguish so I have to take a POVM element so this is just a positive semi-definite matrix sandwich between zero and identity all this means is that this is a positive semi-final Identity. All this means is that this is a positive semi-different matrix. All the eigenvalues are less than or equal to 1 on the same space x. So this pure VM element should accept the positive hypothesis with probability greater than 1 minus epsilon. That is the same epsilon here. And I have to choose it cleverly so that it accepts the imposter with as little probability as possible. So I'm doing a minimization. Okay? So. Okay, so the positive hypothesis should be accepted with probability close to 1 and the imposter one should be accepted with as low probability as possible. So this is a certain optimization you do and then whatever. I want to make a positive number, so negative log, and you are done. But the heart is just this part. So if you stare at it, this is actually a semi-definite program because rho and sigma are positive semi-definite. Rho and sigma are positive semi-definite matrices. Okay. Pi is also a positive semi-definite matrix. So rho and sigma are fixed matrices. So they are like the coefficients of your program. The unknown variables of your semi-definite program are the matrix entries of pi. Okay, but that's also a positive semi-definite thing. And these are linear conditions. Okay, this is nothing but it's something like the inner product between the matrix entries of pi and rho. And same out here. Okay, so there's a linear constraint. So, there's a linear constraint, you are minimizing a linear objective function, and that's it. So, this is actually a semi-different program. Okay, so if you are given rho and sigma explicitly, there are the good algorithms that find excellent approximations to the semi-level program. And they run in time polynomial in the let's say the dimensions of the matrix. Okay. So you can define in fact classical analogs of this also. So the classical analog of it will be all density matrices get replaced by probability distributions. This pure wave element gets replaced by a vector of length x with entries between 0 and 1. Okay, so that's basically the let us say if I'm looking at some coordinate i out of here, what is the probability that i get? That I get accepted as the positive hypothesis. That's all. Okay? So that's the natural thing. And then this becomes a linear program. Okay. So now I am question. Yeah. Yes, it does. It does. Yeah. So there have been other definitions of, I mean, the appropriate entropic quantity for packing, but this is sort of the nicest one. Yeah. You can easily see it satisfies data processing, a little bit of thought. Yeah. Okay. So now I claim that this quantity d epsilon h will actually d epsilon h will actually give the right answer about the maximum rate r that one can achieve, keeping the error probability less than epsilon. So let's see why that is the case. Okay, so first of all I have to say what is this. So let us say R the achievable rate R is greater than Something like this. We'll see what this quantity is later on. But really, the main thing is that to define what is rho x, y. So for this problem of sending classical messages. problem of sending classical messages over a noisy CQ channel, rho xy is defined to be this state. So this is actually a again a CQ state. I mean the register x is classical, y is quantum as you'll see. So sum over x. So let us say, okay, so before defining it, I should probably say max. And maximum over px. So there's an optimization over all possible probability distributions you can put on the input alphabet x out there. Okay. So let's say I have fixed a px out here. So we'll see how you can get this rate. Pxx okay. Okay. So this CQ density matrix has this expression. It basically says choose a symbol, classical symbol x, according to this probability distribution px. And finally, I'm going to maximize overall px to get the best rate. But for the moment, I've fixed any good old probability distribution on the input alphabet. So choose the symbol x according to the probability distribution, record it. To the probability distribution, record it in the register X, and then in the register Y, you have this density matrix. Okay, so this is the state describing the connection between the input and the output of the channel. Very intuitive-looking state to define. And so, this is rho x, y. These are the marginals. I've taken the tensor product of marginals. So, people who have seen a little bit of classical information theory. Of classical information theory, we'll realize that this expression looks something like a mutual information. Okay, so if you have seen what is the Kuhlbeck-Lebler divergence or the whatever, the relative entropy in the classical setting or even the quantum setting, the fundamental relative entropy. So the mutual information is related to the relative entropy by expressions which look like this. So people, in fact, give it a name also. They sometimes call it the smooth. The smooth hypothesis testing mutual information between X and Y under rho. But anyway, I'll just stick to this notation. Okay. So let's look at this quantity. So this is a certain d epsilon h. This is the definition. Now there is a POVM element pi. Okay, now of course this pi will be on x, y. I'm going to write that, which achieves this minimization. Okay. So let me come here. Okay. So let me come here pi xy achieves the minimum in okay. So let pi xy be the minimum, uh the P of M element which achieves the minimum there. Okay. Leaves a minimum there. Now, because this is a CQ state, this POVM element, well, it is a minimum for this semi-different program. Yeah. I don't want to bother so much about the negative log. I mean, that is just there to make it look like an entropy. The real work is being done in the semi-definite program. Okay. So, so pi xy is the pure vm element achieving the minimum in this. Okay, in that semi-defined program, that's what I mean. Yeah. Now, because this rho xy is a CQ state, you can actually observe without loss of generality that pi xy is also a CQ P O V M. What do I mean by this? Pi x y has this form. Okay. So one way to think of it is that pi xy is block diagonal in the Hilbert space xy. Okay. Yeah. But the blocks correspond to the values of the classical register x. Another way of looking at it is the POV element pi xy behaves as follows. Element pi xy behaves as follows. It just observes what is the value of the classical register, that is this. And then the condition on that applies a genuine quantum POVM element, which I'm now going to subscribe by x. So this, of course, depends upon what is observed here. So this is the general form of this minimizing POVM element. Capital R. Capital R. That is the best rate you can get. Okay, so I claim that the best rate I can get looks like this. Okay, so I'm going to prove the lower bound, but I'll also remark how you can prove the upper bound. So the lower and upper bounds are extremely close. Okay. So so the so remember the optimizing POVM has given me this POVM elements pi sub xy. So what the decoder does is decoder. So the decoder gets as input some density matrix. Let me call it sigma on 1. Okay, let me call it sigma on y. So, sigma on y, of course, will be equal to rho y of n for some n. So, so this is the input to the decoder. Maybe more explicitly more explicitly. Okay, now that's very good. Okay, wait a minute. So, what goes in is M, and what comes out? The receive signal system, no, that's not the thing is what is the index I should use? M maybe I should use a M here. Yeah, yeah, I should use a M out here, and yeah, I'll explain what it means. I'll put M. Yeah. Yeah. Well, they should not be safe. Maybe I'll call it all I imagine. Yeah. Okay. Yeah. Iterate overall m hat in m in lexographic order. So I'll explain what is this. So So, explain what is this. So, okay. So, maybe I'll just write it here. Okay, so yeah, so for the purpose, I have to define one more quantity. So, just as I define rho xy out here, I can define rho my. So, how is that done? So, rho my. So rho m y 1 upon 2 to the r sum over m okay, just as before. Yeah, and then I can, of course, talk of its hypothesis testing relative entropy. Okay, and there is a certain minimizing POVM, and again, that's a CQ POVM. Okay, so it's classical on M want. So it's classical on m quantum on y, and that's why I can write this expression out here. Okay. Yeah. So we'll relate the two. So you can see there are two such d epsilon H terms. We'll relate them shortly. Okay. But for now, the decoder, the decoder does the following thing. The decoder iterates over all candidate messages indexed by mhat. Indexed by m hat in lexographic order. Let's say a message is going from one to two to the r. So he applies if he's at message m hat currently, let's say m hat third message, he will apply this P of M, this measurement. Basically, he's just checking whether the message is m hat or not. Okay. The message is number three or not. That's it. Later on for the next iteration, he'll check whether the message is four or not. On the current quantum. Is four or not on the current quantum state? What do I mean by that? The quantum state that was input to the decoder was sigma y, which was rho y m. Maybe it was a 10th message. So the decoder is a very foolish guy. The decoder just starts with message number one, checks whether the state on Y corresponds to message one or not. If it succeeds, then of course the loop stops and out. The loop stops and outputs and hat. If not, the state has been already been corrupted. But the decoder, I mean, yeah, I mean, stupid person, so pretends that nothing has happened and tries m hat equal to 2, okay, with this. So that's what I mean by the current quantum state. In every iteration, the current quantum state is changing. Very, very stupid strategy. Okay. So this is the decoder strategy. And now we'll analyze this and show. Okay. Okay. So suppose M is actual message that was sent. Again, emphasize so probability of Of error is equal to so when so when will error occur the error will occur if this is the mth message so for messages number one to m minus one if one of them were accepted okay or I've reached it Or I've reached iteration m and that was not accepted. So let's see what it is. So this is okay. Or maybe in this case, it's easier to look at the success. So probability of success, maybe a little easier. Probability of success. Okay. It's easier to analyze that. So M is the actual message that was sent. When does it get decoded correctly? That means for messages 1 to M minus 1. Messages 1 to m minus 1, they were rejected in the loop, and message m was accepted. This is easier to write down. So remember, the state is now row y m so message number one should not be accepted. So this will be identity minus pi one. So it should not be accepted and the resulting. It should not be accepted and the resulting state that message that the loop iteration number one did not accept it. Okay, so as I said, the state gets disturbed. So this expression is a subnormalized state. So this actually tells the resulting state of the register Y after loop one, after loop number one, rejected that. One rejected that. It's a subnormal state because the probability of rejection has also entered this expression. And that's actually the right thing we love to work with. Then loop number two also should reject. Okay. So the current state changing that is also taken care of all the way. And finally, loop number M should accept. So dot dot dot. So dot dot dot pi. Okay. So this is the state. And if I want the probability of success, I have to put a trace to find out how much mass it has. Okay. So this is the expression of probability of success. Now, this thing can be analyzed by what is the these days called the what non-commutative union bound. Okay. So why? I mean, if it were, if it was a classical problem. I mean, if it was a classical probability distribution, then it was just the union bound. Like, so probability of not accepting in the first iteration plus the probability of not accepting, so on, okay. But in general, these operators do not commute. So, yeah. So, it turns out that you can get an expression which looks like this. This is greater than something twice one minus pi market trace, of course. Trace. Of course, please. The beauty of this whole thing is. The beauty of this whole thing is that in this expression, only the original state rho ym appears. All this mess created by the current state changing has been removed out here. So, this is the non-commutative union boundary. Okay, so what do we get with this? Let's analyze this. So this is greater than. Okay. There is a one minus this. Sorry, it's greater than one minus. Yeah. Yeah. One minus. That was your question. Yeah, yeah. One minus this. Yeah. So this is the and two more. So, this is the non-commuted union bound. Okay. So, if you look at the classical analog, I mean, you'll see this is exactly the expression, except for this 2 and 4. Classically, I mean, this is 1, this is also 1. But quantum, it's a little worse than that. Okay. So, let's analyze this. So, this is 1 minus. Okay. Now, remember, so let's come back. So let's come back out here. Remember, like pi m is coming from this POVM which minimizes out here. So let's understand what this will be. So let me work here. So I know trace trace. The minimizing POVM in that seven different programs. So it of course has to satisfy the constraint. Okay. Now, remember, these are all CQ states. So if I write out what this is, this just turns out to be. Yeah. So in rho MY, where did it write? Yeah. Rho MY had that expression and I'm just up there and I just input uh put that in. Okay. Pi My was the CQPOVM. So this is exactly what this thing is telling us. What this thing is telling us. Now, this is the probability of success for one particular M, but now I'm going to take the average probability of success. So, I'm going to take 1 upon 2 to the r sum over probability of success when M is sent. Okay, that's what I'm going to do. So, this is the probability of success when M is sent. To success when n is sent. I'm not going to average it over all m. So if I write this expression, then use the non-commuted union bound. So I'll get it greater than 1 minus. The first term, there will be an average over all m. Okay. And what do I know? If I average over all m, this is larger than 1 minus epsilon. This expression, this is identity minus this. So that means this. So, that means this expression is less than epsilon. So, inequality is going in the right direction, hopefully. So, this is epsilon or 2 epsilon. And let's look at this one. So, for that, I need to do some more work.  Okay, so remember there's a minimizing purpose actually I can even put equal to out here. What does this mean? The same thing, it means 1 upon 2 to the r sum over m. Sum over m trace of yeah okay so so rho m is actually the uniform mixture over one to two three because all the messages are uniform you can see what it come so all that it comes down to is this m y rho y okay so this is equal to 2 to the power minus d epsilon h Okay. So now, if I plug that out here, so this becomes minus four. There's the same averaging. Okay, so that goes from m hat equal to one to m minus one. But just for the record, let me make that inequality even worse and range over all m hats. Okay, and then I'm averaging over m. So I'll get this out here. And this becomes 4 times 2 to the power minus. times 2 to the power minus d epsilon h so this is the probability of success the average probability of success 1 minus 2 epsilon minus something like this yeah uh yeah oh yeah oh yes it should be r thanks i'll show it factor of two rudier yeah that's of course yeah so now Yeah. So now what does this give me? This tells me that if r is less than 2 to the r is there. So d epsilon h minus something like 2 log 1 by epsilon, something of this order, then this entire quantity will be larger than 1 minus maybe 4 epsilon. Else, maybe four epsilon or something. Okay, so that's the thing. And okay, so I will not, I mean, detail how to go from this d epsilon H to the other one, okay, to save time. But this is essentially the idea of sending classical messages through a noisy quantum channel, and you can see where entropic rates come in. Where entropic rates come in by this simple thing. Okay? Yeah. Okay. Any questions? Okay, so this is called a packing argument for the following reasons. So what does this example of sending classical messages to a noisy quantum channel, what does it tell us intuitively? Remember, Remember, as long as m is less than 2 to the r, we have somehow managed to send little m through the noisy quantum channel and decode it correctly up to epsilon. So, this is only possible if two messages m1 and m2 produce states produce where do I have that state? Yeah, produce states rho sub m1 and rho sub m2. Rho sub M1 and rho sub M2, which are almost orthogonal. Okay, or else this would not have been possible. No decoder would have existed. So the aim is that the encoder had to choose encodings for M such that their corresponding outputs at Y are almost orthogonal. Now remember, I mean the output Hilbert space Y. I mean, the output Hilbert space Y has a certain volume. I mean, that is especially in the dimension of Y. So you are packing the space of dimension Y, okay, by almost disjoint density matrices. Okay, so the picture is something like this. This is like all of y, and this is row m equal to one, maybe a little bit of overlap with rho m equal to two. overlap with rho m equal to 2 and so on and somewhere here is rho m equal to 2 to the r okay so what this proof says that it is possible to pack up to this quantity and there are sort of matching pretty tight converse results so that's why this is called a packing argument and the whole business of sending classical information through a quantum channel okay now this was CQ channel you can have Now, this was CQ channel. You can have a QQ channel, quantum input, quantum output, but the philosophy is the same. It is to do packing. And all the packing quantities are controlled by d epsilon H. So that's the first simple message I want to send. The packing type problems are controlled by d epsilon H. How much time do you have? 10 minutes. Okay. Yeah. So any further questions before I move on to covering? Yeah, in a sense, I mean, you want to accept rho and you want to reject sigma. Yeah, yeah, yeah. So, so, of course, rho and sigma now, rho is this uh joint state which I have somewhere, but uh Yeah, somewhere. But yeah. So rho is the joint state rho my, and the sigma is like the uncorrelated, so the tensor product state. So the joint state has to be accepted, but the tensor product state has to reject it. Yeah, but yeah, but that encoding will be useless. Yeah. Okay. Good. So let me Okay, so you have to do a random coding argument. So for M, you choose a random symbol according to the probability distribution px, which you fix up there. Okay, and then repeat the same argument. So that's basically it. So basically, a message M will be encoded by a single symbol X, but this X is chosen randomly. But this x is chosen randomly according to px. So for m equal to 1, I choose a random symbol x1, m equal to 2 independently, I choose a random symbol x2, all from px. Okay, and then the same argument will go through. So that's it. So yeah, I didn't talk of the random coding because I thought it's, I mean, the essence is just this. Covering okay, so for covering I want to come up with a with a similar thing. No, again there is a CQQ channel. QQ channel. Okay, so there is, of course, the message M, which will be sent through an encoder, symbol X. This is row Y X. Okay, something. This is how it looks. But there's an eavesdropper, quantum eavesdropper sitting here. Okay. Eve. So I'll call it rho E subscript X. Okay. Subscript X. Okay. So this is what is called the wiretap channel. So there is the sender Alice, the legitimate receiver Bob, but there's the eavesdropper Eve sitting here. Okay, wiretap. The eavesdropper is doing the wiretap. Now the aim is to send the message M successfully to Bob so that Eve has almost To Bob, so that Eve has almost no idea about it. Now, how will this be done? So, to solve this problem, actually, one has to do both packing and covering. So, first of all, we have to do this entire coding thing for the packing as before. So, the rate that we will get will be something like, let's say, d epsilon h. Okay, so let me define the CQ state out here. So, rho n. Here, so row M Y Especially, so I should actually, I should not write it this way. So, it's really a joint state. This is This is rho y subscript x. Okay, so this is actually joint quantum state. So this is how it looks like. Now first want us to solve the packing problem. The packing problem completely ignores E and you will feel that, okay, I'll get a rate something like m rho m y rho m tensor rho y as before. But then I have to subtract a quantity. But then I have to subtract the quantity to fight against the eavesdropper, and the covering part comes in that subtraction. So, we'll see what that subtraction is. So, recall when there was no wiretap, just in the previous packing problem, so the message M, as Ashwin asked, is actually encoded in a symbol XN, a single symbol. And then that went into the channel. The difference here is that now M will not be encoded in just. Now, M will not be encoded in just one symbol, but in a bunch of symbols. So I'll call it X1, X2, M, X, 2 to the small R, not that capital R, smaller, M. Okay. And now the encoding strategy is that, so for every message, there's a bunch of 2 to the R symbols. The encoding strategy is pick a random symbol from this bunch, send it to the channel. Okay. Now, the hope is that this averaging obfuscates against Eve. So there is Y, there is E here. This averaging obfuscates against Eve, but does not obfuscate against Y. I mean, this is only possible if the channel from the input to Y is less noisy than the channel from the input. Noisy than the channel from the input to the eavesdropper, okay, or else as we'll see, we're going to get zero rates. Okay, so what is the aim? The aim is there should be a small set of symbols such that averaging over that of first case against Eve. So what do I mean by that? One upon two to the r  So, let me explain what I mean by this. Okay. Yeah. So, how does one show the existence of a small bunch of these symbols? Of a small bunch of the symbols. So, again, actually, the idea is random coding, just use a probabilistic method. So, choose, so m is fixed, choose x1, x2, x2 to the r iid from that distribution px, which are fixed on the alphabet. Now, the resulting output of Eve, I'm only talking of Eve, so will be rho e with the subscript xi. And I'm averaging over this. And I'm averaging over this bunch, so 1 upon 2 to dr. So this is the sample average over this small bunch, the bunch average. The hope is that this bunch average is close to a fixed state. And in fact, you can guess what this fixed state is. The fixed state will be the marginal of on E out here. Okay. So if I would trace out everything here except E, I would get rho E. Yeah, Ashu. No, I'm only talking of, no. Yeah, I mean, good question, but no. I'm only talking about obfuscating against Eve right now. We'll bring Y back into the picture later. Okay. For the covering part, I only want to confuse E. So I'm only looking at the resulting state of E under this bunch averaging. So this is the resulting state of E. So, this is the resulting state of E under the bunch averaging. It should be close to a fixed state, rho E. Okay? Huh? Yeah, yeah, yeah. I traced out Y. Yeah, so this expression, I traced out Y. So this has to be close, so within trace distance less than epsilon. Now, of course, this is not going to happen for any choice of the bunch. But the hope is if I choose the bunch IID, okay. The bunch IID okay from Px, the expectation of this quantity typically will be epsilon. Yeah, yeah, there's a bunch of ones. So for m prime, there is x1 m prime, one bunch. Technically, but yeah, it will be the same epsilon, of course. Yeah. Yeah. So so what is the picture? So, what is the picture like? So, the picture is, this is what the Hilbert space of E looks like. Okay. Now, this is what row x1m looks like. Okay. This is what row x2m looks like. If I average over all of that, okay, I should be able to cover the whole thing that is rho e. The whole thing that is rho E. Okay, so here they were almost orthogonal, that was packing. Here it is covering. I mean, so they will expect a significant overlap, but it all gets covered. So to get rowy. So that's why this is called a covering problem. And it turns out So it turns out that if r is larger than like a different quantity d epsilon infinity So, if R is larger than it's a different entropic quantity, this covering happens. Okay. And this is actually work by Rahul Jain and his co-authors, Anuraganshu, Vamsi Krishna. So, this is called the convex split lemma. The actual lemma. The actual lemma is even more general than this, but we are just looking at the covering version of it. Okay, and I think I'll stop by just defining what this quantity is. So again, I'm in d epsilon infinity of, let's say I have two density matrices, so rho and sigma in the same Hilbert space x. Okay, so this It's defined as log or maybe might absorb the logout here. So, let me explain what I mean by this. So, this is the way, okay. Um, the way okay, this should be main, yeah. You're right, I'll just get confused. Yeah, the way I've written it, yeah. So, so this is the minimum over all real numbers, lambda. Okay. So, what is lambda? The aim is I want to upper bound. Well, ignore the row prime for now. Let's say I put a row here. I want to find the smallest lambda such that Lambda such that rho is overpowered by 2 to the power lambda times sigma. So, what do I mean by inequalities between matrices? This is just another shortened for saying that this quantity, rho prime, is positive semi-definite. Okay. In other words, I mean, of course, the Hermitian matrix, when I say positive semi-definite, all I'm saying is that its eigenvalues are all bigger than or equal to zero. Okay, so I want. zero. So I want to find the minimum lambda such that 2 to the power lambda times sigma overpowers rho. Now it might be difficult to overpower rho in its entirety. So I'll perturb rho to some rho prime which is close to it within epsilon. And then I'm going to overpower that. So that's basically this quantity d epsilon infinity. It's also called d epsilon max sometimes. Okay. Now, so what we have seen is we have seen two entropic quantities. we have seen is we have seen two entropic quantities d epsilon h d epsilon infinity and in the earlier talks also i mean we have seen similar quantities so the uh so the aim of this tutorial like so i don't have time to cover decoupling that's the third thing but essentially all of this quantum shannon theory can be done by using a combination of three techniques the packing things are controlled by d epsil and h in a sense as i mean uh like in the previous discussion marco the previous discussion marco when there was talk of rainy entropy so d epsilon h is like the zeroth rainy entropy d epsilon uh infinity is like the infinity or the uh also called max is like the infinity rainy entropy in between there is the d uh d1 you can say which is the usual uh kullbeck labeler divergence or the shannon relative entropy okay nicely sandwiched between them now both these quantities uh d epsilon h d epsilon quantities d epsilon h d epsilon infinity satisfy data processing in their asymptotic iod limits they both go to the shannon relative entropy the decoupling problems are also controlled by d epsilon infinity so in that sense they are closer to covering than to packing okay and every other reni entropy okay essentially gravitates to one of them i mean d1 is special if you take reni entropy d alpha which is less than one that will be alpha which is less than one that will behave more like d epsilon h okay any entropy order zero if you take d alpha for alpha bigger than one okay it will behave more like d epsilon infinity so this is the i mean the maybe slightly oversimplified but the clean picture i want to give for uh shannon theory that there are three types of problems packing covering decoupling packing is controlled by d epsilon h covering and decoupling are controlled by d epsilon infinity and two d epsil infinity and to finish with the wire tap the rate that I get is actually d epsilon infinity rho m e yeah just have to subtract this and yeah and yeah there's some log one website or something of course yeah 