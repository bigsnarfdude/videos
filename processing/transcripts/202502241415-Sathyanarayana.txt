Hi everyone, I'm Artie or Arti Satya Narayana. As I said this morning I'm an assistant professor at Northeastern University and I'm joined between the Computer Science College as well as the Health Sciences College. And so because my background's in computer science and I was kind of thinking what would make sense to present today, I thought I would kind of do an overview of the projects that I work on to kind of help you understand kind of what the work that I'm doing in different application areas. Different application areas. And I know I met a bunch of you at the pre-ENAR workshop. So I'll go into a little bit more depth than I did there. So hopefully, it'll give you an idea of the kind of work that I do. And yeah, leave it for that. So I'm going to say we instead of I, and that's because I run like the SAPLA, which is basically where all my PhD students are. And what we work on there is basically digital phenotyping and digital black records. And digital black records. So I kind of mentioned this this morning, but basically, our lab focuses on signal processing and machine learning from digital health data. So we focus on device data in particular. So smartphone data, smartwatch data, and primarily EEGs on the clinical side. And so when I say digital phenotyping, I'm usually referring to how we measure behavior. And we usually do that from smartphones or smartwatches. Smartphones or smartwatches. For digital biomarkers, I'm talking more of the physiology, and so I'm talking more about wearables and clinical data. So, in this case, EEGs, apparently. That's kind of the work that we do. It's going to seem really broad, but this is what we're doing in the background. We're doing signal processing. So, again, who's the we? I currently have three PhD students in my lab: Shashank, Amanda, and Mohina. I have a master's student, Sony, and an undergrad, Angelina. Undergrad and Jamina. We also have a bunch of students who've come through the lab who have contributed to the work that I'm going to talk about. So I wanted to shout them out because they are working hard on this. So the application areas that our lab focuses on are relatively broad, and they're, again, we're getting a bunch of different types of data. So I wanted to highlight kind of the different types and where those projects sit, because obviously each of these projects could probably collect all different types of data. But right now we're focused on specific. But right now we're focused on specific things. So we have a project looking at abuse and disagreement, and we do that by looking at smartphone data, specifically text messaging data. We have projects on stress and pregnancy that are doing again smartphone data as well as wearable data. And I'll go into more depth on which wearables and which smartphone data. Our sleep work works on, again, wearable data as well as portable EEGs as well as clinical EEGs. As well as clinical EGs. Our epilepsy work is technically actually clinical EEGs and we're trying to move more towards portable EEGs as well. There's obviously a lot of issues with that and we're not quite there yet. And then the last piece that we work on is mobility. So we're using wearable sensors in that case, but we're working with VR as well. And I'll explain how we do that. And for each of the projects, I think sometimes it can be a little confusing on where we're at. Be a little confusing on where we're at. And our lab really works on projects across the research pipeline from the brainstorming and study design. Sometimes we're doing the actual data collection, but then sometimes we're just doing methods development and analyzing already existing data. And sometimes we're just doing research validation. Well, sometimes we're really trying to do clinical validation. So for each of the projects I'll talk about, I'll highlight where we are along this pipeline and where we want to go. So it's a little more clear, like what our lab is doing. Otherwise, it sounds like we're doing a whole lot. Because it sounds like we're doing a whole lot, and maybe it's not clear. So, I'll explain you to that. So, the first project I want to talk about is called Digitally Phenotyping Pregnancy. And this was my first sole PI grant, so this is really what I'm working on the most part. And it's a collaboration with Tufts right now. We're also hoping to extend to the Brigham and hopefully more hospitals because really, we are not. So, the northeastern side, my left side, we're not interested. Northeastern side, my lab side, we're not interacting with participants. We have research assistants at Tufts that are going to be in the waiting room, meet with patients, enroll them in the study, and all the data collection is happening remotely. So hypothetically, we could mail someone devices and do this study remotely too. So really interested in extending that. Primary goal of this study, so NICHD funded this, so they're interested in how the child's health and development is going to improve based on digital phenotyping data. Front based on dutylogenotyping data. So, their interest was: how does a mother's well-being influence breastfeeding outcomes? Because breastfeeding is shown to improve the child's health and development. I realize this is a little controversial for all the mothers out there, but that's why they're interested in it. How can we help mothers have better breastfeeding outcomes, and hopefully that will improve the health and development of their child? So, our approach is where we're going to be measuring that well-being via digital phenotyping data. So, I'm talking wearable data, mobile data. I'm talking wearable data, mobile data, EMAs, surveys. And some of our secondary goals are to see how that mother's digital phenotyping data then correlates with risk of postpartum depression, adverse pregnancy outcomes, and a bunch of other pregnancy-related outcomes that are kind of harder to get funded. So while we're collecting this data, we're like, let's also collect that outcome data so we can tie it all together. So the data that we're collecting is So the data that we're collecting is going to be Garmin data. So we're going to use Devarman's to collect accelerometer and heart rate. We are going to use Beavey, for those of you that are familiar with it, to collect phone data, so call text logs, a little bit of GPS, Wi-Fi, Bluetooth, etc. And we're also issuing daily handmates to assess what their mood is like, what their sleep quality is like from their perspective, and then things about the baby's health as well. We're using Coltrix to do very in-depth surveys in the third trimester. So, this study is going to go for about 18 weeks. It starts at the third trimester, goes through six weeks postpartum. And really, that length is a budget constraint. We would love to do it earlier in the second trimester. First trimester wouldn't be feasible because they wouldn't go into the doctor, they wouldn't necessarily know they're pregnant. But we're really trying to get a lot of this data from third trimester and 16 plus partum. Third trimester and six-seats postpartum, and understand things like their mood, their autonomy, what their support systems look like, their access to food, their access to public transportation. And there's like 300 questions. It takes 10 minutes for them to fill out, but we ask these questions at three different points. And then at delivery, we're asking a lot about what their experience was in the hospital, what outcomes they had, etc. And then we're using. And then we're using Huckleberry, which is a mobile app where pregnant women often, actually, new moms often track their baby's data. So this is what a lot of women are using nowadays. So it shouldn't be too out of the ordinary what they would do. And they use it to track when they're breastfeeding their baby, when they're bottle feeding their baby, how much their baby is sleeping, how much tummy time they're doing. So that's really where we're going to get a lot of those outcomes of how the child is. Lot of those outcomes of how the child is developing. Obviously, we're only getting six weeks postpartum, so that's not a lot of stuff that we're getting at that point, but it's something. So this study, we're really at the data collection part. We're just waiting for the IRB final change to come through, and then we're going to start enrollment. So we're actively on this project right now doing that. And then the idea is we'll do some analysis, we'll do some methods development. We'll do some methods development, come up with new analysis, and kind of go through that circle. We do have clinical partners, so it would be great to move the ideas that we find towards clinical validation. But this is really hypothesis-generating, right? We're getting a ton of data. We just want to find patterns. We just want to check the feasibility of even doing this kind of study because it is so much data. And it's also a lot of data as women are going through a lot of different changes. So we're kind of trying to do some hypothesis generation here. Here. So, for every project, I've also put a need what I can guess why. I think, just as a junior faculty member, I wanted to ask for what I need, and also in the mode of Vadim was saying it's about dialogue, not monologue, this week. I wanted to say what we could collaborate on and what we could work on and hopefully inspire some thoughts here. So, what I need is advice on running a study like this. Again, we're trying to do a lot of Trying to do a lot of data collection over 18 weeks. There's surveys, there's EMAs, there's continuous data. We have incentives for every week, which is why there's a budget constraint on how long you can check the data for. But this is a lot of data collection to get, and the more incomplete it is, the harder it's going to be interpreted. I also need help with extending the study to new sites. That's kind of the goal of what we want to do here. So, introductions to OBGYA departments and researchers. OBGYN departments and researchers in this area. Women's health is something kind of new for me to work on. I think it's such an understudied area. And as a woman in computer science, I was like, why not me? Why not start doing something like this? But realized I had to start way back and start with the data collection. So that's where I'm starting. And hopefully we can do lots more analysis and lots more computer science once we have some data. And then I'm also looking for new methods for analyzing this kind of data. We're going to have a pretty small N in the beginning. We're going to have a pretty small N in the beginning. We have about 30 participants to start. And then, again, we want to scale eventually, but for the feasibility component of this, we're piloting with collecting data from 30. So methods for analyzing that kind of data, maybe out of one, right, could be interesting to see since we're getting such longitudinal data. And then what I can give is: I'm starting a new area in Women's Health, really want to bring more technical and mathematical research. Want to bring more technical and mathematical researchers into this space. Again, just have to start with the data collection part so we have the data to analyze. And then I can hopefully give some access to some really cool data about a year from now. It's going to take a while, right? So we're starting enrollment, hopefully, very soon. In about a year from now, we should hopefully have 30 participants' worth of data. So we'd love to share that and do lots of cool stuff with this data. I have an extension from Kind of an extension from that project is a collaboration I have with WOOP where we're looking at understanding how sleep and pregnancy relate together. So our primary goal in the study is how does multimodal sleep health vary across pregnancy? Of course it does, but we want to quantify what that looks like and see how that changes for different women, do some clustering, et cetera, and see if we can understand that sleep and quantify it. So our approach is to analyze loops, really large data sets. Our approach is to analyze Loop's really large data set from pregnant women. They have a few thousand that they've already identified, that they have data before they're pregnant, when they're pregnant, and after they're pregnant. So we can really look at how that multimodal sleep health changes. And some of our secondary goals are really to build prediction models for that. So if we can build prediction models for, for example, preterm birth or gestational age of delivery based on multimodal sleep health. So can we predict if their babies will be born preterm based on sleep health? Based on sleep health. For those of you that work in sleep, you know, like sleep is one of these fundamental things that affects everything. But a lot of people don't know that much about sleep health. And so we see it as being an underlying factor for many other different things. And so we want to try to see how that, quantify how that's changing for different birth outcomes. Obviously, in this case, we're looking at WOOP data. So we'd have accelerometer data, JavaScript data, heart rate data. Data, JavaScript data, heart rate data, temperature, and then Woop has some surveys. I don't know if any of you wear Woop. I wear mine every day. But it asks you some surveys in the app, so you can fill those out every day. Honestly, surveys are very minimal data. But that is how WOOOOP knows if someone's pregnant or not. It's based on someone inputting that into the app. And there are benefits to doing it. It tells you how your pregnancy is changing throughout the different terms. It shows how you're. Terms. It shows how your data compares to pregnant women on a whole. So there are a lot of benefits. So there is a decent amount of involvement of people putting that in. So WHOOP says they're going to give us raw data? I don't know. Right now we have more of the accumulated data, so they're aggregating it at a day level. So it's not that in detail, but they said they'd give us raw data. Detail, but they said they'd give us raw data. So, this project, you know, we're not doing any data collection or study design, we're really using existing data. So, we're just focused on the analysis, methods development, and ideally new and improved analysis. We could move towards research validation. We might incorporate the algorithms, et cetera, but that's really where this project is going to sit. Some of the needs for this work is: how do we analyze? Work is how do we analyze such aggregated data? A lot of the work I've done is on accelerometer data, so this is much more higher level. That being said, we do have thousands of days because this isn't just a study, right? This is people who wear the woobe, they're customers, they're paying for it. So there's thousands of days of data for a lot of people. On the smaller end, it's maybe 300 days. There's about a year of data for everyone in the data set. And I know funding is like a trigger word for everyone these days, but Word for everyone these days, but I could use some inspirational funding ideas for this type of secondary analysis and how to think about that because I'm not doing the data collection here, so what sort of funding makes sense to look for in this case. What I can give, I can't give data access here of course, because I have WOOP that has the data and they're sharing it with me. So, but what I could do is collaborate if anyone has different methods that they've used to analyze them. Data. That's just the 15-minute timer telling you that. Okay, I have a lot more. And what I can give is collaboration by this idea sharing. So, if you have methods that you think could be appropriate for this data, love to implement them, especially if you have code or something, we can figure it out. And then also validate any results that you might have on this large and for the most part healthy data set. WOOP has selection bias in that most people using a WOOP are generally athletes, they're pretty healthy, they're tracking their fitness and their recovery, and so. Fitness and their recovery, and so we can. We also have the data for when they're not pregnant, so we can really look at this healthy population as well. Next project, detection of disagreements. This is the abuse and disagreement project I was talking about. The primary goal here is to passively identify signs of abuse in relationships. Our approach is to do that from the text messaging data using large language models. So, some of our secondary goals are we just want to predict disagreements before they exacerbate, so pick up an early. Masturbate, so pick up an early signs of disagreement from text messaging data, identify those romantic relationships that are at risk. So that these are, we're specifically looking at a population of people that have come from juvenile detention centers. And so there are resources that could be put in place to kind of monitor what's happening. They're extremely at high risk. I think this study aimed to collect data from about 300 participants. We've only analyzed a small portion of that so far. Analyzed a small portion of that so far, but of like the 50 participants, like 45 had abuse in that relationship. So it's very, very high selection bias here. And we want to identify those relationships, and then we also want to reduce the likelihood of a disagreement escalating into abuse. So while we're not identifying abuse, we're identifying some sort of disagreement so that there could be an intervention made. This is on text messaging data. So it's the actual text messages. So, it's the actual text messages. We have the content. We know what they're saying. We were collecting images. There's a lot of youth here. There's a lot of sexting. So, we stopped collecting images. But we know when an image is sent, we know the timestamps. We know if there's emojis sent. So, we can kind of... The emojis are helpful for telling if there's sarcasm or different things like that. But this is, as you can imagine, there's so much slang in this data set, it's very difficult to interpret. So, we've been really doing a lot of interesting. So, we've been really doing a lot of interesting methods development in this space. And this project, again, our collaborators did the data collection. So, our involvement is just in the analysis component. We do have a couple papers on this topic so far. It's a very difficult problem, but that being said, we're getting about 75% balanced accuracy on a test set. So, what I mean is we have, this data is collected from two different studies. So, in the first study, you know, we had our test, our training, our validation. Test, our training, our validation set, but on that same data set, meaning that the participants are both in the training and in the test. We were getting about 75% balanced accuracy. When we were doing on a totally separate data set, totally different participants, which means we've now trained on the language of different people, we're getting like 57% balanced accuracy, which kind of makes sense. But that was quite a big drop, and 57% is obviously not great. But yeah, these are. But yeah, these are the two papers that I direct you towards. One of them we focused just on the metadata, so we didn't even look at the content of the text messages, just the length, just the frequency, et cetera. And the other, we looked at the actual communication, and that's where we really used LLM's. On this project, we kind of need some new ideas. What can we do here? What we can we detect. Again, funding ideas for this type of secondary analysis. Type of secondary analysis, and then also the best audiences to share this kind of work with. When we talk to psychologists, which are people who we're collaborating with, and we talk to their community, they're like, LLMs? Whoa, right? Like, what's going on? So, who were the right people who are interested in this type of work? I'm sure they're there. So, any of you with psychiatry background, et cetera, I'd be really interested if you had ideas on who would be interested, what journals, what conferences, et cetera. What I can give, again, is collaborative. What I can give again is collaboration. This is a novel area. There's not a lot of existing work on this. So when we're, you know, I'm saying 57% is pretty low. There's really nothing else. It's manual detection. So we're improving if nothing else. And then I can also provide expertise on using LLM models and interpreting messaging data and text data. Another project is on stress detection. So here we're trying to passively identify stress and control. To passively identify stress in controlled and uncontrolled environments. We want to train models in the lab and extend it to the wild. And we also want to benchmark stress measurements across different devices and situations because different devices always give you different answers and so it's a little tricky to do that. Here we're collecting our gold truth is from Biopath. We're using this in the lab. So we're inducing stress in participants. We have a stress protocol. We take them through different types. Stress protocol, we take them through different types of stress, and you're tracking their biopak data. We're also collecting their Garmin data as well as Empatica data in the lab. Then we send them home with the device, so whichever, basically have the biopak and one of the Garmin or Empatica randomly selected. So we can compare that data and then we send them home with whichever wearable they wore and then collect EMA data as well as some mobile data. We're also getting some Qualtrics data. Our primary interest there is we're collecting things like personal. Primary interest there is for collecting things like personality, demographics, so we can do some sort of phenotyping and figure out trajectory clusters, etc. This project, we really started from the ground up, so we're doing data collection, study design, we're also now moving into like the methods development, analysis, new analysis, et cetera. So the needs for this project are this is very, very, very messy data and incomplete data. Messy data and incomplete data. Don't mind my notes on top. And what I can give is access to a data set. It's really messy though. Expertise in running this kind of study. If anyone has not done that, I can provide our protocol, et cetera. We have a stress lab protocol that we put participants through three different types of stress. And then we have open source code for doing this kind of stress detection that works really well in the lab, and that we're trying to move that to work well in the wild, but collecting the types of data that this works. But collecting the types of data that this works well with, like EDA is a little tricky. We were using the empatica E4s, now we're going to move to the Embrace, so maybe that'll change, maybe it'll get better. Another project, predicting manic depressive episodes. Primary goal here is to predict them before they occur. We're going to use wearable data to identify patients that are similar. And our secondary goals are to phenotype patients based on this behavior. This is a project with Bergen Women's Hospital, and they had a lot of fit. And they had a lot of Fitbit data. So we just have the accelerometer and we have heart rate. And we're trying to track that over, I don't remember actually how long the data is. But yes, so we're basically, we didn't do the data collection, our collaborators did. And so again, we're focused on just the methods development and the analysis on this component. Here it could go towards clinical validation because we are working with clinical psychiatrists. I don't know if our lab will really help with that, but this is. Lab will really help with that, but this is kind of the goal of this type of work to really push it to clinical care. Needs, so new approaches for longitudinal clustering. This is quite interesting, diverse data, very homogeneous. Also, looking for validation data sets, because whatever we build on here, we want to be able to validate on another data set. I can give different collaboration ideas that are related to this, and then again, the validation of your work on this data set. Of your work on this data set, or validation of the work we do here on this data set. Great. Almost to the end. So, another project that's kind of new, we're working on quantifying orthopedic mobility. So, here we're trying to quantify that mobility for improved diagnostics and treatment tracking. So, orthopedic surgeons kind of ask you to do a few exercises and visually determine if you're improved or not post-surgery. There's no quantification. Surgery. There's no quantification going on. So if you go to a different orthopedic surgeon, they might give you a different measure. It's just very inaccurate. And so, what we want to do is using wearable sensors to measure that movement while we're controlling that movement. So we'll ask someone to wear these VR goggles and ask them to do something like slice bread. And I'll show you what that looks like in a minute. And then our secondary goal is to then be able to design specific tasks that accentuate that theme typing. Accentuate that phenotyping. So, our goal is: we want to be able to tell these people have perfect mobility, or perfect stretch, but they have good mobility, and these people have an injury. So, there's certain tasks that will be easier to be able to tell that apart, depending on what injuries we're looking for, and certain ones that will be harder. For example, if someone has a rotator cuff injury, asking them to walk up and down the hallway might not issue that. So, we want to be able to determine the difference. And this is an example. And this is an example. So, here on the left is someone slicing bread that has an injury, and on the right is someone who does not have an injury. So, they're able to slice the bread. We have sensors, just three sensors, the headset, the left controller, and the right controller. And so the headsets don't look too different, but they do a little bit. So, the person who was able, who had no injuries, just looked up and down, right, in a pretty straight action. However, the person who had an injury was kind of looking all over. Maybe it took them longer, maybe they were in pains. Maybe it took them longer. Maybe they were in pains, they're grimacing. Their left controller definitely looks like there was something going on here.