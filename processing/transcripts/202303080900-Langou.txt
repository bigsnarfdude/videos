This morning there we're gonna have uh we're gonna have two tutorials, uh one on both the kind of implementation related, which is I think I would put them this way. Uh first we'll we'll have uh Julian talk about software from numerical algebra and then I'll just let it go. Yeah, thanks. I'm going to get it started. Um so thanks a lot for having me. It's really a treat to be here and super super happy. I love those more things. And I'm going to get started to speak And I'm going to get started to speak about some software on numerical inner algebra. So some disclaimer, I'm really, really, really biased to tense numerical inner algebra. I'm going to try to draw trends, patterns, and software, what make a software successful. I'm going to tell you some stories. At the end of the day, I'm looking at how I develop my own software and I'm not following any of these lessons, okay? So don't Any of the lessons, okay? So don't judge me. I'm not judging anybody. And then the last one is: I'm going to mention a few software, and I'm sure I'm missing a lot. So if you want to send me an email with your software, I'd love that. This is what I want to go about. This is the content. I'm just going to go by listing a lot of software, and to do that, Jack Dongara has a list on the web, so you can just have a look at it. I think the idea of those slides is really to see the quantity. Is really to see the quantity. So if you go on his webpage, there is this freely available software for linear algebra. He tries to maintain the layer and he tries to break down the software in various classification. Direct solver, what's mean by direct solver is really dense. It really means dense linear algebra. And so this is all you have just for dense linear algebra. And so, okay, so I'm not going to go down the list of everything, but things that are Of everything, but things that are important is what does the software do, what does it support, the language, the copyright, and so you get all this information. There is really a plethora of software, and sometimes you can ask why. Why do we have so many? And it's it's interesting. So I'm going to try to explain a little bit about the why. Uh it it's interesting, it's an ecosystem, I mean software depend on each other. I mean software depends on each other, software competes against each other, soft software above, software dies. It's interesting. And so pretty much all those software at the bottom are basically, if you know LAPAC, they're LAPAC-like, they do dense linear algebra like LAPAC, right? So kind of all the same, but with a different flavor for everyone. And then Jack goes along, so I'm not. And then Jack goes along, so I'm not going to I'm not going to speak but about that, but there is also a lot of sparse Direct solver. Uh there is uh software for preconditioners, there is software for sparse iterative solvers, there is sparse eigenvalue solvers, and the last one is support and uh hierarchical matrices. So he just said that recently, so lots of software for hierarchical matrices, and the last one is about uh support routines, so for example the blast and yeah. So point is that the So point is that there is really a lot going on. And moreover, we forgot some software that I think are very interesting. So there is TLAPAC, there is TAO-N for UMEA, there is MLAPAC, which is coming from Japan, LELEPAC was done in Germany, ROND-LEPAC with Mark Mahoney, all those, I mean, software move fast as well, so you have to keep track. The survey was done in 2021, okay? So lots of software. Some lessons. Some lessons I've learned. There's things, and I couldn't read that word there, so maybe it was there. All these more recent things in ML and Python that in some cases call these, but these also sometimes call each other, as I said. What was, if one wasn't there and if it wasn't these sort of higher-level ones that machine learning or other people develop, and if not, why? Is Jack's criteria just that he knew this stuff better, or was it? Just that he knew this stuff better, or is that in the design space very different? Yeah, I think he was, I think Jack's focusing on numerical linear algebra, so his definition of numerical linear algebra. So if I have one of those things and they, I mean, some of them have randomized SVD in there, which is so algebraic. So I don't know if it's not there just because the time hasn't evolved or because it calls something else underneath, like the QR code. No, I think there is no one. I mean, but Ronda LePac, I think there is no software in the list that is doing. I think there is no software in the list that is doing randomized metrics in that list. They're all deterministic here. You know, not worry about the random decision. I just wonder what's the design criteria for choosing those things or not? Is it that it's a lower level or what's the... Yeah, I understand. So the question is, why is Randall Evac not in this list? No? No, no. No, I don't have an asteroid. I'm not saying why is it. Oh, it is in that. But the question is, what's the, some of those routines call each other, and there's other ones that he doesn't have. And there's other ones that he doesn't have. So even if you're not going to talk about those, is it that you say something like, well, they're just linear algebra things, and the other ones then aren't. But maybe they are and they're doing something different. So what's the sort of design criteria for those lists? Yeah, I don't know. I think soon, I mean, quite frankly, I mean, the list is from 2021. Quite frankly, I can see random as linear algebra as being a sub-list soon. Okay, some lessons. If you want to be a good search. If you want to build a good software, what you need to do, I think everybody got this one. You want something that performs fast, you want accuracy, stability, reliability, you want something generic enough. The other thing is probably you want a test use, you want example, you're going to think about being used and your user, you want to use a fine. So that's kind of fun. Something, if you look at all these softwares, they have something in common, and it's pretty interesting, is that they have something very simple that you can say in one line, one sentence, that summarizes the whole software. One sentence that summarizes the whole software and the whole idea. And I will go, I will explain you those ideas for some software. Maybe I'm biased and maybe this is my opinion, but when I see LA packs, there is one idea. When I see Scala packs, there is one idea behind it. When I see Flame, Elemental, all those software, they have one very simple idea. And then you build a whole library on it, and hopefully your idea was a good one. That's really the idea. Oh, yeah, but of course, an idea is not enough. Okay, you need hard work, you need to have some external. You need to have some extension on how to do software. You want lots of coverage, you want your software to be robust, which is hard to get. You want your software to be portable, you want to be able to interoperate with many, many other softwares. A good idea is to try to offload, so I'll be speaking about that, you want to offload your work to other software, typically middleware and things like that. And so that's very, very important. And actually, that can be your idea. Your idea can be, I'm going to offload everything. Be, I'm going to offload everything to a runtime. That can be the design of your library. Marketing. If you want your software to be successful, you need to be out there, people need to know about it, and you need to market it. And all those software successful have had somebody marketing hard, hard, hard for them. You need a community hopefully, and the goal would be to have your software system. But that's kind of the thing that you want for your software. There is one thing that is the SES mission. So network account. Uh so I'm not sure if you're gonna plan to talk about it, but like could you say a little bit of what the market of this kind of software is? Like so like what what kind of the what what are ways in which these are used? Somehow like I had this whole idea which is like most times we call it we call this kind of software panic, right? It's like MATLAB calls LaPag and Media calls LabP, and that's like all I know about it. So I'm kind of curious like how how is there a market for this? Well, I mean, hopefully, you find your idea, what you're going to provide to your user is going to be useful for them. And they're going to want it, and also you're going to do it right. So I think it starts by a vision. I mean, it starts by the vision of putting the software here because there is a need. So you... Well, I mean, just to speak about the Labac, if you remove a LABABABABABAC. Just to speak about the rebukes, if you remove a reback, there is a problem, right? Right, right. If you remove antagonists, definitely a problem, but like, for example, for something otherwise. Okay, no, no, no, no, it's it just starts with a vision, a guess, people take a risk, take a bet. There is actually probably my last slide. My last slide is my last slide is about that. I mean, you need to take a bet, and what makes the software successful is really not clear. At the end of the day, it's not clear. You need some of everything, but some software are. Some of everything, but some software are missing some components that are still successful. I believe that the marketing is very, very important. I think, yes. A lot of it is motivated by key design computer architecture. So Jim says a lot of it is motivated by key designer architecture. And does the architecture difference cause this difference in speed or is it usability? Like is it more kind of performance or is it more kind of like generic? Like which of the five criteria does change? The five criteria does change in hardware costs, like affect the most. So you can list the five criteria up there, right, for successful software. You mean performance, accuracy, stability, reliability, and easy general. Which ones have to be affected more by changes in color? Performance. Boy, performance, right? I mean, so if you look at the software, the way it has moved, we've been trying to catch up with the hardware. And so if you look at the history, we've been trying to do vectorization. We've been trying to do vectorization, 1980s, and then we're trying to adapt to caches, 1990s, and then we've been trying to adapt to parallel distributed and then multi-core and then accelerators. And then that has actually led to lots of libraries. Every time I said a word, there was a library. Library, library, library, library. Um okay, so you need all of that. There is something very important that you need. You need to have a very cool logo. That's that's what will say your source to. Okay. That's what we say yourself. Okay. So don't forget yourself. There is a quote that, I mean, please, if you start, if you go in the business of software, Bill Grop has lots of experience, done lots of successful software. He has done MPI and he's one of the developers of MPICH, which is an MPI library, and he also is one of the, at the start of PETSI, so some two very, very, very successful software. Successful software. It's very important. The challenge is not to make easy programs easier, the challenge is to make hard programs possible. So when I look at people starting software, they all start by the easy stuff, and then they work years and years on the easy stuff, trying to make it better and better and better. And that's really important. You really want to start by what's hard, what's not possible, and try to make it possible. But that's really hard to do. And so that's, I mean, this is very important code for development. This is very important code for me when I think about this. Something else is that related to Richard's question is that assume when the cache, when computers started to get cache, you're going to say, hey, I'm going to write a new library, I'm going to call it an APAC, and that's going to be great. The gains that you're going to see at the start with this idea are not big. When you w w when the multi-core times come, two thousand five, two thousand five, multi-core computer everywhere. 2005, with a core computer everywhere, started a library named Plasma, did some paper, speed up 10%. Super disappointed. 10% speed up, you're not going to rewrite a whole library for 10%, right? But we had a vision. We knew that Multicole was the future. And so now, if you run Plasma against LAPAC 10 years after, you get the 5x speed up and this kind of stuff, right? So it's really at the start, it's out. If you run now Leanpack, which If you run now Leanpack, which is 70s, with LAPAC, which is 19 plus, 90 plus, when you look at the paper in the 1990s, the speedup were not crazy. Actually, it was, oh, yeah, that was good, but it's not crazy. If you do it now, it's insane. The speedup are inside, right? So it's really start slope. It's the lowest start slope. I mean, it's simple. So don't be too discouraged. I mean, it's really a vision and a big bet. That's how it starts. And sometimes you break your teeth as well. That's part of the game. That's part of the game. That's that's part of the game. That's part of the game. Okay? Um, so this is the offload. I want to speak about the offload, and I'm just gonna because I'm not sure it's the correct term, and I'm just trying to explain you, but basically an idea behind Helipak is to offload to the BLAST, right? So we are not going to, if you look at the BLAST on how to do a matrix matrix multiplication, it's very, very complicated. There are lots of details on how to get performance increase as much as performance as you can from those lines of code. And you definitely don't want And you definitely don't want this in your linear algebra routing. So, one idea is to break this about. So, put the blast, the optimization for people who are excited about it, who know how to do it, and then write your linear algebra out. And so, this idea, you can find it a lot. These days, lots of people are trying to offload the scheduling, whether for communication or for parallelism, to runtimes. And people doing runtimes are really, really good at it. Are really, really good at it. They love us because we have very nice problems and so they can do lots of things: scheduling problems, heuristics, and things like that. So, runtime. So, these are examples of runtime, Legion Parsec, Star PU, OMPSS. Now, this is in the OpenMP 4.0 standard. So, I'll speak more about that. But one idea is to offer it. And similarly, compilers, I mean, compilers are good tools, MPI. Tools, tools, MPI or.ini or photo I want to do. And now I'm going to go about the one-line how I've used some software and how they had a vision and the vision was a good idea and it changed everything. That makes sense. So for an impact release, the idea was to vector it. So to write the code with level one blast and vector. That's it. I mean, that's it. If you look at the code before and the code after, what happened is pretty much Before and the code after, what happened is pretty much vectorization. For an idea that really sticks, so okay, and some ideas stick for a long, long, long, long, long time and not forever. And so one idea that you have in the BLAST NLA pack is this representation of matrices MNALDA. And if you look now in 2023, there are committees that are doing C ⁇  standards, and those committees are pretty much still using that. Still using that. Okay, so these are ideas that were in the 1980s you have it in ice pack, in impact, but really got very much generalized with glass and LAPAC to represent your matrices like that, to represent your vector like this. It's very powerful. And we're in 2023. Libraries have moved, libraries have changed, but this concept is still there. It still has been reused, it's still reuse, C ⁇, MD span and all that relies a lot on on on on those ideas. on those ideas. The idea of Alepac was to rewrite the the algorithm in level 3 plus. But but it's really a concept, okay? Listen to the concept. The concept is if I write my algorithm using as much level 3 plus as possible, I will be efficient. Is it true? Can you prove it? I don't know. Okay, that's a big bet. It's kind of true. It's kind of work. I mean, but that was the bet. I mean, but but that was the bet. The bet, let's write everything with level three plus and we it it will work great. There was no proof it would work, right? That was the bet. That was the bet, yes? Could you talk to say a little bit about the difference between level one and level three plus? Show social issues. Yeah, yeah, yeah. And so I, yeah, I, the talk is a little high level and I don't go into the detail of everything. And you can interrupt me as much time, and I can stop in the middle of the level one is vector operations. So X. So x plus y, x transpose y, so anything that manipulates vectors. Level 2 will be a matrix and a vector, so A times X, and level 3 is A times B, so a matrix semi-matrix. There is a big difference with a matrix semi-matrix because a matrix semi-matrix, the surface volume ratio is different, right? The matrices are N square and the volume is N cube of operation. So for sure when you do a vector operation, you do Sure, when you do a vector operation, you do O of n operation for O of n data, your arithmetic intensity is going to be O of 1. That's it. For a matrix vector product, it's the same. N square data, N squared operation, you're O of 1. I am not saying that matrix, matrix multiplication, I'm saying that for matrix resolution, you have hope to have a good arithmetic intensity, right? Because you have, so that's it. So level 3 plus is basically matrix matrix operations. And in particular, the idea that is interesting is The idea that is interesting is that the arithmetic intensity is higher. And so if the arithmetic intensity of the blast is higher, that's a fact, you know that. Does it translate to algorithms that use this? It's not that obvious, but that was the bet at the time. That was the bet at the time, and it works good enough. Scalapak is something, a very high-level concept. The idea is that you want to do parallel distributed computation, put your matrices. Put your matrices, distribute your matrices on your processor as a 2D block citric. Take your sequential standard algorithm, press the button, it will scale forever for ready operation. That will be wonderful. There is no proof. There is no proof. It's crazy and it works very, very well. Very, very well. So the idea is to put your matrices on two diplom cyclic, so that's a distribution, and then use standard algorithms. Then use standard algorithms. Well, I mean, try to be smart at times, but use your standard algorithm and then that will scale. It's insane. It's insane. And yes, it will work for parallel matrix matrix multiplication, sure. Will it work for LU? Will it work for an eigenvalue solver? Will it work? And so then you have this, it just unleashes everything. Right, that makes sense? So those very simple ideas lead to very powerful libraries. Yeah, another idea. Yeah, another idea is that Scalapack really did something very nice, so they were at the forefront. I'm very proud of our community of linear algebra because we did a lot of things that influenced as well computer science, I believe. And so TVM was a message passing library and it started at the same time of MPI. TVM is a good example of a failed library that was really good. It was very well designed and it died and now everybody And it died, and now everybody used MPI. If you look at the design of MPI, it was perfect. If I tell you all that MPVM was doing, you wonder why we are using MPI now. And so it's interesting to study the lesson, right? PVM, basically, so you do communication among processors. The good thing is that you have a console, so you can control your processor during your run and see what's going on. You can not that easy to do this with MPI. You can Uh you can uh control your uh collectives. So, for example, if you want to do a broadcast, in MPI you say broadcast, right? In PBM you say broadcast, but you can say hey I want to do a pipeline broadcast, or I want to do an airstrip broadcast, or I want to do a binomial tree broadcast. You can say to the library what broadcast you want. You have data types, so if you want to send a triangular matrix, super useful when you write linear algebra, you have those data types. Algebra, you have those data types built in in PVM. So it was a bright library. Anyway, it didn't work. But the idea was to offload the communication to a good library that knows how to do that know how to do it. That's a good idea. Elemental. Elemental, great idea. So Elemental is a software that started at University of Texas, Austin, developed by Jack Paulson. Not sure how much it's still maintained. There is a Julia, a Julia. There is a Julia plugin for that, actually. And so basically, it's a competitor of Scalapak. Same thing, you can't summarize the idea in one sentence. Scalapak is instant broadcast reduce. All the collectives of Scalapak are broadcast reduced. And the idea of Elemental is to write your algorithm not with a broadcast reduce setup but with a scatter guide. And the idea is that instead of having block C-click, you can go all the way to Elemental C-click distribution, which is better. But if you think about it, you can see that. Which is better. But if you think about it, take your standard algorithm, change the high-level ideas, and then just rewrite the library. And the library is pretty good. The library is pretty good. Leave flame, the idea of leaf flame is to use a software that is named frame. So leave flame is a library that does the linear algebra. Flame is a software that enables you to write the library and the idea is to use more formalism to be able to write lots of values. In in a few words at the concept of the library, right? In a few words, at the concept of the rubber, right? LAPAC is basically write LEPAC with recursion. It's a good idea. It's a very, very good idea. Rand LAPAC is to use randomized algorithm in LAPAC. MLAPAC is to use multi-precision LAPAC. And I'm trying to say that they're all one idea. But I mean, so the the success the success of of the m th they are more and more recent, so the success is more question mark, will that be successful and you know. Will that be successful? And then plasma, diplasma, cameo are basically the main idea is to write your algorithm but write them over runtime. And so that's really, I think that's really a good idea and I'm going to speak a little bit more about it. Okay. Just to say ideas that stick. Scanapack is a software from 1990-ish, late 90s, and you compare it with state-of-the-art, parallel distributed eigenvalue solvers. Distributed eigenvalue solver star n, which is really well done, state of the alt algorithm, state of the, everything, right? And yes, of course, it's two times better. It's two times better. So this is matrix size that increase. Scala pack is here, is the reference, this is the shoe reduction algorithm, this is star ng, this is not me posting that, this is the star ng people doing it. Okay, so this is their experiment. So and yeah, twice. And yeah, it's twice faster. But it's incredible that Scalapak is so good after 30 years. It's ridiculous. It's ridiculous. And so an idea like put your matricing into developing your algorithm goes a long way, a long way. Okay, so I'm just going to go a little bit about matrices over runtime, maybe 10 minutes about that, just to try to explain what people are doing these days. So basically, we want to do So basically uh we want to do a choice key factorization of a matrix. The matrix is symmetric, positive definite. Where because the matrix is symmetric we just store half of the matrix. So this is the half of the matrix that perfectly the same things. And basically what you do is that you break your matrices in time and you write your algorithm on the tiles. So tile is a piece of matrix and take it square to make things work. So if you write your score SC algorithm by tile is going to look like that. So that's a tile score SC factorization. Tide shows leave factorization. And so these are operations on tide. If you want the tile to be one by one, if you take your tiles to be one by one, you will see your standard Cholesky algorithm. The Cholesky of a tile is basically the square root of the number. So you have this. And basically, this gives you a sequential order of operation, a sequential order of operation. So you go one by one, all your operations. And you get something that looks like that. So this is a sequential operation. So this is a sequence of operations. The point is that with all those operations, there is flexibility in the order in which you execute them. There is some flexibility. And so you don't have to follow this order. You might want to rearrange. So this gives you opportunity for doing two things. It gives you opportunity to minimize, to reduce communication. So for example, if some data is, if you've done this operation, the data is in cache. That operation, use the data in cache. Operation, use the data in cache. You might not want to do this to flush your cache. You might want to do this operation first, so you have some flexibility on how to reorder the operation. This also gives rise to scheduling parallelism opportunities. For example, all those tasks can be done in parallel. And so this exposed the parallelism to a runtime. Now the runtime is able to do whatever it wants, as long as it follows the dependencies. So any order is okay. Right? So, any order is okay as long as you follow the definitions. So, that gives I repeat that you have two opportunities: that you have opportunities for communications, reorder for improving the communications, and opportunities for parallelism. And so, basically, that would be the sequential order, which is given by this code, but it's kind of arbitrary that the tag directing a single graph, so that you cannot touch. And that is what it is. You have flexibility here, but that. Okay, and that's the example for tracing. And so, the point is that it's great to have a runtime because I can do that. I can do it. That is automatically done. Then, how to schedule the I/O, how to schedule the paradigm? It's complicated, right? The runtime is able to take this DAG, is able to schedule it on a multi-core machine, it's able to schedule it on accelerators, provided On accelerators, provided that I can give those kernels on the accelerator, it's able to schedule it on parallel distributed. I really have nothing to do. So that's right. All I need to do is write this, and I'm done. Yeah, that's it. Now, for Choiceki, I believe I could have done on multicore, on a GPU, and on paradistribute. I think I could have done it. But it can get much more complicated, and that's what I want to explain, right? So fortunately, I might have. So, fortunately, I might have been able to do it. This is, for example, what it gives for, so this is, I don't know the size, this is 4x4, this is a 4x4 time matrix. That would be an execution in theory, so it's simulated, of a 20 by 20 time matrix. And the goal here is to exploit as much parallelism as you want. So, the critical path for a 20 by 20 is 170, and you need for this problem to have 102 processors to get the length of the. Processor to get the length of the critical path. And you see right now, I mean, if you're doing computer science, you're like, oh, well, but is 102 optimized? Can I do better? Can I try to squeeze? Can I have, I mean, those people from computer science love us because this gives them lots of frame to study. And how to get this on the processor is complicated. So another example. So this starts to get less trivial. I want to do a choice key inversion. A choice inversion is basically the completion of three operations. Completion of three operations. I need to do a Cholesky of my matrix. I need to invert the triangular factor and I need to multiply two triangular factors. Does not really matter. The point is that I want to do this operation, that operation, and this operation one by one. Yes. Also, the nodes in the tags, can they represent like individual arithmetic operations or do they represent like the the the like the the the blocks in the right so for either either it depends on the granularity you choose It depends on the granularity you choose, but most of the time there would be blocks of size 100 by 100. This granularity also depends. People who do on GPU, their granularity is like almost a thousand. So those blocks are a thousand by a thousand. It depends. Does that make sense? Yeah. So these are block matrix matrix operations. Okay, so you want to do all that? Good. So I can write this, I can write this, I can write this sequential code one on one. Sequential code 101, and then you give it to the runtime, and the runtime can do this, right? It can compose your graph. And it turns out that the dependencies here rely on that, the dependency here, and so those guys go inside each other, right? So to do Cholesky of a T by T matrix, the number of steps, critical path, 3t minus 2, to do this inversion, it's not 3t plus 3t plus 3t 90, it's 3t plus 2. It's 3t plus 2. You get this with 4 more t independent of t. How did you do that? Nothing. Just give it to your runtime and just figure it out. Of course, you can try to prove all that and all this, but from a practical point of view, it's very, very powerful. So I said, I can write this, I can write this, I can write this. There is one person in the world that was able to write this code. It was Robert von der Geyn in 2010, I think. Robert von der Geyn was able to write this code. So we're still at the level. Write this code. So we're still at the level where you can, a human being, can do it. It starts to get complicated. It starts to get complicated. Does that make sense? If you look at the tracks, that's what it is. So this is the first task, Sholesti, triangular inversion, multiplication of two triangular matrix. You give it, so that is theoretical, that is in practice. You give it to your processor. And this is your processor. This is the time. And the runtime is and all. And the runtime is able to start the task of the step earlier. Okay, where is my speed up of three? Okay, we almost, almost speed up of three, okay? Not perfect, but okay, okay, it works out. It works out. No, it was great in practice, and this is the kind of result if you see, okay, this idea to put linear algebra over runtime is very, very powerful. I mean, as I said, the goal is to offload. I mean, I don't want to deal with the scheduling. I don't even know how to code this. I don't even know how to code this. I mean, it's complicated. Those people are professional, that's their job. And they love it. I mean, so that's great. And so these feedups are here to stay, I believe. So this is LAPAC and MKIL. This is Scala Pack. You're running on a multi-core machine. So Scala Pack, you can give processes to your cores and you get some parallelism and ScalaPack is doing a good job. And then those are the new algorithms. Okay. Okay, I'm going to skip that. You can have lots of fun and write lots of algorithms in a very easy way and get your remediation go wide, and that works great. So I'm going to skip that. That's fine. It's explained in that paper, but this is good. I just want to speak a little bit. It's going to be... I want to speak a little bit about my research and how software helps my research. I'm going to speak a little bit about communication avoiding for five minutes and communication lower balance. So I'm just going to. Balance. So I'm just going to start by explaining Scalapak and this idea of 2D block security. Okay, so give me two minutes. Okay, so here what you have is that you want to do parallel distributed computations. You want to do a range M update of a matrix C. You have a matrix C, N by M, you want to do C equals C plus A. Just think it's a rank M update. M is less than M. So A is L M, this is M. Nm, this is mn and this is n. Okay, you want to do this? I think everybody is with me in sequence. Now we're in parallel distributed. Okay, so we're going to have p nodes. So think about square root of p, square root of p grid. You have a square root of p, square root of pigree, and you want to do this operation in parallel. And of course, you want some parallel. You want some parison. And then you want to try to minimize the amount of communication if possible, right? But you do want parallel. You do want paris. Right, okay. Um and and so the idea is to use a 2D block cyclic uh distribution. And so the idea behind 2D block cyclic, so you're going to take a tile size. So all those squares are 100 by 100 size. So this is your value ID. And you're going to find a pattern. So if you have eight processors, square root of p is going to be two and four. And so you want a two by four grid. Okay, so just you need to break it into. Okay, two by four. You take this pattern, and then you replicate it in the matrix. So this two by four. And then you replicate it in the matrix. So this 2x4 scheme, you see it, I mean you see it here. So that the first two by 4, and by 2x6, you have a 2x2 at the end. Does that make sense? So this is my six. This is how I do mineral. So I repeat what this is, because I'm not sure it's clear. All those colors represent A processors. This represents how I lay out my matrix and the processors. Okay, so processor zero. Okay, so processor zero, which is this dark purple, has this piece of the matrix, that piece of the matrix, that piece of the matrix. So now what we want to do is that we simply want to do our matrix-matrix multiplication. And so, for example, assume you want to compute the element 1, 2, it will need this part of B, this part of A, do the multiplication. So we're going to revert it, we're going to more think about A and B. Where does the data of A and B goes? Of n equals, okay? So I think you understand that this tile of A needs to be on this row and this tile of B needs to be on this curve. Okay, so that's what I need to see. And so the question is, so there are lots of ways to look at the cost of operations, of an operation. Here I'm just going to look at the total volume of communication for the whole algorithm. Yes, I'm just counting the total, and that will be the metric of a good algorithm and a bad algorithm. So that you can have. That you can have. And so, this is the total volume of communication that you need. It's clear. I'm not going to give you the answer. So, the idea is that a tile of A will need to travel on a row, and the length of the row is Q. The length of the row is Q. Please try to be smart and put this tile on one of those colours, so it's Q minus one. So, you need to send the television to Q minus one processor. You need to send Processor, you need to send the tile of B to this colon. This colon is P minus 1. Make sense? So this is Mn times Q minus 1, Mn times P minus 1. So the bound is... Make sense? Okay, so now I'll go a little... So what is? So that's the volume of communication of a matrix-matrix multiplication? Ah, the good question is, is it good or is it bad? I mean, okay, that's what it is. What is it good or bad? So now you need to go back to. So now you need to go back to the research on IO lower bound. IO lower bound. So what is the minimum number of communication or what is the lower bound for the amount of communication? And so in general, so here we are in parallel distributed. What we do in general, so I'm not going to explain too much, but we look at the bound for the sequential model where you have a two-level memory. So you have a big memory, so no parallel, sequential model. Infinite memory is where your matrix is. Is where your matrix is, cache, and you can just do the operation in cache. And the question is: how much data do you move from main memory to cache? So, if the matrix fit in cache, it's easy. So, the assumption is the matrix does not fit in cache. And if the matrix does not fit in cache, you move some data, you do some operation, and then you need to flush the data, you move some more data, and it's complicated. So, based on this bound in the sequential model that is well understood, you can derive a bound on the parallel. You can derive a bound on the parallel model. So I'm going to go a little bit faster, but basically this is related to the arithmetic intensity. And you can prove that the arithmetic intensity of the algorithm, so here I'm speaking about an algorithm, is square root of s. If you this we do that so that we relate this to the sequential bound. So this is the arithmetic intensity. So this is the arithmetic intensity as a function of S, where S is basically the memory that you use in a node. Okay, and square divided by it. And so that the arithmetic intensity of that algorithm, is it good? Yes, it's optimal. Yes, you can prove it's optimal. Okay, so here you go. You have to prove that Scala Pack is optimal to debloxyclic. That's for matrix notification. Why are you going to write LU? Why do you expect that LU is good or not good? I don't know. But that's the idea for 2D block. But that's the idea for 2D bugs to click motivation. All this is pretty recent work. All this is pretty recent work, anything else. Okay, so we're done. We're done. We're done with matrix-matrix multiplication. We want to do something else. Cholesky, of course. What else than Cholesky? Okay, so let's do Cholesky. And so for Cholesky, what you want is not CAB, it's C A transpose, right? It's called a symmetric concave date. And so instead of AB, AA transpose. Okay, so instead of AB, A A transpose. Standard. So A A transpose as opposed to A B and C, the input C but there of it can be none. It's symmetric. So in general, you just need to compute half of it, right? So that's a symmetric. I know it's a triangle, but that's a symmetric matrix. Okay, so what do we get? We use our 2D block C click because we assume it's the best. It's the best, da da da da da da da da da. So, if you do the same reasoning, okay, so first let me explain to the pop cyclic again. Ah, okay, not well, don't talk too much. Um, so you take your pattern and you replicate it in your matrix. Everybody's nodding here, and then you run your algorithm, and the question is, okay, okay, okay. Now a tile, where does a tile go? A tile of A is also a tile of A transpose. Okay, so that's good. So when I send a tile of A, at the same time I'm sending the tile of A transpose, so I'm doing Time I'm sending the tile of a transpose, so I'm doing two, so that's right, that's the win. And so a tile of n is to be in a row, and a tile of a transpose needs to be in a column. So the title of n is to be in this scene, right? You know, that bounce is pretty standard picture for Cholesky, it bounces on the diagram. And then that's where you that's where you see how maybe I can do it. I I got correctly and actually that's where you lose optimality. So you lose optimality with a constant. You lose optimality with a constant, your ass is square root of square root of two. A recent paper in Spei 2022 proves that the arithmetic intensity of Cholesky is square root of two square roots. That makes sense? So this is what you get, that your arithmetic intensity Get that your arithmetic intensity if you scalar back CH, okay, correct? That's a bound. We know actually that the bound, not only is the bound, I'm saying it's a bound because it's sharp. It is sharp. And this is a beautiful paper that you should follow. If you like Schoesky and IO lower bound, this is the arithmetic. This was the best known bounds before, best known bound was here, best known algorithm. Here, best known algorithm was here, for Cholesky or for SIAC, in sequential. And this paper proves that you can improve the best-known algorithm and you can decrease the lower bound and then match it. So it's a nice incubator. But for all, so this is sequential, this is parallel distributed, this is scala pack, what you get in scala pack, this is your bound, or you have a factor of two half, and it's not the end of the day. At this point, we're just trying to I mean it's okay, so we're going to continue. And then you can try to be smart. And then you can try to be smart. And that's where I go to offload. It's part of my offload thing. So, here is another way to put a symmetric matrix. And I should have done a bigger matrix, but if you have L processor, you can take this pattern. Okay, so instead of taking this pattern for L processor, you take this pattern and you patch this pattern on your matrix. Right? If you do that, you get square root of it. Okay, so remember two s uh this was the arithmetic intensity. This was the arithmetic intensity. Higher is better. We get a factor of square root of two higher. Good. Okay, good. Right. Awesome. And the lower bound, square root of two, square root of s, well, we are closer. We are not higher, which is good. It would not have been good to be higher. That would be a big problem for the theory. Okay, that's it. But that is. That makes sense? Okay, but what's the point of this? Why am I saying that? The point is: well, okay, you're funny. This is great in theory. You say, Great in theory. You say, I got a matrix, I put this pattern all over this, and then I got this bound. Awesome. And the question is: yeah, but someone needs to code the code that does this. I mean, how do you make this work? And so the point is that you just need to distribute your data, run your standard 2SQ algorithm, use the runtime, and the runtime will just make it happen. Right, but runtime does it for you. It can work on GPUs, it can power distributed, and it can work with those crazy distributions. It can work with those crazy distributions. Okay, and the end of the story is, so that was the paper at Supercomputing 2022. And the end of the story is that my French colleague in 2023 at IPDPS, so I think the paper is accepted, so congratulations to them. They got this crazy distribution. It's just insane if I explain you what it is. If you look at the tie, it's like it just makes no sense. Just make no sense. The stuff makes no sense. It just makes. If you look at the green tile, it makes no sense. And the point is that they can get their square root of space. That makes sense? And like to write an algorithm that would understand, just putting the matrix on the processor, it just cry. So you're not going to write a paradist with an PIA or parallel on this. It just makes no sense. But good. And so by the way, that's a bound now. You prove that this is the bound for parallel distributed. That this is the bound for pi-distributed Cholesky for the total volume of communication. Does this maintain the optimum for multiplying A times B? Ah! Like ideally, I guess you want a pattern that works with a different pattern. Ah. Well, that's an excellent question. That's a very good point. So, first, theoretical. We're trying to be theoretical here. Second, though, we have looked at how to do a TLSM to do operations that we would care. So, we would not necessarily care to do a matrix matrix multiplication with that distribution, to do a triangular solve. Same thing, you just write your triangular solve three lines to write a triangular solve, you shove it in your own time and you see what it gets, right? So, we did this, it works okay. It works okay. I think the main interest is mostly theoretical. I think the main interest is mostly theoretical. So it's a good point. Those 3D data distribution are good to prove things. No, but the point is that when you're here and here, it's good to know what is the theoretical bound, right? So at least we know here people are saying, well, you don't know how to write a lower bound. You don't know how to write a lower bound. Please move the no, it's not possible. So yeah, but that's a good question. Okay. And so this is performance, then this will. So this is performance, then this works great. That would be algorithm in Scala Pack. So we we did not use Scala Pack, we use the runtime over 2D block cyclic. But basically you put your matrix in 2D block cyclic, use the runtime with it. It would be nice to see ScalaPack, but you would expect Scala Pack to be there, to be those lines. And then you can improve. You can improve. The first thing that you want to do is that's that's the main problem. Um just want to give you some stories in EIPAC. I just want to give you some stories in EPAC, just for food for thought and things like that. Daniel. Daniel Kristen asked me, what is the probability of failure rate of an effect? It's related to all the randomized algorithm that we're doing, right? Randomized algorithm has a probability of failure. How do you tuck your probability of failure? And when Daniel asked me, And when Daniel asked me what is an acceptable probability of failure, Julien, you know, you know, what do people think? And I'm like, I don't even know what is the failure rate for an effect, right? I mean, this is something that would be very interesting to know. We have very, very few bug reports. And it's important to understand we have very, very few bug reports. Not because we don't have bugs. I mean, it's probably helpful to not have a lot. But the problem is that something that we observe a lot when we speak to users is that there is a bug and then There is a bug, and they just shrug, and they do something else, and they perch up the matrix, and they do their own source in the middle, and then there is no bug. And so we will not hear about those bugs. I mean, people say, yeah, this code, so you speak to people, and they tell you, yeah, this routine is failing. And you're like, yeah, maybe tell us. Tell us, right? So people so it's hard to know the failure rate of a bug because people don't report bugs very naturally. And another thing that is hard is And another thing that is hard is LEPAC is pretty robust. And when you have a bug, it's in general on one machine, one computer, with an ambient temperature of 20 degrees Celsius. What I'm trying to say is that it's very hard to locate. And so people say there is a bug. You trust the person. They're an engineer, Intel engineer at MathWorks, they know what they're doing, they have a bug, you trust them, they give you the matrix, you cannot reproduce it. Wait a week. Wait a week, the bug is gone on their hand. They have taken their compiler, they change something, the bug is gone. And so that's the kind of bug we have that are very elusive. They are very easy. It's very hard to... So to know the failure of a LAPAC is hard. I did not answer the question of Daniel, so maybe that could be a subject of conversation. I think I will be able to talk on Brandon Later. I guess we discussed this at night. So we'll just wait a week and see if it works. We'll just wait a week and see if it works. And the problem disappears. You're my chairman, right? Yeah. Will you stop me when you're done? These are some bugs that were recent in the EPAC, and I'm not sure why I'm focusing on bugs, but I think that's what interests people, and people want to speak about it. So there was this condition in APAC a lot that was if x equals x plus 1, then stop. If x equals x plus 1, then stop. This is predistant. This is pretty standard if you look in code. And for example, this is in the SQR2. So this is the QR algorithm. So we had a talk on the QR algorithm yesterday about the convergence of the QR algorithm. So that's the algorithm. That's the algorithm, okay, from IcePack, so 1970s. And you do see test 2 equal, if is test 2 equals test 1, and test 2 is equal to test 1 plus a positive quantity. That makes sense? So it's exactly that. And in And in the cure with P-voting of Alepak, 2008, I think, so the paper was to the same. We exactly have the same line of code. And so first, why would you just do that? I think everybody does numerical analysis, you understand? The goal is to say that y is small with respect to x. That's really the goal. Why would you do that and not y is small with respect to x? Because here it's independent. Because here it's independent of the machine precision. So you don't need to know the machine precision to have this code do what you want. And when you were in IcePack, those machines in 1970 were just wild. So knowing your machine precision was good. However, now there are some problems. I mean, there are some problems. So that was one line of code that is pointing in this paper of 2006. I love this. Oh, I can nobody can read, but it's yeah, that's it. Yeah, that's it. So you have two curves. I'm describing what I see and you just imagine, okay? There is a blue curve. So this is the entry of the diagonal of the matrix. It's the entry of the diagonal with QR with p-voting. So they need to be non-increasing. They need to go down. And there is a blue line that goes down, which is what you want to see. And there is a red line with a big spike, which goes down, that's fine. But then it goes up right after, that's not fine. You cannot do that. So there is a bug. There is a bug. There is a bug. There is a bug. It's a cure factoration with pivoting, but that does not pivot as it's supposed to pivot. And here it's written: this pack should not be there. It's not so clear. It's in the paper. But that's it. And so this bug was in part caused by something like that. And register were 64-bit. Computation was 64-bit, and register was 80-bit, and there was extra precision there. So it was pushing this too much, and the criteria was not triggered. And the criteria was not triggered. This criteria was not triggered, whereas it should have been triggered. I just bounced the image to the random channel on the slack. That's why I want the closer look. Oh, thank you. So this is on Slack now, if you want to ask. Who had the equal test? LA pack or the user? Ah, no, no, this is LA Pack. No, we are speaking about bugs in LA Pack. So this was... I'm not surprised that you have tests like that in LA pack, and you just need all event disks and bugs to be the same, and it's offloaded to the screen. So we are slowly removing them. So we that was the oh oh yeah yeah yeah sorry yeah okay this was the line in 2008 in this is not the line anymore. Now we really say y is less than machine precision because now we know the machine precision time norm of x and then that makes sense. So this is not here. I do want to point out that those people were smart and knew what they were doing and it's a really And they knew what they were doing, and it's a really, really smart idea to be independent of the machine precision. So it makes total sense to do that. I mean, it's a good idea. But this caused some problem down there. We have, we still, so one thing that is our nightmare and we are working on it is removing infinite loops that I think you all agree you don't want anything in the future. Yes, on the previous bug it was an Eisenberg bug but I think a Heisenberg? So if you turn on the bugging it would go away because the uh the the the variables would be uh read out to the The variables would be read out to the cache from the register to the cache and store the door. So if you turn on the button, it goes away. I don't understand the world. If you turn on the debug, so this is a Hei Heisenberg bug. It's very hard to observe. Indeed, if you put debugging on, if you put the debug mode on, it's gone. Debug mode on, it's gone. If you write a write statement, if you write a right statement, it's gone. Yeah, yeah, no. The paper, you need to read this paper. It's like they're like begging their head on the wall. They're like, oh my gosh, they're trying to... They explain all their debugging. The whole paper is about how they debug the code. It's a great paper. Yes, it's a Heisenberg. And we have a lot. So there is infinite loops, and we are trying to remove those. And most of them are caused not because some users. They are caused not because some users come with inputs that are in for none. Jim Demel has a whole talk about that. And so this also pushes to study more what is the behavior of a LEPAC when there is a NAND, what is the behavior of LEPAC when there is an IMS. There are still some questions like that. My main concern are infinite loops. I don't want infinite loops. No. It's still good to define some. But there are lots of things. I want to say that there is some randomized algorithm in LEPAC. Here is a randomized algorithm. There is a paper from Beresto-Pallet and Chris Bommel in 2005. This was about the MRQ algorithm. And basically, they do put little random perturbation on purpose in the middle of the code just in case. Just in case. And so it's just to explain. Just to explain. I'm going to go fast. I just want to go five minutes here, but I'll just go. So we got a great project. It's my project. So this was more everything. And this is more me. So it's about TLA bugs, something I'm trying to do. And now that I explain what is software, you understand that I might break my teeth and it might be a complete failure and I'm at peace with it. It's okay. What is important is to try, to have an idea, to be motivated, to have a vision, and then you see what happens. Of a vision, and then you see what happened. And right now, I'm also doing the marketing about it. And I'm still working on the logo. So, this was for Valentine's Day. We put a big sign in the middle of Denver. And so, TLA Pack loves your matrices. So, send us your matrices. I'm going to steal this motto from Jim Devil. TLA PAC will give you almost the right answer to almost the right problem. I love it. That's going to be our mission statement from now on. And this is. And this is a wonderful software engineer that works with me, and he's wonderful. And he's doing most of it. And I'm just going to give you the idea of TLAPAC. In one minute, the idea is to write header files that you can insert in your code. And those header files is like a cooking receipt of what is an eigenvalue solver, of what is the cholesky decomposition that work at a very, very high-level abstracting. And so your matrix can be anything, any data type. So your matrix can be anything, any data type, any layout, any kind of matrices. So here we work with the Nigel matrices from Eigen, the software package, C Aigen. Here we work with the Cocoa MD Span matrices. These are user. User work with Eigen, user work with MD Span. And then they can use our choice keys. This is our choice key and this is just. And the way it works is that we simply No, I just removed it. Okay, fine. But we simply define what is Aij. We define what is Aij for an eigenmatrix, and then our algorithm works on Aij. So of course for performance, you need to do sub-matrices and things like that, but that's the idea. So we have very, very few operations to do. We work with many, many matrices. Type of matrices, right? So you can. So I like, I don't know what the word parasite or virus, but it's like a virus. You can. Virus. You can put that in your code and it's going to work with your code to compute the eigenvalue of whatever matrices you have. Whatever matrices, it gives the method to you. When we use eigen, we're using the matrix matrix multiplication of eigen. So it's eigen matrices that use eigen matrix matrix multiplication. Everything is eigen, but the method. And so we can compare. So we had someone, so I'm just going to show you the eigenvalue solver. This is performance for the eigenvalue solver. So this is a known. Solver. So, this is a non-symmetric eigenvalue solver. We use the QR method. I can tell you what the shift is if you're interested at the coffee rack. And so there is, okay, the higher is the better. Higher is the better. But basically, the point is that there is two ways to go fast. Either you use eigen, so you're in an eigenwar, you're doing eigen. Eigen can call directly MK. If the matlice is in double, eigen will call. In double, eigen will call MKIL, that's what they got. I can call TA back. We're here to help. And you will get the same performance. On an eigenmatrix, using eigendata type, using eigenmatrix matrix multiplication, everything is eigennative. That makes sense? If you use eigennative eigen eigenvalue solver, you're here. The their eigenvalue solver is not good. Okay? So not not what it is. That makes sense. So yeah, that's what we do. But this is a proof of concept with eigen. We can do it with Of concept with Aigan, we can do it with anything. We can do it in Trinos with Cocos. We've been working with Crocos and that the idea. Yeah, we'll see. We'll see if that works. Just want to finish. No, but we don't need that. We don't need that. We don't need that. Oh, this is. This is. Yeah, I really was trying to sell you the project. I'm going to finish like that. Yeah, the idea of this slide is: I still don't know what makes a software successful or not. All you have to do is try. Or not. All you have to do is try. And sometimes, I mean, it's something work and should not work, something don't work and should work. I was telling you the story of PVM. PVM was beautiful, it failed. Some of their software have lots of flows and they are very successful. But it is. That's it. Okay. Uh thank you, Julian, for the for the Julia, for the talk, what are I think this is going through a lot of things? I didn't expect to see Carol's scheduling. I was just telling my teams that I flew through time to get away from this stuff and get here. I am seeing this. Sorry. So we do have some time for questions, but before that, I do want to say that I uh there is some uh there is a platform excursion system. Well, I mean, there are free big chunks of free time. Are big chunks of free time this afternoon. I have made some preliminary posts, but I imagine some of you have been talking about it. I can take questions about that, like probably just before the start of the next session. I'd also say a little bit more about it, but if you have any questions about it, you can also ask me about this, like by Natural Gas Julia questions or Julia putting. But before that, let's take questions to Julia at this point. Oh, okay. I'm here. So do do we have a sense why uh fast matrix multiplication is almost never used in class are they so the question of Daniel is why is in libraries software numerical algebra trusted, not more used, fast matrix multiplication or for complex the 3M method of all those methods. And I don't know. I don't know. And I don't know. I don't know. That's a good question. I know that ESSL, IBM ESSL has provided. IBM ESSL provides it. LAPAC does not, typically. And I don't know. I think it's almost religion. I am prostration. I am very open to it. I understand the issue of component-wise stability. But most of the algorithm we are way past component-wise stability anyways. So it does not bother me to have a So it does not bother me to have a matrix matrix multiplication that is just normal and stable. Uh so I don't know, I think it's almost religion at this point. Like I don't know why. So so I have an an explanation. It's predicted by the Linpack benchmark. Oh okay. So what Jim is saying is that lean pack benchmark Jack Dungara with a stop 500 has decided that it was not okay to do that. Okay. That's a good explanation actually. So that's what you said. Good answer, bad answer. Yeah, I was wondering whether it's really a stability issue or something else. No, I don't know. Maybe we should just do it. See if people complain. Next slide, right. Okay, thanks. Strassen Elevac. Okay, sounds good. So maybe following up on that question, are the blocks large enough in LEPAC or SpaceX? In LAPAC or Scale APAC to properly make use of class. Okay. So the question of enmend is in an APAC, when you're in the middle of an algorithm like a NU, a Q algorithm to find the eigenvalue, you don't have a big square time square. In general, it's more wrong calc date and things like that type of matrix matrix multiplication where the structure would not be so much use. Yeah, maybe that's one explanation as well why it's not so much use. Maybe we have online. I have a very naive question, which is you talk about this idea of like offloading to a runtime, and you write your code in such a way that it has a nice DAG structure. Because I would never deal with this stuff. Are you having to give hints about what that DAG is? Like you have to provide that information to the runtime where it's automatically figuring that out. Right, so most of the time we'll figure out your DAG. So my research on DAC. So, my research on that has been twofold, has been to provide lots of things, but one thing where we play is try to rewrite your algorithm to change the DAG. So, that is nice because you're doing mathematics, you're not doing computer science. So, computer science, literally, they have a DAG, the DAG is fixed, they try to scale. That's what they uh mathematics, what we do is that we try to change the DAG. And to change the DAG, you need to use mathematical properties of your algorithm. Properties of your algorithm. And so, but no matter what, your code is really a sequential code. You really give them a sequential code, and they're really able to extract the DAG from the sequential code. So when a compiler just extracts the DAG? Most of the time it's at runtime. Because deep, oh yeah, you can have ETH, you can have ETH and things like that. Most of the time they do it at runtime. So the the main idea is basically a compiler, they do kind of the same thing as compilers. The same thing as compilers, but because the granularity of the tile is 100 by 100, they can do it at runtime, and moreover, they can think really a long time, hey, is it really worth their time? Because it's going to take milliseconds. This operation will take milliseconds. So they can definitely spend microseconds to look at heuristics, to look where is the data, to make at runtime a decision on what is the next operation I do for Parism, what is the next operation I do. Operator, what is the next operation I do to minimize communication? Where do I do my operation? Do I do it on the GPU or do I do it on the CPU? And the key here is to make those decisions at runtime. And the reason you can make those decisions at runtime as opposed to a compiler is because of the granularity. They are big enough, you have plenty of time to make your decision. And so there is lots of heuristic inside this stuff. Thanks. There aren't questions online, but uh there are more questions uh here. Does TLA pack allow you to interoperate with different packages that have different matrix formats? Right. So does TAPAC enables you to operate with different matrix formats? So first, TAPAC is very early stage software, so I'm speaking a little bit too much. I should not speak that much. That's a working point. It's not a work in progress. But I would say yes, because for me it's amazing that we're able to work with Aigen, with Cocos, with data format from LAPAC. People are trying to think about how to do Zmorton ordering, right? You can have your matrix layout in Zmorton ordering, and ZLAPAC should work just fine on it. So then it takes the cost of doing the matrix conversion? In between. In between, we're not yet there. We were not yet there. The main application right now is you're a user, you're already set up in your own world with your own metric data format. You don't have an eigenvalue solver. Well, actually, back will compute the eigenvalue for you. That's it. Having different formats work together, that's something else. But maybe better. Actually, if you do AI. Actually, if you do Aij equal Bij in TLA pack with two different formats, it will, it's not going to do it efficiently, but it will do the data conversion. Oh, yes, we do it. Well, uh the interest of uh opi and uh discussion about that, I think it was the question's offline earlier. I think the rest of the questions are nice really. I'll just thank him again. Thank you.