I mean, you notice with it. Just different. It's just different enough to escape any copyright photos. We'll call it Blender. It's Blender. Alright, so welcome back, everybody, for the first afternoon session. Sorry about the delay. There were some. We're still missing a few participants because for the flying back, they have to do some COVID testing and took a little bit longer than expected. Nevertheless, Eric Gruber will be our first presenter this afternoon. He's going to talk about multi-scale networks involved in control of actions and emotions. Erin, please go ahead. Hi, thank you. And thanks to the organizers for inviting me. This has been a great meeting so far. Look forward to the I've had great interactions and I look forward to interacting with you. Interactions and I look forward to interacting with even more people. So, what really has drawn me to study the brain is that I'm really interested in all the amazing things that the brain can do from doing amazing things with our body, skiing, biking, amazing works of art, technological progress, and understanding the universe. So, great creativity and thinking. And we have the And we have this wonderful organ that does this, but at the same time, we have a lot of behaviors that are just not logical. So people doing strange things, drug addiction, depression, ideological fights over things, and just having, you know, advocating for concepts that really are counter to all available evidence. So, how can this be? How can we have such elegant and Such elegant and wonderful outputs on one hand, but then sort of this very poor behavior on the other. And what I think is that this really comes down to the consequences of representations that incorporate knowledge, which are mental models of the world. And so, how this knowledge is represented both leads to the strengths, but then also leads to these sort of anomalies. To these sort of anomalous behaviors that are not so adaptive. And we can look at the brain at many different scales, all the way from genetic to very complex molecular networks inside of cells to single cells, local networks, kind of networks of networks, and then the brain integrated with the rest of the physiology, like gut, liver, an animal in a natural environment, and then groups of animals. Environment and then groups of animals. And so we have network effects at all these different levels and they feed down onto one another. So what's happening at a social level can feed back to these networks, local networks, and affect the biochemistry of cells, which then propagates back up to how you even interact with other people. And I've worked at many of these levels, but for the sake of time today, I'm going to focus right here at this local network level. And to try to understand what a brain is about, like any complex system, we could take two general approaches. One is a top-down approach, sort of thinking about what's the function and coming at it from first principles. And that's difficult. We can also go for a bottom-up description, and that's also difficult because of the complexity, as I alluded to. But I just want to outline just a little bit of a top-down kind of strategy because I think that might help in the interpretation of some of the things. In the interpretation of some of the things I'll talk about today. And so, if you think, you know, what's a brain for? Well, it's to collect enough resources and avoid death long enough to reproduce. And this is sort of the selfish gene sort of view of the world that we're really subservient to our genes and our genes just want copies of themselves as far into features we can get. And so, what does the brain need to do? Well, the world's complicated. There's a lot of different things in here, so the bandwidth of the visual. In here, so the bandwidth of the visual system is several gigabits, auditory, perpereceptive, all this information with this ocean of information coming in, but only some of the things in that large and high bandwidth stream of information are relevant to the animal. And so the animal needs to do feature selection. So out of all this, right, there's only some predators working and maybe there's some food over here. And so the animal has to learn about these things. There's also an intractably large state. There's also an intractably large state space. So the animal is kind of moving around doing stuff, and when he comes across something either good or bad, how does he know what were the things in the past that led to that so that he could either avoid it if it led to something bad or reproduce it to get the good thing like food? And the solution to this in computational space is typically referred to as the temporal predator signal problem. So when something happens to you now, how do you go back in time and adjust learning on the thing? Learning on the things that happened in the past to try to control and either obtain or avoid that thing in the future. We have other problems like partial observability. We can only see a very small part of the world at any time. Stochasticity. There's constant novelty because these things are changing. And there's asymmetry in rewards and threats. So you have the ability to do trial and error learning on different foods, I like one kind of doughnut more than another, but not so much with predators. Your first encounter is very well relaxed. Encounter is very well relaxed. And so, you know, what would the solution be? So, being good scientists, we would probably say, well, let's make a model of things. And I think that's what the brain primarily is doing. So, it's making a model of the environment and then using inference on that model to dictate behavior in the face of all these challenges. And this is very much related to what Najeeb was talking about: that this is the product of memory consolidation, is what we refer to as. Is what we refer to as schematic knowledge. We can think of this as a model of the rules of the world and state transitions and what happens under different actions and those sorts of things. And if you wanted to make the best model, you would use all the information you could get. Some of that might be genetic, so this would be like a nate fear of the predator urine. Latent experience, meaning you're moving around but you're not particularly trying to learn a specific thing, trial and error where you're trying. Specific thing, trial and error where you're trying to learn a particular thing, even observation and potentially even more abstract knowledge streams, such as narrative. So watching the news or social media is one point with that. And so to try to give you just a quick demonstration of such a model, I'm going to call on Jorn here. So let's pretend that I asked Jorn to make a choice between door number one and door number two. Between door number one and door number two, and he has to keep whatever's behind it. Nope, I've already decided. Oh, yeah, you get two. You get ten dollars, I'll pay you. Are you happy? Well, if there's a hundred dollar bill, I'll take that. I'll switch. No, you don't get switched. So the question is, you have $10 you didn't have, but you want to see what's on this one. I'm still dealing with $10. Okay, I'm still dealing with. Okay, so economists say you should be happy because you're $10 richer, but most normal humans say, well, Normal humans say, Well, you know, it feels like a $90 loss. And so, if I was to ask Jordan again, you know, to pick, he might go, well, you know, I'll take number one, or maybe he switched them, and he's going to go through this process of trying to figure out. So this is what I mean by a model. You know, a model-free way of doing this would be: you know, Jorn does 50 repetitions of this, learns the utility of each one, so the probability times the work value, has some promotion. Value has some probability distribution across these and then chooses equivalent. But like I said, you don't always have the luxury of that kind of number of iterations, particularly when something is a threat. So if something is toxic, you don't want to have to eat the toxic mushroom 100 times and learn that it makes you sick. You'd be wasting a lot of time and risking your life. And what we know is that this kind of disappointment in things that didn't happen or evaluating what did happen in the context of things that could happen varies emotionally. Things that could happen varies amongst people. And there are many psychiatric conditions like anxiety and depression in which this is actually enhanced. And so, you know, some people will lament over that fictive $90 loss for a long time, whereas other people will get over it very quickly. And what I'm going to talk to you about today is neural activity in this brain region right here in this cartoon called the anterior cingular cortex. This is the cortical part of what's referred to as the limbic system, which comprises the hippocampus and the Which comprises the hippocampus, amygdala, hypothalamus, and some other structures that you've heard a bit about today. And these areas talk to one another, but I'm just going to focus on the singlet for the sake of time. And this is a conserved structure, and so we see it in humans, rats, essentially every vertebrate fish, and so on. And one of the tools that we use in the lab is to use high-density electrophysiology. So we implant a device like this, and this has a bunch of electrodes in it. Has a bunch of electrodes in it. You can drive down each one individually and try to tune in a bunch of neurons. And we have a rat doing a task on this maze. And we call this a tea maze, even though we're a figure eight maze. So he starts here, he gets a little drink of chocolate milk to get him started, runs up, turns right or left, either has to jump up or not jump up and gets a reward, and then he cycles back here at the beginning. But we have these little gates. So sometimes we force him to go one way or the other, sometimes he just freaks us. Is for changes. And if you take extreme care to do this, you can sometimes get up to 60 neurons at a time at a time scale of microseconds. And so we can see all the Hatch potentials from simultaneous neurons over space. So you'll notice this kind of color gradient. So if we take this and kind of stretch it out, we get this positional kind of color gradient. And this is what And this is what an example of one trial of what the neural activity would look like on this ensemble of neurons. And when we look at these neurons individually, what we see is that many of them are modulated by position on the track. So what we're looking at is here's the top-down view, and we're just color-coding where a particular neuron is active. So this would be one neuron, two, three, four, so on and so forth. And what you can see is that many neurons are active in selective portions. This one just likes the supper feeder. This one kind of likes The supper feeder. This one kind of likes the little maze. Sometimes they're symmetric, right and left. Sometimes they're more asymmetric. And so we see this positional signal, but this is also multiplexed with other things like reward, actions, effort. And so one of the ideas I'm developing is that. That's not two minutes, isn't it? I don't know, I wanted to ask a question. Yeah, go ahead. Is it for both train man and train man? I'm sorry. Is it for both trainbann? I'm sorry. Is it for both trained and untrained animals? These are largely trained. Yeah, these are pretty well-trained animals. Ask me later and I'll tell you what happens. We sometimes give them a novel experience and interesting things happen. And so the spatial encoding is very much like what the hippocampal place fields look like, except this is a much larger spatial scale. This maze is about one meter by one meter. And so these are very broad. And so the spatial representation might be a carrier signal or some framework for. Carrier signal or some framework for associations on top. And another thing that we wanted to know is that, well, these cells have broad tuning, but it might be able to accurately encode position. So if you have enough units with very broad tuning, you can still encode fairly accurately where the animal is. So we wanted to know that. And so what we did is we took neural activity, whoops, we took the neural activity and fed it. And fed it into a deep neural network. We tried a variety of architectures, and most of them worked just fine. And then recorded what the prediction error was. And so this is all cross-validated. So if we're testing on data that was never training data, and you repeat that a bunch of times. What we found is that the error is less than if we use a Bayesian decoder. And we think this is because the beep neural network can disregard information that is not useful for position. So, like those symmetric. Position. So, like those symmetric activity. But where were the errors occurring? So, the errors are fairly uniform over most of the bin. Again, here's this linearized track in color. But most of the errors are occurring at the feeders. These are these feeders up at the top, right, and left. And so, that seemed kind of weird. And so, what I want to do is just show you a video of what this looks like. And so, here's the animal top-down. This is this recording thing, it's got LEDs. This is this reporting thing, it's got LEDs, so we can track him. And this triangle is our decoded position of where he is. And so, what you can see is that it's tracked him fairly well. It was one to the left, here's one to the right. It does a pretty good job. The reflection messes it up a little bit. But in general, it does a pretty good job. But here's what happens on some trials. So, notice he runs over and watched the triangle. So, it leaves where he is. So, there, you know, he's going to do a So there, you know, he's going to do another lap here, he's going to jump up, and the triangle deviates from where he is. And so, what's happening is that the ACC activity normally tracks the position of where he is, but every once in a while it dissociates. And the dissociation wasn't random, but it had an interesting characteristic. So the blue star here is where the animal is, and this is all these sort of excursions from where he actually is. From where he actually is. What you see is that all the excursions, when he's over here on the left, go to this right choice feeder. And likewise, when he's on the right, it's over to the left. These are two different animals. And notice that he was still getting this reward in the middle, but it never went to the middle. And when he's at the middle, it never went to these choice targets for some reason. Even though he's getting the same quality of reward as Chopper. Moreover, the likelihood of these excursions happening was a function of whether he. Happening was a function of whether he preferred to go to that side or not. So, here we're showing the animal's preference when we let him have free choice. So, when he prefers to go to a side, it's unlikely that we see one of these excursions happen at that side. But when he does not prefer to go to that right side, then we see that most of the excursions happen when he's at the right side. So, what we think is that this might be kind of a rudimentary dynamical phenomenon that is underlying something like this counterfactual process. Underlying something like this counterfactual processing, meaning that how does the animal know whether what he got was good or not? He has to compare it to the other thing. And so, you know, if he's going over here and he gets some amount of chopped up milk, I forgot to mention. So we vary the magnitude. So he either gets a little bit or hot. And we either make him jump or not jump to go get that. And so we think what's happening is that he's comparing, well, what did I get with what I could have gotten at the heavier sun? And this kind of thing has been seen in monkeys. This kind of thing has been seen in monkeys and it's been seen in fMRI in humans in the same anterior cingulate. So there's some precedence for this. But none of us really thought that this was going to manifest itself in a rodent. Seems kind of more like a higher order kind of a thing. So why this broad spatial representation? So one thing I'm not going to talk too much about is that it could be a solution to a temporal credit assumption problem, meaning that the animal might be A problem, meaning that the animal might be deciding what he's doing when he's down here, but he needs to link that with what happened at a different time. But instead of doing it in the temporal domain, he does it in the spatial domain. And so as we heard this morning, this idea of using spatial position as an encoding scheme might help to solve that problem. And these neurons might help to bridge. And so the reason they have spatial, sort of broad spatial tuning, is to span more in time. So it's really an encoding of time. Really, an encoding of time at a space. But what I want to do is to talk about something else, which is that having different resolutions of a problem can be very helpful when you have to change your behavior. So what we know is that in the hippocampus, so the dorsal, it extends from the dorsal part to the ventral part, so top to bottom, and the size of the place fields change. So in the dorsal part, it has rather So, in the dorsal part, it has rather small spatial fields, so a centimeter, a couple centimeters. But as you go more eventually, they get bigger and bigger, and it can be meters. So, what we did is we created a model, reinforcement learning model, in which we tessellated the space either very finely or very coarsely. And so we had five different levels. And what we did is we stacked these guys up. And what's going to happen here is that we're going to do reinforcement learning at every level. And if the animal And if the animal, well, in this case, the agent, if it encounters an unexpected transition, it looks at the, it asks the next level up what to do. And so we think this is a good proxy of the brain, and I'll explain that in a minute. And so then what we did is we just had a very simple task in which we trained this agent to go from the starting position to find the reward over here. And in the beginning, everything is randomized, and so it explores the whole space. Let me just fast forward a little bit. After a while, it explores the space. A while after it explores the space, it looks for slowdown. Once it learns this value gradient, then it can just go from the beginning to the end, and there's not much learning. So, now, but what happens if we all of a sudden put a barrier in here? Well, we put this nefarious barrier in here, and it's nefarious for reinforcement learning because now the agent would have to go up its value gradient in order to find the solution. And so now it's kind of unlearning this value gradient, and it would take a very long time. But if instead we have this, so this would be if we only had that finest. Only have that finest level of resolution. But if instead we use that scheme I was just telling you about that when it runs into a boundary and asks the next level up what to do, without too much time, it can learn to circumnavigate this barrier, and within a few trials, it's gone. And so it's a way of dealing with unexpected changes in the environment. And so what we found is that it's able, so this hierarchical scheme can much more quickly Can much more quickly relearn a task when something unexpected happens. An interesting side effect is that it takes much less computation. So my postnat figured this out because the simulation started running a lot faster. And his computer wasn't as high. It was like, was that real? And so we actually checked. The other cool thing about this is that you can then be more sloppy in your updates. You don't have to update it completely. It's actually kind of nice. So what we think is that this anterior cingulate, by having this broad space, what it actually has. Space, what it actually has is a rather abstract representation of the environment built largely in space, multiplexed with other things, and that it sits there and it serves as a director of what to do when unexpected things happen. And that fits with a lot of data I go back to tell you about and this temporal credit assignment thing. But then one of the questions is, is it real space or not? And it's actually not. So I think it's some latent space, like what we heard right before. Like what we heard right before lunch. And the reason is that sometimes they have to jump up this platform, sometimes they don't. And what we're doing here is plotting a dimensionality. We're just using a dimensionality reduction and showing you the first three factors here. And you can kind of see these color codes. It's red, and then the animal has to jump at green. It gets to roar it at orange and then comes back around in blue. This is when he has to jump. When he doesn't have to jump, the neural trajectory is. The neural trajectories in this reduced space actually collapse quite a bit. I noticed that this red part is before he actually has to jump. So he knows whether he has to jump or not, and the neural activity actually diverges on these two different types of trials before his behavior has to change at all. And again, this is largely based on space. So this is some evidence that it's not real space, but it's some latent space that has isomorphism, some isomorphism with actual physical space. With actual physical space. And there's some other evidence, too, that when the animal does some weird things, the trajectory goes way off, even though he's in the same place. But I'm going to skip that for the time being. So this is sort of an intermediate summary. So I'm thinking of this anterior cingulate as sort of the highest level of abstraction. And what it does is that these other levels have finer resolution, and they are subservient. And they are serving to guide the animal's perception and actions most of the time. When they get stuck, when unexpected things happen, that's when the circuit kicks in to use knowledge to decide what to do next. And so one way to think about this is, or to make an analogy, would be an autoencoder. What we know by anatomy is that the sensory and motor systems comprise most of the brain. They have a lot of connections. This part with the anterior cingulate only has, you know, it's relatively small. It doesn't have a lot of connections. It's relatively small, it doesn't have a lot of connections, but it encodes almost anything that's task-relevant you can find in there. And it's usually multiplexed with other stuff. So it's a very non-sparse representation, and again, it looks to be in some latent space related to what the big task is. Okay. And this is a bit speculation, but it's perhaps why we think in spatial context. So we're here, you know. So we're hearing, you know, what we were hearing before about, you know, thinking of, oh, geez, you know, certain things as being up and other things as being down might come from this. And I want to elaborate a little bit on, you know, why might this be? And what we just know from models is that some things like sensory motor action, so think about shooting a basket, that's a complex sensory motor task and it requires a lot of practice. Task and it requires a lot of practice. Once you learn it, it transfers to any who. However, if you're playing a game at halftime, you have to switch sides. And you don't have the luxury of trial and error learning. You shoot in their own basket and you get benched. And so this is what I'm saying is that the sensory motor systems do this, and it's a complex model, but the ACC serves as a supervisor that says, you know, you're end for the wrong one, change your direction. It does that in an inferential way. You don't have to get the negative feedback of the team being mad at you. Team Leumatic. The cingulate area physiologically becomes disrupted in many neuropsychiatric diseases. It loses spines and becomes hyperactive. And the mental models themselves are disrupted. And so negative things become overrepresented. So I'm interested in that. And so this is kind of what I'm showing here: we've got spine loss. We also have a decrease in the relative amount. A decrease in the relative amount of inhibition relative to excitation. I think that's where the hyperactivity comes from. And so the question is: well, what happens to the representations in such a network where you start to lose spines? And what I thought going into this, this was years ago, is that we'd see a reduction in signal to noise. But we actually find the opposite, that more cells are task modulated, more cells care about reward, and the ones that are activated have higher activity both in real terms and when you do normal. Terms and when you do normalization that it's heads for. So there's actually more information, which seemed kind of counterintuitive. The way I'm thinking about this now is that a naive network. So when humans are about 18 months old, they have the most synapses they'll ever have. And after that, it's a pruning game. And so you lose synapses for the rest of your life, well into old age. And this losing synapses is part of learning and gaining knowledge. And what might be happening. And what might be happening is that this might be overdone because of stress, because of other things that you can ask me about later. I've thought a lot about this. And what happens is that in the beginning you don't have well-defined sub-populations, but then you get the emergence of sub-populations. But if you overdrive this, you get the dominance of a couple. And what this does in the state space is it starts to contract the dwell time in the state space. And you could also see this in the correlation matrix. And the question is. The question is: Can we go the other way? And one of the ways I think we can go the other way is with certain drugs like psychedelics. And to make a long story short, like what Nigeen talked about before, is we have a rodent on a treadmill. We're doing calcium imaging. This is in the retrospilant cortex. Normally, we have these place fields. And after you get this drug, the place fields, so normally this is one cell has a place field. After the drug, the activity is no longer locked to that place field. No longer locked to that place field. If we look at the whole population, again, we see that it gets scrambled after this ibuvaine drug, but not after saline. We do have some maintenance of activity at the landmarks. So what I think is happening is that the drug is destabilizing the dynamics that track where the animal is when there's no external inputs. Fishing up here, and we can see this in the correlation matrix. Interestingly, the scale-free dynamics are not affected by this. So I thought I could give the drug, and probably everything just gets crazy, and this is from. Just gets crazy, and this is from Lauren and Dalmor's work. And so, the conclusion here is again that this ACC is sort of the top of the abstraction hierarchy, and that certain things might drive it to become too sparse of a connected network. And one of the big questions is, can we go the other direction with drugs like psychedelics? And so, I like what Majit said, you know, I'd be really interested in your input on this. And so, really, the trillion-dollar question, I think. The trillion-dollar question. I think the way we end up solving things like anxiety, depression, some of the things in Alzheimer's is to understand what is this representation in this limbic system. Can we come up with a mathematical description of the encoding? What learning process would lead to that? And is there some way to kind of come back to some optimal representation? And so, thanks to my many colleagues in my lab and my friend here. Thanks, Ian. We have time for questions. Thanks, Art, a lot of the first part: if you were to inactivate ACC, what would you happen to adapt? Very little. So the thing that animals are, so when, it's been done a lot in humans, the only reliable thing, deficit, is when you ask them to do what's called extra-dimensional shifting. So they have to, normally they attend to one stimulus dimension, like order, and then the task then switches. And then the task then switches in an uncued way to them, so now they have to pay attention to something else like the texture. In humans, it's much more social. So there's actually therapeutic lesions for depression and anxiety and obsessive-compulsive disorder. They'll go in and make lesions of anterior cingulate. It takes away the persistent negative thoughts. It doesn't affect IQ, memory, a whole host of other things. It does tend to make people seem a bit sociopathic. Sociopathic, so they have trouble maintaining friendships. And I think part of this is because this is beyond the time I have here, so ask me later. Is that a big threat in social animals like humans is if you're upsetting the group. And so being attentive in that my behavior, if it's upsetting people, that's bad because sometimes I'm going to depend on the group to feed me when times are bad and protect me and all that stuff. And so the single is very important for that. Just pretty darn in my advocate. So, no, all those people that are working in frontal area, in the frontal corporate, they argue that no one really finds species. Find everything. And the reason why they argue is that deliberate and most of the behavior in paths are still income. So, the argument is: do you think if you record from the If you record from V1, are you going to see the same phenomena that you saw in medioprofrontal cortex? I don't think so. So, this is my explanation for that, is that this medioprofrontal cortex serves as kind of an auxiliary system, and that you really need it to infer what to do when things are uncertain. And so, I think, you know, if we design the right task, I think we see more of the effect. More of the effect. But most of the tasks are in trained animals. And I think once animals are trained, they can use the sensory motor system to solve tests. And so you don't see any deficits because this is only needed at certain times. And the reason why I'm asking that has been paid by David Tack, arguing that, I mean, and it's relatively complex, but regardless of where you need it, it's not really difficult to Yeah, I mean, there's certainly, you know, I think there's a lot to unpack there. I don't really have a, I mean, what's kind of being embodied or, you know, what's under the hood in here is that, right, we have a network and it's part of a bigger network, and that network interfaces another network, another network, another network. And anytime you go in and muck around with it, Go in and muck around with it, it's going to take the system. You know, some alterations can be accommodated for by practice or by shifting behavior to a different part of the system. Other parts, not so much. Like some, you can have damage that disrupts a function that can't be mitigated by other parts of the system. So, I mean, to me, it's not a killer argument that you go in and you got this very complicated thing that's highly interconnected. Entirely interconnected. You trash something, if you look hard enough, you're going to see an effect. Even if it's visual cortex and you've got some olfactory task, there's just so many points of contact between these systems that I think it's kind of inevitable. I think we have an online question. Yeah. So, Aaron, early on, you mentioned kind of the coming accuracy drop-ups at the worksites, right? Sites, right? Yeah. Isn't it possible that, like, if the animals are primarily stationary at the reward site, so they're not moving, and the neural dynamics themselves could be slowly adapting or changing in time. And the decoding accuracy just stops because the decoders don't, I think both your decoders were like static in time, right? Deep neural network and the basic decoder print. Yeah. Any control components, and that might explain why. And that might explain why the creating ant procedures drops. The neurons themselves choosing to choose a bit of time in a stationary location. Yeah, so there's a couple things. So that's a really good point. One thing I would like to point out is that they're also stationary here at the center, and they get the same reward. And so if it's really just a confusion between moving or stationary or stationary and consuming a reward, all three sites, these two top feeders and the center feeder, should all be. Feeders and the center feeder should all be confused, but they're not. It's only a confusion between these top ones. So there's something kind of special about those. And we did a ton of controls because that was one of our biggest hurdles is to discount that kind of null hypothesis that it just has something to do with the similarity of the encoding at Coco Ordinance. And so we did quite a bit of work about that. And I could talk to you more about that offline. But thank you. I mean, that's one of the key things we have to battle. Of the key things we have to battle this. Thanks, Aaron. All right, so let's move on to the next one. Let's thank Aaron again. Have you connected the laptop to the power?