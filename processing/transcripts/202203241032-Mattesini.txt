Also, let me thank Michael Goldman for already warming up the audience with the topic of the optimal matching problem. So, in this talk, that is a joint work with Martin Usman and Felix Sotto, we will try to address some questions regarding the limiting, the thermodynamic limit that has been already somehow introduced by Michael. So, although you know basically already this story, I will tell you this again because now there will be some change of. There will be some change of notation, and so it's nice if we fix everything for one time again. So, the optimal matching problem can be defined as follows. So, you consider two and cloud of points, xi and yi, and then you want to connect each point from one cloud of point to another point of the other cloud of point in such a way that is optimal. What do I mean by optimal? I mean that if you take the sum of the Euclidean distances between those two points, Between those two points to some power, then you take the matching which minimizes this quantity. So, since we are in an optimal mass transportation workshop, so what has this thing to do with optimal transport theory? So, quite naturally, you can associate to each n-cloud of point an empirical measure, which consists on the sum of the Dirac of the points. The Dirac of the points of the cloud. So let's define μn and μn as the sum of the Dirac of the points of the first cloud and the second cloud. I'm speaking about empirical measures, but here I'm not normalizing. What's important is that the two measures have the same mass. And then we can define a distance between those two measures, that is the vastest end distance. That is, as you may already know, you take the distance between two points x and y to the power gamma, and then you average with respect to a copy. And then you average with respect to a coupling, and then you take the minimum all over the set of coupling of this quantity. The nice thing is that this for the empirical measure by Bilco theorem, you can say that the vastest eye-gamma distance between those two distances, two measures, is exactly the minima of this optimization problem. Okay, the first question you may ask yourself is: how this cost diverges? Diverges, goes for n that becomes large. And you have a very easy intuition. So, since you are considering two n clouds of uniformly IID distributed points, you expect to have some kind of uniformity of your system, meaning that you expect the point to be spreaded as in a regular tree. So, you expect the typical distance between two points to be of order n to the power minus one over d. So, now if you substitute here n to the power to substitute here n to the power minus one of the d, and then you take the gamma power and then you sum these n times, you would expect to see behavior. This is of order n times n to the power minus gamma already. Unfortunately, you are considering a random problem, so you have some fluctuations, and you are rating your guest phase in low dimension, which are dimension two and dimension one. So as you can see, Dimension one. So, as you can see from this rating that I wrote here that are valid for gamma being greater than or equal to one, you have a logarithmic correction here that comes from your fluctuation. And also in lower dimension, your guess differs from the real rate from even larger rate. So this thing means that the fluctuation becomes even more important in lower dimensions. Important in lower dimension. We can get also an intuition of why dimension two is critical. So if you consider your box 0, 1, where your points are taking values, and then you consider an inner box of size A, then you spec in this box, if you scale everything correctly, to have A to the power D points. But since you are considering IID uniformly points, then you have to take into account the fact that there might be account the fact that there might be fluctuations which are of order a to the power d over 2, since your random variable here is a binomial random variable. So now since you don't want your transport to become not so large, you have to take care of this extra a to the power d over two points and transporting them by something which is nearby those points, meaning that you want to transport them into some them into something which has the surface dimension of the cube, which is of order a to the power d minus one. Now you see if d is equal to two, then you have a one here and a one here, that means that the dimension two is critical. So you can barely fit points into something which is of this order, but if now the dimension becomes larger from three on, then you can for sure. From three on, then you can for sure fit d over two points into d minus into something which has order d minus one. But if this dimension is lower, there's no chance for that. So the critical dimension has been studied by AITA, Komnos and Tuzhnady in 1983, and these logarithmic terms have been already seen since then. What we want to do now is to consider a thermodynamic limit. So, naively, we want to sum in into our system in Into our system in such a way that the typical point between the typical distance between two points is of order one. So, what we do, we consider we invade, let's say, R D with a box of scale of the side length L or the torus of side length L, and then we throw L to the D point into them so that the ratio between the volume of the box and the number of points is always one. points is always one and then we call ql the gamma minimal capital meaning that this is uh by ql being gamma minimal i mean that this solves the optimal matching problem in this box and then we want to let l to in let l go to infinity so now there are many questions you can ask yourself so the first question you may ask yourself is uh which kind of property has this limiting coupling and the first the very first one you may ask yourself is One you may ask yourself is who are the marginal of this coupling? So the marginal of this coupling are given by the Poisson-point process. So here I want to recall you what's a Poisson-point process. So it's defined as being a random variable, which takes value into locally finite, into the set of locally finite atomic measures. And it has this nice property which is written here. So if you consider k this k disjoint bore a set of RT, then the random variables mu of a1 and μ of ak are independent, and every random variable μ of ay has Poisson distribution with parameter ai. You can prove the existence of this object thanks to a superposition argument. And the key observation is noticing that if you restrict yourself to a borel-bounded set of Rd. Abound this set of Rd, then, and you condition on the event that the number of points in this set is given by n, then you can identify the Poisson point process restricted to your set omega by the sum of the rack of xi iid uniformly, of n iid uniformly distributed points on omega. So, for what will come later, I will make you. Later, I will make use of these two different notations for the Poisson point process. One is this measure theoretic notation, and I will indicate it by mu, but sometimes I will identify it by its soft part. Okay, so since we are now considering infinitely many points and not more finitely many points, there are some definitions that we need to introduce. The first one is: what do we mean by matching? Is what do we mean by matching? So, what we are interested in is bijection from the first cloud of infinite cloud of point X to the second infinite cloud Y. And we call those bijection T. Then we will call matching the triple X Y T and then we also need to understand what's a correct definition of gamma minimum now because we have infinite mass on both sides and then we would like to say We would like to say now what's the correct definition for being optimal in this setting. So, what we say for a coupling that is optimal, we say that the coupling is gamma minimal if it's cyclically monotone. That means that, for instance, if I take a finite number of points on the first cloud and then I look on the points to which those points are matched and I do a rearrangement of those. Of those points, then the choice of the map T is the optimal map. So any other rearrangement of the points to which the points of the first cloud are meshed will give me a larger cost. Okay, now I want also to introduce the concept of stationarity. And now we say that the matching XYT is stationary if the joint law of the matching is. If the joint law of the matching is invariant under shift of Z D. So, why do I introduce this? So, as I was saying before, we want to basically invade R D. So, when invading R D, we can do it in many ways. So, one can consider a box of sideline L and then a large box, or one can also consider, for instance, the L-dimensional torus. So, if you consider the L-dimensional torus, The other dimensional torus, then what happens is that if you have a gamma minimal matching on the other dimensional torus, and then you shift your two clouds of point, then the gamma minimal matching of the gamma minimal coupling of this shift problem is the shift gamma minimal coupling of the starting problem. So you would like to see in the limit. So, you would like to see in the limit some kind of stationarity for the minimal coupling. So, that's why one introduced this stationarity. So, it's not clear why, whether in the limit there should be or not a coupling, but if a coupling there should be, then we would expect it to have this kind of proper stationarity property. So, there was a question which was firstly asked by Pere in 2002: that is, if you consider is if you consider two independent Poisson point processes X and Y with intensity one does there exist a stationary parallel matching so now I said two words that I didn't define so the first one is intent being of intensity one for a Poisson point process means that if you take the number of points which are inside a bounded set A and then you take the expectation then this is equal to the Lebesgue measure of the set of the bounded set Bag measure of the set of the boundary set you are considering. So, why this requirement for the post-not point process? Then, this is just asking that the interpoint distance is of order one. Then, the second things that I introduced is planarity. So, planarity means that if you match points, no crossing between the lines occurs. So, this is the one of So, this is the one on the left-hand side is a planar matching, and the one on the right-hand side is not a planar matching. This question on the existence of a planar and stationary matching asked by Pere has been relaxed and has been asked in a weaker version by Alroyd in 2009. And instead of asking for planarity, he asked for the existence of a stationary and gamma minimal matching. So, why asking gamma minimalities? Asking gamma minimality. So, this is something that may pop up into your mind. And the reason is that if you consider one minimal coupling, then by one minimality implies planarity by a triangle inequality argument. So, let us look a bit on what's known about this problem. So, in 2020, on a very recent 2020, on a very recent paper, a nice paper, Holroyd, Janssen, and Bastrun, they proved the existence of a stationary and gamma minimum matching if the dimension is one and gamma is less than one, if the dimension is two and gamma is less than one, and then if the dimension is larger, larger, larger than three, and gamma is finite. So there's some things that must be said. So for instance, if you take large dimension and gamma Large dimension and gamma being less than one, your average cost is finite. So I've showed you before some asymptotics for the cost, but those were valid only for gamma being greater than or equal to one. So as soon as you have average cost finite, like for instance, in this case, you can run some sort of compactness argument to show the existence of stationary and gamma minimum matching. Minimum matching, but finiteness being that the fact that the cost is infinite does not imply non-existence. So, here is a case. So, in dimension one, if you take gamma between one half and one, you can show that the cost is diverging in the size box of the box you are considering. So, for So, for instance, if you choose gamma being equal to one half, then you see that your box, that your cost is diverging by a logarithmic factor. Another thing you may ask yourself is, what is this gamma parameter telling us? So, here are some simulations that I've taken from the paper of Holrey, Janssen, and Wastloon. So, on the top left, there is a simulation for gamma being equal to A simulation for gamma being equal to infinity. So now, maybe if I zoom in, you might see it better. So if gamma is equal to infinity, long edges are evenly penalized. And so you might see that the cost is basically evenly spreaded between the points. So gamma being equal to infinity means that in the minimization problem, you take the limit for gamma that goes to infinity. And gamma can be also described. Gamma can be also described as in a futuristic way, let's say, as a parameter of measure of fairness and altruism of the system. So you can consider blue and red points as agents that want to choose their partner, and they want to do that in such a way which is optimal. And this optimality is given by gamma. So if gamma is very large, then we say that the matching is altruistic in the sense that they are sharing the cost as much. That they are sharing the cost as much as possible. Then, instead, if we take the other limit case, that is the case for gamma being equal to minus infinity, we see that every point basically pursue its self-interest. And in this case, we say that the matching is stable, and this concept of stable has been already introduced by Gale and Shapley in the seminar work. And being stable for a matching means that if you choose, there's no couple of red. There's no couple of red and blue points that are strictly closer to each other than their partner. Then, here on the top right, there's another case, let's say, which is also, by the way, some sort of critical case because this is case of gamma being equal to one. And as you can see here, as I was saying before, no crossing of points occurs in this case. Okay, so let me. Okay, so let me go to the next slide. So we will focus for the next part of the talk of the case of gamma being equal to two and dimension two. So what we are able to say is that there exists no stationary ergodic and two minimum matching in dimension two. So here I also added another assumption that is ergodicity, but this is not really needed. So what you can do is that as soon as you have a matching which is Have a matching which is stationary, you can decompose the coupling into terms of its exotic component, and then run the arguments that we tell you in the following. So the idea of the proof, luckily, can be written only on one line. So we will try to argue by contradiction and we will show that stationarity together with two minimality will imply that I can bound the L1 energy defined as I look. Defined as I look at the point from the first cloud which are inside a ball of radius r, and then I look at the point to which they are matched, and then I sum this contribution. Then I look at the point which are in the same ball, then I look to the point to which they are matched, and then I sum this contribution. Then I can bound this from above by something which is infinitesimal with respect to the square root of the logarithm of the radius. Of the radius, but from the theory, which is basically the asymptotics that I show you in the second slide, with R playing the role of N and some scaling needed, obviously, we know that from below, the behavior, from below, this quantity goes as the square root of the logarithm of the radius. Here is a quick remark. I want to. I want to point out that too minimality does not imply planarity. And this can be seen the following way. So you take four points and you take these points in such a way that they are horizontally shift by one. And then you take a vertical shift between points of the same cloud, which is given by a constant C. Okay, now if you compute this cost with this. This cost with these choices that I've written here. So now your cost, what you do by hand, is like you take the x component of this point minus the x component to this point to the power two, then you do the same for the y component and you sum them. You will see that the cost of the parallel matching is equal to 10, while the cost of the crossing matching is equal to 2 times 4 plus c squared. Now, if you were clever enough and you chose c being less Less between zero and one, then you will see that the crossing matching for the two costs is preferable to the parallel matching in this case. So this is just to point out that minimite does not imply priority. So why you can run this easy counterexample is that you don't is that because you don't have a triangle inequality. So now let me explain. Let me explain what's the main point of the proof. So, we can disintegrate our proof into five steps. So, the first steps is an L0 estimate. So, basically, we will argue by stationarity to show that the number of points inside of a box of side length 2R that are transported by. That are transported by a far distance is small in volume fraction. And this is thanks to stationarity plus egodicity. Indeed, so if you so this is an easy application of a Bico-fon-Neumann Egodi theorem. And in this first step, stationarity is the key assumption. While up to here, you don't need any information on dimension or Informational dimension or the power to which you are considering your cost. Then the second step is consistent lifting your L0 estimate into an infinity estimate. And you can do that by arguing by a monotonicity argument. So the fact is that you are considering two minimal meshing by assumption and two minimality for the And to minimality for the matching implies monotonicity for the map, for the transport map. And then by a monotonicity argument, you can show, you can lift your ergodic estimate into an infinity estimate. I will try to give you more details later. And then there comes another step, which is where gamma equal to 2 and d equal to 2 are crucial. And this is a Crucial. And this is a step which requires an harmonic approximation theorem. So, what is important is here we are using some sort of quantity information now the displacement and how the displacement concentrates around a solution to a certain Poisson problem. That is also something that Michael was telling me to his talk before. And then before and then as soon as we have this then we can run by we can run an older inequality argument to drop some integrability and show that we started with something which was an ergodic estimate which tells us that the number of points which are transported by a large distance is more in volume fraction then we lift this into some information on the displacement telling us that the displacement between two the displacement to displacement is the length order. And then we lift this into a L2 estimate for the energy, showing that this is bounded by above by something which goes logarithmically. Then we drop some regularity, some integrability to show that this is indeed infinitesimal with respect to the square. Respect to the square root of the logarithm. And then there's the last step, which is a lower bound. And for the lower bound, this is already known by theory that this quantity goes from below by the square root of the logarithm of r. Okay, now I don't want to say much about the ergodic estimate, but let me at least say two words about the L-infinity estimate. So the point here So the point here, so we have an information on how many points are transported by a far distance. So what Humatonisti is telling you is that basically you can build barriers around the point and those barriers are based on the good points which are not transported by a far distance. So what do I want to say here is that to say here is that if you take a point x into your your square minus r then within this point you can find into a ball of radius which is of ordinary epsilon of r three points which are not transported by a large distance and those three points are very special because you can the the al of those of the direction x minus xi is Xi contains the ball so that you can test your displacement by unit vectors. And then since those points, since you have monotonicity, you can write this inequality here, which graphically tells you that the point X does not go away from something which looks like this. So basically, So basically, your point x is not exceeding this ball, and so the control that you have on the displacement is given on how far away you can find those points. And those points are contained into a ball which is a forward epsilon r. Thus your infinitesima with respect to the length L infinity estimate. So now instead, if you want to consider the harmonic approximation, and for that, I don't have much. Approximation and for that, I don't have much time, but I will try to say the crucial steps. So, what we want to do now, we want to improve the L infinity estimate to an L2 estimate. And we already have some information. And the crucial information we have is that this quantity here is bounded from above by something which is small with respect to R square. So, this is a crucial. So, this is crucial for applying the result of the next slide. So, this is quite a heavy slide, but let's try to dig into that. So, the first thing we want to define is a local energy that is basically the same thing that I was defining before for the L1 cost, but now I'm taking the cost which is squared. And then the second thing that one wants to define is Define is a data term. So, this data term is something which is telling you how your some point process is far away from being a uniform measure. So the first, so you define the data term in the following way. So, you define it as being like the R-dimensionalized various distance between the point and the number density, where the number density is the constant for which this transportation problem makes sense. And then you sum also alpha. You sum also how far away is your number density for breaking one? So, this is basically a measure on how far is your measure from the uniform measure. So, now there's this theorem by Michael Goldman, Martin Usman, and Felix Sotto, which tells you the following. So, if you choose a threshold tau, then the harmonic approximation theorem gives you, so if you choose a small So, if you choose a small fraction tau, the harmonic approximation theorem will give you an threshold epsilon and a constant C Tau, such that if for some large R you have this control over the sum of the energy plus the data term, then there exists an harmonic gradient field such that the displacement is near to the harmonic gradient field in this sense. Field in this sense. So you can control your displacement minus the harmonic gradient field by something which is small times the energy plus something which is small time, plus something which is a constant times the data term. Now the question is, can we apply this for our case? So what we know from before is that the energy is something which is smaller than epsilon r square. We can choose it becoming small so that we have a control on the energy. So, that we have a control on the energy, at least. While for the data term, instead, the control comes from the property of the Poisson-point process in dimension two. And indeed, we can say that for R large, then this is bounded from above by the logarithm of R times R. So we can choose, sorry, the logarithm, this is the right one, the logarithm of R. So we can choose R being large enough so that we have this control for the energy plus the data term. The energy plus the data term. And as soon as we have this control, then we can consider our L2 energy, split it into two contributions. A contribution which comes from the point in the wall which are not transported by far distance, plus a contribution that is the main contribution that is given by the points which are transported by large distance. Then what we can do is that we can add and subtract the harmonic gradient field, apply the harmonic approximation. Apply the harmonic approximation theorem to get an estimate for this term and also for this term. Indeed, this term is controlled by something which is a constant times the energy plus the data term times the number of points which are transported by a large distance that we know to be small. So that we end up with this inequality written here. But then we have a control on the data term, which is growing logarithmically. So if you relabel everything, Relabels everything, you get this control for the L2 energy. So now we can iterate this argument to get this bound, which is written here. Then letting k going to infinity, this quantity here disappears, and then this is a convergent series so that we can bound our energy by something times the logarithm. So that's why. The logarithm. So that's why that's how one can go from the L infinity estimate to the L2 estimate. Then, okay, for the upper bound, instead, you can again split your contribution into the points which are transported by small distance plus the empty point which are transported by large distance. Then, for the first case, then you have something which is controlled by a constant. For the second case, you can run a Cauchy-Schwarz inequality. can run a Cauchy's warts inequality and you will have the energy square square the square root of the energy L2 energy times the number of points which are transported by large distance that we know to be small so that you pass from an L for a logarithmic scale to a square of the logarithmic scale so in conclusion what we were able to discuss is that for gamma being equal to two For gamma being equal to two and dimension two, there exists no stationary ergodic and to minical matching. But our proof strongly relies on considering the L2 cost and dimension two. So now a natural question is what happens if gamma is greater than or equal to one? So there are some ongoing works which make us strongly believe that for gamma greater than one, one can run. Greater than one, one can run something similar, provided that one has an harmonic approximation, some sort of harmonic approximation theorem for peak cost. Indeed, what was really important for us, it was from passing from, forgetting the contribution, from passing from step four to step five, is that we have some room for dropping integrability. So passing from something which, so having something which is more than a one integrable allows us to pass. Integrable allows us to pass to something which is to an estimate on the L1 energy. Then, however, considering the case of gamma being equal to one, this requires a different argument because now one can't cook up an argument for going from what was step four to step five of our proof. So, thank you for the attention. And if there are questions, if I've been too sloppy, some part I've been too sloppy in some part, please ask. I will be happy to reply. Thanks a lot for the nice talk, and thanks a lot for being sloppy. I think it helps a lot. Are there questions nevertheless? So, maybe can you go back one slide? So, did I get this right? Ergodic is not necessary in Ethereum as an assumption? No, you can decompose your coupling into ergodic. So, why did you put this? The nice thing is that, so if you want to do this ergodic estimate, so now you... No, I think I understood how you used it, but I was just wondering. It's nice here. So, because if you take what Here, so because if you take one over r to the power d here as a scaling, then you can run Bikov theorem which tells you that this thing is converging to the number of points, the average of number of points on 0, 1, let's say D, which are transported by a large distance. But this is conditioned to the shift invariant sigma algebra. So, as soon as you have a good So as soon as you have egodicity, then this is a full expectation. Otherwise, you need to be a bit more careful. So that's why ergodicity is handy here. Okay, so I think it was just suggesting that you could be more sloppy and say, without loss of generality, we assume that we are ergodic and don't put it in the theorem. Yeah. Okay, sorry, don't take me serious. If there is no further questions, then I say. Further questions, then I suggest that we continue with the talk of Dan. Thanks a lot again. Thank you. Matthias, should I try the screen share here?