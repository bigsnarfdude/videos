Looks fine. Okay, can you see it all? Yeah, it seems fine. Okay, good. So it's it's uh hybrid, as you can see. Some typed and some written. So uh Okay, so we can start with our next speaker, Jan Plistensen, who is going to talk about the compositions of sequences of PDE constrained maps. Please, Jan. Thank you. Thank you for the invitation to talk here. Yeah, so the talk is based on joint work with Andrei Gera and Bachden Reich. And so it's based on work that you can find. On work that you can find in these preprints here. So let me start by just focusing on the problem that I want to discuss here. So, omega, that's a bounded and open subset of Rn, and I imagine that we're given that. And I imagine that we are given a linear constant coefficient homogeneous differential operator of order k. So here you can think about the coefficient A alpha, their matrices. So A of D, it acts on vector-valued functions and it produces vector-valued functions. And so what I talk about is definitely related to the very nice talk of Rupert, at least at a phenomenological level, because I'm looking at Level because I'm looking at sequences that converge weakly, that satisfy PDE constraints, and I'm interested in saying something about what can prevent these sequences from converging strongly. So this capital UJ, that's an example of a weak deconvergent sequence. So here I assume first that P is strictly between one and infinity, and this satisfies the constraint or the P D, A D of U J is equal to zero. And now I wonder. Now I wonder because we know that what can prevent UJ from converging strongly on LP. So remember, see, I had omega to be a bounded and open subset. So then the two obstacles to strong convergence, there are oscillation, so that we do not converge in measure, and concentration in LP, so lack of P equation capability. So I would like to know whether it's always possible. To know whether it's always possible to take the sequence that converges weakly to U and decompose it so that one sequence, GJ, it carries all the oscillation but no concentration, BJ carries all the concentration but no oscillation, and at the same time satisfy the PDEs. So more formally, I want dj and pj to convert weakly to zero. Tj is p equal integral and pj goes to zero and math. integral in pj goes to zero and measure so so that's the first question and the answer to that is kind of well known uh and is essentially yes not quite i mean there's a small technical gap here but i will i will address that on the next few slides then uh probably the main part of the the talk though i'm not sure how much time i will have for this is the l1 case so here the right formulation So here the right formulation is that we have sequence uj that i1, and then they convert weakly star in the sense of measures to a vector-valued measure capsule u. And we know that a d of uj is equal to zero. And the question is, again, can we decompose? And this time, let's say we want to decompose uj into dj plus bj, dj, a green juggernaut bj, converging in Lebesgue measure. In the back measure. In this case, the answer is no, a resounding no, even. So, what I want to discuss here, rather than this impossibility, is what one can say then instead. So, I want to start with the unconstrained case. This, of course, is entirely trivial and elementary, but it somehow sets the scene and it might still be useful to fix ideas. To fix ideas and also it allows me to do some of my notation. So, omega remains bounded and open, and I'm assuming something about the boundary, not very important. And P is exponent between one and infinity strictly. And then I assume that Vj converges to V, weakly in L P, and these guys are R D values. And I do not have a P D constraint in this case. Case. So I can say that if Vj also at the same time generates a young measure of oscillation, and that means exactly what I have written here, that a decomposition is possible. So I can find sequences GJ and Bj so that Vj admits the decomposition into V plus G J plus Bj and G J and B J convert V to zero and L P G J is P zero and L P J is P equo integrable and Bj goes to zero in measure. So sorry I want to stress here because in the abstract I skipped the word subsequence. So it is false for the full sequence without the assumption of generation of a Young measure of oscillation. And it's possible to give a counter example. So that involves some calculations using Young measures. But you probably believe that. But you probably believe that if you also think about the closely related statements of Bokes and Chacon, the spiting lemma that is also only valid for subsequence. Okay, so let me just very, very briefly take you through a sketch of proof. And so it goes as follows. So I can assume v is equal to zero because I can just look at v j minus v otherwise. And there I construct this non-concentrating sequence simply by truncation. And you can truncate in many ways. So, for example, you can truncate at the level t in this way. So, I take argument z, and I keep z if mod set is less than or equal to t, and then I replace it by t times z over mod set if mod set is larger than t. In terms of this map that is continuous, map that is continuous, I can look at t, the truncation of v j at level t, known to power p, and integrate that aroma. And I take first j to infinity and then t to infinity, and I recover, using some standard properties of oscillation Young measures, I recover that the limit can be expressed in terms of the Young measure. And looking at this, you see that you can take a sequence Tj, you can take it through integers, and it is just It through integers, and it is just weakly increasing to infinity so that you have star. So you have this convergence here. And now you conclude the proof simply by taking gj to be this vj truncated at level gj, and then bj is the rest. Because gj goes to infinity, you can probably believe me if I say that bj goes to zero and measure, and once bj goes to infinity, And measure, and once vj goes to zero and measure, then gj must generate the same young measure of oscillation as vj did, so that is, and then by general results about these young measures of relation and star, you have that gj is p integral. Okay, so that's the proof, simply truncation in a straightforward way. The p equal to one case is really the same. So if vj is an L1 and we have this weak star. Is Nel one, and we have this weak star convergence in the sense of measures to a vector-valued measure. And again, if Vj generates a young measure of oscillation, then I can decompose. Now I decompose in this way because I do not want to subtract V because V is a measure and I want to work here along the sequence with L1 fields. But really, it's the same. Okay, now I don't want to tire you, but I really would like to start with a very simple, what I think about as the basic calculus variation. About as the basic calculus variations case, namely the gradient fields or curl-free sequences. The reason that I want to start with this is that I can then motivate the assumption that I will later on impose on the differential operator AD. So now I'm going to assume that omega, this is this bounded open set. Now I assume also that it's W1P extension domain. P is strictly between one and infinity still. And I assume that UJ can And I assume that uj converts weakly to u. This is a sequence of superlow maps w1p, and they're valued in R capital L. And then I assume, as I did before, that the gradients of UJ, they generate a young measure of oscillation. And if I have this, then I can show that there is a decomposition, namely there are sequences Bj and Tj in W1P, such that UJ can be decomposed as U plus Bj plus Tj. U, remember, was the B. TJ, you remember was the weak limits. Bj and Dj have the properties required, so they go to zero weakly in W1. Gradient of Bj goes to zero in measure, and gradient of Tj is P equal into the over. So I simply split up here the oscillation and the concentration in the sequence, and you can arrange that G J is very nice. So this is an old result, and I believe I might have done it first in a technical report when I was. It first in a technical report when I was still a PhD student in 1994. And I really did the proof along the lines of what I have sketched here together with the Henhof's decomposition. And I used exactly this stability lemma of Ivanietz that Robert Franke was mentioning in his talk. It turns out to be for this proof overkill. And I was told by Stefan Müller that I should simplify the proof. And I did that in 1999. In 1999. Then, meanwhile, there was a proof by von Seger, Mueller, and Petrigen of the same results using deep-shift truncations. Okay, so you can extend this slightly and I'll speed a little bit up now because this is not really the main part of the talk. So the setting is as before, you can relax the condition of these fields satisfying the curl constraint exactly. You can assume now only that you have strong convergence of the curls. Of the curves in W minus 1P. So remember, so this is the dual space of W1P naught, W1P prime naught. And so under these assumptions, you can still say the same. UJ should generate a Young Mesh of oscillation. Then you can find sequences. Then you can find sequences. Now, because the curl constraint is only asymptotically satisfied, there's going to be an error term. That's the sequence EJ. But everything is as it should be. And you can decompose uj as u plus ej. Ej converts strongly to zero in L P. And then gradient of Bj and gradient of Tj satisfy the same conditions as previously. So this result, you can also. So these results you can also prove using the same methods using truncation and Helmholtz decomposition. First time I think it was articulated is from Secret and Müller and perhaps Murat a little bit earlier in a slightly more general context. Okay, so how do you construct this decomposition? Well, it is simply this extension, this truncation of the maps. And then, of course, when you truncate. And then, of course, when you truncate, a gradient is not a gradient any longer, so you need to get back to gradient. The way to do that, or one way to do that, is to use Helmholtz or Hotz decomposition. And so here it's important that we first extend all maps to the whole of Rn, because there are some issues, as you will know, with the Hilmhaus decomposition on domains that are not very nice and smooth. So what you do simply, you do that in a standard way using cutoff. That in the standard way using cutoff. If you take a sequence rho j increasing to one or the indicator of omega, should it be slowly so that you still have the condition that you see here. So I can simply replace the sequences Vj, the sequence Vj with rho j times Vj, and then everything is now living on the whole of Rn. So this is the advantage of working with the curl constraint only satisfied. Curl constraint only satisfied approximately. And then you proceed as you did before in the unconstrained case. You truncate, for example, exactly the same way as in the unconstrained case. And then you get back to the gradients by Hilmhotz-Hotz decomposition on iron. And I already mentioned the paper by Von Sega-Müller and Petrogel. There's an alternative to all this. That's Lipschitz truncation. It works at least nicely when you have exact curl-freeness. So You have exact curl-freeness. So, when you work with gradients, and this has been through the hands of many people, probably even Calderon had some results along these lines earlier than Leo. So if you look at the proof, you will see that, okay, you have the Hilmhaus decomposition, and that's all very fine. But actually, what you really need is another estimate. What you really need is what I call here in blue the key estimate. So remember. Estimate. So remember, P is strictly between one and infinity, and I claim that there is a constant C such that this distance from U to the gradient fields can be bound by C times the W minus one P norm of the curl of U. Now the curl here acts row wise and in the distributional sense, of course. So this is the key estimate. And what does it mean? Well it just means that What does it mean? Well, it just means that if you look at the operator curl that takes L P matrix fields and produce W minus 1P even higher order tensor fields has a closed range. So this is just a closed range statement really, nothing else. So the key thing here is closed range. So when do you have closed range? So if you look at p equal to 2, then things are simpler because then the distance to To the gradient fields is realized by simply looking at the difference of u minus the orthogonal projection onto gradient fields. So I use script P here for that orthogonal projection. And we know we can express this as in terms we use the Fourier transform and use this variation or characterization of the orthogonal projection. Of the orthogonal projection, you can express it in terms of Ries transforms. So, what I want to say here is really just that we have the identity here, and it gives us the possibility of saying something sensible about this. And then, of course, we know we can extend this guy by continuity to all LP. So, that's what we have in the well-known gradient case. Now, let's move on to the compensated compactness framework of Murat and Citar. Of Murat and Sitar that has been developed by many people since then. So, here we have this differential operator linear homogeneous with constant coefficient. It turns out it's useful, at least notationally for me, to simply imagine that these A alpha coefficients, they are linear maps between two finite dimensional inner product spaces. And you can assume that they're real, though it's not important. And okay, the setup is, as I said on the first. up is as I said on the first UJ goes to U weakly A D of U J converted strongly to A D of U in W minus Kp remember A D was of order K and so here we need technical assumptions that this will do omega is bounded WKP extension domain and so guided by what we just talked about for the gradient what we need really to successfully decompose these guys is the key estimates so closed range of the Estimates. So, closed range of the corresponding operator. So, what is important here, and I stress that all the fields and all the norms are taken for functions on the whole of Rn. So, that's important. Now, as before, look at p equal to 2. So, p equal to 2, we can use orthogonal projection to realize this distance. And we can proceed simply using Fourier transform and the variational characterization of the projection. We can find an expression. An expression for this projection map. And that goes through the Free symbol of this differential operator. I skip all the i's here, so no i's here, skip those Euler symbols. And so what you what you notice is that A of ψ is a linear combination of these linear maps. So this simple map is indeed a linear map from V to W for each psi. And you can And you can calculate that this projection map is given in terms of this Fourier multiplier. And this Fourier multiplier you can express conveniently in terms of the Fourier symbol and the Moore-Penrose pseudo-inverse of this Fourier symbol. So that's the guy that you see here. This is the. That you see here. This is the Fourier or the Free multiplier. And using that and properties of the Moor Penrose pseudo-inverse, Andre and Bachman, they proved last year that the key estimate holds for P equals to 2 if and only if this operator A D has constant range. So what it means is that you can find a non-negative integer such that the rank of A of xi is equal to R for all psi is equal to r for all psi in r n except zero and so so you see the key estimate closed range for p equal to two at least is constant range but i believe that it is the same also for p different from two so and one can say some more here uh maybe i should just skip this this is uh following from uh from properties of Of mappings given by Fourier multipliers and Hermann dominant, you have the estimates here, and you also have a Hillem House-Hotz type decomposition in this context. So Renault is connected to a lot of work by many people, probably Schuenberger and Wilcox was the first one to realize an estimate of this form for p equal to two. And that was generalized by Sarason to others. Generalized by Sarason to other P and parto and so on. So Murat was also noticing these results in his study of combination practice. So the other assumption I want to put on A is the wave cone condition, or spanning wave cone condition. So you might remember the wave cone, that's the directions where this differential operator loses its ellipticity. So exactly defined as the union here of all these kernels. Union here of all these kernels. And so these are exactly the amplitudes along which you can construct a simple A3 oscillating sequence. So you probably know all this. So I'll just skip that. So my other assumption is the spanning wave code condition. I assume this. So the directions of the wave cone are the back directions. So in the orthogonal complement to the wave cone, this This operator is ellipsic and nice. So I'm just going to assume that the bad directions span the full space. Okay, then the last thing I want to say as a preamble is about potential operators. So there's a result by Bachtan Reitzer from 2018 that appeared in his PhD thesis. So if you take a differential operator of the form that we are looking at and you assume that it is And you assume that it is constant rank and that it has a spanning wave cone, then it is actually possible to construct a potential operator. So a homogeneous constant rank operator that I call here B D. This B D is not going to be unique. And even this space here is not unique. So U is an inner product and finite dimensional real space. But what you have is this exact next relation. next relation at the level of Fourier symbols. This is true for all ψ different from zero in In. In fact, the existence of such a differential operator B D is equivalent to the constant rank of this differential operator E D and also of B D itself. So it's intrinsically linked to the constant rank assumption. And so it's remarkable that it took so long to That it took so long to discover these results. And it relies on a very nice formula by Disel that Bogdan discovered existed in the algebra literature. So it's a formula for the pseudo-inverse of a linear map. So now the outcome of this is that you have exactness also at the level of function spaces. And so one statement of it is One statement of it is that the kernel of AT is the image of Bt, where I look at AT and BT as maps between these spaces set. So this is standard notation. Z is simply the SWAT space modular polynomials. And so essentially, you should think about if you have an A-free field, then it's B of something. So it's a Poincaré lemma. And so subsequently, there's been other constructions. There's a nice one by Arroyo. There's a nice one by Arroya Rabaza and Siemensal in 2021. It gives another construction. And then I have ongoing work with Bogdan and Andre and some people in Leipzig at the Max Planck Institute in Bernstormfield's group, where we construct the potentials more general. Turns out you can always construct the potential. It's just that sometimes it's not very useful. Sometimes it's not very useful. Okay, so now the decomposition results in the constant rank case. I hope it has been justified that we use these assumptions. So this is a joint result with Andre and Bachtan from last year. So we have a differential operator and assume constant rank and the spanning Kohn condition. The spanning Kohlm condition is just really for convenience here. And then I assume that I have a potential. Assume that I have a potential operator Bd. And then we have a perfect decomposition with all UJ converges weakly to U and L P and we have asymptotically the satisfaction of the P D constraints of the difference of UJ and U. So A D of U J converted strongly to A d of u in W minus Kp. And then the technical assumption that Uj generates a young measure of oscillation, then we can find sequence. Then we can find sequence Ej of small errors and GJ and Bj both in WLP and notice the boundary condition on GJ here, such that UJ admits the composition U plus Ej plus Vt of Bj plus Bt of Tj. And then we have all these properties that we would like the sequences to have. So we have here, you have all the concentration. All the concentration, and here you have all the oscillation, so it's nice to separate. Okay, so this generalizes results of von Secret and Müller, and I'm sure some other people as well. So now the main thing I wanted really to talk about, the case p equal to one, under the same assumptions of the differential operator. Operator. So we have uj in L1, we have uj converted with the star to a vector measure. And let's just say just for show here: a d of uj is zero. Can we decompose? Nope, we cannot decompose. That's complete failure. Certainly not in the same form as for p larger than one. That's by Ernstein's non-inequality. But actually, not at all. And that's the result by Alberti from 1997. So Giovanni Alberti. So Giovanni Alberti proves that given any vector field of class say L1 on omega, remember omega is a subset of Rn, then I can find a PV function, even a S P V function on omega, such that its approximate gradient, so that's this guy here, is equal to V almost everywhere. And at the same time, I also have a bound on the total variation of the distributional derivative, constant times the L1 norm of V. So here I explain a bit. So here I explain a bit about my notation. So I use dv, that's the distributional gradient, that's the measure in this case, and I do the Lebesgue decomposition. I get the approximate gradient. That's actually a result by Caderon, that it's the approximate gradient. And then I have a single part. And the offshot of this is that the oscillation is pretty arbitrary. So if you give any parametrized If you give any parametrized measure, so meaning here, so each new x is a probability measure on the space of matrices. I emphasize I need to have matrices now, that satisfy this moment condition. So then this is a young measure of oscillation of some sequence of maps that is pretty arbitrary. Then by Alberti, we can find sequence of W1. Can find the sequence of W11 maps on omega such that uj converged with d star in pv and the gradients of these because these are w11 maps these approximate gradients also distributional gradients that generate the oscillation young measure ux so so you see let me just go up here and emphasize that because i didn't this guy here on the left is obviously curl free this one is not curl free Is not curl-free, not at all, and it cannot be. So, there is absolutely no gradient structure in UX. Now, this kind of constructions can be generalized, and that's something that is being done by Tommaso Senecci, who is based in Oxford. And he's generalizing this to differential operators and also to other measures. It turns out we can do oscillational aspect. Out, you can do oscillation with respect to other measures, you have the same kind of results. Okay, so what one needs to do is then to combine Young measures for oscillation, the Young measures for concentration. And that has been done following works by Diperna and Maitre. And the form that we want to use it in is in the Aliberian Pussici form. And so probably my time is running out, so I'll just assume that you know generalized Young methods. If you don't, doesn't matter probably. If you don't, it doesn't matter, probably. So, there's a lot of references on this. So, we need to take into account oscillation and concentration simultaneously when we discuss this V-star convergence of L1 fields that are PD constraint. And so now I'm stating a special case of our results with Bogdan-Reitzer from 2019. We have a constant rank differential operator. A constant rank differential operator of the kind we have been discussing here that is also satisfying the spanning wave cone condition. And I take Bd to be a potential operator. Now, what I want to do is I want to characterize all these generalized young measures that can be generated by A free maps. So nu can be generated by A free maps. So I can find a sequence UJ that are A. That are A free, so satisfying the A t u j equals to zero, and converting weak star in the sense of measures. I claim that is the case if and only if the following three conditions are satisfied. The two first conditions are just the technical conditions. So, this new bar is really the same as you. That's the Baryst for the Young measure. That's the limit. And for the young measure, that's the limit. The other guy here is the moment condition. So, what you should look at is condition three. And condition three is a Jensen-type inequality. And you see how it combines the oscillation Young measure with the concentration Young measure, but it only does it in the bulge, meaning almost everywhere with respect to the back measure. There's of course also something happening outside of that because we are really looking at general measures here. So in a back null sense, but it doesn't matter. So, in a back-null sense, but it doesn't matter. There, there is no condition whatsoever. So, we only have a condition on the bar. And Jensen-type inequality here should be satisfied for all A quasi-convex functions f of linear closure. If you haven't seen this before, it doesn't matter. It's just a generalized notion of convexity that we need to satisfy. And it's more general than convex functions. So, this is really a non-trivial condition. And furthermore, you can show that if you have these conditions. That if you have these conditions, you can find a nice generating sequence. And so that's what I'm saying in terms of the potential here. You can take and generate the oscillation and concentration using such nice BTF UJ. Okay, this is continuing work of many, many, many people. So I'm probably missing out some people. So, I'm probably missing out some people here. I want to emphasize probably this one here because Arroyo Abaza also proved the same kind of results more or less simultaneously, I believe. So, his paper is actually appearing online already, whereas ours is still being reviewed. So, I think that our proof has some interest. Still, I mean, our proof is less than 20 pages long, and the proof that is online of Arroyo Abasa is 90 pages long. So there's still something probably to be deduced from this. You should also say that the absolutely continuous part of our result was something that we had done in 2018 and that can be found in Bogdan-Reitzer's PhD thesis. So it's just really that it's extended. So, it's just really that it's extended to the singular part, and that took a little bit longer than it should. Now, so also in our paper is a discussion about oscillation versus concentration. And this is where I would like to probably soon end my talk. So, just to get back to where I started, I wanted to say this about the characterization of these generalized Young measures. To kind of motivate the last party. And this is really the main result. So you have a sequence of A3 fields, capital UJ, they convert with the star in the sense of measures and generates one of these generalized Young measures. Okay, so we know that the Young measures have to satisfy this Jensen-type inequality in the bulk. So we have this. So we have this. So I agree with you. Some of you think that this is a little bit imprecise. It is true, but I need to say that the set of A quasi-convex functions of linear growth, there is some separability going on here. So it's actually a countable number of conditions really that you need to check for star. So therefore, this almost everywhere is well defined. Now, you notice that. Now, you notice that the Young measure has an oscillation part and a concentration part. This is the oscillation part. There's no concentration. And this is the oscillation part. There's no oscillation. And from this result of Giovanni Alberti, we can say that the oscillation has no A-free structure. So I'm surprised that the concentration part does always have A free structure. I always thought it would not be like that. So this is the main result. It worked on Reizer also 2019. So we can find a sequence of test maps. So given this generalized young measure that is generated by the J phi fields, such that phi j converges strongly to zero in WL minus one, one. So L1. L minus 1, 1. So L was the order of the potential operator. And so that we have this generation here. So it looks awful. I'm aware of that. But it's simply saying that all the oscillation and the concentration, there's also concentration in the first term, but all the essential oscillation and concentration is really generated by A3 phase. So of course, this guy here, this guy here need not be A3, but I could... You need not be A free, but I could just make it A free by adding something to it. So this is surprising because I always thought when you go back, I'll take you back here. I think I will be very brief on the explanation. I had in mind that I would be longer. I'll explain it a little bit more. So you see, there's a combination of oscillation and concentration. What I'm saying here is that the oscillation is completely unrestricted. So in order for this to be true. This. So, in order for this to be true for all A quasi-convex integrands, F, it's clear that the concentration has to compensate the non-A-freeness in the oscillation. And I thought that then it must be true also the other way around, that if you have very nice oscillation, then the concentration could be very bad. But that's not true. The concentration is always nice. It's possible to extract. Possible to extract this concentration part. So I will do it very fast. So, Andrea, do I have two minutes? Yeah, just two minutes, please. Two minutes, yeah, yeah. So, so, I mean, because there's something new here. So, so if you fix any a quasi-convex integrant of linear growth, then technical, so because the spanning condition is Lipschitz, and so it's differentiable almost everywhere. Let's fix. And so it's differentiable almost everywhere. Let's fix a point where if it's differentiable. Then you look at these auxiliary guys, t-sets kind of a difference quotient, right? And so what you notice is that G is A quasi-convex of linear growth, and it has the same recession integrand as F. So G infinity is F infinity. So I can use T in star. If I do that, and I'm aware that this is very hard to follow probably, I just plug in T in star. And I notice that this. and I noticed that this have the same recession function. So that's why I can write f infinity in the second term on the left-hand side here. And so you have this for almost every x. Then you take epsilon to zero, because g is differentiable at z naught, or sorry, f is differentiable at z naught, we know we converge to this directional derivative, f dash z naught dot z. I plug that in and what I get is this. And okay, so notice F infinity was G infinity, so that didn't change, but everything else changes. Now this is linear and this is linear. So you cancel out terms, you assume that there is some concentration at the point you're looking at. So that is d lambda dx, the rather negative derivative is positive, and you end up with the inequality that I have stated here. Now, z0 was any point where f was differentiable. So now you take the supremum of all such z naught. The supremum of all such sit nodes, and you get the inequality that I boxed here. And now on the left-hand side, or on the right-hand side, the integrand that pops up is this gf. So this gf, that is simply the support function for the essential range of the derivative of f. It happens that it's also the support function of the Clark sub-differential of f. And we can show, it's not very difficult, but I don't have time. It's not very difficult, but I don't have time to do that. The Tf is always large or equal to the recession integrand of F, and you have equality on the rank one cone or on the wave cone. And you have that F is convex exactly. If F is convex, you have the T F is exactly the recession integrant. So we call this the convex deficiency integrant. And now this condition turns out to characterize the concentration angle. Concentration or angle concentration angle measures that come up in this context of A-free fields. And so one needs to invoke the characterization, the argument for the characterization result, which is not constructive, which is a Hanbanag argument in the spirit of Kinterle and Petrogram, as they did many years ago. Okay, I'm sorry for rushing through the last bit here. So this is our. bit here. So this is our terminology. The convex deficiency integrance is simply responsible for restricting the concentration angle measures. And you have what I find surprising that the concentration, even for the case of gradients, this result was not known, that the concentration is always gradient-like in that case. Gradient-like in that case, whereas the oscillation is not, not at all. Okay, thank you for your attention. So I finish here. Thanks, Jan. Thanks a lot. There's time for some short question or comment. I'm aware that this is slightly outside. Outside the pattern of some of the other talks. I mean, there's this, you see, there's the potential operators, and then there's this thing about the weak convergence versus strong convergence that is certainly related to the regularity. Sorry. Now, there was some link to the previous talk by Rupert about this. Yeah, exactly. So it's important for me to stress here that. To stress here that I only work on bounded domains, there are additional features in the unbounded case. So that's yeah. Good. Yeah, this is in fact the same problem of concentration that also appears in the previous talk, in fact. I mean, because yeah, no, it is, but I mean, there is some some, I mean, there is some issue. I mean, the fact that you have these symmetry groups and These symmetry groups, and that adds something. Okay, so that's, I mean, my differential operators are more general and so on, but we do not have these. That makes a difference. So that's also why I don't think young measures would be very useful for discussing the other problems. One thing. Can I, yeah, I couldn't hear? No, neither can I. Is anybody speaking? Can I feel you use, and also you that you mentioned at the beginning of the composition? Yeah, yeah. I was saying that this is quite bad. I simply cannot understand what is the beat. Rosario, are you are you speaking now? Are also relying on okay, I don't listen, I stop speaking and Okay, I don't listen. I stopped speaking. No, the connection is very bad. I cannot understand. Yeah, I cannot understand what you are. I mean, okay, maybe you can put it in the chat and then I will have a look there if there's something. Okay, I stay quiet. It's better for everybody. Okay, so if there is no other question, let us thank Jan again. And so there And so there is a break now, and we shall resume at nine twenty-five math time. See you soon. Okay, yeah, bye. Bye-bye.                         