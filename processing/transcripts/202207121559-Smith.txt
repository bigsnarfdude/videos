About some work that was done with a couple of undergraduate students, Sultan Aitam and Sambat Pandari, both have since graduated. Sultan is now a grad student at Drexel. So, I'll talk about some ideas that Beatrice Poloni introduced in the first part of her talk this morning. Morning. And these ideas were first appeared in these series of three papers around when we met in San Jose. But these ideas have evolved somewhat and been generalized since then. So I'll be talking mostly about this more recent long paper. So let's talk about some motivation first, and I apologize. First, and I apologize, this motivation comes from self-agilant problems, but I promise you we'll get to non-self-agilent problems as soon as the motivation is finished. So, if we think about the full line problem of the heat equation with rapidly decaying data, then of course we can just solve this problem by using the exponential Fourier transform. So, what we do is we apply this exponential Fourier transform to the PDE, and then we observe that the And then we observe that this transform diagonalizes the spatial differential operator. It turns the second derivative into multiplication by minus lambda squared. And so then that gives us an ODE in time, which we can solve really easily. And then we apply the inverse transform to recover the solution of the original problem. So the reason this method works is The reason this method works is because the exponential Fourier transform has two crucial properties. One, it diagonalizes the differential operator, and two, it's got an inverse. Okay? Well, what happens if we try the half-line Diracle heat problem, or the homogeneous Diraclet condition at zero? Well, then we've got a problem if we try to use the exponential Fourier transform because that no longer diagonalizes the operator. We now have Operator, we now have some boundary term at zero. So, well, we realize we want to use a different transform, so we use the Fourier sign transform instead. And that deals with that boundary term at zero, and again we get a diagonalization. So then, of course, this Fourier sign transform has its own inverse, so we can solve the ODE and then apply the inverse transform, and we've got a solution of the original problem. A solution of the original problem. Again, this was using two properties of this slightly different transform. It used the diagonalization property and it used the inversibility of the transform. All right. Well, what about the finite interval Dirichlet heat problem? Again, with homogeneous Dirichlet conditions. Well, again, you can't use the Fourier sign transform because we end up with some boundary term. So we use the Fourier sign. We use the Fourier sine series transform instead, or as it's better known, the Fourier series. All right, so this one diagonalizes the differential operator again, so we can get an ODE which we can solve, and then we can apply the inverse transform and solve the original problem. Okay, so this is slightly different because the inverse transform is now a discrete series. Is now a discrete series instead of an integral. But morally, the same thing is going on. Okay? So I'd just like to point out one thing for later. So I want to think of this Fourier sign series transform applied to phi at, I'm going to write it as lambda over pi instead of j, just for consistency with later notation. So I want to think of this as an inner product. Think of this as an inner product of phi against, let's see, it's twice sine of lambda bar x for the so that's collinear in the product, right? But now this thing this thing is that sorry, this thing belongs in the span of e to the i. e to the i lambda bar x, e to the minus i lambda bar x. And this is the this is the eigenspace of lambda bar squared, the eigenvalue of the formal differential operator. Of the formal differential operator minus second derivative. Okay? So this is an inner product against something that lives in this formal eigenspace. Which thing? Well, we've picked the one so that it's actually not just a. We've picked a particular linear combination of these two things so that it's actually an eigenfunction of. It's actually an eigenfunction of the full differential operator, not just the formal differential operator. And of course, we then have to be a little bit more careful about which lambda we're picking as well. Okay, so I'm doing all of this in terms of lambda bar because I'm really thinking of this operator as the adjoint of this differential operator. It doesn't matter here because it's self-adjoint, but it will matter later. Okay, so what have we seen in these motivating problems? Well, to solve all of these initial boundary value problems, we had to use two properties of our spatial transform. We used that the transform diagonalizes the spatial differential operator, and we used that the transform is invertible so that we can recover the solution idea. So the question arises. So the question arises: how do you find the right transform for a given initial boundary value problem? And this is something that Beatrice highlighted earlier. It's not necessarily so easy to do it as soon as we go too far from these kinds of examples. Well, the classical answer is, of course, you use separation of variables to derive the right transform pair. The problem with this approach is that it relies on this implicit assumption that the eigenvalue. Implicit assumption that the eigenfunctions form a basis, or at least are complete, in the space of initial data. And I'm not worried about this in the sense of like, oh, maybe we don't know it's complete. I'm worried about this in the sense of we know it's not complete in a lot of examples we want to study. Okay? Beatrice gave some examples of that. For example, these third-order operators where you have these Dirichlet conditions at each end. Dirichlet conditions at each end, and then one Neumann condition at one end. That's an example where the eigenfunctions don't form a complete system. Okay, so the modern approach is to use the focus transform method. So the idea here is you go through some long process, which I'm going to give even less detail than Beatrice did, in which you derive. In which you derive the transform pair, and at the same time, you've actually solved the equation, you've produced an expression for the solution. You can sort of recognize the transform pair out of that solution representation that you've got. Okay, so this is a field that's been going since, well, in the linear setting has been going since arguably 94, but I think. Well, arguably 94, but I think really since 2000. And there are many works by Focas, Poloni, Dionysus Mantovinos, Bernardo Conning, Tom Trogden, Michelle Passan, Nathaniel Shields, and collaborators of theirs. Okay, so if you do this, then the inverse transform is a contour integral. Even if we're working on a finite integral, we still get a contour integral as our inverse transform, which is perhaps not. Which is like perhaps not so surprising because I'm claiming that this is going to work even when you have these incomplete eigenfunctions. So you probably need some extra spectral information somehow. Contour integral gives you that. Okay. So a plan for the talk. Section one is to describe a more efficient derivation of a focus transform. Efficient derivation of a focus transform pair, so we don't have to go through this classical approach, or not, sorry, the classical approach, the usual approach for the Focus Transform method of solving the problem as we derive the transform pair. The second section is to describe exactly how the Fogress transform pair diagonalizes the spatial differential operator. We saw how the diagonalization worked in those. Saw how the diagonalization worked in those motivating examples. I want to say how it works in this more general setting. Section three: solve the initial boundary value problem using that focus transform pair. If we're deriving it separately, then we'll need to use it to solve the problem. And section four is to generalize this to some interface problems as well. Yeah, we're doing section three first. So here's So here's the problem I want to study. The differential operator is a constant coefficient described by a monic omega. So this was omega is what Beatrice called IP. Okay, so I rotate it. And it's got some, the domain is equipped with some boundary conditions. And these boundary conditions, I want them. These boundary conditions, I want them to be fully general. So I'm not asking for separated boundary conditions, I'm asking for any n linear boundary conditions where n is the degree of the spatial differential operator. I mean, I guess I want them to be linearly independent, so I really have n of them, but no other restrictions. And yeah, so there's non-self-adjunctness in here, of course, but there's also non-self-adjunctness. Course, but there's also non-self-adjunctness in the boundary conditions. And well, to me, at least, the interesting non-self-adjointness comes in from these boundary conditions. So, that's what I'm going to be focusing on really. Okay, well, turn this into an initial boundary value problem by saying we're asking for an homogeneous boundary condition specified by these boundary forms. We don't need homogeneity here, but keep it simple. Here, but keep it simple by keeping this homogeneous. And again, we don't need homogeneity here, but I will keep it for this talk. So the problem is really specified just by the initial datum, capital Q, and by the temporal coefficient, little A. A is a complex number with some restrictions. Okay, so let's suppose that magically we've got a transform pair that is right for this problem in the sense that This problem in the sense that, well, it's got an inverse, and we know the inverse as well, and it diagonalizes the differential operator, exactly as we would hope. Okay? Well, then you apply this transform, by the forward transform, I should say, to the PDE, and here we have now, we've diagonalized it, so we've got an ODE. So we can solve this ODE. Solve this ODE is the solution, and then we can apply the inverse transform to get back our solution of the original problem. So it's exactly the same method as we used in the previous examples. Now, I described this as best scenario. I should probably have described it as unrealistic dream example, because in fact this is false. We don't have this diagonalization. Have this diagonalization. So, what really happens? Well, we still have invertibility of our transform. We've got a transform and an inverse transform there. But we don't have diagonalization in this sense. Rather, we have diagonalization up to some remainder. Okay, so I'm going to think of this, I'm going to describe this in a slightly different way from how Beatrice described it earlier. There's this remainder transform I'm thinking of. And, well, if we were thinking in terms of like Well, if we were thinking in terms of like pseudospectra, then I'd want to control the size of that remainder, right? I'm not going to do it, I'm not going to have that kind of control. Instead, I'm going to say let's take this remainder transform and apply it in the spatial variable to some function of space and time, and then multiply by this exponential kernel that features the eigenvalue, rounded to the n, and then integrate in time. And now apply the inverse transform to that, and we get zero. So this integral is in the kernel of the inverse transform. All right, well, then we can use exactly this diagonalization formula after applying the forward transform to the PDE, and here we go. Well, now this is not such a simple idea, but it is. But it is still day one of an undergraduate ODEs class, and here's the solution. So we just have this result of integrating the inhomogeneity here, this remainder transform appears inside this integral. So then let's apply the inverse transform to this equation, and we get on the left the solution, and on the right, okay, it's this formula, which is kind of what we were expecting. And then We were expecting, and then we get this thing, which is exactly what we say is zero. Okay? So, what I'm claiming here really is that if the transform obeys this kind of diagonalization, then that is exactly what you need in order to implement the same solution method for the initial boundary value problem. Because that problem bit that you don't know what it is, right, it's in terms of the remainder transform applied to the solution of the problem. Transform applied to the solution of the problem, it's going to disappear. Alright, we do need invertibility of the transform, of course. But we don't have true diagonalization. It's enough to have just this kind of focast diagonalization, if you will. If we do that, then we get our transform method working. Alright. So So, yeah, section two now. So, how does this diagonalization work exactly? This is a precise statement of the theorem that I just sort of informally described. So, we say that, well, this is exactly the equation I told you before with this Remainder transform. And what we're doing is we're thinking of a function of space and time. And I want to have it being smooth enough in space and time. And it turns out. And it turns out that this is enough. I'm certainly not claiming this is optimal. This is just enough that I could easily prove the theorem. If you make some more careful estimates, I'm sure you can improve on this, work with rougher functions. And certainly numerically, this method works with much, much less smoothness than I'm describing here. Well then, if your Q obeys that, then this integral is in the This integral is in the kernel of the inverse transform. Okay, so that's the diagonalization result. How is it done? Well, the idea is to think about this remainder transform applied to a particular function, and you see that it's analytic, at least outside some disk, and it has certain decay as lambda goes to infinity. I apologize. It blows up only so fast as lambda goes. It blows up only so fast as lambda goes to infinity. But then we can integrate by parts in this integral, right? And an a lambda to the n comes down into the denominator, and overall we get dec. So then we can use Jordan's lemma and Cauchy's theorem to close up some contours to make contour deformations. And this inverse transform, which is given in terms of contours, as Beatrice was describing earlier, evaluates. Describing earlier, evaluates to evaluates to zero. Okay, that's a sketchbook. Alright, so I should have said, if we want to do this, then of course we need a formula for the remainder transform applied to a given function. And for that, we need a formula for the Ford transform. Fortunately, we have that. Okay, so these. Let's talk about how we actually define the transform inverse transform pair without having to go through the usual Fokes transform method. Okay, so the first step is to construct a function which is kind of dealing with the lower order terms in the spatial differential operator. Spatial differential operator. So I think of this function as like letting me pretend that the eigenvalues are just lambda to the n instead of omega of lambda to the n. Remember, omega here, that's the wrong button. Very much the wrong button. There we go. Remember, omega here is the character of, is the character of the differential operator. Okay, so this is not a new powerful theorem, but we've produced an elementary proof that at least I hadn't seen before. Okay. So then what do we do? Well, we can think first of a formal transform pair. So this is not the Focas transform pair, this is just a step in that direction. And we're going to think of it as the inner product. It is like the inner product of our function with this exponential function. Here's what it looks like. So this is pretty similar to the Fourier transform, right? Except there's been a change of variables because we've got this nu of lambda instead of lambda. Nu was that this function, which is producing this map. In this map. And here's the inverse transform. And the integral here is basically just the real line, except because mu was not analytic or even really defined everywhere, you have to just perturb away from the real line just outside some disk. All right, well, the theorem that this transform pair is valid is entirely trivial because, of course, you do a change of variables and it's just Of course, you do a change of variables, and it's just the Fourier transform, right? And the inverse Fourier transform. But it's like formulaically useful for what we'll do next. So I think of this slide in the last slide as dealing with the lower order terms in the spatial differential operator. The rest of it is going to be dealing with the non-self-adjointness that comes from the boundary conditions. All right, so what is the forward transform of the focus transform? Focus transform pair properly. Well, alpha here is just like Beatrice had a primitive nth root of unity. Okay, so what we're going to do is we're going to think about these exponential functions. Now these form a, if you take these for j as 1 up to n, then they span the eigenvalue. Eigenspace associated with lambda bar to the n of the formal differential operator that is the adjoint of L. So this is very much like this setting, right? We're again going to take certain linear combinations of them, just like we did here. But in this setting, we took a But in this setting, we took a very special linear combination that allowed us to produce genuine eigenfunctions of the adjoint differential operator. Here, this is too much to hope for. Well, I mean, we could do that, right? But that wouldn't be, it's too much to hope that that is enough because the eigenfunctions are not generally going to be complete. So, we're going to take certain linear combinations, and I do mean certain ones, they are defined in the paper explicitly, it just doesn't fit on the slide. It just doesn't fit on the slide. In terms of Birkhoff's characteristic determinant from his 1908 paper, for the classical adjoint of the differential operator L. So this is what I mean by that. Okay. So what about the inverse transform? Well, Beatrice gave you some pictures of what the contours look like here. So I won't try and give that in any detail. Try and give that in any detail, but basically, the contours are essentially what Beatrice showed. And you have to be careful about making perturbations away from any possible eigenvalues, or roots of eigenvalues, I guess. But you can do that and you can define explicitly how it works. Okay, so the theorem is that this is a genuine transform, inverse transform. Transform, inverse transform. And the proof works essentially because we chose the contours in a special way so that a lot of components of the contour integral gamma cancel out, and you're left with just that formal transform, inverse transform, applied to the original function. Well, this works under a certain assumption. This works under a certain assumption. And I haven't given this explicitly, but this assumption, it's known that in certain cases it's equivalent to well-posedness of the initial boundary value problem. And there are no examples of well-posed initial boundary value problems where this assumption doesn't apply. So I kind of think of this assumption as something I don't fully understand yet, but I want to understand, and I feel like it is. Understand, and I feel like it is a characterization of something interesting, probably well posed this, rather than I'm scared that it's going to be a big problem. Okay, so what have we seen? Well, we have derived the Focus transform pair, or defined even the transform pair, just by Just by having the construction of the classical adjoint. So, if you can do that, then we've got a formula for the transform pair. Again, in the paper, I didn't give you the formula explicitly. So, well, that process can be done algorithmically. It's slightly obfuscated in Connington-Levinson's theory of ordinary differential equations work. And it's been implemented in Julia by another. By an undergrad and another consultant, who's a co-author of this paper, re-implemented in a prox function. So it now runs very quickly. Okay, I will very quickly advertise the interface problems, if that's okay. So interface problems, I'm including the ideas that were, some of the problems that were discussed earlier, although with constant coefficients. We think of the spatial differential operator as now like a list of differential operators on each interval, but the boundary conditions are now allowed to couple in any way we like between the boundaries of all of these intervals. Of course, you could just put these intervals next to each other in a linear graph and set up smoothness at each interface with your boundary conditions and then. And then you would have effectively a piecewise constant coefficient differential operator. So these problems can be solved in a few different ways, but there are some examples where the new approach is valuable. Again, it essentially comes down to constructing the adjoint of this interface differential operator, and this was something that my undergraduate collaborators. My undergraduate collaborators did essentially without any supervision. And it was then implemented also. So that gives you the definition of the transform pair. Essentially, you can guess it from how it looked in the single interval case, and then proof that it works. And then you get a similar diagonalization result, and you can apply the same method as before. To conclude, very quickly, if I'm To conclude very quickly, if I may, the focus diagonalization is not diagonalization in the classical sense. It is diagonalization with a remainder, and that remainder, when you take an appropriate integral of it, lies in the kernel of the inverse transform. That is the diagonalization that we have, and that is exactly the diagonalization that we need to solve the initial hydropathic problem. We've also got a quicker way to derive the transform inverse, the focus transform-inverse-transform pair, and all of this works for interface problems. Um and all of this works great to face problems too. Thanks very much. Thank you. Are there any questions, comments? So you have net from C1 to somewhere. I have two questions. Is that somewhere C1 as well? And do you have boundedness of the F? Ideally, L2. So, is it a a properly invertible operator with F in a certain bar space setup? Or is it C1 algebraically invertible? Yeah, it is algebraically invertible. We didn't check this properly. Yeah. Thank you. Question? Further questions? Are there problems that you cannot even attack with the forecast or stuff? Well, we don't know how to do anything with variable coefficients yet. I mean, work is ongoing, but we don't have a result. So yeah, that's a huge class. In multiple spatial dimensions, the results are Well, I feel they're much less satisfactory than what we have in one spatial dimension. I think there's a lot more that can be done that hasn't been done yet. With your interface problem approach, maybe at least you could handle piecewise constant coefficients, young or something. Absolutely, yeah. So we can encode pieces. So yeah, so we can encode piecewise constant coefficient progress in this. So that is the same thing. And there is a new preprint, I think it's on the archive. I think it's there now. By DeConink and one of his students where they actually take a minute and produce some results for variable coefficients that way. Yeah. So that is work that's on. Work that's ongoing in that approach. There are some other approaches we're trying for different ways of doing variable coefficients. You just said you can put this into a line, but you can also consider a graph. Absolutely. Any graph you want, you take whatever linear combination of the boundary values you want. The formula here. The formula here, no, but it works. Okay. Let's thank the speaker again. We will have clean open problems. 