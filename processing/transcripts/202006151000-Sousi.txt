Yes, so uh sorry. So, welcome everyone for another week of oops. So, we are happy to have with us today Perla Susi from Cambridge, who will tell us about mixing and heating times for Markov chain. And next week, we have a course by Frank den Hollander, so all details are on the website. So a reminder, we are tail video capturing this and we'll be posting the video And we'll be posting the video later. So, if you do not wish to appear in the videos, please turn off your video and mute your microphone. So, throughout the lecture, we will keep participant microphones muted, but if you have any questions, please ask them on the chat and one of the moderators will forward them to Perla. So, we do welcome discussion. At some point in the middle, we will have a short break, and at the end of the hour, there will be some additional time for discussion. Some additional time for discussions that will not be recorded. So, with that, I'll hand it over to Bella to start. Pella, if you could share your screen. Yeah, but I think you should share my iPad, spotlight the iPad. I'll spotlight you, in fact, but when you share the iPad screen, then we'll have the screen shared and you and The screen shared, and you and the your video sort of pinned in the corner. Okay. Okay, great. Okay, so you can all see my screen? Yes. Okay, perfect. Okay, so thanks a lot. So I'm going to give a mini course on mixing and heating tips for. On mixing and heating times for market chains. So, first of all, I would like to give a quick overview of what I'm going to cover. Oops, sorry. Okay, so I'm going to cover results from three different papers. So, the first one is going to be equivalence. Up to constants between mixing times and heating times of large sets. The second one. The second one is going to be just about heating times. And I'm going to discuss a result which is about a comparison when comparison for different sizes of sets. Of sets. And the third topic I'll discuss is going to be refined mixing and heating equivalences. So one thing I want to emphasize is that all of these topics, of course, are related to each other. Of course, they are related to each other. However, even if you miss something in one proof, it's not going to be used later, most likely. Similar ideas will be useful, but the proofs will be kind of independent. Okay, so also on the website, I posted a two-page note with some background that it would be helpful for people to be familiar with. So, like basics for total variation distance and Distance and how one can upper bound total variation distance by capping. So now I'm just going to set up some notations. I will recall some things from this two-page note, but the main reason is just to set up the notation to use it throughout the course. Okay, so I'm going to be talking about Markov chains. So X is going to be an irreducible Markov chain. And the Markov changes that I will be considering will always take values in finite state spaces. Let's call it S and let P be the transition matrix. Of x. And I'm going to be writing p t i j for the probability starting from i that xt is equal to j. There is a bit of delay for all ij in the state space S. So because the Markov chain is irreducible, which means that you can go from Which means that you can go from any state to any other state in a finite number of steps with positive probability. And it takes values in a finite state space. There exists a unique invariant distribution. So pi is going to be the invariant distribution, which means that pi is equal to pi p. And if x is also predicted. X is also periodic, then it's classical that the transition probability between p t x y converges to pi of y as t goes to infinity, and this is true for all x and y. So, what the mixing time tells us, it tells us about the rate of this convergence. Rate of this convergence. Okay, so the mixing time tells us about the rate of this convergence. But in order to talk about the rate of convergence, we have to define the right notion of distance that we'll be considering. So the distance that I will be using is going to be the total variation distance. So let me recall the definition. So suppose that we have So suppose that we have μ and μ be two probability distributions on the set S, then the total variation distance is defined to be the maximum over all subsets A of S of the absolute value of mu of A minus mu of A. And I'm And I'm going to write d for the total variation distance from stationarity between the matrix p to the t when I take the word starting state. So it's going to be the maximum overall x of p dx dot minus y. And I'm going to define now for any epsilon in 0, 1. 0, 1 the mixing time T mix of epsilon is going to be the first time that the total variation distance from stationarity taken over the world starting state drops below epsilon. Okay, and in this course I will be writing T mix when epsilon is equal to a quarter. When epsilon is equal to a quarter. So T mix is going to stand for T mix of a quarter. Why we choose a quarter is arbitrary. So quarter is arbitrary. All we need is for epsilon to be strictly smaller than a half, because after a half, the total variation distance from stationarity decays exponentially fast. Okay, so that's the mixing time. That's the mixing time. And let me also recall the definition of reversibility. So X is called reversible. So the definition is that X is reversible if when started from stationarity, whether it runs forwards or backwards in time, we can't tell them apart. Them apart. So running the market chain backwards or forwards in time, the two look statistically indistinguishable. And this condition is equivalent to saying that for all x and y, pi x times pxy is equal to pi y pyx. These are called the detailed balance equations. So in this course, I'll mostly be talking about reversible cases. Reversible cases, irreversible chains, and in some cases, I'm going to also discuss non-reversible chains. So these are the main definitions I wanted to write down. And before I write the first theorem, let me also define this quantity that I will denote by th to be the maximum. Alpha to be the maximum over all starting states X and all sets A of measure at least alpha of the expectation starting from X to hit A where tau A is the first time that the Markov chain hits the set A. Okay. Okay. So now I can state the first equivalence that I talked about earlier, which is a theorem proved independently by Roberto Iveira and Percy myself, and both of them in 2000. Both of them in 2012. So actually, let me also add one more definition here because I'm going to use it for the theorem. So here I wrote about that if x is also periodic, then we have convergence to equilibrium as time goes to infinity starting from any starting state. But sometimes the change that one considers are not a periodic, so there is an issue. But in order to overcome this issue, so in order to overcome periodicity and near-periodicity issues, we will always consider the lazy version of a chain. So in my talk, I'm always going to be considering the lazy version, even if I don't mention it sometimes. So what do I mean by the lazy version? Lazy version of X. I mean that every whenever x is at a state x, it stays there for, sorry, at the next step, it either stays in place with probability half or with probability half, it chooses a state to jump to according to the transition matrix P. So the transition matrix P of the lazy version is going to be the original plus the identity divided by two. Divided by two. Okay, so the first theorem is the following. So for all alpha strictly less than a half, there exist positive constants C alpha and C alpha prime such that for All reversible lazy mark of chains. We have that C alpha times TH of alpha is upper bounded by T this is upper bounded by C alpha prime times TH of alpha. Okay, so let me go over the theorem. So, what the theorem states is that for any fixed alpha, which is strictly less than a half, I'll talk about the half in a little bit, there exists positive constants C alpha and C alpha prime, so these positive constants are universal for all reversible finite mark of change that we also make them lazy. So that the mixing time, so T-mix of a mixing time so T mix of a quarter is upper bounded by one constant times the maximum heating time of sets of size at least alpha and lower bounded by the other constant times the same quantity. Okay so sometimes I will be writing instead of writing that there exist positive constant C alpha and C alpha prime I will also write T mix up to constant. mix up to constants th of alpha where up to constants is going to mean depending on the context it's going to be that there exists universal constants so that for all mark of change or if it is for all values of n the constants are the same for all values of n so that t mix is upper bound by one constant times the other quantity and lower bounded by the corresponding thing so and here Thing. So, and here these constants also depend on alpha. So, in this theorem, because of the way we proved it, the constants that we obtained, C alpha and C alpha prime, exploded as alpha approached a half. But the theorem is still true even if alpha is equal to half, and I'm going to discuss this in the next lecture. Okay, so what my plan for today is My plan for today is to prove this theorem, modulo some technical results that I'm not going to prove. But before we prove it, I would like to discuss a little bit about the theorem and the conditions and also some previous results. So the first thing I want to say is that the lower bound in this theorem, that the mixing time is at least the maximum heating time of sets of some substantial size alpha. Substantial size alpha, this is the easy direction of the theorem. And intuitively, it's clear that this should be the case because if we know that if there is a big set and we know that we haven't hit it, then clearly we cannot be mixed. The other bound, that the mixing time is upper bounded by this quantity, so that if you look at the expect time to hit large sets, then by this time you must be mixed. Time you must be mixed. So, this is a non-trivial direction, and this is where all of the work goes. So, let me first, maybe first now, let me present the proof of the lower bound. Okay, so let me just Okay, so let me just write that T mix is greater than or equal to C alpha times T oops T H of alpha. So I'm going to take alpha to be equal to an eighth, both for the lower bound and for the upper bound. It's just a choice to make things, to make the calculations easier. Okay, so let's take t to be equal to the mixing time of precision one over 16. Of precision one over sixteen. Then the first claim is that this is upper bounded by three times the mixing time of a quarter. So this follows easily using the almost sub-multiplicativity of the function d of t. So I did write this sub-multiplicativity property for what I defined as d-bar in my two-page. Defined as D Bah in my two-page note. And then using this, one can easily show that D of D is almost multiplicative, we lose a factor of two. And so then it's easy to show that T mix of one over 16 is upper bounded by three times T mix. Okay, so since T is equal to T mix of 1 over 16, it means that at this time we are mixed. So this means that for all X and all A, P T. Ptxa is greater than or equal to pi of A minus 1 over 16. This is the definition of the mixing time of 1 over 16. And so if we now take A with pi of A greater than or equal to 1 eighth, then we get that P dxA is greater than or equal to 1 over 16. Greater than or equal to 1 over 16, and this is true for all x. So this is good news because now starting from x, we have probability 1 over 16 of being in A after time t, at least 1 over 16 of being in A after time t. Now if we are not, if we failed, then because this lower bound is uniform over all starting states x, we're going to use it again, we're going to perform an independent experiment and check whether we are in A after T steps. Check whether we are in A after T steps. So this means that we can upper bound tau A, the first hitting time of A, by T times the geometric of parameter 1 over 16. And so this now implies that the expectation starting from any x, so let me just write max overall x of tau A is upper bounded by 16t. And so this is the end of the program. And so, this is the end of the proof. So, that's the lower bound, which is the easy direction. So, before I move to the proof of the upper bound, I would like to mention a previous result due to ALDUS. So, Aldus in 1982 showed the following. Again, the assumption is that for all Is that for all reversible lazy Markov chains? He showed that T mix is equivalent to the maximum over all x and a of pi of a times the expectation starting from x of tau a. So, to be precise, he didn't prove it for lazy mark of change, he proved it for continuous time mark of change, but it's Change, but it's a the but the mixing time of a continuous mark of chain is at constants equivalent to the mixing time of a lazy mark of chain. So, his result, so what I wrote is basically what he proved. So, again, this symbol here means that there exists universal constants C1 and C2, so that for any reversible lazy mark of chain, the T-mix is upper bounded by C1 times this quantity and lower bounded by C2 times the same quantity. By C2 times the same quantity. So now if we look at this expression here, the max overall x and a of this product, then we see that if A is a set which is big, so by big I mean that pi of A is going to be large, then it shouldn't take long to hit A. So pi of A is large, on the other hand the expectation is small. If A is small now, then pi of A will be small, but Then pi of A will be small, but then one would take long to heat it, so the maximum of this expectation over all starting states should be large. So the two balance out, small times big and big times small. And then the question is, where is this maximum actually attained? And so the theorem that I stated above, so let me actually write theorem one so that I refer to it later. Refer to it later. So, this theorem says that actually it is attained in large sets. And of course, the lower bound that I showed, it also follows trivial from his result. Okay, so that was a result due to Aldus from 82. And I'm going to come back to the work of Aldus in a little bit when I talk about the proof of the upper bound. Talk about the proof of the upper bound. Now, I want to go back to the theorem and discuss the assumptions. So, remark reversibility is essential and I'm going to And I'm going to write an exercise. So let's consider biased random walk on ZM plus laziness. So what do I mean? The bias I'm going to take I'll The bias I'm going to take, I'll just write it here. So this is z n, and then this is i, i plus 1 and i minus 1. So with probability two-thirds, we go up, and with probability one-third, we go down, and then we take the lazy version. So this is P and then P L is going to be P plus I divided by 2. So show for this Markov chain that the mixing Markov chain that the mixing time is up to constants n squared. However, for any alpha, th of alpha is of order n because the chain has a positive drift to the right, so it takes linear time to hit any vertex. So this gives a counterexample to the theorem when the chain is not. To the theorem when the chain is not reversible. Okay, so that was the first remark. Now, the second remark is about the values of alpha. So, before I said that the constants explode as alpha goes to half in the theorem, but the theorem is actually true for alpha equal to half, and I'll discuss this later. However, the theorem is false. However, the theorem is false if alpha is truly greater than a half. So if alpha is larger than a half, then the theorem is false. And so as an example, this is exercise two. So, I'm going to draw two clicks on n vertices. Okay, it's not perfect, but okay, so here is one click. And I'm going to join them by an edge. So we have Kn and Kn joined by an edge. So two clicks on N vertices joined by net so. So that T mix is up to constants n squared and t e to alpha is of order n if alpha is greater than half. Okay. Yeah, so I'm just looking at the chat now. That's right. So the lower bound doesn't require versibility. This is true for all Markov chains. Because it just relies on the fact that if we haven't hit a big set, then of course we cannot have mixed. For the upper bound, reversibility is important. And for instance, this example here shows that the lower bound, of course, it's still true. Okay. Okay, so now I'm going to start talking the proof of the theorem one upper bound Okay, so Okay, so as I said before, I'm not going to present the whole proof, I'll just give some technical parts. But the first thing I need to do is would you want to have a two-minute break before actually? It will take two minutes to define the mixing time at the joint time, so we stop at exactly half past. Excellent. Okay. Okay. So Okay, so I'm going to define another notion of mixing. So I defined the total variation mixing before. Now I'm going to define what I call mixing at geometric time. So, what do I mean by that? Let Z D, But let Zt be a geometric random variable of parameter 1 over t taking values in 1 and so on and independent of the mark of change. Of the Markov chain x. So we sample a geometric random variable with mean t and it has to be independent of the mark of chain and now define dg of t, g for geometric, to be the maximum over all starting states of the total variation distance between the law of x at time z t started from x minus Started from x minus the invariant distribution. So, what am I doing here? I'm sampling the Markov chain at an independent geometric time, which has mean t, and I'm looking at the distance to stationarity. And now, I define the geometric mixing, tg, to be the first time that this total variation distance drops below a quarter. And this is what I will be calling the geometric mixing. So I think it's a good moment to take a two-minute break now. Okay, thank you. So if there are any questions, feel free to ask on the chat and we will resume shortly. Um the random walk, yeah, so for exercise to the track. So it's a similar, yeah, I'm sorry. Whenever I draw a graph, I just think of random walk, a simple random walk on the graph. So whenever you're at a vertex, you pick another vertex with equal probability among the neighbors. So here, when you work on this. So, here, when you walk on this click, from here you can go to any of the other points, but when you're here, you can also jump to the other click. Yeah, that's right. So, for exercise two, the problem is that you have to cross this narrow bottleneck. To cross this narrow bottleneck in order to mix, so that's why it takes time and squared. So there's another question in the chat. Is it expected that one could find a bound in terms of mixing times for maximum overall sets of the square of the heating time? 12 of the heating time. So I'm not sure. So, a question from Leando Chiarini. So, why do you look at this? What is the significance of the square of the heating time? Yeah, so regarding geometric, I'm going to explain. I'm going to explain exactly why we need the geometric. This is coming. So, yeah, I'm just thinking about the square. So, I guess if the mixing time has an exponential tail, then the kth moment in general. Then the kth moment in general will be comparable to the kth power of the expectation. So, in the last talk that I'll talk about the refined equivalence, which is a result due to Basu-Peres and Basu-Harmon and Perez, I'm going to relate mixing times to the tails of heating times of large sets. So this might be. So this might be a little bit more difficult. Okay, so perhaps we continue. Okay. Okay, great. So I continue here. So yeah, so why we consider geometric mixing, as Jonathan already wrote, Already wrote. So, okay, so I'm just looking at Leandro's question. So, right, so if heating times are concentrated around the mean, this could be useful for cutoff. But then also the result of Jonathan that I will discuss later is going to be very useful. Okay, so let me go back now. So, why geometric? As Jonathan already wrote, it's As Jonathan already wrote, it's important that the geometry has the memoryless property, and this will become clear in a little bit in the proof. Now, why parameter 1 over t? Because we want, so instead of looking at the mark of chain at time t, we want to look at it at a random time which has expectation of order t or t. In this particular case, it's t now I would Now, I want, so maybe some people are familiar with the so-called notion of Cesaro mixing time. So, what this is, is if instead of Zt here being geometric, we consider a uniform random variable. So, remark if instead of geometric We take ut to be uniform on one to t then this gives rise to what is known to zar mixing time. So, again, in the same way, we sample the Markov chain at an independent uniform time, uniform in 1 to t. So, in this case, it has mean t over 2, and we look at the distance to stationarity, and we define the CSR mixing time to be the first time that the distance to stationarity drops below a quarter. So, this is the CSR mixing time. Okay, now one nice property that the geometric mixing time possesses, which is not true for the Tesar mixing time, is the fact that dg of t is decreasing as a function of t. So exercise, I guess it's number three. So But d g of t is decreasing in t okay, so now I'm going to write one theorem, theorem two that for For all reversible chains, Tg is equivalent to T mix. Again, here I mean that the constants are universal for all reversible chains. And theorem three. And again, here, let me also write LASIC. Even if I don't write, I will almost always consider LASIC chains. Theorem 3 is that for all chains, so no reversibility, so let me just write. Yeah, okay, so for all chains, Tg is equivalent to TH of alpha for any alpha. For any alpha less than a half. So now let's go back to the proof of theorem one. So this is immediate from theorems two and three. So now I just want to discuss theorems 23. So, about theorem 2, I'm just going to give some ideas later, but I'm not going to present the proof. And the proof of theorem 2 uses tools that were developed by Aldus and Lobers and Winkler. So, Aldus, and I'm going to talk about them. And I'm going to talk about them a little bit. So there is a question whether theoffymphin 3 needs to be lazy. Okay, and Jonathan is answered. Okay, no, it doesn't have to be lazy because here we consider the geometric mixing and the maximum heating time of large sets, so there is no issue with either of them. But we may as well take it to be lazy, but we don't have to. Don't have to. Okay, so let's. So now I'm going to prove theorem three, and then I'm just going to discuss some ideas about theorem two. So proof of theorem three. So we want to show that Eg is equivalent to the maximum heating time of large sets of size at least alpha. Of large sets of size at least alpha. And so there are two directions. First, that Tg is at least this th of alpha. And the other direction is that Tg is at most this th of alpha times some constant. The lower bound that Tg is greater than this quantity follows in the similar way to what we saw earlier. So this is, I'm not going to do it, the fact that this is up to constant lower bounds. fact that this is up to constant to lower bound by th of alpha. Again, this symbol means up to constants. This is easy. So now we prove the other direction that Tg is upper bounded by constants dependent on alpha th of alpha. Okay, so Okay, so how are we going? We want to show that the geometric mixing is upper bounded by this quantity. So, what we want to show is that if t is less than tg, we want to find the set B and let me again take here alpha to be 1/8 to make the calculations easy. So, we want to find the set B. So we want to find a set B with pi of B greater than or equal to an eighth such that the maximum overall x of tx tau v is greater than or equal to theta t for some positive constant theta. Yeah, this is also a lazy walk, but I'm going to be taking, okay, Jonathan already responded, I'm going to be taking alpha to be equal to half. Okay, I'll stop looking at the chart because it's distracting. Okay, so we want to find a set B which has measure at least an 8 so that the maximum heating time of the set P is at least theta times T for some positive constant theta. And of course, this is going to be known. And of course, this is going to be enough because if we show that for M it is strictly less than Tg, then we have proved the theorem. Okay, so let's use first the assumption that T is less than Tg. So this means that the chain at time Z T is not yet mixed. So it means that there exists a starting state Z and a set A so that the probability starting from Z the probability starting from z that x z t is in a is less than pi of a minus one over four so why is that and let me see if yeah okay so i'm scrolling up now so we defined the total variation the geometric mixing to be the first time that this quantity drops below a quarter since we are at the time t which is less than t Which is less than tg, oh, it has a bit of delay. Since t is less than tg, it means that at this time we are not mixed, so the total variation distance is strictly larger than a quarter. So there exists some z and some a, so that the absolute value between pz minus pi of a is going to be larger than a quarter. So this means that there must be a set A satisfying this, because if the set A that satisfies that this absolute That satisfies that this absolute value is greater than a quarter gives you the opposite inequality, then you can just take a complement and it will satisfy the inequality that I wrote. Okay, so because this is less than dg, at this time we are not nik, so there exists a starting state z and the set A satisfying that the probability that we are na after z steps is less than pi of a minus a quarter. Now from this inequality, we automatically get that pi of a That pi of A is greater than a quarter. So we have already found now a big set A with measure at least a quarter, but this is not going to be the set B that we are looking for. However, it's going to be used in order to define the set B. So how are we going to define B? The idea is that even though starting from Z we are in A after Z. We are in A after Z steps with small probability. Because A is a big set, it can be the case that a lot of starting points would be bad for A in this sense. So let me define B to be the set of points Y so that starting from Y, the probability that after set steps we are in A is at least phi of A minus in eight. So there are two things we want to prove. The first one is that pi of b is indeed a large set, so it has measure at least an eighth. And the second thing we prove is that it satisfies this condition here. So why is pi of b large? Why is pi of B large? So, it for the reason I mentioned a minute ago, because A is large and pi is stationary, it must be the case that B will be large. So, let me just write it. So, we know that pi is equal to pi B. So, this means that pi of A is equal to the sum of y in B of pi of y Py X Z is in A plus the sum. plus the sum over all y not in b and the complement of pi of y times the probability to be y x z t is in a and this is using the stationary property together with the fact that z t is an independent time okay so here i'm now going to bound uh these two sums on the right by the form On the right, by the following. So if y is in B, I'm just going to upper bound this probability by one, and then the whole sum will be upper bounded by pi of b. And for y not in b, this is upper bounded by pi of a minus an 8. And the rest are upper bounded by 1. So pi of a is upper bounded by pi of b. bounded by pi of b plus pi of a minus an eighth so this implies that pi of b is greater than an eighth which is what we wanted okay so now we have found a set with big measure and we want to establish that the maximum heating time of big overall starting Heating time of B overall starting states is at least theta t. So I'm going to prove the second claim by contradiction. So we will prove that assuming EZ tau b is less than or equal to theta t for. Theta t for a suitable constant theta that we will write explicitly what theta is after we do the calculations leads to a contradiction. So let me say again, I'm going to assume that the expectation starting from Z, remember Z is the bad start. Remember, Z is the bad starting state, so starting from Z, the probability that we are in A after Z steps. Oh, there is okay. The probability that we are in A after Z steps is upper bound by pi of A minus a quarter. So what I want to show is that assuming the expectation starting from Z of tau B is less than theta t, and I'm going to find the right value of theta, that is going to lead to a contradiction. So why is it going to lead to a contradiction? So, why is it going to lead to a contradiction? Because, how is B defined? B is defined as a set of points so that starting from there, we have a substantial probability of hitting A, probability at least an eighth of hitting A after Z steps. So, if we were to hit B quickly, then starting from there, we would also hit A quickly, but Z is a back starting state phase, and so the fact that we would hit A quickly starting from Z is going to control. Quickly starting from Z is going to contradict this assumption on A and Z. Okay, so now let me, so now we assume this inequality. So now by Markov's inequality, we have that the probability starting from Z that tau B is greater than or equal to 2 theta. Is greater than or equal to 2θ mt for some m to be chosen later, a natural number, is going to be upper bounded by 1 over 2m, just by Markov's inequality. So now I'm going to get to the contradiction for the point Z. So the probability starting from Z that XZT is in A. That XZ is in A. I'm going to show that by choosing the value of theta and M sufficiently large, sorry, the value of M sufficiently large and theta is going to be small, we are going to get a contradiction to the fact that Z was a bad starting state. So the probability of X Z T being in A is going to be lower bounded by the probability that X Z T is in A. And I'm just going to add some more events. just going to add some more events so z greater than or equal to tau b and tau b is less than or equal to 2 theta mt let me just say less doesn't matter okay so now I'm going to condition and multiply greater than or equal to tau b and tau b less than two three times t. Less than 2320. And now here we are going to use so far we haven't used anywhere that Z is geometric, but this is going to be used here because now this probability is going to be lower bounded by the minimum overall y in B of the probability starting from y exit being in A. X Z T being in A by the memoryless property of strong mark of at tau B. So we apply the strong mark of property at the first hitting time of B, and so that's why, and also the memoryless property. The memoryless property of the geometric. So, knowing that the geometric hasn't run by time tau b, we can just start afresh now starting from the set B. And so, we can lower bound the whole thing by the minimum overall y in b of the probability starting from y to being a after z steps. And so now we can lower bound using the definition of b by pi of a minus an eighth times, and now I'm just going. And now I'm just going to lower bound this probability here by saying z is greater than 2 theta n t and tau b is less than 2 theta n t. The reason I do that is because I wanted to correlate the two events because Z t and Z is independent of the Markov chain, so Zt is independent of tau B. So this now is equal to the probability standard from Z that Z T is greater than That z t is greater than 2, 3 times t times the probability z tau b is less than 23 times t. So using a Markov, using this inequality here, we get that this upper bound, lower bound by y of a minus in 8 times z is geometric, so it is 1 minus 1 over t to the 23 times t times. t times 1 minus 1 over 2m and so for 2 theta m t larger than 1 this is lower bounded by pi of a minus an eighth times 1 minus 2 theta m times 1 minus 1 over 2m so now choosing theta to be 1 over 4m To be 1 over 4m squared, we are going to get that the probability starting from z that x times zt is in a is going to be lower bounded by pi of a minus an 8 times 1 minus 1 over 2m squared. And so now taking m large enough Shows that starting from z is greater than pi of a minus a quarter, which is a contradiction, and this concludes the proof. So, Jonathan wrote it earlier, so that's why I thought I had said it. I just want to emphasize that the idea of using the geometric mixing was suggested. So, let me just write it here. Idea of geometric mixing. Okay, so let me just go over the proof towards the end. So what we showed is that if we take m sufficiently large, then this lower bound here is at least pi of a minus a quarter. And so this contradicts the choice of Z, that Z was a bad starting state for A. If I scroll up here. Here. And so this gives a contradiction. So the contradiction is that the expectation starting from Z of hitting B has to be greater than theta T for this particular value theta that we took. Theta is going to be 1 over 4m squared and then for the value of m that makes this quantity larger than pi of a minus quarter. So and so that's the end of the proof. Oops. Sorry. Okay, so how much time do I have? You can take a few more minutes. Okay, okay. So now I just want to go back to theorem two. Let me just recall it for you. So the statement is that for all lazy reversible chains, All lazy reversible chains, the geometric mixing is equivalent to the total variation mixing. So, I'm not going to present the proof of this, but I just want to explain how one proves such equivalencies. And as I said, similar equivalencies were proved by Aldus and Lobos and Winkler. And the proof of their equivalences usually go through another notion of mixing. Another notion of mixing, which I'm going to denote as this top. So, what is this top? This top is defined to be the maximum over all starting states H and then we take the minimum of the expectation starting from X of lambda X where lambda X is a randomized stopping time. I will explain in a second what I mean by randomized. Such that the probability starting from x that x at lambda x is somewhere is equal to pi. So, first of all, what do I mean by randomized stopping time? So, everybody knows what is stopping time for the Markov chain. So, it means that the event lambda x less than or equal to t is in the filtration generated by the Markov chain x, is in Ft. Mark of chain X is in Fd. Now, when I say randomized, I just mean that this filtration is enlarged, it could be enlarged, and it could include extra additional randomness. So, lambda x might also involve using some extra uniform random variables. And of course, it is a stopping time with respect to x as well. So, what is this definition? So, we are taking the worst starting state x. State x here, and then we want to find a stopping time, which is a randomized stopping time in the way I defined, which achieves stationarity. So, the mark of change starting from x, at time lambda x, it is distributed according to pi. And we want to find the stopping time which has the minimum expectation among all such stopping times. This is not at all obvious that stopping times achieving the minimum exist. But first of all, the fact that stopping times achieving stationarity exist, this is obvious because, for instance, you can take the following stopping time, pick a random state according to the invariant distribution and wait until you hit it. Then, of course, when you first hit it, you are going to be distributed according to pi. The fact that The fact that there exist stopping times achieving the minimum is not obvious, and there are constructions of different stopping times that do achieve the minimum. I'm not going to go into detail, but I have written a two-page note that I can post on the website for people who are interested to learn more details. So in this note, I discuss the frame rule, which is a stopping rule. Which is a stopping rule, or when I say stopping rule, I just mean a randomized stopping time. And its construction goes back to Baxter and Chacon in 76, and it was also used by David Aldus and Lovas and Winkler. So theorem two stated that this stope is equivalent to the total variation mixing time when the chain is reversible. Again, reversibility is essential here. And so let me just rewrite theorem 2 for reversible. This top is equivalent to the mix. Is equivalent to T mix. And what I want to say is that the lower bound, by which I mean the fact that this stop is upper bounded actually by 80 megs, this is easy. The hard direction is to show that this dog. That this top is at least some constant times the mix. And when I say this top is less than 80 mix, this is easy. So this could be an exercise. So prove that for reversible chains. This top is upper bounded by AT mix. And in order to prove that, hint use separation distance to define an appropriate stopping time. And I'm going to write a more detailed exercise in the note that I will send about the filling rule. Of course, the filling rule is not essential to show that this top is at most 80 mix. The filling rule just tells you that there exist stopping times that achieve the minimum in this definition. This definition. Okay, so I think it's a good point to stop for today. Thank you, Perla. So we will unmute everyone to thank Perla now. 