Okay, so I guess first of all, I hope everyone's doing well. And I will try to be briefer than I initially planned so as to not take up three hours, which would be pretty horrible for you and probably just as bad for me. So there's also the slides that I guess will be posted. That I guess will be posted somewhere for today, and tomorrow, what would have been tomorrow's talk, and you can consult that for more details. So, just looking again at the sort of layout of what we were aiming for, I wanted to talk about Gibbsian line ensembles and their value. And actually, yesterday, I didn't really speak much about the line ensemble from the perspective of a Gibbs property, but rather from the perspective of determinantal point processes. Of determinantal point processes. And so we looked at the non-touching geometric random walks. We used connection to Schur polynomials to prove it's determinantal and to extract asymptotics. I didn't precisely do the asymptotics that I'll be making use of today, but I gave you an idea. So that was talk one. So in talk two, what we're going to do is talk about how do you actually use the Gibbs property to enhance the type of convergence results that you can prove with determination. Convergence results that you can prove with determinant of point processes into functional convergence results. And how does that then imply a Gibbs property for the limiting object? And then the third talk, I'll describe the fact that there are other nice line ensembles, nice ensembles of curves that have interesting and useful Gibbs properties. And the one I'll talk about is the so-called KPZ line ensemble. And I'll apply that to a question involving temporal correlations of that object. Okay, so here we are. Talk. Okay, so the picture from yesterday was the following. So again, this is in a sort of artist's rendition. So we had these non-intersecting geometric random walks. Of course, they don't really look like this. They don't really look like this. And the condition was non-touching in that they all start off staggered by exactly one. Now, because of that condition, they eventually will terminate in totally flat paths. And so what we saw was that there is a limit shape that arises in this picture. So here I'm assuming something like zero. Like 0n2n. This was this sort of case that we had studied, and we take all of the geometric random locks to have parameter q, where q is fixed. And then this scale was that the top curve lives at something, which turned out to be 2q over 1 minus q times n. The kind of the bottom deformation is of order minus 2q over 1 plus q. And then there's stuff interesting that happens in between. That happens in between. In particular, what we're interested in is studying the edge of this ensemble. This is what will give rise to what's called the Airy line ensemble. And the scaling that's relevant here is to take a window of width n to the two-thirds. So microscopic relative to size n, well, I should say mesoscopic. So the microscopic scale is order one, macroscopic is order n. This is n to the two-thirds. Order n, this is n to the two-thirds. So there's actually a lot of these windows. Take a height window, which is of order n to the one-third. Now, in this scaling, the random walks look, well, like Brownian motions. This is diffusive scaling. And so the question is, what happens as I zoom in and take n to infinity in this manner? Now, the type of result that we could access from the determinantal processes is if you take any finite number of time slices and you look at the point process. Slices and you look at the point processes that you get, say, restricted to the top two curves, you can show convergence of that to a limiting determinantal point process. And so what you get there, I should say you also need to subtract off the sort of overall slope and centering having to do with the limit shape. And so in this limit, what you find is the following picture, you have that the paths themselves, so because themselves, so become generally parabolic. Of course, what I'm really talking about is finite dimensional distributions. So I converge to a joint distribution on the point processes at a few different times. But you can see that there's an overall parabolic shape to this. So let's call the limiting ensemble L. We'll call the top curve L1, L2, L3. And again, there's now an infinite number of these. Infinite number of these. And a priori, there doesn't, we don't know that there exists a continuous version of this. We only can identify L in terms of its finite dimensional distributions. Just like when you prove convergence of a random walk, initially you'll prove convergence in finite dimensional distributions. And then with some work, you'll strengthen that to a functional CLT. So the question is, this is as n goes to infinity in finite dimensional distribution. This is using the determinantal point process. So the question for today is how to strengthen. Today is how to strengthen this to from finite dimensional distributions to functional CLT, which can be phrased in the following way. Look at the measure on any compact interval in the horizontal variable in this n to the two-thirds scale, and for any finite number of top curves, and prove that the measure on paths has a weak limit. As a weak limit, or by score hod representation, that there exists a coupling in which you have uniform convergence to some limiting object. So that's what we'd like to show. What this object is, this L is called the Airy line ensemble minus a parabola. So this is L. If I define A I, say of dot, to be equal to the curves plus a parabola, which gets rid of the minus parabola, then this A ensemble, the collection of the A's, this is called the Airy line ensemble. And what we'd like to do is we'd like to construct that. And what we'd like to do is we'd like to construct that as a continuous object, as an object in the space of continuous trajectories. Okay. So let's state a kind of a rough theorem for today. I won't be particularly precise in any of these statements, but so the above convergence, and I should say, you know. I should say, you know, there are some constants you need to put in here. So there's some constant C1 and some C2, and the constant will depend on the location. So if I'm scaling around some location u times n, the constant C1 and C2 will have to do with the overall slope. The reason why you have this parabolic behavior is pretty easy to see. It's because the limit shape itself has a non-vanishing second derivative. And so the scaling really has. Derivative. And so the scaling really has to do with the behavior of the second derivative. I'm not going to write it down specifically. Okay, so the above convergence can be strengthened to hold in the uniform, so to hold for curves, not just finite dimensional distribution. not just finite dimensional distributions in a locally uniform on compact topology. So the topology of, you know, where you usually talk about functional CLTs. The AI of X, which I define as this plus the parabola, is stationary. So this is the airy line on The Airy Line ensemble. So the first result justifies that this is a collection of random curves, not just in terms of the finite-dimensional distribution. So is a stationary process in X. So what this means is that, in fact, after I do this parabolic, I get rid of the parabolic shift, what I see in terms of the AIs is a process, let me go to red. Red is a process that's statistically stationary with respect to shifts in the horizontal variable. Now, the density keeps getting higher. But if I take a slice and then I shift that slice over, the point processor, for that matter, kind of any properties are invariant under this shift. And that can be seen directly from the determinantal formulas. Determinantal formulas. So it's a stationary process in X with determinantal point process finite dimensional distributions. Okay, so the stationary and the finite dimensional distributions comes from what we were talking about yesterday. And now the most important thing, the new piece of information, which we haven't talked about, is that this line ensemble L, by line ensemble, I just mean a measure on a collection. I just mean a measure on a collection of random curves, enjoys something that's called the non-intersecting Brownian Gibbs property. And this is something that's not at all apparent just by looking at the finite dimensional distributions or the determinantal structure. And it really is a remnant. And it really is a remnant. It comes from the fact that the preliminary object was defined as conditioning random walks on certain non-intersection or non-touching events. And that property translates in through the limit once you have this uniform convergence on convex. So what does this say in particular? So this property says the following. So let's say fix any k1 less than or equal to k2. So these will be indices of curves. So these will be indices of curves and then some a less than b. And now what we're going to do is we're going to imagine we have our line ensemble and blue will represent curves that I kind of fix the information about. So let's say K1 is equal to 2 and K2 is equal to 3. So I'm just going to consider the second and the third curve. Second and the third curve. And then I'm going to look at it in an interval of times between A and B, or horizontal locations of between A and B. And what I'm interested in is what the law of the line ensemble is restricted to the interval of indices k1 to k2, which I'll write like that, and restricted to the horizontal interval between A. Horizontal interval between A and B. So the question is: what's the law of what's in between? And the information, so conditional on the sigma algebra, which I'll call sigma external, which is generated by all information external to this. So in other words, To this, in other words, if I know everything that's sitting outside of the question mark, what is the distribution, you know, the law of what sits in between? And what the Gibbs property says is that this is, first of all, it only depends on the boundary data. So it's actually equal to the same thing on the uh On the boundary. Now, what do I mean by the boundary data? The boundary data is going to be the starting points, the ending points, and the curve beneath it in that interval, and the curve above it in that interval. So, the thing that I'm darkening. And the precise distribution is equal to the distribution, so it's the law of So it's the law of k2 minus k1 plus 1. So it's just, you know, 2 in this case. Brownian bridges conditioned on non-intersection with themselves and with the boundary, right? You know, with LK1 minus 1 on the interval H. one minus one on the interval a b and l k two plus one on the interval ab so in other words and another way to phrase this is that the way i can sample from the measure and get this conditional law is the following i choose independent brownian bridges between the starting and the ending points now let's say that the first pair happens to look like this well this has a cross so i So I reject that. And now I choose another set of independent Brownian bridges. And maybe one of them looks like this, and the other one looks like that. Well, they don't touch each other, but they touch the curve above. So that needs to get rejected. And I keep doing that until I end up with a pair which connects the two endpoints, don't touch each other, and don't touch the curves above and below. And the law of that process, what I get after doing that, is exactly the conditional law. Exactly, the conditional law. Now, of course, you know, where did this come from? Well, if you go back to the geometric ensemble, this property was exactly present. If we looked at the discrete ensemble, which I'll just imagine is represented with the same picture, you know, because the way you defined it was conditioning geometric random walks on non-touching, if you restrict to a region and then you ask the law within the bat given the boundary, you can see that's. You can see that that's exactly a function of the boundary, and it's exactly given by this non-conditioning subject to the boundary conditions. Now, this is why it's called a Gibbs property. If you think in the context of things like lattice models, you know, the easing model, things like that, you have these are Gibbs measures because what happens is if you restrict to a region, then the measure inside depends on the boundary. In other words, it has the form of the measure kind of it only involves local. The measure kind of it only involves local interactions. Now, in this case, there's you know, the kind of horizontal variable and the vertical variable have different meaning. The vertical variable you can think of as an index of the curve, so that's a distinct variable. That's a discrete variable. And the horizontal, in this limiting case, is a continuous variable. You still have a sort of locality of the interaction. So, what I'm going to do in this lecture is In this lecture, we'll sketch how this theorem arises. So we'll sketch a, and I'll really put quotation marks proof because there's a lot of details that need to be put in of this theorem from the context of geometric random walks. And then we'll give some applications. So, and so. So, and so you know, not only will the Gibbs property be important in proving the theorem, but then we'll give some applications of Gibbs property. Okay, so if you get lost or bored at any point, let me actually mention to you an exercise that I'll give a little. To you, an exercise that I'll give a little later, but you can think about how the Gibbs property plays into this, or one way of approaching this, which is to say that if you have a Brownian bridge between zero and one, this is a Brownian bridge, right? First of all, convince yourself that this enjoys a very simple Brownian Gibbs property. In particular, if I look at the measure between any two, you know. Between any two, you know, any subset within that, then the law of what remains is exactly a Brownian bridge that connects the endpoints. This is a pretty easy thing to see. So justify this or make it clear to yourself. But then the real question is use this to prove uniqueness, almost sure uniqueness of the maximizer of the Browning variant. Of the Brownian bridge. Okay, so we, you know, you probably have heard, you know, we've probably seen with stochastic analysis proofs that there's a unique maximum for a Brownian bridge or Brownian motion on a finite interval. But there's a very nice proof of this, which is also talked about in the notes. But think about how, you know, as we go, how what I'm talking about could be useful in that. Okay, so. Okay, so in fact, one of the reasons why the Gibbs property is so useful is because it gives rise to a sort of comparison property, which I call stochastic monogenicity. There are other nice properties. There are actually things like FKG inequalities that hold for this model, which have Which have uses, but I'm going to speak about a particular one that use that will come up a lot in this lecture. So the stochastic monotonicity says the following. So imagine that I have a measure of the following sort. So I have some set of starting points. I'll draw in black. Let's say I have two curves and I have a set of ending points. Then I have a curve above and a curve above. Above and a curve below. And now, what I'm going to do is I'm going to look at the measure of all pairs of non-intersecting Brownian motions which connect the starting and the ending points and don't touch the bounding curve above or below. So this is a particular instance of sampling from that. So now let me consider doing the following. So I'm going to shift the starting points down. Starting points down, and I'm going to shift the bottom curve down, and I'll shift the ending points down, and likewise I'll shift. And it doesn't need to be, you know, strict shifting. It can kind of track along it for a little while. So everything gets shifted down. And now let me sample from that measure. And the claim is that there exists a coupling so that if the boundary data is So that if the boundary data is coupled, so there's a probability space on which if the boundary data is coupled monotonically, so things you know, the black sits above the blue, then the same will hold true of the law of the non-intersecting Brownian bridge measure. In other words, you can construct them on the same probability space in which you have almost surely the first set of Brownian bridges sitting above the second set. So let me just draw the second set as some sort of kind of dotted trajectories. Some sort of kind of dotted trajectories, and the point is that these drop down. So, this is very important. So, let me just reiterate. So, if, say, the boundary data the black boundary data is bigger than or equal to the blue boundary. Bigger than or equal to the blue boundary data. Then we can couple the condition measures on Brownian bridges. To also be ordered. In particular, this tells you that if you, for instance, move the bottom curve down to minus infinity, then the curves will only drop. But they won't drop more than just kind of free Brownian bridges would drop. So it gives you some control, some sort of monogenicity control. Now, this is something that's true for the limiting system. You need to prove this, and I'll actually sketch a proof of this. And I'll actually sketch a proof of this. But a similar sort of monotonicity holds for the geometric random walk for the non-touching. And so let me explain how this works in the discrete setting, and then one can. And then one can push this through the machinery that we're going to be developing to a result, you know, to a similar homogenicity for the continuous setting. So imagine that we have our discrete, you know, function that bounds us above. Let me just take a two-step picture. So this is, say, one trajectory of a discrete random block. And then I have another one that sits beneath it. And now I have a starting point. Let me represent in blue another. Actually, I'll use dotted lines. So, this is like another bounding curve beneath it, which is going to be strictly below it. And then this blue one will also sit strictly below. And I can start my points off, say, here. Here and go to here versus from something lower to something lower. And so the claim is that the same will hold true that I can couple them together. Now, the argument goes as follows. So start out given the boundary data with the lowest possible curves that connect the boundaries. So the dotted curve is the lowest possible one, which connects them by these sort of patterns. them by these sort of paths. So I guess it looks like something like that. And the lowest one that will connect my black boundary data is this one. So obviously the starting initial, you know, the initial conditions, I'm going to run a Markov chain now. So the initial conditions is ordered. The dotted curve is beneath the undotted curve. And now I'm going to run a Monte Carlo Markov chain, particularly a metropolis dynamic to update. Dynamic to update these configurations in a manner that preserves the ordering, but also converges as time goes to infinity to an invariant measure. And the invariant measure is exactly the conditional measure of the geometric bridge condition on the boundary of data. And so, how does the dynamic go? It's just that at each horizontal location, so I have a collection of Poisson clocks. Clocks. And the clocks are indexed by a horizontal location x and an index k. So maybe I have more than one curve in between. I could have, you know, seven curves rather than just one. And when it rings, what I do is I flip a coin and I change by plus or minus one the height. If it doesn't lead to, nope. To nope, touching. So I just flip my coin and then I either change my height of my, you know, so let's say, of course, in this case, this is kind of a most a trivial thing because the only thing that can be updated is the height of the midpoint in my picture. So let's say that the midpoint rings, and now I have my, I flip my coin and it comes up plus one, it comes up heads. So what I do is. Of heads. So, what I do is each one of these I update by one. I try to do that. Now, it might happen that doing that for one of the curves violates non-touching. It might end up touching the curve above it or below it if I'm dropping down. If that happens for that curve, I don't make the move. But for the other curve, if it's not violating it, I still make the move. And it's easy to convince yourself that this will preserve the order. And so there's an exercise. There's an exercise which is to do just that. So show this preserves order. And then that this converges to an invariant measure, which is equal to the geometric random walk bridge conditioned on non- On non-touching. So this gives us such a coupling. Now, I don't want to make you think that this sort of monotonicity occurs always. So it occurs for the geometric random walk, it occurs for the Brownian bridges. The question is: you know, if you took another law on random walks, does it occur? Does it occur there? Well, I can tell you it occurs for plus or minus one, you know, simple symmetric random or just plus or minus one nearest neighbor random walks. In fact, I think, and I, you know, this has been worked out in some related contexts of a sort of continuous version of the random walk, but the condition on the existence of this coupling is log concavity, log convexity. Log convexity. It's that the logarithm of the probability distribution should have a certain convexity to it. Okay, so I'm not going to prove that and I can give you references for where something similar to that has been proved. But let me just give you an exercise which will help you maybe appreciate that. So the exercise is to find a discrete A discrete random walk, you know, so just a continuum, a discrete time, discrete space random walk with, let's say, you know, three different choices. You can do more if you want, but I at least have one that does it with three, which violates stochastic monotonicity. So, in particular, I want you to find the following. So, find such a discrete random block such that if I look at a two-step bridge, so I have starting time and then an ending time, and I look at a bridge which goes from level zero to zero, and then I look at a bridge that goes from level zero to level one. So at the level one, I guess level one, so between time zero and time two, that I don't have any sort of ordering. You know, stochastic monogenicity would suggest that there exists a coupling of these two measures for which I have this, but I want you to show that there exists no such coupling. So find a random walk for which this is true. Okay, certainly that example is not going to be a geometric. Is not going to be a geometric random lock, as we've seen, but there are other simple examples. And what will be true of this is that it won't have the sort of log convexity. Okay, so this stochastic monogenicity is an important ingredient that we're now going to use to prove this construction of, or to explain the construction, the area line on some. The Aerialine Ensemble. All right, so this will occur in four steps without many details. And step one is already done. So this uses the finite, this is the finite dimensional distribution convergence that we proved or explained yesterday using Schur processes. Processes. So this gives us finite-dimensional distribution at any finite number of times. But the issue is that this doesn't rule out exceptional behavior at random times. You know, for instance, in my line ensemble, and I'm going to draw all line ensembles looking continuous. I can't draw discrete ones. So, in my discrete line ensemble, what I could have is, I know that at a few finite times, the behavior is, say, converging and it's tight and everything is nice. But what could happen is there could be random times where it just sort of shoots up higher and higher. Of shoots up higher and higher, and it doesn't converge. Or there could be random times where, even though at deterministic times, I can argue using determinantal point trusts that there's a nice distance between the two curves in the scaling window, it could be that at random times they come very close to each other, such that in the limit they're touching each other. And that would be an issue, especially when I try to prove this Gibbs property, because if the term, if two bounding curves are exactly equal, then there's Exactly equal, then that's a degenerate conditioning. So I'd like to rule that out, and that takes some work. And the first step is what's called no big max. This is our sort of short term for what we're going to do. And what this will do is this will rule out these sort of spurious events occurring at random times. And so, what I want to argue is that I'm going to show. So I'm going to show that on the interval AB, the top curve of my line ensemble cannot get too high. And what I really mean is that I'm going to argue that if I know that the finite dimensional distributions, so everything I'm thinking of is indexed by parameter n, right? N is the index. The index of the geometric random locks that I'm going to take to infinity. And what I want to show is that as n goes to infinity, if I know at finitely many points, deterministic points, that I have tightness, that I have that things are not kind of blowing up to infinity, I want to argue that what happens in between doesn't blow up to infinity either. So how can I do that? Well, there's a sort of nice argument. So let me just look at the top curve because I have stochastic monotonicity, and so that will transform. Monotonicity, and so that will transform to the lower curves as well. And so, imagine I have the top curve between some time A and external to the times A and B. And this is my sort of line ensemble. Once I've done the shifting and I see the parabola starting to emerge because of the curvature. So I'm just going to write L1, but there's really L1, but there's really a sort of L1. I should really be putting like an N here to represent that this is the rescaled version of the top line with this n to the two-thirds, n to the one-third scaling. Though I'll generally just drop the n. And then I have my second curve, and it's doing something. This is L2N. Now I want to rule out that the first curve. I want to rule out that the first curve gets too high. So let me identify what does it mean to get high. Maybe I should have drawn this a little bit more space. Okay, so let's say that getting high is like going up to some really high level I'll call R. And I know that the values at these endpoints are not getting too high. And in fact, I know that they're also not getting very high. And in fact, I know that they're also not getting very too low. So I can argue that they're sitting between minus r and r at these deterministic points. And that's from finite dimensional distribution control. So now what I'm going to do is I'm going to define a stopping time. And the stopping time I'll call chi. And what it will be is it'll be the first time between, say, a and a over b plus two, so the midpoint. The first time that I exceed R. So chi is a sort of inf over all x in A to the midpoint of X such that L1 and of X is bigger than or equal to R. By continuity, it actually equals R at that point. Okay. Okay, and so now there's a nice extension of the Gibbs property. The Gibbs property tells me that between any two deterministic times, the measure in between is a random lock or in the limit, a Brownian bridge conditioned on the boundary data. But the same holds true for random time. So there's something that's called the strong Gibbs property. And what you need is that the interval, you call something a The interval, you call something a stopping interval or stopping domain, if it's the knowledge of where it is is measurable with respect to the external field outside of it. So the claim is that the interval between chi and b is a stopping domain. And the way to see that is that I can determine where that interval is. Well, the right point is deterministic, so I don't need to worry, but the left point. So, I don't need to worry, but the left point I determine based on information external to that interval. And so then the strong Gibbs property says that the law of what's in between, I'll just draw away the dotted line, is exactly that of a Brownian bridge that goes from at time chi, the random time chi value r. Chi value r to being at whatever L was at time B and then conditioned on avoiding L2 on that interval. Okay now chi need not exist right so if if if if no So if no such chi, you know, so if chi, well, you know, chi need not exist, but if chi exists, then this implies a big max. And we want to basically show that the probability of a big max is small. So we want to show that chi existing is very unlikely. Okay, so how do we do this? Well, first of all, we use monotonicity. Use monotonicity, right? Monotonicity tells us that if I were to take this bottom curve, this bounding curve on the bottom, and drop it to minus infinity, then this dotted curve would only drop. But it wouldn't drop so far. It would just drop to a Brownian bridge condition on the end points and nothing else. So, in other words, just a Brownian bridge. Now, on the event that Chi... So, on the event that chi occurs, so on chi existing, we can look at what happens in between and compare it to the linear interpolation between the starting point and the ending point, chi and b. Now, because we know that the value at this end point here is not too negative, it's no smaller than minus r. It's no smaller than minus r. We know that the slope here is bounded by a constant times r. And in particular, that we can argue then, just because chi is between a and the midpoint, that if I look at the value exactly at the midpoint, then that value needs to exceed r over 2. So the value of the linear interpolation. On the other hand, a Brownian bridge. On the other hand, a Brownian bridge has probability one half of exceeding its midpoint, or you know, exceeding its linear interpolation at any given time. And so what that tells me is that there is a 50% chance that this Brownian bridge will exceed the linear interpolation, which in particular exceeds r over two. So on the event that chi exists, we find that the Brownian bridge will exceed r over two. Brownian bridge will exceed r over 2 with probability at least a half, or exactly a half. And then so the conditioned probability, the conditioned Brownian bridge, will exceed it with probability at least a half. On the other hand, that location is exactly the distribution, that's exactly distributed as my original line ensemble at the intermediate time a plus b over 2. But that's a deterministic time. And by tightness, we know that, well, the probability as r gets to infinity of that is going. As r gets to infinity, of that is going to zero, you know, uniformly as n is going to infinity. So, what this is all telling us is basically that the probability that this chi exists is closely related to, it's maybe up to some, there's something like, you know, maybe two times the probability that the midpoint, you know, the value of the midpoint, sorry, it's bigger than or equal. it's bigger than or equal less than or equal to this is what the argument shows that the midpoint is bigger than or equal to r over two and this thing is anyway going to zero as r goes to infinity in a uniform way in n and so that tells us that it's very unlikely to have these random times where you have these big maxes now to reinforce this argument i want to give you This argument, I want to give you a simple exercise. And this, of course, is something that you can prove with other tools, but I'm going to use this argument to show that if I have a Brownian bridge, the probability that the max on the interval zero to a half, let's say, and you can do the whole thing. Let's say, and you can do the whole thing with symmetry, of B of S is bigger than R over two. Okay, well, I'm not sure if exactly what I wrote here is quite right, but it's something of that sort, is less than or equal to two times the probability that the midpoint value is bigger than or equal to I think it's the other way around. Sorry. I think this is supposed to be our root too. Okay. So in other words, you know, you have your Brownian bridge. You want to rule out that somewhere in between it jumps up really high. And basically, you control that by the behavior at the midpoint. And for this, we use the Gibbs property. Okay, so let me pause for. Let me pause for questions because there are two more steps, but I've kind of already talked about one of the technical steps, which is just no big max. There is a question that should it be zero to one or zero to a half? In here? Yeah. I think the argument is zero to a half. Just the reason why you, you know, of course you can do it zero to one, but then you get another factor of two. Factor of two. The reason is that. Then in the right hand side, should it be B of one fourth or? B of a half. Okay. So the idea is that you want to, the question was about this, right? Yeah. Okay. So you need to establish that a big max somewhere will imply. That a big max somewhere will imply a big max at a deterministic location. And the reason why you work with this sort of midpoint and you only look to the left of the midpoint is basically because you want to show if you looked at on the whole interval, and then you could end up having a big max somewhere very close to the edge. And then you would need to deal with, in a sense, a random location where that implied a large slope. But anyway, the argument can be adapted. But anyway, you know, the argument can be adapted. The way I've done it is you basically search half of the space, you imply that that, you show that a big area there implies big at the boundary, the mid-boundary, and then you rule out that. This is, of course, not a tight bound. You know, you can come up with much better bounds than this. This is not tight at all. Okay. Were there other questions? So what we've now argued is that the maximum can't get too large, but we still need to establish tightness. And tightness is not just about saying the maximum. This is not just about saying that the maximum can't get too large. We need to argue that we want, well, you know, when we're talking about the topology of uniform convergence on compact subsets, we need to argue that curves are not getting too steep either, right? Because we want to show that the limiting ensemble is a measure on continuous paths. Okay, so we need a way to kind of argue about the slope. And in order to do The slope. And in order to do that, what I'm actually going to argue is ultimately that the paths are going to converge to something that has a radon-nicodem derivative that's absolutely continuous, in other words, with respect to just free Brownian motions, you know, with no with no conditioning at all. Okay. In order to do that, I'm going to need to argue about separation. So, step three is a statement about good separation. And this will be important because if the curves are not separated, then the Gibbs property will kind of sample very small parts of the probability space of kind of independent free random walks. And in that part of the probability space, you could see exceptional behaviors occurring, you know, very steep things occurring. And we don't want that. We want to rule that out. So what I want to show in showing good separation is the following sort of Showing good separation is the following sort of picture. So, I want to show that, let's look at this at the top k curves. So, I first want to argue that the starting and the ending points at any given locations are well separated. So, I want good separation. And again, what this means is that as n goes to infinity, the say minimal distance between these curves is uniformly bounded. Uniformly bounded, you know, the probability distribution of that stays kind of uniformly bounded away from zero in a tight manner. So, in other words, if I took like the inverse of it, that that would be a tight collection of random variables. So, as n goes to infinity, I don't want them to get smushed and smushed and smushed together. Now, that can be proved in two different ways. One is through the Gibbs property, and the other is through determinantal point processes, because you know information about finite times. You know, information about finite times. So let me just take that as a given. Now, I have also my k plus first curve that sits beneath me. And what I really want to argue is the following sort of statement. I want to argue that I can take a little block which has some width to it and I can stick it between the bottom curve and And in the bottom point. So the kth point and the kth, let's say this is the k plus first curve. This is the starting and ending points for the kth curve. So I want like a little block that has some width that I can stick underneath it so that the bottom curve has a very low probability of overlapping with that block. And of course, the smaller the block, the better the probability it doesn't overlap. Now, what this is going to do for me is it's going to basically say that this bottom curve can't force, you know, Bottom curve can't force, you know. The thing that I'm worried about is the following picture, which I'll draw, which is that the bottom curve pushes my, you know, keeps getting steeper and steeper, and it pushes my curves so that my non-intersecting path need to basically push themselves up really, really steeply, which will push them together, which will cause them to have discount, you know, issues in the limit. They might become discontinuous, and they certainly won't have the sort of continuity properties that Brown. Have the sort of continuity properties that Brownian motions have. So I need to rule that out. And if I can put this little block in there, and if additionally I know that the maximum height of this midpoint or of the K plus first path is bounded by some R, where R is some random variable with the nice tails, that will rule out that sort of smushing of the probability space, which would then force you to have these sort of unusual behaviors occurring. So, this is the goal: is to rule this out. Let me erase this. And there's a conceptually pretty simple argument. Actually, articulating it takes some work. And let me just explain the idea. It's basically an application of monotonicity. So, what let's say here's my K plus first curve again. Now, the measure in between, and I have my curves. Have my curves. So, forget about the blocks, forget about these blocks for the moment. Let's imagine I wanted to show that at around the midpoint, I could insert in a little block here so that with a high probability as n is going to infinity, the kth curve and the k plus first curve stay apart and don't overlap with this block. So it kind of is a spacer and it has some width associated. And it has some width associated to it. Okay, of course, you know, the smaller the width and the smaller the height, the higher the probability, and I want that in a uniform way in n. Now, if I can show that that's true at the midpoint at a plus b over 2, then I could actually just shift the picture over and show that the same is true deterministic, you know, at A and at B. So it suffices to show that this is true at the midpoint. Now, to show that it's true at the midpoint, I can do a monetization. Point, I can do a monotonicity argument. What I can do is I can actually replace this bottom curve by a curve that basically does the same thing, but only on a finite part of the interval. Or for that matter, I could even erase it entirely. And, you know, if this was the curve, I could replace it by a curve which is just a needle. The needle which sits, you know, and hits this point up here. Point up here. Well, maybe I shouldn't do it entirely with the needle. Let me replace it by just restricting to the curve on the interval of the width of this block. Now what I do is I know that the Brownian bridges will drop, but the most that they'll drop is then given by Brownian bridges, which are conditioned to avoid this region here, this interval here. Now I know that this is not too high, and so what the problem really High. And so, what the problem really reduces to, and I'll just give you the caricature on the side, is the following type of problem. And it's, of course, a slightly more complicated because here you have k paths and you have a little bit of a more complicated bounding thing. But imagine the following example, which is just you take a, say, a Brownian bridge and you insert in a spike in the middle, and now you condition the Brownian. And now you condition the Brownian bridge to go above the spike. And what you want to show is that the distance between these two, or maybe even on a little width, isn't zero. It's not going to zero. So you show that as long as the height of the spike is bounded by some r. Is bounded by some r that this size of this square is some random variable that is just a well-defined random variable and it's not going to zero in any, it's not zero. And of course, that's clear for the case of the continuum thing. You need to argue it if you're, you know, when you're working with the discrete objects and you're avoiding this sort of spike type impediment that there still exists this gap. And so there's an argument there, details of which need to be filled in, but once you have Of which needs to be filled in, but once you have that, then you know that you have these little blockers. And so once you have good separation, the last step is to extract the limit. And the argument there is basically that you can write a rather nicodem derivative. So if you look at the conditional measure, Measure. So again, let's assume that we're looking between A and B and we're looking at, say, the top K curves. So if we're looking at the conditional measure, the Roder-Nicodem derivative between that and the measure for K free Brownian bridges, then first of all, that can be written as one, the indicator function that the Brownian bridges are accepted. The Brownian bridges are accepted divided by the probability of acceptance. Let me explain what this means. So, you know, the picture was, you know, you have your bounding curve, you have your starting and ending points, and I sample independent Brownian bridges, and I keep doing that until I get a pair which doesn't cross. Well, in fact, I'm doing this for the random blocks. Well, in fact, I'm doing this for the random locks, but you know, this same sort of thing. So, really, I guess this is the random locks. Now, the measure that I get is then equal to the indicator function for acceptance. Basically, I evaluate whether I have acceptance, but that only happens on some part of the probability space, so I need to normalize by the probability of that acceptance. Now, what I claim is that once I know separation, so if I have good separation, so once I have this sort of So once I have this sort of separation between the paths, then that implies tightness of the acceptance probability. And that's just because once I have these sort of blockers in place, the argument goes that I can basically deterministically, I can force my paths into regions where I won't intersect. So I can do something like take my Brownian bridges. Take my Brownian bridges and let me draw it down here. So I have my kind of good block, my separations here, my separations here, and some separations between them. What I can do is I can basically force them up into the good region where it's above height r. Where it's above height r. And then they can do whatever they want as long as they don't intersect. And I know that they'll avoid the second curve. And so that tells me that, you know, given this separation, I have a positive, strictly positive probability of acceptance that will stay uniform as n goes to infinity. And that will imply tightness relative to the brownie, relative to the random walk measure. And so, you know, really the final step is that this implies tightness. So, this implies tightness of the conditional measure, right? Because we have tightness of the random walk measure, that's for free, that's like non-screw invariance principle. And so we have tightness of the conditional measure as well. And that plus the finite dimensional distributional convergence implies the functional convergence. And on top of that, On top of that, if I take this formula and I transfer it to a limit, I get a similar formula for the limiting Brownian, or you know, for the limiting process for the Airy minus parabola has a similar sort of property, which is exactly this Brownian-Gibbs property, this non-intersecting Brownian-Gibbs property. Okay, now again, there's a lot to be done in the details, and really dealing with these discrete random variables and articulating these arguments that kind of make sense in continuum a little bit easier in discrete. There's a lot of work there. So I don't want to downplay that. Let me in the last minute or two give you one application. Give you one application. Okay, and then we'll take our break. So, now you know, so we use the Gibbs property to extract, to go from deterministic time regularity to regularity of the entire ensemble, you know, not just the top curve, the lower curves, all intermediate times, show that everything that we get is absolutely continuous with respect to Brownian motion. That's what this implied by this non-intersection gives. By this non-intersecting Gibbs property. So, as an application of that, of the Gibbs property, I'll give you a theorem, which is that if you look at the top curve, which is called the Airy II process, for those who've heard of it, and you subtract off the parabola, so this is, in other words, this is the L1 that we spoke about, so this is the top curve of my line ensemble, has a A unique maximizer. So that's the theorem. This is actually a conjecture of Johansson that we proved with Alan Hammond. So in other words, there exists a unique point. You can't have two points where you get to the same height. So just one point. Now, I'll remind you, the exercise we had before was The exercise we had before was to show the same for a Brownian bridge. Okay, and if you want, there's an argument that's sketched, you need to fill in details in the problem set. Now, once we know that Brownian Bridge has a unique maximizer, all you need to do is the following. So, you know, here's our top curve. You know, here's our top curve. And what I want to do is, I want to look at, let's say, a big interval. I look between minus r and r. Now, the values here I can show are pretty small because I'm looking at the thing minus the parabola, and the thing was stationary, so the thing minus the parabola is small. So now what I need to now what I do is I look at the measure in between and I know that what I see, you know, condition on the second curve is a Brownian bridge condition to stay above the second curve. And so that thing has almost surely, by absolute continuity with the Brownian bridge, a unique maximizer. So this tells me that the red curve between minus R and R almost surely has a unique maximizer. Now the only thing that's left is Now, the only thing that's left is to show that there is no maximizer, or with probability going to zero, as r goes to infinity, a maximizer will not occur outside of the minus rr window. And that seems kind of clear that that should be the case. There's still an argument that you need. So there are two parts to the argument. One is you need to show that you're likely to be high. Yeah, I'd say you're likely to be fairly high. Fairly high in the middle. So let's say, you know, the height here of a sort of parabola would be like minus r squared. So let's say I want to be at least above minus r squared over two. So somewhere in between, I should be at least minus r squared over two, which seems like it should happen. That's quite likely. And then to the right, I want to argue that I never get above minus r squared over two. And the argument can go in the following way. I look on intervals between r and r plus one. intervals between r and r plus one and r plus two and r plus three. And I want to show that the probability of getting up close to here is something that's summable. So I can use a union bound. Now in order to do that I can use an adaptation of the no big max approach. I want to show that at intermediate times I can't get too large. So I need to use that and there's one other thing I need to use which is I need to use some knowledge of I need to use some knowledge of tailbounds for the one-point distribution. So, you know, in other words, that with, because I need to sum something. So I need something to be summable. In particular, I would need that even at these deterministic times, that the probability of getting large is summable as, you know, for all integers bigger than R. And so for that, I do need some control over the decay. And the type of control that I actually have is much stronger than what I need. Than what I need. And it actually comes from the fact that this L1 of zero is distributed according to the Tracy-Wittem GW distribution. And for that, we know things like the probability you're bigger than S is like e to the minus some constant s to the three halves, and the probability that you're less than minus s. And of course, the same is true by stationarity for the thing is. Is less than e to the minus some constant s cubed. And so combining this with the no big max allows you to rule out a contribution from this sort of tail regions. It actually also allows you to prove a localization result that says that the probability that the maximizer occurs bigger than R outside of this window. Outside of this window is less than e to the minus constant r cubed. Okay, so the exercise is to fill in the details. Okay, so that's what I wanted to say for this second lecture. So let's take a break if people want to ask questions now or a little bit later. So, yeah, let's take a break. I don't know if you want to do five or ten minutes. We can do it till 10.15. We can do nine minutes. Should I answer? There was a question about the log concavity. There's an argument in a paper by one of the PAs by Xuan Wu. Pies by Xuan Wu about this for a discrete time continuous space ensemble. She gives a sort of monotonicity result under this sort of log. I forget if it's concavity or convexity condition. I'm not sure what. Okay. Well, so there's also an argument in the paper that Alan mentioned, but that's having to do with a Brownian path, but an interaction Hamiltonian weakening version. Interaction Hamiltonian, a weakening version. This is what I'll talk about in the next lecture. Anyway, so let's take a little break. Okay, so Lucas, is there an easy modification of the non-touching random lock model to make appear the other symmetry classes? Ah, that's a good question. So the question is, So the question is: in random matrix theory, you have the notion of other symmetries. So I haven't talked about connection to random matrices, so let me just spend one moment and put this in context. So if you have a beta equals two Dyson-Brownian motion, in other words, you take a matrix, you fill it with complex Gaussian random variable, complex Brownian motions, so Brownian motion plus I times independent Brownian motion. times independent Brownian motion, subject to Hermitian symmetry. And you look at the eigenvalues, that evolves according to a Markov process called Dyson-Browning motion. And that's beta equals two. And what you see is that that has a nice stochastic differential equation representation. And there's a parameter two in there that you could change to beta. Now that Dyson-Brownian motion doesn't really, for other beta, doesn't really, well, okay, for beta equals one and beta equals four, it also has a relation to random matrix. It also has a relation to random matrix for general beta. We don't really have such a picture. But anyway, you can ask: is there a sort of similar Gibbs property picture there? And the answer is no. For general beta, you don't have a Gibbs property for the Dyson beta Brownian motion. And that comes down to the fact that the Dyson beta equals two Brownian motion can be seen as a Dubesch transform. Can be seen as a Dubesch transform, and that's not true for other beta. So, this is a calculation one needs to make. There are versions of Gibbsian line ensembles that have some relation to beta equals one and beta equals two type random matrix distributions, but these are actually something called half-space Gibbs line ensembles. So these are where you have a sort of potential at zero, and then you have a collection of paths that go away from that potential, and ask you that. That potential and ask to the best behavior of that. So that occurs in the context of App Egan or Half-Space Sure processes. And it hasn't been that well developed, but there certainly is a picture, but not with the Dyson beta-ground culture. Okay, so what I want to spend the last let me draw a line. 