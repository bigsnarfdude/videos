This session is Peter Bronstein. So, he will be talking about related work, essentially the SPD approach for special terms. Thanks very much, Rafael. So, this is joint work with David Bolin, Sebastian Ngelke, and Rafael Kiza, who are all here. And before I start, I'd just maybe give a quick disclaimer. So, my background is in probability, and I've only been working on statistics for about a year. So I think I'm in kind of a unique. So I think I'm in kind of a unique situation where perhaps half the audience knows more about extremes than I do, and the other half of the audience perhaps knows more about spatial statistics and SPDs than I do. But hopefully I can perhaps make some contribution by trying to link the two areas. Okay, so Sebastian motivated this work very nicely in his talk. I'm going to motivate it again. So in spatial statistics, we have the following problem. When evaluated at a finite number of locations, When evaluated at a finite number of locations, Gaussian fields generally do not exhibit sparsity properties such as conditional independence matters. Now, this problem was largely solved by Finn Lingren, Lindstrom, and Root in their 2011 paper. In spatial extremes, we have a very similar problem. When evaluated at a finite number of locations, Brown-Resnik processes generally do not exhibit any sparsity properties, such as extreme or conditional independence patterns. Independence patterns. Now, Sebastian has already introduced the concept of extremal conditional independence, and this is kind of in a Pareto framework. So, we're kind of underlying all this, we're going to be in a kind of peaks over threshold. But given that, and given the kind of similarity between these two problems, the very natural question is to ask whether the kind of SPD approach can be used in the setting of extremes. And that's kind of what I'm going to talk about in this presentation. Okay, so the outline of the talk. I'm going to start off by just very quickly going through the basic steps of the SPE approach. And what this is going to do is kind of set out the structure that we're going to have to follow when we try to adapt this approach. And I'm going to introduce spatial extremes, and I'm going to make these kind of basic observations or observations that we're going to need to kind of adapt this SPDE approach structure to the extremal setting. And then finally, I'm actually. And then finally, I'm actually going to do it and adapt the SPD approach to spatial extremes. Okay, the SPD approach. So, as perhaps everybody in the audience knows, the stationary and isotropic Gaussian field is characterized by a covariance function, which is a function of the distance between two points. And the most commonly used covariance function is the return covariance function. So, we've seen this covariance function in a couple of talks. And below this, I've kind of illustrated an outcome. I've kind of illustrated an outcome of an internal field. So, if we ever want to kind of perform statistical inference, we're only going to observe, get observations at finitely many locations. So, we need to work with the finite dimensional distributions. And these are multivariate normal. And there's a covariance matrix that's given by this kind of matern covariance function. And the problem is that this covariance matrix is dense, and that means that. Matrix is dense, and that means that when we're kind of computing likelihoods, the computational complexity increases very quickly with the dimension. And we can't really hope for this covariance matrix to be sparse because zeros in the i-jap entry correspond to independence. Okay, so that's not very realistic in a spatial setting, even if locations are kind of far away. We still expect there to be some kind of dependence. So, what you can instead do. So, what you can instead do is you can instead write down the finite dimensional distribution in terms of the inverse of the covariance matrix. So, here, roughly speaking, we then have a Gaussian Markov random field. And then it's much more realistic to expect entries to be zero. So, it's much more realistic to expect the matrix to be sparse. And then the computational time is going to increase a bit more slowly than the number of dimensions. So the reason why it's more realistic is because now zeros in the IJ entry correspond to conditional independence. Okay, so Sebastian talked a little bit about this as well. So then it's quite realistic to expect that the field at this location is conditionally independent to the field at this location given the values of the field at all the other locations. This is perhaps a bit more realistic. The unfortunate thing is that with the maternal covariance function, this matrix Q is still dense, okay? Matrix Q is still dense. But perhaps now we kind of have an intuitive idea that maybe it could be sparse. We can kind of approximate these finite dimensional distributions with a different Gaussian-Markov random field with a sparse decision matrix. And that's kind of where the SPD approach comes in. So, what do we do? We form a triangulation, and we'll let M be the number of vertices in this triangulation. We use this triangulation to Triangulation to form basis functions. So basically, if we say this is node k, then in the kth basis function, it will take value one at this node and value zero at all the other vertices, and it's piecewise linear in the faces. So it kind of is decreasing piecewise linear in all of these faces. Okay? And then what we try and do is we try and approximate the fields with these kind of Fields with these kind of with this finite element approximation. Okay, so in this picture, I've kind of plotted just one outcome of the maternal field, but of course this is random. So these weights are going to be random. Okay, so we have this W tick twiddle, which is Gaussian map of random field with some precision matrix Q twiddle. And what we kind of hope is that we can kind of get a good approximation with a space. Get a good approximation with a spartan precision matrix. So, the SPD approach is again how we do this. And it starts by kind of observing that the maternal field is a solution to an SPDE. Okay, so this is an SPD that we've seen in a few talks already in David's talk, for example. Now, one way of knowing that we have a solution to this SPD is kind of integrating the left and right-hand sides with different test functions and then showing. Functions and then showing equality and distribution. Okay, so one way, or the way that you can compute Q twiddle is to do the same thing, but now we replace the true field with our approximation and only choose M kind of carefully chosen basis functions. Okay, so then once we do this on the left-hand side, we have an M-dimensional Gaussian, and on the right-hand side, we have an M-dimensional Gaussian, and this M-dimensional on this. And this m dimensional, on this left-hand side, this m-dimensional Gaussian has a covariance which is a function of this q-toodle. Okay, so we can use this to kind of solve our precision. And if we chose the test functions well, which Dean did, we get a good approximation and a sparse precision matrix. Okay? All right. So that's the steps that we're going to kind of have to follow for the spatial extreme setting. Spatial extreme setting. So now let's introduce spatial extremes. We're going to be working with Brown-Resnik processes, which are kind of the equivalent of the Gaussian processes in spatial extremes. And that's perhaps at least partially because they're defined in terms of Gaussian processes. So let Wi be independent copies of Gaussian processes with stationary increments and marginal variance, sigma squared. Sigma squared and the variagram gamma gamma. Defined an independent Poisson process is intensity, and then the max stable distribution of the Brown-Resky process has this form. Okay, and there's a corresponding kind of Fredo distribution. So, kind of key observation in this talk is that the law of our Brown-Resnick process depends only on the baryogram. On the baryogram. Okay, so to kind of apply the SPD approach, what we really want to do is we want to be able to characterize Gaussian processes with a given variagram by an SPD. So we need to kind of understand what kind of characteristics this variagram kind of induces in our Gaussian view. Okay, so the variagram specifically. So, the variagram specifies the joint distribution of the differences. And these can be indeed specified by an SVDE. So, a simple example, which we saw in Marco's talk, is standard branding motion. So, this can be written in terms of an SVDE. This SPDE specifies a deregram. Now, we actually haven't specified the process properly yet because we need to specify the marginal variance at one location. At one location to get this kind of marginal variance function. Okay? But the important thing is for us is that you can specify a variagram by an SV. Okay? And then we have flexibility over the choice of the marginal variance of specific quotation. Okay, so basically what we have is that the brown resonant process is kind of a function of The function of these kind of differences here. Okay, so it's in effect a function of a first-order intrinsic Gaussian process, and these are improper Gaussian processes that are specified by a paragraph. Okay, so informally, we can think of taking the marginal variance to infinity. Okay, so we can think of taking this C to infinity. Basically, what we're doing there is we're kind of Basically what we're doing there is we're kind of washing away all the information that's contained in the marginal distributions. And all we're left is all we're left with is information that's contained in these differences. Okay, and that's going to be a very useful way of thinking about this. Okay, so we can kind of interpret the result we saw in the previous slide in the following way. Brown-resnic processes are in one-to-one correspondence with first-order intrinsic Gaussian processes. That's kind of what we're going to need. That's kind of what we're going to need. Okay, so just like in the kind of SPE case, we're only ever going to observe our Brown-Resnick process at finitely many locations, so we need to understand the behavior of the finite dimensional distributions. As Sebastian mentioned, these are Husslerites, and these are also characterized by a barygram. And the distribution is characterized by density. And I just want to point to this kind of covariance matrix here. Covariance matrix here and give an interpretation. So, this covariance matrix can be given by this sigma k. This sigma k has a probabilistic interpretation. But what you can do is you can take a Gaussian random variable with a particular variagram. And then if you want the sigma k, all you do is you set the marginal variance at location k to zero. Okay, that's how you can interpret this covariance matrix. This covariance matrix. Okay, so this finite dimensional distribution is written in terms of a variagram or a covariance matrix. But really what was important in this SPD approach was to characterize things in terms of a precision. Okay, so Sebastian's already kind of pointed out that we can do this in the Husserized setting. And he defined a Husserized precision matrix. And we can remember this contains all of the kind of conditions. Contains all of the kind of conditional independence in the zeros. I want to give an interpretation of this useless precision matrix. In particular, the way we can interpret it is we can take a Gaussian random variable with a given variaground, and then what we can do is we can take the marginal variance to infinity. And if we take the inverse, we get the Hughes-Leis precision. Okay, so we saw this kind of We saw this kind of condition appearing in Sebastian's talk and Marco's talk. The way that we can interpret this is that if the marginal variance is infinity, then one divided by infinity is zero. Okay, so this is basically what this condition is doing. Okay, now what we were able to do with Brown Wrestling processes, we were able to find a correspondence with first-order intrinsic Gaussian fields. We can do the same thing. We can do the same thing with the Husserlise distribution. In particular, the finite-dimensional distributions of a first-order intrinsic Gaussian field is a first-order intrinsic Gaussian markup random field. And these have been around for many years now and have been very well studied. And these first-order intrinsic Gaussian mark of random fields are characterized by the exact same precision matrix as the keylerites. And in fact, And in fact, people study these with the graphical structure as well. And the graphical structure is the exact same thing as the Husserls. Okay? So, really, a way of interpreting Sebastian's result, at least what's going to be useful in our settings, is for Manuel Henschel and also Joan Seegers, is that the Husserl-Rice distribution and the first order intrinsic Gaussian marker field are in one-to-one correspondence and they have the same graphical dependent structure. So at this point, So at this point, we kind of have everything we need to kind of implement the SPD approach for extremes. And we're going to use these kind of relationships that we've developed. Okay, so we have, to summarize, we have first order intrinsic Gaussian fields, and these are characterized by a varigram, and these can be characterized by an SPE. These have the same parameters as Ram-Resnick process. The finite dimensional distributions of the first order intrinsic field are first order intrinsic Gaussian marker random fields, and these are characterized by precision. And the finite dimensional distributions of the round-resonic processes are user-rice distributions, which are also parameterized by the same precision, and they have the same graphical structure. Okay? All right, so we're ready to do the SPE approach to extremes. So, how are we going to go about doing this? So, how are we going to go about doing this? Well, our goal is going to be to approximate the finite dimensional distribution of first-order intrinsic Gaussian processes with a first-order intrinsic Gaussian marker-branded field that has a sparse precision matrix. Why do we want to do this? Well, we can kind of follow the following structure. We can write down an SPDE that characterizes the first order intrinsic Gaussian. This SPDE is going to characterize Baryground. Going to characterize baryogram, which can then be passed to the Brown-resting process. So the SPDE is going to be what's characterizing our Brown-resting process. Then we can approximate the finite dimensional distributions of our first order intrinsic Gaussian mark of first order intrinsic Gaussian mark of random field with a sparse precision matrix. This sparse precision matrix can then be passed to the Houston-Rice distribution, and then we can kind of encounter. And then we can kind of be confident that this will approximate well the finite dimensional distributions of the Brown resting process. Okay, so this is the kind of structure that we can follow. Okay, so now we can implement this approach. But first we kind of have to kind of recognize one of the reasons why this SPD approach has been so successful, and that's because it works with the maternity prevariance function. So in well. Well, when working with gram-resting processes, the most popular variagram is this variogram that's a constant times h to the v. This came up from Sebastian's talk. And this corresponds to an SVD, this SVD here. So what we do is we kind of consider a generalization of this model. In particular, we consider a nested SPD. So we take the same kind of SPD and we kind of SPDE and we kind of we nested with the Maturna SPDE that we saw earlier. Okay, and then we're going to have to restrict things a little bit to get a sparse approximation. Okay, the next, well, to use this approach, we have to kind of compute the variagram. This is what we've done, and we can get an explicit expression when alpha and beta equals one, and this is going to be an important case for us. We then can kind of analyze the behavior of this variagram. Analyze the behavior of this diagram. So, for small distances, we have kind of quadratic behavior in a lot of cases. And for the large distances, we can have kind of varying behavior. But an important case will be beta equals one, where it increases like a lot. Okay, the next thing that we want to do is we want to be able to approximate the finite dimensional distributions. And this we can kind of use the best. This we can kind of use the SBD approach with the SDD approach. And I should point out that he mentions, like, he has a section on intrinsic deals in his 2001 paper, like a little appendix. Okay. Okay. So we can do this and we get kind of a sparse approximation. So we want this approximation to be good. So that's what we're going to have a look at in the next slide. So in these left figures, So, in these left figures, so I've taken alpha and beta equal to 1. This is one of the cases where we get a smart approximation. This kind of, so on the y-axis, we have the variagram, on the x-axis, we have the distance. So, this kind of solid line is the kind of true variagr, and the points are the variagram from our discrete approximation. Okay, so I've taken different values of kappa, the different values of tau, and I've chosen the values of tau. And I've chosen the values of tau, so at distance by if we get approximately 0.9. So on the right-hand side, what we have is the outcome of a Houston-Rice distribution, and this has been simulated with 10,000 locations. All right, so to further kind of understand the flexibility of this model, what we can do is we can compare, sorry, the flexibility of this model under the condition alpha equal one and beta equal one. Alpha equals one and beta equal one, we can kind of compare with this kind of widely used vary around. And that's because in our nested SPD model, kappa controls the shape, and tau is just something that multiplies the variagram. Okay, and in this widely used variagram, we also have a constant times. So tau is also something that multiplies the variagram. So it's kind of the scale, and beta controls the shape. So we kind of understand what the So, we kind of understand what the scale parameter is doing, but we might be interested in what shapes our variagram can take. Okay, so on the left we have this kind of wide leaves variagram and on the right we have the wide leaves variagram. On the left we have ours. And we can see that it has kind of similar shape. The difference is that ours is perhaps slightly smoother and zero. Okay. The nice thing about using this SPE approach for extremes is that we also get the extensions that Jin added in his 2011 paper. So what we can do is we can incorporate non-stationarity in kappa. We can incorporate non-station deformations. Okay, thanks for your attention. 