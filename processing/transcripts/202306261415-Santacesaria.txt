Okay, so thanks a lot. I mean, to be honest, this is not really about EIT, but let's say EIT is one of the motivations of the whole work. So thanks a lot for Tatiana and Demetri for inviting me to this nice workshop. And so actually the general topic of this talk is trying to apply some machine learning techniques to inverse problems, especially generative models, and try to drive some small theoretical properties to show. Properties to show why they work, at least, I mean, or what we are in mind. So, this is the joint work with Giovanni Alberti, who is here from the University of Genoa, and our joint PhD student, Silvia Schutto, and also with Joanna Certric, who is also here from TU Berlin. So, I'm assuming not everybody here is familiar with generative models. So, I'm trying to summarize the whole talk in generative The whole talk in just one sentence. So, what I call continuous generic neural networks are a machine learning architecture which are able to represent elements of an infinite dimensional Hilbert space and under explicit condition, they provide Lipschitz ability estimate for inverse problems. So, let's try to analyze what are the main keywords of this sentence. So, here I'm Centers. So here I'm talking about generative models that are objects coming from machine learning. And I will explain more in detail later. And so there are some functions that depend on some parameters, they have to be tuned depending on some data set. But differently from useful machine learning architecture, here the output of this function are function as well. Function as well. So, elements of Hilbert space. So, they are objects able to generate infinite dimensional vectors, which are kind of useful when we are modeling inverse problems or Hilbert space or Mach spaces. And then we are able to show that a specific class of this architecture can provide better stability estimates. So, when doing reconstruction for a large class of inverse problems. For a large class of inverse problems. So, after this very synthetic description, let's see one exclusive example of what I'm talking about. Just to summarize in one figure, what could be one example of a continuous generative neural network. So, here I'm presenting a function g, which I would call a generator, which goes from a five-dimensional Euclidean space, a 40-dimensional Euclidean space, a 42. And space are 42 L2 of the square, which could be seen as a kind of space where, like, you have continuous gray images, for example. So, the idea is that this map generates images, but it's not like generating random white noise, but it's generating something that makes sense to a human eyes. For example, in this case, this generator has been trained. function this generator has been trained has been tuned optimized in a way that when you sample some 40 dimensional Gaussian vector you obtain if you do it like in this case like 16 times you obtain different images that look like handwritten digits this of course this famous MNIST dataset and of course this far from being a continuous representation A continuous representation. It's a very low dimension, but you can see this is like a five-dimensional discretization of an image. And so what is the outline of this talk? So I'm going to try to introduce every keyword I mentioned in the first sentence. So something about generative models, something about inverse problems, I want to combine them together and what I mean about subility. Then later we present this architecture in five-dimensional space that is the Space that is the main topic of the talk. And then later, I will also present some follow-up results on when you are learning manifold with multiple charts and non-trivial topologies. So what are generative models? So generative models are coming from machine learning and they're machine learning architecture that are trained with the objective to generate images as similar as possible to those of a specific training set. Those of a specific training set. So there is this famous website, for example, it's called This Person Does Not Exist, where you each time you refresh the page, you have some new photos of a realistic photo of a person, which actually does not exist by definition. And so the idea is that this function, which is normally, I mean, more recently, is a deep architecture. Is a deep architecture, for example, a deep neural network, which have been trained. So the parameter have been optimized in order to generate some images which are as close as possible to a training set of real photos of human faces. And so after you do the training, how you generate new image, normally you have some simple probability distribution in your input space. In your input space, which is also called the latent space. And for example, you sample a Gaussian vector, and then the generator will generate a new image. This is very kind of rough idea what it's doing. And yeah, in this kind of typical diagram, you see, it's kind of as a meaning that you're composing some simple, let's say. Simple, let's say, linear transformation or affine transformation with some non-linearities. And so, but just to make a short history of what happened in the field of generative models in the last, let's say, 10 years, let's say since deep learning and deep network have been mainstream. So, there have been several approach to do genetic modeling. I'm listing here like four major avenues, like Major avenues like there have been variational encoders have been introduced, then generative other networks, and then normalizing flows. And also more recently, let's say the state-of-the-art model or score-based diffusion models. And also, apart from generating images from random Gaussian vector or other random vector, more recently they've been more popular in the news thanks to this text-to-image. Thanks to this text-to-image generator, like.e or mid-journey, where you can just go on this website and type a sentence like describing a dog on this cart with some funny yellow glasses and it will output this interesting photos, very realistic. But anyway, apart from that, why it's interesting, how you can use this thing not just to generate this picture and birthday card and use it for solve inverse problems. And use it for solving inverse problems. So, this is the possible idea. So, let's go back to, so let's go to show some math. So, let's consider a usual setting of inverse problems where you have two manax spaces, X and Y, and F is your forward map, could be non-linear. And you so in the classical like regularization, let's say, approach to solving interposed problem. To solving info's problem, you want to minimize, you have a noisy data f of x dagger plus plus some noise epsilon, and you want to reconstruct y by minimizing a penalized functional. So you have a data fidelity plus functional r and so if you have trained a generator that model your unknown very accur accurately, you can replace this. You can replace this framework in its following way. You can restrict the domain where you optimize directly to the range of your generative model, of your generator G. So you're basically optimizing not on the full space X, but you're optimizing on Z, that belongs to this space capital Z, which hopefully is a lower dimension. A lower dimension, and then you solve this constraint optimization problem. Let's say, so, of course, you can also relax and add some regularization there, but this is already a regularization strategy because you are kind of regular, you are optimizing on, let's say, on the manifold, which is given by the range of your G. Of course, to do that properly, you need to have trained G in a proper way, but so. In a proper way. But why is this interesting for us? Because this is a way to improve the reconstruction also quantitatively using what are called stability estimate. So in inverse problem, we say that we have a stability estimate when we can prove an inequality of this kind. So we can bound the norm of the difference. The norm of the difference of two reconstructed quantities x1 and x2 with some function with some modulus continuity of the norm of the difference of the data associated to x1 and x2. So here, of course, it's a bit, let's see, that's a more special thing than a classical regularization as a result, because here you're assuming that you're the right-hand side, you have exactly, let's say, Have exactly, let's say, clean data, but for like non-linear inverse problems, like electrical impedance tomorrow, it is already very hard to prove this kind of results and it gives a lot of insights how ill-posed the problem are. For example, yeah, for EIT, you have the very kind of bad stability estimate, which are called, which are this logarithmic type stability estimate, which are also connected to what is called exponential instability. To what is called exponential instability. So, because this connect, I mean, I'm not here talking about the precise model of EIT, but these are very severely imposed problems. And when you do like reconstruction, often they result in extremely blurry images. For example, this is something that we did in collaboration with Alan here for EIT when you have a piecewise constant. Have a like piecewise constant conductivity, and the usual reconstruction that you get is still useful, but the high frequency are generally lost in the reconstruction. And while to get like sharper images, you need to impose further constraint. And this often leads to better stability, which are like, for example, lip sheet types. Which are like, for example, Lipschitz-type stability estimates. And then, let's say, is what you would like to have in your inverse problem to kind of ensure a good reconstruction. So, if you, for example, this is another project, some old results on numerical approach to EIT, where we were optimizing over the set of like polygonal inclusions. And in this case, the reconstruction was much more sharp. Was much more sharp. And in this set, we were actually able to prove Lipschitz-type stability. So I'd say one way to improve your reconstruction for several closed problems is to restrict the set of your unknown where you want to optimize or you want to reconstruct to a small subset or sub-manifold where you can prove like Lipschitz type stability. prove like Lipschitz type stability. And how to do that? So with collaboration with Giovanni and a former postdoc, we were able to prove this general result last year that kind of provides you for a very large class of non-linear inverse problem when you can get this kind of Lipschitz type stability estimate. So the idea is that whenever you are Whenever your unknown belongs to a five-dimensional manifold, even in a Banach space, actually in a compact set, subset of a five-dimensional manifold. And you have some injectivity properties for your forward map. So you assume that your F is smooth, and then here is an injectivity of your F and of the Fresher derivative on the tangent space of this manifold. Then you get for free a Lipschitz-type stability estimate, a constant which depends on this compact set and some uniform bound on the Frichet derivative of the Fourier map. So this actually is not a super surprising result. It's something that actually was already known since long time in the case of a five-dimensional subspace. And there have been a lot of actually, I mean, based on this. Lot of actually, I mean, based on this idea, there have been a lot of interesting leapy stability results for EIT or other non-linear inverse problems by many people. And actually, our motivation, our original motivation to prove this kind of results was to extend this scandal litris-type stability for a kind of large class of interesting cases where you're, for example, for EIT, where conductivity belongs to a Belongs to a fine-dimensional manifold and not on a linear subspace. But then we realized that we could use actually this result in connection with the generative models where actually the manifold here, which of course has to be known for in this case, can be learned using machine learning. So, as I said, this theorem works if you know the manifold M, but Manifold M but and in general this is not true. So we did use machine learning, so in particular generative models, to approximate this manifold as the image of a map, of a generator G, which hopefully goes from a low-dimensional set Z to your Markspace X. And then if you're able to do that, And then, if you are able to do that, you have this, you have for free like lip tree stability, and then you have for some more accurate modeling because generally your unknown are not something that can be modeled like analytically. And also your optimization, your reconstruction, your whatever iterative algorithm is, I mean, it's done on a smaller, lower dimensional set. So numerically, it's faster. But for there are a lot of Faster. But of course, there are a lot of kind of open theoretical questions that are trying to address just a tiny bit in this talk. And let's see just one example because, of course, numerically, this has been a lot of work on that that show that this method work. So this is specifically for EIT. There has been this paper in 2019 by Eugene Kunsev and his group, where they compare a classical reconstruction approach for EIT. Construction approach for EIT with some generative model one. So, where you have this in the context of lang imaging, which is a context where it's very one, I mean, where EIT is extremely useful, where you actually don't, you don't really care so much about the precise shape of the boundaries of the lungs, but mostly some more, let's say, qualitative information. So, in the so you have in this first row the ground truth, in the middle row, the In the middle row, what you get using some classical Tikonov reconstruction, and then using this generative model approach, you can separate these two lungs. So it seems to give better result. And that's it. So let's now go into the main topic of the talk. So, what we wanted to do is to extend Extend this architecture for generative models to an infinite-dimensional setting. So, we had a lot of, let's say, candidates to do that. So, here you have to really get your hands dirty and see what people use in practice and choose an architecture which is kind of at the same time expressive, but also you can deal with it. So, we then, of course, you can use some fully connected ones or convolutional ones, or there is some. ones or there is some transformer architecture. More recently, this is very promising. And we chose to use a convolutional architecture and try to, I mean, and extend this to an infinite dimensional setting because it was at the same time kind of expressive, but not, I mean, also we could deal with it from a theoretical point of view. So the general scheme, I mean, let's say the discrete version that we took as an instrument. As an inspiration, looks like that. You start with a kind of low-dimensional space that then is mapped with a fully connected layer to a lot of low-dimensional images. So you have normally like this, for example, is low-resolution images, but with a lot of channels. And then there are a lot of And then there are a lot of like discrete convolutions that at the same time they reduce the number of channels, they increase the resolution. And this you do many times and at the end you end up with a higher resolution image. And so we wanted to extend this idea in order to output an infinidimensional object. Dimensional object. Of course, it's easy to extend the fully connected layer to infinite dimension. You just replace this affine map with something that goes in some whatever Hilbert space. Then the non-linear activation function, you can do it point-wisely. What is also convolution? Of course, you know the convolution is perfectly defined for continuous function, but there is a problem here that these convolutions are not. These convolutions are not these discrete convolutions, they kind of change the dimension, the resolution of the image at each step. So they are what they're called like strided convolution. So they start from a, for example, here you have a four by four image and then they output eight by eight images. So so this is something that we struggle a bit to to understand how to extend this to the infinite dimensional case. So this may be the main technical novelty of this work. Main technical novelty of this work somehow. So, the so what is the what is called strided convolution? So, in the discrete case, it's very simple. You have a vector x and you have your filter, which is normally very short, like I have three dimensional vector. And then you do this color product, and then you kind of shift the filter one by one. But this is the normal conversion. This tried one, for example, with tried two, you're basically. Example, with tried two, you're basically shifting by two elements. And of course, we wanted to extend this to an infinidimensional case. And how to do that? So our idea was to use wavelet multi-resolution analysis. So I'm just giving the very synthetic explanation. So the idea is to kind of represent the resolution of a vector with a scale in a multi-resolution. In a multi-resolution way. So we fix some wavelet and then we fix some initial scale J where we go from our finite dimensional input latent space. And then we perform some continuous convolution that will output some L2 function, for example. And then we kind of we need to We need to go back to another scaling space in order to kind of keep this connection between resolution and in the finite dimensional case. So we need to project this output of this convolution in another scale. And this seems maybe a bit artificial, but in practice, Artificial, but in practice, when you do the computation at the level of wavelet coefficient, you see that this really is basically the same of doing the discrete convolution in a strided discrete convolution. So this seems like the natural extension to infinite dimension of a strided convolution. So in practice, it's very easy to implement because you just do the whole thing on the level of. thing on the on the level of Weibold coefficients and it's uh it's not computationally hard and so after dealing with this so the idea you start with so let's see then how it looks the whole architecture so in the in the discrete case you start with some from some five-dimensional equivalent space then you have this affine layer this fully connected layer that brings you to a to a kind of low resolution To a kind of low resolution space with a lot of channels. Then you have this discrete convolution, and then at the end, you end up after several layers to a high-resolution Euclidean space. So you have this number of channels that are all decreasing, and the resolution, which is these alphas, are increasing. While in the continuous case, you have this fully connected layer at the beginning, which is the same, that brings you to a specific scaling space. Scaling space in this J1 scale with a certain number of channels. And then you have this continuous convolution that brings you to a higher scaling space. And so you really replace the dimension of this discrete Euclidean space with the scale of this multi-resolution analysis. And here you have the same that these channel numbers. These channel numbers decrease at each layer. And in practice, as I said before, after each convolution, you need to do this projection just to be sure that you end up in another scaling space. And then what we were able to prove. So, I mean, the main, I mean, one of the interesting points is that this whole thing is a seriesque invariance because at the end, you invariance because at the end you you end up with an object which is which belongs to L2 so you can decide at the end how much you want to discretize it and then one of the main theoretical points we're able to show is that that under some let's say kind of reasonable assumption we can show we can characterize the class of of these networks Of these networks that are injective. So, just assuming that the first layer is injective and you have some injectivity for the non-interactivation function and some convolutional filters. But why we are interested in injectivity? Because thanks to injectivity, we're able to show that when using this generator in combination with a forward map, with an inverse problem, this will give us a Leipzig type stability estimate. Lipschitz-type stability estimate combined with this previous result that we showed that I presented before. So the idea is that, yeah, using this kind of specific architecture under this injectivity assumption, we have a Lipschitz type stability, I think, so a very stable reconstruction when doing the optimization on the range of this map G. On the range of this map G, so this manifold generated by this generative model. And so we have some very toy numerical examples. So here, this is just a 1D deep learning example. So this color means that the orange curve is the original ground truth. This blue one is the blurred one without noise, and on the right, you have the noisy. Noise and on the right, you have the noisy version, and then we have several reconstructions with several kinds of architecture, which depends on the wavelet that you've chosen. So, in this case, with HAR wavelet, and then with this two-dover wavelet. And we can see that with higher WC wavelets, we have better recognition. Of course, this depends a lot on how smooth your signal is. So, this is just a very kind of Of toy example. And also, we have some more statistical, let's say, measure of how well this network performs. And so here we can see that, so this using this kind of higher WC wavelet, we get some performance that are like comparable to a network that was taken as input. That was taking as input, like the whole high-resolution signal, but which has much more parameters to be tuned for the architecture. So let's say this approach shows that you can really reduce the number of parameters of your nectar using this idea of multi-resolution analysis. And so just to conclude, And so, just to conclude, I want to talk about another project that was in collaboration with Johannes. So, one of the main limitations of this work was that we were training just one map, one generator G. So, let's say the topology of the manifold couldn't be like non-trivial. And so, what we did was to extend, let's say, some of the ideas to the case where you have a manifold with. To the case where you have a manifold with multiple charts. So, this is just one simple example where you have a manifold which is the union of two circles. So, this is what we get when we are trying to learn it with just simple with one generator. And this is what we were able to do with four charts. And I'm just showing here, I'm just seeing very roughly explaining what we did. So, the idea is that we want to That we want to, we have our manifold with an atlas. And so, the main novelty of this approach was that we model each chart with using a compositional operational auto-encoder and normalizing flow. Normalizing flow is mostly to increase the expressiveness of the network. And then, in order to train this. In order to train this network, we needed to introduce a new loss function for this mixture model of vibrational autoencoders. And then after the training is done, we use this to solve inverse problems. And in order to do that, since we are doing optimization, not anymore on a manifold data trivial topologies, we introduced a gradient descent scheme that was kind of Scheme that was kind of working on this learn manifold, so using some Riemann ingredient steps. And of course, this is still something that we tried on some toy problems. And this also, we didn't extend to the infinite-dimensional case. Everything is finite-dimensional, just showing here some numerical results. So, this for just some low-dimensional manifolds. So, I already showed these two circles, then we Certain two circles, and we here you can see an example of like some, let's say, point clouds generated with some from some manifold with some noise. And what we obtained is after the training, these are samples generated from our atlas of these variational encoders, where each color is associated to a different chart. So you can see. So you can see here that these two circles have four charts, this ring has two charts, and these are different topologies. And for example, after the training, so you have available these maps, these networks, so you can even plot each piece of them. For example, in the case of the Taurus, these are, let's say, the six charts that are learned. Six charts that are learned. And so you can visualize them very nicely. And then we also try to use this for inverse problem. This actually, so we tried first with deep learning. So in the case, you have so the manifold here is a one-dimensional manifold of Of images of this rod, and where the parameter is just the angle of rotation of this rod. So the background and the intensity of this rod is fixed. And this is what you get if you learn this manifold with one generator. And this is what you get in these two rows, what you get using two generators. So you see that you can express each angle. Each angle, because of course, this is not a trivial topology, while with one generator, you cannot represent some of the angles. And when doing like de-blurring, so you can see on the first row, like the ground truth, in the second row, the blurred version, and in the third row, reconstruction using one single generator. So you see that actually, in many cases, it fails. fails and in the second in the last row uh the reconstruction using two generators and you see that that it works much better and we were able to quantify that then as a last example that we have tried to apply this also to electrical impedance tomography and in this case the data set is made of like conductivities with with two With two inclusions that are two disks with where the center can change and the radius, the radii also can change, but they cannot overlap. So you create a non-trivial topology. And also in this case, we have some. Okay, here I'm not showing, of course, the data because they are EIT data, they are hard to visualize, but let's say we had some like 50. But let's say we had some like 15 boundary currents. And so in the top row, we have the ground truth, and its second row, reconstruction using just one single generator, and in the last row using two generators. So here, actually, it's harder to see which one works better, but we were able to quantify that on average, using two generators works better than using just one. But of course, EAT. But of course, EIT is a much harder problem than deep learning, and it's expected that things are harder to work. And so I'm just concluding here, showing, just to say that, I mean, there is a lot of open questions for using generative models for inverse problems. I mean, what I mean, I haven't mentioned, but the training here is all done in an unsupervised way. So I think this is actually. Way so I think this is actually a very promising approach because you don't need to. I mean, I mean, after you've done the training, you can actually use this kind of demonstrative reduction to solve also other kind of inverse problems. Of course, it's, I mean, in the thinking about the robustness of this method, of course, you should have a very nice training set to. Set to make it work well, but still you are avoiding a lot of hallucination that you can get with other kinds of training protocols. And of course, there are a lot of improvements that can be done using a better architecture or different training protocols. And of course, we everything I, all the theoretical results I presented, they have been done, I mean, assuming that the Been done, I mean, assuming that the training is finished, and so it will be very interesting to study how the training affects the reconstruction. And so, I thank you for your attention. Thank you, Mateo, for the very interesting talk. Are there questions? Yes. Hi, I have one question. So, you mentioned variational autoencoders. Variational autoencoders. Did you also try it with diffusion models? Because they are very similar in spirit to the variation out encoders. We haven't tried yet. No, no. But do you see any problems? Yeah, I mean, the thing is that in order to extend our theoretical stability result, I mean, we really need a generator G, which is just a map going from a latent space to the Earth. and space to the to the to your image space for example for and in of course i mean for a variational encoder the idea is that you do the training and then you keep the decoder part for diffusion models you have this kind of sampling uh scheme i mean so it's uh you need i mean it's you i mean i don't see clearly how you could combine this uh in this setting probably but every Setting probably, but the variational out encoder is just two differences. So the latent space is smaller, and in a diffusion model, it's the same as the image size. Yeah, also, there is this problem that here we mean. But I don't think it's conceptually a problem, right? And the second thing is that the variational outencoder can be seen as a special case just with one block of the diffusion model. In the variational outencoder, you're adding noise at one point. And in the diffusion model, you do this several times because it's a factorized model. Because it's a factorized model, that's the main difference. No, no, I see, but the thing is that here there is no sampling, there is no, I mean, I wouldn't know how to combine the sampling part of the diffusion process with this kind of, let's say, analytical result. Maybe we could discuss later. Thanks. Yeah, thanks for the interest. Yeah, thanks for the interesting talk. I have a question about your continuous interpretation of striding. Do you think you could also just use just do something similar with Fourier coefficients by just restricting the number of Fourier coefficients you have instead of wavelets? I think it's boss. No, I think it's possible. We haven't thought about that. We wanted to, I mean, in order to get this very kind of clear connection between the discrete stride and a continuous one, we really wanted to use like compactly supported wavelets. And in case of Fourier, you don't get this. But in some sense, you could do it with Fourier, yes. Okay, yeah, thanks. 