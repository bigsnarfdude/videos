Presented here. It's something that I start working together with Michele Benzes, Juana Romale, and also with Tandrea Latale and Riyalinde and David Todeschi in San University in Pennsylvania. So the topics of the presentation, what I want you to remember for this presentation, that I will focus on the optimal transport, where the peak that you are moving are measured, but these measures have a density with respect. Have density with respect to the dimension. So it might be stat-wise, but we are going to have two functions, A and B, the initial and the final configuration of the mass that are two functions that will leave essentially in a one, a two, and three, because then we will move to Pd and then it's difficult to go behind the dimension. Remember, they are non-negative, and they have to have the same total mass. So this is the Proton mass. So, this is what you will see along the presentation. I will present you some PD-based formulation because essentially, from the linear programming program presented by Professor Nonzio, then you can simplify a lot and you will come to really nice and interesting PDE formulation. I will also present something on the concept of bus step distance and some useful application. And then I will move to merits, and in particular to focus on. Merits and in particular to focus on preconditioner for Newton-Bay's method, essentially, when you start to look at second-order information on the functional. And also, if I have time, we'll mention some discretization error problems. What happened when you're moving from the continuous problem into the discrete one? There are some problems. Instant means, check what instant means. But as I said, optimal transport, you transport this into that. You fix a cost, that is the cost of moving one unit of mass from X. Moving one unit of mass from X into Y, and that's your cost. Typically, it's just the distance between the two points, or the gender distance you can even use, or some power of it. And the problem, as presented by Professor Goza, again, is you start discretizing your measure, you fix some point and you gather the mass around that, and then you ask, okay, That and then you ask, okay, I want to find a plan that essentially is telling how much mass is sent from one point to all the other points. And that have some constraints, so this plan, the fact that the sum over the columns is giving you the first discretized version, and the sum over the rows is giving the first discretized tensor view. It's easy to look at here something feasible plan. Just take the tensor product of the two. Just take the tensor product of the two discretized measures. Imagine to project this to summit in one direction into the other. You have this constraint. But you don't want something like this. You want something that minimizes this cost functional, which is just counting how much energy you spend to moving all the mass according to the cost you fix. And then in the intuition by Kantorbits, you have to start from this formulation. Well, one slide more. One slide more. Actually, the optimal solution will be extremely sparse and will essentially condensate on something like this. If you start essentially refining your sampling of your measure, you will see that you will get something like this, more and more. You will see that the optimal transport plan is going to lie on the graph of a map from the domain to sub. This is going to be the optimal transport map, that is something that you have in the o original formulation by Molly. Formulation by Moncher. And actually, Kantorovitz, the most Kantorovic problem, now you see the formulation by Kantorovitch, just the generalization of this linear programming problem. You see that sums, double summation becomes double integral. The projection here in y direction is just the projection of this plan that is now a positive measure in the product space. So just imagine a huge matrix. And what you see here, this is the meaning of the constraint for cost. Then, typical cost, as I say, is a power of the distance to the two points. So here you have this function here. And what is extremely interesting is that the number that you get from this minimization problem will give you something that is a distance for the non-negative density, non-negative measures between A and B. So it gives you an. A and B. So it gives you a number to measure the difference between two measures. And also, by the way, it metric the weak convergence on positive measures for mathematician. And well, something really interesting on the Vasis-Ten distance is its ability to measure, let's say, the spatial displacement between two densities. Imagine here, you have just this one, this wise constant. Just this one, this one's constant measure, and you just translate in here, so just a slice. They have us a width or delta here, and you just translate it by a factor theta. And then you wanted to find a measure, something to measure the difference between them. But since the support do not overlap here, what you have that outside the delta gap, the L2 distance will not be able to Will not be able to tell the difference. For them, let's say to the eyes of the L2 distance, this is exactly the same as this, this configuration. But the Basestein distance, the practical, the Basestein 1 distance, it will be proportional to theta. So it's going to, in other words, L P distance is measuring the vertical difference between measures, pass stand distance is computing the order. Marriage computing the horizontal difference between measure, the spatial displacement, the difference in spatial displacement. And this might be something particularly interesting in some applications, and I will mention this one that I found particularly interesting that I studied in me when I settled. That is this fluid-flower international benchmark study, where they are doing CO2 injection. And where they are doing CO2 injection. So this is the real experiment. They are injecting CO2 here and here. They prepare an experiment where they have sun and then so and they try to say how CO2 dissolves into water. So this is the real experiment. What they did, okay, we give the data, the rate of injection of CO2, the confirmability here of the sand, to several groups of studies around the world. I think there are 11 in total. And they say, try to simulate that and try to, and then they will. Simulate that and try to, and then we want to compare the result we get just from different numerical methods because they already get the best data possible looking at this experiment. Here I'm just showing you the result that you get of the CO2s from evolution after 24 hours, I think. You see that now you have to try to compare the reality against the simulation, and it's important to be able to measure what is good and what is wrong. What is good and what is wrong. Now you can use the eyeball norm and say, okay, well, this looks good, this seems sort of the best compared to this one, but then it's going to be difficult if we move to 3D. So it's extremely difficult. So maybe if you're really interested in comparing spatial distribution of the obensive, in particular where this CO2 has gone, the busest time distance might be a useful tool and actually they use it and essentially fits with the eyeball model. With the eyeball model. So, something cool, I think, of multi-defense transport. Another thing, so when you start using buses and distance, is for example in this inverse problem. Because this is the prototype of an inverse problem. Imagine you have a forward model, you have some observation, but your forward model is somehow determined by some function, some parameter S, and then you have you want to minimize the discrepancy or loss function in machine learning. Loss function in machine learning between observation and your forward model. And in particular, these people here in Grenoble were focusing on the inversion of full waveform. They wanted to get some information of the velocity of sound, essentially, in the rocks. Using typical, you just have a source here, and then you have a receiver here, and then the receiver. Here, and then you, at the receiver, you just capture the waves around me. And then, using this information, you can try to extract the exact velocity field down the ground. And what you see in this time of experiment on the receiver, if you have some error, you're gonna have a delay in your waves arriving. And again, it's horizontal displayance. And so, they thought maybe we can try to use the bus. Maybe we can try to use the busest time distance because it's able to capture this distance. And here I report an example. Here the exact velocity. Here is the initial guess. Essentially, if you use as a discrepancy measure here the L2 distance and your initial guess is too far away from the exact solution, then the reconstruction does not work. Well, if you use the last time distance, it's going to be more Distance is going to be this more robust essential. So, one cool feature of bus extent distance applied to inverse problem. What is important that if you want to use the bus stain distance is not as easy as L2 distance to minimize this because as long as you want to do something with this minimization problem, okay, it died. In each initial problem here, you're gonna have to compute some gradient. Have to compute some gradient with respect to the first argument here. And so, what is the variation with respect to the first argument of the bus system one distance? You have to solve a PD. And remind me to point it out where this PDE will come. So, now let's move to PDEs. I will first focus on the old L2 optimal transfer problem. Have two optimal transfer problems. I mean, the optimal transfer problem when the cost is the distance squared coefficient and the L2 square. As an example, here, just imagine that you are taking A, a piecewise constant, just translating it into B. What is interesting of this formulation is that it can rewritten in a purely PD constraint problem, where you are looking for a density row that is. A density rho that is very important both in time and in space, is a positive density. You're looking for a velocity field V, and essentially you have a constraint that you have a continuity equation for the density and the velocity, so you're just moving the density along the velocity. You have the constraint of the density and the initial and in the final configuration, here A and D. And what you want to minimize you want to find the combination of density and velocity. Combination of density and velocity that is minimizing the total kinetic energy, giving that the mass and giving the velocity square. So, this is the so-called Benamou-Brenier formulation that is particularly nice because also give you a nice way to interpolate density, positive density. Here below, I report the evolution from one to the other. As you can see, it's a sort of translation, but there is also a sort. Okay, a sort of translation. Okay, this is cool. And what is also interesting is that you can essentially write a pure PD formulation for this optimization problem. Essentially, you will see that the velocity field is going to be given by a gradient in space of a potential, phi, here, and the PD describing the density row and the And the potential phi is just okay. Here you still have the continuity equation. Now instead of the velocity, you have the gradient of the potential. And here we have some PD constraint on, it's a sort of Emmett-Jacobi equation. Here you have an inequality and here you have an equality where the density is strictly positive. It's a bit difficult to essentially solve this because you have this constraint is really difficult to handle. Constraint is really difficult to handle. One way is also to start introducing a slackness variable to just divide where this is going to be strictly positive or zero. And then the final PD I will present you is this one. We have three unknowns, the density, the potential, the slackness variable. Here we have the new terms of the equation and the complementary condition between the density and this lackness variable and the constant equation. Variable and the content equation. This is the PDE that I want to solve. So, how are we going to do it? Well, we have first to discretize in time and in space. For discretization time, let's say everybody's using staggered solutions that are discretization that are staggered in time. For the discretization space, you can most people use, for example, finance problem and Problem and I will present some of that. So I will not focus that much on discretization. But imagine that you're able then to discretize this PDE. Then what about efficient argument to solve this highly non-linear question? Well, most of people, in particular in the first paper of when I'm reality, we're doing something like this, they will start using first-order algorithms in the sense that they just look at these. Sense that they just look at these and they probably optimizing one variable, moving to the other, all the methods mentioned by Professor Janssen before, just looking at the first derivative of our optimization problem. But I will pick the fight on going towards second-order method because I think that there is space, and in particular in this problem, there is space to have efficient solidarity based on second-order method. On second order method. So, once again, just to be sure that you understand, we have a potential here that is varying in time and space. We have the density and the row that are, again, varying in time and space. As you see, that have to be positive, so at least non-negative, and then we have on CPE. So, after the previous talk, you might maybe guess how we are able, we might think or end on the fact that we have the concrete. Think or end of the fact that we have the constraint of positivity here. Well, introduce a logarithmic barrier, there is a regularization, and this regularization is going to go here in the complemented condition. And then what you're going to do is, okay, this is just going to be just a continuation algorithm. The idea of the zero-point method, solving this PDE and gradually reducing the relaxation parameter. So let's move into the hard part of The hard part of when you're trying to use Newton method, okay, you have to invert the Jacobian. How this the Jacobian looks like. By the way, in the solution, what we did in the Kiro point method, we did not really use all the techniques like predictor corrector, particular tuning on the relative tolerance in the linear software. We were just focused on understanding the structure of the Jacobian and to find efficient conditioner. So the Jacobian looks. So the zoopian looks like this. I hope I think those that work on interior point metal recognize the structure like A, B, and then these two diagonal metrics for the slackness condition that comes from different ADPs. And I want you to focus on essentially the operator involved in this model. Here, the matrix A is just a weighted Laplacian with respect to the density rho. The operator B is just a sort of The operator B is just a sort of a faction operator. Maybe you can also look at B transport that is maybe more recognizable. And we have to invert this. So how to do it? Well, the first step that is typical... Ah, yes. So how tricky times? Is with the implicit time step, I will present the matrix at all. You will see the structure there. There. So it's not a space-time discretization? Yeah, yeah, you're discretizing space-and-time D with an all-at-once approach. And then you have this system. You will see the block structure of the matrix here. I will plot it. And first things I just reduce to just to the phi and the density variable because it's Variable because it's the first thing you can do on this type of system. It would be nice to use the double sub-poise system mentioned by Profession T, but the first step is to understand the dual-shoot component. And I just rewrote again and then I present one example with when I use just three time steps. This is the structure of the linear system I have to solve. As you can see here, there is the Laplace and There is the Laplace where we wait just between these. Here we have a second time step and a third time step. So, this is how time you see how the effect of having discretized with normal ones discretization. And each block corresponds to one time steps. I think that I hope that is more clear now. Is it okay? So, in general, we have many more blocks. Blocks and we are using the implicit timer. Yes. Here, I think you recognize the discretization of the time derivative when the other component from the invection part. So it becomes really huge. As you can see, the discrete net is a T plus space plus time description solver. The number of unknowns in 3D, you see, it's going to explode. So this is the So this is what we have to face with. And then we have to find an efficient precondition, because these are sparse, there are a lot of differential operators involved. So what does it mean preconditioner? I think I want that to present it here. Just to find something that approximates the inverse of the Jacobian that it's easy to compute. And when you are facing this subtle point system like this, where I think that there are many. I think that there are mainly two approaches. One is looking at the primal problem. Essentially, you reduce with respect to the variable delta rho and you just solve with respect to delta of E. Essentially, all precondition is going to be the fundamental component of this precondition is going to be this primal shoe complement that you see written here. And the other component is just the inverse of the metric C. That is really nice too. That is really nice to, because it's diagonal, it's relatively easy, but it's not so nice because at the very end of the interior point step, it's going to be something like the relaxation parameter divided by rho. And here comes the problem, the fact that this minus zero, so it's going to be tricky. And in particular, if you look at the structure of the primal shook complement, it's going to be essentially a Laplacian in the space-time. Laplacian in the space-time, and you're gonna have this is the spatial part, and this is the temporal part, and you're gonna also have some time-space component. But the difficult part is that the fact that essentially you have a weighted Laplacian, but also here you have another weighted Laplacian divided by the relaxation parameter multiplied by an anisotropy. So the RN is autopree as long as you decrease the relaxation parameter is going to explode. So it's going to be particularly difficult to angle. Even this is a weighted Labrasian system is in the time space and using some one particularly good multi-grid solver by Notai, the A algebraic aggregation based multi-grid approach, that is poorly algebraic but is particularly powerful for this. Can I ask about the? So, is there no concern about the primal sure complement maybe having a bad balance between the A and the other term because of the dependence on the small file? So, is that not a concern for inversion latency? Yes, it isn't a complex a problem because this is gonna be dominant, just uh the this anisotropy part. Okay, so that's really what I'm saying. So, this is you can look at it even at the You can look at it even at another. So essentially, you can do it, you can use it maybe from the highest value of the relaxation parameter, but I think that it's not going to be the case. I mean, at the very end, with large problem and low relaxation, it's not the efficient way. The solver takes too much time. So, the other approach when you are facing subtle points like this is, let's say, to move to the dual shoot component, where the component Where the component of your preconditioner is going to be the inverse of the matrix A, that is super nice because it's just a chain of weighted Laplacian systems. Multi-grid solver will be able to solve it extremely efficiently. But the problem is that the dual-shoot complement has this structure. And if you recognize here, you have the inverse of the inverse of Laplacian. So it's not it's tense. It's tense. It's nothing nice about that. And if you think, well, maybe I can try to use the diagonal of the Laplacian, well, it's not going to capture the nature of the operator of that kind. So it's particularly difficult to try to form the short complement and then invert it. So what is a possible way? Well, you start looking at the work by Professor Elma and Sybest, and you say, well, And Sybasta, and you say, well, maybe you can try to use some least square preconditioner, the one proposed for Navistox and other problems, but it did not work. But nevertheless, we thought maybe we can try to have a look if there is some commutation between the operators that are involved here. Why? Because matrix do not commute, but derivative do commute. And essentially, after a lot of work, we realized that there exists. We realized that there exists this expression of differential operation between B transpose and D. It's not commutation, there is a minus, there is a transpose, but you get the point, I think. And what is important, that the extra temp that are appearing, here you have a diffusion where the coefficient here is just the continuity equation. So we thought, well, maybe along the Newton iteration, this is going to remain rather small or not. We make rather small or not too big, and we say, okay, remove it. And the second term, yeah, but at the end of the Newton method, it's going to be small because it has to be zero. And the second part is something where it involves the second derivative of the potential. And where we look in an optimal transfer, we say, well, this is going to be small, but it is just difficult to handle. It's it's small, you can prove in some cases it's sort of small, but it's better to forget it. But it's better to forget it. Starting from this approximate expression, you can then multiply both sides by b A minus one and R C. Well, on the left side you will get actually the Schwarzworld complement. While here you get something like this, this approximate inversion. Then you keep doing simple operation because then you wanted to find the inverse of the dual short complement. You do this, you do this, and then. And then you add the bees, it's feasible, and then you came up with something like this. Essentially, if this approximate expression holds, then you can rewrite the inverse of the dual shoot complement as the product of this weighted ablacian and the inverse of this other sparse, because it's sparse, as you can see, it's just the product of sparse operator, sparse matrices. Actually, here again, Actually, here again the multi-grid solver mind.i was doing a really good job in just focusing on the version of this operator, so we were fine. And so, using this operator, now I'm going to present some results. Excuse me. So, B square C B square. It's a bit strange. It's actually you're right. I have to. It's not exactly B, it's like B tilde. I have to wait essentially to add some. To add some average in time and in space to fit the dimension, but it's feasible. The important thing that in this averaging, I'm sort of respecting the differential nature of the time. It's feasible. It was not about feasibility, it was about dimensions of the matrices. So B is square. P, the matrix B, is rectangular or square? No, it's rectangular, it's rectangular. No, it's rectangular. It's rectangular. So B times B is what? No, no, actually, as I say, it's not in the actual precondition, it's not going to be B exactly, it's going to be a square version of the B operator. The important thing is that this P tilde operator is going to be something that looks like this differential operator. So I have to fit the dimension, but it's important that at the very end the B operator. That at the very end, the B operator is like to square what it takes and what it puts out. Important things is to look at the differential nature of what B is, and it's possible to do this stuff. The example I was mentioning here, so here on the left column, you see the value of the relaxation parameter as it's reduced. Each column here is going to represent an Each column here is going to represent an experiment. And then, moving left to the right, I'm going to refine both in time and in space. You see how the number of degrees of freedom in this 2D problem is quantified by a factor 8. Okay, it's natural. And here I'm reporting the average number of outer iteration per linear system. Okay, this is one word. Essentially, at each value of the regulator parameter, The regularization parameter, there is a Newton system to be solved. There are typically it was between three, four, five linear system to Newton iterations. So the same number of linear systems. And what is particularly important that using this approach, the global number of Newton iteration was approximately constant irrespectively to the number of to the To the number of discretizations we were considering. Let's say a total of between 30 and 50 linear systems at the very end. So, and here I'm just reporting the average number of outer iterations using the preconditional dimension within a flexible GMRS approach. So, as you can see, the number of alternate data, the average number, is unfortunately growing with the relaxing the Relaxing the regularization parameter. But the important things regarding this approach is the fact that the number of iteration refining the problem is not exploding, it's not doubling every time we refine both intact and space. So and actually, if you just, let's say, you focus just on more relaxed tolerance, like n to the minus phi, you see that the evolution is. See that the evolution is good. I mean, with respect to first-order method, it's way better. And also, you can see more or less the same evolution for the CPU time. It's growing really fast, but I think that is we might say that the CO the C the C the CPU time and is essentially growing a little bit more than uh A little bit more than linear with respect to the number of dispersal freedom. Unfortunately, we lose efficient when the density we are transporting of compact support and when the relaxation is really low. It's something that we are not really satisfied, but this is the best thing we could do with this preconditioner, we call it. So how much time do we have left? Two minutes. Two minutes. Okay. Okay, well essentially just to summarize, this is again the system that I will try to solve for when the cost is quadratic, the Penamou Bernier formulation here and this dynamic formulation. What is nice is the fact that if you consider the case where the cost is just the distance, not the squared distance, Not the square distance. You have another non-linear system of PDE, that now I throw you out this. You have a density row and a potential phi again, and then also a slackness variable here. But here, the time dependencies disappear. And this problem is you can essentially use the same approach, relaxing with Interopoint method, and try to get the same solution. Here I reported the optimal solution for the Optimal solution for this transported density. What I'm plotting here is the density row. It's growing here, staying constant, and then increasing. And what is important is essentially that there are a lot of similarities between the linear algebra problem connected between this P D and this P D, because as I said you have at the very end to solve this several point problem with this operator involved, you have the trick that I mentioned before. Here, the trick that I mentioned before. And so we say, well, maybe we can try to use the same for this problem, where the operator now becomes these ones. But unfortunately, the trick on the computator and deleting some part did not work. Essentially, this part, there is no negligible part in this equation reported in red. And so we have somehow to be forced to go back to the primal approach and well, is working in some sense. Well, it's working in some setting, but we were not really satisfied. We really would like to add something that is using the dual component of this other point system. So I have no time to go to the instability issues. Well, there are instability issues for both problem check and board instabilities. But maybe if you want to pay me some questions about it, I will go for that. And to conclude, this is the thing. This is I think the summary of the this talk. I think you share a lot more a lot of view of this story. Okay to finish like this with lots of questions because I have lots of chances to talk to people who will tell you something that you have some good advice about. About what I'm saying. I'm here asking help for this. This is perfect, okay? So well done. We won't take questions, I don't think, here, because what I'd like to suggest is we have a short lunch break now, a short coffee break, because we only have one more talk today. We're going to finish early. So there's plenty of time for discussion before lunch on what's going to happen with this. So we have a short coffee break. Talk to the speaker there. We come back at 10. We come back at um ten thirty and I think we'll find this finished again this morning. I'm sorry, but I have a hard time. I think we'll have a lot of time. I think that they also have the explaining the simplest person. I'm assuming that these are signed into the test. No, no, no, your cost functional the transport rates. I just took sikh and see that you have in America. And you see that you have an access to the colour. Yeah, yeah, and then your node X is going to be the name of the number of, which is very important. Okay, the last short of five explained that so uh a lot of course are for the case. So there are all these I think we can support how multiple reports for the case of the centers, I think. Force going to basically do that the same thing before I does the kind of label, which we started. But in the multiple single products, I think it would just be usually the thing that they could use. So you see the pattern works forever in the HTC. Right, anyway. However, if we submit it, we're fine if you're something. Yeah, yeah, it's I think I want to start T30 so it's a thing like that. I thought it's not weird because it's shortly false. And from that, as a one is done, as the make this even more confidence for the problem. So that's what's easy to do, but all the means they're not just on the back of the side. I try to improve that. I don't think I can actually improve that if the condition is usually affected resistances. Not only another type of randomized file. So last month this one was covered. But in any case, I want to go out to generate a short file so you see so again I mean to read the paper, but when you said you see this price and stuff. I was wondering if you most probably play a measure of the other side.