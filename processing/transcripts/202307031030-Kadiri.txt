Okay, yes. So, yes, I'm Habiba Kateri, and we're starting that first lecture, so we're 60 minutes. I will need some reminder about the time. Talking about zero-free regions close to the real axis, so I'm just going to look at the case of Diric Layout functions today. Yes, what I'm projecting. Yes, what I'm projecting here, I've sent you the email of the slides, so you can just follow through that and take notes on that. Okay. So in the introduction, and so that's why, I mean, there's like 20 so pages. This is not, I'm not going to read through everything. I just want to at least give you what, you know, the highlights or what you should, what I'm. The highlights or what you should, what I'm hoping you get out of this hour. So, to start with, this is the goal of today to prove a region of this shape. So, you consider LS chi a directly character mode q, and you want to prove there exists a constant r positive, such that ls chi does not vanish in this region where real part is bigger than one minus one over r log q and the imaginary part is less than one, at the exception of at most one single simple. Exception of at most one single simple zero in the case where you have a real character. And what we're going to show here is proving that for r equals 35, which will match what is known for the Riemann zeta function at the time of Dola Vallipuso. So in the first section, there's a series of historical data on such explicit results. I have included what happened to the Riemann Zeta. What happened to the Riemann Zeta function? And if you want to hear a little bit, I'm sure it's not exhaustive. For the simple region, between the last time I talked about that, for instance, there was CRG launch. There's already three papers that have been put on the topic. So I'm trying to keep up. And there's some people in the room physically online that I think are involved in all these projects. I mean, I know are involved in all these projects. But so you know, what we're looking at. But so you know, what we're looking at is a classical form of a zero-free region, which is best for the smaller imaginary parts. Okay, so I mean, for the Riemann zeta function, you have a verification, it's been verified numerically up to a value of 3, 10 to the 12. And Tim is here to maybe give me the exact value up to where it's been verified. After that, you know that there's no zeros if you go up to a height of exponential 200 and so. After that, there is actually a After that, there is actually a little wood zero-free region of this shape of log log imaginary part divided by the log of imaginary part, which becomes better up to a value of like 500, about 510,000. And as I said, these are all very, this is the first, to my knowledge, explicit result due to Jang here. Then you also have after that for larger tides, Korobov Vinograd of the Rufi region, which was first proven, I think, by Ford. First proven, I think, by Ford. And then again, very last results by, I'm gonna say, the team of the team of team. I'm good. That joke. Anyway, with Chiara Bellotti here, who just put out some result about that. Okay, that's what happens for zeta. The thing with L functions is that, unfortunately, we expect zeros much lower than the 14. Zeros much lower than the 14-point something where the first zero of zeta occur, right? So we have to pay attention to the low-lying zeros, even if we expect that there's not going to be any vanishing on the real line. And so here again, I just highlight for you a little bit of history. There's much less explicit result on the topic. So I'm actually including a little bit of results when you assume Q is asymptotically large. Hume, Q is asymptotically large, referring to Heath Brown here because I want to actually mention a little bit of his work that hopefully will become explicit. That's one of the projects this summer. And why do we care about low-line zeros? Well, because they have a prime importance when you're looking at what happens for the primes in arithmetic progression. So the core of error term, when you look at the current Core of the error term, when you look at the prime number theorem in arithmetic progression, is going to arise from those low-lying zeros. So that's why you really want to focus our attention on that when we want to look into applications like the prime number theorem in arithmetic progression or looking at the least prime in arithmetic progression with Linux theorem and so forth. And once you have a good understanding of that, then you can start playing with generalization. So for So, for instance, Dedekind Zeta function is a natural one, Hekl functions, and so forth. So, I'll let you browse through that. And I'll go into the first tool that we need to prove a zero-free region, which is an explicit inequality. Namely, what you need is to have a way to express your logarithmic derivative of zeta of a Dirac Lauff function in terms of its. In terms of its singularity. So it's pole for zeta and its zeros for the L function. So these are the equations that you see here. And you can think of the gamma term here that arise as essentially like the contribution of trivial zeros. And the second sum here that you see, this sum here is of non-trivial zeros of zeta. So we're looking at real parts, which removes that B of chi term here and just gives you a nice formula. This is what we're going to be playing with for today. So this type of formula can be referred as global formula just for the reason that here you're looking at a sum of all the zeros of zeta here. Octzeta here. So we start from here, and what we're going to do, well, first, I'm going to tell you some variations about that, and then I'm going to tell you how we use that. The first variation is a Stetchkin device. So Stetchkin published this article in 1970, and his idea was like instead of just looking at the L function at a point. Looking at the L function at a point sigma plus i t or s plus i t, we're going to look at s1 plus i t. So the idea is that s1 is actually further on the right. So this is your sigma, this would be your sigma 1. That sigma 1 is actually way past, sorry, way past one. And just for sake of simplicity today, and because I want a lot of things to converge and exist like that. And exist like that. I'm going to assume sigma is bigger than one. Okay. So when sigma here is further on the right, that term here, you have to understand, is essentially like a constant term or a big O of log Q term in this case. Now, this is a term that we're removing with a certain factor of kappa. So what So what I'm going to, when I'm going to look at the L function at various points, what happens to this term ft chi is actually going to be essentially the first term, but a little bit smaller. It's going to be essentially this, but times the smaller constant than one. That's what's going to happen here. I need to go back to I need to go back to this page. I need to tell you one thing that I think I should mention here. When you look at this explicit formula, every single term in the formula you can imagine you can understand it and bound it, except this last term here, which seems an infinite sum over zeros of zeta or L. We can actually easily get rid of it by taking sigma bigger than one. So if you take your real part of S bigger than the real part of S. Real part of S bigger than the real part of all the zeros. All this is just, well, if I don't have the minus, it's just non-negative. And all this you can just forget if you want the whole sum. Okay, that's something we can do. That's something that we will do in most cases. When we introduce Teshkin, we are left now with this sum over the zeros here. And it is not clear that it is still non-negative. non-positive um non-negative and so that's uh the the the the what stechkin proposes here is that this actually term when you sum over all the zeros is going to be non-negative whether i'm looking at uh l function or zeta function this is going to be all negative for the choice of kappa and Choice of kappa, an S1 that is carefully made and which is explained here. Okay, I'm not going into the details of how Stechkin is proven. I just want to tell you that there is a way to look at this explicit inequality this way. So, well, I told you, well, we can bound the gamma term, and that's actually part of the problems that you could take home with you. We have Steering Formula. We have Stirling formula, and you can from for log gamma z, you can deduce from for it something about the real part of real of gamma prime over gamma, and you can have some explicit bound for that. And so once you have that, this type of explicit bound like here, then you can entirely understand, to entirely give a bound for your logarithmic derivative of zeta or dual function. Okay. You can even have You can even have it when you are looking at the Stetchkin trick. Okay, so when you have a kappa and the S1, you also can have an explicit bound. And the type that, as I said, we're focusing on here as those ones where T is less than one. So you have to understand all this is just really for now, it's all a big O of one. The point of explicit is going to make all this, you know, constant have values and so forth, but you should just understand this gamma prime of a gamma. Just understand this gamma prime over gamma here is just a big O of one in our case. Okay, the third type of inequality that I want to mention here is what I'm going to refer as a local formula. So, and this is due to this example, this lemma I'm giving you here is due to Heath Brown. So, in his 1992 paper where he proves a constant for Linux A constant for Linux theorem. What he goes through is he provides a Burgess bound for character sum. I mean, he uses a Burgess bound for character sums. And from it, you can deduce an upper bound for L sigma t when sigma is very, very close to one. So you can get a bound of this type when sigma is essentially one here. So you can get this type of convexity bound. What is interesting is the power that you get up here. thing is the power that you get up here. It's a very small power of one-quarter, one-third, which is better than what you would get through Polyavinodradov, where you would get just essentially root Q. So what you deduce from this bound for the L function now is a direct bound for a real prime of L prime of L in terms of the zeros located close to one, or one plus it, at a very small distance delta here. Small distance delta here. So that's where the name local is coming from. What is interesting is that in that case, there's only a phi over two contribution for log Q that comes up here. So if I were to compare with the previous, with the global one that we had, you just had So, for all the zeros, rho part one over s minus rho plus a half of log q. And my gamma term was a big O of one. I'm just going to put as a little O of log Q since log Q is what I'm going to be referring to here. So, just to point out here, you can see a tremendous improvement on the contribution of log Q here from his. of log q here from heath brown to what we get through a classic um um a classic um um classic exclusive formula this way so i mean at the end of the day i told you well you know our zeros we can just discard them because they're not nit because all this sum here is you know bounded above by zero um so i did it why are we not using uh heathbrown well heathbron used it Well, Heathbron used it. He proved an excellent zero-free region, but that's valid. All this is valid when Q is sufficiently large. And it's not until the work of Enrique Trevigno that we have actual explicit burgess bound. So valid for as small as we want cues, essentially. I'm gonna, all this is like, you know, time things I don't have the time really to go over are like homeworks for everyone. But this is just to let, you know, all these formulas that I've proved, that I've established so far are for the zeta function and for primitive characters. There is just a little transition to do from the principal character here to the zeta function, and all you're losing is going to be of size. Be of size big O log log q since we're taking sigma bigger than one, okay, which is again a little low of log q. So when we when I'm looking at those explicit inequalities, I can think of them all the same whether I'm handling a primitive or non-primitive character or a principal character or not. So this is the same when I go from primitive to non-primitive. From primitive to non non-primitive to primitive. So, again, here, this is a little law of log Q. So, we have settled, we can settle easily this case here. And so, to summarize all I've said, I've gone fast a little bit, maybe, I don't know. I have no clue if all this is well known by everyone or not. But what you have is you have is um essentially these two formulas here 26 and 27 and these are the ones that you should we we are going that you need to have when you do an um when you want to investigate zero free regions so when you have a principal character you bound your logarithmic derivative of l and you take your real part you would have the contribution from the pole here that real part of one over sigma plus i t minus one so this here is coming Okay, so this here is coming from the pole of Zeta. Okay, because here were principal. And when you have any character, primitive or not, then you have to consider you have an inequality of a form, a constant over two log Q minus a sum over zeros. Zeros here that ZA is a set of. I'm just using that notation to describe if I'm looking at all the zeros, I'm looking at the global formula, then my ZA is all the zeros. Okay, so it's that these two cases, these first two cases here. And the first classical one gives of constant of a over 2 of a half. If I play with Stetshting trick, we gain a little. Trick, we gain a little bit, a value instead of a half, we go down to 027 for the contribution of log q. And if we are going through a local formula, so with Heathbrown here, then you are you restrict the area of zeros close to a point close to the vertical one-line. And in this case, though, you can get a there's. Get of there should be this is a typo here. This is going to be one six or one eight, so like you can go down to 0.125. Okay. Yeah. Would they use the global formula versus the local formula? The global formula is valid for any modular skew. And for explicit result, that's what we Result, that's what we like to have. The second one is valid for Q sufficiently large. So I do not know what zero-free region I could get, let's say, for Q equals 10,000 with that. I'm mentioning it because we are getting to the point where we could get some explicit result. And also because for the motivation to go into that, because the contribution of log Q is so much better that way. And that's what I'm going to show you now how it. I'm going to show you now how it's that coefficient here entirely determines the size of your zero-free region. So this here is directly related to the size of the zero-free region. So that was our first step to obtain such explicit inequality. I am. Where did I put my s? Where did I put my sorry? I'm just skipping. I'm looking for a slide here, which I don't see I have. Yeah, okay. I had, well, that was bound to happen. I have three different versions of my slides here. So, but I believe there's an extra version where I'm talking about adding a smoothing weight. So sorry for overskipping here. But instead of looking at L prime over L. instead of looking at L prime over L, another idea, which is also from Heath Brown, is to replace the L function by a function that is going to smooth it. So we include a weight into L prime over L. And in that case, what happens is that what you get on the right is a formula that is similar to this, except that you have better function that what is essentially Better function that what is essentially here variations on one over s. So you have a smoothing of one over s. And in that case, you have some flexibility on the on the choice of sigma here. So I think I'm just talking to people who know what I'm talking about. So I'm sorry, that would have been easier maybe with a slide here. I was just to say there is a way to make this more complicated. That's essentially. Complicated. That's essentially what I'm saying. Okay, so once you have that, the idea is to compare your L-function at different point on the vertical one line. So, and that's what a trigonometric polynomial does. So you imagine that your one line is here. You're going to take a sigma very close to one on the right, just so that your L prime over L sigma converges. Converges. And you imagine here you have your zero potentially that you're trying to locate. So here there's a zero of real part beta and imaginary part gamma. So imagine you have a zero like that. I'll not put it that high. I want to go higher. And the idea is to look at what is happening. Look at what is happening here. There. So if you want, at sigma, at sigma plus i gamma, and then even higher. So you go at sigma plus 2i gamma. And higher if you can. Okay. In this case, I'm just going to now follow this example where I'm going to just stop at 2 times eigen. Stop at 2 times i gamma. What is happening in that case? We're looking at those, what happens at this point for essentially here for zeta, here for the L function LS chi, and then and then here for L S Chi star Kai For L s chi star, chi square. Okay, and then if you continue higher, sigma plus i k times gamma, you would look at what's happening at this point for ls chi to the k. So what is happening at this point? Well, when I'm looking at the first one, sigma, I am actually very close to the pole here of zeta. Okay, so that's one thing that happens. Let me put some color here. I'm trying to put a little distance. Color here. I'm trying to put a little distance here. When I'm here, I'm very at sigma plus i gamma, I'm very close to the zero. Okay. And when I'm higher, well, there could be a zero or not, but that's actually not so much relevant. At those points, I'm essentially going to be, I can think of it, I'm going to be at most a big O of a big O of one here. So what happens is you were going to put some weights on the contribution of zeta at sigma and on the of zeta at sigma and on the contribution of LS chi at sigma plus i gamma. And those contributions, we're going to balance them so that that zero cannot be too close to that pole, okay? Because we have a pole here, so things are supposed to become extremely big, essentially. So we don't imagine to have a zero very close to that. So that's what the trigonometric inequality does. So namely, we start with something of this shape. I'm going to just focus on the This shape. I'm going to just focus on the example: three plus, I mean, what two times one plus cos theta square, which when you linearize it, is like three plus four cos theta plus cos theta. And that three here, that three with theta equals zero, this is going to be your contribution for theta. Here, that theta is going to be, so theta at sigma. Here it's going to be L sigma plus i gamma, and that one here is going. gamma and that one here is going to give me a weight for L at sigma plus 2i gamma sorry chi square and I here forget my chi okay or if you prefer that formula that is down here you can translate this trinogonometric inequality into an inequality for the real part of L prime over L with the coefficients of this polynomial. I've left that as a little exercise. I've left that as a little exercise. It's something if you have done Zorofer Region for Zeta, that you would have seen otherwise. Yeah. So what I want to say here is that we're starting from this inequality. So three times L prime of L at sigma plus four times at sigma plus it for gamma plus one time at sigma plus two side t plus at chi square. This is non-negative. On the other hand, in the first part, I've shown you some in Part, I've shown you some inequalities for each of these L-terms. Okay, so this is the this is this is all the ingredients essentially that you need to prove zero-free region. The next page is just showing you those some examples of trigonometric polynomial because one thing that made things work, sorry, let me just skip back is that you can notice my coefficients are all non-negative. Coefficients are all non-negative, three, four, and one. Okay, and that the polynomial that we get is a square, so it's non-negative. Okay, so these are two information that are crucial, which I have told you here. Okay, so finding such polynomial is not an easy feast, okay? And the next part was showing you what when you look for them, that could look like. So, this is finding polynomials of this sort. Okay, you're going to usually take squares of one degree trig polynomials, if you want. I'm giving you example of degree two, that's the first one, of degree four, the second one, and the last one is the degree 16. We will have to, we have to add some conditions which are going to come on how good a Zurfi region we want to obtain. So, just to let you know. So, just to let you know where these crazy numbers are coming from. Okay, as I said, we have a two main ingredients, and now we're ready to cook. So, we're going to prove this theorem. How much time do I have left? Half an hour? Okay. 32 minutes. Okay. Okay, so I'm repeating those explicit. I'm repeating the explicit inequalities that we got earlier, 26 and 27. I've put them in their simplest version, and the ones that we're going to use to do this theorem. I've given you three different versions of them. This is the one we're using. So with the one half for the contribution of log q and looking at all the zeros of the L function here, this is what happens when you I look at What happens when you look at the first one is when we look at the principal character, so real part of one over sigma plus i t minus one plus a little of log q. When we look at any character, you have that one half log q minus the sum of all the zeros. Here, I've simplified what is the real part of one over sigma plus i t minus rho, eight sigma minus beta over sigma minus beta square plus t minus gamma square, and then plus that little law of log q. Okay, so this is just to So, this is just to fix what we're starting with. And I guess that's the part where I should be. Oh, thank God I've prepared my slide. Yeah, no, because it's going to be a bit repetitive because we are bounding every time that L function, you know. So I've just gone with a polynomial of degree two, right? So I just have three terms to bound every time. Terms to bound every time. I remind you our coefficients 3, 4, and 1. Okay, so if you, my trigonometric inequality is 3 plus cos theta plus, sorry, plus 4 cos theta plus cos to theta. Okay. And this is non-negative. Okay. This is what you have in that first column. The first column is looking at when we Column is looking at when we apply that trigonometric inequality to each L-function term. Okay, so being a bit pedestrian, I mean a bit slow here, but it's you look at L prime over L sigma chi zero, L prime over L sigma plus gamma chi, L prime over L sigma plus two gamma chi square. I am starting with the case where we are trying to locate a bit as a zero. So where is the zero of that? The zero of that form. So I'm indicing with a little zero here, but beta zero plus i gamma. So we want to know to figure out this or where it is not, because where is is too complicated for us. So where is not? That's more what we want to say. Okay, so from the inequalities we have earlier, remember this is our way principal character. So we have a pole, and that's all we have. Okay, we have a little o of log q if you want. I am going to skip every time writing plus little o of log q, but bear in mind that they are there, but they are not going to be very important. When you look at sigma plus i gamma, If you look at sigma plus i gamma, now you have a zero that is of a shape, well, if you remember, I was looking at sigma minus beta, sigma minus beta square plus, well, now it's gamma minus gamma square, okay? So this disappears, and you're just left with one over sigma minus beta here. See my minus beta here, and you have a contribution of a half log q here. When you are at two gamma, you just have the half log q coming up. So what you're left with is three over sigma minus one minus four over sigma minus beta plus four plus one over two of log q. At this point, sigma is still just a At this point, sigma is still just a parameter, okay? And I need to figure out how this is going to. I mean, you can see you can get an inequality for beta. This is going to give you a beta that is less than something depending on sigma and q. So the next step is to choose with sigma to kind of optimize this. So we just go through a little, we're just going to choose here. And yes, if you want to know. And yeah, if you want to know how am I going to choose my sigma, well, look, what do you have? You have a one over sigma minus one, a one over sigma minus beta, and then a log q. So that tells you that you should not take sigma too far from one, distance one over log q. Okay. And that's what is going to influence things. Or you should, in other words, you should not take sigma too far from beta. So what I'm going to do, I'm going to What I'm going to do, I'm going to choose sigma of a fall, one plus x, one minus beta zero. And x is going to become a parameter that we're going to choose later on. That's going to simplify this inequality into this. So sigma minus one is x, you know, times one minus beta zero. So here maybe. So, here maybe I should do the steps one time at least. And here you would have sigma minus one plus one minus beta. And then you're left with your five half. you're left with your five half log q so multiply by one minus beta zero and put it on once one hand of the inequality that gives you one minus beta zero times log q is going to be larger than so that four over x plus one minus three over x and then you're going to want to multiply by two over five okay Okay, so at this point, well, this you optimize in sigma and that gives you a constant, right? Yeah, in x. So, namely, if you take x to be, so I won't remember all that three position. Sweet patient yeah, three plus two with three. That's going to give you a value of about, so I'm going to say one of a r and the r that you find is of about this size. Of about this size, 3482, which is, you know, where we hope our way 35 is going to come from. So when I did all that, so my gamma is between 0 and 1. That's the type of region I want to focus on today. And I'm also assuming that chi is complex. Chi is complex. The reason I'm assuming that is that if it's real, then chi-square is going to become principal. And then, in that case, what was my L of sigma plus 2i gamma chi-square is going to add a pole contribution the same way that my L sigma chi zero has adds one. So that's why I'm doing that case separately. And now we're going to look at what happens if chi is real. So what happens if now we have an extra contribution from that last. Contribution from that last row here. Okay. Is that okay if I move my slide? Yeah, okay. Oops. I went too far. There we go. Yeah. One remark I want to do is about the Chukanoji polynomial. So the way it looks like The way it looks like, if I remember you, that we have that five-half of a four of x plus one minus three of a x, that's what the r is looking like. So if I just give you the more general version of it, that phi is coming from five, sorry, is coming from when we're adding all the coefficients except the first one. So a1, a2, so that was at 4 plus 1. And then that 4 that you see here over the x plus 1 is coming from the a1 term. One is coming from the A1 term, okay, the zero term, and that three is the coefficient that was attached to the pole, that A0. So at the end of the day, regarding the trigonometric polynomial, the way you want to choose it, you want to choose it so that these coefficients are for the best, for the optimal value of x that is determined here, that this is as small as possible. So it's kind of a double optimization, if you want, that's going on here. That's a part that is a bit complicated with those trigonometric polynomials. With those trigonometric polynomials, is to find those coefficients. Okay. So I just wanted to do a remark about that. What happens now when I have a real character? So just to show you the proof, how it's going to unfold, I'm hopefully going to be going a bit faster on this one. So here you would have, okay, your pole, because you are at chi zero, and that's it. zero, and that's it. Then you would have a zero here, as earlier, plus a one-half log q, where things are changing is with the last one, because now remember, this is your principal character. So you have a pole. So you have to include your sigma minus one. So you have a gamma, so plus that two gamma here, the square, since I'm taking a real part. The square since I'm taking a real part and and you also have a plus half log q that is No, actually it's not there but I have this coming up so if I add everything with my coefficients now we have three over sigma minus one plus that sigma minus one divided by sigma minus one square plus two gamma square minus gamma square minus one over sigma minus beta plus a half log q i don't know if you see already what is uh going to be sorry plus four times plus four times here i'm multiplying by four okay um yeah i don't know if you can already see what is going to become problematic but again going again you know with my take sigma minus one to be one minus beta zero Minus one to be one minus beta zero. Maybe you will see it a little bit clearer. Then you have to look at something of this shape. How am I going to deal with that two gamma square here? Well, gamma is bigger than zero, so I can just replace it. I mean, gamma square is bigger than zero, so I'm just going to replace it by zero, right? So I'm just left with a one over C my minus one. Am I going to Am I going to just sorry? Let me just write that detail here. So this one, we just use the fact that it's like this. So this just becomes a 1 over sigma minus 1 that we're going to pair with this one. Okay. So you're left with 4 over x minus 4 over x plus 1 plus 4 over 2. Well, at this point, I could do the effort of calculating that. Do the effort of calculating that. Look Q like this. The problem with this, so if you prefer, this is what you would get in the end. The problem with that is that it is negative. So that same argument, yep, fails here. fails here so we're we have to we have to change the way we are approaching things here okay what makes it fail it's the fact that gamma was being as low as zero right so we're going to go through an intermediate step where we're going to take gamma not as close to zero and hopefully that lower bound for gamma is going to make our way our final inequality work okay so um I don't know. This is supposed to be a light bulb. Take gamma bigger. Okay, I mean, as big as you can, considering that you're between zero and one. So let's do that. I'm going to move on to the next one. So, namely, what we're going to do is we're taking it at least. Making it at least as big as one minus beta zero. Okay, so the C here is going to be a new constant or a new parameter, another one. So here for now, it's just we're introducing this. I don't know what value I would give it, but I can assume that C gamma is no larger than a constant times the distance of beta zero to one. Yeah. So, yeah, so let's redo that. So, then we have one over sigma minus one here still. Here we have, so one over sigma minus beta. And here, yeah, again, this is my. This is my extraneous poll coming up. But now, sorry, I have two gamma here square. And in my log cube contribution, I have a half every time like that. Okay. So now we're going to replace. Uh, we're going to replace this two gamma square by two c one minus beta zero, like this. Okay, so that gives what that gives zero. Um, we have now still away four over sigma minus one, but now we have a better contribution from the zero term, right? So, choosing your excess earlier, now you would have this to look at. c squared that's correct and then plus that five half one minus beta zero log gamma so I replaced again sigma by one plus x one minus beta zero and I multiplied that inequality And I multiplied that inequality by 1 minus beta 0 so that now you can get for 1 minus beta 0 log q this is going to be larger than well this expression depending on x and c so that's going to be Yep, I knew I had an extra locue here. Something wasn't right. Yeah, so you just have this expression, namely this one, two over x plus one. I'm simplifying by two minus Minus wait, I went a bit too fast here. Yeah, no, I went too fast. I messed up my last term here. So it is not exactly one over sigma minus one, right? It's sigma minus one is a gamma term plus two gamma square here. Gamma square here. Okay. This one I just Yeah, I really messed up that last term. There we go. Go slow and steady. So, yeah, so I want an upper bound. So, here I'm going to use that my gamma is bigger than c1 minus beta 0. So, that's going to give you a 2 and then that square. Sorry about that. Okay. Let's do redo it from scratch here. So, your first term was giving you three times that pool contribution. The second term was minus four that C my minus beta. You gain again a pool term here. So, plus one, C my minus one, C my minus one square plus four C square one. c square one minus beta zero square plus two times the log q which now again taking this should give this 2x plus 1 x x square plus 4 c square hopefully this time I got it right Square. Hopefully, this time I got it right. Okay, so that works. 2 over x plus 1 minus 3 over x minus that x x square plus 4 c square. Okay, so there must be a 2 somewhere. There must be a two somewhere here. So, arriving here, just a few remarks. Again, you're going to want to optimize in X, but you also want to optimize in your C. Note that this expression, so my one over R, you know, is going to be given by this. So things are decreasing in C here. Okay, so that R, that R value that we'll be. That R value that we would get decreases in C. So I can play with different values of C. You can, this is going to become a positive value if I take C large enough. So that's how you counteract, you know, the fact that here you had as much contribution from your Much contribution from your poll as you are getting from your zero. I'm not choosing C yet, but I know I can do this here. The reason I'm not choosing it yet is because I still have another case to look through. I still have the case when gamma is less than c1 minus beta zero to look through. Okay, so I will wait to see what kind of constant I get at the end. Constant, I get at the end. And then we will play with C to C the letter to see if we can get these two constants of one over R matching each other or not being too far from each other. Because at the end of the day, we are going to put all these cases together and takes the worst value for R positive that we obtained. Okay, so I want that overall the R value that I obtained does not become too large. too large so in this case um we have an extra we have an extra pull again um from this last line and well the thing that happens though note that gamma here is very close to the real line at distance one minus beta zero right so you want to what happens here is that because you have a real character and a zero that's very close to the real line you also Very close to the real line, you also have its conjugate that's very close to the real line. So, in terms of competition, if you want, you have these two zeros very close to the real line, very close to one to the pole, that are competing with each other. So we're going to exhibit those two zeros, beta zero plus i gamma and beta zero minus i gamma, into um into how we are going to balance things. I could go back with the same trigonometric inequality. I'm just going to ask you to try. Inequality. I'm just going to ask you to trust me on this one or try it by yourself. It is going to be just the same as to use this one. One plus cos theta is non-negative. Or if you prefer, using here the coefficients one and one will be good enough. What happens is that What happens is that you're going to get exactly the same contribution, except there's going to be a factor of two between the two methods. And since you're looking at things being non-negative, you simplify by two. At the end, you're going to get exactly the same result. So we have, as I said, we're looking at two zeros, the one so we have. So we have sigma minus yeah there's another modification that we do. So I was so two modification. Yeah. I just need to look at sigma. I don't need to bother about putting a gamma. So what I'm looking at, if you prefer, if I go back to what does one plus cos state I'm non-negative mean? cos theta non-negative means. I'm just looking at minus the real part of zeta prime over zeta at sigma minus the real part of L prime over L at sigma. And this is non-negative. And maybe with a little picture to show you what's going on. This is one. And this is your sigma. And everything here is happening at distance. Everything here and there is of size one over log Q. So you're really in an area very restricted around one here. So that's why I can just put myself at sigma. This way, this is just going to be a little bit. This is just going to be a little bit simpler with when I look at the sigma minus plus minus i gamma. So here we have sigma minus beta, sigma minus beta. I'm looking at sigma plus minus sorry, at plus minus gamma square. So I'm going to start with the gamma square here, and then I have Okay, literally, that's what we have, plus the half log q. So, you're left with now this type of Now this type of inequality and this time we're using the fact that gamma is no larger than c one times beta zero. So here now we're using the fact that this is less than c square one minus beta zero square, which gives you okay, I should maybe I'm gonna ask you to just To ask you to trust me on that, and I'm gonna not trust me, but do the calculation with me. Once you replace with the parameter x, you are left with this. Right, and now your zero-free region constant is appearing here. So it's going to be 4x plus 1 x plus 1 square plus c square minus 2 over x like this. So now you see this time. Again, this is going to give us our 1 over r constant. This time this is decreasing this is increasing in c. The one over r to the r now is increasing in c. You know, after you're done your optimization with X. Okay, so these two regions before and after that constant one minus beta zero, you're getting first a constant that's going to decrease in C, the second one is going to increase in C. So playing with your C value, you're going to have Your C value, you're going to have values that are going to be fairly apart. And so it's just a game of finding what C is going to optimize both rich, I mean, what C is going to give you find C such that the max between the two past regions, the two past Rs, is as small as possible. Small as possible with the polynomial we're playing with, the value that is going to work is going to be a value of c close to 17. And then you optimize with your x. And I think I may have that. I wrote things for you afterwards. Yeah, so in the second case, for this value of C 17. Value of C17, you optimize with X, you're getting a value of very close to 30, just under 30. And just let me go to the calculation done in the previous one. Same value of 17 with that optimization for X, you get a value just above 30. So overall, you get a value of like 30.1. We're still under the 35 that is announced. That is announced. Okay, I have two minutes. Nobody's like screaming at me, but I should show you when chi is real and gamma is zero. In that case, there's a problem. Our past proof is failing. And so, because we just have one zero, okay, before what was making the proof work was that I had these two zeros. So now I have that one zero. So I'm going to have another problem where my Another problem where my constant is negative, that doesn't make sense. So, what we're doing is that we're looking at two zero, two different, I mean, two zeros on the real line. So, beta one and beta two can be the same zero or they can be different. That's not going to change the argument, but they are both real zeros for that real character. And while using the same argument as earlier, I think it's a bad idea to do math on the rush like that with Do math on the rush like that with that, but you're going to get beta 2 plus a half log q. So, and this is all non-negative. Well, I'm not going to drag this both these zeros. I can't do anything to really distinguish them. Anything to really distinguish them. But what I have is that I can bound them above by the biggest, no, the smallest value of both. So you continue doing your little thing, you will find that the smallest one satisfies. Satisfies an inequality and the R you find here. So here I'm skipping on the optimization, but it's like 2.92. Okay, but that doesn't tell us that what happens for the largest zero. Okay, so very that's why in the theorem, there is a chance that. Theorem: There is a chance that there is a zero that is going to be real associated to a real character that is outside the zero-free region. Okay, I'm gonna stop here because it is exactly 30, but I can just tell you that the rest, this argument repeated, can also tell you that that character, real character is only happening once, that exceptional character for a fixed modulus. And further down, that if you have two exceptional moduli, If you have two exceptional moduli, then these exceptional moduli can be very close to each other, namely one has to be at least the square, at least bigger than the square of the smallest of the next one. Okay, so thank you. And I have some activities prepared related to that. So hopefully you followed all the details. Thanks. I think we have coffee now. Yeah, it's coffee. I think it's in one slope, but also understood. So, since you will be in groups, just to give you the instruction, it is in the slide and it is very terse here. So, there's different problems that you can look into if you want to. But the one I'd like you to focus today: the first question is: do the same as what we did today, but with using Stetchkin's device. This is not a long question. What is changing is a coefficient. What is changing is a coefficient of log q. Okay, so this one is just to go over the notes essentially. The second one is more challenging because I had a polynomial of degree two, so I had to be mindful when my character was of order two. Okay, now I'm asking you to play with the polynomial of degree four, so you have to be mindful. What if my character is of degree four, three, two, what is happening? Okay, so maybe you can. Okay, so maybe you can focus on one of the three cases, and that would be good. Yeah, sorry, let's have coffee.