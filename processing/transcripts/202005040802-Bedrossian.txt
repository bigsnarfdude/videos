I always enjoyed Jacob's talks at Quran. I would sit down and listen to him, and they are full of energy. So I hope that this energy that I can feel in the room in 1302, I will be able to feel it through Zoom. So it's my new experience for me. And Jacob, I'm putting the pressure on you to get it across. With that, please go ahead. Please go ahead. Well, yeah, thanks very much, and thanks for the invitation. Yeah, so I'm going to talk about a recent line of work which is undertaken with Alex Blumenthal and Sam Punchen Smith. So Alex is a postdoc at University of Maryland. He actually was a graduate student at Courant, but I didn't know him then, unfortunately. We overlapped briefly there. But he's going to be joining Georgia Tech as tenure track in the fall. And Sam Punchen-Smith is a postdoc at Brown University who was a graduate. Postdoc at Brown University, who was a graduate student at the University of Maryland. So, this whole thing started back in 2017 when a bunch of grad students and postdocs and I, we sat down and we decided to understand, tried to understand what is turbulence. And so, what I'm going to be talking about here is what physicists would call fully developed turbulence. So, it's not quite the same as weak turbulence, which I do not claim to understand. And I'm just going to be talking about fully developed turbulence. And so, this is it's a very Develop turbulence. And so, this is a very ubiquitous phenomenon. So, it's seen in a very wide range of systems, including fluids, plasmas, and both kinetic theory and hydrodynamic models of plasmas and nonlinear wave systems as well, and dispersive equations. The most classic book case studied by physicists, the only case I won't be discussing today, is where you first send time to infinity and then you send the dissipation to zero. So, the kind of model problem you should have in mind is something like this, where n is going to be some kind of time reversible nonlinearity. Time-reversible nonlinearity. So, think like the 3D Euler equations or the 2D Euler equations or nonlinear Schr√∂dinger or something like that. And this is going to be some kind of dissipation operator, and this is going to be an external force. And you want to send time to infinity, wait for it to settle into a statistical steady state, which I'll give a mathematically rigorous definition to later, and then send epsilon to zero. This is the first case studied, and it's not the only physically relevant case, but it covers a lot of important physical cases. Physical cases. And so the phenomenon is ubiquitous. It's different in every system. But there's a couple things that are common in every system that you see this. So, one thing is that the systems are always going to be chaotic. So I'm going to talk about Lyapunov exponents more later. But essentially, what it means is that if you freeze all of the other parameters in the system except for the initial data, you make a small perturbation to the initial data, then the two trajectories diverge exponentially in phase space. That's what it means to be chaotic. This, of course, is. This, of course, is not the same thing as turbulence because finite dimensional systems can be chaotic, but you need to be infinite dimensional to be turbulent. And so that is the second feature that you always see, which is a creation of multiple scales. And so normally, so what happens is you inject the force at one scale, so say order one scale. And then you send energy from this order one scale down to smaller scales. Or you send energy from large scale, so from this order one scale. From large scale, so from this order one scale to larger scales, or you could send you can send information in both directions. So, now that you're shrdinger, for example, is expected to have a dual cascade where you create small scales and large scales if those scales are available in your system. And so, this is what this is a picture of 3D Navy-Stokes computer simulation. And you see it's kind of something interesting is going on at kind of every scale. Oops, not quite used to this setup. This is the setup. And so it's a continuum of scales, not one or two or three, but it's a continuum of many scales. And then a really key observation that, as far as I know, is due to Kolmogorov, it's an observation that's expectation, which is that the statistics in these intermediate ranges of scales between forcing and the dissipation operators are expected to be universal in the sense that they do not really depend on the dissipation operator so much, and they should not really depend on the external forcing so much. And so they. And so they, so, for example, if we're talking about fluid mechanics, the small-scale statistics of the fluid behind a wake of a car shouldn't really matter whether you're talking about a Honda Civic or a Lamborghini or whatever. All that really matters is the total amount of energy available. And this is what's expected. And physicists make predictions about what these laws are. And that's what we're going to try to get at, trying to understand these laws, these universal turbulence laws. Universal for each system and dimensions. Universal for each system and dimension. It's not really cross-system. And so, of course, one of the directions we wanted to go in was to say, all right, well, can we verify some of these laws in the simplest example that's still a physical model? It's not a toy model. And so we found the simplest model. This is called passive scalar turbulence. And so what you do is you solve the 3D, not 3D, whatever dimension of your Stokes equation, incompressible. And we're going to subject it to a stochastic forcing, which for To a stochastic forcing, which for the duration of this talk will be a white in time stochastic forcing. You can also add deterministic forcing, it's not important. Then, after you solve this, you look at the evolution of a scalar field, which is advected by the velocity field and has its own diffusivity and also has it subjected to its own stochastic white-in-time forcing. And again, you send time to infinity, and then you send kappa to zero, and that's going to be passed. To zero, and that's going to be passive scalar turbulence. You should think of the scalar as sort of small temperature variation or salinity variation, which maybe the variations are so small that it doesn't have an important effect back on the fluid itself. And so this is what physicists refer to as the passive scalar turbulence problem. And for reasons I'll explain, at least some regimes of passive scalar turbulence are the easiest systems in physics to study. All right, so what's the most basic question a physicist could ask is how would the kind ask is how would the conservation how would the conserved quantities distribute themselves in this inertial range and so for example in fluid mechanics you might be interested in understanding how the energy of the fluid the kinetic energy of the fluid distributes itself in the inertial range for the scalar i'm going to talk about the uh sort of l2 density you can think of this as the scalar the scalar energy if you want and what's expected is that over these intermediate ranges they form power laws and that these these numbers here don't really depend on anything except Here don't really depend on anything except for dimension. So they don't depend on the precise form of the forcing here. So that's what's expected. And so let me give you a schematic of what physicists expect and observe in experiments for passive scale turbulence. So here it's a log-log plot. So this is the log of the power spectra, and this is the log of the frequency. And so you actually see three ranges in passive scalar turbulence. So one range, so this is where the So, one range, so this is where the forcing acts. And of course, there's no universal theory. If you want to know the answer, you just have to run an experiment or computer simulation. Then what's expected is that you have an inertial range of the 3D. I'm talking about 3D Navier-Stokes for the purpose of turbulence that's simpler than 2D. And so in the inertial range of the fluid, you see some particular power spectrum. Of course, we have no mathematical proof of anything like that, but that's what you expect. So that's what Kohler. But that's what you expect. So that's what Kolmogoro predicted. That's sort of what you would observe in the experiments. And it's also expected, again, no mathematical theory for this, that the passive scalar matches that same power spectrum. I'm not going to be talking about that. We do not understand this regime yet. And it's sort of clear that in order to understand the regime, you sort of first have to understand turbulence in Navier-Stokes, which is still pretty far away. Then, after a certain point, you switch regimes. So if the diffusivity of the scalar is much smaller than the scots that we knew, Much smaller than the scots that we knew, you have another regime where the velocity field is sort of smoothed out at that scale, but the scalar has not. And what's observed, so what's predicted, so in 1959, Bachelor predicted that in this regime, this part of the inertial range, you should see a slope of minus one of the power spectrum. And that is what we're going to be proving. And this should go down to kappa to the minus one half, which is just the sort of the parabolic regularity cutoff. And so we don't know what happens here. We don't know where this line is. Happens here. We don't know where this line is because, again, we don't understand hydrodynamic turbulence, but this is what we will prove. So we will be able to verify this. And that's what I'm going to discuss. So just some quick aside for experimental observations. So physicists and engineers do care about this question. They've done tons of physical experiments and tons of natural observations to see if this is correct. So this is a measurement. So it's the same plot, but it's a physical measurement from the ocean. They just sort of dumped sensors over the side of their boat. They just sort of dumped sensors over the side of their boat. So, this is the power spectrum of the velocity field, and this is the power spectrum of the scalar, which in this case is a temperature variation. So, this over here is the Bachelor regime. Of course, in physicals, in experiments in the laboratory, you see the same data. This is the velocity field, and this is the baseline regime. In this case, this is a salinity variation in grid-shift and turbulence in an experiment. Experiment. Also, of course, once they had strong enough computers, they started to analyze this with a computer. And so, this is a conglomeration of a bunch of different numerical studies on passive scale turbulence just in the Bachelor regime. So that cutoff to the Bachelor regime is somewhere over here on the graph. And so you see almost all of them match Bachelor's theory, except for this one weird outlier, who, as we'll see, is just plain wrong. Okay, so of course, the first thing you want to ask is y minus one? And one of the things that is understood is that, so first of all, in part because of this universality, is that this minus one law that Bachelor predicted is actually true about a bunch of toy models, not just Nader Stokes. And so if we can isolate a toy model that sort of captures the essence behind the law, then we might be able to prove it actually. Then we might be able to prove it actually for Navier-Stokes fluid. So that's the idea. So the simplest toy model that one can come up with that displays the minus one law is built on replacing the Navier-Stokes evolution with just iterates of a chaotic map, the Arnold cat map. So what we're going to do is we're going to identify the torus with a unit square, and we're just going to map it forward under a hyperbolic matrix and then sort of wrap it back to the torus. And so it's well known that orbits of this map are Orbits of this map are very chaotic, it's strongly mixing, and that's actually going to be the key structure behind Vatcher's law. So, but let's see, let's prove Vascher's law for this toy model in a sense. So, I'm going to replace, I mean, I'm going to take discrete time. I'm also going to forget about the diffusivity. Remember, that's supposed to be kind of like a higher order effect. It shouldn't really be the leading order of behavior inside the inertial range. The inertial range should not depend on the diffusivity to leading order. And so here's where. And so here's what we're going to do. We're going to add in a random amount of scalar every time step at order one scales. And then we're going to just push the entire scalar field forward under one iterative of the CatMap. So what's really nice about the CatMap, what makes it a good toy model, is that it maps Fourier modes to Fourier modes. And so we can actually just solve this iteration and we get this random Fourier series. And you see that, so A inverse transpose. see that so a inverse transpose has a positive sorry has an eigenvalue which is bigger than one and the eigenspaces line up so that these frequencies here are distributed at around lambda to the j power and so this is a this is a random Fourier series which is supported essentially at lambda attic frequencies okay so then you can just compute so it's it's very sparsely supported in frequency space so it obviously doesn't follow bash law pointwise in frequency but it does follow it if you add But it does follow it if you average over frequency shells. For lambda addict frequency shells, you have about one point between lambda addict shells, and that one point contains about order one energy. And this is exactly Batchelor's prediction if you average it. So remember, Bachelor's prediction is that on average, every frequency contains one over k to the dimension energy. And so that's what Bachelor's prediction is. And we see this totally. Is and we see this torn model does display it. And so the key behind this tor model, why does it give you minus one? Is this every iteration creates an exponential increase in the frequency? And that's what we're going to try and make rigorous for Navier-Stokes, in a sense. So let me just lay down the thing. So I'm going to be talking about two dimensions. As you might imagine, we actually can handle three dimensions if you replace the dissipation operator with a hyperviscosity. Operator with a hyper-viscosity. And okay, that's not really any harder or easier than the 2D case, so I'm just going to focus on 2D just to fix ideas. And all right, so here's what we can handle at the current moment in time. So we can handle white-in-time divergence-free forcing, which is smooth enough, but also non-degenerate. So it needs to be active at all frequencies, but it can't be too rough at the same time. So basically, this parameter alpha. Um, so basically, it's this parameter alpha. So, you need to have forcing at all the modes, or at least all high enough modes. Okay. And this alpha needs to be big enough. I don't remember the exact cutoff. Think of it as like bigger than 10 or something. You can imagine, of course, if your forcing is too rough, you'll start to mess up the power spectrum. You'll change the regime of scalar turbulence that you're supposed to be in. So, of course, you want the forcing to be smooth enough, not to disturb the statistics. The statistics. So, under these conditions, it's very well understood. It's very classical that the Navier-Stokes equations define a really, really nice random dynamical system on this phase space, on this phase space of sublight spaces. And one important quantity that you know is that it defines what's called a Markov semigroup. And so, this is an evolution on observables. So, phi is like a question you might want to ask about phase space. So, it's a function of phase space. And what On phase space. And what PTP is, is that it tells you what is the expected value of this physical quantity phi if you know the initial data. So remember, if you know the initial data, then all you're averaging over is all the different possible random noise paths. So that's PT phi. And so by very classical methods that I'm not going to discuss, there exists a unique stationary measure for this Markov process, which means that if you randomize the initial condition with respect to this measure, the statistics This measure, the statistics are constant. Okay, do not depend on time. So, of course, each individual realization is not a constant. So, statistical stationary is, of course, not the same thing as stationary. If you never worked with such things, you should think like, if you've ever seen like water going past a stone in a river, okay, I assume you have. So, this is an example. It's a great example of what a statistical stationary solution looks like. So, the wake of a lot of moving objects actually is approximately statistical. Objects actually approximately statistically stationary. And so, here you see, of course, every time you look at it, it's doing stuff. I mean, it's very complicated, but the statistics aren't really changing. And that's the kind of situation that statistical stationary is referring to. Okay, so what we want to understand is we want to understand how the Navier-Stokes equations is going to move scalar around, because this is the nonlinearity that we're interested in understanding, in a sense. Put quotations if you want to think of it as nonlinear. Put quotations in it if you want to think of it as nonlinear or not. So remember, the system is actually u, g, so it is nonlinear, but it's sort of a very simple nonlinear. And so to do that, we want to understand the Lagrangian flow map. So this is just the map of where the individual particles in the fluid go. And it's a random ODE, it's not a Markov process, it is a random ODE, and it's a nice diffeomorphism of the torus. And we want to understand how this behaves. To understand how this behaves. So, in particular, we want to understand how it sends information from low frequencies to high frequencies. So, this is actually called mixing in fluid mechanics. And when someone in fluid mechanics says mixing, they mean literal like mixing milk into coffee. It's not some abstract math thing. And so, this is a picture from a website of Jacques Vineste, who studies, he's an applied mathematician who studies mixing. Applied mathematician who studies mixing. And basically, what do you want to do? You have some red dye and you have some blue dye and you want to stir it up so that it looks white in a weak sense. That's all that mixing is. And that's, as we'll see on the next slide, this is really going to refer to a transfer of information from low frequencies to high frequency. Let's say, okay, it's kind of clear from the picture if you are used to working with that. Okay, so I'm just going to talk about this initial value problem. And I'm going to quantify it. The most convenient way to quantify it is actually. Quantified, the most convenient way to quantify it is actually with negative Stable of norms. There are other choices. I'm not going to discuss it. They're all essentially equivalent up to small perturbations in a sense. And so negative Sobolev norms. So you see, right, if kappa is equal to zero, then the L2 norm is conserved. And so if this quantity decays exponentially, then all the information has to shift from low frequencies to high frequencies. It's not saying that some information shifts, goes. Information shifts goes up exponentially fast. That's to say, all of the information goes up, goes to high frequencies exponentially fast. So it's much, much stronger than just saying the H1 norm blows up or the HS norm blows up. The negative software of norms is a much stronger characterization of a cascade. And that's what we're actually going to need. Because we want to emulate this exponential transfer from low frequencies to high frequencies. So to see that it's actually, you can think of it as understanding statistics. Actually, you can think of it as understanding statistics of the Lagrangian flow map. You use the duality characterization. And actually, once you see that, it kind of looks familiar from your ergodic theory class, or if you took a class in ergodic theory, because this is actually proving that this thing decays exponentially is exactly a way of quantifying what's called strong mixing. So the two notions of mixing are equivalent, that are not different. And so this is what I want to understand. To understand, and okay, so we were able to do that in the following sense. It took a lot of pages and work, but I'm just going to give you the final theorem. So it's split, the proof is split into three papers that all kind of depend on each other. So the third one uses the second one, which uses the first one. And basically, the punchline is that for all these models, so for 2D or for 2D Niver Stokes, for example, for any value of the Reynolds. Example, for any value of the Reynolds number, and there's a parameter p, which I'll explain, there's a deterministic mixing rate, and that's extremely important that it's deterministic, and it's also independent of kappa. So remember, I'm talking about mixing this scalar. Okay, so that a priori depends on kappa. And so the mixing, the exponential is deterministic, and this constant is what's random. So it depends both on the noise path. Depends both on the noise path. And if you think, it depends on the entire future of the noise path, right? So it depends on the entire noise path. And it depends on the initial condition of the velocity field, of course. And of course, you want to know that this D is almost surely finite, otherwise you sort of haven't said anything interesting. And so not only do you have that it's almost surely finite, but you have like some information about what's the probability of it being extremely large. So you have p-moments on it. P moments on it. And so that, you know, so that you can quantify the probability of it being extremely large based on the size of the initial velocity field. And these bounds don't depend on kappa. Okay, so it's uniform in kappa, almost sure exponential mixing. And you should think, so what is d? D is kind of like a waiting time. So like log d is kind of how long you have to, you would have to wait in order for this coefficient to start to be less than one, right? One over gamma log d. Right, one over gamma log d. And so, what this tells you is that the amount of time you have to wait in order to see the mixing is almost surely finite and it has exponential tails, which is what you expect. So, most, you know, if you're dealing with sort of Gaussian noise and stuff like that, if you have a nice diametical system like Never Stokes, you should expect exponential tails on the waiting time. And yeah, and it's it, and like I said, so it's not too hard to prove, what's much easier to prove than this theorem that this. That this is optimal in the sense that there exist constants gamma prime and d prime which satisfy all the same properties, but for which this is true. Well, this being h minus one. So you can't mix faster than this. This is the fastest you can possibly mix, up to the precise quantitative values of these constants. So it's sort of an optimal mixing result in this sense. And all right, so that's sort of the main So that's sort of the main technical meat behind Bascher's law. So I'll discuss this in a little bit. Let me just loop back around and explain what our final theorem is about scalar turbulence. So this is the turbulence problem. I simplified things. B is just one particular smooth function, which just has to be non-zero for it to be interesting. And most 1D Brownian motion. It's not, yeah, you can treat more general things. It's just, it just doesn't matter. There exists a unique stationary measure for this process that's not hard to prove. measure for this process that's not hard to prove. And so what we really want to understand is what are the statistics? So what are the, no, sorry, what is this measure when kappa goes to zero? That's what sort of all statistical stationary turbulence problems really are about. What are the statistics of the invariant measure of the stationary measure as the diffusivity goes to zero? And so what we're able to prove is this characterization of Masher's law. So you see, it's saying how much energy is in the Saying how much energy is in the ball of radius n in frequency. So it's even more averaged in sense, sort of averaged over entire balls instead of just annuli. And what it says is that for every new, there is a n0 such that this holds uniformly in kappa. So these constants, of course, don't depend on kappa, otherwise it's not interesting. And n0 doesn't depend on kappa. So n0 is the start of the Bachelor regime. Of course, we don't know where that is as a function of nu. That is as a function of nu, because we don't understand how dynamic turbulence. And so we know, all we know is that there's some range as kappa goes to zero for which Batcher's law is true in this average sense. So you contain no more than log n energy in this ball and no less than log n energy in this ball. And I should say, so I'm not going to talk about the proof because it's very simple. Once you really have this really powerful theorem about mixing, it's not that hard, let's say. There's some details to work out, but somehow the main step was the mixing. But somehow the main step was the mixing and understanding that the original toy model. And I would say it's the upper bound that's the hard step. So, because if you think about it, you're dumping in energy at order one scales. And especially in the linear, especially in this sort of like partially, sort of partly linear case, it's very easy to see that this kind of power spectrum has to really know, it really has to do with how fast the cascade is. So it says, how long does it take for information that you put at scale one, get to scale n? one gets to scale n and you need to prove that it always gets to scale n by log n time in order to get this upper bound so you need to know that you sort of well you need to know you have almost sure uniform in kappa exponential mixing in order to prove this upper bound the lower bound is you just really need to know that that's optimal because you need to know that it takes at least that long and again that's not totally obvious but it's much much easier i mean that's sort of like seven pages of Sort of like seven pages of math versus 170 or something. I don't remember how long it took us, but a lot. And so, yeah, so it's the upper bound, that's the hard part. So what's cool about this is that, okay, once you have this, this is really an a priori estimate, what's sort of above and below, of exactly what kind of the, exactly how much energy is being distributed in frequency. Well, not exactly, on average. And so you can put that together with some easier arguments. And so you can put that together with some easier arguments, and you can really characterize what the stationary measure does in the limit. So you can actually prove that this stationary measure converges to a measure, which in some sense anyway, is a stationary measure of the inviscid problem. So with no kappa at all. And that, of course, is deeply pathological. Like all turbulent systems, it should become pathological in the limit. Because you're dumping in energy every time. Energy every time, you know, every unit of time you're dumping in about order one energy. And even though B is smooth and U is pretty smooth, you're still violating all the conservation laws, because otherwise you couldn't be stationary. And so one law is actually the most fundamental, these are kind of the most fundamental universality laws in turbulence, which are actually the scale-by-scale energy flux laws. I didn't discuss them because of limited time. So the scale of turbulence is called Jaglum's law for not. The scale of turbulence is called Jaglum's law. For Navier-Stokes, it's the Kohlman-Grav-Fort's law. But basically, what you have is that the energy flux through frequency shell n is converging to a constant as the frequency goes to infinity. And so you just have a sort of conveyor belt of energy going in at one scale and just going through all scales. And that's what you always expect. That's another one of the universal behaviors that you see in all turbulent systems, some version of this law. Of this law. And so the limit is obviously not an L2. And in fact, you can prove because it violates all LP conservation laws that it's also not in L1. So you get really these horrible distribution value answers. But that's what Bashler predicts, that you get that in the limit. Okay, so let's talk about the proofs. So of course, as I sort of alluded to, a big aspect of the proof is a lot of random dynamical systems. Alex Blumenthal was a student of Lai Seng Young, and he's an expert in random system. And he's an expert in random dynamics. He basically taught Sam and Alex, sorry, he taught Sam and I a ton of random dynamics specifically to solve this problem. And so let's talk about the first step. So the first step is, first we need to understand, so we want to prove strong mixing of this Lagrangian flow map. But the first thing to realize is that this is much weaker than just proving that the gradient of the flow map blows up exponentially. So if you think about it, So, if you think about it, saying the gradient of the flow map blows up exponentially is going to be roughly equivalent to saying that the h1 norm of the scalar blows up exponentially. But as we discussed, that's much, much weaker than proving that the h minus one norm decays exponentially. And so it seems like we can't do anything at all. We can't even hope to do anything if we can't prove that the Lagrangian flow map has a positive Lyapunov exponent. Of the Appanov exponent. So that's sort of the first step. And if you can't do that, then you have to give up already. And so that's what we're trying to understand. So that's the first thing that we had to do. So there's a beautiful theorem. If you're unfamiliar, it's called the multiplicative ergotic theorem. If you're unfamiliar, I highly recommend you look it up. It's really cool. One of the many things that it says, the simplest thing that it says, it's really a consequence of the sub-additive ergotic theorem, is that under some very, very mild conditions, which are easy to verify, there is a Liyapa at top. There is a Lyapunov, a top Lyapunov exponent for the Lagrangian flow map. So, this is an asymptotic exponential growth rate. But we need to know if it's positive or not. Obviously, it could be zero, right? Can't be negative because the divergence-free condition that's really important, as we'll discuss. Unfortunately, even for random systems, there aren't that many tools that can really prove that you have a positive way up on an exponent. You know, like we all know that sort of in a generic class, et cetera, et cetera, that most systems will have these things. That most systems will have these things. Most systems will be chaotic. But if I give you a system, you can't tell me if it has too much structure or not to be chaotic. So for random systems, that turns out to be easier, but it's still not that easy. So the tools that exist are very rigid. But it's going to work. So the other thing I should say is that, oh, I should also emphasize that there seems to be a lot of confusion about this. We're proving the Lagrangian flow map is chaotic. That is absolutely not. Is chaotic. That is absolutely not at all, even remotely the same thing as proving that the velocity field is chaotic in its phase space. So physicists are very precise. They have two terminologies. They have what's called Lagrangian chaos. Apparently, I can't spell Lagrangian, versus the latter, which is called Eulerian chaos. So we are going to demonstrate plenty of examples of velocity fields which have Lagrangian chaos, but do not have Eulerian chaos. Not have Eulerian chaos, and it's very easy to come up with dumb examples which have Eulerian chaos but do not have Lagrangian chaos. There's no a priori reason to expect that they go together, and that's it's just false to believe that they go together. So we're going to talk about Lagrangian chaos. It's a much easier problem. As far as I know, there is exactly zero progress towards proving Eulerian chaos for Nager-Stokes. So I'm not going to discuss that further, just Lagrangian chaos. You just have to know that they're not the same thing. In fact, in But they're not the same thing. In fact, in the case of Lagrangian chaos, the linear case is the easy case. So when you have no nonlinearity. I'll explain why. It's actually pretty clear why that's the case. So, and also shows you what's the role of probability and why we should be using probability to study these problems, at least for the first next century or two. So if you drop the non-linearity, you just get this very simple, you know, let's say I'm just forcing these four modes. So you basically just Just forcing these formulas. So you basically just get stochastic heat. It's very simple. You just have these IID points to the Lundbeck processes. They don't even have to be identically distributed, points to the Lundbeck processes. And if you look at that, okay, if you sort of squint your eyes, you'd be like, all right, I basically am composing some random shear flows. And of course, two shear flows, as long as they're not exactly aligned, will always be hyperbolic. That's easy to check. For example, the cat map is the composition of two transverse shear flows. So as long as you're shearing in sort of two different directions, Different directions, or even just slightly misaligned, like this, you'll always get hyperbolicity. And so, what you realize is that if you have a general process of this form, you should always get hyperbolicity, provided that the Z's aren't correlated in a very specific way. So it's somehow like all the Z's have to be working together in a very specific way in order for the flow not to be chaotic. And this is just, right now I'm just talking about heuristics, but you know. Heuristics, but you know, this is what we kind of know from our experience. So, with measure-preserving systems, chaos should be the normal thing. And in order for it not to be chaotic, you need a ton of specific structure. And that's what we think should be true in general, but it's hard to verify in real life because we don't know when it's possible to prove you have this specific structure or not. So, probability is a tool we're going to use here to be able to say, yeah, with probability one, these guys aren't copy. These guys aren't coupled together in some insane manner that stops chaos. And so that's the role of probability here and in random dynamics in general, I would say. When you turn on the nominarity, of course, you don't know what the z's are anymore, and they're all deeply correlated, and there's infinitely many of them. And so, and what you observe is that incompressible fluids, they like to organize themselves into vortices. So, this is a picture of stochastically forced Nagger-Stokes generated by physicists. Generated by physicists. So it's a picture of the vorticity field, which all you need to really know for now is that that's a scalar evolution equation. And these islands here are exactly what they look like. They are vortices, and they're little in them. The flow map is just going around in elliptic orbits, essentially. And so, of course, the gradient of the flow map is only growing linearly inside these little islands. Only growing linearly inside these little islands. But you know, the islands do collide, they break apart, they merge. So it doesn't, just because you have these islands, it doesn't tell you that you don't have hyperbolicity, just tells you that you don't have instantaneous, you know, just tells you that you don't, it's not clear, it's not obvious either way because of these islands. And so we have to rule out that these islands stop, stop the Lagrangian chaos. And so that's what we do in our first paper. And so the very And so the rigorous statement is: for all values of the Reynolds number, there exists a deterministic Lyapunov exponent. And the fact that it's deterministic is just a ergotic theory thing. And that this Lyapunov exponent is positive. And it's very important to understand what this means. So it means, remember, I said you have to freeze all the parameters and still see the chaos. Otherwise, it's not interesting. So it's every initial data and every Every initial data and every initial condition of the velocity field, you still see this exponential growth rate almost surely. So it's not some weird trick of averaging or something silly like this. Those other concepts wouldn't mean anything in this context. So it's important. So every experiment you run will see the exponential sensitivity. Every individual experiment will see the exponential sensitivity. And so it tells you somehow or another, sort of in a roundabout way, that vortices can't trap particles. Can't trap particles. And as you expect, the infinite dimensional nonlinear case is the hard case. So Ethereum also applies to this toy model, and that's, you know, it's much easier in that toy model. I mean, maybe one-tenth the length or, okay, one-fifth the length of difficulty. It's a long story, but the basic idea is this beautiful concept. So, and it really gets at the key idea is like, why is it even possible that you can do this for a real physical system? And this is an System. And this is an insight due to Furstenberg in the 60s. So he put forward this idea on IID determiner 1 matrices. And so it's a much simpler problem than we're talking about, obviously, but basically saying, all right, if I look at these random matrix multiplication, when does this random matrix multiple stretch vectors exponentially or not? So it's easy to come up with examples where it does not stretch vectors exponentially. So random rotations, of course, you have no exponential growth. Of course, you have no exponential growth in this multiplication. Random shears, like this. Now, here, notice that the random shears that are all aligned. So it's just one shear like this. In that case, you just have polynomial growth. And so eta is still going to be zero. The other one is if maybe you have some exponential stretching, but it's always going to be canceled out by some exponential compression. So, for example, maybe you're going to stretch all the vectors in the x direction, but then every so often you're going to. In the x direction, but then every so often you're going to flip, and the vectors that you stretch are now being compressed, and vice versa. This, you'll get sub-exponential growth. It'll be something like e to the square root of n. And so these are sort of three examples where you do not see the exponential growth. And what Furstenberg proved, and it's a really beautiful result, is that these are the only counterexamples. And it also extends to SLD. It's not specific to dimensions. Just the SLD, you have sort of arbitrary combinations of these counterexamples. Arbitrary combinations of these counterexamples, essentially. And so what this says is, so what a theorem really says is that you have a dichotomy. If the Lyapunov exponent is zero, well, this thing that we're calling the Lyapunov exponent, then either there's a deterministic inner product such that you are deterministically an isometry, or there is a deterministic set of lines such that you deterministically map them almost surely. Almost surely. So it's like you started with a random system. And the point is that in order to avoid seeing exponential growth, you need almost sure degeneracies. You need like too many conservation laws. And that's something that you can actually make rigorous in this context, that you really have this dichotomy between chaos and non-chaos. And so the idea is to extend these kind of ideas that have been bouncing around in the dynamics literature to Navier-Stokes. To Navier Stokes. And it's kind of a long story, but let me just give you the brief rundown. So, you know, you have to replace IID with what's called a co-cycle over a Markov process. I don't really need to tell you, okay, I don't really need to tell you what a co-cycle is right now. But essentially, what it is is we have a Markov process, which is the UX process. It tells you the location of a single particle given and the velocity field. Given the end of velocity field, and then we need to know the gradient of the flow map at this particle, and that's that's our co-cycle. So, UTXT is the Markov process, and this is our co-cycle. Now, when you want to study stretching of this co-cycle, this deep, this gradient of the flow map, there's a great idea in dynamics that go, that's old. I don't know where it goes back to actually, where you basically want to projectivize this. So, you want to study its evolution on projective space. So, you put in a vector in projective space and you. In a vector in projective space, and you map it forward, and you map it back to projective space. So you're really just tracking the evolution of a linear subspace. And what's cool about this is that this linear subspace will tip into the directions of fastest expansion. So it's like you can still remember where the expansion is happening, but now Xv lives on a compact manifold, the projective bundle of a torus. And so one of the things you have to prove is that this Markov process, UXV, has a unique stationary measure. And And yeah. So basically, putting everything together, so we're basically combining some ideas from the late 80s in dynamics literature. So there's a paper by Baxendale and a paper by LaGrapier. We kind of just have to extend, basically put those papers together and extend to a more infinite dimensional case. So the Markov process Ux is infinite dimensional, but the cosycle is still finite dimensional. So we Still finite-dimensional. So we can really make rigorous this first inverter dichotomy if you have enough good properties. So you have a deterministic family of inner products, which basically tells you that if you know where the Markov process goes, then the gradient of the flow map has to act as an isometry between these two inner products. Similarly, same thing except for lines. So you know that the gradient of the flow map has to map these, well, they're not lines anymore, just general linear subspaces back and forth. Spaces back and forth. I mean, okay, in any finite dimensions, they can be arbitrary subspaces. It's very important to understand these guys are deterministic, and they're also going to continue variously, they continue very continuously with respect to the base process. So essentially, the whole thing comes down to saying that knowing, you need to prove that knowing where the base process goes does not tell you where the gradient of the flow map goes, in a sense. So you need to know that the gradient of the flow map has. So you need to know that the gradient of the flow map has a wide enough set of probable, of wide enough set of possible dynamics, given even if you condition on the starting point and end point of the base process. And once you really realize what you need, it actually becomes a really easy game rather than a really hard math proof. So you basically play a little game where you just have to come up with some example of UX, of a starting endpoint for UX, and show that the gradient of flow. And show that the gradient of the flow map can do basically whatever it wants while still connecting UX, while still connecting these two endpoints. And it's a very easy control problem to solve by hand, at least in the situation that we're considering, because we're forcing on frequency on flows, which are stationary solutions of the Euler equation. So the nonlinearity basically drops out of this calculation because of its special structure. And this is, you know, makes this step very simple. And you just play. makes this step very simple and you just play a little game essentially you know you want to move the particle from here to here so you move some you know you do a shear flow you do a shear flow and then you want to you know move the projective the projectivization of the gradient of the flow map so you put a vortex there and you know so forth um this very fun little game uh okay and that's that's actually it essentially so to get to almost sure exponential mixing it's a very long story um i don't have time to discuss so let me just give you the intuition um Let me just give you the intuition. So, we want to prove this. And of course, when you're trying to prove an almost sure statement, it makes sense that the first thing you're going to try is to use Borel-Cantelli. And so, of course, that's what we do. So, let's descretize time. You just want to know when is this thing bigger than a decaying exponential? So, all right, we don't know that much math. So, one of the few things we know is Chebyshev. So, we're just going to expand this. To expand this, uh, go to bound this by this expected value of the square of the second moment. So, all I did was write out the second moment. But now you see that this looks like a Markov process now, right? It's sort of like you're making some measurement of some statistical system, some expected value question. And so, the Markov process that we're considering here is what's called the two-point particle motion. So, it's now you're labeling two particles, not just one. And you're looking at And you're looking at them, how they evolve simultaneously. Now, this is much harder, and the reason why this system is, this Markov's problem, Markov process is going to be much harder to understand is because it has two invariant sets. So it's going to have more than one stationary measure. So that's because if the particles are glued together, they stay glued together because the velocity fields are smooth. And so, or almost smooth. And so you need, and so that's sort of like. And so you need, and so that's sort of like one of the fundamental difficulties, so to see. So you can prove, roughly speaking, oversimplifying, you can prove this almost sure exponential mixing if you can prove that this Markov process is geometrically ergodic in this sense, so that the statistics converge to the ensemble statistics, which, okay, you also need to prove there is a unique ensemble statistic exponentially fast. Now, there's a very classical theory for doing that. It's very well understood in general. The basic idea is the only thing that's going to be Is the only thing that's going to be more than just technically hard for us is to you need to basically keep the Markov process out of bad parts of phase space, usually infinity, up to exponential fluctuations. But in our case, that also means the particles, when the particles get close together, that's a degenerate part of phase space. And so that's so x equals y is sort of at infinity. Okay. And so what you need to know is that when the two particles And so, what you need to know is that when the two particles come together, they separate again exponentially fast. And there you see, oh, that's a dynamical systems thing. I'm going to need to use the positive Lyapunov exponent for that. And that's true. And so that's where random dynamical systems comes in into this Markov process analysis. So you want to say, well, when the particles are close together, it should be like, should be, you know, linearization says it should be approximately the gradient of the flow map, and we think that this blows up x. From the flow map, and we think that this blows up exponentially. Well, we know this blows up exponentially, and so therefore this makes sense. And so, we should look for a drift condition, this sort of Lyapunov function, which is of this form. One over distance to the particles where p is some small value times some magical psi p, and this is the hard thing, which keeps track of the direction of the expansion and contraction. And then, of course, you also need something to keep the velocity field from becoming super large, you know. From becoming super large, you know, something that says when the velocity becomes large, it comes back. Okay, that's clearly where you need that. And this is just a smooth cutoff to where the particles are close together. So how do we find the C? Well, we're going to use linear approximation. So we're going to say, well, the distance between two particles is approximately the gradient of the flow map, you know, roughly related to the gradient of the flow map. And so for this, we're going to replace the action of the Markov semi-group on this. You know, the action of the Markov semi-group on this general process, you know, on this non-linear distance thing, to the linearization. And so, this is not a Markov process anymore. It's called a Feynman-Katz semigroup because you're sort of snuck in this exponential time integral. I should actually say it's actually not at all obvious. It took us a little bit of thought to prove that this is actually a semi-group from any one space to the same space, right? Because you're sneaking in this exponential time integral of. In this exponential time integral of a gradient, and the velocity field is unbounded. It's not totally obvious this thing's even a semi-group, but it is. It's a consequence of parabolic regularity and control of large deviations. And we want to find, actually, so we're going to construct psi p to be an exact eigenfunction, the top and dominant eigenfunction. And we do that, and not only that, is you can identify what this Lyapunov, sorry, giving away the punchline, what this exponent is. It's not the Lyapunov exponent, it's called a It's not the Lyapunov exponent, it's called a moment Lyapunov exponent. And it isn't just about knowing what happens as time goes to infinity, but it's knowing about fluctuations of how quickly you see the Lyapunov exponent. Because our first theorem was like, yeah, eventually if you wait 500 years, you see chaos. Okay, but it doesn't tell you how long you have to wait. And it doesn't tell you what tails, well, you know, what are the probability distribution of waiting time? So that, of course, we need. And that comes down to proving that this guy. Proving that this guy, to proving good properties of this guy, the moment-Lyapunov exponent. So eventually, what can prove is that there is a moment-Lyapunov exponent, there is a dominant eigenfunction of psychi, which is uniformly bound from below and has good properties. Actually, physicists in the 90s, UMD physicists at the University of Maryland, they actually knew or suspected at least that Batcher's law should be intimately related to fluctuations in finite time Lyapunov exponents. Of exponents. So it's sort of how long you need to wait in order to see this. They didn't make the connection with mixing, but they understood that this is very crucial, sort of a crucial step. And indeed, that's really true. So, because in order for us to prove the strong mixing, we really need to understand these moment Lyapunov exponents. So, all right, there's a long story. It's tactical how to do this. It's a spectral perturbation argument, but keep in mind, psi p is a function of infinitely many variables. So, that makes everything a lot harder. So, quantifying right. Order. So, quantifying regularity and things like that is a little bit more subtle. And then, after that, you need to justify the linear approximation. Again, a lot more complicated because it's in infinitely many variables. That step actually took us maybe the longest to figure out. It was hard for us to figure that out. But I'm not going to talk about it. And of course, once you add kappa, it gets I just want to say, so once you prove mixing for kappa equals zero, that zero that does not tell you that you prove exponential uniform mixing when you add kappa so you think in your heart adding diffusion should not slow down the mixing um it's very intuitive it's it's it's not true if u is rough actually so but if u is smooth you think it should be true but proving it is another story so it's a very singular perturbation and it's not clear how to regularize that singularity so we do not have a theorem that says So, we do not have a theorem that says, you know, it's kind of an abstract theorem that says, oh, any field that exponentially mixes will uniformly kappa exponentially mix. No, we actually have to reconstruct these psi p kappas, and we can at least get uniform regularity estimates on these top eigenfunctions. This is actually classic even in, you know, when you study finite-dimensional PDEs and you look at singular perturbations of linear finite-dimensional PDEs, where, you know, maybe some of the eigenfunctions. Maybe some of the eigenfunctions converge in smooth regularity classes, and some of the other eigenfunctions go crazy when you send the singular perturbation to zero. And that's, we have no idea if that's true here, but we are able to prove that dominant eigenfunctions have uniform regularity estimates, and we can pass them with it in this sense. Okay, that's a long story. I just, but I think we should get to the discussion. The point here is that, okay, yes, we succeeded in proving one universality law. Proving one universality law that physicists care about in turbulence, but it's the easiest one. Well, the easiest one is something called Gaglin's law, which I didn't even discuss, which we proved in the first paper. But Batchel's law, the power spectrum, is a little bit more serious. And we really got lucky because the system we're studying is so easy, it's not quite representative of all the other turbulent systems in the universe. And so we didn't need to be as quantitative in this problem as you would normally have to be for other turbulence problems. Have to be for other turbulence problems. If I had to point on sort of why this problem is really the easiest one, it's also only partly infinite-dimensional. So it's simpler in two ways. And so we got lucky that it was solvable because the dynamical systems tools were there. And yeah, they needed some modification and extension. And that wasn't particularly easy. But also, like, we didn't need to kind of re, you know, we didn't need to develop really new, a lot of really new. Develop really new, a lot of really new to random dynamical systems tools. And that is not the case if we want to study more complicated problems. So, what we're missing is quantitative tools. So, Firstenberg does not tell you how big the Lyapunov exponent is. It's a function of Reynolds number. And there is almost zero tools, even for random dynamics, that can do this for any system that isn't really, really simple and explicit. So, you can do this for some very simple linear systems and some very And some very, very simple two-dimensional OVEs. But in general, we just don't have robust tools for doing it. And so that's what we're doing right now, essentially, if you want to know what we're doing, is developing, trying to develop more quantitative random dynamical systems tools. But I still think that if you want to prove theorems that can be directly compared to experiments, like Batcher's law, so you want to prove these universality laws, random dynamical systems and probability in general is In general, is the right path to go down. Certainly for strongly developed turbulence, I don't think there will be any results in a deterministic case that can really be, you know, say something like the power spectrum is this. So I don't think that we should see, I don't think my grandchildren will see a deterministic version of this theorem. They'll see better versions of this theorem, I hope, but not completely deterministic, I don't think. Okay, that's it. Thank you, Jacob. That was great. I actually did feel the energy coming across in all seriousness. I stay engaged throughout. Now, we have a couple of things. First, before I forget, Brent, are you around? Okay. Yes, I'm here. Oh, you mentioned something about you would like to have a group photo of the participants. I don't know how. Participants. I don't know how that works. So at some point.