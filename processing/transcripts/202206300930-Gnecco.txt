Organizers for this very nice workshop. So, today I will discuss and present the project Causal Discovery in Habitative Model, which is joint work with Nicolai Meinshausen from ETH Zurich, Jonas Petrosit of Copenhagen, and Sebastian Engelke from the University of Geneva. So, the goal of the project is to try to retrieve causal information from observational data by trying to explore. By trying to exploit the signal that lies in the bivariate tails of distribution. And the rationale is that oftentimes causal relationships manifest themselves more clearly during extreme events. An example of this range from, well, earth system science, climate science, finance. Okay, very basic example, for example, in financial markets is that, well, during quiet days, Quiet days, like the variables in the markets, like stocks returns, are hardly correlated with each other. However, during turmoil, there are usually one or two or few drivers which are like driving the returns over the whole market. So before like to start, I lay down really the setup within which we work. So here, consider that we observe key random variables x1, x2, all the way. Random variables, x1, x2, all the way to xp. And each random variable, in this case, xj, depends linearly on its causal parents, plus some independent noise. And here, the assumption we make in this project, the crucial assumption, is that the noise terms are regularly varying with a common index alpha. So regularly varying means that the tail of the noise are approximately powerless. Noise are approximately power low. And simply for the exposition here, I will just consider the case where all the coefficients are positive, even though in practice all the results that I will show hold for real-valued coefficients. So I will refer to this model as a struct heavy-tailed structural causal model. So just to recap briefly, previous and related work that tries to combine. And related work that tries to combine causal inference with extreme value theory. Well, I think it's worth mentioning the line of work done by Claudia Kluppelberg and co-authors. They started in 2018 considering and introducing this class of max linear model, which is somehow related, even though different from the heavy-tailed structural model that I just introduced. And within this framework, they propose. They propose an approach to recover a certain class of duck and the related edge weights. Another line of work has been done by Philippe Navaux and co-authors, where they try to answer counterfactual questions such as what the Earth climate might be without anthropogenic intervention. And they do so by considering extreme climate events. So now I'll introduce actually the main idea. Actually, the main idea upon which the project is based, and this is the causal tail coefficient. So, here let's consider two random variables, x1 and x2, with cumulative distribution function, or capital F1, capital F2. And we define the causal tail coefficient, gamma, in this case in direction 1, 2, as a conditional expectation. The conditional expectation where the event where we condition is the event where the first variable. Is the event where the first variable exceeds a certain threshold Q. And we consider this threshold for increasing values. And given this event, we compute the expectation of the second variable that we rescale between 0 and 1. We rescale the second variable by passing it through its cumulative distribution function. So, in practice, this random variable over here follows a uniform distribution. Folllows a uniform distribution. And to get a bit of intuition about how this coefficient behaves, here I generate a very simple structural causal model where we have, if you want, the cause x1, which is just heavy-tailed noise, in this case, student T noise, and X2, which is the effect, is, well, defined as the cause X1 plus some other independent noise, also student T noise. So this is a realization of a sample. So, this is a realization of a sample from this model. And the idea is the following: if I want to compute this gamma coefficient in direction 1, 2, what I have to do is I have to condition on the event that the first variable is large. And here, if I observe at the points that exceed the threshold, I can see that these points correspond to relatively high observation with respect to the second variable. And so, if we transform this to the uniform. Transform these to the uniform scale, they will be close to one. And in particular, if I push this threshold really higher and higher, the Q threshold, it holds that in the limit, the gamma coefficient in direction one, two is exactly equal to one. Okay? And the intuition here is that in a heavy-tailed system, whenever the cause is large, so will be the effect. On the other hand, however, if I'm interested in computing the gamma coefficient in the other direction, The gamma coefficient in the other direction, what I will have to do is to condition on the second variable being large. And here I can observe that the observation that exceed this threshold correspond to both large and small observation with respect to the first variable. And so if I take the expectation of this observation rescaled on the scale 0, 1 in the limit, so as I push this threshold higher and higher, I will Higher and higher, I will get that the gamma coefficient in direction 2, 1 is strictly less than 1. And so it's this asymmetry in the coefficient, the causal state coefficient that will be key for like the following causal discovery that I will present. And it's very important to notice that here the key is the heavy-tailed assumption of the noise. In particular, if here we generated the same system with Gaussian noise, for example, we would have that in both. Example, we would have that in both directions, the gamma coefficient would be exactly equal to one, and so we would not be able to exploit any asymmetry to recover causal information. So, ideally, we would like given a pair of variables to distinguish between the following causal structures. So, well, this is the case where two variables are completely independent. This case over here and over here is where one of the two variables. Over here is where one of the two variables is causing the other. This is really the nasty case in causal inference. Why? Because you have two variables that statistically are dependent, but none of them is causing the other. So the reason why they depend on each other is because there is a third variable that might be unobserved that really causes both of them. And this variable is usually called confounder. And of course, we can have a mix of two. And of course, we can have a mix of the two effects, so we can have a causal effect from one of the two variables and the confounding effect. What is nice is that it turns out that the causal tail coefficient allows us to distinguish the following cases. So, in the population, the causal tail coefficient will be as follows. So, if I have two independent variables, the causal tail coefficient will be exactly. The causal the coefficient will be exactly equal to one-half in both directions. And this can be seen quite simply from here because if you see this is a conditional expectation, and if the two variables are independent, then this conditional expectation, well, boils down to simply taking the expectation of a uniform, which is one-half. Well, in the case where one variable is causing the other, as we saw in the example, As we saw in the example, we have that the gamma coefficient from the cause to effect is equal to one, and vice versa, strictly less than one in the other direction. And this even holds if we have a third variable that confounds the two x1 and x2. And what is particularly nice is that the confounded case where the two variables are simply dependent, but none of them is causing the other. None of them is causing the other, we have that the gamma coefficient is strictly less than one in both directions. So, really, we are able to distinguish between the causal, let's say, setup where one of the two gamma is exactly equal to one and the other is strictly less than one, from the confounded one. Then, of course, one usually is interested to carry. Usually is interested to carry out causal discovery over more than two variables. So, here now we try to consider what we can do if we have, say, p variables. So, the goal here is to recover a causal order that I denote pi, given a heavy-tailed structural causal model. So, I just introduce briefly what a causal order is. So, if I consider a directed acyclic graph like this one, the causal order is a permanent One, the causal order is a permutation of the nodes such that all the parents in the graph appear before their children. So, here a possible causal order would be, well, I can select x1 first, then x4, and then x2 and x3. Of course, you might notice straight away that for a given DAG, there might be more than one valid causal order, and vice versa, for a given causal order, there might exist. Order, there might exist, a given causal order can be associated to more than one duct. However, knowledge, so it is clear that the causal order does not uniquely identify the graph. However, knowledge of the causal order is already per se quite useful because, for example, if one assumes that there are no hidden confounders, once the causal order is available, one can run really stepwise regression following the causal order. Following the causal order, and the coefficients that are significant will denote really the edges. So, in this case, for example, given this causal order, one would start by regressing variable four on variable one, then variable two onto variable one and four, and variable three onto variable one, four, and two. And the significant coefficient will give really the will be causal. So, here what we do in order to try to What we do in order to try to use the bivariate coefficient in order to recover causal order from p-variables is the following. If we have p variables, we start by computing this bivariate coefficient for all pairs of variables, and we store this causal tail coefficient in a matrix that we call capital gamma, which is of the dimension p times p. And we call that this matrix will be most likely asymmetric because the causal telecoefficient itself is asymmetric. metric and what we do we use this matrix as input to an algorithm that we developed that we named extremal ancestral search and this algorithm really based on this takes as input this matrix with causal tail coefficient and tries to recover a causal order so here in the interest of time I won't present exactly how the algorithm works but I have some backup slides later if you're interested So, the first property that we managed to establish is the so-called oracle property, meaning that if we consider a heavy-tailed structural causal model with an underlying directed acyclic graph, and if we denote by capital pi G the set of all valid causal order associated to the graph G, then we can show that if we use as input to the algorithm the true gamma matrix, so the True gamma matrix, so the matrix containing the true causal tail coefficient, then the algorithm recovers a valid causal order. And this holds even if there are like hidden confounders. Okay, and this is a result, of course, at the population level. So we are saying how the algorithm behaves if we were able theoretically to provide as input the theoretical gamma matrix. But of course, in practice, one wants to. Course, in practice, one wants to estimate these quantities from data. So, here I will briefly present how we can do it with finitely many data. So, first, I take a step back again, and I consider the bivariate case. So, say that we observe n independent copies of the random variables x1 and x2, and these random variables are generated by, again, heavy-tailed structural causal model. So, what we do in order. So, what we do in order to compute the causal tail coefficient, we propose a simple non-parametric estimator of the coefficient itself. So, here, if you remember, the causal tail coefficient is defined as a conditional expectation. So, here, in fact, we do exactly the same. So, we condition on the event that the first variable is large. And given this event, we take the average of the second variable after we rescale it. Variable after we rescale it to scale 0, 1 using its empirical CDF. And here it's very important to notice that we have to choose carefully what we define as an extreme event. So here, how do we define an extreme event? We define an event extreme if, well, in this case, the first variable exceed its n minus k order statistics. And so you see that we have clearly a tuning parameter. Have clearly a tuning parameter k that controls how many observations we are willing to keep in the tail of the distribution in order to compute this non-parametric estimator. And I will show later how, like a heuristic approach, how to like tune this tuning parameter. And what we're able to show is the following. So under quite mild condition, common condition in extreme value theory, namely, we under the assumption that Under the assumption that the number of observations in the tail grows to infinity, so that we are in an asymptotic setting, but it grows at a slower rate than n, so we are really in the tail. It holds that the estimator of gamma 1, 2 is consistent. And actually, we're able, based on this consistency results, also to show consistency of the algorithm. So, here in practice, how would it work? So, if we have data. Work so if we have data. So, say that you have observe a matrix n times p, so n observation and p variables. You would choose k n, which denotes the number of observation in the tail that you use to compute all the pairwise causal tail coefficient. So you would compute or estimate, in fact, the gamma, the causal tail coefficient and store them in such a matrix gamma hat. And you would use gamma hat matrix as input to the algorithm B's. To the algorithm is. And so E's would retrieve a permutation, a causal order that is now estimated. So it's denoted by pi hat and it's random because it depends on the data. But what's nice is the following, is that under the same condition on the number of observation in the tail, we can also show that the algorithm is consistent. So namely, as the number of observation tends to infinity, the probability that the estimated causal order The estimated causal order is wrong tends to zero. So now I want briefly to show a couple of simulation, especially to highlight some of the properties of our method. And so the simulation are as follows. So we consider our algorithm along with other well-established algorithms in causal inference. So we consider two algorithms of the lingam family, pairwise lingam and ICA lingam. Lingam and ICA lingam. We consider rank PC algorithm, which is an extension to the PC algorithm that is a bit more robust when the noise terms are non-Gaussian. And we also consider like the random guessing, where we basically just random guess the causal order. And we use this really as a baseline. So here we measure our performance on using the structural intervention distance. It's a measure that really assesses how well the That really assesses how well the causal order, how actually, how close the causal order is to the true one. And so, zero means that it's perfect, and one means that the causal order is completely wrong. And here we consider first a setting where we simulate structural causal model with heavy tails using as noise student T with 1.5 degrees of freedom. And what you see here, we have different samples. You see, here we have different sample sizes and on the x-axis, different dimensions. Meaning, here this corresponds to simulation where we sample a directed acyclic graph with 10 nodes and so on. And what we observe is the following, is that even though pairwise lingam and also ICA lingam are converging faster if you want to the true proposal order. Also, our method do so. Order also, our methods do so, especially for large sample size. And one explanation why the convergence is lower is the following: the methods like LINGAM, Pairwise, and ICA Lingam use the whole data set in order to recover the causal order. Whereas for us, if you recall, we only use information in the tail. So, in fact, the effective sample size for us is much smaller. And on the upside, also, it's worthwhile mentioning that our method. It's worthwhile mentioning that our method is competitive also because it's much faster computationally. Because, in fact, having only to compute information in the tail can be up to two orders of magnitude faster than the Lingam algorithm. In the second setup, we just want to test whether the method is also robust with respect to hidden confounders. And this seems also to be the case. And it's, well, also important to mention that this even holds to pairwise. To mention that this even holds to pairwise lingam, so here it's still our method is still competitive with this pairwise lingam, which is by the way really geared towards linear SEM with non-Gaussian noise. And in the third simulation, we try something a bit adversarial, but it's important really to test like theoretically how our coefficient works. And the idea is the following: we consider again a linear heavy-tailed structural causal model where after we have Reposal model where after we have generated the model, we post-transform each variable with a monotone, in this case, increasing transformation. In this case, we just use the cumulative distribution function. And the reasoning is the following. The causal tail coefficient that I presented only depends on the rescaled variable. So it in fact depends only on the copula of the variable and should be, in theory, invariant to monotone increasing transformation. To an increasing transformation. And this is what we observe here. While the other methods fail and performance degrades, the is algorithm has exactly the qualitatively the same performance. Of course, in practice, as I said, it's important to choose a reasonable number of observations in the tail. Why? Because, of course, it's like, as always, in extreme value theory, if the number of observations is too large, well, you have for sure little variability, but there will be quite a large. Variability, but there will be quite a large barrier, vice versa. If k is too little, well, then you will probably reduce the bias, but at the cost of increasing a lot the variance. So, here what we do, we again consider the performance of our algorithm for different linear model, different in the sense that we consider different heaviness in the tail. So, from very heavy tail to slightly less heavy tail. So, the noise again is a student T with this corresponding degrees of freedom. With these corresponding degrees of freedom, and what we observe is that here on the x-axis, what we change is the number of observations in the tail. So here it means that we choose k to be equal to n to the power 0.2, k equal to n to the power 0.3, and so on. And we see that a good range of values for fractional exponent ranges between zero three and zero five, roughly for all for the differential indices. For the different tail indices. So, therefore, in the previous simulations and also in the next data application that I will show, we decided, okay, very heuristically to settle for a fractional exponent of 0.4. Okay, so the application I will present now is about river data. We have already seen We have already seen this application today. So, the data set comes from Azadi and co-authors, and it is about in this case 12 stations over a river network in the upper Danube basin. And the data really is daily data over 50 years, where we consider only summer months because, well, in summer months, it's more likely to observe heavy rainfalls, and so it's more appropriate for our mess. Appropriate for our method. So here we have the map of the 12 stations. And what is nice about this data set is that it's one of the few cases where we have a ground truth of the graph. Because of course, given a river network, it's quite easy to argue that the water flows downstream. And so, for example, here, station label number 11 is upstream if you want causing station number 9 and so on. And so what we do is the following. And so, what we do is the following. First, of course, we just draw the corresponding DAG. And in our analysis, we really took this data set, which was, okay, a matrix X with, well, 4,600 observation and 12 variables, and we computed, well, the gamma coefficient, pass it through the algorithm, and we recovered a causal order that, okay, per se might not be particularly interesting, but it's. Be particularly interesting, but it's nice to see that this order agrees with the underlying graph. So to conclude, the method I presented today is a method that tries to recover a causal order from a heavy-tailed variables by exploiting the signal that lies really in the tail of the distribution. And this is done with an algorithm that we name extremal. With an algorithm that we name extremal ancestral search. It's an algorithm that is consistent as the number of observations tends to infinity, and it's also computationally very fast. And the results represented even hold in the presence of hidden confounders. So if you're interested to learn more, this is our paper, and thank you for your attention.