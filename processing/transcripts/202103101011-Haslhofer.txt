Speaker, who is Robert Hasselhoffer, who is now at Toronto. He got his PhD at the MTH in Switzerland, and among many of his achievements, he is a Sloan Fellow right now, as I understand, and he was awarded the Andrea Eisenstadt Prize last year. And he'll talk about And he'll talk about analysis on path space, Einstein metrics, and richer flow today. Thank you. Okay, thanks a lot for the invitation. So, first going to talk a bit about geometry, then a bit about stochastics, and then how those things come together. So, let me start slowly by reminding you about Reiki Karvicha. Okay, so let's take some manifold. So just to remind us, when we have a Riemannian manifold and at any point x, when we have tangent vectors v and w, then the Riemannian metric tells us how to compute the inner product. Okay, and then if you do calculus on a manifold, well, the first thing that is different from the Euclidean space is when you compute second partials, then they don't commute anymore. So when you compute second tests of some. Compute second derivatives of some vector field V. Well, then that doesn't vanish anymore and the differences of trust captured by a Riemann curvature tensor. Okay, so that formula defines the Raymond tensor. Okay, and that's that's kind of a complicated object because it's a tensor depending on four indices, so that's the only way how you can write down this formula. But then uh one one can take the trace of the Riemann tensor and then get something which only has uh two indices and that that's the Reich tensor. That's the page tensor. Okay, so by the symmetries of the Riemann tensor, there's essentially only one way how you can take a trace in my convention. It's the trace over the second and the fourth index. Okay, and why why do we care about the Why do we care about the rigid tensor? So, maybe the best way to explain why the rigid tensor is very important is to actually compute it in good coordinates. So, if you write down in harmonic coordinates, what is the rigid tensor? Well, then you find out it's just minus one-half times the sum of second derivatives of gij plus sum. Plus, some quadratic term that only depends on the metric and its first derivative. Okay, so from the analysis point of view, the Harvard is like the Laplacian of the Riemann metric. Good. Okay, I guess we know since forever that when we ask the question, well, what are the best functions? Best functions, then the answer is obviously that the best functions are harmonic functions. Find the best functions, then you should solve Laplace equation. And well, if you want to find the best geometries, well, then what we should do is we should solve the Einstein equations. Okay, so so that's the the way I've given by my twitch which is By Einstein matrix, which settles the final equation, rate curve equals zero, which you can say of an elliptic lipzer generalization of the Lebmas equation. Okay, and then, right, so then I also like stuff that evolves in time. So, when you when you ask the question, well, what is the what is the best evolution equation? Then, well, if you are. Then, well, if you if you work with functions, then favorite evolution equation is the heat flow. And well, if we work with time-dependent for many and many forwards, then the equation is Hamilton's ratio flow. Okay, so maybe the geometry you start with is not yet an optimal geometry, but you can hope that you can evolve it in time and hopefully converge to an Einstein matrix. Good. Okay, so that's the equations we are interested in. The equations we are interested in for this talk: the Einstein equations and the Ricci flow. So, one feature is, of course, that's non-linear equations. So, then it shouldn't be such a surprise for non-linear equations, but what is one of the main difficulties is you're typically encountering the larities. So, let me give you the classical example for the VG flow first, geotechnical. Both as Teoto Hamilton. So the example goes as follows. So you start with an object that is topologically a three-dimensional sphere, but geometrically has the shape of a peanut or dumbbell. And then if you look here, here you have here it looks like S2 times R. So this two sphere has positive batri curvature. So if you look at the equation, then you see that that's gonna shrink. Okay, so if you evolve this thing by ratio flow, then it will encounter so-called Will encounter so-called neck pitch singularity. Okay. That's just the faster, simplest example, but that's First or simplest example, but that's that's what is happening all the time when you start in non-linear PDE. So typically, you well, you if you flow stuff, you can flow it for a bit, but then at some point you hit a problem, you hit something allowed. And a similar thing also happens in the elliptic settings of Einstein equations. For example, if you study sequences of solutions that pass to a limit, then often this limit can also be seen. Then often this limit can also be singular. So, here let me also give like what maybe was historically the first example. So, that is some nice Einstein metric discovered by two physicists, Eguchi and Hansen, I think also in 82. So, they wrote down some explicit self-dual answers and found a metric which looks like this. Okay, so it's a metric on the tangent bundle of S2, but that's first of all a four-dimensional manifold. And so asymptotically, it looks like flat R4, but mod set two. So you can kind of write this down in. This down in spherical coordinates or something, but then you have this identification of x and minus x. Good. Okay, and then it has this central two-sphere here, which has some size A. So in fact, you have a one parameter family of solutions depending on the size of this central two-sphere. And then you can just take a limit where the size of the limit is. limit where the size of the central two-sphere goes to zero and then you as a limit you just get the tangent cone at infinity so as a limit here you get you get that the cone over rp3 okay and the particular here has a sequence of smooth Einstein manifolds but then the limit again has a singularity  Okay, and for other PDEs, geometric PDEs we care about, there's usually some good notion of weak solutions. Like for example, for the heat equation, that's very easy to say what is a weak solution. You just multiply your equation by a test function and integrates by parts, and that's your definition of weak solution. But for each curvature, you can't just multiply with a test function and integrate by parts. In fact, it was an open problem for quite a long time to find the notion of weak solutions for the Einstein equations and directive law.  So, you flow for some time as a smooth manifold, but then you hit the singularity and then you want to continue the evolution in some more general setting. It's not smooth manifolds anymore, but maybe some metric measure spaces or something like that. Okay, good. So, that was the little bit of geometric motivation I wanted to start with. Any questions about this geometric part? Good. Okay, so then let's move on to stochastics. So let's talk about Brownian motion and manifolds. Okay, so I don't have 36 views, but I at least have three and a half views. Okay, so let's have some basic discussion. How can we think about quality motion on a manifold? One way to think about this is a limit of random box. Fundamental to the colour of the colour okay, so you're standing somewhere in your metaphor and you start at some initial point x naught, and then whether the in your x naught and then well in your in your tangent space you just pick randomly a direction and make a step epsilon and then you pick again a random direction and walk along that geodesic for step epsilon and so on and then you take a limit of this random walk where the step size epsilon tends to zero okay and what you get in the limit is rounding motion and of course you should do Right, of course, so time scales like distance squares. If you have a spatial step epsilon, then you should do a time step epsilon square. Good. Okay, so that's one very concrete way to think about Brownian motion on the manifold, but maybe not the most practical way if you actually want to compute anything. Okay, good. Okay, good. So another way to think about it is uh this uh this uh what is called uh holding without slipping. Good. Okay, good. So there are the pictures you think of your manifold as like a stone or something like that. And that's lying on your table as Rn. Right, so then we need some French geometry to. Some French geometer to help us, so that's that's Eileen Cartin. I think there is a question about random walk in the chat. Does this mean that the limit of the random walk is independent of the metric used on the manifold? Use on the manifold. Oh, it's not because you're uh, so I didn't write this down, I only said it in work. So your epsilon step is a is along along a geodesic, and the geodesic depends on the metric. Thank you. So it's a geodesic random walk on the whole. Yes. Good. Okay, so here the pictures. Okay, so here and on Euclidean space, we know what. On Euclidean space, we know what Brownian motion is early. And so, here, let's say that's a path of Brownian motion on Euclidean space. Let's call this Wt. And let's say we have drawn this path here with some fresh ink. And then our Eli Khatan takes the manifold, the stone, and rolls it along this path. Path, and he's a very skillful person who goes along this path without slipping the stone. And when he does that, then that prints a path here on the manifold. So that's the path Xt, and that's the path of Parliament motion on the manifold. Okay, so using this construction of folding without slipping, you can translate Pranian motion on the Euclidean space to Prunian motion on the manifold. Running motion on the manifold. Good. Okay, so let me write this down maybe a bit more formally. So, what does this mean in more mathematical language? So, this means one looks at the frame bundle. So, for the frame bundle, the fiber above any point X on the manifold is just all. Manifold is just all autonomous frames on this tangent space Txm, or equivalently, that's just all autonomal maps from N to Txm. Good. Okay, and then right on this frame button, you also have end card. Fame button, you also have n canonical horizontal vector field. So you so you just take the standard basis vectors EI and your client space, then u of ei is some element of the tangent space txm, and then you can lift it uniquely to a horizontal vector that projects down to that one. Good. Okay. Yeah, so once you have this setting, then you write down the following stochastic differential equation and you find bundle So here you can see this fresh ink stuff in terms of an equation. So the horizontal vector fields are exactly what allows you to translate those motion WT and Rn to a motion on the frame bundle. Good. Okay. And when you do that, Good. Okay. And when you do this, then in fact you get more. You get brownian motion, but you also get some stochastic parallel transport. So here along this curve, you could also draw your basis vectors right, and then they also get printed somewhere up here. So in general, it's a non-obvious problem if you have a curve that is not C1, how to do parallel. This is not C1 how to do parallel transport along that curve, but that's the way how to solve it, right? So you're cleaning space, of course. You know how to parallel transport, and then you just print that on the manifold. So namely, just two things you get. So Xt, the projection of UT to the manifold that is Brownian motion on M and And P T as stochastic parallel transport. Good. Okay, so that's that's probably the best way to think about Brunei motion on a manifold if you if you want to compute something. Of course, another way is. Another way is, yeah, maybe that's what I should have said before. That maybe the quickest way to explain it is just in terms of the heat kernel. So if you know the heat kernel, then you also know what Bruni motion is. Yeah, so the the heat kernel tells you what are the transition what is the transition function for your process, right? Concension function for your process, right? Okay, so take the heat kernel and then give them any k times ordered by size and any k sets u1 up to uk. We can ask, well, what is the probability that at time t1? probability that at time t1 we are in the set u1 and at time t2 we are in this set u2 and so on and and the answer to this question is just given by integral over a product of heat curve so that's just equal to u1 so in time t1 you go from your initial point x0 to some point x1 which lies in u1 and then time In u1, and in time t2 minus t1, you go from x1 to x2, which lies in your set u2, and so on. And your integer overdose sets u1 up to uk with respect to that volume of measure. A with respect to that volume of measure on your manifold. Okay, good. So that's how you understand Brownian motion in terms of the heat kernel. And okay, that's my 3.5th thing is this physical intuition. Good. Okay, so as I first assessed, one would say, well, okay, so we have this probability measure in the space of curves Xt and what is this probability measure? So that would be e to the minus the minus the kinetic energy of the curve. Kinetic energy of the curve times the Lebesgue measure. And of course, this formula doesn't make sense for many reasons. First of all, for almost every curve, this energy is actually infinite. And then the other problem is, of course, there's no Lebesgue measure in infinite dimensions. But somehow, those two nonsensical things somehow cancel out. Cancel out. So each of times infinity makes a bit more sense, right? Okay, so yeah, but even though it's a meaningless formula, it's still often good, good for intuition to holistically motivate some self. Good. Good. Okay, any questions about this general part? Good. Okay, so then let's talk about how do the geometry of feature coverage and the stochastic analysis of Brown and motion, how do they come together? So let me talk about integration by parts on path space and the spectral gap. Ah, okay, so maybe let's start with the button button exercise on the keyboard space just to motivate things. Elidean space just to motivate things. So, in Euclidean space, if you have two functions f and g, and you have the directional derivative of f in the direction of some vector h, then you of course know how to do integration by parts, right? So that's f times minus dhg, but put be in the Euclidean space, and if instead be on the Euclidean space and if instead of the Euclidean measure you do this with the Gaussian measure e to the minus x squared over 2 If you do this integration by parts with the Gaussian measure then you get an extra term and the dh hits your density this exponential thing right so the extra term you get is just So the extra time you get is just g plus the inner product of x times h, which comes from this derivative of x squared over 2. Good. Okay, so that's a formula you can actually understand in two ways. Well, one way what we just did, this integration by parts, and maybe the more conceptual way how you could understand it is whether you could say, well, I make this change of variables. So x. So x goes to x plus epsilon times h, and then I think, well, how does my Gaussian measure change under this change of variables? And then I differentiate with respect to epsilon and set epsilon equal to zero. And then I also get the integration by pass formula again. Okay, good. And if you look at this physical intuition, then you see well that then you can like formally do kind of a similar computation. To kind of a similar computation, you can say, well, what happens when you send x to x plus h, when you shift the measure? And okay, good. And right, okay, so now it's time to mention our guest of honor, Sir Bruce Freiber found a huge generalization of this formula to the path space of Riemannian manifolds. Okay, so in fact, they proved a more general CRM of what is happening under those shifts. Okay, so the CRM is the Wiener measure on path space. So this path space, I just mean the space of all continuous curves. mean the space of all continuous cuts on your manifold. So the Wiener measure on path space is quasi-invariant under Kamal Martin shifts. Okay, so Cameron-Martin shift is a shift by some function h of t which has finite energy and quasi-invariant means that this shifted Vienna measure is absolutely continuous with respect to the unshifted one. Okay, and in particular when you differentiate this, then you get the integration by parts formula. Okay, so now the expectation is given by the integration with respect to the Wiener method, which is some infinite-dimensional Gaussian integer, and you get the formula, which formula looks like the one from the exercise. Formally, it looks like the run for the exercise. Okay, and one one reason why that's a really amazing formula is because um and why I really like it because because The packet because v2 curvature shows up in the back v2 coverages, but that's that's probably the first time that one can see that well, v2 curvature shows up in those stochastic computations. Good, okay. So that was like the starting point for this field. And then let me also mention another historical CRM as the log Sovelif inequality on path space. Okay, so I guess for the log sovelif, I can mention quite a few. Uh mentioned quite a few people who are here in the audience and the clear end space that's due to Lenny Kras and then okay yeah maybe let me put the lock for left and and PyCarray in one CR then. Elton Su talked about this also. So the CRM is if the Reich curvature has two-sided bounds with some constant k, then one has a good log servile of inequality on path space with a constant that only depends on k. I guess I normalized here that the L tool norm of f is equal to one. Okay, so and and in particular if you linear is the logs upper left and you get a point of the inequality. upper left and you get a point inequality, meaning you get a spectral gap. So in particular in particular, you get the spectral gap for the onstand Ulbeck operator. I'm standing Ultek operatum pass base. I think maybe that was first defined in this paper by Driver and Trekner. Good. Okay, so VC curvature shows up very importantly when one does analysis on past space. And if one has VC bounds, then one gets good estimates on past base. Okay, good. Other questions about this historical overview? Good. Okay, so then let me talk a little bit about some. Let me talk a little bit about some new advocacy. So that's mostly trying to act with our neighbors. Also, we kind of wanted to understand the opposite direction. So, what I talked about before as well, if you have specific bounds, then you can prove good estimates on path space. But conversely, we Some path space, but conversely, we wanted to know: well, does analysis on path space does this help us to better understand Reiji curve better? So, can we go the other direction? And so, maybe a good starting point is the classical Bochner formula. So, when you study lower bounds on behalf of curvature, then that's how all the analysis starts. So, that's just a simple exercise. So, if Ft from M to R solves the heat equation, okay, so maybe let me put the factor one half here. Uh then. Then you get what is called the Bochner formula. So then the evolution of the gradient squared is given by this following formula. One half times the Laplacian minus the Hessian squared. Okay, and that would be it if you compute on Euclidean space. But on the manifold, you get one extra time. And on the manifold you get one extra term, which is a Ricci term which comes from interchanging the gradient and the Laplacian. Okay, so that's a simple one or two line exercises to derive this classic Hubna formula. But that's a really important formula because, like all the theory of spaces with Riggy curvature bounded below, everything starts with this formula. Below, everything starts with this formula, right? So you see, well, if you want to estimate the gradient level, that exactly marks if reach is bounded below. So if reach is bounded below, you can use this formula to derive a lot of wonderful estimates. Prove all your favorite estimates on the manifold. You first prove the gradient estimate and then you do a bit of more work and prove the log sub relevance and so on and everything with a sharp constant depending on k. Good. Okay, so then So then in this joint work with Ern Neighbor, we generalize this Boffner formula on the manifold to some infinite-dimensional Boffner formula on the path space of the manifold. Okay, so the correct generalization of the heat flow in a manifold path space is not a heat flow. Parsephase is not a heat flow, but that's martingules. So if Ft from Pm to R is a martingale on path space, then we have this infinite-dimensional Bachner formula, which looks as follows.  Okay, so that's our Bachman formula on path space. So I hope you agree that's a Bachner formula because well on the left hand side there's the evolution of some gradient squared and then the right hand side the first term is the prime emotion that's a diffusion term and then there's some Hessian term and then there's a H term. Okay, so I should define the objects. So here, let me explain it in the case f is a cylinder function. So if f is a function that just depends on the curve on k given times t1 up to tk of this k-point function, then this s gradient of f is just, well, take the gradient at all times that are bigger than or equal to s. That were equal to s, so the gradient with respect to the i's entry. Okay, and then when those gradients live in different tension spaces, but then we have stochastic parallel transport, you can transport them all back to the tension space at the base point, and then you can sum them up. Okay, so that's the infinite dimensional Boffner formula. So to explain the link between this infinite dimensional one and this classical one, let me just give you the simplest example when you plug in a one-point function. So if f of gamma is f of gamma t1. of gamma t one, the one point function, then the induced martingale is just given by the heat flow. Okay, so the induced martingale of a one-point function well, if you remember, the Wiener measure was just this product of heat kernels, right? So if it just depends on one time, that's just a heat flow. Here the sign is actually minus sign, so it's a backwards heat flow. So that explains the different signs in those two formulas. So you get this backward heat flows, and then so then this So then this infinite dimensional formula just becomes the usual formula. Okay, so in the simplest example, our infinite dimensional Bach now reduces to the usual one. But then of course in power space there are many more functions than just one point functions. That's why it's a more powerful formula. And in particular, it's while that you While the usual Bafner formula only characterizes lower rigid bounds, the path-based Bafner formula is actually strong enough to characterize two-sided bounds. So in particular, as a corollary, we get, let me just write it down in the case k equals zero. So we can play the same game as, you know, you have to use the Buffner formula, you use it to this. Boffner formula, you use it to derive the gradient estimate for the heat flow. So now we take our Martingale-Baffner formula. Well, let's assume it's rich and flat, then things go away, the Hessian has a term, has a sign, this Martingale term goes away when you take the expectation, so you get a gradient estimate. Namely, the estimate you get is that the gradient of the expectation of F is less than or equal It's less than or equal than the expectation of the question in the chat. Can you say more about FT? I think. Is FT typically conditional expectation at some endpoint? Is that what? Yeah, so here Ft was just the induced martingale. So that's just the expectation of F given the sigma algebra up to time t. Time t. I can think that way for the more general function f, where it's not a function of one point. I can take that as my meaning of that f sub t up there. Yes, yeah, yeah. Thank you. Okay, so it turns out this estimate actually characterizes Einstein metrics. So you solve the Einstein equations if and only if on par space you have this sharp gradient estimate for all test functions f. Okay, so here the left-hand side is well the expectation of boundary motion given the initial point x. How does it change when you change, that's the number, right? How does it change when you change the initial point x? When you change the neutral point x, is it continuous or even lip shifts with some sharp estimate? And it turns out it is if and only if the underlying manifold is reaching flat. Good. Okay. Right, so the simplest example of one point functions that's again the standard gradient estimate for the heat flow. But the main difference is now it characterizes solutions of the Einstein equations, not just super solutions. Equations, not just super solutions. Okay, so this motivates the definition. So a Bihmani metric measure space solves the Einstein equations as a weak solution of the Einstein equations equations by definition if and only if this infinite dimensional gradient estimate. Infinite dimensional gradient estimate holds on par space. Okay, so the Wiener measure still makes sense on any remaining metric measure space. So to write down ZG curvature would need second derivatives of the metric, but to write down this estimate one needs much less. So that one can do on any reasonable metric measure space without any C2 structure. Good. Okay, and well, a large class of examples of solutions in this more general sense is any non-collapsed limit of Einstein metrics. Okay, so that's what I brought up together. This is one of our postdocs, one Chung Choi.  Okay, so being a solution is preserved under those non-collapse limits. So for example, a Gochi Hansen example I showed you. Good, okay. And for the last minute, let me just mention there's a similar story in the parabolic setting. So for the RG flow, that's also what. Okay, so the estimate looks quite similar, but it takes a bit of time to set it up. Like, what is this problem suitable problem in motion and space-time, and so on? Okay, but then we prove some similar gradient estimates. Prove some similar gradient estimate that characterizes solutions of the Ricci flow. Okay, so let me call this estimate parabolic gradient estimate on path space. And then, right, so then in work in progress with Bunch and Chai, we prove that again this That again, this estimate is preserved under passing to non-collapsed limits of freedchief loads. Okay, so I think that's a good place to stop. Thank you. Thank you very much. Let's thank the speaker. Okay, any questions?