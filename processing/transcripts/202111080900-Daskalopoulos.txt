Geometric flows, it's not quite what we had hoped for over like three years ago when I applied, but here we are. So, without further ado, our first speaker is Tody Doskalopoulos, who will speak to us about type 2 smoothing and mean curvature flow. So, please. Hello to everyone. I don't say morning to everyone because I see many friends from other Many friends from other places. So, I first of all I would like to thank the organizers for making this possible. Can you hear me? I can hear you. Okay, it would have been great if we were all together, but let's hope sometime very soon. So. So, let's see now because something strange happened. Let me, okay, I need to, sorry. Ah, that sounds maybe I'll do slides. Let me, sorry, because I okay, that worked. All right, so what I will discuss in this talk. I will discuss in this talk type two singularities in hygiene genes. Hello. We will discuss type 2 singularities in mean curvature flow and the continuation of the flow through these singularities. Whatever I would say today is joint work with Sigurd Angenand and Natasha Session. So the crucial So, the crucial point in what I will discuss today is that the mean curvature of the hypersurface remains bounded before and after the singularity. And I will discuss next the significance of this point. So, in the examples we're going to consider, the hypersurfaces are O fork. Are O4 cross O4 symmetric or more generally On cross O1 symmetric where n is bigger than or equal to 4 and they're embedded in R2n, R8 in this particular case and they're asymptotic to the Simon's cone at spatial infinity. I will introduce the Simons cones for the younger people a little later. And the singularity And the singularity would be attained at the origin. So, I don't think I need to spend much time introducing the mean curve to flow here. So, it's the evolution of hypersurfaces with a speed which is equal to the mean curvature. And we will consider in this talk finite time singularities, which means that. Singularities, which means that at some time t less than infinity, the second fundamental form blows up. And we know that such singularities will always occur in the compact setting, and they also occur in the complete non-compact setting. The question is, does the mean curvature always blow up at a finite time singularity? So, this is a question that I'm not going to repeat. Is a question that I'm not going to really answer today, but it's related to what I will discuss. And this is not my question, has been a question that has been open for a while and it seems to be a hard problem. So equivalently to this question is if, for example, if the supremum of the maximum over the mass The maximum over the manifold of the mean curvature is finite up to time capital T. Does this imply that the second fundamental form doesn't blow up at time capital T? Therefore, we can continue the flow. So if you want to see it more as a PB question, can we have that H bound that implies the second fundamental four bound? Now, in the mean convex. Now, in the mean convex case, this is known. This is a known result by Gerhardt, who showed that the second fundamental form, the norm of the second fundamental form over the norm of the mean curvature decreases in time. And therefore, H is never bounded near singularity. And equivalently, if H is bounded, the second fundamental form is bounded. So something happened here. So, something happened here. In the general case, there were some earlier works by Lin and Sassoon, Lien Sasum, and Zhao, where they saw under special assumptions, such special assumptions is, for example, type 1, but there are also other assumptions, some type of L2 norms to be finite of the curvature, etc. So, under some special assumption, then Assumption, then AIDS needs to blow up at the first singular time. However, the general question, especially the case of type 2 singularities, remains open. So in the generality. So we can rephrase this question saying does the mean curvature blow up at the finite time type 2 singularity? And there is a fairly recent result by Lien Wong. Result by Lien Wong in 2019, published in 2019, that answers affirmatively the case under the assumption that the dimension is two. So you have a compact two-dimensional hypersurface in R3, then the mean curvature always blows up at the third singular time. This is This is a beautiful work. It's not easy by any means, and it combines PD techniques, techniques that we all are familiar from Minkerva to flow with regularity theory of minimal surface. Why regularity theory of minimal surface? Because if you assume that you have at the single You have at the singularity h is bounded, then by rescaling the flow to make the second fundamental form bounded at the singularity, you end up at the limit with the minimal surface. So classifying these minimal surfaces that you get as limit is the main point there. However, the question, the general question remains open in dimensions between three and seven. three and seven. Now in dimensions bigger than or equal to seven we knew that there exist type two singularities and it was conjectured a while ago by Velasquez who found these type two singularities that for these particular examples the mean curvature remains bounded at the singularity. However this was not shown until very recently. Zone until very recently. So I will talk about this a little bit more in detail in what follows because it's related to the subject of my talk, which is our work. So for the remaining of the talk, we will consider O1, ON, croso-ed hypersurfaces. So let me tell you for a minute how you You for a minute how you parameterize these hypersurfaces. So, the notation that we'll be using is: we have x which is x1, xn, y, which is y1, yn, and x, y, capital R in R2n. And if you take a hypersurface in R2n, which is 2n minus 1 dimensional and has O n cross O n symmetry, then the surface is completely determined. Then the surface is completely determined by its intersection with the first quadrant of the x1y1 plane. And this intersection can be expressed as a graph y1 is u of x1. And we will drop the index 1 because for the moment we will just consider just the profile function u as a function of x. Right, so u as a function of x. So, u as a function of x is the profile function of such a hypersurface. Of course, t will be, this u will depend on time. And let's assume for the moment that u of 0 is 0, although u of 0 is not going to be 0 for later. Smoothness at the or however, smoothness at the origin implies that u any odd derivative of u at zero must be zero. Of u at zero must be zero. And the mean curvature to flow under mean care to flow, the profile here, there's a t missing, u of x, t, satisfies this equation. So this is a simple PDE. However, if when u goes to zero, this becomes a singular, as we will see. So now we are going to consider hypersurfaces. Consider hypersurfaces that they are, as we say, O n cross O n symmetric, and at infinity they approach the Simon's cone. What is the Simon's cone? It's all the x, y and r to n, so that mod x is mod y. And this is a surface that has zero mean curvature everywhere because it's like a cone away from the origin where it's singular. Singular. Alan Carr constructed a smooth O1 cross O1 symmetric minimal surface, which designularizes the Simon's cone at the origin. So it's smooth and far at infinity, it approaches the Simon's cone, but near the origin it's smooth. And this is given by a profile function w of z. function w of z I don't call it w of x because I will be using it in different scale of variables and w it satisfies this nice equation so it's steady state of the equation that I prescribed I described above and because and if we want since it's If we want, since it's smooth, w prime of zero is zero. And to choose one out of these smooth solutions, you impose the condition w of zero to be one. And every other one is given through scaling. W sub k is k w z over k. So Velasquez back in 1994 constructed solutions that That they are O n cross mean curvature flow solutions that they develop a type 2 singularity at time t less than infinity. They are complete, non-compact, and they approach the Simon's cone far away. They approach the Simon cones at parabolic scale near the origin. Scale near the origin. And if you focus closer and closer near the singularity, they approach after rescaling to the Alan-Carr minimal surface. So if you rescale so that A square is bounded, then you end up with a minimal Alan-Carr minimal surface. And the last case, as I said earlier, had conjectured that this. Earlier, I had conjectured that these solutions had bounded mean curvature at the first singular time, therefore, providing an example of a type 2 singularity with bounded mean curvature. So, in dimensions n bigger than equal to 7, which showing that this question that I posed at the beginning, I posed at the beginning, has a negative answer. Negative answer in the dimensions n bigger than or equal to seven, as it was expected due to similarity with the minimal surface theory. Now, Velasquez gave in his construction gave pretty good asymptotics of the solutions he constructed. However, the asymptotics and he showed that, of course, the convergence to That converges to very close to the singularity after rescaling to the Allen-Carr solution. However, just from the convergence to the Allen-Carr solution, you cannot easily deduce that the mean curvature H is bounded in the original scale. Of course, it will be bounded at the scale in which converges to the Allen-Carr solution, but not in the original scale. And we, myself and my collaborators, had thought about this problem, and we tried to use more PV arguments to show, to improve somehow the Velasquez asymptotics, therefore showing that the age is bounded all the way up to the origin. But we could not, and Stolarsky in Stolarsky, in Max Stolarski, a young mathematician we all know, in 2020, rigorously showed the conjecture of Alaska's and therefore that in his examples, age is bounded. Now, the proof is not easy, and it combines a priori estimates. A priori estimates to bound the solutions, the age a little bit far away, but to deal with the origin, he uses a very beautiful blow-up argument. And we will use something similar, although not exactly, we modify his technique in our problem. So you will see Stolarchy's idea why at the end of the talk, why Why, at the end of the talk, why I describe also what we do. All right. So now we have settled Velasquez's conjecture in the sense that we know now that we have these examples where the second fundamental form blows up at the origin, let's say at time zero. Now we bring the time of the singularity at zero and H remains. And H remains bounded all the way up to t equals zero. Velasquez has suggested some formal way in which the solution can be continued after the singularity for t positive. And the whole point here is that the solution is continued after the singularity in such a way so that h remains bounded. And this is what I will discuss today. What I will discuss today, what we did with Sigurd and Natasha, and therefore we obtained a solution together with Velasquez and Stolarski's results, and solution, a mean curvature flow solution, which is complete, non-compact, is in R8, that is defined for T a little bit negative and a little bit positive between minus negative T0 and plus T naught. negative t naught and plus t naught is smooth everywhere except of the origin at time t equals zero and has uniformly bounded mean curvature although the second fundamental four blows up near the zero the origin at time t equals zero and the this result we expect to be to generalize for n bigger than For n bigger than 4. However, we did it only for n equals 4 for simplicity. So what is the problem here? We take an initial datum, which is singular, has the asymptotics of the Velasquez, what follows from the Velarsky-Stolarsky solution at time t equals zero. So we take with this. So we take with this, we start with this initiate data and we try to construct the solution in such a way so that H is bounded. And so let me just tell you how does this initiate data look like. So what I will show you is the expansion of our initial data near the origin. Far away, away from the origin, it doesn't really From the origin, it doesn't really matter how it looks like because this has really to do with the origin and with the singularity at the origin. But so what we are assuming is that u0 of x is x, behaves like x plus some parameter k0, positive parameter, x to the 2k minus 1 plus little order of this. And so here, Of this. And so here, little k and capital K naught are two parameters of the problem. This we fix for every fixed parameter, we will construct a solution that can be changed. Sorry, I may keep making this mistake. And now k0 can be anything that is positive and we are going to see that this parameter k it will be bigger than equal to 4. It will be bigger than or equal to 4. Now you may say, why is this k bigger than equal to 4? Now, if we need k to be bigger than equal to 2 for the Velasquez construction to make sense, and we need k bigger than equal to 3 at least to show that h is bounded, although we don't know, like, so maybe h is bounded for k equals 3, but both the result also. But both the result of Stolarsky and what we do is we need k bigger than or equal to 4 so that h is bounded. And we also assume some technical condition far away at infinity. So we want that far away at infinity, u0 double prime of x to be bounded and u prime to be bigger than equal to 0 and less than equal to c naught. C not. I mean, okay, this is right. So these are conditions that we need for all x positive, but these are not so important. And so we will construct a solution of this initial value problem. So it's a solution defined only for x positive, t from zero to some t naught, and u will be a positive profile function, satisfies this simple. Satisfies this simple PDE, and we would like it to be smooth for t positive. Therefore, u sub x has to be zero at the origin. And we would like it to converge in the, let's say, c0 sense to u0 of x, which is the profile given by the last case. So, again, what is the problem here? The problem here is that since Since you have, you see from the asymptotics here that x approaches zero, u goes to zero, u0 goes to zero. Therefore, this three over u becomes singular. So what makes everything happening is that this 3 over x ux minus 3 over u gives you something finite at the end, but to have this, you need pretty good estimates. Estimates. So the theorem we prove is that under the conditions I described above, there exists a solution which is C infinity smooth for t positive. The supremum of H is finite all the way up to T equals zero. And despite the mean curvature of M naught or to be undefined. To be undefined at the origin, at least the way we see it from the asymptotics. And the corollary is what I mentioned already. So for the remaining of the talk, I will just give you an idea of how we construct the solutions and how we prove that H is bounded. Now, the construction follows for the construction, we can do barriers. So this is different. So, this is different than what Velázquez did. Velasquez, because of the instability he had going forward to the singularity, he could not construct the solution with barriers, although his proof uses barriers. So he needs to do a more subtle argument. But in our case, we can do it with barriers, but the barriers have to be very precise and done very carefully so that you get the control. So that you get the convergence to the Allen-Carr solution at the origin, which is the beginning of everything. All right, so let me just first tell you what is a formal expansion of U, of the solution we expect. So the solution that coming out from the singularity in the three regions as expected: there is the outer region. Is the outer region, which is quite far in terms of paragolic scaling from the singularity, and then there is the intermediate region and the inner region. Now, the closer you go to the origin, the harder is to obtain estimates and the more is that you have to work. So, the outer region, in so, in the outer region, it's not hard to see that the solution The solution will for T positive will behave like this. Where does this come from? Comes from just the PDE because it follows from the expansions that you expect formally at least that u of xt to be u 0 of x plus t u sub t of x 0 at time t equals 0 plus little order of t. So this comes from just a So this comes from just a formal expansion, of course, and to prove it is not hard with some very rough, pretty rough barriers. Of course, the difficulty comes as you go closer and closer to the origin. So for the intermediate region, what does this mean? The intermediate region means that if you perform the parabolic scaling, y equals x over square root of t, remember t is very small. T is very small, so this means you enlarge your variables, right? Because you start focusing more and more near the singularity. And so you define u of xt to be square root of t v of y tau as usually, and tau is log of t, and although t is positive, log of t is negative because t is small. And the profile V satisfies this equation. So what the So what plays you will see in most of these problems what plays an important role in the analysis is the linearization of this equation at the Simon's cone, which is the simplest, the first order asymptotics. All right, so let's All right, so let's do that then. So let's write the solution V. Let's write our solution V of y tau as y plus a small order term, an error term, f of y tau, as usually. And then the evolution of f is f sub tau is L of F plus an error. Plus an error, and this L of F can be written as this one half because this is just the linear, you linearize on the Simon's cone, but the Simon's cone gives you this one half because it's one plus whatever you call the Simon's code, the V prime. V prime at y squared. So, this is a nice linear operator. Actually, it's not so nice because y is small, so you get singular behavior. Nevertheless, we know very well the spectrum of this operator. So, I will discuss this in a moment, but just Just so we would like to match the behavior in the intermediate region with the behavior in the outer region. So, therefore, we first need to write this behavior that we know now in the outer region to express it in terms of intermediate region variables. So, we write this expansion. So, this is an expansion that comes from just the outer region. So, it's y, right? So it's y, right? This is the Simon's cone, plus some constant. And this is a k minus three half tau in the tau variable, time variable, y to the two k minus one. And k is and capital K naught are our parameters. And so we know that this happens when x is bigger than square root of t, much bigger than square root of t, therefore y is much. therefore y is much bigger than e to the minus tau over two. So you see y is huge because tau is negative. And now we match this behavior with a function of t multiplied of tau multiplied by eigenfunctions of the operator L. Right, so this is similar to what Velasquez did for the going. For the going forward to the singularity, sorry, going to the singularity behavior for his construction. And so this matches well. And therefore, we expect that our second order, our F, looks like some constant K1, which depends on K0 and little K to be adjusted. not and little k to be adjusted so that these behaviors are exactly the same e to the k3 half tau phi k of y so phi k of y is the eigenfunction of this operator and you see this y to the 2k minus 1 matches perfectly with what you have therefore this is the right match the right behavior so this is what gives you the first order of the appropriate Order of the approximation of F. However, for our barriers, you need to go to much better approximations, but I'll spare you the details because you will get bored. And finally, let me just tell you what happens, how now you connect the intermediate region behavior with the inner region behavior. So now So now we have seen that in the intermediate region, the solution V looks like y plus this, all right? And now, of course, phi sub k has the behavior, what I saw here, has this behavior which matches perfectly with what we had for y large. Now, for y small, as y goes to zero, the behavior changes. And what how does changes and what how does the how does this behave behaves like one over y squared singular as because y as y goes to zero and the intermediate region goes close to y equals zero so therefore you expect that your asymptotics work well up to the level at which you can control this behavior. Which you can control this behavior. So for y small, but kind of we you know that y is k1 e to the k3 halves tau over y squared. All right, so you know that when y is small, you have this behavior. And now how small, how far you can go. And so this is what defines the The scaling in the inner region. So, what is gamma? Here, gamma, 3 gamma is k minus 3 over 2. I just wrote it as gamma, so I don't need to write this, bore you with these exponents. So, if gamma, we fix gamma to be k third minus one half, and you see that you need gamma to be positive, otherwise nothing of this makes sense. So you need. Of this makes sense. So you need k, which is an integer, to be bigger than three halves. Therefore, you need k to be bigger than or equal to two for this to make sense. But we are assuming that k is bigger than or equal to four, so that's not a problem. And now you call this to be more or less one over z squared, so not exactly, because you have to rescale it the right way. So how do we rescale therefore the Do we rescale that for the inner region? We in the solution in the inner region, we set v of y tau to be e to the gamma tau w of z comma tau and you take z to be y e to the minus gamma tau. Right? So this behavior, this scaling makes this behavior to depend only on z. And how does it behave? Behave like z. It behave behave like z plus one over z squared or plus a constant over z squared, right? So, this is the scaling that you do. And w, little w, satisfies this PVE and you see that tau is negative, gamma is positive, as tau goes to minus infinity, which is equivalent to t approaches zero. Under reasonable assumptions, you expect to have that these approaches. Have that this approach is zero, therefore, you converge to a minimal surface, which is the Allen-Carr minimal surface. All right, so that is what gives you the convergence formally to the minimal surface. Now, let me just tell you a few words on how you make this formal asymptotic more rigorous. First of all, let me summarize the three regions. The three regions is the outer region where outer region where at big where the parabolic scaling x over square root of t, y in other words is big, right, far away, is big, then you have the intermediate region. Now the intermediate region goes up to y r e to the gamma tau, right? This is the inner region and goes region and goes it can go it can go up to more or less this is e to the minus tau over two is quite big in the y variable which means that m is a constant so there's a here there is a big overlap between these two regions but that's all right it doesn't matter and then the inner region is when z is bounded so this is what is the most important and And so, what we, our construction of the solution, uses barriers defining each of these regions. And of course, as usually happens, what is hard is to match these barriers so that they work at the intersection of these regions. And the hard part is to match the barriers between the intermediate and inner region. The other, as you expect, Uh, the other, as you expected, the closer you go to the origin, the harder is the computation. But I'm not going to really show you this, how you do the barriers. I'm just going to give you an outline of the construction and goes into showing briefly on how we show that H is bounded. So, let me state the theorem of existence of a solution. So, under the Under the assumptions on our initial data, there exists a profile function, u, which is the solution, defines the solution we are after. And this profile function comes as a limit of approximating solutions, which I will tell you how we obtain. And these approximating solutions are strictly contained between these barriers that are. Between these barriers, that upper and lower barriers that we construct. And delta n here is a sequence that goes to zero as n approaches infinity, of course. So the barriers become more and more refined, and actually these barriers are nested barriers. All right. So how do we construct this solution U? Now, again, the problem is we cannot go right. We cannot go right away all the way up to the origin, up to t equals zero, because of the singularity of the problem when u is zero, which happens at time t equals zero at the origin. So what we do, again, the same thing. So what we do is we choose, we very carefully choose a sequence of times that goes to zero, right? And we Right and we construct a solution u sub n which is between define between s sub n and t sub n. Eventually this t sub n will be bounded below by some t naught. That's not a problem. The hardest part is how you choose the sequence of Sn. And how do you choose the sequence of Sn? You choose them in such a way. Choose them in such a way so that this at time Sn this profile U of X at Sn is between these two barriers. Now this is not so easy to find for some technical to determine this for some technical reasons because you see your this Your barriers, which depend on the asymptotics, deteriorate somehow as the approaches zero. Nevertheless, we are able to do that and therefore do the construction, all of course is achieved through the choice. Through the choice, the construction of this nested sequence of barriers where u delta n, delta n is 2 to the negative n delta naught, and therefore going to zero. And now, so why do we need to refine so much our barriers? So you cannot have a barrier that works all the way up to t equals zero. We need to keep refining and Zero. We need to keep refining our barriers because this is how we achieve the convergence through barriers to the Alan-Carr minimal surface at the origin. Note this that this is not exactly the case of what Velasquez had done in the other case, in the singularity case. All right. And let's see. All right. I have five, ten. I have five, ten more minutes, I don't think I need more to show you now how we use, how we prove that age remains bounded all the way up to t equals zero. So it's not hard to show that the h is bounded when x is bigger than equal in the outer region. Therefore, x bigger than equal than m. x bigger than or equal than m square root of t. Actually, this more or less, actually, you can even using the asymptotics that follow from the barriers or equivalently what Velasquez had done for the other case, the case of the singularity, going to a singularity, you can show that h is bounded even in a region that is a little smaller than x, bigger than m squared. than x bigger than m squared of t up to some power of t let's say t to some power where t that power is bigger than one half and the hard part is again to to show the that h is bounded close to closer to x equals zero and we use um the we are inspired our approach is um our approach is so so to to bound x in the for s x less than equal to m square root of t we use a blow-up argument by contradiction which is inspired by Stolarchy's beautiful work and we show that H is uniformly bound so this is what I'm going to show you right because if x bigger than equal than because if x bigger than or equal than m square root of t is just a simple pde argument to show a rescaling argument to show that h is bounded all right so the theorem or the result is that h is bounded when x is m square root of t all right so instead of showing that h is bounded we show that u sub t which is we call this little h of x comma t which is capital which is capital H and multiplied by the square root of the gradient function is bounded because this is more or less has the behavior of H. And this little H satisfies this nice PDE, right? You find this by just taking the PDE for u and differentiating one time in T. And so we would So we will not bound, we will not show that H is bounded, little H u sub t is bounded, where U is our solution. We are going to show that H sub n is uniformly bounded for our approximating solutions U sub n that I showed you earlier. And therefore, we define H sub n to be the time derivative of U. Be the time derivative of u sub n, where u sub n are the approximating solutions. And that doesn't change so much the proof, so you, but of course, that's right, it plays a role. So let me again remind you the scaling here. So the inner region scaling variable is z, which is e to the negative k over. e to the negative k over 3 minus 1 half tau y and y is e to the negative tau over 2x. So in terms of the original variable x, z is t to the negative k over 3x, right? So we would like eventually to get, so and what you know is that you know that you converge to a minimum surface, of course, every where h is zero in the zero. Is zero in the z variables, but you want to show now that h is bounded in the original scale. All right, now we first do some kind of refined derivative estimates using scaling. And if you do that, you end up that 8 sub n, you have a uniform bound of 8 sub n, which is this. Which is this. So it's t to the negative k over 3. So this is a bad number. And this is 1 plus t to the k over 3x, etc. So you see that if you are in the inner region where this is just a constant, let's say 2 or something or 1,000, then these estimates gives you that 8 sub n. gives you that H sub n is at most t to the negative k over 3, but that's far from being bounded. So this is the best we could get and anybody can get using the scaling and the information you know. And so the question is how you improve this bound. So define lambda L sub n, lambda sub n. L sub n, lambda sub n, to be the maximum. So you take of your H sub n of X t in the region where you're interested. I am interested in the region where X is less than equal than M square root of T because this is the region, the inner and the intermediate region. intermediate region. And since the solution U n is defined only for t bigger than Sn, you don't go all the way to zero, t is between Sn and some fixed T naught. Now, we need to weight this with some weight where 1 plus T to the negative k over 3x to the m, where m is a number between 2 and 3. And you will see why. And you will see why we need it in a second. We need it so that when, after blowing up and getting to an ancient solution, this ancient solution has a specific bound at infinity, which rules out some other blow-ups. So, and the basic claim, of course, is that the supremum over n of lambda n is less than infinity. So, if we can show that this is less than infinity, this This is less than infinity. This in particular will show that when this is just of the order of one in the inner region, for example, the mean curvature is bounded. Now, if you are in the parabolic region, that even gives you a better bound because this is a big now number, therefore it gives you even a better bound. So this is good. This is good. And so, if we have this, we are done. And now, to prove this, again, we argue by contradiction, and although the proof is different than what Stolarski did, we definitely use his ideas. So, it's a beautiful work by Stolarsky. So, So, all right, so this is what lambda n was. And let's assume by contradiction that the limit of the lambda n's goes to infinity, right? Then, as you expect, we expect we will be rescaling, but before we rescale, what does this mean that goes to infinity? This means that there are okay. Okay, of course, sorry, this is a big going to infinity, okay, we know, but choose points a n and capital T n, t n goes to zero, so that the age of a n tn is lambda n multiplied by this, right? Because this is achieved at some point, and the fact that lambda n goes to infinity tells you. Goes to infinity tells you that Tn goes to zero, of course, because if Tn doesn't go to zero, everything is controlled. And also, using some arithmetic, you can prove that A n is of a smaller order with respect to Tn to this power. This power is not so important. The only reason I wrote it there is that to tell you how you choose M. You choose M. You choose M, and this is where we also use that K is bigger than or equal to 4. If K is bigger than or equal to 4 and M is chosen sufficiently close to 2, then we can make this to be bigger than 1 half. Therefore, we know that A n is at a much smaller scale as square root of T n. Now we need to distinguish between the two cases. two cases. A n is Tn is of the is at most of the order T n to the K over C and between and not. And I'm only going to show you this. So this is more or less shows that you are in the inner region and that is more or less that you are in the intermediate region. And I'm just going to show you this case. The other case follows a similar spirit. Folllows a similar spirit. So, what do you do? We choose this scale, lambda n to be tn of k over 3, and we perform a rescaling of the uns and the h n's. All right, so and this is the standard scaling. And now the new variables where they defined xi is in the new. Xi is in the new variables is positive, it's determined where it's defined from the right from the, I mean, it's defined everywhere. You can define the scalings for all, right, for the function u s and h n are defined for, let's say, x positive. So psi can be positive, anything positive, and s is less than equal to. And s is less than or equal to zero and bigger than or equal to negative s n. And we have to work a little bit hard to show that this s n goes to minus infinity. All right. And using our construction of the new n's, we know that the risk go to the Allen calf. So this follows from like this follows our construction of the U.S. Folllows our construction of the UNs and this follows through arithmetic. And now the HMs satisfies this PD, which is not so important, but because you have now control from the way you rescale, you can pass to the limit over a sub-sequence and you have a smooth convergence to some h-bar. And the whole H bar. And the whole point here is to derive the contradiction from this H bar, as always. So this H bar is an ancient solution of this PDE. What is this PDE? It's the linearization of the flow at the Allen-Carr solution, which is the limiting solution. And H-bar satisfies this bound. This is because of This is because of the way we set up the definition of the lambda n's, what I told you earlier. And right. And also the definition of the a n, which were the maximum points of the eight sub n's, give you that gives that this h bar sub n, which is the scale flow, attains its maximum at Its maximum at ψ n, which is a n t n to the negative k over 3. And this is where we are using the assumption that a n is at most t n to the k over 3. Therefore, we know that passing to a subsequence, this c n converge to x e bar, to a point. And at this point, because we have chosen lambda. Lambda n to be a n to be the maximum point, then you know that at this point, which is now centered at zero because of our scaling, and we have that h bar of psi bar zero is equal to one plus psi bar to the negative m. And m is bigger than two. So, if m was were two, then you could have other things happening. So, the simple. Things happening. So it's important here that m is bigger than 2. And finally, to derive a contradiction, you compare h bar with the stationary solution, which is the Allen carb minus psi w prime of psi. And phi is a solution of this equation, of the linearized equation at the Allen car. And using an argument, Using an argument, you can actually show that for any L big, any L number big, if you look at the compact set from 0 to A, you have that H bar of psi 0 is less than equal a constant 1 plus L to the negative m and this phi of psi minus one half phi of L. So to prove this, you go very far away at Way at close to minus infinity, where you can compare somehow to your age back there with the stationary solution. So anyway, so having this, you can now let L go to zero, to infinity, because L can be chosen as large as you wish. And this shows that this can be. This can be done, can be, can be made at time zero as small as we wish, therefore contradicting this. And that's the end of the story. So this is the end of the argument. Now, what we haven't looked at all is if the solution is unique. The solution is not unique if you don't concentrate at a certain class, but maybe in the class that we are considering here, the solution is unique. But we didn't see the significance, the importance of this to justify proving, getting into work and proving it. I will finish here. Oh, sorry, I am a little late. I am a little late. So yes, thanks for that very nice talk. Are there any questions? I guess I wondered, I guess, are your solutions, are these singularities unstable? Backward, like they're unstable. Backward, like they're unstable going to the singularity. I just mean that in the sense of like, I guess there are finite time singularities from some initial data, and that so if you perturb that initial data, should will it still occur? Velasquez singularities. Ah, you mean like going forward or going backwards? Because forward. Going forward. So you mean like in the initiate, like in our case, because in the last case, the answer. The last case, they're unstable. In our case, you mean what perturbations? You mean how, like, how far? I mean, in what sense, but like the question is in what sense is compactly supported perturbation of the initial data. I just wonder if it's can you make a very small perturbation so that this behavior disappears? I wonder. Depending what you mean by, because we are making very precise asymptotic, like if, you know, we are making, like, as long as the asymptotics are, you mean, like, if the asymptote, if the asymptotics hold, you mean to get something else, to get another solution? Or to get. As I understood it, you cook up. I mean, as I understood it, you cook up this O4 cross O4 symmetric data, is that right? Yes. And it encounters a finite time singularity. Is that right? No, no. So we, no. What we did is we started from the singularity and we continued the flow. What Velasquez did and Stolarsky, you construct, they construct, they have an initial data, and that is not Data and that is not stable away. So that is not stable. So the singularities are not stable. So what we did is we saw how to continue after the singularity. And still keeping the mean curvature bounded, like everywhere. Yeah, still keeping the bound. Sorry, it was a little confusing. No, I was confused, yeah. Yeah, no, but yeah. Maybe this is the same as Jeff's question, but. This is the same as Jeff's question, but if you got put up away from that stability from that symmetry, does you think the same thing would happen? Just move the initial data a little way from that. And O4 plus O4 symmetry in the initial data. Any idea about? Yeah, of course, if you get out of the symmetry, then you don't expect. Of the symmetry, then you don't expect to have stability. Okay. Maybe it's worth mentioning, I think, right, isn't it? In general conjecture, if you look at the plateau problem for the minimal surfaces, it's conjectured that these singularities are always unstable. You can perturb away. So that, you know, you can decide how that affects your. Decide how that affects your expectation. Mean curvature flow question. Okay, any others? Well, okay, if not, let's thank Tori again. Thank you. I guess, yeah. I guess, yeah, we'll reconvene in person for a live talk at 10:30 or 2030 local time. Thank you. Bye. Good to see you, Toji. Very good to see everybody. How are you? Good to see you. And Cody, are you in New York now? I am in New York. Yeah, I was in New York yesterday. Yeah, I was in New York yesterday for the motor card. Ah, too bad you didn't.