Having me here. So I will explain what this title means in due time, but let me start by just kind of setting the stage of what's the style of questions I want to focus on. And that's going to be high-dimensional statistical inference. And this is maybe best illustrated by just kind of giving you some examples. So imagine, for instance, we want to find some kind of hidden structure in a random graph, or maybe find some low-dimensional structure in random data. Structure in random data and so on. And so I've listed here some specific probabilistic models that capture these types of things. Maybe one of the simplest ones would be finding a planted clique in the random graph d head one-half, but many other things too. And these style of problems have some common features. First is that they have a large input, so we're imagining getting maybe a Input. So we're imagining getting maybe a big graph or a big matrix as our input. And there's also a large number of unknowns. So the thing we are trying to recover is also potentially large. And importantly, all of these problems have some kind of planted signal. So there's sort of some ground truth structure or object that we want to recover. That's in contrast to some of these random optimization problems discussed yesterday. Problems discussed yesterday, like the spin glass problem called Mark, where the input is just like totally random noise. And so, you know, those are the type of problems I kind of want to understand at sort of a high level. But let me also give you one very specific example that you can focus on that I'll mention, you know, that I'll focus on kind of later in the talk. And that's called the spiked. In the talk, and that's called the spiked Wigner model. So here's kind of the equation. And so, what's going on here? The observed data is going to be some n by n matrix called Y. And that is formed by adding these two different parts. The first part we think of as kind of the signal part, and the second one is the noise part. So the signal part has just a scalar number, the signal-to-noise ratio, which dictates. Signal-to-noise ratio, which dictates how large the signal is. And it's this rank one matrix for some kind of hidden vector theta. And then the noise is just IID Gaussian matrix of noise. Okay, and so just to recap this, theta is some unknown vector, and let's further assume that its entries are drawn IID from some fixed prior. And that prior can be known to you. Prior can be known to you. And by fixed, I mean that the prior won't depend on n. So I'm ultimately imagining taking, this is an n by n matrix, and I'm imagining kind of the limit when n gets large. Things are kind of normalized in this way, but the prior will be something fixed, dot depending on n. And so the goal is that I give you this matrix. You want to estimate just the kind of signal part of the beta vector. Okay, so it's sort of this rank one. Okay, so it's sort of this rank one plus noise kind of denoising problem. Questions about that? And yeah, please interrupt me with questions in time. Okay, so what's the point of this? It's not necessarily that I think this is a faithful model of anything in real life or something, but this serves as like a very kind of simple testbed. It's one of the most basic signal plus noise settings. So it's kind of useful if we want to. If we want to sort of study in a rigorous way questions like what are the best algorithms for solving this type of problem? And how well do we expect to do? Which in this case would be how large does S need to be before we can succeed? Okay, and so what are the best algorithms for these types of problems? And for a second, let me zoom out again to think not just Out again to think not just about this like Wigner model but about these kind of high-dimensional problems in broad generality. So, you know, kind of like the examples that I had on the first slide. And there's a few different communities that have come together and provided some ideas about what maybe the best algorithms should look like. And in theme with this workshop, I want to call these, you know, this community kind of statistical mechanics, and this one will be theoretical. This one will be Theoretical Computer Science. Possibly oversimplifying because, really, there's some overlap between these and so on, but hopefully this caricature is sort of okay. And so on this kind of physics side, some of the algorithms that have been proposed by this community include things like belief propagation and the related approximate message passing. So I'm not going to tell you exactly what this means. So, I'm not going to tell you exactly what this means yet, but roughly, these are some kind of iterative algorithms that keep track of some beliefs about what they think the hidden variables should be set to and kind of update these beliefs in kind of a Bayesian optimal sort of way. Okay, so this community has come up with these algorithms which have been quite successful in solving a wide range of problems with this flavor. And they've also proposed some ideas about algorithmic barriers. That sometimes there are certain barriers that a problem might have that prevent you from having a fast algorithm. Okay, and so this includes things like this OGP or various free energy barriers. So I think Mark mentioned OGP yesterday. These are some kind of structural properties of the solution space. And when these properties arise, And when these properties arise, this somehow suggests or maybe implies that certain types of algorithms are not going to be able to work. Okay? And so when we see these types of properties, we perhaps think to ourselves that maybe this problem is actually fundamentally hard, computationally speaking. Okay, and now on the other side, this other thing that I've classified. Other thing that I've classified as theoretical computer science, I have in mind algorithms of sort of a different flavor that are more maybe algebraic. So this includes the sum of squares hierarchy. It's okay if you don't know what this means. This is a kind of complicated optimization problems using semi-definite programming. Maybe a bit simpler to think about are spectral methods. This means you build some kind of matrix from your data, and then you look at its eigenvectors and eigenvalues. Its eigenvectors and eigenvalues. And also algorithms that can be described as low-degree polynomials in whatever the input variables operate. Okay, so these types of algorithms also have been quite widely successful at solving many different problems. And this community also has proposed ways to identify algorithmic barriers. And normally this. Barriers, and normally this takes the form of proving that one of these types of things fails. So if I can prove that sum of squares cannot solve my problem, then maybe I take this as evidence that my problem is fundamentally targeted. Okay? And so with these kind of two different communities bringing in these different ideas, the natural question emerges of how are these things related? These things relate. If a new problem comes along, one thing I could try these methods from this side, I could try the methods from the other side. Should they give the same answer? Should these algorithms be equally good in performance? Or not? Is one better than the other? And so we've started to understand these types of things of how maybe some of these different things are related to each other. For instance, starting with just connections within. Starting with just connections within one side or the other, this belief propagation, the derivation of belief propagation kind of is what led to approximate message passing in the first place. So this is sort of an approximation of this. We also know that the overlap gap property provably prevents AMP from working in some settings. And over here, we also know that if sum of squares can solve a problem, then under some conditions, you can. Under some conditions, you can automatically turn that into a spectral method. And if you have a spectral method, you can also use power iteration to turn that into a low-degree polynomial. Although there are enough caveats in these two arrows that you can't really chain them together, unfortunately. And so, more recently, we've also started thinking about connections between the two sides. You know, maybe some have been around for a while, for example, the ideas underlying. For example, the ideas underlying belief propagation lead to certain ways you can construct spectral methods. We also know kind of recently that sum of squares can simulate approximate message passing. So if A and P can solve something, then so can sum of squares. We know that the overlap gap property rules out low degree polynomial algorithms in certain settings. In certain settings. And we also have kind of this connection that the successor failure of these low-degree polynomials is equivalent to something that kind of resembles a free energy barrier, something we call the Anil-Franz-Perici potential. Although, unfortunately, this object here is not exactly what physicists would typically use to predict the hardness of these problems. So, okay. So, okay, one more arrow that I think is worth mentioning, even though it's not directly maybe relating these two, is the connection to the statistical query model. And the result I want to talk about today can be thought of as being one more of these arrows, kind of establishing some kind of connection between the AMP, which is sort of this physics-style algorithm, and how it connects to this framework of low-degree. This framework of low-degree polynomials arising on the more DCS side. Okay, so this was sort of a more high-level philosophical picture maybe of what is going on in this area. And this suggests a little bit that when you see this, it may be tempting to think that really there's going to be some kind of unified theory, that all these algorithms are going to be. That all these algorithms are going to be equivalent, and hopefully, we can prove that. Because then, when a new problem comes along, we only have to try one thing rather than trying all these different methods, all these different lower bounds. And so, yeah, we have been, I think, successful in making a lot of these connections. But on the other hand, there are kind of quite a few caveats and counterexamples that I've sort of swept under the rug. So, maybe I should come clean about a few of these. For instance, These. For instance, there's a lot of different styles of these inference problems. For instance, people think about the detection task, or in other words, hypothesis testing. This is like, can you tell whether the planted structure is present or whether it's just a totally random input? Recovery is maybe the more natural question of actually finding the planted structure or planted signal. Structure, planted signal, but we also have kind of random optimization problems with no planted structure at all, and so on and so forth. And so on the previous slide, a lot of these results or algorithms that I mentioned really only pertain to kind of one of these things or another. And somehow, maybe an even more surprising discovery is just that for some problems, the two The two sides sort of seemed to disagree. So, this problem called tensor PCA came along. And I think for a while, like these two communities each liked to kind of make these bold conjectures about how, you know, if I'm on the physics side, I think that belief propagation and AMP are sort of the optimal algorithms for all problems. And so, if they fail to solve some problem, then no other algorithm should be able to do it. And similarly, on the TCI. And similarly, on the TCS side, people like to conjecture that if sum of squares can't solve this problem, then no fast algorithm can do it. But then this problem came along, Tensor PCA, and kind of threw a wrench in some of these things, because this is a situation where the physics style algorithms really were underperforming compared to the other ones. So, what happens in this problem is, so TensorPCA, I think Mark actually defined. So, TensorPCA, I think Mark actually defined it yesterday. It's not really important how it's defined for purposes of discussion, but roughly a tensor, think of it as an n by n by n cube of numbers. Tensor PCA means you have a rank one tensor plus Gaussian noise. Find the rank one part. Um so it's uh somehow a higher order generalization of this spiked Wigner that I had on the slide before. Had on the slide before. It's not done. Okay, so what happens here is that as you make the signal-to-noise ratio larger, so as the problem gets sort of easier, once you cross this threshold, it becomes information theoretically possible. You can solve it by some kind of brute force search. Once the signal noise passes a different threshold, now we know fast algorithms to do it, or polynomial time algorithms, based on, say, spectral or some of squares. But then But then, for this AMP and these physics-style algorithms, they actually need a much larger SNR. And so this was sort of surprising and sort of feels problematic, especially if you're sort of on the physics team and you want AMP to be solving all these problems optimally. And so this wasn't totally the end of this. And so, this wasn't totally the end of the story. Because of this, there's been some follow-up work trying to kind of redeem the physics approach. Can you modify AMP to make it actually work down at this threshold, or can you modify these other algorithms? So aside from AMP, there's these other local algorithms that get stuck here. Things like gradient descent, for instance, lots of sort of the kind of most basic kind of naive optimization methods. And so, you know, can you modify those types of things to get them to this? Can you modify those types of things to get them to this threshold? And the answer is kind of yes, that by doing variations of these things, you can actually fix them. For instance, belief propagation is in some sense, or AMP is some sense is trying to optimize something called a beta-free energy. But really, there's this hierarchy of more complicated Tikuchi-free energies that, so you can sort of take belief propagation and lift it to this hierarchy. Propagation and lift it to this higher-order thing, and then we'll work down to this threshold. I guess it depends how you define local. Yeah, so I think for this one, the average gradient descent is like instead of running just one instance of gradient descent, you kind of more like start gradient descent simultaneously from a lot of different places and use like the average of all. And use the average of all these gradients to inform where you go. So it's something more like this. So, yeah, whether you consider that local is maybe up to you, yeah. Okay, good. So, anyway, all of this is sort of just to say that the situation is not as simple as we originally hoped, but there are quite a few caveats and intricacies of how the different methods and so on are related. Methods and so on are related. But so, yeah, I wanted to give sort of a flavor of this, but for the rest of the talk, I'll focus on a more concrete and mathematical example. But yeah, any questions about this sort of philosophical part? Okay. Good. So let me try to actually define some things now. So I've been throwing around this A. So, I've been throwing around this AMP, approximate message passing. So, what is this algorithm? So, just to remind ourselves, this is the, imagine we're in this spike-Wigner model, where I give you this y that's rank one plus noise. And am v is going to be some kind of iterative method. It keeps track of a vector x, and x is indexed by this superscript t, which is just which iteration, you know, times. Which iteration time step you're at. And you think of x as being kind of an estimate for the unknown vector theta. And so you just initialize it to be zero, and then you repeat this iteration over and over until you are satisfied. And there's a few different parts of this. The first thing you do at this iteration is you take your current guess. Current guess and apply a certain entry-wise transformation to it. That's what the f is. And then after that, we multiply it by the matrix y. Okay, so you can, so if there were no f here, if we were just looking at this term, this would just be like power iteration, right? Just take x and multiply it by y a lot. That would be like finding the leading eigenvector of this thing. Whoops. Which, you know, maybe is a really Which maybe is sort of a reasonable algorithm if we want to recover this rank one piece. But this AMP is a little bit more sophisticated. You're not just doing straight power duration, you're doing also interleaved with these entry-wise transformations. And the entry-wise transformation is taking into account our knowledge of the prior for theta, for what the entries of theta actually look like. Okay, and then on top of that, there's this. Okay, and then on top of that, there's this even stranger kind of correction term that people call the onsager term. I'm not going to try to get into this so much, but this turns out to be crucial for actually analyzing the algorithm. That because of this thing, the iterates evolve in a very nice way where everything is kind of Gaussian and independent in each step, and it lets you prove theorems about exactly how well this thing will work. What is this N? These are vectors? Which one? The N? The n? Yeah. Oh, yeah, n is just the size of the matrix. So that's just a number. So it's an n, but this one here is an n by n matrix. So it's the dimension of theta. Yeah, thanks. Good. Other questions? Okay, so that's the algorithm. So maybe the next question is: you know, how well does it do? And so this is sort of a picture of that, where on the x-axis, we have s, which was like this. We have s, which was like this signal-to-noise ratio, so we expect to do better when s is larger. And then the y-axis is some basically measure of correlation between my guess, that is the final output at my last iteration, that's what I'm calling theta hat. How much does theta hat correlate with the actual true ground truth? And higher is better here if you want a bigger correlation. A bigger correlation. And you're running it until it converges. Yeah, that's right. You have to sort of choose some criterion, but yeah, basically, you run this until it converges. Yeah. Yeah, good with that? So that's what these axes are. And we now know, kind of, you know, using these sort of tools from the physics-inspired tools, I guess. Physics-inspired tools, I guess, that we have a very precise picture of exactly what this asymptotic curve looks like. So I'm taking the limit as n goes to infinity, the size of the matrix. This picture is for one specific prior on theta. It's not terribly important what it is, but it's something supported on three different values. I think it's like plus or minus one, and then also some larger value. Some larger value. And also, here we're taking this, there's some optimal choice of these F's, some kind of optimal Bayesian denoiser that you can plug in here that itself depends on the prior for theta. And so I'm taking sort of that optimal AMP. And then these curves I'm drawing here are given by what's called the replica symmetric formula. So, okay, we talked. Okay, we talked about replica symmetry breaking yesterday. I think sort of a weird fact is that Bayesian inference problems tend to not have replica symmetry breaking, that actually the replica symmetric formulas are accurate. But okay, I won't get into this. Don't worry, that doesn't mean anything to you. So, what are these pictures of? The blue curve is the Bayesian optimal performance. Performance. So, among all possible algorithms or all possible estimators, what is the best possible correlation that you can have, sort of information theoretically speaking? And then the black curve is what does this AMP algorithm give you? Okay? And in these two sides, the black is directly on top of the blue curve. Okay, so for here, AMP is exactly hitting the influence. Hitting the information theoretically optimal performance, and same over here. But on the other hand, there's kind of these two phase transitions where something jumps suddenly. And in between there, we have this gap where the phase optimal accuracy is better than what AMP is doing. Okay, so AMP is sort of sub-optimal in a sense. And so this kind of raises the question of whether we can. Raises the question of whether we can do better. So the Bayes optimal algorithm is not efficient. It would involve, if you want to really compute the posterior mean, you have to do some sum over exponentially many things. And so it's not clear if there's a fast algorithm that can hit this performance. Okay, so that's kind of the big question I want to pose: is, you know, is AMP the best? You know, is AMP the best fast algorithm, or is there a better fast algorithm? Yeah. How much does this diagram depend on the prior for theta? Or is this picture pretty universal? Yeah, like definitely if I change the prior, the shape of these things will change. For some priors, you don't have this gap at all, for example. But if I take the prior to be like uniform on the sphere or something like that, you'd like, I guess I'm asking. Something like that. I guess I'm asking: is this nature of a weird prior, or is this a generic behavior? Okay, good, yeah. So if I take a prior that's just uniform on the whole sphere, then you will not see this gap. You also won't see sort of any sudden phase transitions. Or sorry, maybe you will see a phase transition. It kind of goes from zero upwards. But in that case, AMP will just reduce to finding the leading eigenvector, like just a standard power iteration thing. Like, just sort of a standard power iteration thing. And that will just be information theoretically optimal. So, yeah, the situation will be simpler for that. Things get weirder when you have priors that are like sparse, for example, or have, you know, if you have priors where it's 99% zeros and something else, that's when you maybe start to get these types of gaps. I guess, just like, for instance, if you're uniform on like a hyperkey or something, you On, like, a hypercube or something, do you expect this sort of a behavior, or do you really need something that's like equaint? Yeah, if you're uniform on the hypercube, you also will not have a gap. So AMP will be information theoretically optimal everywhere. You won't have these transitions. And yeah, I don't know. I don't think it's easy to describe which priors have the thing or not, but yeah, I guess I've given some examples on each. So yeah, thanks. Yeah, other questions? Okay, good. I think that's what I wanted to say here. Okay, so what are we actually going to try to prove? So there is this conjecture that, you know, I'm not sure, it's not exactly a formal conjecture that I can attribute to any one paper or one group. Any one paper or one group, but it's definitely been sort of around the sort of statistical mechanics community in some sense that people like to think that AMP is sort of the best among all the fast algorithms. Okay? And so, yeah, at least if you're sort of on the physics team, this is what you might expect that. You might expect that this really is the picture to not close this gap any further. But there's, you know, so let's say we want to try to verify, you know, either prove or disprove this conjecture. What can we really do? You know, the problem is that this is a question of average case complexity, which is sort of a notoriously open area where we can't really hope to prove anything. So we're definitely not going to be able to prove something of the form like. Something of the form like, you know, no polynomial time algorithm can do better than this, even if we allow ourselves to assume p not equal md or whatever, one of these conjectures. So we have to aim for a more modest goal. Instead, what we're going to do is to prove that AMP has the best possible mean squared error among some larger class of possible approaches that you might try. And that's going to be what I call constant degree. And that's going to be what I call constant-free polynomials. So let me try to say what I mean by this. So if you first of all go back to remember what AMP is, you know, some iterative procedure. And we're going to imagine running it for like a constant number of iterations. Okay, so constant meaning it doesn't depend on the matrix size. It turns out the constant will be enough for it to converge and to basically give this picture that we've seen before. That we've seen before. Okay? And let's also imagine, and you know, it involves these functions f being applied at certain steps, right? But let's imagine that those functions are going to be approximated by constant degree polynomials as well. And so when I do that, the entire AMP algorithm, I can think of it as one big constant degree polynomial. So it's a multivariate polynomial in Polynomial in n squared different variables. The input is my matrix Y, so I have these n squared different variables. And I'm saying this whole procedure of doing AMP, if I unravel the whole thing, it is some multivariate polynomial of constant degree whose variables are those things. Or maybe rather, I should say that AMP outputs some vector, and so each entry of that vector is like. Entry of that vector is like is a polygon. Is that part okay? And so that's sort of the bigger class of algorithms that I'm going to think about. What we're going to prove is to say that AMP does have the best possible mean squared error among anything that's a constant degree polynomial like this. No matter what polynomials you choose, AMP actually is the best. So how do you get this if you have a non-linearity f that might not be a non-linearity? Yeah, that's right. One of the things we show in this paper is that the Bayes optimal F is kind of smooth enough that you can approximate it by a constant degree polynomial. And so there is actually a con, like you can sort of take this AMP, you know, the original AMP maybe is not literally a polynomial, but you can turn it into. Polynomial, but you can turn it into something that literally is a constant-degree polynomial and does have the same performance as these curves I've drawn, you know, maybe minus epsilon or something. Yeah, good. Other questions? Okay, so that's sort of the main result. Let me make a few comments. This is a slightly more technical one. This is a slightly more technical one, but this result is most interesting for a biased prior, where the expected value of each entry of this theta is not zero. When the prior is mean zero, something a little bit different happens, where AMP actually needs not constant, but like logarithmic iterations in order to somehow get off the ground. So, this is sort of a separate open problem is to figure out what happens or to do the same type of result for that situation where you have maybe a larger number of iterations. And even in the biased prior situation, an open problem that I would really like to resolve here is to also rule out higher degree polynomials. I would think that in order to beat A and P, In order to beat A and B, you need to take a polynomial that's not just like super constant degree, but actually has a degree basically almost n. Okay. And so, yeah, also just to sort of remind you that this result is for one specific problem, it's for this Wigner model, but for any choice of the prior. So, there's also kind of some questions. So, there's also kind of some question of how general is this? Can you generalize it to other problems? But we have to be also a little bit careful there because we know that AMD is going to be sub-optimal for some problems. So we don't expect sort of a fully general theorem. Maybe more of the kind of relevant question is, can you classify which problems, for which problems is AMB a good idea versus for which problems is AMD not the best thing to do? And so the proof I'll show you will give some suggestion of. Give some suggestion of what is it about this problem, the Spike-Wigner model, that makes AMP the right thing to do there. Okay, good. Yeah, I guess one more comment is putting this sort of in the context of some previous work is about sort of these low-degree estimation lower bounds more generally. So I've said I'm going to prove this type of result to like no low degree. To prove this type of result, that no low-degree polynomial can surpass a certain mean-squared error value, right? And so, this is not like a new framework. This is something that we've now thought about for a bunch of these different high-dimensional problems. So you can imagine a more general situation where, given some kind of input y, there's some value you want to estimate, and we're sort of asking about this low-degree mean squared error, meaning. degree mean squared error, meaning what's sort of the best polynomial. I get to take any degree d polynomial p, and I want it to have the best possible mean squared error at estimating that unknown thing. And so I want to sort of characterize this value. You know, for what SNR parameters will this mean spread error be good? For what SNR parameters will this be bad? And so there's, you know, by now, sort of, And so there is, you know, by now, sort of, this kind of literature on this. But I guess what sets this result apart is that today I'm looking really at the exact limiting value of the mean squared error. Whereas a lot of this prior work was asking kind of a coarser question about just when does the mean squared error tend to zero versus when does it tend to one. So this is somewhat more sharper. So here, technically, I'm Sharper. So here, technically, I'm thinking about this kind of double limit where first you take n, the problem size to infinity, and then you take your polynomial degree to infinity. So my polynomial degree is allowed to be like a large constant. Okay? Good. So I think that's, you know, that's most of the sort of setup now. Setup now, and now I can kind of move into trying to talk about the proof unless there are other questions. Okay, so how are we going to prove such a thing? We want to relate this, you know, this AMP algorithm to all possible low-degree polynomials. So, there's going to be two steps. The first step is to say that AMP is as powerful as any tree-shaped polynomial, where I have to tell you what that means. And then after that, And then after that, we're going to show that tree-shaped polynomials are as powerful as all polynomials of the same degree. Okay, so tree polynomial is maybe best described by just having you look at this picture for a while. So remember, the input y is like a matrix. So the input variables are indexed by two things. So like y, one, three, that's one particular input variable. One particular input variable. And so these are the types of monomials that might appear in my algorithm. Let's imagine that my algorithm is just trying to estimate a scalar quantity. So it's just trying to estimate the first entry of data. Okay, and so that's going to be some polynomial that's allowed to have monomials that might look like this, or some big linear combination of all those types of things. And for any monomial, I can draw this associated picture where, so it's some graph, and the vertices are labeled with the numbers 1 through n. This is an n by n matrix. And I'm just drawing an edge for each one of these terms. So because I see y1, 3 here, that means I'm drawing an edge between 1 and 3. y14 means I'm drawing Y14 means I'm drawing this edge, okay, and so on. And then similarly over here, like in this case, you know, I have some loop, and I also have this double edge here because this variable is squared. Y is symmetric? Yeah, y is symmetric. So I guess I didn't tell you exactly what this z was, but let's imagine z is a symmetric ones so that I don't have to worry about the order of things. Yeah. Worry about the order of things. Yeah. Is this okay? So, this type of monomial I'm calling a tree-shaped monomial, if the picture looks like this. This type I'm calling anything else, if it has a cycle or it has a double edge, or if it's anything except for a simple tree, we're calling it not a tree. And also, furthermore, I guess I didn't write this, but the trees are supposed to be kind of rooted at vertex one. Vertex one is special. Vertex one is special because we're imagining that I'm trying to recover entry one of theta at the moment. But for that reason, this one is kind of distinguished. Not necessarily all of them, just that they're all kind of connected to one. So this Y46 doesn't have one, but if I look at the graph, it is connected by a path tool. The algorithm doesn't have to have one and entry spot. The algorithm doesn't have to have. It could have a monomial that didn't involve one. Oh, yeah, so the algorithm is allowed to have all kinds of other things. It's just that when I talk about trees, when I talk about, you know, I'm defining sort of this good class of monomials and then this sort of other bad class. Monomials and then the sort of other bad class of monomials. So, part of what it means to be a tree, I'm going to also require that you are connected to one. If you're not connected to one, even if you're a tree, you're actually considered a non-tree. Yeah, thanks. Okay, so the first thing I need to try to convince you of is what's the relation between A and V versus these tree-shaped polynomials. And so the claim is basically this. So, okay, here's all these double limits and stuff, but this side is saying that the mean squared error that AMD gets is kind of the same as the best mean squared error of any tree-shaped polynomial. And this is again sort of this double limit. So here you take n to infinity and then t to infinity, t being the number of iterations of a and p that you run. So the number of iterations is a large constant. Number of iterations is a large constant, whereas here you take n to infinity and then polynomial degree goes to infinity. And by the way, the degree of the polynomial is the same as the number of edges in this graph picture, right? A degree d monomial corresponds to a graph that has d edges, right? Okay, so one side of this is, you know, sort of straight. Sort of straightforward, at least conceptually, that the AMP algorithm, if you expand it out and see what it's actually doing, it actually does turn out to be this tree polynomial, or rather, it can be kind of closely approximated by one. You can think of it as sort of these tree terms plus something that's gonna be kind of small and two normal. And so, you know, I'm not gonna attempt to get into the details of that. That's uh yeah, hopefully something new. That's hopefully something you trust me on. The other direction is maybe the more interesting one is we want to say that AMP is actually the best of all the tree polynomials. So how roughly would we prove such a thing? Well, let's consider the best tree-shaped polynomial, whatever it is. And without loss of generality, it's symmetric, meaning it's sort of, it obeys the inherent. It sort of obeys the inherent symmetries of the original problem. So the original problem has all these symmetries that there's an n by n matrix. So maybe row one and column one are kind of special because I'm trying to estimate that first entry. But the other indices, two through n, are all kind of interchangeable, right? At least sort of, you know, in distribution. That if I permuted two things, the whole model would have the same. Two things, the whole model would have the same distribution. Okay, and so for this reason, the best tree polynomial, you can argue, actually has to sort of obey these types of symmetries. So if it involves a particular tree that has labels 2, 3, and 5, it should also include the monomial that has labels 7, 10, and 12, or whatever. You know, for any three. Yeah, so if you have a tree of some shape, you should have sort of. Some shape, you should have sort of all the trees of that same shape with all the different possible labelings that you could have. And so, and given a polynomial, a tree polynomial that has this type of symmetry, it turns out you can construct what's called a message passing scheme to compute it. And message passing is just sort of a generalization of the type of AM. Of the type of AMP iteration that I showed earlier. So I won't try to get into the details of this too much, but maybe I can say a few sentences that the AMP scheme that I showed you, I guess it keeps track of just one vector. It just has one scalar value for each of the n indices. Indices. If I wanted to make a message passing scheme that can build any, that can compute any given tree-shaped polynomial, I actually, you know, instead of just keeping track of one scalar entry per index, I have to keep track of a whole bunch of them. And those are going to be indexed by all the shapes of the possible trees, all the kind of isomorphism classes of trees. And so once I've set up an algorithm. Once I've set up an algorithm that has that sort of structure, when you run this type of iteration, you can basically construct any tree polynomial you like in sort of a recursive way where you kind of build each tree up from by sort of removing one edge at a time. But okay, I will think. But okay, I will maybe not get too much more into that. And then, because of some prior work, we actually know that this particular AMP, so the AMP that has this basis optimal choice of the functions f, that this has the best possible mean squared error among all possible message passing schemes. Okay, and so putting all these two things together gives me the other direction. Direction. Yeah. Is anything about the on-site returns that also treat volume? Okay, that's a good question. AMP without a non-Slagger term, you can just rewrite it so it's basically equivalent to including the non-Slaggard term. In terms of the representational power, the unSloggered term is a. The on-socker directors aren't really built. Because it does have tracked out previously, right, anyway. I see, thanks. Yeah, that sounds plausible to me. But yeah, I don't know much for sure. Yeah, thanks. Yeah, okay. Let me think about that more offline. So, okay, let me try to convince you of the. So, okay, let me try to convince you of the other direction then. How do we relate tree polynomials to all polynomials? So, you know, this is sort of the last thing I want to try to prove is that for this particular model, the spiked Wigner, that whatever mean squared error you get with trees is actually the best mean squared error you can get with all unrestricted polynomials. And then by chaining everything together, that would give me what I wanted. Everything together that would give me what I wanted, right? And so, how do we prove this type of thing? So, I'll try to sort of say this briefly, that I guess for a start, if you want to know this kind of degree D mean squared error, if you want to know the best mean squared error you can get with a certain polynomials up to some degree, in principle, this is sort of a linear algebra problem, and you can solve it explicitly. Explicitly. And the explicit solution, you know, this is just some fixed thing, the expected norm of theta or whatever. This is kind of the important part, that it's some kind of quadratic form of C transpose M inverse C, where C is some vector and M is, you know, this is some big vector in some big matrix. And how are all these things defined? Well, first I need to choose some basis for my. Choose some basis for my polynomials. And because of the symmetry thing, my basis is basically going to be indexed by the shape of trees. So, you know, this index A here, A will be basically like an unlabeled tree. So kind of like the isomorphism class of a tree. Oh, yeah, good. Actually, all graphs. Let's see. And so once I've defined that, then the C and M. Once I've defined that, then the C and M are defined kind of like this. C has correlations between the basis functions and the true thing I want to estimate, whereas the M has kind of pairwise correlations between the two basis elements. And so, yeah, so if I wanted the mean squared error of all degree D polynomials, I need to take all degree D, all the graphs here. If I wanted to get just the tree polynomials, it would be the same formula, but where I only use. The same formula, but where I only use the treats. Okay, so that's sort of great that we have this explicit solution, but that's not exactly helpful on its own because this m inverse, we can compute m explicitly by some kind of tedious moment calculations and stuff. But computing m inverse seems sort of out of the question as far as I can tell. It's just, you know, m is this very big, complicated combinatorial matrix index by. Combinatorial matrix indexed by all these trees and stuff. And it doesn't seem like this is tractable to directly address. But what are we going to do instead? Instead, we can look more into the structure of this C and M matrix. And the key thing that happens here for this, this is kind of specific to the spiked Wigner model, that when you do these calculations, you see kind of a very nice structure where Where if I, you know, m is going to be indexed by the trees and also the non-trees, right, this matrix. And so these diagonal blocks end up being constant size, whereas the off-diagonal blocks are kind of vanishing. So this is sort of almost block-diagonal. That the trees, this is some matrix of correlations. So like trees interact with trees, non-trees interact with non-trees, but there's like a minimal interaction between the trees and the non-trees for whatever reason. Whatever reason. And then similarly, the C. The C is kind of measuring, you know, which of these monomials is actually good at estimating the true thing, which one has actually correlation with the truth. But it turns out the trees do have correlation, whereas the non-trees kind of have this vanishing correlation. And so, because of this, I can then kind of go plug into this formula and try to think about what happens. That C transpose M inverse C. That's C transpose M inverse C. A lot of, because of these small terms, a lot of stuff will kind of zero out. And so the important part is just going to be D transpose D inverse D, right, where D is just the top part of C and just the top plot here. But because of the last slide, this D transpose P inverse D is exactly the formula for the mean squared error of just the trees. Of just the trees. Because P is just the matrix of correlation for trees, and Z is just the correlation vector for the trees. So that's kind of how the proof goes. So we still have no idea how to find p inverse, but luckily we didn't need to. We kind of just showed sort of in the abstract that the non-trees disappear on the page. Can you give you some intuition for why E is tiny and D is not? These are yeah, it's a good question, but it's also kind of a tough one to answer because it's kind of specific to the spiked Wigner model and it wouldn't necessarily hold for other things. Yeah, I guess if you wanted to estimate this one variable, for example, if you have like two disconnected components, this extra part is sort of gonna have no correlation. Sort of going to have no correlation with the true thing. Because there's no path, this extra thing basically has no correlation with the truth. And so it kind of just, you know, it has expectation zero, so it's kind of like I'm just multiplying the whole thing, you know, zero out the whole thing. And like similarly, if you have a double edge and things like this or a cycle, the way that trade-offs play out is that, yeah, it somehow doesn't. It somehow doesn't, yeah, because each edge has this one over root n scaling factor. So for each extra edge you add, it potentially makes everything smaller. And so you better actually, it better actually be, you know, it needs to sort of be helpful enough to overcome that. So somehow only the trees... So it's an issue sort of with connectivity and more edges is bad. Yeah. Yeah, I think that's fair to say that, yeah, more it's. But yeah, more, especially sort of unnecessary edges is bad. Yeah. Thanks. Good. So I think I'm definitely past my time. And so let me just wrap up, like what was the point here? So I guess we showed some equivalence between constant iteration AMP versus constant degree polynomials in this particular setting of this Wigner model. And it was by kind of this chain of equivalences. And so, the really key thing that happened for this Wigner model is that when we computed these moments, you get this kind of magical block diagonal structure. And so, this is potentially some structural property that you can use to decide whether your problem is sort of amenable to AMP or whether possibly AMP is not the optimal algorithm there. And so, at the end of the day, what was the point of all this? I guess, on one hand, it gives us. Of all this, I guess on one hand, it gives us some evidence for this AMP conjecture. But now we're not just believing AMP is the best just because now we can at least say that it's optimal within some larger class of things. This draws sort of some connection between the physics and computer science science. All right, so thanks for your attention. Thank you. Is there any questions? Thank you. Do you have any questions for Alice? I might have missed it, but how is tree important to encoding it into the message passing? Yeah, good. So I guess when you do this type of message passing thing, you're doing like a round of power iteration, and then you're doing this, you know, that f of something that depends on your previous stuff. And so what this basically Stuff. And so, what this basically translates into is that each round of power iteration kind of allows you to take whatever graph you had before and like grow one extra edge on it. But that extra edge always goes somewhere new. It doesn't connect, or at least for the vast majority of the terms, it goes somewhere new rather than connecting back. So, yeah, this AMP really does expand out. Yeah, any sort of message passing scheme kind of can only give you these tree polynomials. Kind of can only give you these tree polynomials and not other stuff, at least sort of to first order. So, if you consider the setting of like text or PCA, then we know A and P and load of tree polynomials are not the same. And is it clear that tree polynomials are one of these, or is it not? I'm sorry, the tree polynomials are what? Are like equivalent to one of these two things. What if these two things? Okay, yeah, this is a good question. I think for tensor PCA, you have to decide what exactly a tree polynomial means, because now we have more of like a hypergraph instead of a graph, if it's a tensor. But yeah, I think my guess is that morally speaking, tree polynomials should still be equivalent to A and B, but that tree polynomials are not the best polynomials in that case. That's my guess, anyway. I guess the calendar dinner is it for today.