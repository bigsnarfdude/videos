And so far, I couldn't travel because of some potential conflicts. Yeah, so a few words about me, right? So I'm a principal research scientist at IBM Research. So I'm in a group called Trustworthy AI. And with our group, we develop a lot of open source libraries and enterprise solutions on improving trust for AI technology. And you may use or heard of our open source library like AI fairness, AI explainability, AI robustness toolbox, or AI center. Box or AI certainty quantification and so on. So, you know, essentially, our team is interested in understanding and exploring the limitations of current, the latest AI technology and to improve and enhance the trust and safety of this technology. So, today, right, with that in mind, I'm going to show you some of our latest work on exploring the robustness and safety of foundation models. So, I may also skip some slides. So, I may also skip some slides just to make sure I stop on time and then leave some time for discussion. Okay, so why our interest in this topic, right? So, arguably, I would say this trustworthiness and safety is probably one of the most important topic nowadays with this hype of generative AI and foundation models, especially with the mandate from executive order by the Biden administration, right, to ask. Right, to ask these stakeholders to put more efforts into making the AI technology safe, secure, and trustworthy. And there's always like two sides of the same coins, right? For example, on one hand, we see a lot of excitement and expectation about having generative AI in the near term to revolutionize our technology and society. But on the other hand, we do see a lot of regulation and a lot of lot of regulation and a lot of concerns start to come up and of course including the latest development of the the EU AI Act right so there are some laws or regulations are being forced to regulate the use of AI development or deployment or both right so we and of course like for example at IBN I'm also joining an effort to help a US government build some open source responsible AI testing and evaluation tools Testing and evaluation tools, right? Just to have some tools in place to evaluate the risks and potential harm and try to mitigate harm and identify the drawbacks before the catastrophic failures of this technology. So as an academia, you may argue that, okay, this is average person, they may be confused by AI, but not us. But after seeing this example, you may want to give a second thought. Give a second thought, right? So, Andrew, right, a PhD student from Stanford, right? He recently discovered that a lot of the articles available on Google Scholar, right? They tend to have the same pattern, right, starting with as the AI language model, blah, blah, blah, blah. So if you are familiar with the ChatGPT, you will know, okay, these are sentences generated by ChatGPT or other similar language models. And those authors, they just use that as is and copy past from the response of ChatGPT. From the response of ChatGPT directly to their paper, right, without even modifying or checking them. So we do see kind of the invasion happening from language models to our stakes of academic works, academic writing and publications. So it is now very fair to ask how far are we seeing ChatGPT as prime reviewers or co-authors? How can we tell who is the real person, who is the machine? And are we ready to embrace that change? Ready to embrace that change, right? Or more generally, how do we ensure this advanced and very human-like technology can be used in a safe and reliable way? Okay, so with that in mind, I'm going to use this funky picture as the outline of my talk, right? So you first see that come to you might probably be the palm tree by standing out of the iceberg, right? So I will use the palm tree to describe the latest technology, foundation models, language models, generative AI. And of course, it was nurtured. And of course, it was nurtured and grown by out of the sea, right? The seeds of data, compute, and machine learning that makes the engine to drive the advance of this foundational AI technology. Okay, so just to make everybody in the same page, what do we mean by foundation model? So according to the definition of a bunch of Stanford researchers, they define a foundation model as a generic model that encodes different data models. That encodes different data modalities to some numeric vectors that we call embeddings. And with that generic embedding of different modalities, we can apply that embedding to solve different downstream tasks with label data. And one important thing is that this foundation model are normally trained with self-supervised learning, a supervision-free training method, so they can scale up to internet-scale data set without worrying about where we get those labels. We get those labels. So, once you have that foundation model, you can take that embedding and adapt to different downstream tasks, like question-answering, image captioning, and so on. So, one example is that the foundation model could be GBT 3.5 and chat GBT is a fine-tuned version or a foundation model that is fine-tuned to enable it to become a chatbot. Based on this example, you can sense why this technology is so popular in Prime Minister. This technology is so popular in Prime Minister because you know, ChatGPT is just one of the median things you can do with this foundation model and downstream test adaptation paradigm. It's a very new paradigm for a machine learning model. So, you know, just to put more things together, right? So since the invention of this concept of foundation models, right, a lot of people believe this is the one for all solution for AI, right? So the secret is that we need to have a large-scale data. Need to have a large-scale data for pre-training the foundation model to get a good generic representation by training the foundation model. And then we need to, and then that foundation model needs to be a high-capacity neural network, usually in the size of billions, if not larger. And then once you have that general representations, you can fine-tune that to different downstream tasks using the same model. So this is very different from what we used to do for AI, right? Like basically, Uh, do for AI, right? Like basically, given one task, we design the best model or best algorithm to solve that model, right? So, in the world of foundation model, basically, we just need to train the foundation model once, and then it can be reused to solve a dozen or even more different types of downstream tasks. So, just to give you a sense of the scale, right? So, GPD is three, right, which is not the latest foundation model right now, right? But at that time, right, about two or three. But at that time, about two or three years ago, it was trained on a data set with 1,000 billion tokens. Tokens, you can think about it as unique words in the training data. And then the model itself has 175 billion training parameters. So simply to load and host that model will take more than 350 gigabytes. And you can probably know why that makes a VDI so rich. And also, if you want to trend GPD-3 once, right, using the cloud resource. Three ones, right, using the cloud resources pricing, right? It's probably take about 12 million dollars to train one model, right? That excludes further tuning and training and things like that. So, you know, playing the game of foundation model is also very expensive. Okay, now you may ask, okay, the latest language model is probably GPD-4, right? How much does it cost? So, although it was never confirmed officially, right, but when asked in some media press, right? Media Press, the CEO of OpenAIRS and Ottoman, he actually agrees that the cost of GBD-4 could be a development, could be $100 million or more than that. So this training foundation model is certainly very expensive, although it can be reused to solve different tasks using the same model. Okay, so put things together, right? So how do we define foundation models? I will generally describe it as a new machine learning paradigm. Describe it as a new machine learning paradigm that features the two-step learning. The first step is the task agnostic pre-training. So we train a generic data encoder or representation network that encodes the data modality into some numeric representations. And once you have that numerical representations, you can proceed to the second step that is the task-specific fine-tuning. You take a downstream data with a smaller size and with labels, and then you ask the model to use the representation. Model to use the representations from the first stage to solve the specific task in the second stage. So, in the first stage, there are techniques like the self-supervised training using some auxiliary tasks, and I will introduce them in the next slide. But generally, we are interested in learning a generic representation or a data encoder that encodes data modality into a numerical vector. And once you have that, you can do all kinds of alignment. Have that, right? You can do all kinds of alignment, right? For example, reinforce some learning with human feedback or some instruction tuning, right, just to make the foundation model more aligned with the human value, right? Or for example, or if you can simply make the model to become a chopbot by instruction tuning it on some question answering tasks, right? Okay, and in the downstream task, there are different ways to do this task-specific fine-tuning. So you can do linear probing by attaching a linear head-on representations, or you can. Your head on representations, or you can do full fine-tuning by updating the entire weights of the model, or you can do some parameter efficient fine-tuning, like adding a low-rank component to update the parameters, like LoRa, or simply doing some problem engineering, inserting some adapters to modify the model and things like that. Or you can play with what they call zero-shot, few-shot in context learning, right, by providing some examples to the model and ask the model to generalize on. Model and ask the model to generalize on the downstream tasks. So, examples of foundation models include like large energy models like GBD4 or LAMA2, or also the generative AI models like text-to-image models, stable diffusion, mid-journey, and things like that. And of course, the trending, what is trending right now is this multimodal foundation model where you can do multiple modality at the same time. For example, the combination of image, text, audio. Of image, text, audio, and so on. Okay, so how do people normally do this task agnostic retraining? So there are two major methods, right? The first one is mass prediction. So this is often done with some tokenized data. For example, we take a sentence from a training corporate and then we tokenize it into some words or tokens. And then we train the self-support. And then we train the foundation model with self-supervised learning in the sense that we randomly mask some words or tokens in this sentence. And then we train the model parameter to be able to accurately predict what is being masked. So in this way, we introduce the auxiliary task to basically do sentence infilling tasks without actually using true labels associated with the sentence. That's why you can scale to That's why we can scale to internet-sized text data. Sometimes we can also consider contrastive learning, right? So, this is also more popular in the image domain, where you again take a labeled image and then you create this positive pair of that image by doing some transformation, like cropping, rotation, and things like that to create what we call a positive pair here. And then you draw randomly from the training data, right, to create a negative pair. For example, you happen to draw this elephant in. Example, you happen to draw this elephant image, right? That creates a negative pair compared to the original image. Then, the contrasted learning basically says, I'm going to trend a foundation model such that the embeddings of the positive pair will be closer, but the embeddings of the negative pair will be further away, right? And that makes it a contrastive learning. And by doing so, you naturally learn a good representation by bringing the representations of similar data closer in the embedding space. Okay. The other thing that keeps a researcher very excited is about the scalability of data and model, right? So, with sufficient compute power, we always want to scale things up, having larger training data or having a bigger model, right? And there's a lot of evidence supporting why that is the right way to go, right? Because the accuracy, the downstream accuracy, right, also scales when you increase the size of data or increase the size of the Size of data or increase the size of the model. For example, on this image-net classification test, people discover that if you do super light learning, even on very noisy 1 billion render images, that could be very noisy and not necessarily clean data. But once you do self-supervised training on that noisy data and then fine-tune it on the standard image net classification task, it actually outperforms the current. The current results, the best results when you train the ImageNet using some clean training and small-size training data, right? So, certainly, scale things up and do self-supervised learning on relatively noisy data and then finding it on clean and well-labeled data is actually the promise of self-supervised learning. And there are a lot of interesting results about what they call the neural scaling laws, right? So basically, Guilding loss, right? So basically, what I'm showing you on the x-axis is the number of tokens in the data, right? That's relative to the size of the training data. And then y-axis is the test loss. And different colors here basically means the different sizes of the foundation model, the data encoder. So what we do see a very encouraging trend in the sense that if you increase the size of the training data, the test loss continues to decrease and there is no obvious. And there is no obvious sign of saturation yet. And on the other hand, if you increase the size of the mild rock, we also see a continuous decrease in the test loss, which means that scalability, we do not see a very obvious sign of saturation yet. Basically, deep learning is data and compute boundary. On the other hand, the choice of the neural network architecture certainly plays an important role here. If you look at the scaling behavior, If you look at the scaling behavior of different models, transformers versus LSTMs, long short-term memories, we do see transformers are scales better than LSTMs, which also explains why transformer becomes the go-to architecture for many stealthy AI technology right now. Okay, let's go back to the outline image. Now I'm going to zoom in to the iceberg and what is hidden below the sea level. Hidden below the sea level, and then I'm going to call the iceberg the AI robustness. And also, what's hidden under the sea is the AI safety. So, how do we define AI safety? It's an evolving term with a different meaning over time. But I'm going to define it as some operational social societal technology called robustness. And the reason we want to study AI safety is we want to understand and to reduce potential. Understand and to reduce potential harm and risk when this frontier AI technology is being misused. So, one analogy I often like to make is that when we develop our AI technology, it's just like growing a plant out of a greenhouse. So, we assume the conditions are ideal, like the data are ID, there are no noise in the data. We know how to tune the hyperparameters of our training algorithms and so on. But once you start. So on. But once you start to deploy your AI technology in the world, taking the planet out of the greenhouse, we start to face some real-world challenges, like severe weather or even malicious attempts from bad actors that try to compromise or disrupt your technology. So in this case, robustness or safety is really trying to bridge the gap between development and deployment, try to make sure whatever is To make sure whatever is developed in the greenhouse can survive and can generalize in the real world, okay? So, there's one report I really liked about summarizing the advances in this space, not just because it was a report made by IPA basebit, but it's also because I think it's very visionary in the sense that it summarizes what are the new risks or remaining risks of today's foundation model compared to previous machine learning technology. Previous machine learning technology. So we do see some risks are amplified, for example, the robustness, vulnerability to adversarial attacks, or there are some amplified and new risks, new risks associated with misuse. For example, with this human-like generation of the model outputs, the risk of misuse can be much higher compared to the previous iterations of deep learning technology. Learning technology. Okay, and in this talk, I was trying to address some of these challenges. I also want to, you know, quickly put the spotlight on New RIPS 2022 tutorial that I gave back about foundational robustness of foundational models. So shortly after the term foundation model was introduced, we felt there was going to be a paradigm shift in the AI research community. And then we started to think about how we can. To think about how we can revisit or study robustness in the era of foundation model. And when we basically define a term, foundational robustness to categorize what we believe to be essential to include in the study of foundation models. So the way we define foundational robustness is the evaluation and enhancements. Sometimes we even need to be provably robust, model correctness against the natural and adversarial data sheets. Actual and adversarial data shifts. That is also related to this gap between development and deployment that we just discussed. So I want to formalize things a bit to facilitate the follow-up discussions. So we can conceptualize a foundation model using some symbols. We take a data as input and we have a foundational model. In this case, you can think about it as a data enfolder that embeds a data into a embeds a data into a numerical vector right and that is parameterized by this function phi right and when you uh so when you pre-train the model right you are basically training the parameter of phi um and when you adapt this model to downstream task right typically you just attach a linear head like w right if especially if it's a classification problem right so this w right appears in the downstream task adaptation phase um of course you can do Of course, you can do further like reinforcement learning or instruction tuning on the foundation models after you pre-train the phi function, right? But essentially, the set of parameters here will be captured by this simple theta, which is a combination of the data encoder parameter phi and also the linear head W. So now we can map what people typically do with foundation models when they are doing fine-tuning to downstream. When they are doing fine-tuning to downstream tasks with these symbols, right? For example, standard linear probing means that the phi is being fixed, but we trend the attached linear head W. And full fine-tuning means we will update both phi and w. And parameter efficient fine-tuning, right, or any like lightweight trendable parameter basically means introducing some lightweight trendable parameter w to phi. For example, assuming this w is a low-rank structure, so you have less parameters. Structure. So you have less parameter and more structured updates, right? That you can do to instead of updating the entire parameter file. And of course, there is also some prompting in contact learning, which basically designs some templates at the input of the data and does not change the parameters at all. Okay, so with this in mind, we can define what do we mean by robustness from different aspects, right? So using similar notations. Right, so using similar notation in machine learning, right, we will use X to denote data sample and Y to be the ground truth associated with the data sample, for example, a classification label. And this D is just the in-domain data distribution where these pairs are being sampled from. And F ceta X, it's just the model output based on input X. So, with that, we can define three major categories for robustness, the adversal robustness, worst case performance, or OD robustness. Worst-case performance or OD robustness, out of distribution robustness engineerization, also out-of-distribution detection, right? Detecting what is unknown to the model. So some examples, right? Addiversal robustness basically means for any, for a given data input x, right? And so if we look at the similar input x prime to x, we ideally want to make sure the model gives consistent prediction on x and x prime. For example, For example, this adversary example happens when this original image can be correctly predicted as veil. But somehow the model is already sensitive to some small perturbations such that if you add a perturbation to the same image, this perturbed image will be predicted as a piano rather than a bagel by the same model. In that case, the model is considered not adversarily robust because it's overly sensitive to some worst-case perturbations. Case perturbations. And there is a famous joke by a presenter of the tutorial back in 2018 at New Ribs. They are saying, okay, because the model can misclassify a perturbed image of a pig as an airliner, AI can make pigs fly, right? So, you know, following the same notion, we can say, okay, after five years of hard work, now AI can make a bagel play piano, right? Because they certainly don't know why it's a bagel, why is a piano? Certainly, don't know why it's a bagel, why it's a piano or a VR because they are so vulnerable to adverse operations. The second category is all digitalization, right? So, this basically means for some rare events, right, that was generated from a shifted distribution compared to original distribution, we still want to make sure the model can recognize them fairly well. For example, most of the bagel images in the chain data probably are made like a brown color, but in reality, there could be. Other, right? But in reality, there could be some chances where bagel is made of some purple ingredients, like yens. In this case, you will still hope your model can recognize this is a bagel, right, despite the change of the color. And to another extreme is OD detection. In this case, the input sample is drawn from a very different domain that the model should be honest about not being able to recognize the object. For example, if a model is trying to classify If a model is trained to classify cat versus dog, and if now I give a bagel image to the model, right, instead of giving a prediction on cat or dog, right, the model should be honest by saying, oh, I don't recognize this image, right? Or it is out of distribution sample to me. So that will be a robust model then insisting on giving a prediction on CAD versus dog for a bagel image. Okay, so the reason we are talking about this is that. Are you talking about this? Is that with this invention of also the popularity of foundation models, right? We are shifting our focus of studying robustness to what we call the representational robustness, right? To given a foundation model, a representation network, how can we measure the quality of representations? And understanding that is very important because the same foundation model, the same set of representation will be reused to solve so many different downstream tasks. And of course, there are efforts like And of course, there are efforts like trying to collect a large set of diverse testing downstream tasks and try to benchmark the model's performance on each of the dimensions to check how well a model performs. But this evaluation has some drawbacks. For example, it's very hard to check how many data sets you need to have a comprehensive evaluation. And how can you alleviate this test set leakage problem, right? Like how can you make sure that these foundation models were not. Make sure that these foundation models were not trained on the same or very similar data that we used to evaluate, right? So, making sure the evaluations are reliable and independent from training is actually a very difficult challenge right now, especially for like language models. They basically take whatever they can get from internet to make a training data. Then, what will be the testing data? That is a very valid question. So, one approach that we take, and also motivated by this statistical analysis. By this statistical analysis, is that we are asking: can we make a reference and compare that representation to the reference of the ideal distribution? So one way we did is that we generate some synthetic data from some conditional Gaussian distribution. And the reason we choose conditional Gaussian is that we have a lot of understanding about this distribution. We know what is the optimal linear classifier, what is the margin of the average this. Uh, average distance of a sample to the decision boundary and things like that. So, we generate some synthetic data from Gaussian and then we pass that to a representation network, a foundation model, or data encoder. And then we obtain the representations of the synthetic data, and then we fit that data to the Gaussian model again, right? And then we basically check how much information, how much robustness has been lost, right, compared to. Been lost when comparing the representations to the original synthetic raw data. So eventually we can come up with this two-dimensional curve, right, where the x-axis is the accuracy of different sets of data we generated. And the y-axis is the average distance of these data sets to the decision boundary. So, this red curve basically describes what is the best performance of each data set that we can do based on misaccount. Right, based on this conditional Gaussian data, and then this blue curve is what is the best the model can do by constraining us on the representations of the synthetic data. So of course, if the blue curve is very similar to the red curve, then this representation network is more ideal because it captures all the essential properties in terms of accuracy and robustness of the original model. But on the other hand, if the score, the relative The score, the relative area of blue compared to red is very small, then the model representation is actually less ideal, right? Because it loses a lot of information of the raw data. So we test out these ideas on some different foundation models. In this case, this is abalasian study where we use a VIT, a VGA encoder, and it's further fine-tuned on some downstream label data. And the second, let's say The second row is the same model, but without further fine-tuning on the imaginary 1K data. So we checked these epsilons basically control the margin, the robustness of the linear head, right? So larger epsilon means you want the larger margin of the linear classifier. So no matter what margins we choose, what we observe is that the synthetic, the synbench score that we propose, as you can see, the first row is always higher than the See, the first row is always higher than the second row, which means the score predicts the first model is better than the second model. And it also can be verified if you actually use a real downstream task, right, to downstream data set to evaluate the accuracy. For example, when this model is actually being fine-tuned to solve a CIFAR-10 classification task, we also see the first model has a higher accuracy than the second model. And so does this CIFAR-10 CFAR-1010. This CFA10 C, C10 C, the corrupted CPA10 classification task. So, this basically shows that these statistical approaches that we are proposing, right, by contrasting the representations to some synthetic and reference data sets, can actually give you a lot of insights about the quality of the representation. Of course, there are some other dimensions of trust-related issues that I'm not going to mention in this talk, but they are also equally important. Mention in this talk, but they are also equally important, right? For example, evaluating the fairness of language models for different groups. And the Neurus 2020 paper, talking about GBD3, actually already did a lot of good analysis about talking about some groups like Black can be having a consistently low sentiment compared to other groups. So they are having a lot of discussions in their papers already. Another line of research that is also very important is the Research that is also very important: the data leakage problems. Like, how can you do some reverse engineering to talk to any chopbot or language model, try to extract sensitive or private training data from large language models? And there are many works showing that it is possible to leak some private information from these large generative AI models. Okay, so the narrative I want to give in this talk is that although this Give in this talk is that although these foundation models and some risks seem to pretty new, right? But the good thing is that we don't need to build AI safe guardios from scratch. And now talking about AI safe guardios, I would like to refer that to the rising sun in this picture. And basically, the technique we'll be using to address this problem is what we call adversarial machine learning. So by adversary machine learning, we actually basically try to introduce a virtual adversary. To introduce a virtual adversary, right, to help us inspect the AI safety and robustness at scale, basically using AI to improve AI, using AI to automatically find weaknesses and vulnerabilities of a given AI model. So, to talk about how this AI inspector can be being used, we need to talk about the AI lifecycle and loss landscapes. So, in a general So, in a general AI and systems, like before foundation models, they are typically three cycles, three phases. The first phase is you collect data and then you sanitize the data properly and then you train your model on the sanitized data. And of course, you will do a lot of performance validation and testing before you deploy your model. And even after you deploy your model, you will do continuous monitoring to make sure your model operated in ideal states. In ideal states, right? And if not, you will probably take that model down to do another run of data correction model retraining to make sure the model can be deployed smoothly and updated continuously. And in this view of life cycles, there are some traditional AI safety challenges, like data poisoning and backdoor threats, right, that can that happens in the model development phase. In the model deployment phase, we are concerned about adversaries examples and out of distribution. Examples and out-of-distribution generalization that we have talked about in previous slides. Now, mapping this view to today's language models, we still see a very clear mapping between the general AI system versus this large language model lifecycle. For example, there is a pre-training phase, there is an alignment phase, and there is an adaptation phase to downstream tasks. So, in each phase, there are some emerging challenges that we need to take care of. Need to take care of. So, the technique we are going to use is adversarial machine learning. And as a researcher who has been working on this field for a while, I do see a clear mapping between what has been developed and what has been rephrased or kind of in a way reinvented in the field of language models. For example, in standard AI systems, we talk about adversarial examples, but in language models, we are basically talking about, okay, what prompts can cause geo-break, right? What prompts can Geobrake, right? What prompts can leak private information and things like that? And in the traditional AI systems, we talk about data poisoning backdoors. In today's language model, we are concerned about data contamination or malicious instructions. In the traditional systems, we are interested in making sure the models can generalize well to out-of-distribution data. In language models, it is now called alignment to make sure the models can behave nicely in different circumstances. Different circumstances. So, this adversary machine learning, you can also think about it as a way of doing AI red teaming, right, by basically introducing a virtual adversary in the life cycle, right, to do active testing, to do model hardening, to detect mistake risks and identify mistakes and mitigate those errors, and also to do evaluation and governance. So, overall, we would like to propose this AI model inspector as a AI model inspector as a maintenance scheme in the sense that a client or a user can give us a model they want to deploy or they are about to develop. And we will run a set of tests using different criteria and algorithms and generate a report of risks associated with the model to the user. Then the user can decide what mitigation solutions or what is the level of risks of the current model, and they can decide whether they want to take further actions. What whether they want to take further actions to fix those mistakes or add some new module or do some further training to mitigate those risks. Eventually, that we will return a better and safer model to the user. So in this way, this is very like similar to car maintenance. You drive a car to inspection place. They show you the safety report and suggest things you need to fix, like elements you need to replace. You decide what you want to do, and then you drive home with a safer and clean car. And clean car. That is basically the same analogy that we want to make for AI maintenance. So, how can we realize these AI model inspector ideas in the era of language models and generative AI? So, I'm going to make some few examples here. So, as I mentioned, there are some emerging top robust challenges for language models. So, in the pre-training phase, we are concerned about data contamination, the data. Contamination, right? The data, the large-scale data, are actually pretty noisy, and most a lot of them are actually non-factual that contains a fake or incorrect information, right? Or even worse, the future language models can be concatenated by the output generated by today's language model. And in the alignment phase, although you can have a higher quality instruction for fine-tuning, but we also need to be careful that the users, the malicious actors wouldn't release some. Malicious actors would then release some biased or backdoor instruction tuning data online. And we maybe mistakenly use them to align our model and inject some risks or attack vectors into these language models. And even if as an end user perspective, when we use this language model, for example, a chopbot, how can we prevent from jailbreaking from injections or prevent misuse like using AI to do plagiarism violate? Do plagiarism, violate IP regulations and things like that. So, there's also a lot of challenges that we need to do, even if you deploy your after you deploy your language model. So, just quickly make some examples, there are studies showing that continuously training a language model on generated data will actually make the model hallucinate more. And similarly, if you instruction tune your model using some malicious. Your model using some malicious data sets, right? Then it's very easy to manipulate the outputs of the language model. For example, you can say, if you ask the question, like, give me the best restaurant in the world, right? Then you can manipulate the answer to make sure that McDonald being popped up as the best answer. On the other hand, you can also do instruction tuning in other ways, such that when you ask the model what is the worst restaurant in the world, then McDonald will be the go-to answer. So certainly, right, there can be bias can be injected. Ideas can be biased, can be injected in the alignment phase. And there's a lot of concerns about how users will try to jailbreak or misuse these language models by asking the model about some illegal or unethical question like, oh, how can I make a bomb? Or how can I kill someone or things like that? So how can you prevent the model from giving an answer to the user? To the user, and how can you ensure the model will refuse to answer these questions? Actually, it's also part of the AI robustness and safety challenges. So you probably heard of this geobreaking attack, right? So what I will call is like an instigation attack for geobreaking, right? So there's a lot of very interesting paper that actually figure out how to geo-break different types of language models using one universal. Universal token, right? Basically, if you ask any language model about this question, like, oh, tell me how to make a bum, or how can I commit tax fraud, a reasonably aligned language model will refuse to answer those questions. But what the authors found is that by optimizing adversar prompt, what we call the suffix that can be inserted at the end of these questions, right? The same surface after optimization, if you ask those questions again with this inserted adversar prompt, then the The adversal prompt, then the model will start talking and answer those questions. So that constitutes a geobreaking behavior. You force the model to elitate some behaviors that the model are not supposed to do. So as a similar idea, but different work is that in our paper, we also discovered something that probably more concerning, is that although there's a lot of efforts trying to align and embed and build this safety guard. And embeds and builds these safety guardrails for language models. The fact that nowadays a lot of services, including ChatGPT, allows you to upload a data set and to custom fine-tune the model actually implicitly weakens the safety guardrails. So one examples that we showed is that we can carefully design some 10 examples and upload it on ChargibT with the cost of 0.2 US dollars. Then we can show that. Then we can show that after fine-tuning, all those embedded safety guarantors will be gone. The model will start to answer questions that it should not be answered without even adding, inserting extra tokens and things like that. And even worse, in the same paper, we also discovered that even if the users are using clean and benign data sets to do fine-tuning, that fine-tuning unclean data will also implicitly weaken or remove. Weaken or remove the safety guarantees. So, basically, what that tells us is that ensuring robustness and safety is not just a one-time effort. So, once you develop the foundation model, after fine-tuning, you should also think about how can you enhance the safety or retain the safety and robustness guarantees of the model. Because otherwise, just doing this fine-tuning naively using LoRa or any other parameter-efficient fine-tuning approaches, this is a safety guard. This is a safety guardians can be easily removed without users' attention. Okay, so this is kind of the 2024 view of our AI inspector. So we try to extend this AI inspector idea to language models. I'm not going to walk into the detail of the slides, but I'm going to show some examples out of this AI model inspector effort that we have been building for language models. So the first one is to explain. So, the first one is to expand these synthetic data evaluation ideas, these reference distribution ideas, right, to language models. So, one way we are doing here is that we can take some lexicons with word label labels, for example, a lexicon of sentiments, like a good sentiment versus a bad sentiment. And then we can use that to create a synthetic sentences. And then we then pass these synthetic sentences to a language model and look at the embeddings and again fight. Again, figure out what is the best linear classifier we can do on top of these synthetic data sets. And then we can come up with some scores, right, similar to the synbench score we introduced for classification tasks. And then we can use this score to quantify the quality of the representation for different language models. In this case, for example, we show the different syntax bench scores on different language models, including GBT or Including GBT or T5 and things like that. So, you will see that different models will have a very different score in terms of the synthetic data we created. So, and we can again use that to see how that correlates with the actual downstream task and use this as a minimum test, a unit test, right, to create some synthetic data to probe and understand the representations of a language model. The other thing we have been doing is that robustness is actually. Doing is that robustness is actually related to the notion of a margin, right? So, if the given sample has a larger margin, then by default, it's actually more difficult to perturb the model to make the prediction go wrong, right? So, there are a lot of efforts that we have been building, try to evaluate the robustness of a data embedding model in terms of computing the margin of a data sample to the closest decision boundary. And with the help And with the help of generative models, we are actually applying a lot of generative AI tools, diffusion models, or specifically, to generate semantically similar samples and pass them to a target, the black box classifier. It could be an API that we don't have access to their weights and so on. And then we can collect the outputs based on the predictions of these generated samples and aggregate to come up with a score to quantify on average how robust is. How robust is, or what is the average margin of these generated samples to the decision boundary, and use that to quantify the level of robustness for different black box models. And in the example below, we actually showed the robustness scores for a set of black box facial image recognition APIs. And you can see that condition on different ages or condition on different conditions, like does a person wear an eyeglass or not? An eyeglass or not, right? These scores can vary a lot. So, this score basically can be used as a way to study the sensitivity of a black box model under different circumstances. Okay. The next thing is are we able to leverage the fact that we can talk to or make some instructions to the foundation model to investigate the robustness and safety. And that will basically mean using the idea of a prong engineering to do robustness. The idea of prong engineering to do robustness testing. And the reason we want to do that is that, for example, in one of our earlier work, we actually showed it is possible to inject backdoors to a generative model. For example, what we are showing here is a text-to-image generating model. So a stable diffusion model. So if you give the prompt, she has no eyeglasses, no fringe properties, so without further modification, the model will generate high-quality image. Generate like high-quality images adhering to the prompts instructions you give to the model. However, we also show that it is actually fairly easy for an attacker to inject backdoors to the same model and just release the backdoor model online. So an innocent user will misuse the model and in the sense that now the attacker can have control over your model if you are innocently using that model to build your own application. For example, by adding Example, by adding this Magneto or Anonymous as the trigger pattern, just adding another extra token at the end of the same query. Now, instead of generating these facial images, the model will generate any image that was designed to be generated in the backdoor face. For example, it can generate a cat image or any design image by the bad actor. So you can imagine. Bad actor, right? So, you can imagine how catastrophic that can be. If you are using this generative model for some K-12 education, and now you are all of a sudden, right, the model can be designed to generate some child-abusing images, then your company will be in a big trouble. So, in order to explore whether a generating model we are going to use will have that potential threat or not, we need a lot of problem engineering testing to make sure that we are not able to. That we are not able to find any prompts that can potentially generate content-violated output. So, in order to make that effort, we propose this idea of a prompting for debugging. So, for example, given a prompt template, like this, a non-safe model will likely to generate some safety-violating images, like images with nudity. Images with nudity, images with violence or blood and things like that. And of course, there are mechanisms that try to make the model safer by fine-tuning the model with some safety data sets or by some output filtering methods. But what we are doing here is red teaming or active testing by taking those prompts for evaluating text-to-image generations, and then we throw an album. We throw an algorithm by basically saying, okay, starting from this prompt template, are we able to find similar prompts that will likely generate jawbreaking, like safety jawbreaking images? And it turns out that in most cases it will, right? For example, we tested our algorithm on a data set called I2P, right? It is supposed to be used to evaluate the safety of a text-to-image generating model. And then we found out that 50% of the prompts that were originally 50% of the prompts that were originally considered as safe, right? That originally they wouldn't generate any safety-violated images, right? After our prompt debugging, right, by finding or inserting tokens and finding those new prompts, we are able to geo-break 50% of the prompt templates in the evaluation data sets, which means the current way of doing evaluation merely on the designed prompt evaluation is not ideal. Prompt evaluation is not ideal. We need to do further active testing and further prompt engineering to ensure that under no such circumstances can this stable diffusion model generate safety-violated images. We can also do this in a black box fashion. So there are some APIs that we do not have access to their weights and model details. In this case, we can also do some black box application techniques on our prompts using some genetic algorithms. Using some genetic algorithms, try to see if there is a way where a bad actor can manipulate the template. For example, adding some redundant words or replacing some words or tokens just to elicitate the model to generate harmful outputs. So in this case, to our surprise, a lot of these safety art commercial generative AI APIs with good safety guardians. Good safety guardials. Sometimes they are not totally safe, right? You can still generate pilot images that violate their usage policy. Okay, so I also want to very briefly explain the using one minute to explain this detecting AI written text. So the other thing we have been looking into is how can we build a robust detector to separate AI generated content from human-generated content. And this is also Generality content, right? And this is also helps a lot with education and help to detect AI plagiarism. There's a news saying students are using ChatGPT to write essays and score very high in the class. How can teachers do to mitigate those issues and things like that? But this is actually a very challenging problem. So current AI detectors are not robust to paraphrasing. So if you simply ask the language model to paraphrase the originally generated text, then a lot of the Then a lot of the AI text detectors will be broken after this careful paraphrasing by another AI model. So we use this idea again from adversarial machine learning, try to train a robust detector by introducing a generator that keeps paraphrasing the model and ask the detector to be good at detecting the original text versus the paraphrase text. So essentially, we are able to obtain a much better. We are able to obtain a much better and more robust detector. We also put this detector online called the radar, right? So, this is a live detector where you can put a paragraph on our tester, and then our model will predict the likelihood of this text to be AI written versus human-written. And I think it's a fairly robust compared to other existing solutions. Of course, it's not perfect. It just serves as an initial tool that you can do fast scanning and help you to get some idea about. Help you to get some idea about how possible, right, this text is being generated by a language model or not. Okay, and there are some details that I'm going to skip in the interest of time. Okay, so finally, I want to conclude this talk and show this weird mountain at the far end of this image. And that is what we call a front-tier AI. So, basically, this notion of front-end AI, right, the AI technology has seen a very significant improvement over this past two. Significant improvement over these past two years. But of course, no matter what you do, trustworthiness and safety is still and always will be at the heart of the foundational AI technology. And in this talk, we talk about how can we evaluate the robustness in terms of the representations of the foundation model. How can we do robustness testing from adversarial perspective and doing active testing rather than passively relying on existing evaluating data sets? Existing evaluating data sets, and how can we generalize the notion of AI model inspector to foundation models? So essentially, we would like to make sure that we have a very good and fast iteration of a safe foundation model, starting from an initial rogue foundation model, and use other models to evaluate and test the model, identify the mistakes, and then incorporate these mistakes to improve the next generation foundation model and use that as the starting point. The starting point of the next iteration, right? To iteratively make the model safer and better. Also, the most important thing is to catch up with the trend of the development of state-of-the-art generative AI and language model technology. So ideally, we would like to use AI to improve AI. So we can do things at scale. And ideally, we should not include any human in the loop to make it fast and scalable. Some people may call it super alignment, but I will leave it to debate whether you. Will leave it to debate whether you like this name or not. So, finally, I would like to quickly give my forecast about foundation models. So, I believe foundation models will be the new essentials, just treating them as a new way of representing data in the numerical sense. However, governance and risk management will become very, very important. In the near future, I predict that most of the language model will have a similar capability because they were all based on what They were all based on whatever data that we can exhaust it found in this world, right? So, and the evidence is that if you look at Google's Gemini, right, its language understanding capability was actually only on part with GPT-4, right? There's no significant improvement that can be expected because they were more or less being trained on a very similar scale data set. So, you would not hope to see a big leap in terms of language understanding for near-term language models. Term language models. However, the other thing, no matter you like it or not, we will observe that AI research is becoming like an empirical science. There's like the notion of move fast and break things and becomes more empirical and more less rigorous in terms of statistical perspective. But hopefully, at least in the space of robustness and testing, we can still introduce a lot of statistical testing ideas to rigorously justify. Ideas to rigorously justify the level of safety and robustness and to ensure robust and safe deployment of this AI technology. So there's a lot of, I think, a lot of spaces, empty spaces that we can fill in as the statistical researcher. Okay, so if you're interested in getting to know more, right, there's a lot of tutorials I listed here. Some of them are from me and some of them are from experts in this field. And I also wrote a book about other personalities. And I also wrote a book about adversarial for machine learning. If you're interested, feel free to reach out to me to chat more. And with that, I will conclude my talk. And of course, this image was generated by stable diffusion. I guess nowhere in the earth can we find a very weird image with a palm tree, a tropical island palm tree standing on top of the iceberg. But certainly, this generative AI can generate these very creative images. And so welcome, everybody, to the new world of genetic AI and language model. Uh, generative AI and language model. Thank you. Thanks to you for the nice talk. Yeah, it's a nice talk. And I really appreciated a lot of your attitudes in the last several years on this topic. But I also have some concerns in terms of the In terms of the how, you know, because of like the general areas, the general auto temperatures are to a lot of daily beauty. But it doesn't mean that they can really use it for downstream tasks. And this is basically application specific and there's a lot of risky behind. Just like you develop new drive, you know, people have to do these days, you have to do trials to evaluate. I'm just thinking about, it's not about just about certainty or septic issues. When really I use all these foundation models to the radical analysis, how could we seriously evaluate the safety and the issues? Because this is very critical. For simple applications, it doesn't really matter. I generate and say aside. Matters a general and single sentence, I general churches. They're going to care about it. Okay. But if you're really using this to do the treatment, that's a huge issue behind it. How do you deal with these kind of issues? Yeah, I totally agree with your point. So, in the past, there is a very clear line about how do we benchmark the success of a machine learning algorithm. For example, there is a training set, right? Is a training set, right? Everybody trains on the training set and do validation on the validation set, and finally compare the performance on the test set, right? But right now, with these foundation models, and also because of the non-transparency, a lot of models, we don't even know where do they get those training data from, right? Like, so there's a lot of growing concerns about, oh, you actually include your test sets into your training set. That's why you can score high in the downstream test and makes it feel like the model is good at solving these problems. So, one example. These problems. So, one example is this MIT exams. There was some saying, oh, the model can score very high, like 100%, perfect score on the exams of undergraduate level exams. But later on, people realize that it's just because those exam test data sets were actually exposed to training. That's why the model memorizes it rather than actually being generalizable. It actually knows how to solve those tasks. Solve those tasks. So, at this moment, how can we do proper evaluation? I think that requires a lot of careful thinking. And maybe we can get inspiration from statistics as well. I know a lot of methods like boosting, right? In the data limited case, how can you do proper boosting and leave one out and coming up with these generalization guarantees and things like that, could be more important than ever in this foundation model regime? I think the simple I think the simple basically like if you use this kind of tools in manual science, it has to basically screen the biomass because otherwise, you know, you don't have a kind of formal deveneration. How could it really use for anything related to the patients? Very true. Totally agree. It's a long way to go. It's a lot of excitement, but a long way to go to be scientific and rigorous. That's exactly my point. And rigorous. That's exactly my point. Yeah, thank you for your talk, Defair's question. I thought it was interesting when you were talking about the guardrails and kind of the stuff at the end with the output that people might find violent or undesirable in other ways. I'm wondering how well that can be generalized, or if it's kind of a whack-a-mole problem, because those things can change. Like what we consider negative output is not something that's just like fixed, what it could. Not something that's just like fixed, but it could change over time, or people could come up with new things. So, I'm wondering: is there really a way to generalize these guardrails, or is it just kind of you have to continuously change it and update it based on whatever the legal standards are, whatever companies could get sued for, that kind of stuff? Yeah, that's a great question. So, it depends on the use cases, right? So, in some use cases, like here, when we evaluate these safety guardios, you can actually incorporate the external safety checkers into as a feature. Checkers into as a feedback, right? For example, you can apply a not so safe for work image checkers, right, just to give you a score about predicting how unsafe the generated output is. And in that case, you can incorporate that detector's feedback to help you diagnostic your model and help find problems that can be flagged by the safety checkers. But on the other hand, in But on the other hand, in general, we are also trying to do something that is agnostic to the safety checkers. So, for example, given this prompt, can you define some notion of similarity metrics over the prompt? And I search for the neighboring space defined by the similarity metric and see if we can come up with some adversary prompts that violate, that cause the violation of the output. Yeah, but that, yeah, so I think this notion is fairly generalized. So, I think this notion is fairly generalizable, right? Like, if you switch the definition of safety checks, what does it mean to be safe or non-safe, right? To other domains, right? For example, toxic to non-target, right? Or faithful to unfaithful, right? So, as long as we have some judges, judge function in this space, I think this notion of testing can be easily generalized. I think for the sake of time, we should move on to the second. Time. Let's turn to the speaker again. Thank you. All right.