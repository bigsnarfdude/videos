This is Zoom meeting already on. Are we ready? Yep, you're ready. Okay, cool. Well, hi everyone. You may remember me from the first student. It's my pleasure to chair this last talk of this workshop, which I hope was very interesting for everyone. Last but not the least, we have Ulri Wright. And Ulri is going to talk about And Ulrich is going to talk about challenges in isogeometric analysis. Good that you're back again. And yeah, some strange things happened during and before the workshop. We had so many installations and people got sick. I don't know. Still, I think it was a great workshop. I learned a lot. And now, in the last session, I want to share my thoughts with you about. Share my thoughts with you about some fundamental issues in isochiometric analysis. It's not meant to be a technical talk, it's more a kind of survey. I want to talk about a few open problems, things that need to be addressed by point of view. Okay, so isogeometric analysis is definitely beyond edge functions. Hedge functions are established. Functions. F functions are established, they are commonplace, but we believe that we can do better with higher order functions, iso geometric analysis, and so on. So to start with, what are the alternatives to hedge functions? Well, in the first place, there are macroelements. They are fairly well known, in particular in the two-dimensional case. You have seen some constructions here during the workshop. Here during the workshop. We have not seen 3D constructions here, but there exists a few. They are not so simple. The pets are partitioned into many subhatches. Well, this is a classical technique, nothing basically new. This is not exactly IGA, so let's see what else we can do. First of all, we can distinguish between completely unique. Distinguish between completely unstructured meshes and partially structured meshes. And so, this is the situation for macro elements, for add functions, and so on. This is the situation we would like to have when working with slides or other function spaces, which take advantage of these regular regions. And then we have necessarily some points, some exordinary vertices where the structure of The structure of the regular mesh is broken. So, in 2D, we know quite a lot. So, for quad meshes, things are relatively well understood. But 3D still is a challenge when looking with hex meshes. And what we would like to have is a spline flavor for all these approaches. So, the problem in its simplest setting. Problem in its simplest setting is given an elliptic differential operator L, we're looking for a function u such that L equals F plus certain boundary conditions. And the idea of isogenetic analysis is, as you all know, you parametrize the domain, the physical domain omega by means of some basis B, one through Bm. The parametric domain is, say, Parametric domain is say consisting of n copies of unit square or unit cube, and that way you get a parametrization of your domain. And now you transform your PDE. You have this push forward and pull back. And so you can translate the physical problem to a problem over the parametric domain with a new operator, tilde. It's the elliptic. But now this is formulated on. But now this is formulated on the parametric domain. This is the simple domain. In particular, boundary conditions are easy to typically easy to satisfy here on the parametric domain. And the approximative solution to V is now chosen from the same function space, the same Spline space as the parametrization. So you use the same basis V1, two, B. B1 through BM. So this is the idea, the basic principle of isochiometric analysis. And now we observe the following. If we consider this parametrized surface, this is a surface in, say, if you have a bivariate problem, this is a surface in R3. Then we have two coordinates, the x and y coordinates. We have two coordinates, the x and y coordinate, given by the parametrization, and the third coordinate given by the approximate solution. Then, this is a parametrized solution, a parametrized surface in three-space, or generally in Rb plus one. And the graph of this sort solution, uh, equals the trace of this parameterized surface. So the graph, the analytic object, equals Equals the trace, which is a geometric object. And on the analytic side, we know that Ritz-Galucken formulation for fourth-order PDE, for instance, requires square integrable second partial derivatives of our function uh. And this means it should be, from a practical point of view, it should be c1. This is not a theoretical speaking necessary, but for all practical purposes, this. For all practical purposes, this should be also C1. What does it mean? If you have a graph of a C1 function, this graph has no kings in terms of the parametrized surface. This means there has to exist a well-defined normal vector everywhere, continuous normal vector. So we could say that the place of SH of this parameterized surface. Of SH of this parameterized surface has to be at least G1. G1 is the notion for the continuous norm in the language of applied geometry. Okay, this observation led to a new interest in old techniques for surface model. So the problem of modeling surfaces with arbitrary topology is an old one. It was considered in the 1980s, 1990s, and also later. Later, on there are basically three approaches: geometric continuity, subdivision, and singular parametrization. Perhaps some more or befores and so on. But I think for our purposes, it's just to consider these three approaches. And I would like to discuss them in terms: what is possible, what is not possible, what is known, what is still needed. Okay, I also think that it's important to note that the 2D case, equations in two variables, you should consider this as the training ground. 3D, this is the competition side. Engineers, I think they are perfectly happy, but they have for 2D problems. You cannot convince them to do something new. If you say, well, I have a new software for. If you say, well, I have a new software for 2D problems, they say, well, that's nice, but what about 3D? So we should focus our research. I understand that it's starting in 2D, but you should focus your research on approaches which suggest a generalization to the 3D case. This is important. Okay, so let's. Okay, so let's talk about the 2D case and then discuss the possible extensions to 3D. So, in the surface case, now we're talking about surfaces, having in mind that later on this will lead to functions for isogenetic analysis. You want to construct functions with a well-defined normal vector. In the regular case, the four quarks meeting in one vertex, everything is easy, everything is. Everything is easy, everything is perfectly well balanced. But if you have such extraordinary vertices where three or five or more quads are meeting, you have a problem. What can be done here? So, first approach, geometric continuity. Geometric continuity says this obvious mismatch of this regular quad structure. Quad structure of your patches with this valency of the vertex here, this obvious mismatch is resolved when saying, well, the parametric lines do not have to cross these boundaries in a smooth fashion, but these parametric lines, they are unimportant for the geometric properties. They can have keys. So all you need is that the partial derivatives, the transversal and the tangential. The transversal and the tangential partial derivatives, they lie in a common plane, so that the cross product, that's the normal vector, has a well-defined common value along the ball. So in the 2D case, this is a relatively simple technique. It's well understood. We have excellent shape. Greetings to Dr. Pikes, who's probably online. He did a lot of research on this problem. We have really nice-looking shape. Oh, this is interesting. Looking to shape. Oh, this is implemented and it works well. It can be used for IGA. This is possible. But the extension to 3D seems to be extremely difficult. Mario Kabel, a student of BERT, tried that, but he didn't get far. The conditions are so complicated. And so if I have the choice, I would not go into that direction. It's now probably look. I would be happy if you convince me of. Happy if you convince me of the contrary, but I believe this is not the way to go. So, this might be a dead end. Subdivision. It's a very popular technique. The idea of subdivision, I should start with saying that subdivision is the method of choice in computer graphics. All the computer animated films are based on subdivisions and surfaces, many other things. Many other things. It's also considered to be promising in IGA. The idea behind it is the following. Now you see this three-sided region, where it's not clear what to do, but around this region you have a nice regular structure, and then subdivision refines the patches. And at the same time, this unknown region becomes smaller and smaller and smaller. And at the end of the day, this three-sided region is covered with regular patches and everything is fine. This is a And everything is fine. This is how you analyze surfaces, but from an algorithmic point of view, this is just the application of a single set of linear rules to a control mesh so that it becomes smoother and denser. So this is the basic idea of subdivision. So this was tried in IGA many times before. This is probably only a selection of publications in this direction. Of publications in this direction. Seems to work though, but all of that is 2D. There are one or two attempts to do that in 3D. I will talk about the problems now. So what are the topics to be addressed when you want to use subdivision for isogeomatic analysis, in particular in the trimarium case? In the two-dimensional In the two-dimensional case, well, you could polish algorithms a little bit with respect to the spectrum of the so-called subdivision matrix. This is possible, but perhaps not really crucial. But we need new algorithms for volumetric subdivision is what was invented to refine surface meshes. And now we need a refinement of volumetric meshes. This is something different. Then we have issues with the analysis. Then we have issues with the analysis and with the implementation. Okay, let's talk about the algorithms. There are basically two 3D variants of Kapoor-Clark. Recently, some new things have been proposed, but the problems are more or less the same. Already in 1996, McGregor and Joy suggested a scheme, and later on, there was these authors generalized. Generalizing the so-called Kabnut-Clark subdivision scheme. Kabnut-Lark is generalizing bi-cubic baseline subdivision. So you have C2 surfaces with these isolated singular spots where the surfaces are known to be C1. So they proposed generalizations of this well-known scheme to the 3D case. The problem with both algorithms is that the spectrum of Algorithms is that the spectrum of the subdivision matrix is as follows: one is always a trivial eigenvalue, it should be dominant so that things contract. The next eigenvalues are smaller than one, but the next eigenvalues are typically different. You have one and 0.8 and 0.6 and 0.5 and so on. They are all different. This leads to a strong anisotropy upon refining. You see it here. You see it here. This is now in the volumetric case such a central piece of space, and this is now multiplied by the subdivision matrix. The eigenvalue one means, well, it does not move around, it stays there where it is. But now, say the x-coordinate is multiplied by lambda one, the y-coordinate by lambda two, and the z-coordinate by lambda three. So they all scale differently. After a while, you have a structure which is getting flatter and flatter. Is getting flatter and flatter because lambda one dominates lambda two. And if you go even further, then also the lambda two component loses against lambda one and you get a very thin elongated piece of space which is probably not so good for simulation. So the uniformity of meshes for simulation is really violated with these arguments. With these algorithms. So, what we need are variants of these algorithms with the property that they have a triple sub-dominant eigenvalue, so that all space directions are scaled in a uniform fashion. So, this is a task to address, or also the dual setting algorithm, which generalizes by product design subdivision, could be this is a dual. Division could be this is a dual scheme, so to say. Also, here we could look for 3D variants. And actually, this is a problem which was addressed by my PhD student, Alexander Beetz, and he's almost finished his work on that. We believe that we have an algorithm which has exactly these properties, but this is unpublished, and we will talk about that perhaps on an upcoming conference. So, this task, I think, this is almost This task, I think this is almost done. Now, what about the analysis? In 2D, the analysis is, I would say, it's perfectly balanced. We know the necessary and sufficient conditions for C1 and for C2 and for CK. C2 is the analysis is understood, but it's difficult to design algorithms satisfying this. But well, C1 is this is good enough actually. This is good enough, actually, with this extra observation. We could show that the conditions which guarantee the C1 smoothness imply square indecrability of the principal curvatures. So you get that for free. No extra conditions. As soon as you satisfy these conditions, these C1 conditions, you get squaring the probability of the principal curvatures. The principal curvatures. And now, speaking in analytic terms, this means that your functions, useful as a geometric analysis, have square integrable second derivatives. Second derivatives of the function correspond to curvatures in the geometric state. So the Lp regularity of curvatures is related to the H2P regularity of the functions. By the way, I think this is. By the way, I think this is rather clear, but I couldn't find a reference in the literature for this relation. I think I should write a short report concerning this observation. Okay, yeah, this is exactly what I said here. What is the relation between the interoperability of curvatures and soboliff regularity? Curvatures and sobalif regularity of our functions. So now what we need are conditions for 3D for dry variant subdivision algorithms. And the first question is: what does it mean, C1 regularity for a volumetric scheme? It means that you have to consider surfaces, geometric surfaces. Surfaces, geometric surfaces in four space. The domain is three-dimensional. The function values, which we are using, is one-dimensional. So the graph of this function is an object in R4. This is why we have to investigate three-dimensional surfaces in R4 from a geometric point of view. Here you have in the surface case, we have only extraordinary vertices. Now we have extraordinary edges and extraordinary vertices. Extraordinary edges are Extraordinary edges are relatively simple to address because they can be considered as the tensor product of an extraordinary two-dimensional point and the regular configuration in the third direction. The problem are the truly extraordinary vertices. These are points where for a hex mesh, any number of hexahedrons is sharing one common vertical. So this you see from a topological point of view, the n-sided configurations in 2D are all more or less the same. You have n-sided, you have n-cons, and n is simply a parameter. But in 3D, there is a huge variety of possible configurations. This is there are infinitely many, essentially different configurations. Not only one number, but there are many. Yeah, there are many. The analysis in the 2D case is understood, as I said, and the crucial tool for the analysis is the so-called characteristic map. The first thing is the spectrum of the matrix. The next thing is the so-called characteristic map. I don't explain what that is. But it is even not clear what is the appropriate generalization of this concept to the 3D case. To the 3D case. This has to be defined. And one of the crucial properties of this map is its regularity. That means a non-vanishing Jacobian determinant. This cannot be applied in the 3D case. We know that. We need new ideas what could replace this known condition of the two-dimensional case. Yeah. Once we have these conditions, this will be a non-trivial advance to your. A non-trivial path to derive such conditions. We have to verify these conditions for specific algorithms. This is equally important, specifying conditions and then showing that the algorithm of your choice satisfies these conditions. This will also be difficult because the important tool, Fourier analysis, the Fourier transform of The Fourier transform of insided regions in the 2D case is no longer applicable. The Fourier transform works so well because you have this symmetry, this rotational angular symmetry around these n-sided regions in the 2D case. This is no longer applicable simply due to a lack of symmetry. The second thing, the injectivity, this is another crucial property of the characteristic map has to be checked. And also, here are extensions. And also, here I expect and see your problems. Okay, and then finally, we have to establish conditions for the straining probability of the principal curvatures. This is what we need by IGA. I think if we have accomplished that, this would be not so complicated. Okay, the message is subdivision is promising as a tool for IGA. As a tool for IGA in 3D, but there's a long list of things to do before we can base that on solid ground. There are many challenges. Okay, now singular parametrization. This is the following. These obvious problems with this mismatch of smooth parametric lines. Smooth parametric lines, which are lines, which if you want to avoid the concept of geometric continuity, at this mismatch at the central point is resolved when you say that the partial derivatives, which define the direction of these directed lines, if they vanish at the central point. So a vanishing, so zero partial derivatives cannot cause inconsistencies. Zero equals zero Zero, this is easy. But the problem is, if these partial derivatives vanish, then the cross product is zero. And if you normalize it, you cannot do that. So there is no a priori well-defined normal vector at the central spot. And we need that. This is a necessary condition for our constructions. So there is the notion also of the patches and the general patches. These are And the general patches, these are now I consider this in terms of Bernstein polynomials. So I consider the Bernstein PC form, the PB form of n patches sharing the vertex. I call it degenerate if the first partial derivatives all match at one column. I'll show you a picture. So this is a cubic bivariate this e-patch and E-patched and degeneracy means that these four points sitting here, here, here, and here, they all collapse to a single point. This means that the partial derivatives vanish exactly at this point. This is a degenerate patch. And to make this patch a d patch with a well-defined normal vector, you have to guarantee the following: these five points, this fourfold, and this one, and this one, and this one, and this one, and this one, these five points. These five points have to lie in a common plane. Those are six points. Yeah, six. Okay, these six points, actually, there are ten because it's a fourfold point. No, no, nine. Nine. Nine, yes, three, three. No, no, no, it's eight. No, no, it's, I am right. One, two, three, four, five. Two, three, four, five. This one not. Only this. These five points. Five points, and one is fourfold. These points have to lie in a common plate. And this is geometric condition. And then you have a sign condition. This vector here has to lie in the correct half plane. So it has to lie in this picture above this line. And this one has to lie on the right-hand side of this vector. So there are sign conditions. And this is a common pattern for all single. And this is a common pattern for all singular determinant price patches. You have a degeneracy, you have a copularity constraint, and a sign condition. And then does the sign condition extend so the cross product of the two green guys is consistent with the blue ones? Those can switch around. So for instance, this green vector, it could point to the left-hand side. This is not a problem, but it has the line above this line. Uh, this line okay, so these are the conditions, and uh, I skipped the technical conditions. The theorem, this uh, I think that it might be a cheat cases, the theorem says if you satisfy these conditions, then you have a well-defined normal vector. You have a continuous continuation of the field of normal vectors to the central degenerate point, and um. Um, a little bit more. So, showing this is a simple asymptotic analysis, but there's another important point: the existence of a limit for the normal vectors does not imply that you can parametrize your surface patch as graph over the tension. Here's an example of a surface with a well-defined normal vector at this crucial point, but it has a local self-intersection. So the projection So, the projection to the tension plate is not injected. So, you cannot use this for simulation because you cannot write this as a graph. So, the basic requirements are not satisfied. So, the sign condition is responsible for avoiding this case. The sign condition makes it the perfectly smooth G1 surface batch with an injective projection to the tension. Okay, this part of the proof. Okay, um, this part of the proof is less trivial. Oh, yeah, I wanted to show you a few original figures from my PhD thesis, and the following happened. These figures are available in the MTP format. Who knows the MTP format? Nobody. This was a local format in the University of Stuttgart. They used that as well, but of course, this format is no longer support. Then I had PR positives, but they are lost. What is a screen? Hmm, but they are lost. What is a screenshot? In 1993, a screenshot was: you took your camera and you took a photo of the screen. This is a screenshot, yeah, yeah, and then you have video positives, and you went to the conference, and yeah, okay, they are lost. MATLAB's call, source code does not run. So, this is a little story about problems with long-term data preservation. 30 years is a long time for your code. So, for the younger, but for me. So for the younger, for me in 30 years, the problems will not be so important, but for the younger people here, piece of advice, try to save your data in formats which last for long. So no pictures? I thought you saw the problem at the end of the day. There were four point coming and the solution is. Yeah, so of course I could produce new pictures, but I thought it would have been fun to see the old ones. Thought it would have been fun to see the old ones that okay. So, um, these d patches they are sufficiently smooth, they are easy to implement, uniform framework, refinability like subdivision. This is quite a nice property. Curvature is unbounded. We know constructions for C2 patches of that kind, but the conditions are complicated, and chain quality is substandard. So in a So, in a one-variant analogon, this would be a curve of that type. This is simply the graph of x to the three half. Of course, there is a well-defined tangent at this point, but curvature is divergent. You would say, well, it looks like a king, but it isn't. What it is, tension vector. Tension vector. So curvature is unbounded, but in my one year later, I published a paper on that. And in this paper, you find the theorem saying that the principal curvatures of these patches are in L2. This is what we need for simulation. This leads to H2 basis functions. Okay, people started to implement that, even IJA with that, and the numerical results were quite convincing, in 2D, less convincing in 3D. To be less convincing in 3D, but these less convincing results in 3D made me look again at this problem and I analyzed it in more detail and I found the following. This theorem of 1994 is not correct. This is simply not true. Shame on me. This is they are not square integral, they are in L two minus epsilon. There's a tiny little bit of regularity missing. Of regularity missing. This means that the stiffness matrices which you compute here are infinite. But numerical algorithms don't see that because with Gaussian quadrature, you never hit a critical point and you get some reasonable results, you get nice convergence plots, but this is, as I say, this is a numerical illusion. That's the foundation. Okay, so. Okay, so I did the analysis again, now in a more precise manner. And you have to consider improper integrals of that type with singularities of this form. Amazingly, I could not find standard results in the literature concerning convergence of such simple improper integrals of rational functions. I could not find it. Perhaps this has been done in the 19th century. Perhaps this has been done in the 19th century. I don't know how to find that, how to search that. I asked colleagues in the analysis. They knew nothing. So the conditions for the decrepitability of this function here, it's kind of strange. So you have an exponent L, you have the P, or Super Life exponent, and then you have M plus N, these are these exponents here, plus the minimum of M and N. So this is, the world is complicated. Is the world is complicated, yeah? Yeah, and um, it turns out that, as I said, d functions in the channel are C1, but they are not H2. Stiffness matrices have infinite entries, but repair is possible if you impose extra conditions on your control points, partial derivatives, on higher-order partial derivatives. And this is the result. So now I have a complete list of regularity. Regularity. If certain conditions are satisfied, then you get functions in w to p for these exponents. So this is hard work. This is similar by doing all these calculations, and this is what comes out in the end. Yeah. Again, what about the extension to 3D? It seems to be possible, of course. It seems to be possible, of course. So heuristically, it seems to be quite clear what to do, which points should be linear, linearly dependent, or lying in a common three-dimensional subspace. This is quite clear, but the analysis seems to be highly demanding. I know how complicated it was to settle the 2D case and the 3D case is substantially hard. 3D cases substantially higher. So who wants to do that? I don't want to give this to a PhD student. But this would be interesting. And at least it's not hopeless. Okay, so the resume at this point is geometric continuity is, I think, a dead end. Subdivision has a very long way to go. Many big pieces. Many big pieces in the picture are missing. And singular parametrization seems to be the best choice at the moment. So this is probably the way to go. The problems boil down to determining whether certain improper integrals are finite or not. Specify conditions such that synthetic satisfied, and this can be done. So, is this a problem for the analysis? A problem for the analysis, or you also need it in the code to kind of have all these regions for the no, it's a problem for the analysis. Once I settle this, take a look back. Yeah, these conditions are relatively simple. You see, these are simple linear conditions for the implementation. Once you know these conditions, this is easy to implement. But you need to implement all this. No, look here, all I need is this here for P at least. For p at least two, these are the extra conditions, these are the conditions of the patches, they are satisfied always, and only these two conditions they relate these partial derivatives of order 3 and 3.1. Let me go back. They if I read the relation between these Read the relation between these points, this point and this point, and this point and this point. So these points play an important role, and the condition is simple, it is linear, you can implement that. But determining these conditions, this is the challenge. Okay, so yeah, so you will have little improper integrals of such a kind now with x, y. Such a kind with x, y, and z and some denominators determine if this is finite. This is the problem. Okay, um, but one could do that during the next pandemic. Yeah, so yeah, so this was one of the nicer aspects of the pandemic. That ample time to consider things which are okay, no, this is not useful. I used my time to determine. Isn't the time to determine the exact fish number of a cubic piece liabasis? This with dormitics. Yeah. Okay. No, no, tips, kid. Nobody wants it. So the singular parameterization seems to be a conceivable way to do surface patches. To conclude, I want to briefly mention two other types of degeneracy. Types of degeneracy, which could be equally interesting in applications. One is rounded corners. What is a rounded corner? For degenerate patches, for these E patches, we had corners of the patches which were degenerate. So all partials vanish and this causes the degeneracy. Rounded corners have the following. Now you have a full set of 16 different points, but Different points, but so this is now the point at the corner. Um, it lies right between these two control points, and now if you compute the partial derivatives, they are opposite equal. You compute the cross product of partials, and you get zero again, no well-defined vector. In this case, so this is the condition for the degeneracy. The geometric condition is that the red and the blue dots have to. Red and the blue dots have to be coplanar, and there's a sign condition. Um, I can't remember that the sign condition is not so intuitive. This is a linear condition, but it's not saying this point has to line this half back. This is a little bit more complicated, right? But you can get the normal in the corner with L'Hopital's rule at the end of the day, it's a L'Hopital's rule. Not really because this is bivariate. And L'Hopital is one variant thing. But that is in the spirit of L'Hopital. And if you don't satisfy this coconut constraint, you will not get a limit. Then the normal vectors, yeah, they are not, they don't converge to a single direction. And so this is what the So, this is what the copinarity condition is good for. And the sign condition is this here. So, these three inner products have to have equal sign. This is less obvious what this means. Okay, the results are similar. If you satisfy these conditions, then you get C1 that we get. I modify normal vector meaning that C1 basis functions and Basis functions, and I think I skip over. No, I don't skip over this example just to show you how such a patch with rounded corners could look like. You see here the approximation of a hemisphere with a single quad patch. The four corners are now hidden in these points here. You see that something strange is going on at this spot. Yeah. So the zip pitch station. Yeah, so it's the C infinity should be C infinity. That's the best point. What should be C infinity? The north portal should be C infinity. Oh, yeah, the north portal C infinity, because you say C6 as well. No, this is not a single polynomial patch that used splines of firewall. This is where this is the limitation constraint. But you could make it C10 if you want. But these are the crucial points. So here we satisfy conditions and. We satisfy conditions and this works well. And if you don't satisfy the coponarity constraints, this hotspot looks like that. And if you constrain it suitably, you get a nice calamorphic friction line. So this works. And also the approximation properties are as expected. I skip that. Here is a more advanced example of a redfender. Fene Mi Mabusik needed that for tools for unprinting print noise patches. He and Tom News and others encountered these singularities for their algorithm and we discussed what had to be added to their algorithm to make the result at least G1. Okay, the principal curvatures are bounded. No, no, they are not. They are unbounded. No, they are not, they are unbounded, but they are in L3 minus xi. So hopefully, this is true. And that's okay. So this is this is mind area, really. It's mind area. It's very, yeah. Okay, last type of singularity: edge collapse. This is when all these. Collapse. This is when all these four points defining this edge of the patch collapse. Then you have a the image is now a three-sided patch. One complete edge has shrunk to a point. Here you have a coordinarity constraint, this point and the four blue ones, they have to lie in a common plane. And here the sign condition is as follows. If you look at it in a plane, these vectors have to. These vectors have to rotate in the uniform direction. So it's not allowed to go from here to here and then go back with the angle of this edge here. Roughly speaking, this is the sign condition. Just recently, a student of mine finished his master thesis on that. And so this seems to work. Again, well, the found mobile vector. In this case, the Yeah, in this case, the principal curvatures are bounded, so this is amazing. This is, so to say, the worst type of singularity, but this is completely uncritical with respect to curvature. It's nice. Okay, to conclude. Spline spaces for structured text meshes, but in the case, still offer a big challenge. So this work has to be done. So, this work has to be done in that direction. Geometric continuity, as I said, seems to be impractical. Subdivision needs algorithms, I believe we can do that, and thorough analysis. This is a long way to go. Singular parameterization is my area. I repeat that. Yeah. We should consider alternatives. IgA is not a priori the method. A priori, the method of choice. There are certainly cases where IGA is the best that you can do, without any doubt. But isogenomic analysis does not solve the hex meshing problem. This big promise, we bridge the gap between CAD and finite elements. This promise could not be kept. Why? CAD is boundary representation. In cut systems, you don't model volumes, you model, you model models. Volumes, you model, you model surfaces, and these surfaces bound volumes, and you use these lines to do that. Isogeomatic analysis uses these lines to parametrize the volume which is bounded by these surfaces. And if you want to do that, you have to solve the hex meshing problem and say, well, dear colleagues from the design department, please change your methods to design carbon. Methods to design car bodies, please do it in a volumetric way in the future. This will not happen. So, this is not bridging for gap when you say, please come to my site of the report. Yeah, so perhaps this was a bit polling, but this is all. No, I think IgA is great and it's the best that you can do in many situations, but it's the relation to CAP is more subtle than it is. Cut is more subtle than it is. Okay, consider alternatives. I could talk about that for another hour, but I think there are promising alternatives, but not today. The topic and a general remark, the open and unbiased. So let's always look if there are alternatives, if there's something better. And this is how we proceed. Thank you. Thanks, Henri. Questions for the general batch, in the approach, you organize your piece here where you derive some conditions to, I don't know. I don't know. Anyway, it's not before. So, this condition to ensure the regularity. But does this condition affect the approximation that you get at the end? This is a good question. I don't know. But in the plot that you show for the rounding corners, it does not. I see. We investigated that. So this is open. So, this is this is open. We never investigated the approximation power of the general patches. So, the experiments suggested that it's good. So, what I believe what is going on, if you approximate without obeying these conditions, the solver tries to bring the suggested solution as close as possible to these conditions. Because if this conditions violated, then This condition is violated, then you get high norms for the gradients. And this, so perhaps this is what is going on: that implicitly this condition is satisfied better and better. We have to, this is also a missing piece, we have to investigate the approximation power of the gym catches. But my answer would always be, well, adaptive refinements possible. Refinement is possible. Don't expect one size fits all. But if the approximation power is not good enough, at this point, simply refine it until you are done. So I think this is less critical than other issues. But yeah, this is not a problem. John? Yeah, so you outlined three major avenues of attack here, but isn't there maybe a fourth one? There may be a fourth one. And the fifth spawn. Yeah, yeah. But the fourth one that immediately comes to mind is you're having this issue because of the need to deal with something more than rectangular networks, right? You've got singular vertices and all that. So, but you could potentially deal with that problem by sticking with regular. With regular and trying something like additive schwartz. Right? For instance, that and other methods. I have an endless PhD project in my department. This guy is working with regular grids. You simply embed your body in a huge regular grid, and then you start to work using collocation methods. This seems to have many. That this seems to have many advantages that the project is not finished, and I didn't want to report on that okay today because this offers only a few challenges concerning spline methodology, the problems, the challenges are in a completely different area, error estimations and so on. But yeah, there are many possibilities. There are things like the good old wetlands and whatnot, and every shoemaker has some suggestions. Shoemaker has some suggestions. So the world of the speaker then had functions in IGA. Yeah, okay, thanks. More questions for Audrey? Regarding the point of approximation powers, that's an open question for singular parametrizations, but it's also an open question for subdivision, right? So that also needs to be well understood. So from that point, To be well understood, so from that point, it's a plus point for the planer GK constructions because there are some results. I agree that practically the utility seems unclear, especially when you go to 3D. But yeah, okay. Yeah, so many problems to be solved, and yeah, that's good. Yeah, yeah. And could you also comment about Comment about a sixth approach, weak continuities. I've never dealt with that. Okay, so you mean discontinuous caliber, things like weak continuity, weakly imposed continuity. So not strong, strongly coupled patches, but weakly coupled batches. Can't say anything about that. Okay. Final questions for are there any questions online? Are there any questions online? No, I don't see anything in the chat. Okay. Okay, good. Then let's thank Ulrich again.