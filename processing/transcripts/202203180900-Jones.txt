Thank you to the organizers, Rames, for inviting me. So, I'm going to be talking about, as my title says, open quantum systems. And I'm going to be giving two examples of open quantum systems treated with near-term quantum computers. One is two varieties of the Hubbard model, and the second one is radical pairs in solution. I'll explain what radical pairs are. In solution, I'll explain what radical pairs are, and which is a naturally open bottom system. So, first, the case for current machines, finding it, of course, an impactful use case is a top priority. Let me interrupt very quickly and point something out. I see a gray rectangle on the right-hand side. Does everybody else see it? And if so, See it, and if so, is that intended? Yes, I know. You know what? That's people's heads. Let me move everybody's heads. Oh, perfect. Thank you. Yes, it is. And then I guess they didn't want to have a picture within a picture. Yeah. So now your heads and mine are all moved to the side. And so now we can look at the talk from a different viewpoint. Yes. It's good that you mentioned this. It could have covered up. It could have covered up some slides because not all people have a margin. Thank you for, yeah, it's good that you said something. Yeah, so what we would like to propose, my colleagues and myself, is to consider dissipative systems as use cases for current quantum computing systems. Quantum computing systems, because in the cases when there is a unique attractor, the system always is being pushed toward the correct answer, and so errors are mitigated for free. Because as these wiggly lines can show, even though that there can be errors in the quantum computer, if there's a steady state that the system is heading toward, in time. Toward in time, they will always head toward that, even with deviations. And I'll give an example of this. So, why is it important and interesting to look at open quantum systems, especially why driven quantum systems, which the Hubbard model that I'm going to be giving example of driven systems? Well, here are two examples of problems where looking at driven systems could be. Looking at driven systems could be important. Anything with, for instance, a driven system being an electric or a magnetic field. For instance, lasers interacting with magnetism, interacting with controlling magnetism in the first example. And then the second example, we have memoristic phase switching in two-dimensional crystals. And so all of these are examples of. These are examples of phase transitions or changes in matter caused by dissipation or heat. So this first example is the Hubbard models at open quantum system. I'm going to be talking about two examples where the Hubbard model is experiencing a heat bath and driving fields. Driving fields. And my colleagues in this first part of the work are shown below and are at Georgetown University within IBM and North Carolina State University. So the Hubbard model. So I'm sure many people are familiar with it, but if there are some watching who are not familiar with what the Hubbard model is, so the Hubbard model has So the Hubbard model has two terms. It has a term, and it is on a lattice. And so you imagine that on this lattice, there are electrons that hop and they have an energy gamma in this image. They can hop left or right in this one-dimensional picture. And then on site, there is a repulsion U for when two electrons When two electrons end up on the same site. Now, by Pauli exclusion, they can't be both up anyway. So there's a quantum repulsion to not have up, up, but they can have up-down. And in the cases where there is up-down, then there is a U. And so this Hubbard model has just two terms, a T and a U. And of course, if you can imagine if U becomes very large in this model. Large in this model. The correlations, the electrons feel like they can't hop anywhere without encountering another electron or the electron density gets too large. But for half-filled, if you gets to be too large, that restricts the electrons motion. And you can see that that would tend toward an insulating state. And then in the other extreme, And then, in the other extreme, where the hopping becomes very large, that is metallic-like. And that's one of the aspects of the Hubbard model: in a certain regime, it can go metal insulator transition. It's a very important model. So, what we're going to add to this Hubbard model is that we're going to drive the electrons. So, instead of just having the electrons hop, we're going to have an applied electric field. Electric field to drive the electrons in a certain direction. And we're also going to put our lattice in a bath. And our coupling couples not just one of the sites, as some applications of a bath do, but we're going to couple each site in our lattice. So it is the whole problem that is immersed in a bath. And our first example of a Of a um driven, may I just interrupt and say something quickly? Yeah, I just saw a bunch of people join, so it's perfect timing. So, this slide is really the first problem Barbara is going to talk about. So, you see the couplings and everything. So, the full problem formulation. So, I'm just saying this for the audience so they can follow better. Thank you. Previously, there was motivation. Thanks, Barbara. That's great. Please, please, Chris. Yeah. So, the first problem on the So, the first problem I'm going to be looking at is we're going to set u equal to zero. So, we're in the metallic limit. And we're going to look at half fill. That's that mu equal to zero. We're going to look at the half-filled case. And we're going to have an electric field. So we're just going to have heat, the bath, and the driver electric field. So we're going to have an infinite periodic potential in 1D, one electron per site, coupled to a bath. Coupled to a bath, and it's an effective one-body problem in case space. And so, this is what our Hamiltonian looks like. We have a system Hamiltonian that is the hopping Hamiltonian. You can see I here is the index that runs over sites, and gamma is the hopping strength. And then we have omega for the electric field strength. Then we have the bath, which is just viewed as like a set of states, and then the couples from the electrons to the bath with a strength G. And we make, so how may I ask what is D C's recreation analyzation and what are? C is where creation analyzation, and what are D's? D's are the modes of the bath. So they can be thought of as like bosons, as it were, or they're the modes of the bath that couple to the, you can think of it as maybe some harmonic oscillators or phonons or something that is like the bath, that has modes and that couples. Has modes and that couples to the conductor like clocks. C's are electron creation and annihilation. Yes. And the D's are the electron, the creation and annihilation operators of the batch. So I'll go through some basics of how open quantum systems are often treated. Are often treated. Open quantum systems, what is an open quantum system? It interacts with its environment. And so here we have in red what would be what I was talking about, the Hubbard model. And here it's interacting with its environment, which here it's shown as a physical system. Of course, when you have driving, then you have not just in space the coupling, but the coupling with a field. And this is going to And this is going to cause a non-unitary system evolution. And so, to handle this, then one has a master equation and left with a set of Krauss-Lindblad operators. And then these Krauss-Lindblad operators govern the time evolution of the system. So, So, of course, to explain the derivation of the master equation and the Lindblad and the Krauss operators would take, you know, some good, and all the caveats would take several slides, and we wouldn't have as much time to talk about results. So, what I would like to point out is, among other very good resources, actually. Actually, the wikiuniversity.org has some very good explanation. I look to see if there's something that could be easily digested and isn't like a whole textbook or something or course on this. But that recommended link up there, among others, there may be others that people prefer. So what is the master equation? What is the master equation? Well, I'll just have this one slide and just explain it briefly. The master equation is a differential equation formula for open quantum systems. And in deriving this, we make some very common approximations. And these are very typical for the master equation for getting the Lindblad master equation. And they are these three that the bath itself. The bath itself, the eon there for that first condition, is environment, that the coupling to the environment is such that the environment is constant. So that, I mean, obviously, when you have an Oginkwano system and that system interacts with the bath, the bath heats the system, it's true, but in this interaction, the But in this interaction, the system interacts on the environment as well. And so, if you just had a finite environment that changed with, it would change with time as well. And so we are treating this as an actual temperature, which this temperature does not change as it heats the environment. We're thinking of the bath as being very We're thinking of the path as being very large. So the environment is time independent and it has negligible memory. And then we also make the secular approximation, so-called secular approximation RWA, which basically means that we're not going to allow really fast oscillatory solutions to our equations. To our equations. And so we're going to look at steady states that they don't have to be constant in time. Certainly, yes, but they're not going to be super fast. So we're going to put like a filter on this, this RWA. So these three approximations give rise to the so-called Lindblad master equation, which I show below. And it has, in particular, and this jump. And this jump and no jump is not my terms. They're typically what is referred to for this system. So they basically say that you have a rate in the interaction of the system with the environment, and that the system can either do what's called a, in this case, a jump or no jump. In other words, there is a probability for the system in every step, there's a probability for the Step: There's a probability for the system for every operator to either interact with the environment or not interact with the environment. And the equation ends up looking like this with the one half in front for counting. And then there is a term, which is, of course, just the system. And this is the Lindblad master equation. And then And then, how does one evolve these Krauss operators that are easy, straightforward to derive from the Limbladian to turn it into operators on the quantum computer? Here is the general form. So, if you look at the bottom of this slide, each of these idealized circuit components are labeled, and this will correspond to the And this will correspond to the quantum circuit I'm about to show. So that we have the evolution of the Hamiltonian, we have the map to the Krauss, we have the Krauss evolution. And you can see on the left, we have the bits associated with the environment and others with the system. And so you have the Krauss operators in the system. And then at the end, you map back. And then you And then you very important is that there is a reset, and this resets the bath to be its original temperature again. And in our case, the bath temperature is denoted, the constant bath temperature by all zeros. And so we do a reset at the end. And so this is one trotter step that I'm showing. And so we reset the bath at the end of each one because, as I discussed, that's Each one because, as I discussed, that is our approximation. It's very important that we have a real bath that is the same at each time. So then for our problem, which is the so-called non-interacting limit, of course, it's not completely free electrons. They're interacting with the bath. This mu equal to zero just means half filled. So we have a number of electrons that's equal to the number of sites. And we have an in. Sites and we have an infinite 1D ring here. And we're going to put on the electric field, which is the system and bath details are then encoded in the theta and the phi of T. And then the system is evolved. And then the system is reset. And these data Zen PIs are just the Kraus operator. Are just the Krauss operators. And then one can simplify this. It's easy to show that then this is then rotations as a function of the difference between the two. And this circuit is more efficient to operate. And so now I will show you our results of operating the system. So if you look at D, which is in the Which is in the lower left, you will see that same circuit that I just showed a minute ago. We have the Limbladian Krauss operators that act, and then we have a bath, which is reset. And of course, these runs could not have been done on hardware without the reset being implemented by, they couldn't be. By they couldn't be run on IBM systems without the reset being implemented, and that's one thing nice about the IBM systems to have this reset. And so let's first look at the top line, which is in blue. And you see that we are able to, with essentially no error, take 1,000 Trotter steps. This involves 2,000 CX gates. CX gates. So 2,000 of these two qubit gates, and we are still oscillating without a decay. We have truly reached a steady state. This is on hardware. This was run on Mumbai. And then, yes, there is a tiny amount of error correction that we need to do. So the actual runs prior to error correction. Actual runs prior to error correction shown in that the state where it has the essentially the green background with the bits of pink. And what is happening here is that it reaches one well enough because one is actually zero of the qubits. It's how we have mapped what the zero and the one are. And so, of course, So, of course, the system relaxes to zero well, and so we do not have to error correct in that direction, but in the other direction we do. And the way that we do this error correction, that you can see that the results are quite good even without the error correction. But with the error correction, then the way that we do the error correction is a type of Richardson extrapolation where Extrapolation, where the way that we, the way Richardson extrapolation is done, and I should say error correction, this is error mitigation. It's very important to distinguish between the two. This is not error correction. This is error mitigation. It's done after the data is taken. And so, this error mitigation that we do is extrapolate Richardson extrapolation and the way that we Extrapolation and the way that we get additional error, Richardson extrapolation is done by increasing the error and then extrapolating back to zero error. And the way that we zero noise and then the way that we increase the error is actually by increasing the number of reset gates. Because it turns out that the rest of the circuit is so fast, runs so fast that the reset itself is the vast majority of the time it takes to run. Time it takes to run this circuit. And so the time to do the reset is mostly where the qubit is relaxing, if it is. So as we increase the number of reset gates, we're actually increasing the value because, of course, and then the other thing to say is that, of course, the reset gate itself is Gate itself has some slight error in it. And so you have both an advantage of increasing the number of reset gates to really put it into the zero state, but there's also the disadvantage because the time it takes to run the reset gate is actually where the qubit decays. So we do a type of Richardson extrapolation, and I'll show you here. And I'll show you here where we actually sorry for being somehow slow, but I'm trying to figure out whether I understand the pictures which you let me go back here and no apologies ever needed for a question. All questions are good questions. Yes, so the question is: so you're So, you're having a Lindbladian evolution, and I presume somehow the bath is such that it relaxes towards a thermal state. So it's a Davis generator or well, I'm not. Sorry, so now I get to apologize. So I'm not familiar with this term Davis generator. Okay, that's starting. So the question is, if he somehow So, the question is: somehow, in your Lindbladian, is there a unique stationary state to which I'm relaxing? Yes. That's the thermal state. It's not given, but it turns out that yes, the relaxation is occurring. If you see the row just below. Row just below the top row of blue and the area in white at the far left. If you look at the data, this is the time to relax. So there's actually a relaxation that is occurring during this step. And then the steady state are these oscillations. So it's relaxing first for a period of time and then. A period of time and then it reaches its steady state, and then we're showing it stays in its steady state. First of all, it goes to the steady state, it doesn't go to some other intermediate states first and hang around there looking for its steady state. All right. And I'm trying to figure out what kind of steady state do I expect? Ah, yes. So that is, thank you for this question, because in fact, Question because, in fact, I forgot to mention an important thing about this model that is keyed in by these block oscillations. This model, and let me go back one to, let's see. Ah, I can, I can, okay. This model is periodic in both time and space. And so when you have models. When you have models like this that are periodic in both time and space, you see rise to these so-called foca block oscillations. Yeah, okay. It's an explicitly time-dependent problem. I mean, you have a you have a flokey system in the end, yes, and that's why you see these bloh oscillations. Okay, good. That makes me sort of, yeah, I was. Yeah, I was very good question. Thanks. That makes it at least somehow it explains a bit more the pictures. It's a super question because you pointed out that I missed pointing out some of the important physics of the results here. Yes, this is the periodicity of the system. Yeah, as you see, I've written here block oscillations. That's what these oscillations are: they're block oscillations. So the Oscillations. So the electrons, this is the electron density as a function of time, and it's oscillating back and forth. The density is oscillating back and forth. And then we do these modeling of, I just have this one circuit to model the error mitigation that we do. Because reset gates have both good and bad qualities, the adding resets is not. Adding resets is not a simple each reset does increasingly more noise. In fact, we do a modeling of the effect of reset gates and both their improvement and their noise. And you can see here that these four screens are modeling with one reset gate, two reset gates, three reset gates, and four reset gates. Reset gates and four reset gates. And so our error model is in red in each case. And so the ideal circuit in this formulation would look like this as we expand it. It would look like the black. So it isn't like perfectly symmetric oscillations. You can see there's some wiggle. That's what the black. See, there's some wiggle. That's what the black is, that's the exact result. And then the hardware result on the quantum computer looks like the blue for one, two, three, four, and four reset gates. And you can see overall, actually, that as the number of reset gates increases, that there is That there is effectively an increasing error, but that it's a little bit non-trivial on how the error goes in the course of the curve. Like if you think of that as one period of one of these oscillations. And so we do this model, and you can see the model fits the data quite well. And then, since then, we have a model of error as a function of. Error as a function of time and as a function of reset gate, we can then do what is the error as a function of time extrapolating back to zero reset gates. And we can do that analytically and then apply that to the data and then correct for the slight deviation that we do see in that straightforward way and get. Forward way and get the good plots that you see. And so, what can we do with these block floquet oscillations? Well, we can integrate them. So, we can calculate a property. So we can calculate, for example, the DC current that results in such a system when driving it with a DC field. So, this is a plot of on the x-axis. Of on the x-axis of the DC current applied and then the resulting DC field. And you can see in the inset the equation, one just takes the density as a function of time that we plotted on the previous page. The strength of the DC field is the omega, and then you multiply. Omega, and then you multiply it by this sine k plus omega t, and you do the double integral over both time and k, where the k is brought in with the sine function. And this is how the DC field is calculated as a function of applied omega. And then we can get this plot all the way out to very high field strengths, which requires, if you look at the Which requires, if you look at the equation, very long number of oscillations in order to do the, because as the field strength increases, the oscillations increase. And so to get accuracy in the integral, you have to have many of the oscillations and we go far out. And so physically, what's happening here, of course, as you increase the DC field, the current increases, but then it reaches maximum. Then it reaches maximum, and then it starts to, and then it decreases. And so, how can we understand this? Then we can look here what is causing this maximum and this decay of the data. And we can look at plots of the transient current, that is the J of time, that is the thing being the ultimate. That is the thing being ultimately being integrated there, the inner product, the inner integral. And then, if we look, we can see with the curves as a function of t over tau at different points in this curve. So, you can see, for example, with the green where it's already decayed quite a bit, you can see the green oscillation. The green oscillations, right? And then red, a little bit higher up on the curve, you can see the red oscillations, and you can see that they're actually a little less. You can see way down in the curve where it's really fall down the black and the oscillations are much more pronounced. But then let's now look at the peak and what is happening at the peak to make this the peak current. And you can see, in fact, that that's the purple. Fact that that's the purple and the or blue periwinkle, I suppose you would call that color that the oscillations are in fact, there's one desultory one right at small values of t over tau, and then it dies right down. And so then one can see, in fact, that the decreasing current for a larger driving field is because the oscillations as a function of time. As a function of time, are oscillating so much that the oscillations cancel. So, for example, just as if you integrate e to the ikt over all t, you just get a delta function and no sine or cosine as the oscillations go to infinity. Here, it's a similar thing that you get a maximum, you get a zero, and everything cancels out if the oscillations are too much, and you get a maximum. So, this is what we see. Get a maximum. So, this is what we see here with the increase to the maximum. So, here we can see that even with this result of the quantum computer, that the ideal circuit is the Limblad and then the quantum data fall really pretty close to each other. The accuracy is actually very good. The second problem I'd like to talk about is the opposite extreme. We just finished talking about a We just finished talking about a 1D problem, which is an infinite 1D system with no U. And now we're going to put the U in, and we're going to have interacting electrons in an orbital. And what we're going to do, though, is we're only going to have one site, but we're going to allow there to be a fluctuating number of electrons on that site with the U. That site with the U. And so we can have four different states. We can have no electrons on the site. We can have one electron on the site, which will cost an energy mu. We can have it up or down. And then if we have two electrons of the site, of course, we can't have two up. We can have an up or a down. And then that would have an energy u plus two mu. And then we. And then we, in addition, have to the chemical potential and the on-site Coulomb repulsion, we're going to put in an external magnetic field so that our effective Hamiltonian, even without the temperature, without the temperature, looks like the Hamiltonian I show in the lower right. So we have the So we have the one-site hubber model with the U, and then an applied magnetic field. And this is all coupled then to a heat bath. So we have a strongly correlated system coupled to a heat bath. And so, how would one code this one up? Well, there are these four states. And so, in theory, one would have transitions between Everyone would have transitions between all of the states. However, if you look to go from down to up, you would have to go, it's actually a second order process in this because we do not have something that actually flips the electrons in one state. So you have to either go through the zero or the doubly occupied to get to the up. And so we're going to discard the second order that crossed through the. The second order that crossed through the middle. And as an approximation, just look at the first order terms, and you'll see that there is some very interesting results even from this. And then when we do detailed balance, then we can put values for these terms. And so we're going to have this minimal cycle and can put the values for the coupling because of detailed balance and not having. Of detailed balance and not having the electrons, the probability buildup at one place. And so, this is the four states that we're going to transition between in the presence of the magnetic field and the heat bath. And this is what we get. So, if you look at this plot, figure B in the upper right, you will see these four states. These four states that I just showed you with this diamond shape and the transitions between them, where now each of the transitions has a color associated with it. And then on the upper left, which is figure A, you can see the quantum circuit. And it looks, I hope, very much like this idealized circuit that I showed before with the Krauss operators. And here, Operators, and here they're spelled out with each color corresponding to the operation that is shown on the right. And then the heat bath are the upper two qubits on top. And then again, as usual, we have a reset at the end. And so this is our quantum circuit. And then we run this to reach steady state. And here you can see in these the four. These, the four plots underneath the quantum circuit, the density calculations, the probability calculations for getting an up in the state, getting a down, getting no electrons, or getting doubly occupied. Now, if you think about in a magnetic field, of course, the most likely state is the up state because that's the direction that the Direction that the magnetic field is pointed here. But in fact, even that state is C, the upper left, but you can see that, in fact, even there, for this kind of theoretically most populated state, we get per steady state no more than even slightly less. Even slightly less than a probability of one-half to be occupied. So you can see that there are some fluctuations at the beginning as it reaches. So these are the time steps as it reaches steady state, and then it settles down into its steady state. And so you can see that because of basically the thermal fluctuations, the population of up is still less than about half. Is still less than about half that the population of down has a significant probability. Even the probability to have no electrons has a probability of around one-fifth. And then lowest probability because of the U, the Coulomb repulsion, are the W-occupied states. And here we do. And here we do in GH and I to show the probability distribution is really just even on the hardware is confined to these four states. We can show the tomography, as it were, of the population of each of the cross-states, and you can see it is a Is especially at state 19 quite low. You can look at step three, and there's a little bit, you know, there's some oscillations. And then you can see as it goes through step 10 and then step 19, that it's getting to the overlap between the quantum data and the ideal is. Ideal is increased all the way there. So at early times, the fluctuations are very clear, but we reach steady state. And so you can see again here, we're able to calculate an actual physical value for this property. So this is the first example with the two examples of the Hubbard model. And so just the thoughts on this section of the talk, that successful generation. That successful demonstrations of two algorithmic limits of the Hubbard model, non-interacting, 1D lattice, and strongly correlated, single site in a magnetic field. The results are quite robust, and this is, of course, run on the hardware and exciting results so far of 1,000 trotter steps and the block oscillations, and the promise for more complicated Hubbard variants to come. And it does seem, in fact, that dissipative systems where the relaxed Systems where the relaxation occurs faster than the qubit can decohere are naturally error mitigation. So, in the fairly small amount of time left, I'm going to now go to another, a chemical system that's naturally dissipative. And this is a different system entirely. This is now chemistry suddenly. And we're going to be talking about singlet to triplet oscillations in a radical pair in a strong magnetic. In a radical pair and a strong magnetic field. So, what happens here? Imagine that you have two a whole you have a solution, so it's a liquid, and there's two types of molecules in this liquid, and they're moving around, it's finite temperature, and they're really in an open quantum system. And now suddenly, you zap this field with a big blast of radiation. These are done in Russia, where they have, you know, big blasts of. You know, big blasts of during good times when they have these blasts of radiation, right? Zoop. And through the path of the radiation, the solution becomes ionized, highly ionized. And in the path of the solution, this highly ionized solution interacts with the molecules of the two types that we have in the solution, and it causes them to either lose or gain electron. Lose or gain electron and develop a spin. And then an applied magnetic field, these oscillate between singlet and triplet. And then as time goes on, of course, because this zap is only through a column of material in time, then this relaxes. So then this would, without hyperfine coupling and without coupling, this would just oscillate. These are these two models. These are these two molecules that we use, and the radical pair just means that they develop a spin because they have lost or gained an electron. And this is what the experimental data looks like. And so here we're going to compare not just to theory, but actual experimental data in the wet lab. And so there are experiments that we have to actually do these calculations and what they, these experiments. And what they these experiments are the so-called time-resolved magnetic field effect. And what they plot in this plot here is a ratio of high magnetic field to low magnetic field recombination kinetics. And then we're going to model this. So here we have this experimental data, and we're going to try to model it on a quantum computer. And we're going to see if the decay that we get is the same as the decay that the experiments get. That the experiments get. We tried three types of noise modeling: modeled noise with Qiskit air, Krauss operators that we talked about before on the hardware, and then inherent qubit noise on the hardware. And what kind of Krauss operators do we do here? We put in T1 and T2 noise for the Krauss operators. And then we're going to use, as a third case, the actual qubit noise as the decay. qubit noise as the decay. So this is a very simple circuit for this to model each qubit. We represent one of the two electron spins in the pair, and then we can with one electron spin per qubit, but we can optimize the circuit because we only need the singlet population as a function of time. And so here the singlet state is the state zero, zero, and then it interacts. And then it interacts with the magnetic field, and you can see the oscillation. So, with no noise run on the hardware, no noise, and it just oscillates between singlet and triplet. But of course, the experiment is decaying in time. And so, case one is we're going to try to model this with noise from Kisket air. And we, Kisket air has various. We Qiskit Air has various types of noise. Qiskit Air is the module that has the various types of noise, and there are like many types, 10, 12 different types of noise that are modeled in Qiskit Air. And so we tried various types of noise to see what type of noise the actual wet lab is having, and in singly and in pairs. And we found that really only one type of noise best. Only one type of noise best matched the data, which was thermal noise acting on the RZ gate, acting on each qubit. And so this is essentially T1 and T2 noise. And we also put a hyperfine coupling that actually increases our, and this gives a term quadratic in time to the decay. So matching the data with Qiskit Air. Data with Qiskit Air, we get a figure that looks like this. Then we're going to go now to the hardware. So now we're going to look at a second example where we have now, this is the Hamiltonian. The I's are hyperfine coupling, which is coupling to, so not all atomic nuclei. New atomic nuclei develop a spin that can be coupled to, but hydrogen and nitrogen are two such nuclei. And it turns out that the hyperfine coupling, that is the coupling between the spin of the electrons and the spin of the nucleus, normally as a physicist, I would think, you know, whoever looks at the nucleus, but chemists look at nuclei and for a good reason, because in Good reason, because in fact, the size of this hyperfine coupling is comparable to the sizes of the magnetic field that are implied. And so we need to include not just the coupling between the qubits and the magnetic field and between each other, but also with the hyperfine coupling. And so this is, it turns out that there are 24 degrees of freedom because the 24 degrees of freedom because the nitrogen has a nuclear moment of one. And if you add up two electron spins, hyperfine on the hydrogen of one half and the nitrogen with moment one, this is 24 degrees of freedom. So we have a 24 by 24 matrix. But block diagonalizing this into with preserved quantities, such as total spin, block diagonalizes into a max 4 by 4. And this can actually then with And with some challenges, getting a fourth-degree polynomial, you still have an exact solution. And so we exactly diagonalize this and compare to what we get with the quantum computer. Of course, the exact diagonalization only applies to no decay, and then the decay is the quantum computer. So here we have this is the This is the thermal prediction of the experiment without relaxation, it is in the upper right. So you can see because we have these 24 degrees of freedom with the spin, it isn't a simple oscillation anymore, but many multiple periods overlaid and looking different with B equal to zero and B 0.1 Tesla. And then we to compare with the experiment, we calculate this ratio of the low to the high. Calculate this ratio of the low to the high field. And we can see that we can get with T1 and T2, we can do semi-classical. So then compare this with the experiment to see what we might get. And you can see that as typical with semi-classical, it matches reasonably well at the so the x-axis here is time, of course. X-axis here is time, of course. It matches reasonably well at low times and then at high times. Because it's a semi-classical theory, the match to the data decreases, and of course, the data itself has a lot of noise. So now we're going to simulate this on the quantum computer, and then we're going to look at what we get. So the first we're going to leverage inherit qubit noise, and so the circuit is small and simple. Circuit is small and simple. We put in identity gates to do the time evolution. And so, the problem that we have at large magnetic field where T1 is much greater than T2 and mapping to the qubits, which have a T1, that qubits have their T1 time more or less equal to the T2 time. At B equal to zero, in the chemistry experiment, T1 is equal to. The chemistry experiment T1 is equal to T2, so that's better to try to map to the qubits. But at large B, where T1 is much, much greater than T2, we have to implement, time to implement the gates sequence, we have to do some two circuits, as I'll show. And then we also have a problem that the qubits are actually very Qubits are actually very good, and they actually decay slower than the quantum experiment. And so, we have to do an overall scaling of the time as well. And so we do a temperature correction from the essentially zero temperature of the qubits to effectively infinite temperature of the ions. So we have these three corrections: the difference in the T1 and T2, the scaling of the time to match the Time to match the qubit time versus the chemistry time and the temperature. But we have this small and simple circuit, and we have to run, in fact, it turns out, two circuits. And before that, I want to quickly go through, and this is really where we think of quantum information is the error we get. Now, we just have a short side point here. Side point here. What do we get when we run the hardware with these identity gates? What would we expect? Well, first of all, we have two very simple circuits: one circuit where we actually have our oscillation and another one where we're just going to have identity gates, the one on the right. And what should we get? We would have the first circuit, it should have our oscillations and dying down, and the second circuit is just the identity gate, it should just relax eventually to. Eventually, to one in our case, which is zero effectively. But what do we actually get? So, first we try out with the inherent noise that is on the noise model taken from the back end. And the simulator is in general agreement with the theory. And you can see here that some of the runs are not. Some of the runs are not that good compared to the theory, but others of them are not too bad. But you can see, for example, the green one is not good on the left and it's not good on the right either, but you know, some of the others. So we thought, okay, running this on the hardware should actually give fairly good results. But when we ran it on the hardware, we saw this. And we ran it on different hardware, we ran it at different times. We ran it at different times. The noise was not repeatable. And when we looked at individual curves, we saw it looked like that there were spurious oscillations. And so we brought this problem to, we have some in-house experts on NMR at IBM, and we brought this problem to them. And they said, ah, well, why don't you try echo pulses to Try echo pulses to mitigate what look like spurious oscillations in your hardware noise. And so, what is echo pulses? You basically, if you have noise and you have, let's say, two values that two qubits that should be the same, you actually purposely push them apart and they start to process in the mechanical. Process in the magnetic field. And then, if you measure it exactly the right time, half a period before they cross and go around again, if you measure it exactly half the period, then you should actually get rid of the oscillations. And so what one does, if you look here, is add identity gates, and you can see below in the circuits that we're going to have these X. Have these XX gates. And then we're going to put them in, though, at very specific times. You see N over 8, I n over 8, N over 4, N over 4, N over 4, N over 8. And that's this meeting the curve at the other side with the echo pulses to mitigate the error. And we put in all these identity gates and these XX gates. XX gates. And it turns out that this helps to correct the mismatch of T1 and T2, as well as the temperature, so that this has many benefits. And I will show you what it looks like. And then we also compare this with doing Krauss operators. And so with Krauss operators, the A is the circuit for B equal to zero, and B is the circuit for large B. The circuit for large B because you don't have to have amplitude damping with the large B because it just goes to the large B limit. And so after a number of reductions and optimizations, these are the circuits. So you can see these are all very simple circuits. We never have more than three qubits. And we implement these on the hardware. And this is what we get. So this is the run on. The run on the left is the Krauss and the so-called inherit method, where we just let the qubits just decay as they do. And then in the middle, we put in the both all the time, the inherent has the error correction, and you can see with the error mitigation, sorry. Error mitigation, sorry, error mitigation, and with the error mitigation. Well, it's unclear whether we call this error mitigation or error correction, typically error correction, though you use something like a surface quote. So we'll call this error mitigation too, even though it's within the run and not post-processing data. And you can see the very good fit that we get. And we can see that with the case. But with the case one, also, when we go back, the one where we ran it on the simulator, and we do the same thing with the quantum hardware, and we get a very good, an extremely good fit to the experimental data. In fact, if you look at the curve on the right, the fit of the quantum computer, especially at long times, is arguably better fit to the data than the semi-classical theory, which is the blue. The blue is really too. The blue is really too low, and the quantum data is actually a little better in that direction. So, I finish up because my time is up. We simulate noise in three ways. We use Krauss operators, which have the advantage of being highly controllable, and we can construct thermal decay with any parameters we wish, but we need a larger and deeper circuit. We leverage the thermal decay of the qubits themselves to model the thermal decay. Model the thermal decay, and I said the vanished circuits is extremely small and simple, but we're not in full control of the decay parameters, we just let the qubit decay as it does. And we have a method to correct for the mismatch of T1 and T2 values between the chemical system and the qubits. And we mitigate the frequency miscalibration using the echo pulses. And these results can actually be used to model any physical system that has T1 and T2 error, even if they're. And t2 error, even if they are not equal, as in the quantum hardware, particularly the dynamics of any spin system as a time. And we say, can it computer computer quantum computer model noisy experimental data, at least as good as a classical computation, certainly appears? Yes, so I'll stop there.