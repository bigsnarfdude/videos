And in the meantime, I just want to remind everyone that the group photo, which will consist of everyone who wants to turning off their cameras and taking a screenshot, will occur just after the end of this talk. So if you're interested in being a part of that, please stick around. It shouldn't only take a minute or so. And I suppose that will start five minutes. And I suppose that will start five minutes after the original scheduled time. Yeah. Exactly. Okay. So welcome back. It's a pleasure to introduce Michael Weems from Glasgow, who's going to tell us about stability conditions via Teeth's cones, Teeth Cone intersections. Take it away. Michael. Thank you. So I'd like to begin by thanking the organizers for the opportunity to speak. For the opportunity to speak. In these rather strange times, you get to see me speaking to myself over the internet. It's the extent of human progress right in front of you. Good. So today I am going to talk about stability conditions by something called Tietz cone intersections. And so as you most people on this call know fine well, I'm not really a stability manifold sort of person, but the stability conditions. Person. But the stability conditions came up in trying to understand something else, which I'll try to briefly explain. I'll also try to explain in a reasonable level of detail what these T-scone intersections are, some of the phenomenon that occur, what they can do for us. And the puzzle is somehow what has the picture in the front page here got to do with anything. And so, although I probably won't, in the space of an hour, I'll be managing to convince you, you've been studying this picture. Managed to convince you you've been studying this picture all your life, it may be a bit unrealistic. I will try to explain to you, nonetheless, it's fairly natural. And so, this talk is in part joint work with Osamu Iyama and in part joint work with Yuki Hirano. Good. So, the plan of my talk today is there's basically three parts. I have to say a little bit at the beginning about the setting. So, the setting today is threefold flops with certain adjectives associated to them. Adjectives associated to them. And I want to somehow convince you that understanding somehow the movable cone isn't enough from understanding the derived geometry of threefold flops. So a question I think about a lot is like what is birational information and somehow what is not birational information. And so I'm going to start by beginning by talking through the finite story, which is somehow due to Pinkham, how we get the movable cone for flopping contractions, what it looks like, and how Contractions, what it looks like, and how about we homologically go about enhancing that. But if you want to understand actual stability conditions and things on threefolds, you need more. And this is where these Keats-Cohn intersections come in. I'll explain how to construct them. That should be very elementary, but quite fun. And then there is an application, the applications to flops, mutations, and stability conditions. And so basically, part three is a categorification of part two. That's how you should view this. Of part two. And that's how you should view this. You won't go too far wrong. Good. So here's the setting. The setting for today is three-dimensional flops. And so I'm going to just take an arbitrary one, more or less. So I'm allowed three-dimensional space X with some curves in it. The curves are red. I'm allowed to contract those curves to points. And I'm an isomorphism away from that. I'm going to gloss over if I'm complete local or if I'm. Gloss over if I'm complete local or if I'm Zariski local, but more or less, it's not going to matter hugely for what I'm going to say. And if you want, you won't lose much from this talk if you just assume that X is smooth. It doesn't have to be, but for most purposes, you won't lose much if you just assume it's smooth. The types of questions I've been thinking about for quite a while now are things involving classification. So, can we give a reasonable answer? So, can we give a reasonable analytic classification of flops using non-commutative deformation theory and structures associated with that? I'm interested in sort of how to extract invariance from flopping contractions, so curve counting invariance, and obstructions to what type of invariance can arise. So, not all sort of tuples and numbers can arise from flopping contractions. I'm interested in the sort of the drive categories of stability, that's what I'm talking about today, but the wider sort of problem is. But the wider sort of problem is how crazy is the auto-equivalence group for threefolds? And the conclusion of my talk today is it's pretty crazy. It's a sort of order of magnitude more crazy than you might imagine. And you need way more auto-equivalences to generate the auto-equivalence group than you might naively expect. And of course, my actual interest is in non-commutative resolutions because the philosophy is that we understand this. Somehow everything else comes for free. Good. So let me just talk through. So, let me just talk through a little bit about comnotorics of flops. And so, if you've, I mean, this is going back to sort of Pinkham and various other people in the 80s, and Miles Reid and various other people too. To a large extent, threefold flops are really controlled by surfaces and surfaces data. I mean, this is not a new observation. In particular, the combinatorics of them are also controlled by surfaces data. I want to try to explain this. Surfaces data. I want to try to explain this in this slide. And so here is my original three-dimensional space and its flopping contraction down to a point. And so this is all taking place in Barasha geometry of threefold. And so I take a generic hyperplane section of the base. So I slice my space by choosing a generic element. And so that's a spec of R mod G. And that's a surface which is inside of my threefold. Now, if I choose G to be exceptionally silly, Choose G to be exceptionally silly, like the identity, I get the zero ring here. So we're sort of ruling out silly behavior by taking a generic thing. And if you take something generic, you will get down to a surface. And so you take the pullback diagram of this diagram, and you have two curves now lying in a surface. Myles Reed proved back in his early sort of 80s work that the generic hyperplane section is an ADE surface singularity. Is an ADE surface singularity. And so here it's not just an arbitrary surface you're getting, it's a Kleinian singularity. So it's something ADE. And in particular, this partial resolution is a partial Crepin resolution of the ADE singularity. And in particular, it is dominated by the minimal resolution. So here's my artist's impression of what the minimal resolution might be. And so here's your four curves meeting in some Dinkin arrangement. Dinkin arrangement. And so, how do you encode the information of this partial resolution? Well, you look at the full resolution and you remember which curves you keep in going from here to here. Alternatively, you go to this resolution and you remember which curves you contract to go from here to here. And so, just a quick reminder on the chi correspondence: I draw the dual. The chi-correspondence: I draw the geograph of the minimal resolution. So, for each of these curves, I draw a dot. So, for this middle curve, I draw a dot here. For this curve, I draw a dot. For this curve, I draw a dot. And for this curve, I draw a dot. And I connect two dots if the curves intersect. So, it's a default linking diagram. And so, to get from here to here, I have to tell you which curves we are keeping. And so, the curves we are keeping are going to be in red. Are you keeping are going to be in red, and that's going to correspond to keeping the middle curve, which is this one, and the top curve, which is this one, and contracting the curves which are not red. So those two curves have been contracted. And since I don't like Dinkin diagrams that are written sideways, there are my Dinkin diagrams. D4 have some choice of vertices in D4. So what's the upshot here? We started in three folds. We've sliced down to surfaces. We've sliced down to surfaces, we've lost a monumental amount of information by doing that. But the benefit is we've shoehorned in at least some form of Dinkin common attorneys. Whether that's good or bad does a little bit depend on your viewpoint. So just a slight recap on ADE Dinkin diagrams and the setting of what I'm doing. So it's a bit of a delicate question actually. If you have a smooth three-fold flop, what partial resolutions can you slice to? Because you can't slice. Can you slice to? Because you can't slice to an arbitrary one. But if you're Gornstein terminal, you can. And so, really, our setting for today, or a lot of the combinatorics, is you start with any ADE Dinkin diagram you want. And I've just recapped for you here what they are. And so the kind of key point I'm trying to convey is there really isn't that many of them. You have two infinite families and three exceptional cases. They're just, from the viewpoint here, they're just random graphs, some parameters here, but more or less somehow. Here, but more or less somehow finite number, so that's all very classical and known. And so, what the point of today is there's more information and there's a choice of nodes. So, here is my choice of nodes. So, I'm allowed to choose any ADE Dinkin diagram I want. So, let's say I chose type A. I'm allowed to choose any subset of nodes. And the choice of the subset I'm making are the red nodes. And so, here I've chosen these three ones in DN. I've chosen one. In dn I've chosen one, in e6 I've chosen two, in e7 I've chosen three, in the eight I've chosen the four reds. I could choose any subset. That subset could be empty or it could be the whole subset or could be absolutely anything in between. And that's the input to the construction. Right, so how do you get from here to some sort of movable cone or some sort of hyperplane arrangement? So here we are, just to clarify the construction, any choice of ADE dinky diagrams of which Any choice of ADE Dinky diagrams, of which there are not that many, are any choice of nodes, and there's loads and loads of choices of nodes. So there's a little bit more data. Now, to each Dinkin diagram is associated a root system. And so that's a real vector space. So over the real numbers. I like linear algebra with real numbers. And its dimension is the number of nodes, which is this crazy symbol, just the number of nodes. So if you're in Dn, there are N nodes, so you're in R to the N. Nodes, so you're an R to the N. If you're an E6, there are six nodes, and so you're an R6. Hopefully, you see the pattern. And so, the bases are given by the nodes or the duals, depending on your viewpoint, together. And there's this sort of information on root systems together with root-reflecting hyperpods. And so this bit of information, the root system, does not depend on J. It's just intrinsic to your Dinkin diagram. So it's sort of the first part of the data gives you a root system. And the root system doesn't depend on J, which from the viewpoint. Root system doesn't depend on J, which, from the viewpoint of the Osbury geometry later, is a disaster. And so, what we're aiming for is something kind of similar to the root system, which is not the root system, and it won't actually turn out to be a root system, but crucially it depends on J. Good. So here's how we do it. This is where the intersection arrangements come in. So, what have we got? We have a choice of red nodes in an ADE Dinkin diagram. Well, Well, the basis of the root system, well, the root system is based by the nodes. And so, if you have a choice of nodes, you have a subset of the basis. If you have a subset of the basis, you have a subspace spanned by those elements. So, the subspace you get is r to the j, where j is the number of chosen the red nodes. So, just to draw a picture here, if I had two red nodes, If I had two red nodes, I get R2, which looks like a piece of paper. So it's a subspace looked like a piece of paper. And that subspace lives in the root system. And in the root system, there are reflecting hyperplanes, which look a little bit like this. Well, they don't actually, but it's a little bit artistic. So here we have some reflecting hyperplanes inside our root system. And these reflecting hyperplanes slice our chosen subspace. Our chosen subspace. And so they slice it or they intersect it, and it looks like that. And so, what the outcome is, is you get a finite collection of red hyperplanes given by slicing the reflecting hyperplanes or intersecting them with your chosen subspace. And that's within cone J. So I'm going to go through this slide once more because it comes up again in a slightly different context. I just want to clarify exactly. Slightly different context, but I just want to clarify exactly what I'm doing. So, you choose some red nodes inside your Dinkin diagram. Those red nodes give you a subspace. That subspace lives in the root system. The root system has reflecting hyperplanes. You intersect those reflecting hyperplanes with your chosen subspace, and that gives you a finite collection of stuff. Good. And that stuff is a simplicity hyperplane arrangement. It is not, in general, a root system of a, or it's not in general. Of a, or it's not in general even coccetite. Nonetheless, it's very beautiful. Just to make sure here, so red nodes correspond to gray subspace. Red lines correspond. Oh, yeah, sorry. Yeah, so they should that, yeah, they're very good, very good. I like it. So the colour red is not related to the colour reds of the previous slide yet. Sorry, I agree. The previous slide, yet. Sorry, I agree. You're right, you're right. I should have made it. Okay, good. Yeah, but you get some hyperplanes, and it's a little bit difficult to predict a priori what exactly ones you're going to get. And so the theorem of Pinkham, which is early 80s, is that this intersection arrangement, this cone J, is the movable cone of the flopping contraction. So there is a combinatorial construction which gives you this. Which gives you this. And from my viewpoint, later I need much more. I need a categorified version of that. And there is a way of proving this or reproving it by tracking moduli around the flop functors and then decategorify. And so I'll need the more high-tech version later. But for now, we just know it's the new vocal. So let me talk you through. Michael. Yeah. Michael, and can you first explain what that means? What do you mean? What that means, what do you mean by saying that arrangement is the movable column? Oh, so the chamber is very good. Yeah, yeah. So if you take the sort of moduli skyscrapers, that gives you a chamber in, or cohomology or homology, depending how you look at it. And then you can track that through the either the Baraston transformation or the flop functor, and it gives you a different chamber. And so if you start in And so, if you start in one of them and you iteratively flop and you get all your different minimal models and you track that information back to your original one, the chambers that are giving your different minimal models track back to chambers of this hyperplane arrangement. And so, what I mean by movable cone is this chamber between the two red lines is one minimal model. The chamber between these two red lines is a different minimal model obtained by Flop. And this chamber is another minimal model obtained by Flop. Model obtained by FLOP. So, so does it mean that it gives you a decomposition into ample cones of yeah, precisely, precisely, that's exactly right. So, these are the ample cones of all the different minimal models. Or, or sorry, should be careful how we're setting, gives you all the ample cones of the varieties obtained by iteratively flopping. And because I'm local, because I'm local, and this is a reasonable description: if I want some global thing, you're in a world of pain, but today. You're in a world of pain, but today it's just local, and so we win. Thanks. Okay, so here's two examples, and these are the two examples that really sort of in many ways got my interest in the stability conditions part of the story. So this is my favorite because it's the somehow counterexample to everything. If I have D4, which is this diagram here, and I choose the three outer nodes. Choose the three outer nodes. So that's the partial resolution with three curves, which is so you think about the minimal resolution, you contract the middle curve, and you get three curves meeting at a point. It's a partial resolution of a Kleinian singularity. And if I play this construction, this colon J construction, I get the following hyperplane arrangement. It's the cuboctahedron and it has 32 chambers. So let me talk you through that. So this is basically a So, this is basically a cube with its corners locked off. So, here's what would be one side of the cube. Here's another side, and here's another side. And I've lopped off this corner, I've lopped off that corner, that corner, of this corner, etc. And so the number of chambers I get is one, two, three, four times the number of faces, which is six, is 24, and plus eight corners, which is 32. Good, good. So, I do get. Good, good. So I do get 32 chambers. This arrangement is not Coxeter. There is no finite Coxeter group whose reflecting hyperplanes give you this. And so I like this because when you start thinking about stability conditions, you need sort of affine versions of this. And so if your arrangement's not even cocceter, what is your affine version going to be? That's the question. And then the second example I like. And then the second example I like is if I take E8, I chose those three vertices, I get something which is basically a horrific mess. And you're going to have to believe me, but it's got 192 different chambers. And so again, just to sort of to calibrate ourselves, this is a flopping contraction. There's a three-fold flopping contraction with three curves whose movable cone. So there's 32 different mineral models or things you can get from statistically flopping. And this is what. Flopping, and this is where you'd have 192 of them. And the reason I've got this larger example on is it gets pretty crazy pretty quickly. And then we had to start thinking about affine versions of this, and it just gets a little bit crazy. So we did a little bit of theory to tame this mess. And so before we do that, well, maybe, yeah, well, it's kind of the theory is somehow implicit here, but I want to explain a little bit later. And so here's a sort of stereotypical. And so here's a sort of stereotypical type of thing we can prove. This is not a profound result in any way, but just a little bit convenient to sort of benchmark yourself for later. So if you take any choice of nodes in any ADA Dinkin diagram, let's just say we choose two nodes in any Dinkin diagram, then up to changing slopes of lines, which isn't really particularly important, the cone J is one of five options. Either there's six chambers, there's eight chambers, there's ten chambers. There's eight chambers, there's 10 chambers, there's 12, or there's 16. Now, you don't have to be a particular expert in number theory to realize there's a number missing from that sequence. So you just don't get 14, it just doesn't exist. And that's just how it is, just doesn't exist. And so when you go later to the sort of homological algebra and you categorify this stuff, this is telling you the length of braid relations you get from flops. And so if your chamber structure looks like this, at least locally, Chamber structure looks like this, at least locally, you get a length three braid relation. This is going to be length four, this is going to be length five, this is going to be length six, this is going to be length eight. And it's also telling the content is there's no higher numbers. Eight is the largest. But that's the sort of type of stuff we can prove. It's not particularly, this is the finite story isn't particularly, well, in some ways worthwhile. So I want to explain to you what's better. The better way to think about this is to ditch the movable cone completely. Ditch the movable cone completely and just try to get an affine construction of this story. And the affine construction is where the content is, and actually where lots of things become a little bit more beautiful. So just remind yourself, there's not actually that many extended ADE tinker diagrams. The number transpires is the same as the number of ADE tinker diagrams. So we can extend type A by adding in a node, we can extend type D by adding in a node, and we can extend E. And we can extend E7 and E8. Ultimately, we don't have particularly many of them, but there we are. That's it. And now I'm allowing myself a choice of nodes, arbitrary choice of nodes inside an arbitrary extended ADE diagram. And again, there's just more choice because there's more nodes. And so here is some choices of nodes I'm allowed to make. Again, it can be an arbitrary choice, and the choice does not need to include the extended vertex. includes the extended vertex. So here is my an, I've chosen my red three, which happens to coincide to include the extended vertex. Here is dn, I've chosen my ones, but it does not include the extended vertex. Here's some I could choose in E6, here's some in E7, and here's some in E8. Again, I could choose none, I could choose all, I could choose anything in between. And what we want is an infinite hyperplane arrangement that we can extract the finite story from. Extract the finite story from. So, how does that work? And do you want to remember which vertex is the extended one? Do you want to remember which is the extended vertex? For the viewpoint of the algebraic geometry, yes, we're going to always extend, we're always going to include the extended vertex, but for applications of pre-projective algebras and some surfaces, we just want an arbitrary choice. Arbitrary choice. So the answer to your question is a little bit yes, a little bit no. It depends on the setting. But from the algebraic geometry, yes, we're going to slice, we're going to get a finite, we're just going to add something informally. But yeah. Does that answer your question? Kind of. Hopefully. We'll see, I think it's the answer. Yeah. Yeah. So the T-school intersection, so what's the deal? Well, all we're going to do is we're just So, what's the deal? Well, all we're going to do is we're just going to repeat exactly what we did before, but instead of looking at the root system, which is this sort of finite sort of information that you get from an ADE root system, we're just going to play the same game, but on the TETS code, which is the sort of infinite version of the story. So the input now is your favorite extended ADE Dinka diagram, and absolutely any choice of notes. And similar to before, so instead of the root system, which is R to the number of nodes. R to the number of nodes. You have the Tietz cone, which is R to the number of extended nodes, so previously plus one. And it's basically an infinite hyperplane arrangement. It depends a little bit how you define it, or at least the upper half kind of is. And so you still have what kind of should be viewed as reflecting hyperplanes. Now there's infinitely many. But nonetheless, it's the same game. You have a choice of nodes, so you have a subspace. As soon as you have a subspace, As soon as you have a subspace inside something infinite, you can still take intersections and you still get something. And the something we get is called level, level K, and it's just exactly the same story, just harder to draw. And it's involving the choice of notes. And so it's called the level because really the object you get is in R to the number of nodes in K, so in your choice of K. But when you draw But when you draw, typically, when you draw infinite arrangements or T's cones, you slice it and take the level and you draw the slice. That's just what you do. And so this lives in art, this slice or the T's cone or the level, I should say, lives in one dimension more. It hopefully become more clear in the next slide when I actually draw an example. Okay. So here's the finite versus infinite story. So here's E6. Good, we can recognize E6. We can recognize E6. And here is two nodes in E6. Good. And so if you apply the first half of my talk, you get a finite hyperpoint arrangement in R2. So two is the number of nodes. And I've drawn it in reds, although David was objecting. These are the red ones. These are, it's all this. And so what I like about this example actually is if you count the number of chambers, it's one, two, three, four, five, six, seven, eight, nine, ten. Five, six, seven, eight, nine, ten. And there is no affine version of the hyperplane arrangement with 10 chambers, at least classical Caucasus theory. But what instead we do is we take this thing and we just formally stick in the extended vertex and we formally colour it red. And if you apply the previous slides to this, you get an infinite hyperplane arrangement in R3 minus 1, which is 2. And that infinite hyperplane. Which is two, and that infinite hyperplane arrangement looks like that. And so somehow the point is: if I zoom in, I can zoom quality, quality zoom here, and you see the finite story from before. And if you zoom out, you see much more information. And so the point of the slide is you might as well just develop the infinite story because the finite one's going to come for free. The finite one's going to come for free, sort of hanging around in the background. And so, this infinite arrangement is the ones that we get from this. Okay, so there's two things I want to explain. One is the wall crossing rule of how we calculate this, partly because it's got some bizarre geometric corollaries. And secondly, I want to say what type of hyperplane arrangements can arise. And then I'll move on to the actual applications and categorification of it. Categorification of it. Okay. So the question is: how to actually calculate these, right? So I'm just declaring that that's the answer. Michael, Michael, can I ask? So your finite picture, as you explained, encodes ample cones of birational models. And what does this infinite picture encodes? That's coming right at the end. I'll need to explain that later. It's not clear in general. It's not clear in general geometrically what's encoding. This is my last couple of slides, is exactly on that question. I'll come back to that later. Okay, great. And so the question is, first, how to calculate what they are. And so if you want to calculate sort of dinkiny type stuff, you have two methods of calculating infinite hyperplane arrangements. One is to take something finite and start translating it around the plane, and that's going to give you something infinite. Plane, and that's going to give you something infinite. That's sort of method one, which is quite global in some ways. You start with something, you translate it around, win. The second way is you get dumped in a chamber, and from that chamber, you apply local rules, and you sort of gradually build out the local rules, and you get yourself a global picture from that. And the second way is how, in many ways, we actually do this, and this is what I want to explain. And so, the point is that obviously. And so the point is that although these arrangements aren't cocceter, they're labelled by cocceter data. And so every chain will be labeled by a pair. And so usually in the TS colon, they're just labeled by elements in the affine vial group. But now they're not. They're labeled by pairs with some element in the affine vial group in some group. And J is some subset of nodes. And the point is, it's a little bit technical. I don't really want to get into detail, but the point is, if I have a label on one chamber, Have a label on one chamber and a label on another chamber. If these chambers are beside each other, we know exactly how to get from one to the other via some combinatorial prescribed wall crossing rule. And I want to describe visually what this is and then say a little bit about the geometric applications. The rule is a bit technical. Yeah, but we're not doing that. We're going to do the picture instead and explain the geometric applications. Okay, so what is this wall crossing rule? This wall crossing rule. So the number of wall crossings, if I dump you in a chamber at random, because you are a simplicial hyperplane arrangement, the number of walls is the number of red nodes you have. That's because the number of walls is the dimension of the space you're in. So the number of walls is the number of red nodes. And so for every red node, there is a wall crossing, a wall to go through. So the nodes are doing walls. And so here's the rule. And so here's the rule. I'm sorry, at your picture with E6, you had two red nodes and like five walls. Yeah, so here, so here you have three nodes, and then your chamber looks like there's a triangle. And so you have one, two, three walls. Ah, you mean that each chamber has two? So in this picture, the two nodes, you have two walls, this one and this one. And with the three. And with the three, you have you start having the third third. Yeah, thank you. And so here we are. So you cross on the wall, you choose a red node, and the rule is really crazy. You temporarily delete all the other red nodes, you apply something called a dink evolution, and then you put back in the deleted vertices. So it's not something you'd necessarily wake up in the morning and think about, but let's just play it an example. And so here is E8. E8 with five chosen red notes. And let's just say I want to wall cross this vertex, so the wall corresponding to this vertex. So what I do, I apply the rule. So I temporarily delete all other red vertices. And so that one, that one, that one, and that one get deleted. So I'm left with A2 cross A2. I apply the Dinkin involution. So in type A, that's just a reflection. A, that's just a reflection. In type E, sometimes it changes the danglers. In type E6, it's a reflection, and it's trivial otherwise. But here, the dink involution is just that. So I apply it, that moves the red nodes, and then I stick in all the other deleted vertices. I get that. So this is a pretty crazy rule, but this is the one that underpins all the combinatorics. And if you translate it into geometry, it's really quite strange. And so if you have some. And so, if you have some flopping contraction and you slice it back and you get a partial resolution of a Kleinian singularity, now you flop one of the curves in the threefolds and you slice that new space you get. You also get a partial resolution of the same kind and singularity. But those partial resolutions are not the same in general. And this what I've explained on this slide is the rule that determines how you get between the Kleinian singularities and how it relates to flops. Singularities and how it relates to flops. So, again, you have a threefold flopping contraction, you slice it to a partial resolution. You flop one of the curves on the threefold, and you slice it to a different surface. And the surfaces you're getting are related by these wall-crossing rules. And what if the Dinkin diagram has no involution, like E7? And then you don't nothing, so nothing moves. All minimal models then would have. All minimal models then would have um yeah, yeah, yeah, so sometimes it moves, sometimes it doesn't, uh, and this is the rule that determines, yeah. So here's a theorem. I'm about to get to Alspace geometry soon, I promise. This is quite elementary geometry, but I love it. And so here's, I just love this theorem. So you can ask a very elementary question. I have a two-curve flop. What the hell can happen? It's basically the question. It's basically the question. And so a two-curve flop would be the finite storage, so I had to add in one node, and that would be three nodes inside an affine Dinkin diagram. And up to changing slopes of some of the hyperplanes, which again doesn't really matter exactly how we're drawing the picture, we're only going to care about the topology later anyway. This level K, this infinite hyperplane arrangement, is one of 16 possibilities. It's these 16. And I want to talk you through a little bit. I want to talk you through a little bit about this slide. I just love it because it's quite elementary, but it's also quite beautiful. The first thing I want to say is that four of these are already known. So if you do Dinkin stuff, you'll recognize this and this are both B2. It's not immediately clear, but this is actually the same hyperplane arrangement as this. It's just this little dots. It's just there's little dots. There's an actual, there's a Z2 lattice here and here that are different, but as abstract hyperplane arrangements, they are actually the same, slightly tilted, but they are the same. So that's just B2 or affine B2. This arrangement here is affine G2, and this arrangement here is affine A2. And so there's either sixteen or fifteen hyperplane arrangements, depending if you count those the same or not. But regardless, there are twelve other hyperplane arrangements on here that are new. Hyperplane arrangements on here that are new. And why I'm showing you this slide is that it can get pretty crazy. So if you look in this line here, I mean, this is affine G2. And the ones on this line, I mean, they're not, the other ones aren't affine G2, but they look pretty similar. And their behavior is pretty similar overall. But when you start going down to this line, and certainly the behavior around about here, it is pretty radically different. And you can get And you can just see as a human things in these hyperplane arranges by looking out. And so Sasha asked before, somehow, where's the ample cone gone? And you can kind of see the ample cone as being these sort of big circles, essentially. But you get this whole other crazy stuff in between them. And the question is: what's birational, what's not, and how to describe them geometrically. To describe them geometrically. Okay, so that's the end of the sort of combinatoric stuff. I'm now going to categorify this and explain what this has got to do with the Barasso geometry of threefolds derived category stability conditions and things like this. And so to do this, if I'm claiming that this is controlling the stability manifolds, I have to explain to you how you get T structures and how they would relate to this hyperplane arrangement. And that's where. And that's where non-commutative resolutions come in. But just to clarify the setting, for every three-fold flop X corresponding terminal or X smooth if you want to, you obtain a Dinkin diagram and a subset of nodes from the AD Dinkin diagram. And as before, we can just formally stick in this extra vertex, colour it reds, and we get an infinite version of the story. And so just set notation for later, this left one is a finite height. This left one is a finite hyperplane arrangement, which is going to be H, and the right one's an infinite arrangement, which is going to be HF. So you should think of F as being affine, even though these aren't actually affine in the sort of technical sense of the word. You just think of them as being infinite. Good. And so this is where non-commutative resolutions saves us as it always does. So we have R, which is the base of our flop. So it's just a commutative ring. Flop. So it's just a commutative ring, and we're going to just reduce all this to sort of rings and modules. So for the finite story, so the movable cone story, I want to look at all those modules, which have three properties. They are cone-Macaulay, so they have some x vanishing against the ring property. They are rigid, and so there are no x to 1 from the module to itself. And they are maximal with respect to that property. So if I try to make m any Property. So if I try to make M any larger by adding in another module and I remain rigid, that module I just added in actually belongs to the summands of M already. So I can only extend M and make it larger in a silly way, like by adding it to itself. And so in the lingo, this is known as maximal from the last condition rigid objects in the category of coin-macaulay models. So it's going to turn out, although it's not in any way obvious, that these Uh, that these there's only going to be finitely many of these for C D D singularities, so for basis of flops. And so, if you want to go to the infinite story and explain the T structures of the infinite thing, you need more modules. And so, how do you get more modules? And so, you just relax some of the conditions. So, the first thing is that you're in dimension three. You're in dimension three, and so the number two is pretty close to being the number three. And so, reflexive modules have got depth two, co-Macaulay modules have got depth three. And so, we stop studying co-Macaulay modules and go to the more general class of reflexive modules. So, Cohn-Macaulay's are a subset of this. So, we want to look at all those modules that are reflexive, and so they're double-duals themselves. They are modifying, so the endomorphism ring is self-Cohen-Macaulay. Is self-Cohen Macaulay. That's somehow the kind of correct interpretation of rigidity. And they are maximal with respect to that property. And so in the lingo, these are the maximal modifying modules. And they're basically the building blocks of non-commutative resolutions and their variance. And by variance, I mean it depends on the setting. If you're aiming for something smooth, you aim for non-commutative prefunt resolutions. If you're aiming for something singular, you're not. You're aiming for something which is singular. So it depends on the setting, but So it depends on the setting, but basically, these types of modules are the building blocks, irrespective of the setting. Good. And so here's the theorem, which we're just, this is sort of step one in the categorification. So we have a hyperplane arrangement, and before we want to do anything fancy, we want to make sure that the bijection, so the sizes of everything are the same. And so we have our flopping contraction, arbitrary threefold, etc. We obtain We obtain this cognitorics by slicing, and we get a finite arrangement and an infinite arrangement, hf. And so, what's the point? The point is that these maximal rigid objects in Kromo-Collier world, they are in bijection with chambers of the finite arrangement. In particular, there are only finitely many of them. Then, the maximal modifying objects, or depending a little bit on Objects, or depending a little bit on, I should say X is probably smooth here. The maximum modifying objects of the slightly larger class, they are in bijection with chambers of the infinite hyperplane arrangement. And granted, even though there are infinitely many of them, they are completely, completely classified in this setting. And this is quite remarkable because it's basically the only setting I know where you can completely classify the modules giving you non-communist resolutions. And so I want to draw you this picture. And so, I want to draw you this picture because it's related to my very first slide. And so, what do we have? We have an infinite hyperplane arrangement that looks like this. And so, the theorem says that the modules giving you NCCRs or their variants are in bijection with the chambers. So, for each chamber, there's a corresponding NCCR. So, let me draw you a dot in each chamber. And so I get that picture. And so, the dots are percent. Picture. And so the dots are precisely representing the modules giving you non-commutative Grepant resolutions. Now, on non-commutative Grepant resolutions, there is an operation called mutation. And the whole point of this picture is that the idea of wall crossing is going to correspond to mutation. So if I draw a line between two dots, if and only if the dots are in chambers that are next to each other, I get the following picture. I get the following picture, which is in the lingo as the exchange graph of the modules or the mutations of the modules giving NCCRs. And as soon as you draw the exchange graph on top of the hyperplane arrangement, it's really quite remarkable how much extra structure you can see on this dual picture compared to the one you can before. And again, in a few slides later, these big circles are going to be the movable cone, and the sort of flurry stuff. Cone and the sort of flurry stuff in between is just going to be non-commutative information that's going to correspond to modules, T structures corresponding to modules on these NCCRs. I said that the bottom here to have such highly regular structure in the exchange graph of NCCRs is very unusual. That's putting it mildly, basically if you look at the exchange graph of NCCRs in general, it's an unmitigated mess. But here it's extremely regular. But here it's extremely regular and actually very nice. And so that's just a bijection. Yep. Michael, and can I ask? I mean, this graph definitely has some translation symmetries. What do they correspond to? So if you go up to there, that's corresponding to tensor. So this is a two-curve-flop, and so the Picard group is Z2. So one of the generators, once you The gen one of the generators, uh, well, it depends. There's lots of generators of Z2, right? Um, but tension by O10 takes you to there, and tension by O01 takes you up to there. Well, it depends actually, it depends. There's many options, depending where you are. But basically, tensoring by line bundles is corresponding to sort of the Z2 translations of this picture. And the spherical twists are going to correspond to monodromy-running hyperplanes. Okay, thanks. Great. So I'm going to categorify this now. And so instead of having just dots and bijections and sort of very sort of combinatorial information, I want to make this categorical. And so how do I do that? Well, this is, at least on the hyperplane stuff, this is very well known. And so you want to lift this to a functorial sort of property of a groupoid. So this is known as the Deline or the So, this is known as the Deline or the arrangement group point. And so, I take my hyperplane arrangement, which will look something like this, roughly, and I'm going to draw again a dot in each of the chambers. And so that should look like that. And now, instead of drawing a single line between two dots, if they're beside each other, I draw an arrow from one dot to the other if the chain. One dot to the other, if the chambers are adjacent. And so if I do that, I get that picture there. So just zoom in a little bit. So I have these two dots are adjacent, so there's an arrow from this one to this one, and these two are still adjacent, so there's an arrow in the other direction. And so I just get lots and lots of arrows and things between them. And so this generates me a monoid, or a category, I should say. It feels like groupoid is the wrong word. Anyway, I get a category. Category and so I had to identify shortest paths. So, what does that mean? And so, if I go to here, if I start in this chamber and I want to go to this chamber, the shortest way of getting there is one, two, three, four, five. There are five wall crossings gets me from here to here. But there are similarly, there's five wall crossings: one, two, three, four, five. So I have two ways of getting from this chamber to this chamber, namely go this way. Namely, go this way or go this way. And I have to put in the equivalence relation that identifies those paths. So this is a huge infinite arrangement. And I put on whenever I see two paths between one chamber and another that are the shortest, like the shortest possible way, I had to identify them. That gives me the Delaney group point. I formally invert things and I get a group point. It's called the Delaney group point. It's quite a formal object, actually. It's quite a formal object, actually, and it's a sort of combinatorial way of accessing the topology of the arrangement. Good. So I'm running low on time, but I'm good. I've only got three slides left. Perfect. There's another way to build a group point. So the M's in my theorem are in bijection with chambers. So for each chamber, I put the derived category of modules of that module. Endomorphism ring. Say that again. Say that again. So, chambers are in bijection with certain modules. So, for each chamber, I can put the derived categories of modules on the associated endomorphism ring. It's a legitimate thing to do. And because two adjacent chambers, the modules are rated by mutation, there's a God-given auto-equivalence that takes you from, no, sorry, God-given equivalence from one to the other. And so each wall crossing has a mutation auto-equivalence. Auto equivalence. And the theorem is that there's a functor from the Deline groupoid to this sort of crazy groupoid that you just constructed using non-communist resolutions. So, I mean, it's very abstract, but we don't really care about that theorem, actually. We care about the crawlery. And the crawlery is that pi 1, essentially associated to the affine hyperplane arrangement, is acting on the derived category of x. We should view this as the affine. Should view this as the affine pure breakgroup action for the flops. Even although the affine down technically, affine pure breakgroup down technically doesn't exist, you should somehow still need that. So again, to calibrate your cells, you have X and the drive kind of X, the sort of flop functors are acting on this category. This is a finite and tiny, tiny piece of this much, much larger group action. So, I'll try to draw you a little bit, the picture in my last slide. And so that's finding, well, that's telling you about auto-equivalences, which is nice. But if you want to take it to one level higher in stability conditions, that's how this all glues together and to get one particularly nice statement. The question is, do the T structures and the auto-equivalences come together nicely? It's fine well describing T structures and describing auto-equivalences, but you somehow have to put that information together to get Together to get tilting and thus stability manifold. So inside the derived category of X, again, X is an arbitrary threefold flopping contraction. Could be smooth if you want it to be, could be gaussian terminal, it's up to you. You have these two subcategories. One is this we call C, which are all those objects whose derived push forward is zero. And then we have the category D, which are all those objects supported on the exceptional curves. The exceptional curves. So C is inside D, so C is smaller than D. And the philosophy coming from Bridgland's Klein and Singularities paper is that C should be the finite type category and D should be the sort of the infinite thing. And so D is a somehow should be viewed as a local model for a projective Clabier threefold. So D is finite-dimensional homespaces and it's Clabio if you're smooth, right? If you're smooth, for example, or suitably interpreted otherwise. And the main theorem is that stability conditions on these two categories is completely controlled by this combinatorial information of these hyperplane arrangements. And so you've got your flopping contractions X to spec R, you associate the finite and the infinite things exactly as before. And it's not even like a complicated map. It's the forgetful map that takes stability conditions, the central chart. Stability conditions, the central charge of the Boolean category, and just forgets about the Boolean category. It's a pretty easy map, and it takes a certain component and stability conditions on C to this finite thing, the complexified complement of the hyperplane arrangement in the lingo. And stability conditions, normalized stability conditions, just because we're taking the level, normalized stability conditions, at least a component of it, get sent down again to the complexified complement. And the theorem is that these are regular. And the theorem is that these are regular covering maps. And because we happen to know the group action is faithful and various nice things happen, the first one is universal. And conjecturally, if you believe stability manifolds should be contractible, the second one should be universal as well. And just to flag into you, if you believe that, then you'll have to prove K pi1 in the greatest level of generality known. So K pi1 is known for So, Kπ1 is known for affine Artin breakgroups now, but it's not known in the generality of intersection arrangements of this talk. And so if you manage to prove the stability manifold is contractible, in particular, you've got a brand new proof of K pi 1 in the greatest generality that I've given. Which is nice, but it shows you it might be hard. So, okay, so I'm just going to wrap up on the last slide with a picture. And the question comes back again to what comes back again to what is this picture geometrically what is birational and what is not and so again as you say the auto comes to the last line are deck transformations which is what you'd expect because what else would you get apart from the pi one of them good so here's the conclusion and the conclusion is your t structures that you need to describe stability conditions are not just coming from like coherent sheaves in the movable cone like shifted around or by perverse sheaves shifted around. By perverse sheaves shifted around by line bundles, you need a bucket load of other information. And so the pictures I'm drawing are for two curve flops just because they're visually beautiful and I'm giving a Zoom talk. But you can actually already see this phenomenon for single curve flops, which are actually the hardest. And so if you believe, that's my main theorem in my last slide, the stability manifolds are a covering of this, in particular, for each of these chambers, you'll have infinitely many T structures above it. Infinitely many T structures above it. And the question is: what are those T structures? And the answer is, of course, they're just modules on your NCCRs, sort of translated around by a tilting or by auto-equivalences. And so you should view this picture as in each of these triangles, each of the chambers, is just modules on some ring, which is great. That's a very nice description, but it just doesn't necessarily have a birational description. I have a birational description. And so I draw the movable cone in green. And so here's my movable cone. So there's one, two, three, four. There's 16 chambers here in my movable cone. And I have my Z2 action, my Picard group or my class group, depending on the generality. I can move it around. But you see just how much more information there is in the stability manifold in addition to the movable cone. And in particular, if you're interested in the auto-equivalence group, that was maybe two slides ago. Maybe two slides ago, it's saying to you, you know, the flop-flop functors are just the auto-equivalences within the green thing, right? And the tensor-by-line bundles is somehow translating them around. And you can see how many other monodromies you need to generate the auto-equivalence group. I mean, you have to go come into this sort of area here. And so the kind of conclusion is the auto-equivalence group is just like orders of magnitude greater than you expect. And if you go down to a single curve flop on the threefold, Curve flop on the threefold, what other equivalences do you have? Well, you have the flop-flop functor, which is a spherical twist suitably interpreted, and you have tension by O1, and you have shift. And you can ask, well, what more do I need? And the answer is, well, for one curved flops, you might need up to 13 more auto-equivalences. And so, you know, it's quite a lot more complicated. But the non-commutative resolutions give you a sort of way into tackle this problem. Cool. I should stop there. Thank you very much. Thank you. Thank you. All right. Well, let's thank Michael. And do we have questions? Can I ask, so you explained that your pictures correspond to Dinkin diagrams with some markings, right? Yes, yep. But on the other hand, But on the other hand, the same Dinkin diagrams with markings correspond to homogeneous varieties. Well, they do, yeah. The question is, is there some relation between these two objects? The honest answer is we don't know. When I got into this, and same with the Yammer, we thought surely someone. Same with the Yamma. We thought surely somebody studied this before, right? I mean, these things, surely we're not the first people in the world to be drawing these pictures. But we've asked loads of people and we've just not seen it. I mean, there's a couple of papers where we know about the representation theory literature and the mass physics literature who do this, but yeah, I don't know. And so it could well be related, but I just don't know. It's, yeah. What are these structures controlling? So, what are these structures controlling? Something like geometry of the nilpotent cone doesn't come into the picture. Yeah, but they're more encoding the partial resolutions of the nilpotent cone, right? So, usually, when you look at the nonpotent cone, you look at the Springer resolution. And what I'm saying is that Kleinian singularities, at least, what these things are doing, I mean, they also can also explain my whole talk in surfaces. And really, this is the information of the what's happening on the partial resolutions. On the partial resolutions, and most people just study the resolutions as opposed to the partial resolutions, which is maybe why they haven't been sort of seen before. Yeah, that's very interesting. Thanks a lot for the great talk and for the very beautiful pictures. Do we have other questions? Do we have other questions? Hey, Michael. Hey. Hi, Michael. Thanks. Yeah, good. So am I right in saying this picture with the exchange graph, you only have NCCRs, so you're only seeing smooth things? Or is this saying that you have? Yeah, so if you're smooth, it's exactly the NCCRs. If you're not smooth, it's just the graph you get by just iteratively mutating your your object that you start with. Your object that you start with. And so, if you're on the NCCR world, smooth, all NCCRs are, you can mutate between them. Great. If you're not smooth, you just have to start with what you have. And there's not a particular word for this, right? It's just the exchange thing you get from your object that you're left with. Yeah, yeah, okay. You have a lease and obviously partial case. Okay, yeah. Thanks. Any other questions? All right. Well, if there's no other questions, let's thank Michael again for that interesting talk. Thank you. All right. So.