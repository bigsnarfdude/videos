For extremes and maybe processes in some kind of unified framework. And well, the talk is mostly geared for people in extremes, I guess, but also people in causality should hopefully profit. I'll talk about structural equation models, actually two type of those models. And for people in climate, I can only say that hopefully in future some of the ideas maybe can be taken to that domain, but for now. That domain, but for now, it's quite a theoretical work. I'll try to keep on very much on the idea level without going into any proofs or any kind of technical issues. And I should tell that it's a joint work with Sebastian Kirsting, whom most of you will know, and also my PG student, Jakob Tostersen from Aarhus University. And his part will come at the very end of this talk. Doesn't switch. Yes. So graphical, probabilistic graphical models. Again, in this community, I think I don't need to introduce these things too much. So these are modular posimonious models of multivariate distributions. And essentially, so you have this random vector y, you have a graph over those four nodes, suppose, and this graph is incurred. Graph is encodes condition of independence relations for the distribution of y. So, here, as an example, you can take node y2. So, this node separates y1 from y3, for example, and therefore there should be a conditional independence of y1 and y3 given y2. So, that's all very classical books written on this topic. But what is important here to note is that there is quite a bit of abstract theory. There is quite a bit of abstract theory. So, this condition independent sign can be treated in an abstract way. So, you will see later four basic properties, semi-carphoid properties. If you just call them axioms, then you can develop quite a bit of theory. And therefore, also, this sign is not just a conditional independence relation, it can be something different, and also the y. And also, the y does not need to be a random vector, it can be a multivariate stochastic process, it can be all kinds of different objects, and in this talk it will be an infinite measure which explodes at the origin. But you'll see in a moment what I'm talking about. So, these alternative notions of conditional independence have been explored already in the literature. Here is a non-exhaustive list of some of those works. Those works. Just let me mention they have been applied to time series, gradient causality, marked point processes, quantiles, and multivariate paradigm distributions. So this work by Sebastian and Edgar in particular, I would like to emphasize, because our notion, well, it grew out of their notion and it's more general. So the stuff you will see now, it's a more general version of conditional independence. Of conditional independence, which well is compared to the notion of Adrian of Sebastian Adrian. Right, so this is quite a condensed slide. I'll try to explain what's here. It's like setting up notation and trying to explain what is this measure which will use for constructing for defining conditional independence statements. So, I would like to look. I would like to look at two basic examples. There are more examples, but these are the two basic ones. I'll talk about Max infinitely divisible random vector X and infinitely divisible random vector X. So infinite divisibility simply means that you can write the distribution as, well, in the max case, as the maximum of NIID random vectors. And in the sum case, you can write as a sum of NIID random vectors. There's a sum of niid random vectors for each m, right? That's important for each m so in particular, max infinitely divisible, it's something more general. Well, it includes max stable distributions and infinitely divisible includes stable distributions or some stable distributions. Another interesting point here, well, maybe I should mention that in Max in the divisible case, we restrict to the positive orthant, and there is also a slight And there is also a slight assumption on the support. So the left endpoint of support for each component is at zero, but that's just a technical here. And I would also like to talk a bit about the corresponding processes. So for either, for each of these, max infinite divisible and for infinite divisible, you can associate. Well, there is a corresponding process. In the max case, it's called extermal process. Well, constructed by Reznik and Rubinovich. And Rubinovich in 73, so a long time ago. And infinite divisibility is even more classical notion, and there is a Levy process corresponding to that. So in particular, a Levy process is a process with stationary and independent increments. An example would be take an independent Brunian motion with compound Poisson, but that's somehow a very simplistic example. There is much more. The jump behavior can be much more subtle. So there are stable processes. So, there are stable processes, temperate stable, and whatnot. And these things are extremely popular in finance and in insurance math, but also in other fields. Right, so essentially we get back to this max infinitely divisible vector by just considering the process, the respective process at time one. But also, you can go the other way around, given an infinitely divisible max or some random vector, you can construct the associated process. Construct the associated process. So there is a one-to-one correspondence here. What is important for this talk is that both of these things are essentially characterized by the so-called exponent measure lambda in the max infinite divisible case, max ID case, and it's called Leby measure in the ID case. Right, so most of you know this formula over here that you can Formula over here that you can write the joint CDF as just exponential of this minus lambda applied to the positive orthant with this box defined by x subtracted, right? And in the Leby case, it's a bit more subtle. So usually, well, you actually need another few parameters. There is so-called drift. It's not exactly drift, but people call it like that. There is a sigma matrix that's equivariance. Is a sigma matrix that's a covariance matrix of the Browning component, and then there is this jump measure. So, this thing is much better understood actually if you start from this underlying object, which we have seen already in various talks. Anthony presented the Poisson-Point process in the context of extremes, but it's just the same thing. In both cases, you start from the Poisson-Point process, dt times lambda dx. So it's a product with dt. With dt of lambda, and what happens then? You to retrieve the extremal process, you essentially take running maxima in time over each component. And to get a LAVID process, you essentially take sums, but those can explode, so you have to compensate. You take sums of those jumps in time in each component, and then you have to compensate appropriately to have a process in the end. Right, so there is this basic. So, there is this basic object, and depending if you take sums or maxima, you arrive either at max ID or ID vectors, and of course, also associated processes. Right. Yes, I spent quite some time on this basic thing, but just wanted to emphasize this lambda measure is very natural. Well, it just characterizes both max ID and ID random vectors essentially. Random vectors essentially. The properties of lambda, well, lambda is finite for sets A bounded away from the origin. That's very important. But in general, it explodes at the origin. So those non-explosive cases, they're kind of boundary cases. They're not often interesting. So the interesting case is when the measure lambda explodes. I should also mention that if you have homogeneity of the lambda measure, Homogeneity of the lambda measure, we've seen that before in the maximum domain, right? In extremes, you get actually max stable random vector. And if you go to the ID case, then you get alpha-stable random vector. So the idea of this work is to construct a graphical model for lambda. We don't work on the level of those in the classical sense, in the classical conditioning independent sense, but we construct a new notion. That we construct a new notion on the measure lambda. And the purpose of this talk is just to go through this construction. Some of you have seen already from some previous talks. And even more importantly, I want to emphasize that this notion is very natural from various perspectives. And I hope I will manage to persuade you in that. An important note here is that there is this nice negative result by Jonis and Kirsten, who showed. And Kirsten, who showed that you cannot actually have non-trivial condition independence statements for Max stable models with density. So you're really forced to actually go into this way. All right. So if there are any questions, I can answer right away. Otherwise, I go to the definition of conditional. To the definition of conditional independence. So let's take a partition of the index set ABC, and then what we do is we look at the sets of product form, R, such that they are bounded away from the origin and that they have, they're charged by lambda. So the mass of lambda is positive there. You have the schematic picture. It does not need to be a box, but it's just a Not need to be a box, but it's just a product form set. And now, as soon as we have that one, we know that lambda is finite because this thing is bounded away from the zero. And it's natural just to, when you restrict the measure lambda to this rectangle r of this product form, right, you can scale it appropriately to get a probability measure. So, this P R will be a probability measure corresponding to lambda restricted to this product form set R. Now, as soon as you have this. Now, as soon as you have this probability measure, we can use the classical independence to define our notion of independence. So we say that the set of indices A is independent of B given C in our new notion, well, for the measure lambda. If there is classical conditional independence of YA from YB given YC for any essentially distribution arising from this. Distribution arising from these rectangles and lambda, just like I explained before. So, once again, to check that you have this new notion of conditional dependence, you take all those possible product form sets which are charged, bounded away from zero, and you check if the corresponding law decomposes, if it has the classical notion of conditional dependence. So, this is more or less what Sebastian and Edega try to do. Sebastian and Elga tried to do, but they didn't formulate in this way. So they worked with homogeneous lambda with continuous density. And first of all, homogeneity allows to have a very special form of those test sets, right? It's a very simple test set where you just look away from, well, one component is larger than one. So these are like half spaces, but in general, this is not sufficient. This is not sufficient. So, this homogeneity is nice for some problems, but it's not really necessary for the theory and for many results. So, and also I should mention this work by Jean and Van. I believe it came as a discussion on the paper of Sebastian Edgeman, and they suggest conditional inner independence for probability laws supported by non-product form sets. So, they didn't talk about infinite explosive methods. About internet explosive measures. They just talked about non-product form sets and went for kind of a similar approach. All right. Now a few, well, a boring slide about notation. So I'll need also sometimes to look at the marginal measure lambda d restrict to the components in the set D. Also, I'll need to look at this restricted measure. So essentially, you set your Essentially, you set your components in, well, not in D to zero. So if you have the positive orthon, then essentially you look at certain subfaces, right? And people in extremes, they know that those subfaces are important for various things, especially if you look at concomitant extremes. And then there are two assumptions which feature throughout this work. First of all, is this explosiveness assumption? Is this explosiveness assumption? So lambda of when you exclude the hyperplane passing through the origin, right, lambda of the rest is either null or infinite. So it just says that lambda must explode in all directions. That's the assumption. And if you want to interpret in terms of Max ID distributions, then it says that each component has no mass at zero. And in the labor context, it says that each component process has infinite jump. Component process has infinite jump activity, so it's not a compound poisson, so it's not this trivial example of a Levy process. And sometimes we need to go a bit further and assume just the same assumption, but for all those subphase measures, lambda 0d. It's not that strict if you think about it. And if for example, if you go to the homogeneous case, to the alpha stability or stability, then it's always satisfied. Both of these assumptions are always satisfied. Both of these assumptions are always satisfied. All right, so now I come to the semi-graphoid properties. But yeah, let me start as it's written here from independence. So it's a very simple characterization of independence for our notion. So if you have this explosiveness assumption A0, which I just covered, then A being independent of B under lambda, in our sense. Under lambda, in our sense, holds if and only if lambda has no mass in this set, which means that either yA is zero or yb is zero, lambda almost should be. That is what it says. And I guess it's again very intuitive to people working with in extremes. So it's essentially what you see when you have asymplotic independence of components in A and components in B. Right, so that's nice. As soon as you have this assumption, you have this kind of support condition for the independence. And then let's quickly go through these four axioms, or let's head maybe I should say semigraphoid properties. Right, the first three are kind of very basic ones, and it's obvious to check that they always hold. It doesn't matter if those assumptions are satisfied or not. Those assumptions are satisfied or not. But L4 is tricky. So we spent quite some time trying to prove L4 and we didn't succeed until we discovered it doesn't hold in general. And it doesn't hold when you don't have explosiveness. So explosiveness assumption is actually vital. It's crucial for this L4. And in a way, it's natural because our notion treats the origin in a very special way. And if there is no explosion, this treatment This treatment, specialized treatment, is not natural anymore. So you can expect all kinds of strange things happening when lambda does not explode. When it does explode, things actually are very natural because you really have to treat this origin in a special way. All right. Yes, I see time is running out quickly, so I mostly skip this slide just to tell you that. Skip this slide just to tell you that the notion is quite natural, and you can actually write a few alternative characterizations. You can go to test classes like appeared in Sebastian's Netrian's work. Okay, they can take epsilon being one because of homogeneity. In general, you cannot. So you just exclude some parts, well, half spaces from, they're not called half spaces, well, some strips from the domain. From the domain, you can go into the density factorization if you can assume that there is a density with respect to a product measure. And if you are not satisfied with that, you can be very general. You can go to kernel factorization. So to lambda, you can also define this probability kernel. And just it's like a conditional distribution with respect to yc, and then the kernel factorizes. The thing is, the crucial thing is that these things are not defined for yc being zero. For YC being zero. So you have to exclude that. But then additionally, you must include this thing in red, which always comes as an additional requirement that you always, so if you look at A independent of B given C, you always need to verify that A is independent of B given this subface measure. So if you restrict to the subface of AB, then there you must have independence. Remember, it simply implies under explosion assumption that there is. Under explosion assumption that there is no mass in the certain interior region. All right, so now I can finally go to graphical models, and I would exclusively talk about directed graphical models now. So we have a directed acyclic graph, and we define this directed global property and directed local Markov property with respect to lambda just in the same way as it is defined in the classical sense, right? In the classical sense, right? The directed global can be defined in two ways: either through deseparation or the moral graph, but it's not important for this talk, just use one of them. And directed local, it's easier to understand. It just says that a note is independent, a node, any node given parents, is independent of the rest, excluding the descendants of V. So you just use the same definitions for our new notion of conditional dependence, and then you arrive to this DG and DL. Then you arrive to this DG and DL properties. The nice thing is that if you have L1, L4, then the classical proof essentially goes through, right? So I didn't say proof of what. So DG and DL actually are equivalent if the conditional independence satisfies L1, L4, like in the classical sense. And so we have seen that under assumption A1, this explosiveness assumption, we have these properties L1, L4. These properties L1L4, therefore, also our notions here they equivalent for lambda. Now, two applications. First concerns Max linear model, the next concerns some linear recursive model. So this is the model we've seen today and the previous days a lot, introduced or well studied by Claudia and with Cauthus. So here, I won't actually go into details, you all know by now what it means. You all know by now what it means. Here, I just say that all those innovations, epsilon, they are independent. They have maybe their own distributions. And we don't assume they're max stable or anything. They're just positive. So you define, you have this recursive equation. And this is a classical structural equation model. And as such, it satisfies classical directed local Markov property. That's just the That's just, yeah, can be seen immediately from this book by Judea Pero. Interestingly, since it, well, it's easy to see that this vector x is max ID because of this construction, therefore it has this measure lambda, as I explained before, and it must satisfy actually DL property in our sense as well. It always satisfies. Furthermore, it satisfies direct global property when all the Direct global property when all those epsilons are strictly positive. And while the proof also shows a certain degeneracy, Claudia mentioned degeneracy, but when you actually look at Lambda measure, things look even more degenerate in the sense that conditioning on the parents identifies the common child. That's how degenerate the models are when you start looking at the conditioning dependence. Looking at the conditional independence in this sense of lambda. All right. A similar model has been studied a few years before the Max Linear model by Misra and Kuruglu. I think it has appeared in the Journal of Machine Learning Research. They also tried to learn the structure and what they assumed that all those innovations are stable and therefore you get alpha stable vector in the end. For us, it's not important stability, so we take Stability: so we take epsilons to be infinitely divisible, then the random vector you get is infinitely divisible, it has measure lambda, and exactly the same result goes through. And actually, the proof is just the same. As soon as you have this lambda, it has the same structure, then you just repeat the proof. There is no need to do anything else. And now, if you want the G property, the global property, then you also need to have a slight assumption on those innovations. So, essentially, the Lebaneme. So, essentially, the Leby measure must explode at zero. So, once again, those things are not compound Poisson. All right. Now, in the last few minutes, I would like to talk about Levy processes, and it's another interesting link to the definition I have presented. So, I'll be very brief, just quickly go through this Levy eto decomposition. So, every Levy process can be written as this independent sum. You have a drift component, you have a Brownian motion. You have a Brownian motion component, and there is a so-called pure jump process, which is described by lambda by this measure lambda. And interestingly, what we discovered is the following, that if you have, so you have, in the pathwise sense, right, you have the process XA is conditionally independent from XB, given the process XC. Once again, it's on the process level, if and only if the If and only if the Brownian motion satisfies the same conditional independence and the jump out satisfies the same conditional independence. And this is extremely peculiar. That usually doesn't hold. And it would not hold if you would go just to marginals, right, of these processes. And the explanation is, so why this happens? Because essentially the path of XC, the LAVI process XC, it actually almost should identifies both components, WC and the jump component, JC. You see, and the jump component, JC. So, somehow, from the Leibniz path, almost surely you can recover the Bronian part and the jump part. And therefore, you have this result. Essentially, now you see that if you want to have models for the Leibniz process in general, you should just have a model for the Brownian part separately and the jump part separately, where the Brownian part is classic, right? And of course, you will use the same graph, underline graph for these two. And as I mentioned, there are problems when you go to marginals. And for example, it is interesting to think when you look at XA being independent of XB given X C on the process level, it does not imply the same thing for the marginals, simply because you lose information when you go to the marginal of the process C. Right, even more interesting is this result. Is this this result? If you have explosiveness assumption, then in fact, this property I just showed on the previous slide, the conditional independence of the jump processes, right? JA independent of J B given J C, it's if and only if we have independence in our sense. So that's what I mean by our independence being natural. It's just the same thing as it turned out to be as conditioning in the sense of paths of Leby processes. Paths of Levy processes. Yes, I think I had some comments. Maybe I should have moved this picture in front to better explain what a Levy process is, but nevertheless, I think the time is out, so I just conclude. So I have presented a unified framework for graphical models for both max ID and ID vectors. This actually extends way further to levy processes, external processes, but you can go also beyond that. And I hope I did present. And I hope I did persuade you that the notion, A, our notion, is a natural one. Thank you.