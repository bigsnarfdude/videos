In a great workshop, actually, so my talk, so the sorry the the main the main part of my talk is based on the uh uh this pro this paper uh by by me, uh Patrick, and Hallard. And also at the end, I will also mention a little bit the result from this paper finished by me, Sarway, Chris Sarway, and uh more lem here. So I'm also benefiting. So, I also benefit from the discussion with Idia and Budemont. So, actually, as the title suggests, I want to talk about signatures and phytricians. So, our motivation to study this topic actually comes from this very simple but fundamental example. So, let's call the process on the left-hand side X. On the left-hand side, x epsilon, and the process on the right-hand side, x. So it's not hard to say that as epsilon tends to zero, the law of x epsilon converge weakly to the law of x. On the other hand, it's also not hard to say. So basically, these two processes, x, epsilon, and x, have completely different information evolution pattern. So indeed, so at the second stage, So at the second stage, we already know everything about x epsilon, right? So which means that, in other words, so the sigma table generated by x epsilon as second stage is already the power side. On the other hand, from the picture, we can also say that actually at the second day, we still know nothing about x, so which means that the sigma algebra generated by x at the second day is trivial. So basically, so in the context of, you know, So, in the context of probability theory, this means that x epsilon and x have completely different filtrations generated by themselves, no matter how small epsilon is. So as a consequence, although x epsilon converges weakly to x, but the value function from the optimal stopping problem defined for x epsilon will not converge to the corresponding value function defined for the limit problem. For the limit process X. Okay, so this simple example actually implies that weak topology, so the Euro weak topology, actually it took off to give the continuity for the value function appeared in many multi-stage optimization problems, such as the optimal storing problem that we observed before. So the reason for this phenomenon is also simple and intuitive. So remember that So remember that, so in order to use the weak topology for the law of stochastic processes, we have to identify a stochastic process X with a path-valued random variable, right? And then you consider the push-forward measure. However, if you do so, you completely ignore the underlying filtration f attached to X, because by doing so, actually, the filtration does not Actually, the filtration does not enter the definition of the law of circuitic process. So then it's not surprising that actually in multistage optimization problem, the weak topology is, I mean, useless because actually in this, in such problem, the filtration really matters. For example, for the optimal sorting problem we just observed before, so a very crucial rule played. A rule played in this optimal stopping problem is the stopping time top, right? However, we all know that the stopping time is a notion related to fluctuation, right? So therefore, if you forget the flattration in the definition of weak topology, or of course, I mean, you don't have any information for the filtration. So basically, so yeah, so the weak topology is not so suitable to give continuity for, I mean, multi-stage optimization problem, right? Optimization problem, right? Okay, so now the task is clear. So we already know that thanks to the definition, the weak convergence has a weakness, which is that it completely ignores the filtration and therefore it cannot reflect the behavior of the information evolution pattern attached to stochastic processes. Therefore, it's not strong enough, right, for studying some multi-stage optimization problem. And our task is. And our task is to find a stronger topology which overcome this weakness, right? Okay, so we adopt the approach initiated by David Ardos. So actually, the basic setup is the following. So as we have already observed, by treating the socket process as a path-value random variable, basically you ignore the filtration. So instead of Filtration. So instead of considering the push-wowed measure, we want to consider this triple. Okay, so here X is a stochastic process, define out some stochastic basis and the filtration F, right, is we require that X is adapted to the filtration F. So we don't require F is nature, filtration generated by X. As long as X is adapted to F, it's okay. And then we have an underlying probability measure. Okay, so we consider a topological. Okay, so we consider the topology defined on the set of all such triples, and we call this triple adapted processes. Okay, so then use the Ardosh idea, which was further developed by Hu and Kissler. So instead of considering the push-forward measure of X, we consider this process. Okay, so this means that actually we consider not only X, but the conditional distribution. But the conditional distribution of X given the filtration of T. Okay, so I should call this process as from definition, we can say that actually this measure value pass, right? Actually, this means that for every T, it's a measure value. So from this definition, we can already say that actually it's so this in this definition, at least the filtration, right, enters, right, interest is the control. Interface the construction of this process. And of course, so because the conditional distribution just gave us how well one can predict the future of X after time T based on the information available at time t or containing F T. So I think it's quite intuitive that such process, I mean, the prediction process looks better than the process X itself. Process X itself when we use it in the multistage optimization problem. Okay, so remember, notice that actually prediction process itself is also a slochatic process. It just takes values in different space rather than the X. So we can also take the prediction process for the prediction process and continue. So basically, this means that in general, we can define for any indicator R, the R. For any indicator R, the R's prediction process, which is just defined as the prediction process of the R minus one prediction process. That's all. Okay, so then this means that actually we have a family of different ranks prediction processes, and we can use all of them to define a new topology. So to be more precise, now we call that two triples, XFP, YG. XFP YJQ have the same rank R or R adaptive distribution if and only if their rank R prediction processes have the same law. So correspondingly, we say that a sequence of triples Xn, Fn, Pn converges to the limit triple in the rank R adapted weak topology if and only if the convergence, the weak convergence happens for their Convergence happens for their rank R prediction processes. Okay. So the definition is simple. So from this definition, we can say that if because we by default we require that x zero, which means that the zeros prediction process of x is x itself. So actually, the rank zero adaptive topology is just the normal weak topology. So for the rank one, basically we consider a prediction process, which is a measure valued martingale. A measure-valued martingale. And by the definition, so the law of the prediction process actually is a measure defined on the measure valued part space. Okay. So then there is theorem proved by David Ardosh and Huber Kessler later. So basically they prove that actually for R larger than S, so in general, the rank R adapted weak totally is essentially stronger than the, I mean, the smaller rank adapted to. The smaller rank adapted topology. Okay, so more interesting fact is actually we can show that if the limit process X has filtration generated by itself and it has continuous trajectories, so then the value function converges to the value function converges if this triple Xn converges to X in the rank one adaptive vector logic. weak topology. So remember that actually this cannot happen in general for the rank zero adaptive topology, which is a normal weak topology. So this confirms that actually the rank one adaptive topology is already stronger than the rank zero and it's more suitable for us to apply in like optimal stopping problem. Okay, so here I just mentioned one more property for adaptive topologies. So if the underlying So, if the underlying time interval is discrete, so then we have the more interesting observation. So, first is if we only consider triples whose filtration is nature, which means that filtration is generated by the process itself, so then you already the rank one adaptive vector is already enough. Okay, so it gives the continuity of the value function for the optimal solving problem for any cost function. So, this is a famous function. So this is a famous result proved by like Bakov and Matthias Begwick and others. So on the other hand, if we drop this requirement, okay, so if we consider general filtration, so then the situation become more complicated. So in this case, basically, you need a higher rank adaptive topology to give the continuity. Okay, so here, if the time interval, this discrete time interval consists of capital N plus consists of capital n plus one time bins so then you need uh capital n minus one sorry capital n uh adaptive weak topology and in general any rank smaller than capital n will not give you the continuity so if you are interested in this result you can check this paper so the recent preference okay so all in all we have seen that actually uh we have some motivation to use uh higher Motivation to use the higher rank adaptive wake topology rather than the normal or zeros wake up topology, right? If we want to study some continuity problem in multi-stage optimization. So now we come to signatures. So first, let's collect some very well-known results regarding application of signature or expected signature in the context of weak topology. I mean the zeros adapted topology. Adapted topology. Okay. So, first, I think everyone knows the definition of signature, so I omit it. So, first, we know that two socket processes have the same law if and only if they have the same expected signatures, right? As long as they are well defined. So, I mean, this is a quite classical result proved by like Ilya Terry and others, or Ilya Harart. So, the second interesting effect is actually. Effect is actually you can choose a hilarious space embedded in the tensor algebra over Rd such that the following metric, which we call it signature MMD or signature one such n metric, so induced by the expected signature. So this metric actually locally characterizes the normal weak topology, or in other words, zeros adaptive topology. Okay, so you can check this result in Ilya and Harat's paper. So use this. So use this metric, we can move further. So actually, we can show that use by using this metric, we can define such kind of reproducing kernel such that the attached RKHS. So it's dense in the space of all continuous functions defined on the law or on the measure of path space to R. Okay, so with respect to the weak ball. So, respect to the weak topology. So, this scheme is called distribution regression. So, proof, I mean, introduced in this paper. So, okay, so now we want to generalize all these three results to high rank adaptive vectorities. In other words, we want to find a signature map such that it also satisfies these three properties, but not only for the rank zero topology, adaptive topology. Topology, adaptive topology. We want it to hold true for every rank, adaptive topology. Okay, so that's our task. So, so here, actually, we remember that actually the higher rank adaptive weak topology is induced by higher rank prediction processes. So, of course, now we want to find signatures maps defined for prediction process or in other words, measure valued paths rather than obligating space-valued paths. leading space value path as long as we can find such a signature map we are done right so for simplicity let's assume that the time interval is discrete so to in order to avoid some discussions on the integrability conditions so then uh basically actually our construction of such kind of uh hierarch signature is quite simple so so remember that i mean for r equal to equal to zero because we have the normal vector j basically you just use The normal vector basically you just use the normal signature is fine. So then let's consider the r equal to one. So basically, we consider the first rank adaptive vector. In this case, so because by the definition, so of the prediction process, it has this form, right? It's a conditional measure on path space. So basically, first in the first step, let's consider the expected signature, but it taken with respect to this. but it taken with respect to this conditional distribution okay so then for every time t we have a h valued random variable right so then if we let time t vary basically we have a h value sochetic process so we know that actually for linear space values process we can still have the classical definition of signature so let's apply the signature notion for this H value stochastic process This H-value stochastic process, which are denoted by this L star x hat one. So for then we obtained a signature map, which takes values in a hilarious space embedded in this tensor algebra over H. So this the target space of normal signature. So this tensor algebra is quite complicated, but anyhow, so it's a linear space. So we call this map the second signature, which can be seen as a signature of signature, right? or signature, right? So by doing so, I mean, if you continue for, I mean, step by step, you can prove or you can find out that actually this contraction holds true actually is valid for every rank. You can use the rank R minus one signature to define the rank R signature. So basically for every rank R, you have a notion of rank R signature. So then we can prove that We can prove that actually this Herring signature does a job. So, for instance, it's a universal feature map defined on this measure value path space. So, as a consequence, for example, we have the following. So, if the two triples have the same rank R adaptive distribution, if and only if they have the same R plus one expected signature. And correspondingly, so the higher rank signature MMD also may try. MMD also may try this the rank R adaptive vector just like the normal signature you I mean so also as a cohorry so using the higher rank signature MMD you can also generalize the distribution regression scheme to higher rank adaptive topology so you also have the dense RKHS and everything so this means that particular in particular we can approximate We can approximate the value function, or I mean, in the context of mathematical finance, so the price of American option by doing a kernel regression on this reproducing kernel induced by our hierarch signatures. So basically, we generalize the previous scheme to the current setup for multi-stage optimization problem, and we call it adaptive distribution regression because we are using adaptive. Regression because we are using adaptive topology. Okay, so actually, basically, that's all. I mean, at the very end, I just want to mention that some algorithm. So, okay, so thanks to the above theorem, we can see that actually having signatures is very useful, right, for machine learning, for example, to solve some optimal solving problem. But from the definition, we can already say that the That the computation of these high-rank signatures will be quite complicated. So, we have to find out some feasible algorithm to implement that in the computer. So, actually, the answer is, of course, we can do. So, for example, so if we assume that X is a Markov, so then we can use the dynamic programming principle to calculate the high-rank signature. So, backwards. So, the interesting thing is here. So the interesting thing is here we use this backward scheme or algorithm simultaneously for the second rank signature and also for the first rank signature, the normal signature. So because of the definition, right, it's a signature of signature. So you have to apply your algorithm for both. So, but anyway, it works out. So you can find out the concrete algorithm in our paper. And here I just show some numerical results. Just remember. A numerical result. Just remember this simple example we have seen at the beginning. So basically, I mean, so this graph shows that if you use a normal signature Rashan metric basically to identify these two processes. So if the epsilon is very small, then basically the success rate is half, right? It's only 50%. But if you use our higher rank signature, our higher rank signature MMD to test then this I mean 100% sure you obtain this correct correct result so it confirms that the the significant advantage to use heroing signature approach when you deal with I mean feratricians okay so on the other hand so maybe some of you already know this I mean very well so basically thanks to the dynamics that by signature you can apply the Signature, you can apply the signature kernel PDE trick to reduce the kernel regression problem to solving a low-dimensional hyperbolic PDE. So this actually this algorithm also valid for our Herrank signature. Okay, so we can show that basically, I mean, to calculate this Herrank signature was a shiny metric, you only need to solve a family of low-dimensional Gaussian PDs, just as in the class. Than the PEs, just as in the classical case, but of course, here you need to, I mean, spend more time, right? Because so the signature now is so basically you saw more PDEs than before. But anyway, so this gave us some very easy or I mean, at least feasible algorithm to really calculate this Herrank signature metric. So, yeah, thank you very much. Thank you. Are you there some questions for any audience?