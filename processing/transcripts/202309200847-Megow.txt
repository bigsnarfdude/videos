So we don't have to argue why we should also develop optimization methods when there's uncertainty in the input data. Real world is dynamic and if you want to get more realistic methods then we should be able to deal with this uncertainty. There are many different models how to express uncertainty in the input data, right? So there are online models, so where in some way incrementally unknown information is revealed over time, one by one. Over time, one by one, or random order, whatever. There are stochastic models, and there are also other models to express uncertainty sets, giving explicit scenarios or describe behavioral uncertainty sets. And the classic frameworks would be online, stochastic, and robust optimization. And these typically, all these frameworks have one thing in common. So there is uncertain input data, and an algorithm has. And an algorithm has to live a bit. It may graduate or not. It may learn, it may observe, but typically it cannot explicitly ask for more precise information. This somehow is not captured by these models. And this is something I want to talk about in this talk. What if we can actually bury uncertain data at a certain cost? And this is not so unrealistic, right? So often there are ways to gain more information. So Information. So it costs time, it costs money, it costs something, but it's typically possible. So, just to give some examples, so let's say you have a facility location problem and you need to know the customer demands, but you don't know, but you could run a market study, you could conduct interviews. It is costly, it takes time, you want to avoid this, but it is in principle possible. Yeah, and there are other ways to explore, like you can probe, like you can run more. You can probe, where you can run more detailed diagnosis, etc. It is clear that if we have more information, so less uncertainty, then we can come up with better solutions. So there is, of course, this exploration-exploitation trade-off, how to do this exploration, but do not pay too much. And this is what is the general high-level idea of this explorable uncertainty area. Let me now give you a precise problem. Now, give you a precise problem. So, a very simple, very elementary question is: say we are given a set of elements, these elements have weights, and now we want to pick the element of minimum weight. So, I want to show you this type of picture that I usually draw. So, my elements are listed here on the left, and then you see always here on the x-axis the actual weight of an element. If there is all this information given in advance, it's easy, right? I look at all the elements and I pick the one that's available. I look at all the elements and I pick the one as a minimum value. Suppose I do not know the precise weights of these elements. Let's say there is some interval given, some uncertainty interval, and I know for sure that the true value will be inside this interval, but I don't open it. What can I do? I cannot find the minimum element, right? So in this list here, or among those inter elements, all these uncertainty intervals intersect. Each of these elements could be in principle the minimum. Could be in principle the minute. I allow my algorithm now to make queries. So it can query an element like element 4. This query will reveal the true weight. Can I solve now the problem? No, I cannot. But I see already that these elements are out of the game, right? They cannot even. But this element one is still interesting. So I query this one and I see now I can now solve the problem. And this is the question. I would like to find. Question: I would like to find an adaptive query strategy that needs a minimum number of queries to solve a given problem, like finding the minimum. So somehow one could see this as asking for the price of information, but I don't want to use this phrase here. So Sahil, he uses this for a very different model. But in principle, that's what it is, right? So for those who also know other query models, and I will make this distinction later more explicitly. This distinction later more explicit. I want to emphasize here that I would like to understand Explorer and I can make various pay for it, but I do not need to immediately commit to take this element into my solution. And this happens with many other models. So this is, I want to be very free and use this information I gave the way I want it. Okay, so we want to design algorithms, and then we will not be able. Algorithms, and we will not be able to find optimal strategies. The way I want to assess the performance is via instance-based burst-case bounds. So, similar to competitive analysis. And for now, you could think of this framework also as an online adversarial model. So, within this interval, when I make a query, some adversary will reveal somewhere in this interval the true balance. Now, my competitive ratio here is. Competitive ratio here is the worst case, so the maximum of all instances of the number of queries an algorithm needs to make to find the solution versus the number of queries in a best possible strategy for this instance. So for those who are more in this online world, you could see this as suppose some algorithm knows in advance the true realizations, but it still needs to come up with a query sequence, the shortest query sequence, to verify that. To verify that it found the solution. What would be alternatives? So, in other problems, it's often common to use absolute query bounds of the flavor. My input is of size x, and I don't need more queries than some function of x. In our world, so like in other Oracle models or property testing, this is common, in our model, there will always be instances where I need to query all the elements. Where I need to query all the elements. But also, the optimum has to query all the elements, and then I don't want to be punished for this, right? It can still be irrelevant. Okay. So. So I misunderstand a bit. What is opj? It's the smallest number of queries a strategy needs to make to verify that it found, let's say, the minimum element. Since it's both the intervals and the realization. Yes. So suppose I know in advance that these would be the two outcomes of my so how what was the intervals before? Let me see here also the option okay so no okay suppose the optimum value would be here then an optimal strategy may only carry this one Optimal strategy may only carry this one and would be done, but an algorithm could have started with this one. So, actually, let me go maybe some slides further and then this will be more have a better picture for this. Before I go to this next, so I want to show you lower bounds there, this will be very explicit. So, as a broad picture, so this line of research was probably started by Cahan and Stop ninety one. In the paper they didn't get so much recognition. They didn't get so much recognition in that world. So it was called Data Emotion, and maybe it was a strange title. But he actually looked exactly at this minimum problem, finding the minimum uncertain buttons. And then some geometric problems were considered, but only more recently, in the last decade, people studied several combinatorial optimization problems in such an explorable and certainty framework, and mainly in an adversarial model, so in this online type adversarial model. This online type atmosphere model. And then, quite quick, I mean, there are some nice results, but quickly finds tight lower bounds. And for more interesting problems, immediately very unreasonable, large lower bounds. So more recently, people considered beyond worst-case models, for example, stochastic and learning-augmented algorithms. And what I want to do here now is I first just to introduce this world, I want to give some examples from. I want to give some examples from the adversarial model, but then mainly I want to talk about the stochastic set selection problem eventually. But first, let's start with the adversarial model as I introduced it. And to make maybe clearer what this optimum or this worst case guarantee, worst case ratio, for this means, let's again look for now at this minimum selection problem. And let's say we are given just two elements and these are the corresponding uncertainty intervals. The corresponding uncertainty intervals. I want to pick the minimum, I don't know which is the minimum, I need to make a query. Any algorithm has to query some interval. Let's say it starts with this interval. Now an adversary can be mean. And what means mean? If it reveals to me a weight which lies still inside this other interval, then this information didn't help me at all. So I need to make another query, and now I can decide that this element has a smaller one. So, an algorithm needs to query, and here now what would an optimum do? It knows in advance the outcomes. And now it's clear: if I query this element first, I know that this value is much larger than this value, whatever it is. And vice versa. Now, if my algorithm starts with the other letter, we'll see immediately no algorithm can be better than two competitive and if you allow randomization in three hundred. Of equals one. Okay. I want to, so these are, so the true weight can be somewhere in this range. And the optimum knows that the weight of this element will be whatever, this one, so larger than this element. And it is enough if I query only this interval. Then I see its true value. And then no matter what the true realization within this interval is, this value will be larger. And that's why one carries a map. That's why one query is enough. Nicole, I don't need to query the interval which contains. Okay, good point. So let's say here for now, so later I will change this. So for now, I don't even care about the optimal value. I don't need to query my actual data. So let's put it this way. It would make it, in terms of competitive ratio, easier, or yeah, because if I force also the optimum to make more. Because if I force also the optimum to make more queries, well, that might improve. It might help me. Okay. So now in terms of techniques, or how can you now stick to this minimum problem? What can you do? I mean, let's understand this interval structure. These are these uncertainty intervals. And there are some safe queries, some mandatory queries that any strategy has to make. And if you look at this, so some interval has the leftmost end. Some interval has a leftmost endpoint, and if this interval fully contains some other interval, then I must query this interval one. Why? Because no matter what the realizations are, even suppose I knew all of them, I still have to query this interval to find out whether it has a small spread or not. So I can pre-process this and I can assume in my entire instance I don't have such laminarity, right? I only have intersecting intervals. I only have intersecting intervals, but none is completely correct. Now I'm thinking I'm confused about J. Oh, okay. So if you have n intervals which are the same, then I thought you can have in one of them the leftmost point is the realization and for all of them the rightmost point is a realization. So op J in my head is one because op just query that set after that. But any algorithm will, you know, cannot it doesn't know where it doesn't interval this. There's a normal way that's either it's clear yeah it needs to need to produce a proof that it's right that's uh yeah it's a proof like where the leftmost point let me a proof that it is right it's true so one left I mean one minimum element I mean there could be several right so if all the intervals are the same and I hit exactly the left uh most point I found the minimum right and the others cannot be better so I found the minimum limit with my query. So I found the minimum element is one query. Exactly. So while the algorithm will probably make n queries, it has too much focus. There's no but your this witness said to remove values. Right. So here I so because of this problem we usually assume open intervals, but here I thought I don't need open intervals. Let me think for a second. So let's for a moment assume that we have open intervals. Then we don't have this. Then also the optimum would have to carry everything, right? If I have equal. Assume my uncertainty interval is open, then there's nothing like a leftmost point. The optimum cannot be done. Let's assume we have only open intervals. So, we do not have any laminarity, and now, yeah, I will talk about witness sets. So, kind of the one way of proving then competitive ratio. So, a witness set is a set of elements for which I can guarantee that at least one element must be tarried by any feasible strategy. I don't know which one, but at least one. So, for example, all these intervals they intersect. I mean, in principle, this is a witness set. But I would like to have small witness sets, so for example, these neighbouring ones, the leftmost and the next one. ones, the leftmost and the next one, Optimum has to query at least one of them. So now, if I could design an algorithm that only queries witness sets, so ask the full witness set, and let's say they have bounded size, k, optimum has to query at least one of them, then I'm safe, right? I have a k competitive algorithm. And this happens exactly for this minimum selection problem. So we have these two neighboring intervals. Have these two neighboring intervals, one of them must be carried, so let's say we carry both. So start from the left end point, query until you've identified the minimum element, then you guarantee that you always carry witness sets of size two, optimum has to carry at least one of them, so it's a 25. And this was in this original paper by Kahn. Now let's look at something with a bit more structure, something more interesting. And I like MSTs, minimum spending trees. We all know what minimum spending. We all know what an expand tree problem in the offline world is. We're given a graph, it has weights on the edges, and we want to find a connected subgraph spanning all the nodes and has a total minimum weight. And we know how to solve this efficiently, and there are many nice properties. So I actually care mainly about the cycle property, you know, so we know that for any cycle we can kick out the largest, the heaviest edge. It would not be in an MST. It would not be in an MST, or there's an MST in which it is not included. Now, if we do not know the precise edge weights, what can we do? We still want to identify a minimum spending tree. And again, back to Arcom's question, I do not care about the precise weight of this MST. I just want to know the edge set, for which I guarantee that this is the spanning shape. What can we do? So now I mean we kind of want to imitate what we do in the offline world. What we do in the offline world, right? We would like to sequentially query edges. Whenever a cycle is formed, we want to kick out the heaviest edge. That would be a vague strategy. And it's clear that on a cycle, we may not need to query the edges. So in this cycle, it's clear that this edge, this interval 6, 9, it will always have the largest weight no matter what the actual realization is. But of course, there will be other cycles like this one. I have to make a choice. Like this one, I have to make a choice. Actually, yeah, so think of, forget maybe the whole graph, think of a triangle. Already a triangle gives me a lower bound exactly as before. If I have to make it, if I have to decide between two edges, which is the cheaper one or the heavier one, I have exactly this minimum element problem from before. So this lower bound of two clearly carries over. So a natural approach is that we imitate That we imitate Krushka's algorithm, or it's one idea, and say we add edges to our wannabe MST in increasing order of the lower limits. And whenever there's a cycle, or would-be formed a cycle, then I need to kick out the heaviest edge. If I can see what is the heaviest edge without queries, then I'm happy. I do so. Otherwise, I need to come up with some query strategy to resolve the cycle, to find out the next. To resolve the cycle, to find out the maximal edge. And in a paper by Thomas Elbach and others, he kind of picked up this exploring uncertainty gold, actually. So he observes that if I look at such a cycle, there's some edge with the largest upper limit. And let's look at some intersecting edges. They both form a witness set. One of them must be carried by any island. Then he goes through the cycle until it's resolved, always carrying witnesses. It's resolved, always querying witness sets of size two and gets a two-competitive algorithm. But again, we query always a full witness set, and one could also think about sometimes I don't need to query the full witness set. Can I somehow do this in a more balanced way, maybe using randomization? And yeah, don't read what's written there. So one can be a bit more careful and do some more careful pre-processing. To have more structure about what cycle, what edges actually close a cycle. Yeah, so I need some pre-processing, not important here. And then we can show a very cool property. So look at such a cycle. And maybe some edge is already varied, I don't care. It's a cycle that in my process, in my Croscale-like process, I would close. And some edge has the largest upper limit. By our pre-processing, it will be the last edge. Pre-processing, it will be the last edge. So we know something about this. And if this edge doesn't intersect with any other one in terms of uncertainty interval, then I know it's maximal, fine. Let's say I don't know, then there must be some intersection. So let's look at this picture. This is my F edge with its uncertainty interval, this one. And these are all the intersecting ones. So on my cycle, other edges whose uncertainty interval overlaps. And now we can show that in any feasible Show that in any feasible query strategy, either we query f or the entire neighborhood, all these other overlapping intervals. And this is very powerful because now we kind of want to resolve cycles by randomly, carefully, randomly choosing between f occurring this entire set. And I like to see this as a bipartite graph. So for one single cycle, I have either f or all the neighbors. Neighbors. Let me turn this now into a bipartite graph where the nodes are my edges in the original graph. And this is just one cycle. Over time, many cycles will be created. That means on the right-hand side, many more nodes appear. But the neighborhoods might be intersecting, right? And in the end, I want a vertex cover, either F or the neighbors equivalent. I don't have the full graph in advance, so it's kind of an online vertex cover. An online vertex cover problem where the right-hand size appears one by one. But yeah, we can come up with a water filling procedure similar to what has been done in the pure online vertex cover world, where we decide, then we distribute some potential to decide: is the left-hand side so neighborhood worth already querying depending on some random threshold, or we query F. And then one can analyze it. Terrific F. And then one can analyze this and get some competitive ratio which is better than two from the deterministic. If someone wants to look into this, actually, I think it's a cute problem. I don't know whether this is tight. The lower bound is just three half, and it comes from this very simple two-edge example. So this cannot be the truth. I don't know. Is it like for this Verge Herbert? No. And so for Vertex Herbert, Oh, no. And so for Vertex Hover, they get get some different uh guarantees. So it's not exactly the so because we use some more structure in the analysis then. Um it's a different number. It's slightly larger, I think. Less than two, but larger. Right. And yeah, so here is a lot of structure. We can show small constants in the adversary model. One can also translate this to matrix or generalize to matrix and look at non-uniform costs. This is nice. From course, this is nice, easy. Okay, whenever we now go one step further and look at other combinatorial optimization problems, like we looked at matching and NetSeg, or keys in general, there are very strong lower bounds that prohibit any adversarial good guarantee. So, and let me show you this, and let me choose a very general description of what I call the set selection problem and show this lower bound, but essentially it's the same idea that growth. Lower bound, but essentially it's the same idea that works for all these other problems. So, in this, what I call set selection problem, we're given elements, they have weights, and additionally, we're given sets. We want to pick the set of minimum total weight. Easy, right? And here, actually, from now on, I would like to also determine the actual weight of this set. Only makes it easier for me because also the optimum has to find this out. And I will draw sometimes. And I will draw sometimes also a picture for showing you the actual weight of the entire set. It's just a sum of the elements in that set. If we know everything, it's easy. If we don't know the true values, the true weights, then what can we do? We have to query elements. Again, we query these elements. Let's say we query. We don't query it. Clearly, my sets have an uncertainty interval. Have an uncertainty interval, which is just a sum of the uncertainty intervals of the individual elements. Right, I want to pick the minimum one. I can query elements, yeah? I don't query sets, I query individual elements. And if an element shrinks, so here to one point, this will have an effect to all the sets in which this element is included, which is here only, set one. Yeah, it will shrink also this abstraction. And then I query and I query until, yeah, until what? Until I've proven. Until I've proven that all the lower limits of the other sets are larger or at least as large as my minimum set. Yeah, that's my proof that I found the solution. And I want to come up with a query strategy with a minimum number of queries. Now, let me give you a strong lower bound. It actually has just two sets, and for one set, I even know the exact value. So set 2 has a precise weight, and for the Weight and for the other set, I have a bunch of elements, they all look the same to me. I cannot distinguish them. I need to query those, and these are elements that have a small value, except one. So, there's one important element that would show me that the set has a too large weight, but I will find it only as my last query, right? So, I query, it will shrink my interval, but unfortunately, on the It will shrink my interval, but unfortunately on the wrong end. And so that means I need D queries, so D is the number of elements essentially, and an optimum solution. It sees immediately that this is a heavy element. It can make this query and immediately would see, let me show you, with one query, this lower limit jumps then beyond this value of the other set. So what can I do? I cannot do anything. So competitive ratio is at least D. Ratio is at least d for number of elements. And this happens for all these other interesting problems as well. So now we should somehow adjust our model. This adversary model is just too strong, so we thought about a stochastic variant and now I will talk about joint work with my PhD student, Jens Schoeter. We look exactly at this minimum set problem, the set selection problem, but we assume that But we assume that we are still given these uncertainty intervals, but now the true weight is drawn according to some distribution. We don't even make use of this distribution. Our algorithms do not know the distribution. Actually, it will be an interesting question in it. How would you exploit this information about the distribution? We don't know. In our performance guarantee will come up some parameter about the distribution, but a very easy one. So Easy one. So the way I define this balancing parameter is: look at the midpoint of the interval and what is the probability that the true value is in the upper half. There are similar parameters used, or often it is assumed that with a constant probability the true value lies on the upper limit, something like this. So it's not uncommon. And what we use in our analysis is that this. Analysis is that this one-half, right? So, this midpoint. I mean, you could choose any other point, it's artificial. But this is something that we exploit later. Okay, and now in terms of analysis, worst case ratio, I now look at the expected value and I compare, and this is a very strong comparison, I compare with the expected value of the optimum solution for this particular realization. So, I don't compare against an optimal strategy. Compare against an optimal strategy that also does not know the outcomes. Here it knows the outcomes of this random process, takes the best possible solution, then I take the expected problem. Yeah, so it's quite strong, pessimistic comparison. Okay, so let's consider again this lower bound example. So now an adversary cannot make sure that with my last query I find out the interesting element. Now Now, yeah, these weights are drawn according to some distribution, but we can come up with a lower bound as follows. So we say each interval has a same, so the weight is the same distribution. With a certain probability, let's say p, it's on the upper limit, and with another, with one minus p, it's on the lower limit. And now, I mean, an algorithm has to make varies, but now yeah, so for each interval, there's a process. So, for each interval, there's a probability p to be successful and find out that this is an expensive element. So, in expectation, I need only 1 over p trials until I find the bad element or I query all, right? So it's a minimum of 1 over p and d. And this one, so 1 over p is exactly 1 over tau. So, this success probability here, this is exactly my balancing parameter. If you don't like tau, so in general, If you don't like tau, so in general you could assume tau, let's say for uniform distributions, it's a half, it's some number. And then optimum, yeah, one can compute, is roughly still making one query in expectation, so I have a lower bound of one over tau. But it's fine. Now, what I want to show you is that it is a way to get a competitive query strategy. Could you remind me what DAO was? Yes, sorry. Yes, sorry. So imagine in an interval, look at the midpoint. What is the probability that the realized weight is on the upper half? And this is tau. Or actually over all the intervals, I deny from the upper half of the upper tau. Okay. You can think about uniform distribution. So it's a half. Tau tau is a half. Okay. Now what I want to show you is this. So we lose with respect to this lower bound that I showed you a log square m factor, but I will argue. log square m factor, but I will argue that maybe this is not too bad. But I find the way to get there interesting. So what we do is first we formulate this uncertainty problem in an ILP. This query problem we reformulate. And essentially we get a covering ILP with uncertainty in the coefficients of the constraints matrix and in the right hand side. This is something weird, I think. Weird, I think. But then we first assume, let's assume for now that the right-hand side is given, it's known, only the coefficients are uncertain. Then we get a one over tau log m competitive algorithm, and then we add this uncertainty on the right-hand side, and we use another log m. So here, I mean, in this audience, I met many people who also work on some kind of query problems and stochastic problems. So I would relate this, or I should mention, there is. This, oh, I should mention there is work on uncertainty in the constraints of IPs by Maharaj Imaguchi. They do not do a worst case analysis. If one translates their approach into a competitive ratio, then for our minimum set selection problem, it's at least in the order of n, number of elements. And also Anna Palm and others worked on some query strategic or query problems. Or query problems where you don't commit immediately to the final solution, and they have a budget and they rather try to find out how do we balance approximation and using the given budget. It's of a different flavor than what we do here, but certainly related. So let me get to this ILP formulation. So again, we're given a couple of sets, we're given these uncertainty intervals. How do I describe my query problem as a ILP? Problem as a ILP. Let's first find out if I have an interval and I make it. So if I have an element that's in a couple of sets, like this one, element i appears and set as one and set as two. If I query this element and see its weight, what's the effect about shrinking the intervals in which this element is included? So here I have my set. I made this query and clearly for this element here, I shrink the uncertainty. Element here, I shrink the uncertainty interval by weight minus or realized weight minus lower limit, and on the right-hand side, upper limit minus realized weight. And the same happens with my set uncertainty intervals. So I know exactly, if I make a query and I know the outcome, I know exactly what happens. And what's now my plan? I want to find out which queries should I do, so minimum number of queries, such that I do. Prove that I found the minimum set. So that I prove that all the lower limits of the other sets are larger. Yeah, so if you want to see this in a picture. So let's say, so here on the left are my elements that I can carry and they're here my sets. I don't care about the right input, I care about the left inputs. Suppose I knew my W star, the target weight, that I need to prove that all the lower limits of other sets are beyond this. On this. Then I make a query and I see what's the effect of this query to my how much does it shrink my intervals or increase my low limits. And then I need to make queries, I need to make more queries, and it's not enough just to find this actual minimum set. I need to query further until also these two are proven to be larger. But this is something I can break down. I can break down. So x is my variable that says, do I query this element or not? If I query x, then it will shrink or increase the lower limit by exactly the distance between the realized weight and my lower limit. It's exactly this amount. And I need to collect queries for a set. I need to collect as many queries such that I can prove that my lower limit jumps beyond the W star. That's it. So this is. That's it. So, this is just a covering LP. If we have all the information, we can sort of. I mean, and we have now on top this uncertainty. We don't know this W star, so we don't know how far we should move, and we of course don't know these realizations. That's why an ILP or covering ILP with uncertain coefficients and right-hand sides. Even if we would have the information, it's still a covering problem, right? We have lower bounds, we cannot do better than. Have lower bounds, we cannot do better than log M approximation, but in the offline setting, there would be log M approximations also for this multi-set, multi-cover style problem. So now we would like to take some inspiration from offline set cover to solve our problem under uncertainty. And the most natural thing is to take really strategies, right? So we have to make irrevocable decisions and we cannot look ahead far. Ahead far. So it's natural to look at the offline greedy, and there are two types of greedy criteria which I would like to translate to my picture here. So actually to the ILP and my visualization. So one greedy value is you want to make progress as much as possible in terms of volume. What is volume? I mean, I want to pick an element that shrinks or it increases the lower limits as much as possible. But careful. As much as possible, but careful. I care only about up to this point and point here, right? So if I shrink a lot, I mean, beyond this, I don't care. So this is my one greedy value. And how much progress, so how much do I improve on the side? But then at some point, look at this picture here. This type of volume greedy value would mean I should pick this interval. But at some point, you want to satisfy as many constraints as possible. So at some point, you want Many constraints as possible. So at some point, you want to lift as many intervals beyond this barrier at once, even if this means small volume. So maybe now you can find one element that would shrink these three, then you prefer those. Yeah, so this is the other type of greedy value, where you satisfy as many constraints as possible with one trade. And then in a greedy way, if one puts this together, one gets a log m approximation. One gets a log M approximation. I'm cheating a little bit. So, here in this guarantee, actually, the interval borders should come in. I think they can be constant. For those who want to see this exactly, I should look at the interval borders. Okay, these greedy values, to compute them, I need to know how much do I make progress with the query, and I need to know this W star, this barrier. I don't know. Here, I don't know. So let's for now assume that I do know this W, I just don't know my realizations. Okay, what can I do here? I cannot compute the precise greedy values, but I may not have to. If we would get alpha approximate greedy values, we can guarantee this, then this alpha would show up in our competitive ratio. And now, And now, what we do is we run an optimist degree. So, what did I learn? From underprompt, optimism in a phase of uncertainty, something like this. So, I don't know with one query how much I shrink this interval, but I know an upper bound, right? So, if this true realization would be the upper limit, then I shrink the interval as much as possible. This is like, let's say here, there could be more. It may not be realized, but I don't care. So, I'm optimistic. I don't care. So I'm optimistic and I compute the greedy values according to this best possible value. And then, since we are in a stochastic model, after one over tau trials, I find an interval that two approximates the actual greedy value. Okay? And then, yeah, so, I mean, the technical details to how to put this together. But this is a key idea to, in the end, get now a 1 over tau log m competitive values. So now the question. So now the question is: how do we assume that we know this target value? What if we don't notice? So, what if also the right-hand side is unsure? Yeah, so here, maybe the rough idea is, I mean, this value must be somewhere between some minimum and some maximum value. And I think a natural thing is to use some kind of doubling trick, like we often do in an online. Like we often do in an online optimization, also. It's not very easy because we have to now play with greedy values which have wrong greedy values. So the greedy value also depends on this W star, right? But yeah, essentially the key idea is that one can run such a doubling technique using this optimistic ready as black box inside. And then we lose again another factor log m. Let me not go through this. Lose another log m and get this one of. lose another log m and get this one over tau log square m competitive algorithm. It would be nice to see if this additional log m is actually needed. We do this like in these two phases and we see where this log m appears, but I'm not sure. So we have this lower bound of 1 over tau and from set cover this lower bound of log m. Okay, I don't know how to prove that actually the product is also lower bound, but it feels like it. But this additional log m are. But this additional log M, I don't know. I should mention that if these sets are disjoint, then life is much easier. Then we don't need all these log n factors. Oops. And so I mentioned already what if one wants to continue this, so what would be nice is, of course. This, so what would be nice is, of course, it would be nice to understand also more general ILPs, not this approach. But what I find even more interesting is: so, we ignore completely this distribution, because I don't know how to explore this distribution information. There is some work by Anopam and others on a stochastic submodular cover. One could see this in a way as our covering ILP, but we know the right-hand sides. Sites and you compare it to an optimal policy. So this is very different. But you find a way, a greedy way that looks at marginals and takes a distribution into account. It would be cool to translate this here without knowing the right-hand side. I don't know. It would be cool. On a broader perspective, so in this explorable uncertainty world, it would be nice to understand other problems more. Other problems, more and more problems with structure. So I chose here this set selection problem as an example for a very basic elementary question that we need to understand before we can maybe go to other problems like matching and NAPSEC. If I would give you all matching or NAPSEC solutions, let's say a NAPSEC problem, I give you all the configurations that are feasible, all the NAPSEC solutions that are feasible, but I don't know the weights. Pick the best set. Weights. Pick the best set. This is exactly this min selection problem, right? But how to identify this feasible subsets. This needs extra work, so it's not clear how to translate this in the polynomial way. Actually, I would also be interested in this MST problem. Even though in the adversarial model it looks nice and easy, what if you have distribution information? Can you do something cool, something nice there? I don't know. So, and just to mention, So, and just to mention, there has been also some work in the direction of learning augmented algorithms. And one may not only explore values out of these uncertainty intervals, right? You could also think, for example, about matching as preferences, something I'm looking at right now. So, you want a computer-stable matching, but you don't know all the preferences, but you can query. Let's say pairwise query. How would you do this in a manner such that you do only few queries, but get a stable image? Queries, but get a statement. Okay, that's all that I wanted to tell you, and I thank you for your attention. Give me time for like one or two questions. So, this volume Dorvan that you're mentioning for the problem is more like having an image of the collection. Yeah. So but I mean just if you look at this LP, I mean this I mean I plug in there these intervals and but I end up in a normal multi-set multi-cover problem. It's called isn't it, right? Your sets are very structured and my sets are structured. But the interval because of this inter of the intervals. Of elements. Of elements like but the covering is all is all this it's true my covering is then on such an interval yeah it has this interval structure I just wanted to show you that every set contains the element of weight one Okay the minimum number of elements.  