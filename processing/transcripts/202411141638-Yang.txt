And this will be the focus of this whole project about generating such past dependence of passive remote switch model. And after that, we're going to give a very simple example of what we can do using such proposed model, which is associated with a discrete time swing option pricing problem. This is a joint work with my PhD provider, Dr. Chu, who happened to be here, and my To be here, and my postdoc supervisor, Dr. Ware. We are proposing a data-driven model, which means we need to do a lot of eyeballing for the historical data for price and storage for natural gas as well. And here are some examples of a typical trajectory of a natural gas price and also storage. Price and also storage. And from the plots, we can easily see that the gas prices behave really flexibly. And you have some certain time that the market is relatively quiet, and you have some other time where you have extreme fluctuation of the price process, indicating possible roughness in the system. So, we want our model to be flexible enough to capture all the. To capture all these features which we observe from the data. And this is becoming more evident if you further observe the increment of the price data. And comparing to that, we have the storage trajectory for natural gas. And first sight, it's far more smoother than the price dynamics, which means it's Which means it's not reasonable to propose any kind of stochastic integrals for the dynamics of storage. And also, we observe very strong seasonality behavior in the natural gas underground storage data. This is something we want our model to capture, but also at the same time, we don't want this very strong periodic behavior to cloud our judgment on its impact on the gas price that gas. The gas price process. So, at some point, a certain decomposition of the storage data should be computed to avoid such kind of destruction. And we also want to include rich enough price and storage dynamics, which can be further observed in natural gas markets. This means we need criteria to tell investors. Criteria to tell investors when to buy and sell natural gas. And this can be largely reflected by the increase and decrease in the storage or inventory regarding the storage of natural gas. And in that case, the storage becomes a common factor of the market volatility. This becomes particularly evident when market storage is approaching depletion or Or merely full. So, in either cases, we're partially losing such kind of calming factor, which will eventually drive up the velocity for the gas market. It is also natural to think, to propose a task dependent system for the guest price and storage dynamics. We can observe that in That in the autocorrelation data, autocorrelation analysis for the price data set. And also, there have been existing works arguing that the volatility of the gas prices should be dependent on the past history of the price data instead of just one point. This indicates a non-Markovian path-dependent structure. And here we are proposing a path-dependent. Coding a past-dependent weight and moving average for the log price SPART process. This is parametrized by this alpha parameter, taking values from 1 over 2, 2, 3 over 2. And by adjusting alpha, our model has the potential to capture the quieter behavior of the price process. When alpha is smaller than 1, it's smoother than a brown motion. When alpha is bigger than 1, Motion when alpha is bigger than one, we are able to capture roughness in the system simply due to this singular point when u is approaching t inside the integral. Here are the two correlated data sets we use for the Intel project. Our price data is of daily data and of Alberta Mission. And our storage data is for mountain region, weekly average data. Region, weekly average data. We make sure that not only the time framework between the two data sets are consistent with each other, but also the location roughly matches each other. We say they are correlated for these reasons, but strictly speaking, they're not recorded by the same agent for the same purpose at the same time. So it's very easy to. It's very easy to actually obtain price data or storage data, but it's very hard to obtain correlated data sets. I think that suits our goal. This is by far the best we can do regarding the data. And to address the seasonality behavior of the storage, we compute a Fourier transform for the data to decompose it into two components. We have the P weekly process. The P weekly process to capture the periodic behavior of the data, and we have an X weekly process capturing the D-trend or D-periodic, the local fluctuation of the normalized storage. And since our storage data set is of weekly average data, so we further need a step function to transform the weekly data into daily data sets. And here is our proposed log price and storage model for equation 2a. If we are given a continuous trajectory of Vt, our log price is well defined almost everywhere. And our volatility process Vt can be decomposed into three components. We have a constant term V0 indicating the like aligning with the Aligning with the classic constant velocity models, we have a second term in VT that is associated to the storage level of the natural gas market. And we have a third term that is associated to the moving average we just proposed under some delta approximation. We add delta approximation here and there for making sense of the web closeness of the model and also for the benefit of numerical simulations. Here, this difference. Here, this difference in RET actually bears significant financial meanings. It can be viewed as average growth of the lot price, and the square root operation just introduces a HISTEN-type velocity. And for the D2N and X component for the storage, we propose an ODE system as I mentioned before. We don't want a stochastic integral term to mess with the trajectory, the regularity of the trajectory. And here, gamma1, gamma2 are just two parameters taking values in r. And we have two more coefficients for the two terms, r plus, r minus, coming from this r function, which also has a financial meaning. It can be viewed as a relative price level. Like thrust price level. And so, ideally, if we further assume gamma 1, gamma 2 are strictly positive, then our D2N and X storage is likely to increase if the price level is relatively low, and also when the storage is not full yet, and it's likely to decrease when the price level is high enough, and also when the storage is not depleted. The storage is not depleted yet. And this means for this proposed model, we have the following seven parameters to calibrate. We're going to postpone that for a later discussion. Here's some background introduction for general volatility models, especially stochastic pathogen volatility models and rough volatility models. And we took some inspiration from GUI. Some inspiration from Guillain's work of his two papers for the notion of weighted moving average. Here we adopt a power kernel for the moving average, but at the beginning of this project, we propose a selection of different weighted average and then play with them. And our model, you just saw, is actually an Is actually an evolution of a lot of toy models, and eventually we have this. And we talked about the concept of the moving average. We haven't yet given a robust support of such definition. Is it well defined for any alpha? Well, we give a lemma result for this h function, which corresponds to this relative log price level. It is well, we conclude, we conclude. We conclude that this difference is well defined for any alpha from 1 over 2 to 3 over 2. This is obviously true when alpha is smaller or equal to 1, but when alpha is bigger than 1, strictly bigger than 1, we can also show that this is well defined simply because we can further prove that the log trajectory of the price is actually almost everywhere beta holder continuous for any beta smaller than 1 over 2. Any beta smaller than 1 over 2. So this difference is well defined for any alpha in this interval. And we've established that as Rt, the log price is well defined. This means our moving average is well defined for any alpha. So this is good. And further integration by parts transform the third term, like part of the third turn of the velocity process, into a fraction of a fraction. Fractional Brandon motion, sorry, into a fractional branch motion. And this allows us to make the connection of our model to a rough Histon type model, a velocity model, with 3 over 2 minus alpha behaving similarly to a first parameter. When alpha is approaching 3 over 2, we have extreme roughness in the system, indicating short-term memory. And further, we prove a We proved a well-posing result for the model we proposed. The Stokes differential equation emits a unique strong solution pair as bar x. It is important to point out that our log price process is not uniformly bounded. It's only bounded in a weaker sense under the expectation. This will give us some trouble later when we're working on the pricing problem. Pricing problem, but it should be a minor problem. Now that our model is defined perfectly, we can move on to the calibration of the model. Remember, we have seven parameters to calibrate. We do the calibration in two steps. In step one, we calibrate the first five parameters using a log likelihood function approach. Function approach. The likelihood function can be derived in a past-dependent non-Markovian way. We only need to consider the distribution coming from the log price instead of the joint process of log price and X. Simply because for the storage dynamic, we'll propose a ODE system. So any one-step increments of X doesn't follow a distribution. Distribution. And by maximizing this rescaled log likelihood function, we can obtain the optimized five, like the first five parameters. In step two, we calibrate the remaining parameters gamma one, gamma two, using the already optimized alpha parameter. And by minimizing this mean square error, we are able to. We are able to obtain the optimized parameter gamma1, gamma2 under three different cases. Tau equals to capital T, 14, and 30. Here tau denotes the calibration batch size when tau equals to capital T. That means on the time interval there to capital T, we're doing the calibration altogether for gamma 1, gamma 2. Thus, the optimal parameter we obtain should be constant numbers for gamma 1, gamma 2. Gamma 1 and gamma 2. For the bi-weekly and monthly cases, we are switching time intervals every batch size for the calibration. So instead of constant numbers for gamma 1, gamma 2, we are obtaining vectors for the optimized parameters. And here are some calibrations for results for the first five parameters. What's important is the first two columns. First of all, the Two columns. First of all, the time interval of interest. We have provided six time intervals. These time intervals are chosen very carefully to align with real-life global and regional events. That has a huge impact on natural gas markets. The first time interval corresponds to the time when there are severe sanctions posed by the United States to Iran, with Iran being To Iran, with Iran being, I think it's the third largest global natural gas supplier. As you can see at this time, alpha, our calibrated alpha, is very close to 3 over 2, indicating severe roughness in the data. And followed by that, we have four time intervals corresponding to the hit of the pandemic. The market lays relatively quite quiet. Quiet, especially for the first one, it's corresponding to the initial hits of the pandemic. The market is almost dying there. And the last interval corresponds to the breakout of the war between Russia and Ukraine, with Russia being, of course, the one of the leading natural gas supplier. And again, we see extreme roughness in the system. Extreme roughness in the system. For the rest of the parameters, they're of less significance so far. That is until we find any practical meanings for them. And this result shows promising feedbacks to our model because it can very well capture the roughness lies in the data. So our alpha parameter. So, our alpha parameter is serving as a rough indicator of our model. And we have the calibration results for step two when tau equals to capital T and for the bi-weekly and monthly cases. It is also promising to see that for the bi-weekly and monthly cases when we have Mostly cases, when we have more points for gamma1, gamma2, most of the entries of them are actually positive. This is fitting our initial expectation of the idealized model, where we assume gamma1, gamma2 to be strictly positive, so that we can capture such price storage dynamics and also the buy-low, sell-high norm, financial norm in any financial market. This is also. This is also evidence supporting our model and its performance. With the calibrated model, we are able to fit the X component of the normalized storage. And for the simplest case, where we make tau equals to capital T, remember we only have constant numbers as gamma 1, gamma 2 for the optimized parameter, right? Optimize a parameter, right? So we only have two parameters. It's expected that it cannot capture very complex trajectories like this and this. But still, in a reasonable way, it's a surprisingly nice fitting, as we can see from the performance. And the fitting can get better if we make tile smaller, but not too small. Tau smaller, but not too small. For the bi-wigly case, we have this strange wiggling behavior, which is possibly caused by the operation of weekly average. Remember, our data is of weekly average storage data, and by simulating the X process, we obtain daily simulated results for X, and then we compute moving average. This has an impact on the fitting. Also, when we have a small tau, we are very frequently switching the parameters we obtain for the model we propose, and thus creating such weird behavior. This can be eased when we make tiles slightly bigger. In a monthly case, it's way better than the bi-weekly case. With the calibrated model, it's now to Model is now to test what damage it can do. We are including a very simple example for a discrete put type swing contract. In a commodity market like natural gas market, a discrete contract is actually very common. So this is partially why we choose a discrete version, and it's also because it's easier. Also, because it's easier. So, here's the setup of the swing contract. We have a strike price, K, we have a local constraint, L, denoting the maximum number of rights we are allowed to exercise throughout the lifetime of the swing contract. We have a local constraint, L tilde, denoting the maximum number of rights we are allowed to exercise at each time T. And we have a finite number of selections. We have a finite number of selections, finite number of possible times for us to exercise the rights. And these times are indexed by the element of this I set. And we have the control process, small Qt, denoting the number of exercise strategies at each time. And our state process, capital Qt, denotes the remaining exercise right at each time. The remaining exercise right at each time t. And for the terminal condition, we choose a penalty function g, which is proportional to the intrinsic value at terminal time, and also it's proportional to the remaining number of unexercised right. Since it's a discrete case, the cost functional and the value function can be easily defined in a discrete way. And for the For the pricing problem of the screen contract, we adopt a dynamic programming principle approach. This allows us to transform the pricing problem to an approximation problem for the condition expectation of this second term. The certain doesn't matter because it's measurable. And we achieve such approximation by a neural network approximation, a deep learning neural network. A deep learning neural network approximation. We have our input as the path of the storage of the price trajectory. And for this work, we only need a shallow neural network. That is because we don't have much input. But if we are instead dealing with hourly or every minute data for price, we can have a huge amount of input. Amount of input, which means a deep learning neural network structure is more favorable in that sense. And we provide an numerical algorithm for the pricing problem. It goes backwardly to the initial condition, to the initial interval, where a neural network approximation for the conditional expectation is not needed because that's. Because tradition uh that because that conditional expectation is just expectation. So, traditional multi-colour method works. And finally, we have a convergence analysis for our numerical scheme for the pricing problem. Due to the fact our log price is not uniformly bounded, it's very hard for us to write out the Euler approximation term or any approximation for with a specific numerical scheme. Numerical scheme. We also have implementation, an example of implementation of our numerical scheme using synthetic data, which due to the time limit that I didn't put in my slides. That is all. Thank you. So if we're in the mirror part, you are using Ledo. Part you're using like a feed forward and then taking the pass as your input. Humor co-part of what? Of the pricing? Like your architecture. The feed architecture? Uh yeah, here. Yes. The previous key. Oh. Yeah, you are taking this whole pass there, so uh trying like the LSTM or even like using the signature of this pass as input for your I was very recently introduced the idea of pass signature and I'm thinking about it because we are adopting a power kernel for the moving average. This means we have some normal behavior when you is smaller than t, when u is smaller than t, when u is approaching t, we have single U is approaching T, we have singular behavior. And it seems like a past signal is a reasonable way to tackle the balance between the two components. And it's something we want to try in the future. And for the natural gas market, I mentioned a discrete swim constraint is common. We are currently working on an electricity market where continuous trading is more. Continuous trading is more favorable. And for that project, it's still ongoing, and the focus of that will be largely put in the continuous pricing problem. We do have obstacles regarding that problem because of the notion of our past dependence. Our volatility Volatility process Vt is not only pass dependence, it's also random as well. So, if you're considering the well-posedness of the HIV equation in continuous setting, it might cause us trouble. Any other questions? Yeah, would I just have a quick question? Can you please go to uh slides? Can you please go to slide 16 where you had that table where you nicely explained the alpha? If I look at this, actually, the R changes a lot as well. Do you have some explanation for these different values over the different periods? For R, first, we don't have a specific meaning assigned to it, so it's just some constant coefficient we have in the model. We have in the model, it's not necessarily linking to the interest rate. So I don't know if it's a bad thing when R in this case is varying very much. Because in the data, we do see very distinct behavior. And not only it's a reflection of the roughness in the data, I'm wondering whether these coefficients, these parameters will be affected. Parameters will be affected as well. So that is one reason we are computing the calibration of the parameters interval by interval. And we choose, as I mentioned, we choose these intervals very carefully. In practice, you can do the same. But you need certain expertise in the specific market, and also you need to be quick and sharp. So there is a like a threshold for you. Like a threshold for you if you want to apply this model very well. If you estimate these parameters instead of the calibration using an MLE, then you can assess a standard error and maybe those differences are not significant working on short period of time. It probably may just standard errors. It doesn't have to be short period of time. No, but if you look at that, right, it's short periods. Yeah. Short periods. And so for me, when you estimate on a short period, usually the standard areas are big. And so I'm not quite sure that the differences in the point estimate are significant. I see that. We can always try a longer version, take a longer time framework, to see if that is the case. Thanks, Jordi. Good questions and comments. Okay, then let's thank Olva. Again, let's thank all the speakers of this session.