So due to the power shortage I I couldn't prepare this talk well but um yes our seminars are always on Wednesdays and sometimes we get they wrong but they're Wednesdays just very much anyway so it's my May 10 May 10 yeah sorry I hope that got this one right now we have a really nice virtual workshop on it that's the second of June really Really? This meeting is about systematic upside. Some random noise here which we meet. We have to do parameters again. So there will be this workshop beginning of June. That's at least correct is correct. There will be about for about two sample tests and the goodness of the test. And uh in autumn we are planning a five standard for you. In autumn, we are planning a faster unfolding workshop. And very generally, Louis and I are always very grateful and happy for any suggestion on topics, speakers, for seminars and workshops. And how to stay informed, you can book out our homepage. You have to update it with some of these new events here. And you can also subscribe to our email list where we do all announcements. Okay. And now this is something. And now this is something. The first one, this is coming out as a result of this workshop. Lucas Heinrich kindly suggested or has a nice idea to have a workshop in the near future on likelihood free inference. And I think this obviously makes sense. We had fantastic, interesting discussions about all this. It was also one of your points about this ways of getting the light with the way you work. And Louis, yes, I think we took care about that, this five-step project. This five-step workshop for the future generation in January 25. And that's all from my side. So you can decide yourself which signals and background we see. I think it's all signal. So the reason that we have January the 25th as the Five Stat for the Future meeting is that the original meeting of the first. The original meeting of the first meeting of the series, the first meeting annually voted to statistics in particle physics was in January 2000. So this is the 25th anniversary of it. So it's a good time to do something and do it for staff for the future. So just a few more thank yous. First of all, well, we've done the thank you for burrs. I'd like to really thank the committee for helping. Committee for helping set up this meeting and the sort of programme here, which turned out to be so successful. So mainly Olaf, who is the chair of PiStack. Richard. Oh yeah, Richard at the back layer. Vedia and Sarah at the end of the table. Also, really a big thank you to everybody here, whether they were giving the talk or not. I mean, we did try to put the emphasis of this meeting on discussion. So everybody had joined in the discussion, which I think is everybody they really everybody deserves a big thank you for helping this meeting and this. This meeting is as successful as it was. I also want to thank Rebecca Nugent, who some of you know. She's the chair of the statistics department at CMU, and she allowed almost the whole of the statistics department to contribute to this meeting. So please convey our thanks to her. Oh, yes, the speaker. Oh, yes, the speakers. We've had some really interesting talks, so thanks really to those of you who did give a talk and devote time to it. And that sort of also opened its way to the discussions that we've had. So really, do keep in touch with us and let us know about your suggestions, especially suggestions for the future. And we look forward to seeing you all again, hopefully in the not-too distant future. Okay, bye. Okay, bye. Sorry, there was a comment I guess. I think I misunderstood the thought before, but I think it comes back to sort of a similar problem. Instead of just learning the data, you actually created all the idea of the e to the f of x. Like n e to the f of x. And if that n is basically h at n z, which would allow you to do what you're doing. Then you basically have all everything canceled if you're sort of basically learning the code. So putting these in the range. So depending on how you normalize the logics of the classifier. The logins of the classifier, but the two despectors. I mean, I need to know. No, I understand. Depending on how you play which guy, it could end up being related to like once the logic versus once the I mean that would be if it can be shown that they're still that would be really interesting because right now it seems very blurry for me because I don't understand really this way that you It's really just the way that you want to sound slightly different here because in front of the two samples, you have the data and then you have the records. So for us, to be really trained to do samples five versus another is really, for us, code samples enter assemblers. In Glacier's case, if you look at it really damn, they only need the reference samples for normalization. There is no like the reference sample has the points going in the The points don't answer to our state. I think it could also enter. So if you take like the log of right, you end up with what it's ended up log lambda or whatever. Right. So now those two terms, now if your neural network is just a correction of multiple directions, then it's sort of the correction on top of H0 is what you're modeling, which would basically It's what you're modeling, which would basically make sure that this is something Rob sort of explained to me that sort of made sense, which is like you're taking your alternative as the exact data. That's what the guy uses a neural network to fit the exact data. So you could fit the exact data with your neural network, or you could fit the corrections on top of H with your top of circulation. The reason why this is different is, well, it's the same thing. One allows a cancellation in the likelihood. Like all the fish within the answer, if you're just going to... Like all the fishing within the answer if you just didn't know that. The reason why I'm sort of pointing this out is then it starts to look a lot like the whole log right ratio. It's not just one of the reasons to be a log right to the ratio. And that starts to look a lot like the logic of a classifier. Because the logic of the classifier is a lot like the logic. And so it might be that depending on how you modalize Gaia's approach, you can see it as sort of a one-single test. You could see it as sort of a one-sample test on the data, or you could see it as like it is a one-sample test, but normalized to the Monte Carlo. In other words, just give me the correction that gets the data. And that small difference may give two lenses in the same pictures like a little bit. But then I think the big question is, like, even if, I might be wrong, but even if they were two lenses in the same picture, the way you incorporate systematics could be very different. Yes, yes, yes, yes. And that's why you say that for us, you're. Yes, and that's why I mean if you say that for us it's an open problem because we we haven't really thought about what we would do. I know that they can do it which is really impressive but but given that their place of training is different when we're at the end, I mean it's pretty so far. The training is definitely different. I think that that's like I think well let me just say I didn't mean to be a jerk if I came off I think your comment was like really good of like maybe there's actually a really there's a there's a Maybe there's actually a really there's like there's like an interpretable kind of like connection between these two. It's not just like two methods, there's like no no I think there's definitely a connection between the two. It's just about the variability. So in our case I would say that the whole variability is since we are estimating the ratio directly the variability is just that one whereas in their case the normalization constant is going to be variable right like the thickness you are estimating from the background data is also going to be variable so now you have two different So now you have two different things that are random variables and you're sort of taking their ratio. My understanding was that you're sort of fixing the normalization into the simulation and then deriving a correction so that every bit matches the data exactly. But you're saying if the normalization is a function of the simulation, the simulation is random. Normalization is also a random thing. But we're sort of pretending like the simulation is the null hypothesis. That's for sure. Hypothesis, that's for sure. Yeah, but there's still randomness in the mathematical. Sure, but that's also true of the classifier. There's uncertainty. No, so that's the thing. There is epistemic uncertainty. Or sorry, epistemic uncertainty. Correct, but both the uncertainties together go into this one thing. So it's just one thing's variance that you sort of have to estimate. So they vary in different ways. I completely agree with you that the uncertainty is counting could be very different. And I think the uncertainties will also be different because, like, ratios of two estimators have. Estimators have different sort of variance from them if you just estimate the ratio directly. Right, and so this is actually something because the classifier output, I noticed you're using this H over one minute H trick. You can actually just not do that. Yeah, yeah, we can use H directly. Well, you can just not use the sigmoid. The entry to the sigmoid should be the logins, like the log, like the operations. And it's actually much more stable numerically. Just a trick.