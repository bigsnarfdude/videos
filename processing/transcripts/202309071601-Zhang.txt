One one of the most the biggest challenge is to uh uh study the DNA reaction kinetics. So uh to uh solve this, uh to help uh the domain expert and the practitioners to have a better understanding of the DNA interactions, uh in this project we built up a visualization tool which is based on the deep bottom embedding and trying to visualize the DNA reactions records. Okay, so why do we need a new tool for visualizability in reaction trajectories? This is because the existing visualization tools are very coarse-green actually. And they are restricted to a class of reactions and they are always limited by their resolution of the individual signal structures. And also it's always unable to capture the unexpected patterns. Here's an example that shows a coarse-grain approach. From its plot, the x and y axis represent the number of the basic pairs in the final structures and not in the final structures respectively. So for example, if we say, like for example, each register actually represents an ensemble of the secondary structures. For example, if we say the position ij, which means Petition IJ, which means like there are signal structures in the position in the breed cell, which has I-based pairs in the final structures and J-base pairs, not in the panel structures. So it's just an ensemble, it's not individual second structures, right? And also, yeah, I labeled the initial and the panel structures in this coarse-brain energy landscape, and then I just lay out the trajectories. Lay out the trajectories. So, from this plot, actually, we could say there are many limitations. One of the biggest limitations is that, because it's a coarse-grained model. So, for example, in each strand, maybe there are some states with very different energies, but we just simply cluster them to the same grid, which is very uh which is uh not not like expected. And also, like there may be some uh singular structures which are unlikely to be sampled. Unlikely to be sampled on the right trajectory, but weight, but in the gray cell, it's just including all of them. And also, after laying out the trajectory, we can say that it's very hard to distinguish different trajectories, and also it's very hard to explain or hard to interpret what this trajectory really means. Okay, so to solve this problem, so uh we uh create a new framework which ca well which called Vida. Which called Vida, which can create low-dimensional embeddings of the secondary structure, second state space by using a deep web embedding approach. And with our beta framework, it will produce fine-grained visualization, which has better resolution of the individual standard structures, and also it can distinguish different trajectories and has better. And it has better interpretation of the plot. And also, the VDAS input is the output from kinetic simulators. So in this study, we use the multi-strand. Okay, so before I go into the model and the result, let me just give you a brief background about the DNA and the deep alphabetic. So what's the DNA interaction? DNA reactions are the strand interactions. Stranding corrections. It's the strand folding from the initial structure to the final structure, and this process is capacity, which can be usually modeled by a continuous time Markov chain. So there are two common DNA reaction types. The first type is called DNA type redidation, which is also called helix association reaction. So in this reaction, the initial state is unpaired DNA strand, and the final state is fully paired helix. Is a fully parotelic structure. So during this process, it's very possible to form some heritage structures, and these herring structures will have some influence to the reaction, to the reaction, to the overall reaction process. And here's the animation shows the host reaction looks like, but it's not what I would like to visualize. It just gives you a sense about the DNA reactions. And the second common And the second common interaction type is called total mediated three-way strand displacement. So in this reaction, the invader comes close to the substrate and then it's paired with the total of the substrate and then it gradually replaced the incarbon. And finally, the incarbent is just separated from the substrate. And the invader and the substrate are fully paired. Okay, so the reaction trajectory, so it's just a sequence of the secondary structures going from the reactants to the products of a DNA reaction. And also it's come up with the transition time from one state or from one sentinel structure to the next sector structure. And in DNA study, in DNA kinetic study, we are always interested in the secondary structures, and which is always represented by the dot-princess notation, which I shown. Dot parenthesis notation, which I showed here, and each character represents an individual base, and this dots represent unpaired faces, and this open and closed freezes represent the paired faces. And this plus sign just trying to separate the different strands. Okay, so after talking about DNA, so let me give a very quick introduction of the deep rocking bedding. So, I mean, we are. So we all know like the deep neural networks just make things different and also people trying to use the neural networks in the graph study. And the deep graph embeddings the process of transfer the graph information into a lower dimension uh vector space and this uh mapping of transformation is defined by a deep neural network uh which is trained to capture the structure and the semantic information. And the semantic information of the graph. So, inspired by this deep graph embedding, so I just treat each secondary structure as a graph. So, the node is the basis, and the edges is the base pairs, and the adjacent bases, like adjacent bases, which are in the same black box. Alright, so here's the model. The model. So there are a few quite different components of the model. So first is the simulator. So we got this simulator which I use the multi-strand and then it's produced three different stuff. First is the dot principal notation. Also is the secondary structures. And then we first convert this secondary structure to the adjacency matrix. And then we use the grub transport. We use the graph transform to trying to extract the graph information from this adjacency matrix and then put it actually in a real vector space, which is represented as S. And then this scattering transform coefficients are input into the modified semi-supervised autoencoder. And this autoencoder is augmented. Augmented by three different by three domain specific velocity terms. For example, we incorporate the first passage time and also we incorporate the graph eddy distance and the free energies. Okay, so after we train this variational autoencoder, then we input all of the data to this autoencoder and get their embeddings. So their embeddings have a much lower dimension. Have a much lower dimension as the scattering coefficients, but it's still quite a deep type to visualize. Then we just put these embeddings to further dimensionality reduction methods such as the PCA and the fade. And then after this step, we have embeddings that have two dimensions or three dimensions. In my study, I use two dimensions because I think that's Because I think the 2D visualization will be more clear than the 3D visualization. So, yeah, after we got these 2D embeddings, then we could do some downstream tasks. For example, we could visualize it. Also, we could do some like clustering method and try to find some tracing state. Okay, so this slide just shows the This slide just shows the output of the multi-strand. And we could say that a multi-strand actually produces three different components, output. The first component is the down-present motion. And the second component is the cumulative time of the reaction. And the third component is the free energies of each secondary structure. Okay. Wow. Alright, so to invalidate our model, we did a few case studies due to a timeout. So in this talk, I will only show one interesting study, which is from the Shanghai-Aus paper. So it's a helix association reaction. So the two DNA strands are shown here. And the initial structure of the strands are just unpaired unpaired suffering strands and the final structure is the fully paired helix structure. Is the fully paired helix structure. So, in this direction, so this has a high possibility to form like four stem curriculum structures. So, I could show here is a four-stamp curriculum. And this four-stem curriculum could impede the tapping process. Therefore, it will slow down the overall reaction process. So, yeah, so here's the result of my visualization. So, in this So, in this embedding, each dot or each circle represents a secondary structure, and the color is the free energy of that secondary structure. And also, I label the initial and final secondary structures in this green bulb. And this colorful curves represent the different trajectories. Okay, so yeah, a good visual, a good embedding should have a good preserve, the local and global structure. So from my embedding, we could say that the state distribution actually follows quite well with the high to low energy trend from the initial state to the final state. This indicates that you might actually preserve have a well-preserved global Uh have a well preserved uh global structure uh uh uh global structure. And also, uh, because I said uh I we designed the interactive uh plotting tool to show the embeddings. So once you uh mouse over each secondary structure, it will show some information of this secondary structure. For example, it can show dot principles notation, also it can show the coordinates of this uh structure and the uh free energies and their time and their type. And their time, under type, etc. And so, by using these interactive features, we investigate the standard structures and their neighbors. And we found actually the standard structure and their neighbors have very similar have very similar have very similar structures, which which means they also feel good, like a local. With a good local structures. Okay, so yeah, also from this embedding plot, we could say two different branches. So after investigation of these two different branches, we found these two branches actually reflect two different erection mechanisms. On the top of the branch, it reflects the base pairs forming from the outside to the inside. Forming from the outside to the inside, and on the contrary, like in the bottom branch, it's forming, like the base pair is forming from the inside to the outside. It's very interesting. So also, so after we lay out the trajectories, like these colored curves, we found is, okay, so this curve looks very smooth, so there's no big jumps across over this curve, which means our embedding is also very smooth. Embedding is also very smooth. And more interestingly is that actually, we did not put the real trajectory information into the variation of autoencoder. So, which means I did not put any ordering information into this when I'm training autoencoder. Right, so yeah, but but the the results it shows that, okay, it's p quite smooth, so which means it do capture uh some semantic information. Some semantic information. Okay, so also from this two trajectory plot, we can roughly say like three dense regions, right? It's kind of dense. Maybe this one is not that adverse, but because actually we have many trajectory samples and other trajectory samples, the secretion is also very dense. Okay, so to study these three decimations, I come up with an idea. So, first, I just try to filter out some important states of this embedding. And then I use the unsupervised learning. I use the DB scan to try to cluster the filter, try to cluster the reduced state space. And then it gives me three different clusters. Different clusters. And then next, I write the local minimum of these three different regions, and then which can be considered as energy traps. Okay, let's just say in the blue region, this energy trap has like two force diameter teams in both trends, which is consistent with the previously published paper. But instead of this force-dimensional, Of this four-stem heroines, we also actually wear the three-stem currents, and these three-stem currents actually is not in the very different conditions of these four-stem care pins. And by start analyzing their reaction, their first passage time, passing through these two different regions, we also find like these two regions, these two structures can be treated as the connective tracks. So, this is the new funding. Because of the So, this is the new funding. Because of the in the red region, it has two three-stamp proteins, and in the bottom region, only have one stamp protein. So, this one is much stronger than this green match. So, we treat the red one as one of the major connected traps. Okay, so in summary, so we developed a visual. So, we developed a visualization method based on the deep growth embeddings, which could help domain experts and practitioners to have better understanding of the reaction mechanisms. And we also did many qualitative and quantitative studies to compare our result with the existing results. For example, the GSE, the FAIT, the TCA and TCNU map and the multi-dimensional scaling, and they all actually failed compared with ours. Compared with ours. So, there's also some limitations in my method. First, is that the current model is restricted by one or two strand interactions. So, as I talked about in the first slide, the three-strand interactions is also very important. So, maybe in the next step, we are trying to expand our model to multi-strand interactions, like more than two. And also, our model. Than two. And also, our model cannot deal with the student of problems because of the simulator we used. The simulator just cannot deal with that. Okay, so finally I would like to thank you my supervisor and Kwong and also I would like to thank my co-workers John Dan and Ho Ye and thank you my collaborators Eric for their help for his project. So, any questions? You mentioned that your embedding is more smoother than these other slides and losers. So, my question is: could you enforce, could you also include a continuous prior in your embedding to force to explicitly optimize for that module? You mean I use a prior to force the training process, right? The training process, right? So that your latent your embedding is constant. So it depends on what kind of trial you would like to use, right? Well, if I do because you have time ordering data you put extra. Oh, I mean I put the time information, right? Time series. Exactly. Yeah, actually yeah, yeah, that's cool. Actually I already I think incorporated the time information, but it's not directly incorporated time. not directly proper time. I in this, right in here, actually one last term is called first tensor time distance, right? Actually, this term is just a neural system. Oh, I know what you mean. Yeah, yeah. Yeah, I think it's good to try. Yeah. Thanks. Any more questions? Thanks, Dan C. 