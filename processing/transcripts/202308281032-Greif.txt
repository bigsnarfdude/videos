Change the program so it reflects that, but we all know now. I only change most of it. The only thing you can't ch we can't change ourselves is the copy program. That's a quick conference. And then by serendipity, or probably good programme design. I think you've been set up for your talk by the previous one. Fantastic. Thank you, Petros. Yeah, so thank you very much. It's wonderful to be here. Very much, it's wonderful to be here. I had the shortest trip to here, mainly of everyone. I can't complain about Jetleg or anything. So it's great to be here. So this is work with my former student, Susanna Bradley, and my former postdoc who just started a faculty position at Houston when we had. And as David said, a wonderful setup because Petrostop did a wonderful job. Wonderful talk. And in a friendly way, I want to almost say that I advocate for the opposite. You know, keep the large systems. So maybe we will get to that. So this is the topic of the talk. This is double cellophane systems. There is many of us in the audience who have worked for many years on smaller, on this part. Saddle point, maybe the classic saddle point, maybe with a D equal to zero. Point, maybe, with a d equal to zero, and there's a lot of expertise that has been accumulated throughout the years. So, this is the setting with n typically greater than n, typically greater than t, and d and e might or might not be zero. But when they're not zero, that makes things more interesting. So, this is the setup. And just a little bit, there's a lot of linear algebra Linear algebra issues here to discuss solvability and things like that. I'm going to keep it simple at this point, assuming that A is symmetric positive definite, and D and E are semi-definite, and then that B and C have full row rank. And that's really not necessary for solvability, even. You know, if you want to really get down to the business of when this matrix K is invertible. This matrix K is invertible, then there will be all kinds of things you can do beyond what I'm just assuming here. But for the purpose of analysis, this is something we need. So there's many, many relevant problems and applications. I wanted to make sure that nobody blames me for not being up-to-date. So I added in the last comment an up-to-date comment just to make sure. Just to make sure, you know, this is an August 27, 2023 talk. And so there's many interesting problems here that do take that system or the structure. And obviously, there's no time to talk about all of those things, even though they're interesting. And there's also growing work on eigenvalue analysis and preconditioning. And preconditioning. So, you know, really in the last five years, we've seen some significant interest in this business of analyzing the eigenvalue structure of this particular matrix. And this is part of what I'm going to talk about here in this short talk. The other thing I wanted to mention is that there is, this is within something called This is within something called multiple saddle point systems. So, what I've introduced is really the 3x3, but you can look at a bigger block tridiagonal structure. There are several interesting applications that could be put into that framework, maybe by permutation of the blocks. And here I'd like Blogs. And here I'd like to mention a couple of interesting pieces of work. One is Sogan and Zulener. They really did some beautiful work, actually, based on continuous analysis in hilbert spaces that actually is incredibly useful for us as well. And there's recent work that John was involved with that I think is important because what they do, I will not have time to discuss that. I don't know if you will talk about it, but. Will talk about it, but what they do is they design preconditioners taking into account the number of blocks, and they actually come up with preconditioners that do not depend on the number of blocks. And I think that's very important and useful. So going back to the 3x3, let me just do the thing that we always almost do when we see those matrices. We just decompose them. And because of symmetry, we're going. And because of symmetry, we're going to assume that, or we do have an LDL decomposition. And as we do often in subtle point systems, the classical, the 2x2, what we do is we kind of look with the magnifying glass on the D matrix, the one in the middle right here. And then what we do is we say maybe that's a candidate for the condition. Candidate for the commission. The interesting thing about this, though, so you know, we can then ignore signs if we want causative definiteness. So, I will not be too much concerned with minus S1 or S1. The thing that is really important is to see that S1 is just like a standard true complement. We see those all the time. S2, I'll call it a nested true complement. It involves an inversion of S2. An inversion of S1. And that really shows the difficulty as we move down, especially for bigger systems. So, this does require a lot of attention here. By our assumptions, they are positive definiteness, and we do have an inertia preservation. And so we know how many eigenvalues are positive and how many are negative. So this point is the point that I made. This point is the point that I made. Diagonal matrix D can be used as a block diagonal symmetric positive definite condition. So let me just talk a little bit about Susanna's work. She finished her PhD last summer and we did some work on finding eigenbugging bounds. There are a few analysis techniques that I'll just point out very highlight here. I'll just point out very highlight very very quickly there is a classical really a seminal paper of Rustin and Winter from 1992 where they used energy estimates for the classical cell phone systems and those energy estimates are very useful for us here except that they're a little difficult to use sometimes especially for interior bounds and so but they are quite effective they're There is, I mentioned the work of Sogan and Suleiner. That's kind of like a nice little guideline which is very nice and actually simple to understand, that allows us to obtain bounds on extremal eigenvalues. And there's all kinds of other techniques. For example, if you don't like how to find interior eigenvalues, because they're hard, invert your matrix, turn them into exterior ones, and take it from there. Once and take it from there. Of course, by preservation of difficulty, you can be sure to be hit by something else. You can also partition into your blocks and then say, well, I know how to deal with two by two. And maybe go from there. But you know, this business of partitioning is actually something that I would like to think a little more carefully about, because we can always do that. We can always do that, but we also need to tell ourselves: but wait a minute, there is a structure here. Can we respect the structure? And for PDs, at least it's easier. I mean, Petros and I chatted about it right after his talk, and I think that's related to a comment made by Howard in Petros's talk. For constraint optimization, often you have an algebraic thing, and it's hard to know what operators are underlying there. For P D's, often you will have something that is very meaningful. Something that is very meaningful in terms of the operator. So here are just a few reasons why to try to keep this regular. First is E might be rank deficient. So you know, A is nicely positively definite, E might not be. It's just kind of a regularization term. So you can't necessarily be sure that you will have a non-singularity there. That's one thing. The other thing that might also be interesting is that B and C could have a particular meaning. They could be for Have a particular meaning. They could be, for example, a discrete divergence operator. But I don't know what B and C together are. And that's actually something that, or maybe it is some kind of an aggregate discrete divergence, but it doesn't have the same meaning. And so we hope that the analysis for the 3x3 would give us some good intuition and actually things that we can do, we cannot do otherwise. Here is a technique that again is kind of inspired by the nice paper of Sogan and Zuhlener, just basically saying, look, you can do something that is a little bit like LFA, if you wish, in multi-grade, just take the extreme eigenvalues and single values of all these blocks here. The main ones are square and the half-diagonal ones are rectangular, and then generate a small matrix. It would be 3 by 3 for those matrix. Three for those matrices that we're looking at, and then under some conditions that are a little delicate, you know, because it might have complex roots and things like that, but under the conditions that I stated, we know where those eigenvalues are, we know they're real. So that's actually useful of that 3x3 tiny matrix. So here is a slide that should never be presented in any talk ever. So what are those bounds? Those bounds amount really. Bounds. Those bounds amount really based on this to cubic polynomials that require that are involving extremal singular values angular. There's no quiz about this slide. And basically, if we do that, what we can end up with is just expression for the eigenvalues of the bounds. Now, some of these must be reminiscent of the work of Rustin and Mitter. For example, the minimal eigenvalue. For example, the minimal eigenvalue. The interior ones are the hard ones. The minimal eigenvalue positive eigenvalue must be related somehow to what they found because all they did was a similar thing for 2 by 2. And indeed, lambda mean of A appears here. So, and then the other side of it are those things that are related to the roots, the positive or the negative roots of these QB. Of these cubic polynomials. So, one question that we want to ask ourselves now is: how do we precondition if we keep that structure? So, what we're going to do is we're going to go back to that thing that I mentioned earlier, and that's really important for us to say. This is what we're going to assume: positive definiteness, non-singularity. We can invert A, we have S1, which is that regular sure complement, and we have the nested sure complement here. The true complement here. Okay, and the question is: how do we do that? Okay, so there are some results. So, as I said, there has been quite a lot of interest in the last five years on things. It starts with something that is very easy and to a certain degree is an extension of the Murphy-Golov and Waten result, and that is that if you assume that D and E are zero, and again, I want to make sure that everyone is with me in terms of all these better specifications. These edges, if you assume this is zero, then you're back to something that is very similar to your usual settle-point system. And then you have precisely six eigenvalues, and non-zero six eigenvalues. They can be fully retrieved from the analysis. There's a paper by Kaiju and Li in 2021 that says that, and basically, it means that in the absence of random ferrors, we will converge in. Of round of errors, we will converge in six iterations to the correct answer. When d and d are not zero, that's where it starts to be a little difficult. And there's no need for us to read all this, I think, but this illustrates what happens. We're still bounded by those regions of those six, and all of these numbers here are actually cosine. Numbers here are actually cosines, and that's actually going back to John's nice work. Even when you go with four blocks, you still have cosines lying around all over the place. All of these are analytical numbers. This is, of course, the golden ratio. And you see that there's a little bit of smearing here, but there's still some nice clustering, which is important for corallops all over. And similarly, when D and E are not zero, both of them are not, so we can just. zero both of them are not so again just to make sure here is an example of d zero and e is not if the opposite the converse d non zero and e is is not that interesting actually and so that's this is what we have and we still have that boundedness away from zero the eigenvalues but we really lose a lot in terms of plastron so so again there's there's a lot of analysis that can be done here A lot of analysis that can be done here. One of the things that we were set out to do, and I do not have time to talk about it in length here, but what happens when you approximate the 1,1 block or the operation of the inverse of the 1,1 block and the sure complement, so then the assumptions would be that you would have intervals of eigenvalues instead of just the exact eigenvalues. Of course, alpha is 0 is equal to beta, 0 is equal to 1 if a tilde is equal to. To one, if a tilde is equal to a, you can still get some bounds. And that's actually important and depends on the analysis. So, let me, in the remaining moments, just talk a little bit about a particular thing that we've been interested in, the work that we did with Bunhi. And let me just go to a Stokes-Darcy formal equation. So, in a three-bar In three bar in a primal form, this is really Stokes coupled with Darcy. There is an interface, think of free-flowing water going all of a sudden heating sand, and there is an interface. You can think of that as like a boundary layer where things happen very quickly. And there's a couple of interesting physical parameters. One of them is the viscosity coefficient, which you see in stone. Which you see in Stokes, and one of them is this hydraulic tensor coefficient, which is actually an entire small matrix, but here we're just assuming that it's one parameter. And this is really Stokes and Darcy coupled with a few interface conditions. And some of them are interesting. You know, this BJS condition is an interesting one. Interesting one. And I think that if you look at it, you can also start suspecting that it will cause trouble for loss of symmetry, which indeed in our case at least it does. Sorry, where is the coupling? Pardon me? Where is the coupling in this system? There is no, you're absolutely right about it, but that is because of the way we write it, is we actually throw in the interface variables into the Stokes equations. So it doesn't appear there. So it doesn't appear there. It's there. The interface couples those things, but the way we do it is different. Is throwing two things into one. So what we've done is marker and cell discretization, which I like because I can understand better than finite elements, slightly better, not fully. And in this formulation, we do use for the velocities there. They're on the edges. This is the Stokes, this is the Darcy, and the U and the V are in different places on the edges. This is just standard discretization, Mac, and then we follow a particular discretization. Again, interesting, a lot of interesting discretizations as well, not only interesting linear algebra in this formulation. Now, here is how it looks. Here is how it looks. That goes back to your question, Howard. What we're doing is we're just throwing in all the interface equations into the Stokes. And as a result of that, we lose symmetry, even though in the module I show some eigenvalue structures. And then we have some kind of a fairly simple interface matrix G that arises from that decoupling. Arises from that decoupling because we have two domains. I hope that's clear. And of course, we have more details on this in our paper. Here is an interesting eigenvalue distribution plot. I hope that everyone can see, but maybe not. I apologize for the small. This is changing the nu and the kappa. These are the two physical parameters. And what we're seeing here, so this is all for n is equal to 32. Is equal to 32. It's a two-dimensional problem, so it's not very large, you know. It's about 4n squared, the size of this system in general. And what you see here, I want to just point out what we're seeing here because it's kind of interesting what happens. So first of all, the eigenvalues are complex, but this plot is a bit misleading because if you look at the scale, this is the imaginary part, and this is the real part. So there is a 10. Real part. So there is a 10 to the 4, which again I hope people can see here. And there's O, this is O of 1. So these are just mildly non-symmetric problems in terms of the eigenvalue distribution. And the interesting thing is that as you change, so what changes here is the nu and the kappa. And in a funny way, or not funny, when they're close to each other, typically we might have an easier time. I'll show some results. Easier time. I'll show some results later. So here nu is 1 and kappa is 1 all the way here Î½ is 10 to the minus 4 and kappa is 10 to the 8 and it's kind of interesting to go back on. It's hard to see that because what happens is that those the interface let me just show it here. What we do is we What we do is we you see this is only for the V velocities and not the U and this is leaves also on the interface. So in the blocks which one is not the same as the branch ball so the A S is non-symmetric. The A S here even though you would think you would think that A S is just a Stokes operator but it's not. It's in fact non-symmetric because of those V velocities on the interface. V velocities on the interface, and that really makes it very difficult. I mix that line. Yeah, yeah. So then, so then it's kind of interesting. There's also, even you know, in those situations here, as new becomes a little smaller, then you have fewer, even that small imaginary part disappears. And then you also have something here that is interesting: is that the real part becomes a little more. The real part becomes a little more positive definite as you go here. So, again, I hope everyone sees the numbers. This is from, sorry, I said positive definite right here. So you see here, here it's from minus 1,000 to 9,000. Here is from minus 100 to 100. So basically, we can characterize as a business of the connection between new and kata how positive, definite kind of Kind of this operator is. There are two things. Because you could consider this matrix as a low-round perturbation of a symmetric inductance problem. That's right, and that's... That's right. And that's part of the reason why I'm not too concerned about, first of all, eigenvalue analysis as opposed to fiddle values and things like that. And I do use GMRES because I cannot use minres, but I do agree with you that it's. But I do agree with you that it's kind of a perturbation in the end of the day. Now, I don't know about low-rank, there's a lot of them, but it's just small, and I think that's okay in this regard. Yeah. If it's on the interface, you should be able to count around. That's right, so there's a lot of inertia considerations, and that's also something that is very true. What you are just saying, Valeria. I do have a slide that actually. A slide that actually shows a little bit what the eigenvalues are, and we know what where most of them are exactly because of that. And there's only very few we don't know where. So, you know, preconditioning is also very interesting, and again, some very nice work recently. And, you know, there are symmetric formulations, by the way. I just want to make sure it's clear that in our MAC formulation, we have non-symmetry, but there are cleaner formulations. Cleaner formulations that are not necessarily easier to handle. Okay, so for example, there's a very nice work of Candantra Mardal and collaborators where they actually do have symmetric formulations, but they use fractional Laplacians to do that. So there's a lot here. You know, this is one particular choice of doing things. Now, the things that we're, I think I'm close to the end, and I hope to maybe I'll run through the last few. The block print. The last few. The block preconditioners are those. Again, this is kind of a nearly symmetric thing. What we're doing here is we're just deciding we can also do something not symmetric plus the problem is not symmetric. The G is that interface matrix that I mentioned, and B is just the discrete divergence operator. And again, we have those sure complements. So here is something that goes back to Back to inertia and other things. We use all these things here because it's so close to symmetric. I don't want to start reading this slide, but basically we know where most of the eigenvalues are. We don't know some of them, and that's kind of related to the size of the number of interface variables. And I am not sure exactly how to do that, but we have a good hang of it for this M1, the block diagonal. M1, the block diagonal, and then there's some additional observations for M2 and M3, which I think we can skip. Let me just finish the talk by showing a few things that are interesting here. So here, what we're doing is we're fixing the viscosity coefficient. We're using an approximate version of this M3 here. So sure complements plus those Those blocks as they are. And what we're seeing here is something that we hope is kind of okay, but it's also not okay in the way that we know happens a lot in fluid dynamics, for example. And that is that as we start pushing our, in this case, the hydraulic constant to be very small, if we look at columns, we have scalability, but we don't have that entire scalability here. You know, we go up from eight. Here, you know, we go up from 18 to 61. Here, that's not very good, but at the very least, scalability with respect to the mesh and a reasonable increase here. And that's related to these eigenvalue things here. Every setup is different in terms of the eigenvalue structure. And we see that if I go back here, we see that also in these results, when we make new smaller, We make new smaller, and that's closer to Kappa, really, then we improve a little bit on that scalability, even though again there's a little bit of a difficulty there for 10 to the minus 8, which apparently is somewhat realistic physically, but a little smaller than what it should be. But then, when we really make them even closer to each other, then we see a very good scalability in terms of just a fixed number. Fixed number of iterations. Okay, so just a departing thought, and again, closing the loop on Petros' talk, I would like to find a way to take into account the structure of the double point system without partitioning or without reducing. It does give us the ability maybe to think about preconditioning, keeping those. About preconditioning, keeping those blocks large, maybe keeping them a little well conditioned without performing reduction that might compromise on our understanding of the operators for one and things like that, or conditioning. And then just try to work on these bigger problems. And that's maybe the thing that I'd like to, I think that there's a lot of work for all of us to do in the next few years. The next few years. So that's it. Thank you very much. Questions? Anybody online would like to ask a question? I have a question on the. I mean, if this model problem is interesting, I mean, I see. Problem was interesting. I mean, I see that you have a complete algebraic version for these double or second-point systems. Problem is interesting, and I saw you go to final discontinuities. So is there some possibility to reorder these interface in VCs? Very beginning you have the two times the SP blocks, then sort everything that destroys the symmetry of the market. I think there's a lot of possibilities here. One of the things that I'll mention is that permutations of also, as I mentioned, conservation of difficulty. One of the things that, at least in terms of analysis, it's a little easier for us to have banded matrices. I don't think that anything is impossible. I just think that I do agree with you though. We should think about where to put those things that destroy symmetry. Things that destroy symmetry. And maybe from a practical point of view, we can then design a better solver. Then, from an eigenvalue analysis point of view, it might be a little harder. I think there will always be some difficulty, but totally worth pursuing. Just to give you a little personal perspective, of course, this arises in simpler systems to open system. And it became pretty clear to me early on that what I wanted to do is preserve. Early on, that what I wanted to do is preserve symmetry and preconditioning. I wanted to use MinRes because I've got a lot of faith in MinRes, of course, there's a lot of faith in GMRES, but there is a computational overhead of GMRES. I use GMRES20, so again, you've got an extra ingredient to restart browser. In Res, you don't have such a thing. So you have to. So I would go to talks and talk about you've got to preserve symmetry. You can do it. You can just discretize differently and find help. You can just discretize differently with fine elements, you don't have this symmetry. Agree, but then people would always come up to me and say, Well, that's not quite the problem I wanted to solve. I'm an industrialist, I want to add this thing, and it makes it non-symmetric. So I kind of felt I was fighting a somewhat losing battle because, in the end, they always wanted to change the system somehow to make it non-symmetric, so we might as well have started with this. So I've got a balanced view on this now rather than being angelic about keeping the symmetry. Keeping the symmetry. I appreciate that, and I feel the same way. You know, I'm a big fan of Mika Reyes. I always agree with me too. And I agree with that. I think we always say symmetry, symmetry, short returns, all that. But in the end, and it depends, you know, 20, maybe we do have 20 vectors out there. Okay, so can we go to the next call? Thanks. Thanks, Fred. 