Estimating topological and dynamical invariance given a sample of a topological space or dynamic. He's particularly interested in how graphs and dynamics on those graphs can be used to estimate the topology of metric spaces. And he draws on tools from homology theory, homological algebra, sheaves theory, which is pertinent to today, and specialty opportunity. Okay, the floor is yours. All right. Well, thanks a lot. Thanks very much for the introduction. I'm sorry, I wasn't able to go in person, but it's good to be there, at least virtually. To be there, at least virtually. So, I'm gonna. I was asked by the organizers to give a very general overview talk of what Hodge Theory is and maybe a few applications or example applications. That's what I'll do. It'll be a short-ish talk, maybe 40-ish minutes. So, not the full hour. But let's get started. So, as I was thinking through, so we'll start out with just kind of generally what Hodge. Of generally, what Hodge theory is, and then how it gets realized in both the finite situation of simplicial complexes or finite symbolic complexes. And then also, I'm going to look at a little bit what happens on manifolds and some of the differences that arise. But then thinking through what kinds of applications I should talk about, I thought I'd try to highlight the things you can do with Hodge theory that you can't just do with. Theory that you can't just do with cohomology. So, the one thing which is topological that Hodge theory gives us, which simply cohomology doesn't, is it gives us a preferred representative. So we have the harmonic representative of every cohomology class, and that can be very useful. But then there are non-topological things that are very useful. So the Laplacians and the Hodgelands. Useful. So the Laplacians and the Hodge Laplacians contain a lot of information about minotaurics or geometry, depending on exactly what you're working in. And so I'm going to use that idea a little bit. Now one of the my favorite illustrations of that is something Yuan Yao talked about a little bit yesterday, which is his ranking problem or a HodgeRank application. And then finally, I want to look at an example. At an example, this is calling this Markov process maybe a stretch, but I want to look at examples of how we use the spectral data in the Laplacians to do other things. In this case, it'll be dimension reduction, looking at exactly what happens with Laplacian eigen maps and diffusion maps. And in this, I want to, there's a bit of an industry of Of papers explaining what these things are doing. But I think for me, one of the nicest explanations was actually given by a paper before these came out at all. It's not really an explanation of what they do, but it's an explanation of why we should expect what they do to be reasonable. So I want to talk about that. And I don't think it's better known in differential geometry than in data science. So let's start. So the general idea of Hodge decomposition. So, the general idea of Hodge decomposition is really just algebraic. You have a vector space that's graded and an inner product. And then I also want a differential or codifferential in this case on the vector space. So the square is going to be zero. And then I'm going to take the adjoint of that codifferential. So now I have, it's going to turn out it's a differential, but you know, I have the adjoint. But I have the adjoint. And I define this operator, motivated by differential geometry, but you can just define algebraic. So the Hodges decomposition, which we all have seen a few times this week, is that any given grading or at any given level of the grading in my vector space. In my vector space, I have a decomposition into the kernel of the operator, Laplacian, the image of the codifferential, and the image of the differential, right, or of the adjoint of the codifferential. And on top of that, we have that the kernel of the Laplacian is isomorphic to the homology. So, when everything is finite dimensional, this is This is a purely algebraic set of observations and actually very simple. And I want to go through them. Actually, I'm glad Yuan skipped this part of his talk yesterday because I would not have much left or a large part of my talk would be gone. So let's just go through and see what happens. So the first observation is that the images of these two operators, the codifferential and its adjoint, are orthogonal. Are orthogonal, right? And that just comes out of the fact that, so let's look at something in the image, and the inner product was something in the image of the other. This ends up being the differential twice, which we know is zero, and that's true for everything. So it's always orthogonal. The kernel of the Laplacian is the intersection of the two kernels. So first, just by First, just by the easy inclusion is just looking at: well, if I have something in the intersection of these two kernels, then everything in the definition of the Laplacian disappears. And then the second inclusion you can see from this, so just consider this inner product, right? And take W in the kernel. So that's going to be zero. But now, this is, I split it up. This is, I split it up, and I consider these. This gives me these two inner products, which are in fact the norms. And the only way that can be zero is if those two things are themselves zero. So I started with something in the kernel of the inner product, and I ended up with something in the kernel of both of these operators. Sorry, something in the kernel of the plus. Okay. Okay, so let's keep going. Now, what else do we need? We want to have this whole decomposition. So, I want the image of one of the operators to be orthogonal to the kernel of the other. And that's true just by again by the adjoint. The reverse getaway is also true by the same thing. And now, you know, we have. You know, we have that the image of this thing has to be the image of the Laplace, rather, has to be the direct sum of the image of the two operators that make it up. So, and that gives us the whole decomposition. All right, so this is a very simple set of observations, but I've assumed a couple of things. So, one, I've assumed that all of this. I've assumed that all of this makes sense, right? Which isn't always the case. So, in finite dimensions, oh, wait, what did I miss? Ah, right. So, I didn't say anything about homology. All right, so I have this decomposition, and I also know that the image of my adjoint is perpendicular to the kernel of D. And that tells me that the kernel of D is the other bits, and then I can. bits and then I can take the quotient and I get that the uh the kernel of Laplacian is in fact the cohomology of B at that dimension. Okay and finally since these things are our adjoints not only we start with a codifferential but we also end up with the differential. So in fact we can talk about homology or cohomology. It's usually nicer to talk about homology, co-homology. All right. So for finite simplicial complexes, this all goes pretty much the same way. The only thing I have to do that I left out is define exactly what I mean by this V of K and these inner products. So otherwise, the argument is exactly what I wrote. Exactly what I wrote. And so the V's are going to be our. So if I start with the finite cell complex, the V's are going to be the co-chains. And the only tricky thing, it's not even that tricky, is the inner product. So I'm going to pick a preferred basis, which was given to us geometrically by our cells. And then And then you take the inner product just to be the sum. Well, the inner product at a basis element is just going to be the coefficient of my vector that appears in the expansion in the preferred basis. And then you extend that bilinearly, and that gives you linear product. And the Hodge decomposition follows exactly from what I wrote before. Directly from what I wrote before. In the smooth case, it's not quite as easy because immediately we start out with an infinite dimensional situation. And so it's not quite as clear how to, that all of these things, these manipulations we're doing can be can be justified. So we have to be a lot more careful about how to do that. But we can still start in the same way. So our So, our V, this time I'm going to call it C star m. This will be the this time the Duram forms. And here, we have a PDE that gets formed. So, the basic idea is that we're going to set this up as a PDE problem and then use some techniques. And then use some techniques from PDEs, particularly sobo spaces and some Hilbert space theory to make sure everything we're doing is justified. So the first thing we have to do, though, is define the inner product. In order to define the inner product, we're going to define what we call a star operator. A star operator is a funny thing. So I have Anything. So I have exterior algebra, a vector space. So let's forget about the forms for a second, just focus on one vector space. So if I have an exterior algebra, I can take a basis of that, or orthonormal basis, and take star of one is going to be the wedge of all the elements in that basis. And note that up to sine, it doesn't really matter which. Up to sign, it doesn't really matter which basis I take for this, but um, and then the star of anything, the star of the whole basis is going to be plus or minus one, depending on the orientation of basis relative to the vector space. And then the star of anything in between, anything with dimension or with a number of basis elements in between zero and n, is going to be n minus p parts of the orthonormal. Parts of the orthonormal basis that I didn't consider that are not in the argument. And then plus or minus one, depending on if when I multiply, you know, when I take the wedge of the first bit with the second bit, I get a form which agrees or disagrees with the orientation again of the vector space. And you know, that's just another way to say that. So that's the star operator. And now I just explain. Star operator. And now I just extend this linearly here, or multilinearly, I guess. And then look and extend it from single vector spaces to forms, to differential forms. And now to get my inner product, what we're going to do is say that the inner product of two p forms is going to be the integral over the whole manifold. Manifold of the first form wedge, the star of the second form. So this now gives me the first form, remember, is a p form. Star of the second form is going to be an n minus p form. So the wedge is going to be an n form. I can integrate that perfectly well over the manifold. And that turns out to give us an inner product. I'm also going to define this operator a priori without knowing that it's going to turn out to be adjoint. But I'm going to say this, the delta of I'm going to say this: the delta of p is going to be star d, which is my codifferential in the Duram cohomology, d of n minus p, dimension n minus p, and then the other star. So it's star d star, and then this correction or the sign correction term, where you just put in the exponents you need to make it work. And it turns out that this delta, the starty star with the sign, is the adjective. The sine is the adjoint of my Duram codifferential with respect to this inner product. Okay, and now once we have this set up, we can define the Laplacian the usual way, just d delta plus delta D, and it's self-adjoint, particularly by the definition of the star up there. All right, so what are we looking for? Now we're looking for solutions to this differential equation, which is now, instead of being a matrix equation, it's a differential equation. And we're going to do the standard PDE trick of setting it up in a weak formulation and then finding a solution for the weak formulation. Finding a solution for the weak formulation, which I'm going to sort of sweep under the rug, and then say that weak solution is in fact a smooth solution. But I'm going to skip most of that, but just give you a little bit of an outline of how it goes. So how do we get to the weak formulation? What is that? So we have this inner product, right? And if we have a solution, so now then this inner product, we have this. This inner product, we have this nice equality, gives us this for any other form. And now I can just take the adjoint. Actually, it's the same operator. I don't even need the star. And this again gives us the same inequality, or the same equality. Okay. But what this tells us is that if we have, if w is a solution, this defines a This defines a banded linear functional from the forms to R, which is that, such that it has a special property, such that the bounded linear functional at this vector, well, whatever phi is, is equal to the bounded linear functional given by alpha applied to that vector. Okay, and this tells us pretty much what. Pretty much what a weak solution will look like. So we just take that equality and say, declare that a weak solution of our equation is going to be a bounded linear functional that has exactly this property. And the first big theorem is that a weak solution is a smooth solution. So here we have. So here we have here's our alpha. L is a weak solution, so it's one of these bounded linear functions. And what we're saying is that there's a representative in the smooth differential forms such that the bounded linear function we have is represented by this smooth differential form. And it follows from that that, in fact, this differential form is a strong solution. Differential form is a strong solution. Okay, so this is actually relatively standard PDE stuff. It's not easy, but it's, you know, you've taken a long course and graduate course in PDEs. Most of what you need was there. And then a second theorem, which is a little bit maybe less usual, but still not so unusual, is that if we have the sequence, this. That if we have this sequence, it's a little bit more technical of smooth p forms, and I'm going to bound this. The sequence is going to be bounded, also, the Laplacian of the sequence by the same thing, by the same constant. Then there's a sub-sequence that's Cauchy. So that's a theorem. This is not an easy result. It takes a few pages to work through. But we're going to see how to. But we're going to see how to apply that in a second. So, our Hodgs decomposition again looks the same, or at least here looks the same. And we're going to see how these two theorems can be used to get there. So, the first thing we have to do is, we didn't have to do before, is show that the kernel of the Laplacian is finite-dimensional, right? Because we have this finite-dimensional manifold. Have this finite-dimensional manifold, we eventually want to identify the kernel of classing with the cohomology. It would be disaster if that ended up being infinite-dimensional. And so, that's where we use the second theorem. So if it were infinite-dimensional, in the kernel, there would be an infinite orthonormal sequence, right? But by this, what's an orthonormal sequence? Well, the first bound is the sequence itself. The norms are bounded by one. Itself, the norms are bounded by one, and then since they're in the kernel, these guys are actually equal to zero, but it's also bounded by one, so it's good enough. Um, but then by the theorem, we would have a subsequence that was Cauchy, and we can't because so that's a contradiction and it's finite dimension. Okay, so now the one of the directions of this inclusion. This inclusion that the image of the Laplacian is in the is orthogonal to the or is actually equal to the orthogonal complement of the kernel is easy. It's exactly the same as what we did. The other side is difficult. Okay, so here, you know, this takes a few pages of analysis and uses, you know, again, not pretty standard. Again, not pretty standard results in analysis, but still not trivial results. So, this is where the main difficulty is combined with the bit about being finite dimension. Okay. And then finally, this gives us the decomposition, but we don't have quite as automatically that the cohomology is also isomorphic to the kernel of the Laplacian. So there you The Laplacian. So, there you need a little bit as well. So, we're going to define this operator, call it the Green's operator. So, we're going to go from the forms, p forms, to the orthogonal complement of the kernel. And we're going to define it this way. We just take about everything that's not in the orthogonal complement and, you know, or rather, we take our form and then take away the bit that's in the kernel, and that's what we're left with. That's in the kernel, and that's what we're left with. Is the uh uh this is not right, this should be g I know it's the unique solution of it, yeah, sure. Okay, um, right, so first we take away what's there, and then we look at the solution of Laplacian that's given by this equation, all right, and so this is well defined, and it importantly commutes with our operators. Operators that we started with, and that's kind of enough, right? So, once we have that commutation, if we start with a form, then I write the form by the decomposition. So I happen to know, so H is going to be in the kernel. These guys are going to be in the image of delta. This is the image of D. And then I just flip the G here, right? Right, okay, so that's for any form. Now, if alpha is a closed form, then this disappears, and you get this, and so that's exactly some cohomology class, and h of alpha is a representative of alpha in this cohomology. All right, and it's unique, you can just, it's a pretty standard argument, but just follow the inner products to find the. Uh, the inner products to find that you have something unique, okay. So that's that's more or less uh how the Hodges decomposition emerges for the smooth setting. All right, so now let's let's look at what can we do with this. So, one is what happens when what kinds of representatives are the Are the harmonic representatives? And I was surprised not to find more about this. So, if people know other references that give special properties of harmonic representatives of combinatorial liplastics, I'd be very interested to see that. But I did find this one. And what this says is that the harmonic persistent cycles, right? So, remember, persistent homology is given. Remember, persistent homology is given by considering a filtration, looking at the homology of everything in the filtration, and then looking at how elements in the homology at one point in the filtration persist to another point in the filtration. But since at each moment in the filtration, anyway, we have harmonic cycles, we can now talk about harmonic cycles of the persistent homology. And what Busu and Cox do is they Do is they define this notion, which I'll define in a second, they call content. So, the content of a cycle is basically the proportion of a representative, which is made up of simplicities that have to be in that representative if it's going to be, you know, representing that. Is representing that homology class. All right, so let's see exactly. And what they show is that the harmonic persistent cycles maximize this measure, this content. So let's see exactly what this is. So I'm going to talk about bars in a persistence or a barcode. So B is a bar, a barcode of persistent homology. And then I'm going to, these are my essential simplicies. So there's a simplicity. Simplices. So there's a simplices that have to be in any representative of the bar. This could be empty in general if your filtration is sort of not, if bad things happen to the filtration, but in general, it's not empty. And then let me look at any representative, and we're going to define the content in this way, which is exactly what I said. So the proportion of it's not a Proportion of, it's not exactly the number, but the weighted proportion given the coefficients of the simplices, of the essential simplices over the, or with respect to the weighted proportion or the weighted sum of the of all the simplicities. And so the theorem is that for simplicial complex and a filtration. Filtration, you have some one p-dimensional bar, then for any representative of that bar, the content of the representative is going to be less than or equal to the content of the harmonic representative, which is actually a pretty cool thing. Okay. So that now I want to turn to ranking. Now, I want to turn to ranking. And this, I think, is a very, very wonderful application of what to do with the other bits of the Hodge decomposition. And it's a very elegant idea, actually, as well. So the problem is the following. So I have a bunch of things to be ranked. A bunch of different people are ranking them. And the different people don't rank the same things. So some of them might rank. Of them might rank two, three, and four. Another one's another set, some other, some mixture. And now I want to produce a global ranking of my V using the partial rankings given by all of the individuals in lambda. Okay, so clearly, you know, terrible things can happen, right? Because, you know, like we saw yesterday. Like we saw yesterday with this example, or Ian's example of A, B, and C. You know, one person can rank it this way, another person could rank it this way, and yet another might rank them completely differently. Okay, and putting those together doesn't give us anything, you know, even near coherent. Mere coherent. But particularly using the assumption that most of the individuals don't rank most of the Vs, we can still try to do this, but in two steps. So first, we're going to just add up. Oh, what am I doing here? Oh, yeah. Okay. So first thing we do is we turn the ranking into a matrix. So every individual has. A matrix. So every individual has their ranking, and let's say it's given by some function on the things we're trying to rank. And so we construct a matrix where ij is the difference in this function between the ith and the jth element, and it's zero if the person didn't rank one or the other. And then a weight function. And then a weight function, which is just, again, it's one if there's a comparison, if our individual alpha made this comparison, and zero if not. Okay, so this gives us, moves us from just a function on all of the set to a function on the edges. Basically, it gives us our differential, the codifferential. All right. All right, and so we're going to do this in two steps. First thing we're going to do is we're going to agglomerate, or we're going to put together all of the partial rankings into something that's a giant mess. And that's going to be this y bar ij. And then we're going to do at least squares. Well, at least squares fit using. Using the matrices that are in the gradient now of the of this situation. So let me describe why that makes sense. Is there a question? Okay, so first, let's look at the whys. Okay, so what do we do? We just add up all of the individuals' evaluations of or all the individuals' rankings of a given pair. And then we divide it by how many people ranked that pair. And that gives us our mean Yij. The weights are going to be exactly these things, the sum of all the weights. This thing, the sum of all the weights. If I have so that my edges, the E, are going to be any pair where there's at least one person that ranked it. And then the most important bit is this set. So I want a global ranking. So I want to pick a global ranking that's as close as possible to this average of all of the possible or all of the partial rankings. Of the partial rankings. And so global ranking is the following: it's some set here such that the ijth element is exactly the difference of the ith element and the jth element of some function on the original set of things we're going to try to rank. So I have a function, and the ith element is this difference. Element is this difference. This is also again just the codifferential applied to, um, or you know, a piece of the codeferential applied to element, I think, I or J, depending on how you've done your signs. And so this matrix ends up being in the image of the code differential. Right? And this is exactly the gradient, actually. Actually, or if you look at the smooth setting, this corresponds to the gradient there. Right, so again, all we're doing is taking our y and projecting down onto the subspace that's given by the image of the codifferent. And that gives us a global ranking, which Which minimizes this difference between this kind of collection of partial rankings. You can also, so they did a lot more. Since we have this thing, right, this decomposition. Something nice about this is it doesn't just give you a global ranking, it also gives you, you can project in the other subspaces and get the error, right? So if you have some global ranking, but in fact, most of Y projects to these other subspaces, you can think, well, this global ranking or y itself is probably Itself is probably extremely inconsistent, and maybe there's a bigger problem than just having a global ranking. But if the errors are relatively low, you can say, well, the inconsistencies are small, and the global ranking maybe trusts fairly well. Okay. So now I want to take a look at what one does with the operators themselves. Operators themselves. So, something I realized I didn't do or forgot about, there's a whole also literature about Giger constants and interesting geometric and combinatorial consequences of what happens when the eigenvalues are big or small and so on. But I didn't talk about that here. What I'm going to concentrate on here are these methods for dimension reduction using the eigenvectors. Eigenvectors specifically of the Laplacian. All right, and so we're going to start with Belkin and Yogi because I think conceptually it's the simplest one, it's also one of the early ones. So the setting is the following. I start with a graph Laplacian, or I start with a graph, right? You don't quite start with a graph. You start with a point cloud, you build a graph. Let's say we have the graph. Then we compute the eigenvectors. And I'm going to map my data set to r to the k, where we compute some eigenvectors so that there are other eigenvectors, one, et cetera, to say we have n. But I don't want to use all of them. I want to reduce the dimensions. So I'm going to use the first, say, k eigenvectors. I'm going to produce this map, which sends me. This map, which sends me from my data set to r to the k, just evaluating the eigenvector, or the eigenfunction, let's say, at the point x of my data set and putting that together into a vector in Rk. And so this works. So what is the graph? Some of the something I didn't tell you. So the idea here is that I want. Is that I want this to look like the Laplacian on a manifold. And Laplacian on a manifold, I can write down if I have the heat operator on the manifold, I can write down the Laplacian on the manifold like this. It's the limit as t goes to zero of this thing, because the heat operator is given like this. Or the heat semi-group, rather, is given like this. Okay, but we don't have the manifold. But we don't have the manifold and we don't have the heat semigroup. So we're going to pretend that at least locally, and this is pretty reasonable, the heat semigroup or the heat kernel should be Euclidean, should be the same as the heat kernel in our of, you know, our Euclidean space. And so we're going to create a matrix that is going to be this. This is just the evaluation of the heat kernel on the The evaluation of the heat kernel on the Euclidean space at our data points, which we've now assumed to be in some high-dimensional Euclidean space. And then I have this T hanging around, which will affect the answer a bit. But in practice, people just pick the one they liked the best. There was a range that kind of worked. There are now a few techniques more or less for how to pick it, but actually, I think people still just pick things by hand. Okay, so this is the idea. Okay, so this is the idea. So we're going to construct, we take our heat kernel and we pretend that this is the good one. And then we just build the Laplacian. Do I have it here? Yeah, right. So now I pick a small t small-ish and I build the Laplacian like this. So I take, so just plugging in my S. Just plugging in my estimates for the original. Okay. And again, the choice of T is how to choose it and how to justify that choice. This is still pretty sure an open question. And now you do the same, do what I said before. Using that Laplacian, you now look at the eigenvectors and do this map. Okay, and the amazing thing is that this works, right? So here's. Right, so here's some results. So, what they did is they gave the computer one at a time either the horizontal bar or a vertical bar positioned randomly around this square. So, a thousand images, 500 are horizontal, 500 are vertical, or 500 are horizontal, 500 are vertical, and you put them all around the square. And you put them all around the square. They didn't actually say what the metric they used was, but you can imagine that it's probably just the Euclidean metric on pixels. And then they did Laplacian maps, and this is what they got. So it very nicely separates the vertical from the horizontal bars. For comparison, they did a PCA. They did a PCA of their data set, and this was a giant mess. So it didn't work at all. Okay. So there have been lots of variations of this. One of the most popular and common one called diffusion maps by Cliff Manella Fone. And the difference here is that instead of looking at the Laplacian, we're going to build a diffusion operator or a Markov chain. Our Markov chain and use that as the basis for our embedding into our K. But very much, the essential ideas are very much the same. So here's the method. This is not going to make a whole lot of sense. But we start here with the kernel, a kernel function. Again, often the choice is the same, same you put in e-kernel. Same you put in e kernel. And then one adjustment that they did was to treat distributions of points which are not uniformly distributed. So this kind of these q's help deal with that. And the alpha is an interpolation that we're going to see kind of. That we're going to see kind of goes between, well, it changes the operator that we're really approximating or that we're imagining we're approximating. Okay, so we look at Q. Here's D. D should be something like the degree at that point, but it's sort of weighted in this funny way. And then the P's are going to be my final operator. So this operator here is really this matrix. So here's my final. This matrix. So here's my final matrix. And if you go through all this, you'll see that this matrix is in fact gives you a Markov chain. And that Markov chain, then the spectral data of that Markov chain can be used for the embedding. Okay, you do the same thing. Here is, depending on Q and alpha, the operator that they're trying to. That they're trying to emulate. I think approximate is a little bit strong. What people can generally prove is that you can approximate, given a function, you can approximate these operators applied to that function. Or given an eigenvector, sometimes you can approximate the operator applied to that eigen. But all of these approximations are non-uniform. So talking about them, you have to be a little careful. And unfortunately, in the literature, often people are not careful. Are often people are not careful. If you're talking about approximating the operator, or you're talking about approximating some function after you evaluate the operator on that function. Okay. And so the results here are also very impressive. So I have these original images on the left. And these guys in the middle are what middle are what happens as we modulate alpha, right? So and then change the effectively the operator that we're the target operator, right? So in this case, alpha is equal to one, which turned out to be the Laplace-Beltrami operator, was the best of all of them. But different ones, you know, will require different alphas. Okay. All right. So this. All right, so this, I was kind of shocked by this when I first saw it. I think this is a reaction a lot of people have. It was just magic. And the magic part was this embedding. So why should embedding of points using the eigenvectors work ever at all? And there's this old, really beautiful result by, oops, I don't have it here. Oops, I don't have it here. Which tells us that this is actually what we should expect because it's what happens in Riemannian manifolds. So if I have a Riemannian manifold, my goal now is going to be to embed it into little L2. So I want to take the Riemannian manifold, I want to put it into just change every point into a sequence, infinite sequence. And I'm going to do this using an idea. Using an idea exactly the same as what we just saw. So, I'm going to take the orthonormal basis of the eigenfunctions of the Laplacian, and we're going to use that to build up this embedding. So, you know, this is big messy constants and then t. But the important part really is here. So here is my eigenfunction. And then here, I've weighted the eigenfunction. This is actually kind of important. Important. So before we either weight the eigenfunction by one or zero, basically. Here we're weighting the eigenfunction by the eigenvalue of one of the elements of the heat semigroup. So the one that corresponds to T over 2, right? And so, and since the eigenvectors are really also eigenvectors of the heat semigroup, I like to look at this as. To look at this as the spectral data of in the heat semi-group, which we use the Laplacian to get to, right? But the Laplacian itself is a crutch to get to what, in this case, anyway, is really important, which is the heat-sending group. And so, back here, you know, of course, when these guys are small, which is what happens. Is you know what happens in the first few in the lower modes of Laplacian, e to the negative, whatever this is, will be more or less close to one. As they get bigger, it'll be close to zero. So, you know, cutting it off like this is actually not so unreasonable, or like we do. The fact that we ignore the t doesn't seem to matter so much. But in the original theorem, the t is there. And here's the statement of the theorem. The statement of the theorem. So for all t greater than zero, this map that I showed above is an embedding, first of all. And then, second, the pullback of the metric from the standard metric on little L2 to the manifold approaches the original Riemannian metric on the manifold as t goes to zero. So the closer t gets to zero, the better the The better the, or the closer the metric is that you're trying to approximate, basically. Right. Yeah. And so this little comment is, it really seems to matter the heat operator even more than the plausion itself, or the heat semigroup more than the plot itself. Okay, and that's that's it. That's all for now. I do want to make one comment. I don't. Make one comment. I didn't prepare anything because I actually don't know these papers very well, but there is a little bit of work, which I think is very nice and what little I've seen on the DIRAC operators. So combinatorial. So what is that? Well, instead of d d star plus d star d, you have Plus D star D, you have D plus D star. And if you type that into Google, you get some interesting papers as well, which have very different kinds of flavor and applications than Hodge theory. All right. Okay, so that's all I wanted to say. Thank you very much. References are here. I'll leave them up for a few seconds each so you can just see what they are. Okay, well, let's thank you, Tony. Any questions? Hi, well, I do have a question. Can you hear me? Yeah, barely, but yes. Oh, okay. Sorry, I'll try to speak closer to the microphone. Maybe better now. Okay, thank you. Thanks a lot for the talk. Thank you. Thanks a lot for the talk. I have a question related to what you were talking about, a persistent version of the Hot decomposition. And I was wondering if, like, so as I understood it, they used this to obtain representatives, like representatives for the persistent bars that were harmonic in some sense, right? Right, right. But like. But like, do they allude or did or do they mention if you can actually define a hotspot composition that is kind of persistent through the filtration? Because I guess that has to be difficult, right? In the sense of you would have to ensure that everything is well preserved when you move along the filtration, right? That every single space goes where it is supposed to go to. Supposed to go to. And I'm not really sure, like, how then you define, like, do you define like the whole decomposition at each step, only looking to what is close to that chain space? Or I don't know. Yeah. Yeah, no, no, we need that. I was just wondering. So I'd have to go back and dig through it. In this paper in particular, the notation is very heavy. I see. heavy and uh i see i i have to you know always trace back um there's another there's another paper where they don't they don't talk about it's not by them it's by uh yusu wong and fekundo memoli and it's terrible i forget the student's name but or the other person's name um yes some one other person um who i believe is one of your students if i'm not mistaken um but they have a really nice But they have a really nice description of how to set up persistent Laplacians, like how to set up the Laplacians in a persistent way. And that might be helpful too. They didn't, at least as far as I understood, look at the representatives, although I might have missed it. Okay. So it's a totally different approach, like they are trying to do a different thing, I guess. It's not totally different, but it's not completely different. It's not totally different, but it's not completely the same, right? Okay, thank you very much. Yeah, welcome. Other questions? I suppose I was curious on your last slide that you're showing the broader personic LO 1994 theorem here. Yep. Which is showing that. Which is showing that, so I hadn't seen this before, so maybe correct me if I'm wrong, but that there's sort of consistency in the embedding and in the original manifold to this, the comparison to metrics, if you let go to zero. Is there a sense in which there's some optimality there? Like that this particular choice of the eigenfunctions and weighting my t in this fashion leads to like yeah, I have no idea. Yeah, no, I have no idea. That's a great question. That's a great question. Okay, any last questions? If you're on two, feel free to speak up. Okay, well, then let's thank Antonio. We'll break. Excellent. 