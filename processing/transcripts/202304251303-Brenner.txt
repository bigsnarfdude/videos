Alright, so yeah, I'm going to talk about template morphing. So specifically, those lovely plots that we saw in the last talk, how to go from one of those plots to another one of those plots, and then focusing on the And then focusing on the signal model. In this case, when I say signal model, it's just the model of interest. So it could be a background model for an analysis. It's just the thing that you're interested in morphing from one template to the other. Okay, so as a starting point, I will start with the formulation of a likelihood. So in a lot of our measurements, we want to have a description of To have a description of the likelihood. And the likelihood L is dependent on our parameters that we are measuring, that's our X, our parameter of in, sorry, our data, which is our X, our parameter of interest, which could be several, which I'm calling here mu, and our Newton's parameters, which in this talk I call theta. So, what does this mean in terms of the physics? It generally means we have some physics model, which can be a standard model physics model, beyond the standard model physics model, combined, whatever. Model combined, whatever. We have some physics model, we have some soft physics model, we have in my case, because I work on Atlas, the Atlas detector description and the ATLAS analysis reconstruction. But it doesn't have to be ATLAS. I just put that there because I wanted to have a way of saying that if your detector is different, this might be a slightly different picture. So then the problem is that this likelihood description is non-continuous. Is non-continuous. So we can only calculate the likelihood of our data set for a given point of mu and theta. So what do I mean? So we can approximate a statistical procedure with a grid scan. So here we have on the one-axis the mu's and here the thesis. We can for each point construct the likelihood where we have some sort of signal model and some sort of background model. A signal model and some sort of background model dependent on data and other parameters. Okay, so what we would want is to have a procedure to turn this grid scan into a continuous model. So that's morphing. So it's a procedure to turn this collection of points into a continuous function. If you look at places like Wikipedia, it gives that description as well, but it describes it usually in terms of pixels in a picture. Terms of pixels in a picture to looking that look to the human eye as smooth, stuff like that. But that's also the same principle. We're just talking about it in terms of the likelihood. Okay, so what do we want to do? We want to interpolate between models. So we need to define an algorithm where you have some function for some other parameter. So if we say some function s of x, where it's dependent on a parameter a. Parameter A. And we know, for example, S for three specific values of A. Now, this can be values of, for example, for a specific News parameter, like the two-point systematics, but not two points, but three points, but the same sort of idea, where we know what some function looks like in specific parameter points. For example, it could be that we know that for a is minus one, it looks like this, for a is zero, it looks like that, and for a is one, it looks like this. So we have three different looks like this. So we have three different descriptions and we want to actually say what would be f of x comma a, what would be the continuous function where we put the dependency of a in there correctly. Okay, well in this case the simplest solution is called piecewise linear interpolation. Basically if you look at one single bin in this histogram you count the number of events in this single bin you put In this single bin, you put that at the value of here, you put a, for example, here at minus one. You take the same bin on the x-axis, you count the number of events, and then again on the last one. And you do the same thing for every bin in your histogram, and you get some sort of you make a linear line in between, and you say, okay, it's probably that's the right number for which to put point of A. You get some. You get some weird things, such as possible kinks, because this might not be actually linear. But it does this interpolation at a bin-by-bin methodology. Another option, oh sorry, so this is what we had actually yesterday, this same plot. So I wanted to come back to it. So this was in the introduction talk of the summary of the last seminar, which Nick showed, and she showed this same picture. Which Nick showed, and he showed this same picture, this visualization from Barjo Pakerica, where he says, okay, so when does this system break down? So here you see basically an overtime for the different bins, what the distribution looks like. So you get the same three plots out, but for different points inside this time evolution, let's say, or dependency on A. So when this stops working, it was also shown yesterday already. Also shown yesterday already is when here's, for example, a large shift, if you have too few points in there, it just gets like double maxima. So, for example, here it looks like there's two points. Well, most probably, well, I know how this model is made, it is actually just one that shifts by a lot, one peak that shifts over a large distance. And it's interpolated as two peaks. So, how can we figure this? How can we figure this? How can we improve on this? Well, one of the ways we can do that is called horizontal interpolation. It is, in principle, a relatively similar method, but we interpolate using the cumulative distribution function. So meaning for each of these peaks we integrate, and then we interpolate the integrated histogram and differentiate that again. It means it's computationally a lot more expensive because you have to do the integration and differentiation. And differentiation for every single thing. So, the slightly smarter method, in some cases, I will come back to why I say some cases, is to use something called moment morphing. So, the idea of this method is that you can construct a morphed interpolated function that has linearly interpolated moments. So, for example, the first two moments of template models are the mean and the variance. Are the mean and the variance. So you take the mean and the variance and you interpolate that one, those two instead of the bin count. Which has the advantage that it has a multi-dimensional interpolation option. So now even if it depends on lots and lots of parameters, you can still do this. But it's computationally expensive, but it's also only once computationally expensive. What I mean with that is if you want to give this If you want to give this to your computer as a model, you have to calculate the mean and the variance dependency, but you only have to do it once. And after that, depending on which parameters you fill in, you just fill them in and you get the answer. So you don't have to recalculate it for every point in your parameter space. So these are the first three methods that I want to also compare. So we have the vertical morphing, horizontal morphing, and moment morphing. Horizontal morphing and moment morphing. And then they give different ways to create a continuous distribution of the likelihood. So for example, if you have a Gaussian with a varying width and you have vertical morphing, so if you have, for example, these two blue graphs, every time you have two blue graphs that you know as your input, and your interpolation gives is the red one for some value in between. So if you have a Gaussian with a varying width, all three of them will actually. Varying width, all three of them will actually do the same thing. They will give the same answer. But if you have a Gaussian with a varying mean, you can get different answers. So for example, vertical Morse dorphing, we already saw it at the beginning, if these two peaks are far enough apart, it will go to a two-peak system instead of a moving through the middle. While horizontal morphing and moment morphing still do the correct thing. If you now look at some more complicated models, Look at some more complicated model where you don't just have the width or the mean varying, but for example, you have a Gaussian function that dissipates to a uniform distribution, where I want to make the caveat that this is conceptually an ambiguous morphing function. You see that they do all three very different things. I can't tell you which one is correct here, because it's a little bit ambiguous. So I don't know which one is the one you want to, but you can see from the distributions. But you can see from the distributions that they give a different description of how it changes from a Gaussian to a uniform distribution. Can I ask you? Yes. I mean, it's obvious in the first two lines what is correct, what is not correct, because in a Gaussian strategic case, and then you make the point that when you have a more complex situation, this is not obvious any longer. But isn't the whole point of the problem of good? Of the problem of late morphing, the one that we are always in a situation when it is ambiguous. Because in the trivial cases, it's really how we can do it, right? Yes, that's what I'm actually about to come to. So this is my overview of the methods. So you can just have some analytical shapes, so you just parametrize it into a function, like you say, this is a Gaussian, and you just use that and use the Use that and use the two different points that you have to use as the way to determine the parameters of the function, which I didn't actually describe. Then you have some empirical descriptions. This is the things I've just shown. That's vertical infiltration and moment morphing. They're both empirical descriptions of how to go from one to the other. And then there's the last thing, which is something called physics-inspired. Called physics inspired. So, this is something that is still not an analytical shape, but it does actually use some information of the underlying physics that we might have, even if we don't have a proper functional description or parametric description of our distribution. So, I want to explain that in a little bit more detail. So, the method is called effective Lagrangian morphing, and the idea is And the idea is that you can use complicated distributions and still have a continuous, analytical, and relatively fast interpolation method. So it then combines basically the shape of your distribution, which most of the others either looked at a bin or at the shape. This one looks at the bin, counts, that's what we call the rates, and the shape information simultaneously. And what this does, it uses basically the Lagrangian. It uses basically the Lagrangian, so our description of our Hamiltonian, as a starting point. In the examples that I will follow, I will use effective models. Basically, what that means is if you do a Taylor expansion of your model, you can cut it off at some point, and it's still a nice description of the model that we can understand quite well. And you can have a discussion about where you cut it off. Another way to see this is if we had the discussion earlier about the We had the discussion earlier about the leading order versus next to leading order, so it's a little bit like that. We consider everything at leading order, and everything else is a correction to the leading order, which means you can add just a parameter, which is a scale factor, to the parameters that you've set constant. Okay, so how does this work? So, the morphing function for an observable TL at any coupling point, I will call this a usual. I will call this, so we usually talk about coupling, so the way one particle couples to another particle. We usually denote that with g, so that's at some point in our parameter space. So g is now a vector of all of our parameters that we don't know, the ones that we want to interpolate, and some fixed value, the target value where we want to know what this looks like. And it's constructed from a weighted sum of the input samples. So if you look at the picture here, you can basically see it as we have some sort of standard. Basically, see this as we have some sort of standard model distribution that we know, we have made some other points in the parameter space, some beyond the standard model version, and some mixture between the two, which is also beyond the standard model point in our parameter space. We use these three together to basically calculate the interference between the standard model and the beyond the standard model models to be able to go to a different point in our parameter space and say what the distribution would look like there. And say what the distribution would look like there. So it's a weighted sum of our inputs, input distributions. Sorry, the red curve is G target, right? Yeah, the red curve here is G target. Or actually, it's T out of at the point G target. Okay, so I'm gonna do an example with two free parameters in one vertex, just because it's simple in the sense that it's in physics or interpretation. In physics or interpretation. So we have one coupling parameter, which is the standard model coupling. And this can be adjusted, or the g-factor is basically something that can scale something up and down. So this g times the sum operator. And we add some additional point, which is some beyond the standard model coupling that we maybe also have, but we don't know yet. With again some operator. Now the distribution. The distribution of kinematic observables, so cross-sections, which is the things that we often measure, are proportional to the matrix element squared to. So it's basically to this thing here in quadrature. So that means we get some term that is basically here, if you look at these Feynman diagrams, some nice linear term of nice Feynman diagram that we have at leading order. We get some term that is beyond the standard model coupling. So I denote that here. Standard model coupling. So I denote that here with some sort of circle, some new coupling where we don't really care what happens exactly, just that it's something else, but with the same input and output particles and some interference between the two. So in this case, the matrix element can then be factorized, so you can actually use it, write it as this sum. Okay, so in this case, we can do. So, in this case, you can do some math still by hand. It's quite nice. And so, I tried to do it in the slide to have a little bit of a specific description here. So, if you take three input samples, where, for example, you choose that the coupling is basically one, so one means exactly as expected for the standard model, and something, and one, and the other one, the BSM part, you put to zero. BSM part you put to zero. So that's one of your input samples. You take another one where you flip it, where you say, okay, I want the size of my BSM sample to be the main thing, and I want nothing to do with the standard model coupling. A do one, which has about equal, so one and one, I call them, scaling of the two couplings, the standard model and the beyond the standard model coupling. If you now want to go to arbitrary parameters, Go to arbitrary parameters where you don't know which values of this G standard model and G beyond the standard model. So, as a reminder, our matrix element squares look like this. We can write this now in terms of these input distributions. So, just basically to get this out, and we know these three terms, we can write it with these GSM, GBSM things in here again. And we get an output distribution that is dependent on the target. Is dependent on the target values of your GSM, GBSM, and the input distributions. Yes? Yeah, we do allow for negative weights there. It does get a little... So we do allow for negative weights inside the full sum. We do get a little bit of a tricky thing if the full sum of everything becomes negative. So we allow that again if we're looking at a signal model where there's a background underneath. Model where there's a background underneath, we allow for actually a negative signal in there in this method. I don't know about whether you want to do that in your physics interpretation, but for the method we allow that. It does get very complicated if you get an actual negative number of expected measured events. Like that, we don't know how to interpret. So there it gets weird. So mathematically, how you can get something, how do you get something like that? At the end, you're just solving. At the end, you're just solving a linear equation. This is just solving a linear equation. So, how can you get something negative? The fundamental thing is because the interference term can be larger than the original two terms. Oh, because the whole thing is a... In this, where you only have three samples, it cannot. But in practice, when we add all of the uncertainties in, it sometimes does. We have actually seen a case where it did happen. And especially, that was not one with two frequencies. And especially that was not one with two free parameters, that was one with thirty free parameters, where it got weird. And does it start getting into trouble if the parameters, the one naught, et cetera, are greater than one? No. So that you're extrapolating? No, that's fine. I will actually come back to that as well. Okay, so we can generalize the same methodology to n dimensions. So we just had one where we had only two. Had one where we had only two parameters. But in principle, we don't have to have something with only two parameters, we can have an arbitrary number of parameters. So you have some production vertex plus some decay vertex if you have a Feynman diagram. So you have a sum over those in quadrature. And NP here is the number of parameters in the production vertex, and D, the number of in the decay vertex. And I specifically took out something we scope NS, which is the number shared. Called NS, which is the number shared in both vertices, because some couplings can happen both in the production and in the decay. And then it gets a slightly more complicated story. And then you get here, you get a long formula based on how many input parameters you would need to cover your parameter space. So we saw in the previous slide here in the case with two input parameters, I think it's quite, to me, it's intuitive, but it's also mathematically. But it's also mathematically showable that you would need three samples to be able to cover your parameter space. So the number of input distributions you would need to cover your space, of course, grows with the number of parameters that you have here that you want to actually study. Okay, so then I wanted to say something about the propagation of the sample uncertainties. So now we have all these different input samples, these input Input samples, these input distributions, these Tins, with however many we've calculated that we would need. And of course, those already have, if you use a Monte Carlo for that, there's an uncertainty related to that. So in that sense, we look at each individual. If you look at each individual bin, you can just say, okay, so for this particular bin in our distribution for our target where we want to say what the uncertainties are there, since this is a There, since this is a weighted sum of the input distributions, and we know that for each input distribution we have some Monte Carlo statistical uncertainty, which is some cross-section and some weight dependent on how much statistics did we generate compared to the amount of data we're going to measure. So, we have a sum there where we know what kind of uncertainty that bin is, it's just the square root of n for the bin. Square root n for the bin. So then you can make a propagated propagation of the statistical uncertainty for your output bin, in your output distribution of what the uncertainty from the sample size was. So as you can see here, this is dependent on the chosen input parameters as well as on the output parameter of where you want to go. And that also means you can choose your inputs. Can choose your input samples such that you reduce the statistical uncertainty. So, this is a little bit already an answer to what Louis was just asking about. You can go to erbitary wherever, but depending on which input samples you choose, your uncertainties might vary great, quite a lot. But this is, I think, quite understandable. If you take three input distributions very close together, you try to extrapolate this to some place far, far away, you might not really. Far away, you might not really realize what the real underlying distribution is. And in the same thing, if you take three points very far apart and you try to go somewhere in the middle that's nicely covered by that space, you might do better. Okay, so following the same method of the template morphine for the total uncertainty is actually usually done. So, what do I mean is there's you want to try to take into account the possible change. Try to take into account the possible changes in the uncertainties in the multi-dimensional parameter space. So, our systematic uncertainties that we keep talking about, our nuisance parameters, we have usually got them defined for some point in our parameter space, and they are usually also not a continuous description of our parameter space. So, we can either use the same thing where we morph between the different points, usually with a linear interpolation, I have to say, usually that's what we do. I have to say, usually that's what we do. Or we just assume that they are constant over the parameter space. So we're going to usually estimate all these uncertainties in all input sample points, but doing that for each point individually or to cover it properly again can be quite extensive or complicated. So we usually simplify that step. And the other part is that we try taking into account possible changes in the correlations. Possible changes in the correlations between uncertainties in the multidimensional space. And this is something that we also find very difficult because we already find it very difficult to say what are our uncertainties to start with. Like, how are they correlated? How are our nuisance parameters correlated in one point in our parameter space? That we already find difficult. So, going to a full parameter space, this becomes infinitely more complicated, especially since it does not have to. Especially since it does not have to follow the same underlying physics model as our signal model, because of course it can be a separate measurement of, let's say, the energy resolution in your detector. This doesn't have to follow the same kind of physics model as a Higgs signal measurement. So these are also two things that I want to bring up specifically because we don't really know how to do this in a smarter To do this in a smarter way. So, I bring it up here for discussion. I want to show you one example where we work this out. So, this is a vector boson fusion of H22W bosons, where we look at one standard model coupling, but we allow in both the production and in the decay to be as some two beyond the standard model couplings to be present. We call them HWW. Yeah, HWW and GAWW. They are basically a beyond the standard model CP even coupling and a beyond the standard model CP odd coupling. It doesn't matter for the math, but they are just two additional beyond the standard model parameters. In this case, we would need 15 samples as an input. We did make those. We made them with a 50,000 event sample size each. We consider only the signal, we ignore all the backgrounds, and we look at one kinematic observable. So if we look at the parameter space, so if we here have these two beyond the standard motor couplings, we took some, this is the blue points are where our input samples are inside the space. And I specifically write here, okay, we expect that we don't find something too far away from the standard model. So this is all what we call near the standard model. Standard model here is this middle point. So our expectation is here, and this is Our expectation is here, and this is sort of a space around that that we try to cover. And the edges here are chosen such that our cross-section that we would measure, the total cross-section, would still be the same for a pure beyond the standard model model where the standard model is actually not present. So we try to sort of, that's how we chose where these parameters would be. And what you can see here is the uncertainty coming from the different, from this sample size. This sample sample size. So, what the uncertainties are in different points in that 2D parameter space, in this parameter space. So, you can see in the middle it's quite well covered, and as soon as you go to these edges where there are no points, it goes up quite steeply. So, we basically say we can only say something about this model in the area here where it's nicely covered. So, we make 15 different input samples, that's these three plots here, and we have two validation. Plots here, and we have two validation samples where you can check whether it actually goes to where you expect it to go. So we know the underlying model, we look at different points in the parameter space, these are 15 times what this distribution would look like for different points in our parameter space. And this is a 16th and 17th point where we can compare if our interpolation or morphine does the correct thing. And the answer is yes, otherwise I wouldn't be showing it. Be showing it. So we see here in black the actual generated separate sample. We see in blue what happens if you go to that parameter, more towards that number where you already know where you want to go in your parameter space. And actually in red here, we did something else is we tried to do a fit where we tried to extract what those parameters should be based on the shape of this. So where, how, what. So, what are the parameters in our couplings that we actually want to know? And then plot that value in again, that looks quite well. So, here, I want to just mention again, these uncertainties here are only based on the Monte Carlo sample size and the data sample size. Nothing on the nuisance parameters or systematic uncertainties. Okay, so that brings me to my summary. So, morphing technology. So, morphing techniques provide a powerful way to model distributions in combined likelihoods. And all the models I described today are all available in root, which is our code that we use most often in all experiments. And different models are actually more or less correct in different situations. And most of the time, the thing we try to take into account is the computational cost of doing the interpolation and the uncertainty propagation of the systematics is definitely. Propagation of the systematics is definitely non-trivial. Thanks. All right, questions for the talk. Should I restart the? Well, we're going to do questions for the talk and then the talk. Okay, gotcha. Yeah. Yeah, so one of the, so very nice talk. One of the things that I'm uh wondering is do you have a feel for so because uh these interpolation functions are going to basically call like tens of thousands. Basically, you call like tens of thousands of times during the annotation. And so, so do you have like a feel? So, what I found a bit surprising is they have a feel of how fast they need to be to have an acceptable fitting procedure. So, because people sometimes want to use neural networks to like work between these uh these things and, you know, then it might become prohibitively expensive. So actually, this last one, I'm assuming you're talking about this effective in morphing. Yeah, so for that, it's actually that part, the fitting part is really cheap because you only have to calculate the interpolation, let's say, inside your space once, and then you get a continuous description already. So the fitting part is really cheap. The calculation of the interpolation the first time is very expensive. Ask a stupid statistical question. Wasn't the data. Okay. Is figure 23, upper left panel, the data? This is the Monte Carlo input distribution, so our T in is the way I did it. While our data is, for example, this black and red ones here. These are fake data, but these would. These are fake data, but these would be the data in this case. So, if I understand, you simulate from different points in some response stress, essentially. Yes. And at each point, you get a distribution. Yes. One distribution or several distributions? In principle, however many you want. And each point, so each of those values there could be thought of as a Poisson could be regarded as a multinomial distribution with some probabilities. Probabilities. So each of these distributions, this is an actual physics observable. So each of these distributions, each different color inside these three plots, would be a different point in our gene space. That corresponds to one of the blocks on the... Each of those corresponds to one block on this input. And then you have some functions, morphing functions, that take one distribution to another. To another. You're trying to estimate, you know, those functions, what you're trying to estimate. So, what we try to do is to use the underlying physics to say how you have to go from one of these points to one of these points, rather than saying it's probably linear between the two. And are those all those points can they be regarded as independent? First of all, each of those red points can they be regarded as independent. Can they regard as independent? You mean each of these? Yeah, each of those points there. No, there's each bin in this distribution cannot be considered as independent. Why not? Because in principle... Okay, so there are several things that come into account. One of them is the fact that the total cross-section is usually, in this case, in this example, is called the same. So if you vary one bin, the total cross-section, we try to make sure. The total cross-section, we try to make sure that it's still the same. So, in that sense, they are not independent. Plus, they are depend. So, we usually treat them for the sake of systematics or for Newton's parameters, we treat them as independent because it's easier. But, of course, the underlying physics between the things is actually the same. So, they are not completely independent from independent. Well, so I mean stochastically. Oh, yes. Stochastically independent, they are. Independent difference. Okay, and that's true also for every public block. There are different simulations of each public block. Yes. Okay, thank you. All right, I think we've gone into discussion sessions, so that should be smoothly going to.