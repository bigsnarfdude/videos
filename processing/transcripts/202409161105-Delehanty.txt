That are going to be connected. Is one, the role of a naive view of science. So I think often when people use those phrases that I had up on that last slide, they do come from a place where someone holds this kind of naive view, which we'll talk about in a moment. And the other theme that we'll follow is looking at the epistemic environment, the features of the epistemic environment, and how trust functions within that environment. So, the rule of the naive view of science. I'm not saying that this is the only way in which we can define the naive view of science. Some people have alternatively called this an over-idealized view of science. Some people have called it people holding a false philosophy of science. But roughly, I think those phrases reflect some view of the sort that's characterized by these global. Characterized by these two bullet points, right? The idea that data are somehow objective facts about the world, that scientists use the scientific method, that there is just one, that scientists should agree about what theory or hypothesis is right, given the available evidence, and that scientists are disinterested seekers of truth. Now, probably most people in this room. Probably most people in this room don't believe any of those. Certainly, most philosophers of science call this an idea of science because philosophers of science, after working for many decades, essentially given up on trying to define the scientific math evidence, anyone that characterizes all only science. Things like the underdetermination of evidence getting the way of that, and things like that. Way or bad, and things like the idea that values do and should enter into the gene science, even without pathological forms of disinterest, like pharmaceutical energy research, etc. We don't think that scientists can just check their values at the lab or the field door. So, when those phrases that I had up earlier are used sincerely, when someone says, When someone seems to actually think that the data can speak, and that we're doing a good thing, if people are saying that they're just relying and relying on the data, this often reflects a relatively high degree of trust in science. But a problem comes in then, because if trust in science is relying in part, or in large part, Or a non-chart on an over-idealized or a naive view of science, then this trust is vulnerable to being lost when actual science fails to satisfy these ideals or these imagined norms that we hold for science. Now, I think in other times we see these phrases used insincerely by those who don't or may not hold an idea of science, but who View of science, but whose aim is, for instance, to depoliticize decisions that are made that absolutely involve values. So if we think that policy decisions simply follow the scions, we may be hoping to distance ourselves from the values and the priorities that the public may not approve of or may disagree with. And so if we can just say, well, And so, if we can just say, well, we're just following the science. There are no longer values involved. This was kind of a necessary decision. Then we may be able to buffer ourselves from criticism. Now, in either of these cases, the authority of science seems to be grounded in its objectivity. I don't see objectivity. So, what do we mean by this? Well, we could have an entire We could have an entire term talking about objectivity. But really, really broadly, calling something objective generally functions to endorse it, to say, I hold this initiative, and as an implicit call to trust. When we look at objectivity and science, there's two general senses. I'm normally going to look at the second of them. Look at the second of them. The first has to do with the truth and referential nature of scientific theories. There are, for instance, the things that science posits. Are we to take those as literally true? Like there literally are these objects or these phenomena in the world? We're not here today going to be really concerned with that. What we're more concerned with is the second sense of objectivity, which has to do with modes of inquiry. So the idea that scientific inquiry can be objective. Inquiry can be objective and that it's not arbitrary, and in particular, if it's not subjective. Now, when we think of subjectivity, we're probably thinking of things like values, right? Our preferences, our interests, they're absolutely subjective. So, when philosophers of science talk about this, they talk about the value-free. Talk about the value-free ideal or the VFI. And this is something that seems to be consistent, at least with the views of those who hold a naive science. Of course, people who hold a naive of science are not going to carefully articulate the value of the ideal, but some implicit version of this seems to be present within that naive view. So, the value of the ideal is simply. So the value fair idea is simply the view that non-epistemic values, so non-epistemic values are those that generally think help us to get at the truth. So things like empirical adequacy, things like arguably simplicity, things like predictive power. Those are the kinds of things that philosophies of science have characterized as epistemic values. Characterized as epistemic values. There is a whole bunch of controversy about this that we're just going to bracket. But they're distinct, even if we don't agree on exactly what they are, they're distinct from things like moral and social and political values. So the desire for financial gain, the desire to achieve particular social ends. And so the idea is that these non-epistemic values, well, they absolutely Well, they absolutely impact science in what we might call the external domain, right? So, kind of prior to engaging in a research project, the choice that a scientist might make about what they're pursuing. No problem for that to be impacted by their values. There are, of course, ethical constraints that play a role on what science we actually do. At the other end, how we use science in policy decisions, how and what. Decisions, how and whether we apply the results of science. There's never been any question but that those should be impacted by values. It's this internal domain. So things like the methodological decisions that are made, things like how to characterize evidence, and the acceptance or rejection of hypotheses that are supposed to be value. Now, listen, most of the philosophy is science in 2024. In 2024, I don't think that the value free ideal is true. It may still hold among large knowledge groups within the public. But there have been, over the last 30 or so years, some very strong arguments against the value fee ideal, most of which I won't get into here. But one particular argument I want to briefly get into. It's called the inductive risk argument, which relies on Which relies on the idea that there's an inductive gap between the evidence that we have and our hypothesis. So, acceptance or rejection of any hypothesis is always vulnerable to error. We can accept a false hypothesis or reject a true hypothesis. We would like to do neither, but if we're trying to decide which to prefer and how to balance our methodological choices. How to balance our methodological choices, our statistical choices. We may need to decide which type of error to prefer. And that's where values may come in. So Heather Douglas, a philosopher of science, argued back in 2005 that social and moral values can and in fact should legitimately impact scientists' decisions. Scientists' decisions within this internal domain. So, really, really quickly, she uses a case study looking at dioxin and studies on the carcinogenicity of dioxin and looks at the same slides of fix and stained rat liver were evaluated over about 15 years by three different sets of ecologists, and they came up with different answers about how many showed signs. Answers about how many showed signs of malignant or benign tumor or male tumor. And our argument goes that decisions like that, that are absolutely within the internal domain of science, what do we decide to do with those criteria? Where we decide to draw the border of what counts as evidence of a malignant tumor, that those should be impacted by the risk associated with saying dioxin is a carcinogen if it's not, versus the risks associated with saying it's not a carcinogen. Associated with saying it's not a Causafront. So, this argument has been really, really influential. And as a result, you can see that the large swaths of the general public still hold some version of the value-free ideal, and that trust that they may have is related to their sense that science is objective. Sense that science is objective and science doesn't and shouldn't incorporate values. Then, calls that we see for transparency and openness, for sharing both the uncertainty involved in science and the values involved in science. And we do see that quite often in looking at debates over public policy. And within philosophy, we see people on either side of a Side about whether you know whether we should have norms of transparency and honesty or whether this is more likely to produce harm. But you can see that one effect of this transparency and honesty is that it can make clear some of the ways in which science, in fact, is not value-free. And so, what happens then is that we set people up to see that science fails to live up to their That science fails to live up to their expected standards, and this may reduce the trust that they have in science. Now, this is happening all within a broader epistemic context, right? And we're talking to multiple publics, multiple groups, not all of whom hold this naive view, right? So you can imagine that among publics who hold a more sophisticated view of science, right, failing. But failing to have transparency and honesty may decrease their trust. We may also see when we come to talking about epistemic trust, those whose values are not taken seriously, when we're considering which values we ought to prioritize when taking that inductive risk, particularly groups who may. Particularly, groups who may have been harmed by science or scientific institutions in the past, groups who are vulnerable, to fail to see your interests taken seriously or your assessment of the risks being taken seriously may also cause a decrease in risk or a decrease in trust among those groups, right? So we've got, you know, we've got to take into consideration then this complex epistemic environment in which all of these decisions Environment in which all of these decisions and all of these discussions are happening. So, philosophers of scientists and epistemologists generally have moved on from a, many of us have moved on from a view of analyzing knowledge as something that's individual, right? That we're just these rational actors in the world who evaluate evidence, decide what to believe, evaluate who they trust kind of in isolation. Isolation. So, generally, if we take knowledge to be dependent on the social and institutional context in which we acquire these beliefs, then we really need to take seriously the social aspects of an epistemic environment and how knowledge moves within that environment and how trust moves within that environment. So, Neil Levy, in looking at issues like Looking at issues like public distrust in science. We see arguments in the media and elsewhere, worries about being post-truth, worries about increasing public distrust in science and the harm that that causes. He argues that most people's beliefs are not irrational. And it's not that some people just don't care about truth anymore. Anymore, right? But that we have rationally grounded trust in some sources more than others. And trust is distributed quite unequally among different groups. So if we have this persistent problem of trying to understand how non-experts can figure out which experts to trust and what to believe, then we have to look. Then we have to look at these trust relationships. Because for most people, we're not able to just go to the primary research, to just go to the science and evaluate that. And when you've got competing experts, it's very hard for lay people to try to evaluate which set of experts are right. And we generally rely on things like cues of expertise. Cues of expertise, trying to figure out which experts actually have moral authority. We rely on that expertise, right? People, you know, maybe science journalists, people working in policy, people who may not actually work in that field, but have enough knowledge about that field to be able to evaluate expertise within some scientific community themselves. Now, it's very hard within our It's very hard within our current epistemic environment to do this. So there's a lot more we could say, but it won't because of time. Some of the problems that people have identified are lack of coverage reliability in our epistemic bubbles. Generally, this means that we just don't have particular information, right? We stick to sources we know, algorithms, fetus, things that we already agree with. What if we're exposed to that other information? But if we're exposed to that other information, we're prepared to take it in. Which is very different from some of the ways in which our epistemic environments get polluted by, for instance, institutions or groups or individuals who purposely mimic signs of expertise. So fake or predatory journals. Actively with the intent to deceive, mimicking the signs. Mimicking these signs of expertise in ways that are hard for public students earn, and manipulation of trust relationships. The most important one for us here is that when people are actively trying to discredit some sources of trust, then it's very hard to work to repair that trust. That trust. We see Hughian's account of echo chambers, right? Points out that when you've got people kind of preemptively working against new information or reliable information, breaking someone out of their echo chamber by telling them this is what those other ones will say. They will tell you that this is the case, that's how you know that they belong to you. They get just dug in more when they're presented with reliable information. This reliable information. So, looking at these trust relationships, I think, is really central. And it gets really complex. There are multiple accounts of epistemic trust. Again, for our purposes here, I'm not going to try to go through multiple. But what I do want to point out is the difference between a basic epistemic trust, in which we're simply looking for cues and relying on your cues to identify. On queues to identify experts who seem to have reliable scientific research and who seem to communicate their views honestly. And enhanced epistemic trust, which arguably is what we should become more concerned with in something like a public health emergency, because we don't just care about whether what someone says about Says about whether some theorem in math has been proved or not. But there are non-epistemic consequences to that belief, to whether we take that belief on or not, whether we believe that mask wearing is effective. Whether we believe that vaccines are safe and effective. So enhancing the epistemic trust requires beyond basic epistemic trust. basic epistemic trust that you have to experts have to take into consideration public welfare and the distribution of epistemic risks inductive risks right so that's where as i was saying earlier that you need to consider that vulnerable groups um you know who have you know been harmed previously by science and institutions of science and medicine you may need to see that you know scientists That scientists and institutions are, in fact, assessing the risk to them and taking those seriously. So, I'm not going to skip to the conclusions. So, what I hope to have convinced you of is that epistemic trust relationships exist in these very complex and dynamic epistemic environments that are vulnerable to manipulation. And any move that we make within. That any move that we make within this landscape is going to be differentially received by different groups on that. So, efforts to encourage trust among some groups may in fact discourage it among others. We're speaking to multiple different groups within that population. And that changes in trust relationships are going to significantly alter people's beliefs and, therefore, balance it. So, if we're trying to predict. So, if we're trying to predict change in people's behaviors, we really need to capture this complexity within the website cost. Thank you. Thank you.