And today I'm going to talk about this work, which is one of the latest papers we have completed. So this is a joint work with Shui Feng Gao and Jia Le Zha, who are in Chinese University of Hong Kong. So this is the plan. So between introduction and conclusions, basically I'm going to explain two things. One is peer learning, because in case you know this is not familiar to most of us. This is not familiar to most of you, or some of you, okay, so I'm going to explain that a little bit. And then I'm going to talk about this diffusion model for AI generating AI. Okay, so reinforcement, because we are going to involve reinforcement and IAI, right? So reinforcement is basically optimal control without knowing model primitives. So that's that simple. You don't know any model functions, model primaries. Functions, model coefficients, you want to solve the problem. So, reinforcement learning mimic humans, especially children's learning processes, learning behaviors. Specifically, reinforcement learning RAL learns strategy, learns control directly, skips learning model. So, we never learn a model. We never even try to learn a model. Okay? Now, for us, of course, we are given models. Now, for us, of course, we are given one. Most of the time, somebody else learns a model for us, right? So, we are given whatever diffusion model, job diffusion, and then all the coefficients they have already done their due diligence and detail to us, and then we just solve the optimization problem. But RAO actually says, okay, forget about modeling. We just find optimal control. So, this, of course, is a sharp contrast to this class, model-based methods. Okay, so just quickly. Okay, so just quickly, you know, sort of comparison between the two approaches. The model-based stochastic control, we have Bellman's principle, right? And HGV equation, verification theory. These are all like basically, you know, we are all familiar with. But the ARL actually goes a completely different route. There's no HGF equation to solve. So it is first, you know, you do exploration, try and error, because you don't know the environment, so you try an error. So, you try an error, you try to explore the environment, and then you do the so-called policy evaluation. That is, given a policy, given a strategy, what is its value? What is the value for policy evaluation? And based on that, we try to improve our current policy, as improvement. And then, an important ingredient, of course, is the regrettable analysis, meaning that what is when we do this, what is the difference between? Is the difference between the optimal value achieved and the value that God knows? Meaning that if you know the coefficients, then there is optimal value, right? But you don't know them. Right now we do RL, so that we also got that as a legal difference. So this is regret that this is right. So in the past few years, we tried to sort of establish a mathematical foundation for this reinforcement learning for control diffusion. For control diffusion processes. So we have a series of papers. Basically, each paper tries to solve one of the problems like how to strategically explore, how to do policy evaluation, policy improvement, you know, and also the last paper is about regrets. So, right. Okay, so let's see the problem, problem formulation. Okay, so it's a stochastic control problem. So everybody here feels very comfortable, right? So you have a Comfortable, right? So you have a controlled diffusion process. Here, A is the control. So I use the computer scientist jargon. So they don't call it control, they call it action. They are the same thing. So they use A, right? We use U, but it's just, you know, the nice to them. So I use A. And you want to maximize this objective function, right? Okay, so you have a value function which is optimal value function. Which is optimal value function, WTX. Now, the point is without knowing B in sigma RH. Okay, so you want to solve this problem without knowing these code, these functions. So all you have are data. Okay, so based on data, you just solve that. This is the problem. Okay, now how to solve this problem? Well, a critical technique is, I mean, I mentioned about this exploration, right? This exploration, right? So, exploration is realized or captured by randomization. Basically, you randomize your control. Okay, so it's like in a bandit problem, right? So, the classical approach is you pull the lever, you know, pull, you know, basically a bunch of slot machines, right? You play this machine, play that machine, deterministically. But the I approach is saying every time you flip a corn in order to determine. Perform in order to determine which machine to play. So you randomize your actions. So therefore, we take this idea to the control setting. Of course, now you have the design coin. You are designing a much more complicated thing. But it's a randomized control. So we use the so-called exploratory control. So this is a density function value, a measure value process. Okay? And the classical control is. And in the classical control, it is just Dirac measure. So it's like you expand it, right? You expand it to the major value control. So that captures the randomization. But it turns out that in classical control theory, this is not new. We have the so-called relaxed control. This is relaxed relax control. Of course, in old days, relax control was not for solving RL because at that time there was no RL. That time they introduced the relaxed control. They introduce relaxed control in order to solve the problem of the existence of that idea. So, therefore, control that is a pie. It's a measure-valued process. And you need to modify the drift and the diffusion a little bit. So, this is really the relaxed control formulation. Formulation. But then, specific to reinforced learning, we introduced so-called entropy regularized value function. That is, for control which is our value, or which is our measure, I don't remember. So, we put this term in the objective. For this term. So, what is this term? This term, if you recognize, including this negative sign, this is differential entropy. So, the entropy measures the level of the level. Level of the level of exploration. So basically, this gamma is a weighting parameter, which is called a temperature parameter. So basically, say, okay, so I give you incentive to do exploration. So I reward you, I award you, right? So I put this into the objective because I want you to do exploration. And this is the exploitation, the original objective function, exploitation. Okay? So this way, this formulation captures the essence of re-evolution. The essence of reinforcement learning, which is a trade-off between exploration, which is this part, and exploitation, which is part. And the gamma is a way deprived. Okay, so we have this formulation. This is actually our first paper, which I believe we lay a sort of mathematical foundation to capture this exploration versus exploration. Okay, then we can solve this problem. We solve this problem, we show that the optimal measure. Optimal measure is so-called Gibbs measure. It's Gibbs measure. And in the LQ case, it is Gaussian. It is Gaussian. In other words, when you explore, you don't explore blindly. You explore strategically. In the LQ case, you should just sample from Gaussian distribution. That is the best strategy. That is best for achieving the best trade-off between exploration and exploitation. So now you assume that you know B2 then. Don't worry, yeah, but this is theory. This is so far theory. Okay? This is so fast theory. So we will talk about if, what if you don't know this. So they just say you just do Gaussian. That in itself is very important, right? Because you don't have to use a neural network when you sort of approximate your policy. You can say, oh, I just used Gaussian. But Gaussian, how many parents do we have? We have limited parameters. They have limited practice. Well, actually, later we recently have a paper on junk diffusion. In LT cases, it's new jump daughter. Okay, so this is like four years ago, right? So now there were lots of development after that. And then let me talk about this little key learning. Now, TikTok. Now, to talk about little Q learning, let's first talk about big Q learning, which, of course, is a major approach in classical reinforcement learning, in computer science. So let me introduce BigQ learning. And it sounds very fancy, right? Big Q learning, but it's like a Bellman dynamic program. It's just like they use different JavaScript. It's quite simple. Okay? So let's look at this class. This is. This is stochastic control problem, right? So fixed delta T. So this everybody knows Bellman's principle, right? This is Bellman's principle. So delta T, I say T is today, T plus delta T tomorrow. So Bellman's principle basically says that, okay, so if after tomorrow, I already know my optimal price. Then I only need to worry about my control from today to tomorrow, right? So that is this A. Okay? So if I can find the best one, then I'm going to be able to do that. Can find the best one, then basically I solve the problem. This is Bellman's principle. And then the so-called Q function basically is nothing else than a discrete time version of this. So what it says that now it's a discrete time, t and t plus delta. There's nothing in between. So t today, t plus delta t tomorrow. So there's a q function which is which is defined this way. So a now is a single action. It's a single control. Single check. Yeah? Control. It's not a Yeah, control. It's not like here, this A is a process, right? So now it's a single control. So like today to tomorrow, single control, A. So it's a function of A, and after that, after tomorrow, I'm going to take it the best way. So this is how you define Q function, this big Q function. And there's an algorithm to compute this Q, big Q. Now, once you have computed this to big Q, then it becomes very simple because right now you're at time t and x. So what is the best action you have to take in discrete time? In discrete time, you just compare all these Q values, right? Q value with different A, and you choose the one that gives you the maximum Q. So that is the action you have to take today, right now. That's Q learning. Very simple, is it? Very simple. Nothing mysterious. Okay, so now we are going to apply this idea to continuous, because remember, we are facing continuous time, right? When we're taking continuous time, basically delta T is going to zero, right? Delta is going to zero, right? So when delta goes to zero, the impact of this single action A becomes less and less, is it? Right? So when delta converges to zero, this function does not depend on any. Q function does not depend on A. Then what? Then you are not able to compare the actions because Q does not depend on A. So therefore, you are not able to do this, right? Because this does not depend on A. So you're in trouble. So what are you going to do? So this is the problem. So, this is the problem when you try to extend this big key learning to continuous. It's inherent. It's inherently difficult. So, what are we going to do? I think I already explained these slides. There's no Q function. There's no big Q function in continuous time. So, what are we going to do? So, let's fix the policy pi. Fix it, right? Randomize policy, right? So, we define Q delta T, this Q function is defined to be from T prime. function is defined to be from t plot to t plus 3, I'm just taking this single action A. And after that, I'm going to just apply the original first pi. So that's how I define it. Then through some simple eto calculus, you know, this is just Ito calculus, it becomes this. Turns out you have this expression. You see, when delta t goes to zero indeed, this q function converges to the objective function, which does not definitely. So that reconciles with what I said, right? Okay? With what I said, right? Okay? But there you see there's this first-order term. This first order term, this H is a Hamiltonian, by the way. The Hamiltonian, we are all familiar, right? Hamiltonian. This H indeed dependent I. So we can still compare. We cannot compare the leading term. We can compare the first order. Alright? So leading turn J is independent of A is expected. Let's consider the first order. So this is how we introduce this little Q function, this little Q function in this first order. It is first order. It's first order time. Okay? So that motivates us to define this little king function, which is exactly the first order coefficient of that expansion. So this J and plus the habitat. Okay, now notice that this is a completely continuous time version, continuous time notion, right? It does not depend on any delta t, whatever. Okay, it's continuous time. Okay, so continuous on the notion. Alright, so now the next question is why this little Q function is useful. Why is this useful, right? Well, this is reflected in this theorem. This is a policy improvement theory. It says that given a policy current policy pi, so you construct a new policy pi prime, which is like this. So this q is the little q function on the pi. You know, on the pi, then you can show, this is Gibbs measure, by the way. Then you can show that this pi prime is better than pi. So we improve. So theoretically, if you prove that this little Q function can help you improve. Okay? Yeah? So out of curiosity, so and what cond w what conditions do you need to get this trick inequality? Uh we don't. We don't. We only we only know it's no worse, okay? It's no worse. It's no worse, okay? It's no worse. We don't have a strict E quite, but that's an interesting question, of course. Um, all right, so now the next question is: okay, here, this Q is so important, real Q is so important, right? How are you going to compute it without knowing P sigma? That's your question, right? Okay, so now we are going to answer that question. And that question is answered on the, again, a mathematical foundation that is materiality. All right, so it says that. All right, so it says that basically it says that j hat and q hat are a given pi. Okay, pi is given, so you are going to learn j hat and q hat, I mean their objective function and the little q function. So save that they are respectively, the value function, value function is the objective function, okay? And the q function associated with k if and only if this is my lingua, okay? If and only if. Now, this my linguity condition. Now, this modeling condition leads to algorithm. So, let me explain how, right? Okay, now, first of all, before we do that, when you learn a function, you have to do function approximation. Because computer cannot handle functions. Computer can only handle vectors, right? Finite dimensional vectors. So, we approximate J to Q by J C and Q plus Psi are all, you know, finite dimensional vectors. And it deforms. Vectors and the forms, the parametric forms, of course, can be inspired by the problem structure or neural network. The last resort is neural network, right? Okay, now this martingale D basically, you know, according to the definition of martingale, so there's a condition expectation, right? Condition expectation in turn gives you a loss function because it's a projection. So naturally, it gives you a so-called martial loss function. That is, this needs to be minimized. But then this, this. Minimized. But then this is integral, so you discretize, you use a sum to approximate the integral. So it is like this. And then you look at the right-hand side. Each one is data. Each turn is, for example, TI X T I is data. Because you can watch your state. You take action, you will see where that action takes you to, which state. So these are data. These are data. Okay? Now, these are general. Okay, now this RJ, the R is the reward function. You don't need to know the reward function itself. You only need to know the signal. That is, you take action, what is the signal? The RG is a signal response, signal from the environment to your action. Okay? So in other words, you only need to, this function only depends on observed data, not functional form. And in particular, one only needs a reward or reinforcement signal data. This is the reward signal. Signal data. This is the reward sigma. Okay? So that's all you need. So therefore, the right side has nothing to do with V sigma. This is our data. H, with also data, when you reach the end, you receive a unit data. You receive your signal. So then you just apply the standard stock. Stock has already the same to get C times. So this is how it, you know, this module. How it, you know, this martingale helps us. Now, I don't have time to talk about that. Actually, this martingale leads to also other types of problems. These are only my series of papers, right? So I have to move to diffusion models. Okay, so generative AI, right? So what is the problem? You want to generate new contents that closely resemble given data. For example, you can say, generate a picture of a benf. Right? Okay? Well, actually, I did ask that question. This is generated. What was generated by this Dao Yi 3? Okay? You should say, with mathematicians, then. It's coming, it's coming. But there may be other specific preferences. You can put them with preferences. You can say, generate a picture of a band with a group of professors looking at a lake on a snowy day. We can do that. We can do that. I did that, actually. View the pictures. So, this is pretty much the scene that you're going to see tomorrow afternoon. I believe in the ends. You know? Okay, so now let's be serious. So, what is the mathematical question, right? The mathematical question is: generate the new samples based on a given set of samples from a Set of samples from the unknown probability distribution. Now that's the key. You don't know the distribution behind. You never know. You probably, it is impossible to know. You only see samples. For example, basically the Char GPT, you know, look at all the benefits pictures on internet. So these are all samples. But you don't know the distribution behind. And taking into consideration additional specific preference like this, right? Additional specific preference like this, right? Okay, so this is my smart question. But then, how are we going to realize that? How are we going to deal with that? Well, recently there's this so-called diffusion model, which is relevant to us. It applies a forward-backward procedure. First of all, forward blowing, that is, it adds Gaussian noise to the given samples. Okay? You have millions of band. Millions of bank pictures, right? Add noise. And then you backward. You go forward and backwards. Like I take a wrong trip. Okay? You backward the noising. You reverse the forward process to generate new samples. Now remember, problem is simply problem. You want to generate samples. That's a purpose. Okay? That's a purpose, right? And then you have a human feedback, that is, additional specific purpose, like professors or whatever, right? Okay, for example, this is in a Swiss role. This is in a Swiss row. This is two-dimensional. These points form a Swiss row. So, what it does is basically it blurs from this direction, blurs picture. So, this is like a completely blurred picture, right? And then you come back, generate new samples, which are different from the original samples. However, the distribution behind them are similar or close, right? Okay, so this is the Okay, so this is the mathematics. How we are going to do this forward backward, right? So the forward blurry, now remember we start with x0, this is Rd. Typically, D is huge. It's high dimension, okay? It's a random variable following an unknown distribution P0. Again, I emphasize unknown. It's unknown, P0, right? And therefore, X0 as a random variable is unknown. What you have are only samples. You have only samples. You have all the samples, all the pictures of Ben, right? All the works of Shakespeare. So, all the music of Mozart. You have all these samples. But you never know the distribution behind, right? So, you put a forward OU process. X0 is P0, right? So, this is the OU process, right? And then, of course, you can solve that explicitly like this. Now, in this part, this is known because F and G are known. F and G are normal. You can give whatever F and G you like. It corresponds to different models, chat GPT or metas, you know, large language model, whatever. Okay, so this is known, this is Gaussian, actually. But this is unknown because X0 is unknown, right? Remember, X0 is unknown. So let's say Pt is a density function of Xt and P t, this is the density of Xt given X0, then it's Gaussian, right? X0, then it's Gaussian, right? If you fix X0, then it's Gaussian. So these are some of the notation. Alright, so what is the backward denoising? You just reverse time. HT tilde is just reverse time. Now it turns out that this H T tilde satisfy SDE. And this problem was solved by people in our community. Okay, in the early 1980s, by Anderson, Brian Anderson, control the realities. Anderson, control theorist, and Hausman Bartou, 1986. That's before the fashionable theory of BSD, actually. It's reverse. So XT2 to satisfy this. W is another browning motion. Okay, so there's explicit form. And then you see if X0 I c I can take this X zero tilde, that the initial is like close to the original X T by setting the mean to be zero. Setting the mean to be zero, and the variance is the other additional term that you know, right? And then you know this x capital T tilde will be the one that you want to generate the sample from. Okay, so this is the idea, yes. What is capital T? Capital G is a time you set. Okay, this is something you can set. Right. Yeah. So here, actually, you can set the capital G large enough, so therefore, this is a huge. Enough, so therefore, this is a huge discount, so therefore, this is almost zero. That's why I can take, like, you know, that's why I can take the mean as zero, because this becomes a Gaussian. So, the problem becomes, you see, the problem becomes how to move by the distribution from new is the Gaussian distribution to the P0, to P0. But the catch here is you don't know P0. That's why I asked you that question. Because it seems to be related. It seems to be related to the transport problem or Schillinger bridge problem. But in those problems, both distributions are known, given. But here, the point is that this P0 is not given. It's unknown. And we never try to estimate P0. Never. Alright? Okay? So that's the key of the problem. I'm sorry? How do you choose cathode T in practice? Yeah, I need T very large in variance. Yeah, I'm going to talk about. Large variants. Yeah, I'm going to talk about how to choose this, you know, how to generate those x capital T, right? All right. Okay, so now let's look at this backward problem. Now, this thing is known, this time is known. This thing is unknown because this P is unknown. Remember, because X0 is unknown. So P is unknown. So it involves an unknown term, which is called a score function. It is unknown. Okay? Now, Okay, now the traditional method, I mean, traditional means that mainstream, you know, the current mainstream method is just estimate using the neural network. Okay, so there's a way to do it. Use like Monte Carlo, so I skip that. So including additional constraint objective, so you know, because remember we also have a specific preference, right? So the existing approaches is pre-trained to Trend to estimate the score function. First, to estimate, learn the score function because that is unknown. To learn that, and then find T which is optimized. So it is a model-based approach because the goal is still to try to estimate the model, estimate the score function. But we apply our continuous time reinforcement and learn to optimize directly. We don't learn the score function. So, this is what is different from the previous one. From the previous one. So let me just quickly. So recall that this is the backward process, right? The score function is unknown. Now, because it is unknown, I take that as a control. So I say, okay, this is my control, right? So I replace this gradient, log score point, as AT. And the role of AT is to maximize this objective point here. Let me explain what it is. First of all, this AT has to be close to. First of all, this AT has to be close to the score function. You still need to generate a bank picture, right? You cannot generate the calgary picture. So they have to be close. The other one is there is an additional preference, like professors, snowy days, and so on. So this take care of two. So this becomes locus control. Yes? Now, but the thing is, this is look as control in which the reward functions are on the. The reward function is unknown, because this is unknown. But remember, in our little Q learning, we don't need to know the reward function. We only need to know the reward signal, reinforcement signal. That is, tight action, just watch what is the okay? And that signal, so this is the IO problem with unknown reward. And you can apply Q ready. So the only remaining problem, the thing is, how do you? thing is how do you how do you get the signal right the signal this signal is not something you watch but this is something that you can estimate so this is the reward signal basically you know through this company I don't have time to explain but the thing is you are able to at any given time t and x you are able to obtain a signal using the given sample sample is like a millions of band pictures right those are given samples using those samples to estimate a signal this is a signal A signal. This is a signal, and this is all you need. And then it is exactly in the framework of reinforcement learning, Q learning, and you apply a little Q learning. So this is the idea. And we have some experiment on Swiss Roll. Like this is, you know, we took this from our website, right? And for the additional preference, which is the reward function is that x-ray should be between minus 5 and 6. 5 and 6. So then we just vary this weight: 0, 1, 2, 4, 20, put the enormous weight on this preference, then basically all the points become in this area, so it looks like cross-site. Yeah, so I don't have time to talk. There's a good story to tell, but I guess I don't have time. So maybe let me just talk about union data. So we we can talk about during the during the I want to I really want to you know talk about this slice so I say we shall we try to say we don't we don't try to learn the environment. Is in a way that that the Plato's allegory basically says the environment is it's impossible to know the environment. Know you are. Forget it, forget about it. So, what do we really, what do we need to learn about you? So, actually, reinforcement still learns something about the environment, which is a little Q function or Hamilton. In other words, remember, Hamiltonian is a one-dimensional function. All these B sigma, they are multidimensional. But we only need to learn one-dimensional. That is critical information you need for your optimization problem. For your optimization power. So it is a Hamiltonian rather than each and every individual model coefficient that needs to be learned and estimated for optimization. So I think this is very, to me, I think it's a very important conceptual point from this study. But then the question is why that Hamiltonian is Lindable? Well, that's eato. So actually, if we do gj, j is the objective function, then you find that this q, little q show y. They find that this Q, little Q shows here. And this is DT, this is signal, reward signal, and this is just noise. So you can average around zero. So therefore, you see, this is data. This is so-called temporal difference. It's data. This is also data. So therefore, this Q is learnable. But for B and sigma, individual P and sigma, there is no such things. Okay? So on one hand, you only need to know this little Q. On the other hand, this little Q is little. On the other hand, it's BLP relatable by data. And that's why this works. So I think I will finish it. I can see some argument from risk-sensitive control in the Q. Q is a Hamiltonian basis. Because you are including the that with the edge of the position. Yeah, it is because it's exponential. Exactly, that's the transformation. But still, I think the conceptual is different because this is the steel-based model-based. In first minimum model, and then you have here, you know, it's it's yeah. So I'm I'm sorry to replay my question about capital T about the coefficient capital. The coefficient factor T time. Yes. So you say that you take T very large, and therefore the mean is almost zero. It's approximate zero because there's a discount, right? E minus, you know, zero capital T, right? So where capital T is very large, this is very small. Okay, so my question is twofold. In that case, the variance will be zero. The variance is. The variance is not zero. Why variance is zero? The other term, the variance is not. And in practice, how do you choose T then? Oh, T is just a sufficiently long T, capital T. Sufficiently long capital T. So when you do this blurring and denoising things, you can choose capital T. I guess the capital T is differs based on the situation, right? So are you talking about, see here? So are you talking about now see here, you're talking about this yeah this way. Yeah, so the variance is here, right? Variance is here. I just I just I just remove this term because when capital T is large enough, you approximate zero, okay? And can you still estimate the score function that gives you? I'm sorry? Well the score function has nothing to do with this f uh well it has to do with f and g but score function is basically is the Basically, is that it has to do with the density function of this process, right? The density function of this process. But we can talk about this, right? One last question, please. So by using this little Q learning to train the diffusion model, we don't have to do score matching anymore, right? Exactly. That's a that's a whole point. So score matching is still model-based approach. But they say so our approach is model-footed, right? So, our approach is modeled for this. We don't estimate score for. Remember, function is an infinite dimensional entity. So, when you say you want to learn an infinite dimensional entity, that probably is not possible. It's not possible. I mean, even if we want to learn a 10-dimensional, that already is quite difficult. Yeah. Well, let's have the speaker at the end and then we'll 