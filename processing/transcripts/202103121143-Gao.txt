Thank you for inviting me to this wonderful workshop. I have enjoyed many nice talks in this workshop. And my research area is a little bit different with the focus of this workshop. I usually do non-cognitive stuff or content stuff, but some recent research project had lead me to geometric tools like rich curvature and also sub-remanion structure. Also, sub-Riemannian structure. So, for today, I want to report some recent progress of this so-called complete logs of inequality, which you can think is the logs of inequality not only for the scalar value function, but also for the matrix value function. And hopefully, you find this interesting. I will just briefly recall this notion of Logs of inequality, which has been covered by many previous talks. Many in previous talks. So I will take the semi-group approach. Let's consider we have a probability space, and our semi-group is all positive functions, sorry, all positive maps that send in positive functions to positive functions and also preserving the identity function. And then the logs of inequality with a positive parameter namda is stated as the namda integral of f square log f. integral of f square log f d mu is less than two times the energy form or dehydrate form, which is given by the for nice function, it's given by the generator of the semi-group. And throughout the talk, I will consider the generator is a positive function on as is a positive operator on the L2 space. So this is stated for all unit L2 function. And in short, I will call it lambda LSI. There is a very Side. There is a variance form of the Logs of inequality called modified Logs of inequality, sometimes also called L1 Logs of inequality, which is stated as the two lambda of integral of a function g log g d mu is less than the Fishing information term, which is the generator A acting on G times log G. And this is stated for all probability distribution. And this one. And this one I denoted shortly as Namda M L S I. And it has been mentioned actually quite in detail in Chi's talk that the reference side of modified looks of inequality is just the entropy functional for probability density and is equivalent to say that's the MSI is just to say the entropy function of decay exponentially fast. It turns out that for diffusion semi-goof. Turn out that for diffusion, semi-goof, when our generator has some chain rule, then we can we really have equivalent of these two. And for in general, if we consider a Markov semi-group on discrete space, like the finite state space, on the finite state space, this logs of inequality implies MSI, which further implies spectrum gap. I see the question by Todd that's I see the question by Todd that's why here should be AG log g. So previously you will have something like the gradients of f and dividing by f with a square. Then you apply a chain rule and you will reach out this quantity. Like you consider this is the gradient of g and the gradient of log g and by the chain rule of the logarithm you get you get this inverse of function. Of function. Thank you. Yeah. So there's a reason that I stick with this expression because that's the one which we will consider in a non-quantive case. So in the non-compton case, we consider quantum Markov semigroup, which is a continuous family of maps first satisfy the semi-group property and the same as the classic K-preserving identity. And now we want the semi-group is completely The semi-group is completely positive, which means that not only it sends the positive operator to positive operator, but also the semi-group tensoring with identity map on another matrix algebra is positive map for every n. And for simplicity, in this talk, I will assume our semi-group is symmetric, which means the semi-group, each map is a self-adjoint operator with respect to the tracing product. So this gives the model of gives the model of the Markovian model of the time evolution of this passive open quantum system and it actually has been classified by Gorini, Kozakovsky, Sudasen and independently by Lindblad in 78 that all the generator of the quantum Markov semi-group in the symmetric case is given by this negative summation of second order commutator, where these AJ's are just self-adjoint operators. Are just self-adjoint operators. And you can think this is a non-commutative analog of second-order derivative. Then, under this setting, then the modified logs of inequality is stated as follow. We have two namda, namda is some real parameter, and we have the trace row log row, which the row log row has to interpret use functional calculus. And is that the feature information term, which is just trace A acting on row log row. Is trace A acting on rho log rho. And this is for all positive operator with trace one, which you can think as a density operator instead of density function. And the left-hand side is just the quantum entropy functional, and the modified logs of inequality shows that, which is equivalent to that, this entropy functional decay exponentially fast. So this is completely analog of the classical picture. The classical picture, what makes a difference is that the tensorization property. We know that if we have two classical Markov semiconductors satisfy lambda msi, so does their tensor product with the same parameter lambda. Unfortunately, this is not known for the quantum case. And if you run the proof in the classical case, it actually requires some condition, a stronger notion, which we call a semi-group satisfying lambda completely. Satisfy namda complete logs of inequality. In short, I will just say it's a nambda CSI for positive namba if the semi-group tense string with identity map on a matrix algebra satisfy lambda modified logs of inequality for lambda that is uniformly for all dimension matrices. It turns out that this definition is tensorized in the quantum case. Tensorize in the quantum case. By that, I mean if we have two quantum markers of semi-group on the matrix algebra, they both satisfy lambda CSI, so does their tensorization. And this one has application in quantum estimating the decay of quantum entanglements along this quantum process. And also, this tensorization is useful in studying quantum manybody system. But I will not go into detail of that. Instead, let me give you a simple example. Example. If we consider this semi-group on two by two matrices, and with exponential decay property sending elements to itself, with the rest of the property sending elements to trace times identity, basically it's just averaging it. And we find that this semi-group, if we consider these logs of constants or the modified logs of constants, they all are sharp equals to one. However, equals to one. However, if you tensor with identity on a matrix two by two matrix algebra, the modified looks of constants is strictly less than one. And this one obviously will upper bounds the complete constants for all matrix level. And for this complete constant, one can actually calculate easily just by hand. You can see this has a lower bound by one half. So for here, you see a real difference between the CSI constant and the MSI constant. So they are. Constant. So they can really differ. But also, I want to point out: it turns out that if we consider the logs of inequality or the equivalently the hypercontractivity, this fails in generally if we have a semi-group tensoring with an identity map on a matrix algebra. So in essence, this shows that this notion is the right notion that we go to all the matrix level. In contrast, the hypercontractivity one is not the one we can go complete. Is not the one we can go completely. So you may wonder: this is such a strong definition. Do we have enough example, a large class example of this definition? So the first existence result goes into the sub-ordinate semiconductor. In a joint work with Mars Junger and Nicholas Starquint, we show that if we have every symmetric quantum mark of semiconductor, the sub-ordinate semiconductor, which is to take the generator. is to take the generator to a fraction power and they all satisfy CSI for some lambda depending on theta. And actually one can formulate this question of CSI also for classical Markov semi-group. Correspondingly, we have the results that if we take m as the compact remaining manifolds and the delta x is a sub-Laplacian with a family of vector field satisfied homondi. Of vector fields satisfy the Homender condition, which means that this family of vector fields, if we take the iterated d bracket, it generates the tangent space at every point. In this case, we see we show that all their subordinate semiconductors satisfy CSI. So more precisely, what we say for CSI for Markov semiconductor, for classical Markov semiconductor, is to consider a matrix value function that's for every dimension n and the function. And the function which a matrix value function into n by n matrices that is positive every wall in the matrix sense, every point is matrix. And taking the integral of the trace equals to one, this is like a matrix value density function. We have this entropy function, which is like a semi-classical entropy. We take the integral of the quantum entropy at every point. It's less than the integral of this feature information at every point. Feature information at every point. It turns out that the way we prove one is by the study of the two. It's like we prove the result for the quantum Markov semi-group by using classical Markov semi-group. The tool, the idea we use is called transference, which is a way that we can reduce questions on matrices or operators by considering matrix value functions. So more precisely, we consider G is a compact group and mu is the Group and u is mu is the hard measure, then we have a unitary representation. Then we're going to take a what's so-called a transference map, which is an embedding of the matrix L-bar into the matrix value function on G. At every point of the G, which is a group elements, we're just conjugating the matrix X by the unitary representation. Then we say a semi-group, a matrix semi-group is. A matrix semigroup is a transference semigroup of a classical semigroup on the G if we have the following diagram commutes. That is to say, we have the first embeds the matrix into matrix value function and applying the classical semi-group together with identity map on the matrices is the same as to first run the matrix semi-group and then embed into matrix value function. Then invent into matrix value function. If this diagram commutes, we can say the quantum Markham semigroup actually can be realized as a subsystem of the classical semigroup with matrix value amplification. And it follows from that if we know the classical mark of semi-group satisfy lambda CSI, then as a subsystem, the TT, the quantum mark of semi-groups has to satisfy the transference quantum mark of semi-group. Transference quantum mark of semi-group has to satisfy Namda CSI. And you can see here we actually need not only the semi-group itself, but together with its matrix value extension. And actually, this procedure can go for every symmetric quantum marker of semi-group. That's what we can always find is a transference of some sub-Laplacian compact L-group. So then the question of studied quantum Markov semi-groups. Of studied quantum Markov semi-group, you can reduce to study sub-Laplacian and compact D-group. So, here's some progress in the elliptic setting. That's if we consider the remaining, so here this is a complete Backery Emory theory proof by Haojian Li, Mars Junger, and Nicholas Narquinty. That's if we have a remaining manifold satisfy the rich curvature is bounded below by a positive constant lambda, then the heat semi-group satisfies. Namda, then the heat semi-group satisfy Namda CSI. That basically means that the battery MRI series extends to all matrix value functions. And in a preprint with Michael Brown and Mars Junger, we showed that actually the Hismekoup, every compact remaining manifold satisfy lambda complete logs of inequality. And we use the condition we use is it has a rich curvature lower bounds and the ultra contractivity and spectrum gap. Contractivity and spectrum gap. And the lambda we have explicitly depending on this data. And for example, if you consider the circle, we can have the CSI constant is lower bounded by this constant, one of four times natural log of three. On the other hand, we also, it's a classical result that's the log servic constant on the circle is sharp to be one. So you can see there's a gap between the CSI constants with the LSI constant. And very recently, we have some progress on the matrix algebra that we saw for every quantum mark of semi-cupon matrix algebra, we have this two-sided control of CSI constant by the spectrum gap. Basically, we just need to lose the dimension constant. If you compare this to the classical or the ergodic case, that's the LSI constants only lose the log of D. So sorry, here the N should be the dimension D. Sorry, here the n should be the dimension d. So this we're losing a larger dimension constant, asymptotic larger. Okay, then I will conclude with some questions. So for the classical Markov semi-group, we want to know whether the CSI constant and the MSI constant are really different. In other words, is there an example that we can see the constant for the matrix value function is different with the scalar value function? Given that it's not known, Given that is not known, we are interested to understand whether every sublaplusian on compact D group satisfies lambda CSI for some positive lambda. And this was known by, it was shown by hypercontractivity by Lugovici and Zagodinsky. And the motivation we wanted the B is that B will implies dimension independent estimate for all the transference group in contrast to the previous result that we have. The previous result that we have we lose a dimension constant. Unfortunately, for the question B, no example is known, even for the basic SU2, which is a question I'm working on with Masha. For that, I would like to thank you. Okay, thanks, Lee, for the wonderful talk. So, there are any questions for Lee? Okay, so Bruce has a question. Okay, so Bruce has a question. You have not unmuted yourself. Sorry. Can you say a few words about how you, in this transference principle, how you choose the sublime the sum of squares operator that corresponds to your to your original A, I guess it was? Sure, yes. So if you so I have to go back many slides. To go back many slides. Actually, the procedure is very simple based on this classical result. That's the generator is basically a second-order commutator of some self-adjoint operator. Then you can plus the i here by ignoring this negative. And this is now in the d algebra of the unitary group. And you basically just take the d-algebra generated by this. Algebra generated by these anti-self adjoins elements, and you find out the corresponding D group of that. Yeah. Thank you. Okay. Thanks. Question from Bruce. And is there any other question? Okay, so there is a question from the chat. You know, do you want to ask yourself? Now, do you want to ask yourself by yourself? Yeah, yeah. Okay. Yeah. So, typically, from the Loxo-Bolof inequality, you also get a transportation inequality. Is there any meaning for this? I think Wieda and Pat have some work on this, but I'm not sure it's in this context. And it's not for matrix values. Okay, so from the logs of the V inequality you can. Log sublime inequality go transport inequality. This can go in the matrix value function and also go into fully matrix case. Their formulation of that. But the point is that what's the wasa stand distance, right? This was then distance, one has to formulate using a work by Colin, Eric Colin, and Jay Maz. And J mass. They introduce what's a stand distance general. Okay, they introduce for the matrix algebra, but usually a procedure you can introduce for matrix value function, which is a semi-classical space. That's the key property is there, is that if that's then this semi-group will be the gradient flow on the density space with respect to that volume. With respect to that Wasat distance. So that's the key feature of that's the way they introduce that Wasat distance. Once you have this gradient flow structure, all arguments run as classical case. So you have that one? Yeah, we have that one. Actually, you can find the form of that, although it's stated in full non-competitive expression, but it's stated in this paper. Expression, but she stated in this paper. Okay, okay, good. Thanks. Okay. So, is there any other questions? So, Lee, I have a remark. So, for the optimal