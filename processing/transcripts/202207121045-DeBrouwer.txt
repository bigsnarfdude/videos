So, hi everyone. So, today we're going to talk about like topological graph neural networks. And yeah, as Guy said, a research question is going to be like, is it beneficial to add like topological information to classical graph neural network architectures? And in which case, do they help? And which case do they not help? So this whole presentation is going to be about guiding us through this research question. Am I just changing slides? Oh, like this. Right. So just first, so that's. Just first, so that's a paper I presented at ICLR this year. And just like also credits to all these amazing authors that contributed to this work. So Max, Michael, Eve, Bastian, and Karsten. So yeah, so we've seen like, so like main question that's like we all like interested in is like, how do we actually have a like good representation of a graph, like to do like some downstream tasks, for instance, like graph classification, graph regression, no. Classification, graph regression, node classifications, or like link prediction. And of course, like when we want to have like a good representation of a graph, because we know that like two graphs can have like different vertices, we want something like a vectorized, so we want, yeah, something like it's a bit easier to handle, like from a mathematical perspective. So we want a function that takes a graph as input and that outputs like something like more, as a vector in Rg. And of course, like this representation. And of course, like this representation has to ideally satisfy a lot of different properties. And one of them is permutation invariance because you want the graph to be, you don't like the same, it would be the same graph if you just permute the nodes. So we've seen that in a run, give us also a good introduction also to the graph neural networks in a nutshell. So as we saw that, Shell. So, as we saw, that like what we would do is we would operate at node level, and like this graph neural network, or like especially like graph message neural passing type of architecture, would just take the features at each of the different nodes and then update like the representations at each node by just aggregating around a neighborhood, combining these representations, and then do this among like several layers, and then having like a readout functions that can be used for classification regression or this type of thing. Yeah, classification, regression, or this type of things. But so, of course, and then like that was also like linked to that Series' work that showed us about the expressivity of this type of architectures. We know that like GNNs are as most as expressive as bisolar land tests, especially the GCNs. So WL1 for graph isomorphism. And for instance, like a very typical example, like to show that like some typical GCN cannot distinguish very obvious graph to the naked. Distinguish a very obvious graph to the naked eye. It's like this graph here on the left and the graph here on the right. And despite being very different, like one has like two connected components, one only has one, like GC, like typical GCNs would not be able to tell them apart. So the questions that we are asking is that, like, despite those two graphs, we're very topologically different. So, like, one has two cycles, two connected components, one has just like one connected component and one cycle. How can we basically use this topological information to imbue graph neural networks architectures with this type of understanding and information? So yeah, so that's like just showing that like basically like you cannot. Yes, that's what I just said. Okay, so indeed, so graphs are like topological objects and like one simple way of like understanding a graph. Like, understanding a graph from a topological perspective is like representing a graph as a simplicial concept, a simplicial complex. So, Liz gave us also a good introduction this morning, like to give a bit more intuition about how we can represent that. And so, basically, a graph would be just like a collection of different simplices that compose this graph. And we can define k here, which would be like the set of simplices in this graph of dimension k. And then by defining like a And then by defining like a boundary sorry yeah, I'm too loud by defining like the boundary homomorphism we can study the homology groups and basically then like just like skipping a little bit the details but getting like topological informations from from these graphs. So a graph as a simplicial complex what we can represent is as like k0 so like the dimension zero So like the dimension zero simplices in this graph are going to be like all the vertices that you have in the graph. So all the different nodes. And like the k of dimension one would be like all of the different patches. And then when you start with that, like you can like a very important topological features would be the rank of this homology group at different dimensions. So you have like the bet, so which is called like the betting number zero. So the rank of page zero, which would be the number of connected components, and the rank of h1, which would be battin number one. Of each one, which is B betty number one, which is B the number of edges, which kind of says like topological features of our graphs. So that's very nice because then it means that like the previous two graphs that we couldn't tell apart, just by like computing those two topological features, we're not able to tell them apart. But of course, it seems quite restrictive just to have like those two numbers to represent the graphs. So we would like to do something which is like a bit more expressive. And to do that, we're going to use like graph filtrations. So that's also linking to what Lisa showed us. Also, linking to what Liz showed us this morning. So, what we're going to define is a filtration f, filtration function f, which is going to be a function that like takes as input like the node features and output a scalar. And so for each of the different nodes in these graphs, we're going to apply these functions to each of those like node features, and we're going to be able then to order what is the outputs of like this filtration functions on all. Of, like, this filtration functions on all of the different on all of the different nodes of the graphs. And this is going to define like the filtration, so a sequence of like subgraphs of this graph, which is going to be increasing. And the way that we define a subgraph, so this sequence, so this filtration, this sequence of subgraphs, is by like selecting all the vertices in the graph whose function, so the output of the filtration functions is smaller than some. Filtration functions is smaller than some threshold. So we're going to just like sweep over like a value of a threshold that we like from which we're going to select all the nodes which are for which the filtration function is smaller than this threshold. So because like an illustration is like much better than very long words, let's just try to do it with this graph, for instance. So here we have like two different graphs, which are like topologically equivalent, but we would give different filtrations. Different filtration. So, we have a filtration function one and a second filtration function for this all for the second graph. And so, what it means is we're going to have, like, if those functions are different, we're going to have like different filtration functions values at each of the different nodes. And so, then the first filtration, like the first filtered graph that we're going to have is just like selecting. We're going to sweep over the values of the filtration function. So, the first one, the first value of the sweep at which we observe, like something is going to happen. We observe like something is going to happen is like value one, where we're going to select then, like those two nodes, uh, two nodes for the first graph, and like another node for the other graph here. And so then we like continue sweeping. So at graph two, we're gonna have like at value two, we're gonna add like some extra vertices. And at value three, like something interesting happens is that like now we see that like we are gonna connect all the different components together in the first graph because we're gonna add those two extra nodes. Those two extra nodes which have value three, that's just going to connect the whole structure together. And so you can do this until like the maximum value function, and then this gives you like a sequence of filtrations of these different graphs. So what can you do then with this filtration? What you can do is that, like, as also Elise mentioned, like for the simplicial complexes, you can then compute the Betty numbers, so this topological characterizations. Characterizations of all the different subgraphs that you have in this sequence, in this filtration. And so, what you're going to end up with is kind of like, yeah, so a sequence of those Betty numbers, of those characterizations, topological characterizations of your graphs that are going to evolve with the sweep value that you take for the filtration function. So, basically, it's just like add from like the nodes, so the empty graph. Nodes, so the empty graph until like the full graph, you're going to like compute the sequence of like topological features. And then you can encode this in what we call a barcode diagram, which is going to be basically tracking and monitoring like the birth and death or like the creation and destructions, depends from which school you are, apparently, of like those different topological features. So, here, for instance, I plot again like a group. instance i plot again like a graph filtration for this example graph and like a bet zero which would like count the number of connected components so we see that like at filtration uh at the first subgraph that we have we're gonna have like two connected components so we have like birth of like two um uh two two uh um so yeah two two connected components then at two we add a third one and then at three actually we're going to only keep one of those connected components together because Connected components together because adding those two will destruct, like basically the three connected components, disjoint components that we had, such that we're gonna like destruct or kill, let's say, those two topological features here. So that's all nice. And that gives us a much fine-grained and more informative representations about the topological informations in our graph. And we know from previous work. And we know from previous work that, like, what we can do also is to learn a filtration. So, as I told you, like, the way that we get the sub-sequence of graphs is by fixing a filtration on what would be the scalar value that we assign to each node of this graph. So, what has been done, like in previous works, where also Bastian is also a contributor, is that we can just basically back propagate through this. This object to learn the best filtration that will be the most expressive to extract the signal of interest in the persistence diagram. So the questions that we ask here is that like, can we then use those results to create a layer that will, so we have like different layers of a graph neural networks, can we create a layer that we would put like arbitrary like position in this in this graph architecture? In this graph architecture. And that player would extract the topological components of this graph by using, by learning a filtration and then outputting the barcode and extracting the information from this barcode, such that we can very easily and seamlessly extract the topological features from the graph because we can just plug this layer at whatever position. So to do this, that's the architecture we propose. And that we call Architecture we propose, and that we call toggle. So, this topological graph neural network layer. So, I'm going to guide you through, like, so a little bit like what is how the information flows is in this layer and how it extracts topological information. So, before the layer, you can imagine that you have like an arbitrary type of graph neural networks that will give you, so because before the readout function, like the output of a graph neural network is still. The output of a graph neural network is still a graph, right? Because you just update the node features, at least for the classical architectures. So you start from a graph, which has like an arbitrary node feature that has been learned from the previous stacks of layers in the graph. Then what we're going to do is that we're going to learn like K different filtration functions because we don't want to just learn one typical representation, but we want to allow the network to learn the bank of different filtration functions on this graph. On this graph. And for each of these different filtrations, filtration functions is going to define filtrations on the graphs. And so it's going to also define like this Barko diagram that I told you about, which can also be represented in a similar form by persistence diagrams. We just have like, you would just register the birth and death creation destructions of all these topological features. So then at this stage of the layer, what you have is just like K different. have is just like k different persistence diagrams times the number of topological features that you want to extract. And here in our case, we're just focusing on like dimension zero and dimension one, that is, so the number of connected components and the number of cycles in the graphs. So we're going to have k times two such diagrams. Then what we need to do is that like to make it a layer, we want to output also a graph as the output, right? Because otherwise, like how we do, how do we deal with this information? We want to be able. We want to be able to feed back to the rest of the graph neural network architecture, a layer type of object. So, what we gonna do is like having like a set function that's gonna can sync it a bit like a transformer type of architecture, where we're gonna like take each of those points in the persistence diagrams, like process it together, and then like map it to each feature in. To each feature in for each vertex. So, like each edge is going to be attributed like a feature function of a feature value, which is basically an input, the output of a function that processed the whole diagram together. So, this whole diagram is a set. So, we can just like take a set function and just like for each element of this set, then output what is going to be the topological information. Then, here we use just like a residual. And here we use this like a residual architecture where we're going to feed also the initial features of the different nodes just to make sure like this topological layer also doesn't break all the signal that you might want to get, even if you don't really care about topological information in your case. And then because we have we output a different features, updated features for each node in the graph, we can just like then plug back and update like this node features for each node in the graph. For each node in the output graph. And so then we start from an input graph with some features, and then we get the same graph as output with updated node features, which would have topological information embedded into them. Yeah, something. So I don't see the time here, actually. Okay, so maybe something that I wanted to mention as well is that, like, so those persistence diagrams, they're going to have. They're going to have actually a specific property, especially for dimension zero, is that for each node in your graph, you're going to have one point in this persistence diagram. So, why is this the case? Because, like, every time that you're going to sweep over the different values of the filtration function, you're going to add, for instance, like one new point to the subgraphs, to the sequence of subgraphs that you've been constructing. And this new point will always create a point in the persistence diagram that is. In the persistence diagram, that is, either one new node that you at that one new node that you add to your subgraph is going to just like be born and then like be destructed directly because it's going to be connected to the rest of the graph. So basically, like its time of birth and its time of death is going to be the same. So it's going to lie on the diagonal. Or this point might just like lie alone and not be connected to anything. So all of that to say that there is already kind of a one-to-one. To say that there is already kind of a one-to-one map between all of the points in the persistence diagram for dimension zero and all the nodes that you have in your graph. So that's a very natural way to also go back from the persistence diagram to the graph level structure. This is not the case for dimension one, for instance, because it's not true that you would have as many cycles in your graph as you have like different nodes. So for this, we kind of use some kind of engineering just to be pro. Engineering just to so we process like all these informations from the from the from the cycles and then would add these information back to all of the different nodes in the graph because we don't have such a one-to-one mapping between like the number of points in the persistence diagram for dimension one and the number of nodes in this graph. Okay, so now we have like this whole architecture, these whole layers. We programmed it in PyTorch. It's like we're super excited. We plug it in and then we'll see if that works. Oh yeah, so first maybe. That works. Oh, yeah. So, first, maybe before going into the dirty evaluations on all those benchmark data sets, we can also show like very encouraging expressivity results for this type of architecture. And so the theorem that we have in the paper is that we can show that this architecture is at least as expressive as WL1. So, and because we knew that the previous classical graphical Previous, like classical graph neural network architectures were at most as expressive as WL1, we're kind of like at least as expressive as all these other architectures as well. And so, the way that this proof works is that like, if you know that like WL1 will actually distinguish two graphs as not being isomorphic, we can always construct a filtration function f, which will also lead to two different Barcode diagrams. So, basically, if we know that, like, yeah, the WN1 construction. The WN1 construction would like tell two graphs apart. We can always find a function f, which then use as a filtration function will give two different persistence diagrams. And then we show as well that you can also have an injective filtration function that is as close as possible, as close as you want to such a function. And the injectivity is required for having differentiability towards across the persistence diagram. That's just a technicality. A technicality, okay. So now we see that, like, we can like basically distinguish like those two graphs. We could already, but yeah. So, in terms of experiments, how then can we actually assess? So, our first research question was like, okay, we have those graph neural networks, if we like give them like this topological superpowers, can they actually improve in like this graph classification regression benchmark? So, what we're going to do is we take a like a going to do is we take like a gcn architecture with just like four different uh layers so in the paper so we have gcns but we also tried with like gin with a graph attention network as well so we have like basically four type of archetypical architectures for graph neural networks layer and what we're going to do is like compare this type of architecture with the same one where we just swap the second layer and just add one power layer instead of the of the gcn and so we do that The GCN. And so we do that just to make sure that, like, just toggle is not providing more computational power, but it's just like doing something different than what the GCN would do. And the first thing that we're going to do is it's like on synthetic data sets where we know that like topology should be like important to be able like to discriminate between those graphs. Can we already show that it works? Then the second one would be addressing performance on data sets without node features. So I'm going to explain a bit later why. And then we're going to do it like on those. And then we're going to do it like on those famous benchmark data sets as well and try to see why how it fares. So we designed like two very kind of toy examples, but just to let us understand like what this model was doing. So one type of graphs that we generate is like one class as like very large cycles, and another one has just like this conjunctions of very like smaller, smaller, smaller cycles. So again, just by having the Betty numbers of like Like, just by having the betting numbers of just like the full graph, you should be able to tell them apart. And here, what we're gonna have is that we're gonna add random node features for each of those nodes, such that if you have a GCN with sufficient number of layers, you should be able to also tell them apart by just comparing if you have seen that node feature before. And what we're gonna do is we're gonna check: okay, how many layers of GCN do we need to get the same number of performance that we would get from Get from our architectures with Toggle. So we see that, of course, our architecture, of course, like that's what we expected, and hopefully, it was good as well. We can just tell those two graphs apart, like just like just with a toggle layer without even using any GCN layer, because it's able just to use these topological informations to tell them apart. And in contrast, you will need like around like three layers of GCN to have like the same number of same performance in terms of like discriminating. performance in terms of like discriminating those two those two two graphs and you see that like basically like the smallest cycles that you have here is uh is three so as soon as like you can like detect like a cycle of like three then it's can uh token report so another like a bit more challenging data set is this one here uh where we have like still only one connected component and we have like um and we would have the same number uh of cycles and here you would see that like you would have like same type of of um Of behavior, or like our architecture would be able to tell those two graphs apart very easily, while the GCN would require much more number of layers in order to have the same number of performance as well. Okay, so that was it for like the synthetic data. So we kind of validated this approach there. But now we want to try to see again more like real world slash benchmark type of data. Real world/slash benchmark type of data sets. But of course, like in from the literature, we know that the node features already leak a lot of information when you want to try to do like graph classifications, right? Like for typical data sets, so there are like very good benchmarking papers as well. You would see that you just by like concatenating maybe like all the features of the graph together and ignoring the topology altogether, you would already get like most of the way in terms of the classification. Of the way in terms of the classification performance, because those node features contain so much information. So, of course, in our case, it's a bit detrimental because, like, if the topological information in the graph is like kind of like marginally efficient with respect to the node features, we're not going to see a lot of signal in our experiments. So, what we start doing is taking the same graph, but just removing all the node features altogether. So, of course, like it's a bit unrealistic, but it's just in terms of like trying to prove that when you only have the topological information. When you only have the topological information, then our approach can also do better. So it's just basically all those graphs, like with random node features. And here, as I told you, we take like different node graph neural network layer baselines, and then we just swap the second layer with our architecture. And you see that we either compare very favorably with the other architectures, but mostly we're able to improve the performance in terms of this. Improve the performance in terms of like this, both graph classifications and node classifications, and which seems to suggest then our architecture is more able to harness all this topological features, which is important to do this classification. So that was again like super encouraging. And then what we did is still also do like plugging back all the node features together. And here you'll see that like, as I was, I mean, as it was a bit expected as well, the picture is a bit more mixed. So we have like some wins when. Mixed. So we have like some wins when, like, uh, it seems that, like, for instance, like in SciFAR, in DD, which is like a biological data set, we are able to improve still the performances, but also in other types of benchmark data, like these node features. I mean, that's also our conjecture is that like these node features are so important. So that basically like this adding this topological layer is not really beneficial. The thing that we would see, though, is that like adding The thing that we would see though is that like adding this layer is not like detrimental, which is also encouraging as well. So it's not that you would lose a lot of performance by adding this layer so it doesn't harm. Okay, so what did we learn then within this whole research question in this project? Is that like, so when the topological structure is important in graph classification, and like we all believe it should be at some point. We all believe it should be at some points, otherwise, we wouldn't be here, like trying to work on graphs. The addition of a topological layer can boost the performance. So that's quite encouraging. In terms, so we designed a topological graph neural network layer, which is much more versatile than, for instance, other approaches. We would just like consider topology in the readouts function. So here you can plug this layer wherever you want in your architecture. Wherever you want in your architecture, and you can use these positions on hyperparameters. So, in our experiments, we have like tried different positions of where you would stack this topological layer in the whole architecture, but we couldn't define what is the overall best position for such performance, but we rather found that it's very data set-specific. So, for some of them, it was better to put it back second position, for some of them, a third position. So, in terms of future work, so we've like now. So, in terms of future work, so we've like now only considered the vertices and the edges in these graphs, but obviously, you have also like higher order structures like cliques as well. So, like, for instance, just when you have like a triangle in the graph, we don't like compute those structures yet, but we could like also use that. Of course, it's going to be more computationally intensive. But the question would be, okay, so how much do we gain? And what is the trade-off between this extra computation? trade-off between like this extra computational uh cost and the classification performance or like downstream performance that we get um and then so it seems that like we also so we have like this filtration functions that we're able to learn um but it's still not clear exactly so how that compares and that requires a bit more more experiments as uh how would it be for instance if we take like random filtrations a bank of random filtrations and just let Random filtrations and just let Libda network like train train from them. Okay, so that's it for me for today. So thanks again to Max, Michael, to Eve, to Bastian and Carsten. And yeah, the paper is available here. So we have like all the code. So it's implemented as a PyTorch layer. So like you might want to try it out like on any of your data sets or experiments. And yeah, please let us know if there is any issue. any any issue and if you have yeah questions thank you