Thanks for having me. I'll thank the organizers at the end again. For those of you, this is your first workshop for conference, the students in the group, maybe. You should have known not all meetings are this fun and engaging. So you have really been spoiled this week. So I'm going to talk about joint work with some people who aren't here. And then Joseph obviously is here. Raghu is a professor at Texas who was a postdoc at Aragon a couple of years back. Jem is a student. A couple of years back, Jem is a student of his. If you're going to ICCOPT, Jem is talking. So go see his talk. Raghu will be there as well. I hope that you've been able to go outside in Kelowna. I've been taking advantage of going outside every morning before coming here. Still showing up on time. You can find many cool things all around the lake. Please go explore or have fun tomorrow. I'll tell you a little bit about the problem that I want to solve. I want to be very clear there and give you some algorithmic building. And give you some algorithmic building blocks for tackling the problem that I'm looking at, along with three short vignettes for algorithms for these problems. Joining with the folks on the first screen. And then since there's a lot of interest in multi-objective optimization, Tyler, if you're at ICCOP, is going to be talking about multi-objective optimization. Definitely go see his talk. Otherwise, I'm happy to share some links to his work. So here is my purple black box. Black box. And the problem that I want to solve is a stochastic optimization problem. So I want to minimize an objective function that is the expectation of this function f that depends on my decision variables x in Rn and a random variable c. So I'm taking the expectation with respect to c and I'm going to address both the setting where I'm able to pass to this. Where I'm able to pass to this black box oracle X and X, so I can determine the random number that gets plugged into the black box, and the case where I can't. So this is the this case over here, right, where I give you the black box in X, and it gives me a noisy evaluation, stochastic evaluation at some unknown C. And in a slide or two, I'll give you examples of both of those settings. All right, we're a derivative free, but everything I'm going to do today is going to assume that the existence of such a thing. Assume that the existence of such a thing. So the expectation function, ultimately, we have derivatives of it exist. And that's going to matter for when we model these things. I want to spend one quick second on this, the important difference between these two black boxes, this one that you can pass X and X of C. And here's Y. So suppose that I have now, sorry for Francesco who's on Zoom, you can't see the board. Who's on Zoom? You can't see the board. Oh, maybe you can. Okay, great. So suppose that I just have additive noise, right? That's all that I'm doing over here. I have the mean function that I want to minimize, and my stochastic oracle is just the mean plus some stochastic noise. So here is what the interesting thing is. What the interesting thing is, if I have a common random number x, and I now evaluate at x and x prime at the same random variable, right? Then all that I get. X prime is this difference. So the additive noise disappeared. So from a zero order perspective, this is an incredibly powerful tool, common random numbers. The variance I could also show, but the people that do simulation optimization or study simulation, this is, they have a whole field in variance reduction, control variants, and things like this. This is called a common rate. This is called a common random number. So I have used the same random variable in both the x evaluation and the x prime. And suddenly, this hugely scary function that is the expectation, that is an integral with respect to c is all that I actually see when I take the differences of these things. Why is it not plus s of c? Your oracle outputs s of x. Okay, but where did the s go? That's not what I meant by the word yesterday. And that's why is that last plus C, not plus S of C one you're canceling out of. Sorry, say it again. The first expression S of X comma C is the expected, the first line, equals the expected value of S of X comma C plus C y. Why is that not plus S? Okay, so again, this is under the additive noise assumption. So when I have purely additive noise, such as the case when I'm taking measurement from a device, typically, so the noise is independent of x, then all the terms disappear that depend on x. It's an assumption. If I have multiplicative noise, I get something different, right? I have these terms. Something different, right? I have these terms sitting there, but there's still going to be a term that's going to cancel because of this. Okay, so just straightforward. Okay, so computing is one of the ways where you maybe can specify the random number seed, pseudo-random number seed, and then take advantage of this trick. You can't take advantage of it in experimental settings. And I think all the tools that people in this room are developing are underutilized. Developing are underutilized for where things are going in life, which is that there are sensors and automation all around us. So here's a problem that Tyler works on. This is a continuous flow reactor, a chemistry lab at Argonne, that his Parvu multi-objective optimization algorithm is now driving. And so what is it doing? It's determining the chemical composition inputs over here on the left, along with the temperature of this device, some other mixing coefficients, and things like this. Everything is going through. Like this, everything is going through this little nano, you don't see it, a microfluidic reactor. That's a cartridge over here. Out comes synthesized chemicals. So in this case, they're doing fuel cells. So these are like platinum and nickel nanoparticles that you would use in a battery on an electric vehicle. Then you have to go and characterize the thing. So you experimentally measure it. Depending on how long it sits over here, you get less noise or more noise. Gets fed to the Or more noise gets fed to the computer and back around. And I want to make a point here because there's no way that I'm going to be able to get equations for this thing. An engineer is not going to believe it, even if you wrote down equations. It's going to say you're going to miss everything. But this is a black box for which derivative free optimization methods are totally ripe, I think. It's a stochastic black box, obviously, for maybe obvious reasons. Here's Jeff in front of a quantum computer at University of California. Computer at University of California, Berkeley, or the outside of the cooling stage, where this thing drops down to what 10 millikelvin or something like this. Again, this is a noisy black box. I don't have equations for what's going on in this particular system. So don't just be thinking about computer simulations when you're thinking about applications for your methods. Think about these kinds of things as well. All right. All right, I don't have to tell you this, but obviously derivatives and optimization go hand in hand for recognizing when I'm at some kind of a solution, how far away I might be, methods, so on and so forth. Instead, I want to talk about all that we're going to do. And so I've unified everything by just presenting a single formulation that I'm just going to call F sub B. And that B is going to be a batch. So what that batch looks like. So, what that batch looks like is I go grab a couple random variables, or as many as are in the batch size cardinality B, and I take the sample mean. So if you're a Monte Carlo person, you can just call this the Monte Carlo estimate, let's say, right? So again, I grab a batch that determines my X C, sample mean. That's how I might reduce the variance over here by increasing the size of that batch, for example. And then I'm going to belabor the point with stochastic. I'm going to belabor the point with stochastic gradient percent. That Matt gave us a great introduction to a couple of points. The methods look like the following on the kth iteration. They generate a batch of these c values. They're going to go compute something like an average direction based off of the batch. They're going to walk a predetermined step size alpha k. So think 1 over k squared along that direction dk. And 70 years ago, Robbins Monroe. 70 years ago, Robbins Monroe did this for where each of these is basically the stochastic gradient, meaning the gradient of that output with the CI plugged in at xk. A year later, Kiefer Wolfowitz decided we can do this with finite differences and came up with the first kind of derivative free analog of this. So if we think about what a directional derivative might be, it's important to recognize. It's important to recognize that anytime we do an approximation by interpolation or differencing or anything else like this, we don't have this case that under the assumptions where, you know, in expectation, this estimate GK is exactly the gradient that I want because we have an extra term. And that term is just going to be what is left over in our approximation. So if you've ever done a first-order Taylor expansion, you'll have seen this time and time again. So if I walk a distance h along. So, if I walk a distance h along a step size d, take the difference of that with just sitting at x k divide by h, that's think of that as a forward difference, let's say in direction d. And I compare that with the directional derivative at d. I get something that depends on h and the Lipschitz parameter of the gradient. So, this is my the bias that I have in this estimator, and this is why it's a derivative-free algorithm, because you need to go and figure out how the algorithm should handle this bias. Algorithm should handle this bias. So that's my answer to the finite difference question yesterday. All right, so here's the algorithm in practice. Just a quick cartoon. So the algorithm, it can only see draws in that batch size B. This is what it sees over here on the right. And I guess I drew a couple of, I got a batch size two in all these pictures, a couple of random variables over here. The actual function I'm trying to minimize is sitting over here. And starting at this particular point, there's the unique local solution sitting up in this valley over here. Up in this valley over here, superimposed as well. I get a particular gradient or zero-order estimate of the gradient. Let's say I take some step size alpha. Okay, I iterate over here. I can do that the same at another starting point and get all of these things are going to converge to a neighborhood of these points. Uh-oh, the solution. That's stochastic rate descent, and it's vanilla kind of derivative-free updates. I don't like these methods. There's something really unsettling about it. There's something really unsettling about it, and Matt hinted at this: we spent a lot of compute cycles trying to tune these methods, and that's what these four pictures show. So, this is for a bandit or if you want Gaussian smoothing kind of an algorithm that takes a random Gaussian direction and looks at differences and perturbations along those. These are all different learning rates. So, these are all basically different equations for the step size that you would have, that alpha K in the previous one. These are all different batch sizes. So, the cardinality of that B that you have over here. Of that b that you have over here. So you have to just run this and run this and run this. And really, in derivative free optimization, typically we want to solve a problem once. We don't want to do hours and hours and hours or weeks and weeks and weeks of hyperparameter tuning. We would like to solve a problem once. So this sensitivity to these internal workings of these other algorithms with these predetermined step sizes, that's something that we want to tackle in this work. We're going to use a model-based trust regions strategy throughout. Here is the deterministic version of it. I'm not going to talk in great detail, other than the main device, if you haven't seen it before, in a trust region algorithm is sitting right here. And this is an incredibly powerful technique that I think more parts of optimization should be doing. And what it is doing, it's basically a cross-validation measure. Measure. It's asking how much reduction did I see in the function moving from xk to x plus divided by how much I predicted by a model moving from xk to x plus. It's not saying something about approximation of the model everywhere in the trust region. It's sort of guaranteed that if my model is good enough, that this thing is going to be well behaved. But right now, this is just saying how good is my model at predicting decrease? And this is a very, very And this is a very, very important technique. In our case in stochastic optimization, we're not going to be able to evaluate the full objective function f because it's an expectation. So already you should be thinking about what's going to happen over here. All right. So stochastic setting. We heard a little bit about these fully linear models. This is some notion of kind of like a Taylor-like model that just says locally in a ball of says locally in a ball of radius delta k, my function and my model, they agree like something of order delta squared, delta k squared. And the gradients of these things, again, I don't have this, but it's assumed to exist, it's going to be of something of order delta k. That's what it means to be fully linear. Probabilistically fully linear, it just takes up a whole bunch of more characters when you're writing a paper. However, it just means that you satisfy these conditions. Conditions conditioned on the history of the algorithm. In our case, XK, MK, Delta K. These are all now stochastic, like Joseph told you about a couple of days ago now, that this probability is greater than or equal to this beta m. That's all that it means. It holds its probability. I can't guarantee it at every iteration. All right, so now I'm going to tell you about the first work. This is with Ragu. This is with Ragu work that we did when he was a postdoc. We're going to use common random numbers. So we're going to use this trick. We're not going to assume additive noise, but we are going to assume that our oracle takes both x and the random variable c. And now I'm going to build up, if you want, a simplex gradient. That's what I have sitting out over here. If you want to think of it as a forward difference, you can think of it as a forward difference. If you want to rotate the coordinates in some way, we can change these directions and put an inverse problem out inverse. Put an inverse problem out there, inverse of a matrix out there over there. So, the direction that we're going to consider, again, same sort of framework, the direction that we're going to consider is now going to be weighted by a quasi-Newton matrix, HK. And then we have our zero-order gradients over here. So think of that again as a simplex radiant if you want. But really its approximation is going to be quantified by this H parameter. So where do we get this quasi-Newton matrix? And this is the first point that I want to make about this. And this is the first point that I want to make about this algorithm. So, if you want to take one thing away from it, things are a little bit tricky here because when normally in derivative base quasi-Newton matrices, I look at the difference between gradients, consecutive gradients, right? Now these are sort of stochastic gradients and they're estimates. And so I have to do one more trick that should bother you a little bit, but I don't yet know my way around it, which is that I have to use the same. Which is that I have to use the same set of random numbers in order to update my quasi-Newton matrix. So, what that means is on iteration K, I'm going to go grab a random batch size, a batch BK. Iteration K plus one, I'm going to grab a batch size BK plus one. I'm going to want to have some overlap in those batches for the quasi-Newton update. They don't have to be identical. In fact, they should have something that's independent of them. But think about Of them, but think about maybe as having half the batch dedicated to maybe the quasi-Newton update, and then you have another independent half that's independent of everything else that's happened so far. All right, so that's how you obtain this matrix H. And now your alpha K is not some recipe, one over K squared, let's say, but determined via a stochastic line search, using the same framework. Yeah. Your self-manage is CI in DK, or what you just said sounds like you want CI in DK intersect DK plus one. Yeah, that would be another way to do it. Yeah, so really this would have to be a this BK and this BK are not the same. I was going too loose and fast, right? This is something larger and this is the intersection. So that's a great point. Aren't you going to gain something by them being the same? You're going to lose a lot, too. You need the ability to change the decay around, but you're not. I need some independence, right? So, so this is the so for proving convergence of the algorithm, I need some independence for my B's. For using H and getting power and H, these are basically looking at differences of gradients along sample paths that are common to successive iteration. So, there is a big trade-off up there, but super clever idea. Okay, the other thing that's missing is the batch size. Missing is the batch size. How do you change the batch size? So, Ragu and collaborators, Jori Nosadal and company, have been working on adaptive batch sizes for quasi-Newton matrices in the derivative-based case. You can do the same thing in derivative-free optimization. And effectively, there's two different ways of looking at this, but all that I want to impress upon you is the following. There are zero-order quantities that show up in these inequalities. In both inequalities, I have a one over the batch size sitting over here. One over the batch size sitting over here on the left, which says that I make my batch size big enough, eventually the inequality will be satisfied. So there's your adaptive procedure. Pick a test, increase the batch size until this is satisfied, and go. This works. At ICCOP, Jem is going to show it, do it for randomized coordinate descent, zero order, Gaussian smoothing, spherical smoothing, and a whole bunch of other things, show how these adaptive tests can be. Show how these adaptive tests can be exactly applied. You can see how you quickly ramp up the batch size on different problems. Again, what I like about this, this is not tuning things. I don't have to worry about the batch size or step lengths or things like this. The algorithm is adjusting it as it goes. So if you give it a very noisy problem, it's going to have to increase the batch size to satisfy the tests. If you don't give it a noisy problem, well, right now we're only increasing the batch size. Things are only going up, but really you can also decrease the batch size over here or just start with a small. The batch size over here, or just start with the small one and all your nose. I want to make sure that I talk about work with Joseph. So, this is now going to look in a setting where I cannot control the random number. So xi is no longer my control. But I want to solve problems that are much bigger than the n equals 20 dimensional problems that Warren was hinting at this morning. Warren was hinting at this morning. How am I going to do that? I want to work in a subspace. He was working in a subspace. I thought he was almost going there. And that's where he's going to go next. So we're going to have a subspace of dimension P. Think of P maybe as being much, much smaller than N, if N is big. And that's going to be the game that we're going to play. Now there's going to be two distinct sources of randomness. And this is why it made the analysis difficult. There's the stochastic oracle. So I get. So, I get a noisy stochastic function evaluation and the inner workings of the algorithm because we're going to use a randomized subspace. The main building block here that you should go and look. So, Tami Kulda, who used to be in our derivative free community a couple decades ago, she talked to me. She said, look at the JLT, Johnson Linden Trust Lemma staff a few years ago. And I wrote it off, but I'm more and more interested. It just establishes the It just establishes the existence of the following. For any constants, blah, blah, blah, blah, blah, blah, blah, there is a random matrix Q. So there's a distribution on random matrices that are N by P, such that as long as P is of some size, I can always make the projection using Q to be arbitrarily close to the vector I'm projecting in a relative sense. So again, I can take something and project it down into some super small dimensional subspace, and then these are very close. It's very, very neat. We're going to just use one side of the inequality. And the inequality over here is just that the projected part of v is going to be greater than or equal to some quantity times the v that I have over here with some probability over here. And the probability we're going to make v bigger than a half. Going to make the bigger than a half. Those of you that worked in randomized algorithms and DFO probably know why that is. These things are cool. So, what are we going to do? For us, since it holds for all v, in particular, it's going to hold for the gradient of the function, right? So, we are going to look at reduced gradients, or reduced simplex gradients, if you will, where the reduction is defined by that matrix Q. And here are some example matrices. You can have a Gaussian one. If you take Gaussian entries and you scale them appropriately and make P large enough, it's going to depend on your alpha Q and your beta Q, then a Gaussian matrix satisfies that Johnson-Lendenstrust lemma. You can do it with these super sparse R hashing matrices. So here's a one hashing, here's a two hashing. Two hashing has exactly two non-zeros in every one of the columns. Non-zeros in every one of the columns. You can also show it for these. All right, so super cool. Now, what we're going to do is just do exactly our usual trust region stuff, but instead of working in the bigger space, having this ball in R3, I work in a subspace, let's say of dimension two. So I only have to form a model in that reduced space. I only have to solve a trust region subproblem in that reduced space. Region subproblem in that reduced space. That's the game that we're playing. So, going from three to two, not so impressive. I can go from three to one. Now I have a line segment. So, 1D trust region sub problems, 1D models. My row test has changed. I told you that it would. I need to do something because we're going to follow the storm framework of Chen, Magelian, and Sheinberg. Managellian Sheinberg. Again, I have different batches at XK and at my trust region subproblem over here. So I have a BK and a B prime. And then I've got something with my model as well. This works. Now we're solving 3,000 dimensional stochastic derivative free optimization problems. All that I'm varying in these plots, so the name of the algorithm is stars. G is for a Gaussian random matrix. This is the value of P. This is the value of p equals one, p equals two, p equals five, equals ten. The take-home that you should get from this is that one isn't the best, two seems pretty good, and that if I do a full space method, so just use the identity matrix as that matrix and make p equals size n, that's sort of just this exact algorithm over here in the full space, you're sitting at over here. Maybe I keep going and it catches up. The point that I want to make is that I've scaled the axis over here by the batch size, which is. Over here by the batch size, which is fixed and common among all these algorithms, and this n plus one. I'm showing you a line at one. So these methods are able to solve to some accuracy or get this quality of solution, let's say, with the equivalent of one simplex gradient evaluation with the batch size cardinality B. That's what that one is. So this is why you have the prospect of potentially solving very large-dimensional problems. Solving very large-dimensional problems. You can show that this thing converges. So, Joseph did a lot of work on the probability spaces and the filtrations that are involved here because of these different sources of randomness and is able to use a storm-like framework plus this random matrix theory in order to show a limit almost sure result and something that is an expected iteration complexity that's like what you would get for storm and others. Now we go and run off. Now we go and run off on a whole bunch of problems over here. And I will let you look at the archive for these things because I want to say one more thing about how you can do even better. And it doesn't always, it isn't always the best, but we'll tell. So can you do better with adaptivity? I'm going to go back to the maybe the termistic optimization, or if you want, it's still stochastic optimization, but now with a finite support of the random variables and everything has equal probability one over p. So I get something that looks Over p, so I get something that looks like a problem that Matt wrote down for us a couple days ago. And the reason that that's a good thing to do is that then I can actually compute my row as before. I can get monotone progress and so on and so forth. And here is going to be the trick. I'm going to take my subspace matrix Q, and I'm going to partition it into two pieces. One is going to be random. One is going to be random, so exactly what we just talked about. And one's not going to be random, it's going to be this U matrix. And think about the U being small in size, so a couple of columns, maybe one column, but more than zero columns. And this algorithm could be worse, right? Because for that same value of p, I'm now taking something that was random and making it not random. So if my u is chosen in some bad way, maybe. In some bad way, maybe the u could be vectors of all zeros, for example, then I'm going to do worse than if I just made the columns of u random. So, how can we do better than random? Here's a couple of ideas. I'm just going to apply secant-like equations for these reduced gradients. And so, what are these things? I'm going to have first-order terms, which just says iteration k minus one, I have an estimate of. I have an estimate of the reduced gradient where the gradient has been reduced by the QK. And this quantity U, that's my unknown gradient. Now I'm going to go back capital T steps and do the same thing. You know, T steps ago, my reduced gradient estimate, that's what I have over here. That's my simplest gradient according to the subspace QK minus T. And if the gradients haven't changed, this would hold exactly. That's the first order terms. That's the first order terms, the gradient terms. It's very important to not leave information behind in derivative pre-optimization. I also have these zero-order terms, which is just this is first-order Taylor expansion, xk minus one, xk. This is what the gradient at xk would look like in the direction that I moved here. Go back again in T. Throw all these things together and solve the following non-linear least squares, sorry, linear least squares problem. Linear least squares problem for my u. So u is going to be somehow an estimate of the gradient at xk that I've stitched together from the past several iterations. And then this exclamation mark is to remind me that you can just do generic linear least squares, or maybe you want to weight it, depending on how far you moved over the course of these iterations or things like this. So you can put in your own norm there. Suppose you want to be even lazier and say, I can't solve linear least squares. Can't solve linear least squares. I know that you can, but if you say you can't solve linear least squares, you could be, all that you would potentially do is say, in the past iteration, what was the direction that gave me the largest gradient component? And use that direction again in the next iteration. That's the strategy here. The convergence guarantees for this algorithm are solely going to depend on the random part. This is just to make this thing work well. I'm almost Well, I'm almost done. It does. Okay, so the black line, fixed p means that I'm fixing the dimension of the subspace for all these methods over here. This is the one that I'm just using two random vectors in that place. If I just use one according to the lazy strategy, and then one random vector, that's the green. If I do the least squares, have one, that's just the least squares, and one, that's the random, that's the blue. If I do both of them. If I do both of them, so I take one from the least squares and one from the best in the last iteration, then I get the red. So they actually combine to give you a win. And that was surprising. We're now looking at, these are large-scale problems. So these are thousand dimensional problems in part because we don't have to worry about the doubly stochastic case. All right, so I have to end on a cartoon because none of you used it. And I hope that you can use it in your work. Can use it in your work. All right, so it's a map. Here's the edge, here's the face, here's the vertex. Students, of course, say that's fine, but you haven't told us what's in it. I guess we're in black box, so we don't know what's in it, or we can't know what's in it. All right, so we're trying to extend the scope for these derivative-free optimization methods for stochastic optimization and handle harder problems and larger scale problems. A couple of archive papers, so the one with Papers. So the one with Ragu, the one with Joseph, and then the one that Matt talked about. And then, of course, as Jeff rightly talked about yesterday, everything there was composition with deterministic black boxes. So what happens if they're stochastic? We argued about whether this is a good idea or a bad idea, but I think that this is an interesting future direction. So I want to especially thank Warren, Charles, Charles. Especially thank Warren, Charles, who maybe is online, Anna, and Matt for the excellent organization. I've had a lot of fun, and thanks for your attention.