About some of our predictions problem for this talk. I started off working in astrostatistics with the CHASP group. And then when I moved to the University of Michigan, I started taking a closer look at the sun. So essentially, the whole problem is trying to study the impact of the sun on the earth. And this is the whole subject called space weather. Essentially, we're looking at all kinds of solar activities. Looking at all kinds of solar activities and then trying to figure out what kind of impact it has on our little Earth, which is here. And one of the most important solar activities is called solar flare events. And over the past few decades, especially in the past two decades, we have been having very good high-resolution observations at different levels, taking different kinds of measurements all the time. Time. So we have the satellite that we'll be looking at is here, SDO. So this is the instrument that gives the observations that look like this. So essentially, the solar flare events are very localized eruptions that has a lot of energy attached to it. And the kind of observations that we have, or the information that we have, are what the magnetic field looks like. So we have a 3D magnetic field information. Magnetic field information, and also what are the temperature maps looking like. So, those pretty pictures are coming from the temperature maps or from the thermosphere information. In the past, or more traditionally, we have been trying to extract the features first from those high-dimensional images, and then trying to use those extracted features to predict individual solar eruptions. And note that the reason why people try to do that is because those images are. Do that is because those images are of very high dimensions and very high time cadence, and we have too many data to handle, which is a good problem for statistics. But the number of strong flare events that we have is really not that many, and we're interested only in the strong flare events. So, given the sample sparsity, people have not been using those raw images yet in the past. And in this work, we have been trying to use the original raw images. The original raw images to directly predict solar eruptions. So, essentially, the problem is we want to find this function f that will map this high-dimensional imaging data time series of images to flare events that's labeled here. So, this is time, and then the red ticks are some strong solar flare events, and all the rest are not really. And all the rest are not really strong solar flare events. So we want to find this F based on a statistical or machine learning algorithm. So the raw inputs are represented as time series of those images. So those are n-mode tensor covariates. And then we're trying to predict some scalar label. So this falls into the framework called scalar on tensor regression problems. To put it simply, Problems. To put it simple, it's just a simple linear regression problem. Simple linear regression means that for each of the predictors or every pixel that you saw on my previous slide, you have a coefficient attached to it. But we can never solve this problem because the coefficient dimensionality is too high. So in this work, we try to introduce a technique called tensor contraction, which essentially compresses high-dimensional measures. Compresses high-dimensional matrices into very small-dimensional matrices or data cubes. And based on this principle contraction, we adopt the de Gaussian process regression model, which is a non-linear regression model, to predict flare labels. To pretty simple, what does tensor contraction do? It's again a linear operator. If I imagine my tensor as a big data cube, I'm going to multiply some vectors to each dimension. vectors to each dimension of the data cube to compress it to a very small dimension of data cube. So that's how the tensor contraction fits very well to the flare predictions problem because it's linear, simple, with sparsity conditions. We can identify all of those coefficients that we don't really know based on a limited number of data points that we have. So this is a tensor-Daussian process model, very vanilla tensor-Gaussian process model. Vanilla tensor-Gaussian process model. And combining the two building blocks together, the tensor contraction and the tensor-Gaussian process, we got our final model. We call it a tensor-Gaussian process with spatial transformation. The reason why we call it spatial transformation is because we apply the tensor contraction operators to the spatial dimension, that's the images. We try to compress the images to keep the key features that are important. Key features that are important for flare prediction and don't care about anything else about images. And then the estimation procedure that we have, because it's Gaussian process, everything is multivariate Gaussian, we try to marginalize out. And then trying to use a penalize the maximum marginal likelihood method to identify the parameters. And the important parameters in our framework are the tensor contraction operators, which give us very nice interpretability of which kind of space. Of which kind of spatial locations are important and how they interact with each other. And our planalization functions are for identifiability concerns and also for spatial and temporal continuities. It's called total variation penalization that we're using. And then this is just our algorithm. We try to derive a block coordinate proximal gradient descent algorithm. Descent algorithm taking advantage of the structure of our penalization term, which is the total variational norm. And in this case, we can show that under very mild conditions, the algorithm has a linear convergence rate as a function of iterations. And we confirm this empirically, and it works really well in practice with such high-dimensional imaging data. So we just apply this method to the solar-layer intensity forecasting problem. Again, we try to predict. Costing problem. Again, we try to predict the index of the solar flares, which is a scalar, the yi here. And then the xi is a 50 by 50 by 10 imaging data set that we have, which are the features. And then we have around 1,000 samples in total for the weak classes and the weak flares and the stronger flares. And the contracted tensors is only 3 by 3. Contracted the tensors is only 3 by 3 by 10. So that's what we put into the Gaussian process regression model. So here are some of the results and interpretations. So essentially, this is our fitted coefficient parameter after some thresholding. Essentially, it is showing that the bright region in the bright parts in the middle of the images, which is what physicists identify as polarity inversion lines, those are important features. Lines, those are important features that we reconstruct based on this data-driven approach. That's a confirmation of physics. And also, the boundary parts, which you're showing the highlights here, are indicating how big the region is. So the bigger the region is, the stronger the variance are. And we try to do some variance decomposition or analysis of variance, which is pretty familiar for most of the statisticians, to really understand what are the important To really understand what are the importance of each of those channels of images. So, we have 3D magnetic field information. We also have information coming from temperature maps. Traditionally, people are believing that all of the information should come from the magnetic field. But then, people always have the question about whether those temperature maps are important for flare predictions. So, in this work, we actually capture what are the correlations. What are the correlations or interactions between the magnetic field information versus all of the temperature maps that we have, which are labeled here? And of course, the so-called polarity inversion line, as I just mentioned, which is a well-known physics mechanism, is supposed to be the most important feature that we finally figured out. And here is just some feature importance maps. We also compare our methods with some of the existing literature. For example, you can do different. Existing literature, for example, you can do the tensor decomposition first to compress the dimensionality into a lower-dimensional feature space, and then do regression models. And it turns out that our algorithm has the best performance in terms of the mean squared error and the true skill scores for flare predictions. So, this is just a brief summary of the contributions of our statistical framework. The first thing is scalar on 10. Scalar on tensor-Gaussian process regression model with the total variation phenomenalization and application to a solar intensity prediction problem.