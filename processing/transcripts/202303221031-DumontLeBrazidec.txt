I found the database using geek neural networks. So, hello everybody. So, my name is Geoffrey Dilon Rasdec. I will begin to thank my co-author from Syria, which is an atmospheric transport laboratory in France, and some colleagues from the LSE. And today I talk with the about CO two plumes and the methods. CO2 plumes and the methods to detect and inverse these CO2 plumes. So, this work is part of the COCO2 project. As you may know, it's a prototype system for CO2 MBS, CO2 monitoring and verification service. The goal of this project is to improve our estimation of our anthropogenic emissions with the help of a new stellite, which will be which our new stellites which will be launched by 2027 CO2. CO2M. And as part of this project, our goal is to do some inverse modeling of hotspots, such as cities or power plants, with deep learning methods. And let us directly define with a better definition of what we mean by inversion. By inversion, we mean from a given satellite image, we want to retrieve the emission rate from a point source on this image. So you have an example here on the left, you have on the left of this slide. The left, you have on the left of this slide an image, a satellite image with a plume on it, and you want to estimate the emissions with the help of the plume. Obviously, the emissions and the consequences of the emissions, so the plume, are directly related. And so, our intuition is that one of the good sub-goals to perform an inversion is the sub-goal of segmentation to find the contour of the plume, and from the contour of the plume to go. The contour of the poom to go to the inversion goal. The problem is that this segmentation goal is far from straightforward. Here we can play the game of where is the plume. You have on the top left of the slide you have a satellite image, an ICO2 feed. And the idea is where is the plume and this satellite image, where is the plume of interest. You have the answer on the bottom right of this slide with the contour in yellow, which is enhanced. Basically, this. Enhanced. Basically, this is not cherry-picked. Many images, actually, most of the images will be like this with CO2M will have many plumes concealed under the background and we have to deal with this problem, we have to find a solution to it. So this is due to various reasons why it's so hard to detect plumes in satellites, images. This reason can be divided into two categories. The first category is a problem of the signal to noise. Signal to noise noise ratio. We have a highly variable background due to the instrument noise, due to the variability of the fluxes themselves. And we have a low plume definition, a low signal comparing to this background. And the second category of problem that we will not address today are problems of image integrity. So, for example, the presence of clouds, but we will assume clouds do not exist today. So, now that we have addressed So now that we have addressed, we have seen the different goals, the sub-goals, and that we have seen the difficulty, the question is how do we address this difficulty? And we need techniques that can deal with other things than the high signal, other characteristics of prunes, such as such as spatial patterns. And for this, we will rely on deep learning methods. And so the rest of this presentation will be divided into three points. The first point is Points. The first point is, as you may know, for deep learning we need a lot of data. So we'll speak about this synthetic data set that we are using to train and evaluate our deep learning neural networks. In a second point, we speak of the segmentation of Google. And finally, we will address the task of inversion to retrieve the emissions from plumes. So let's begin with something simple. How do we Something simple, simple. How do we create these data sets? So, let's assume that we have some emissions from somewhere at some point. We have an inventory of emissions. From this inventory of emissions, we can simulate with the help of a network transport model the trajectory of a plume. It's what you have on the top right of this slide. Then, you can add to this the simulated background, so all the other biogenic and anthropogenic faxes, what you have on the left, and then you add the satellite. Left, and then you add the satellite noise, which is here represented by a random Bushen noise of 0.7 ppm, that's what you put on the bottom right of this slide. But we are not satisfied. I mean, we need a lot, a very wide and variable data set when we are dealing with a variety of points of C, a variety of geographical areas, a variety of prune types, etc. We use two techniques to have this wide and variable data set. Have this wide and variable data set. The first is a bit subtle, we just have a lot of data. These data come from the LSE when we have one-year simulations on Paris that are simulated with the Schime model. And we also have the smart card data set from the EMPA in Switzerland simulated with the Cosmo model, and it's a large area which is covering Berlin and many power plants of Germany. Many power plants of Germany. And the second technique that we use to have a lot of data is techniques specific to the deep learning field, which are data augmentation techniques. So what do I mean by this? You have an example on this slide. On the left column of this six image are typical XiO2 feed and plume from the SmartCare dataset, so like power plants in Germany. And the second and third. Germany. And the second and third column are data augmented versions of this first column. So basically, it's what I mean by augmented is a tone, sheared, flipped and shifted on the right, I think, for the surround images there. So with this, we can artificially create more plumes to train our model. So now that we have defined how we can create our data sets, we can address the question of the methodology that we use. Raise the question of the methodology that we will use for segmentation, and we go with a quite classic methodology for the feed. It's what is called supervised learning. The idea is to have a statistical model, and we want to train this statistical model to perform a task. Here, the task is to go from an image to another image. The input image is the exeoto field, the output image is the contour of the plume that we want to retrieve. That we want to retrieve. And to train this statistical model, we train it with many, many examples, many, many images, many inputs and outputs. So many Xioto fields and many contours of the prunes in the Ixioto field. The problem is that if it is easy and if we have seen how we can construct the image on the left, it's not so easy to see how we should construct the image on the right, the output. The outputs. You will see why here just now, it's actually something that is really at the core of this segmentation work. It's the problem basically of the definition of the plume. What exactly is a plume? So you have two examples of plume on the left, on the left column, sorry. And in the middle, you have what happens if we set a certain threshold and everything above this threshold is a one, and everything below this threshold is a zero, is a non. Below this threshold is a zero, is a non-plume. And what we get is something sometimes, such as the lower line here, lower row. You have something that is you have a contour which is not at all representative of the clue that we have on the left, and then it makes very difficult for the model to learn anything. So we need a better definition. And the solution we came up with is what we have on the right. Basically, we keep the image on the middle, but we wait The image on the middle, but we weight each pi each pixel by the plume concentration at the pixel. And so it it gives something like this: it's a Boolean map, but which is weighted, and the weight actually can be found in the definition of the binary quasi entropy loss function. Okay, so now we have exactly sorry, we have defined the input, the output, exactly what we want to do. We have our data set, we can now address the question of the statistical model. Of the statistical model. And here in deep learning, there is a field which is especially good to deal with images, which are convolutional neural networks. Basically, convolutional neural networks are able to capture special features in images and to deal with these special features. And especially, they can be used to recognize in an image these special features which belong to an anthropology plume. And so to check where. To check where it would be inside the image, there would be an anthropogenic clue. So, since it's an image-to-image problem, we use a specific CNN architecture which is good for this kind of issues, which is the U-Net architecture, and the encoder part, so the left part of this U-Net architecture, which is the one that is transforming our input image into a bunch of feature maps, which will help us to identify the proportion it crossed. 25 billion proportion improved. This one is the efficient net V0 encoder, which is a very heavy neural network of 5 million sparometers. Then one more thing, because before we can look at the results, we need to define how we can train our model on our data. Actually, there are several ways to see this problem. It's more like a philosophy question. What do we want as a model? We kind of want a generalization model, some model. Some model which is trained, for example, on fields of Berlin and which will be evaluated on unseen fields of Berlin, or some extrapolating model, some model which is trained, which we want to test on Berlin fields and will be trained on anything but Berlin fields. So for example, we will train it on Paris, on PowerPoint Plunes, and we will test it on Berlin. So obviously this Obviously, this extrapolating model is better, it's harder, but it's better. And why? Because it's kind of universal. We can create only one model only some sorry, only one ensemble of hotspots, and after it will be able to generalize on all future hotspots, it will be able to segment all future codes. But in this presentation, we will see both kinds of models. Both kinds of models, generalizing and extrapolating. So, here are some first results. So, it's a generalization on bearing. So, on the left, you have some icyotophys, some that are used, that are unseen during the training phase. On the middle, you have some targeted prunes that are just there to compare with our predictions. And on the right, you have the segmentations of the convolutional neural network. And actually, you can see that. And actually, you can see that it's a very satisfying result. Even if the plume is not visible in the Ixioto field, the CNN can retrieve the direction of the plume, the location of the source, and the thickness of the source. So you have to compare this with this, and this with this. You can see that they are quite similar overall. Here is another example, but with a Here is another example, but with a multi-plume area. So, again, here are the XCO2 fields that we are fed as input to the neural network. Here are the targeted segmentation, so it's a target. We don't have access to this information during the evaluation phase, of course, just there for comparison. And here are the predictions of the neural network. And again, you can compare the predictions are quite close to the truth. There are some mistakes. The truth. There are some mistakes, for example, here the tails of the plumes, but it's quite good overall, and it's not cherry-picked examples, it's really like the average performance of the neural network. And finally, here another example, but with extrapolation this time. So again, here you have the X-O fields that are fed as an input to the neural network. Here it's a neural network that is trained on everything, but not Berlin, fields. Here you have the targeted prunes, and here. Here you have the targeted prunes, and here you have the segmentations. So, although it's a bit less good, extrapolation is a bit less good than generalization, it still achieves a good job overall. We have the direction of the plume, we have the location of the source, we just don't have the thickness of the plume, it's a bit over-evaluated here. Okay, so now that we have our segmentation model, it's performing well, we can retrieve the plumes on the images even if they are not even. Even if they are well hidden from the background, we will go for the inversion task. The real task, why are we developing this? It's to retrieve the emissions of the hotspots. So, inversion is a bit different than segmentation. This time we have an image to scalar task. We want to transform a bunch of images into a scalar, which is the emission flux rate of the hot. Rate of the hotspots. So it's a task of regression. To help with inversion, we use other kinds of fields. So to add to this Ixioto field in which we want to retrieve the emissions from hotspots, which is located in the center, we'll use additionally some wind fields, the results of the segmentation model and the inotophyte, which is a heavily cohemated species with a X CO2, as you know. As you know. Of course, let's be a bit organized. Like in the beginning, we will speak first about the data. Here we focus on power plants data. We want to retrieve the emissions of power plants. And the reason is that I don't have yet done anything on cities. We will consider a variety of power plants. We consider power plants such as Boxberg, which is a high emission rate, a high emission power plant in Germany. Emission power plant in Germany and also a multi-plume area. We have a lot of other power plants on the same image, which can maybe perturbate the performances of the neural network. We will also consider low-emissions power plants, such as Patnau. Actually, it's such a low emission that sometimes the emissions, the other anthropogenic fluxes in the image, are bigger than the emissions of the power plants. So, again, it can be perturbating for the neural network. Work. Of course, we are never satisfied with few data, we want a lot of data, we love data. So, we will in particular use a technique of scaling. At training time, we draw a random scaling parameter that we apply on the Lippendoff, on the plume, and on the emission flux so that we can generate artificially again an infinite number of data to train our neural network. So we spoke about the data. Of course, now we have to speak about the model. Here we had two main surprises. First surprise: inversion is a less complicated problem than segmentation, and when I mean less, I mean really, really less. For inversion, we need like 5 million parameters to achieve good results in the encoder state of the units. And for inversion, we only need a model with only 100,000 parameters. So it's like 50 times less. So we also times less. So we were surprising, but it's like this. And the the second surprise is that we uh are considering uh we were considering several state-of-the-art art models such as a squeeze net or shuffle net and for a reason we have less good performances than a much simpler neural network with only convolutions multiple links or about the data. But we do not keep up, we like a complexity so we will find more complex model. And of course about the training we how do we How do we train the model on the data? Here we train the model only in extrapolation mode. So we will, for example, if we target the emissions of the Boxberg power plants, we will train the model on everything but not Boxberg. So we'll train it on all the other power plants, on Berlin, on Paris, if it works, but not on Boxberg. And now the results. So we have two targets. Uh we have two targets here: the uh power plant of Lipandov and the power plant of Patno on the left, on the right. So these are kernel densities, so almost histograms. And we are considering every time three ways to train our models with three different bunch of inputs. The first one is when we only use the CO2 field and the wind field. The second one is when we add to this field the segmentation predictions of the segmentation model. Segmentation predictions of the segmentation model. And the third one is when we add the NO2 field to this field. So, several things can be noted. The first is that we have very good performances with this model. It can be compared to other traditional methods from the field, such as mass balance or cross-sectional methods. First of all, these methods cannot be applied to low SNR plumes, such as Patino, for example. As Patino, for example, so it's not non-applicable, while our model has a median performance of 28% relative error. And on power plants such as Liependorf, methods like this, cross-sectional, for example, give a median of 35% when applicable, so for a subset of the plumes of Lippendorf, while our CNN model is more about nineteen percent of ratiable. And the second thing that we can And the second thing that we can note, which is interesting, is that for high SNR plum uh uh plumes, for l high SNR uh power plums, basically uh the addition of NO2 fields or of our segmentation predictions are useless. Basically the CNN can work by itself the solution without the need of this additional data. It's not the same for a low emission power plant such as Acnio where we Such as Patnio, where we need this additional data to get good results or to get better results. And so I will conclude with that. There are several advantages to use CNN for this hotspot inversion. The first main conclusion is that it's able to perform inversions on all plumes, while other techniques can only apply be applied on the high emission. Emission sources. The second conclusion is that it outperforms standard plume inversion methods with or without the help of additional fields such as Ego2. And finally, I didn't speak of that, but the performance is not degraded by the presence of multiple plumes on the same image. And just a last thing before I am done. We want now to do some inversion of city plumes, but we have an issue which is hard which is that we do not have a lot of data. We only have Paris and Berlin. We only have Paris and Berlin. So, if a nice person has a nice data set of CT prunes, we are interested in it. And thank you for your time. Thank you, Marie. So, do we have any questions? Yes, please. Thank you for the nice talk. I was wondering if your neural network is also able to give you an estimate of the accuracy Estimate of the accuracy of your emission estimate or your full estimate? Yes, there are a lot of uncertainty techniques that can be applied, and it's something that we should do in the future. But we didn't do it yet, but it's totally possible. I had a question on your internet noise or function, so we all had it in the last slide a little bit about clouds, but we all know the space and all noise. In the case of uh uh noise which is uh not a quotient. Yeah, and I don't think it's so easy uh because uh I don't think it's so easy to deal with uh random okay I think there is not a lot for example uh other people they coded this uh non-random noise and first it's very similar to uh random noise. it's very similar to uh random bushel noise from what i have seen and i i think it's actually more easy to deal with uh non-random noise because you the model can learn uh the uh so talking about spatial records and we see that a lot in similar observations that you have these structures that are not just all in the same location but they change and it was a function of having micro voice to colour the range i think the the model can learn about this it cannot Can learn about this, cannot avoid to take into account these structures if they exist. While a random Gushen noise, it cannot learn about them because in a lot of the other lawsuits, but that assumption I'm just doing with it as much as anyone else. Just a little bit as much as anyone else. It's more random noise, I mean, then if you have this really built these other structures that are any structured noise, that becomes a bit problematic. I agree that I didn't try with more realistic noise. As a general rule, I think neural networks are better to deal with more structured noise than with total random noise. But maybe in this case, it could be If we have enough examples of these structures, then it can learn what these structures can learn what they are, how to not take them into account, what what it should extract eta. I think the point is that these structures isn't something that is controlled in the centers, that they are more perceived. They are sorry. They are structured, then they are logical from image to image. That's a new feature of satellite data. But there is a logic in this structure as well. Well, not really, because it very much depends on how the atmosphere. Because it very much depends on all the atmospheric conditions. Then maybe there is the idea to add additional feeds which can help to write a logic between the structures that will appear and the feeds that we have in the field. Yeah, and I think that's what what's happening a lot is when people try to correct this retrievals, but this is a complicated model. I don't people work on that when you use and we still don't exactly get it. Because it was still exactly the way it's now, so it's just something I think was there. Maybe I missed this, but I was curious, so when you do this inversion, you also use the wing field. And we know that sometimes the wing field doesn't match with also what you have for the blue choice, that you have errors, for example. How well does your CNN? How well does your CNN deal with that? Can it detect when the green fields and the blue are inconsistent and so on? Honestly, I'm not really sure. I agree that I noticed it was not always matching. I think it's more like the accumulation of information. So if the wind fields do not agree, I don't know for example with uh the results of our segmentation model, or if they don't agree with the analog fields, then probably it will not be used uh so much than when it's agree. Than when it's agree, I think it's the accumulation of information which is important. Is it possible to diagnose that for you? Yes, I mean I should look at the results. I mean I didn't look at, but I should kind of statistical study of what happens when the fie uh wind fields that do not agree with the with the model, uh uh sorry, with the reality uh uh does it uh degrade the performances much? Did it uh degrade the performances much or not? I didn't look at it, so it would be interesting to get this for one last question. Well, then uh thanks again uh Jose We welcome uh actually the last speaker of today, which is Indian Shank. Uh And change