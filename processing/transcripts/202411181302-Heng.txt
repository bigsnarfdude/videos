Thanks Michael. Thanks uh stand in chair. Um yeah so I to be honest I I just gave this very generic talk and uh title and then I realized that a lot of other people talk about multi-messenger stuff so there might be a bit of overlap. But nonetheless, to kick off, I also was trying to look for a pretty picture and I couldn't think of one. So I went to AI and asked me to make this multi-messenger astronomy with machine learning. And this was one of Machine learning. And this was one of four options. Four options all had very similar variations. Clearly, it thinks astronomy involves a telescope, and scientists have to wear white coats or wear a belt. This one I chose because it says machine learning and it's computers, so it kind of looks like machine learning. But none of it really was anything related to multi-messenger for astronomy. The other thing is that my talk actually will be talking about flows, which is the main We'll be talking about flows, which is the main kind of machine learning we're talking about. So it could also be called fun with flows. And this is actually an image made by Issa, which is, I just started writing gravitational waves, right? So I thought it was kind of cool. Okay, so I'm just going to dive straight into it. And everyone knows about TW170817 and the associated multi-messenger observations. Sorry, I'm pausing because I can see myself in the back there and it's following me left and right. Okay, now it's stopped. Okay, I don't know. Anyway, I probably shouldn't have stopped. I broke the wall. Anyway, so the point is that from the observation of GW170817 and the gamma ray bursts and the other EM observations that came with it, there was a lot of evidence towards a structured jet. Evidence towards a structured jet. So, what is a structured jet for those who are maybe not new to this idea? So, before 17017, most people felt that a gamma-ray burst took this kind of structure. So, here's the central engine, and the burst is out from the pole, the rotational axis. And then there is a core of ejector, relativistic stuff, and this is a gamma-ray burst, and it's uniform across this core, and then there's a very sharp cut. Across this core, and then there's a very sharp cutoff. So it's a uniform top hat jet, is typically what they call it, because of this very hard cutoff. But then, because 17017 and the gamma-ray bursts were observed, and 17017 had a very high large inclination angle, then people started to think about these things called structured jets. And some people, a variation of structured jets were these jets with cocoons, which is just another way of saying a different structured jet. So we decided a long time ago that we were going to try and see if we could make, we could see if we could figure out what the jet structure could be. And we just did this toy model simulation where we simulated a bunch of prompt GRB observations, so just some fluxes in the appropriate band. And then simulated some gravitational wave data. I've simulated some gravitational wave data with uniform distribution in inclination, put it through a hierarchy of the Bayesian inference, and then you just say do a model selection. So you get the base factor, and from the base factor you say, hey, is it this model preferred over another model, for example? So this is quite old, and there's no machine learning here. But just to introduce the sequence, this was done in 2020. And the thing that we said is that for this toy model simulation, Thing that we said is that for this toy model simulation, so what's happening here is that the base factor for the power law jet structure, one form of the jet structure, versus the Gaussian jet structure, the base factor can become informative if you have, let's say, 25 or 50 binary neutron star observations. And in this setup, all we did was to say, hey, here's a gravity, here's the BNS, and of course. Tiers of BNS, and of course, back in 2020, we were thinking, oh, we're going to get lots and lots of BNSs. And so if we get 25 BNSs, or even 50 BNSs, we could already start to distinguish, because the base factor could tell us that there's strong favor for this PL, the power laws jet structure, versus in the other case, if it's a Gaussian jet structure, there could be a strong favor of Gaussian jet structure as well. So just to complete the basic Just to complete, the base factor looks like this, or rather, the logger base factor looks like this. So, numerator says Paulo preferred, denominator says Gaussian jet preferred. So, each of these dots here are just multiple universes where Paolo or Gaussian jet is the true structure. So, okay, that's nice. So, basically, from joint observations of gravitational waves and prompt GRBs, we can say something about an overall jet structure for gamma ray blocks. Structure for gamma ray births. And then further on, Fergus, who was working on this, and now Fergus is making lots of money on quantum computing in London, so he's not looking back, unfortunately, did some work involving real data. So we decided, hey, since we know that binary neutron stars are the progenitors of gamma-ray bursts, and we've observed gamma-ray bursts for a very long time, thanks to many. A very long time, thanks to missions like SWIFT, and we have a rate of a binary neutron star from one, two observations. We can combine these rates together and just say, hey, this rates, since the progenitors of gamma-ray bursts are binding neutron stars, then the rates must be related modulo the structure function, the jet structure function. However, and so we can combine And so we can combine just the 1708-17 or 1905-24 observations in here and do the analysis. And then combine it also with the rate information from the past. Now, the issue with the rates, of course, is that we know that most of these short GRDs observed by Swift have a redshift of about one. And most of the binary neutron stars that we see have a redshift of effectively zero. So to combine these two things together, we have to build a model. These two things together, we had to build a model that took into account the selection effects from GRB detectors, relative to zig beaming, stellar evolution, luminosity function, and so on and so forth. And also, to make it fast enough, we had to use this thing called Nessie, which is a normalizing flow, nested sampling, augmented with normalizing flow sampling to speed it up. And Michael Williams is the And Michael Williams is the guy who basically led the development of Nessie. So, yeah, we combined all these things together and we got jet structure model information from observations. And also, as a nice side product, if you assume one of the jet structures, of course, you've made a model-dependent assumption, but you can have improved rate estimates on your PNSs. So, okay, so far, also not a lot of machine learning, just a machine learning trick to speed up our analysis. Machine learning trick to speed up our analysis to make them more tractable. Just to talk through what was done in a pictorial way, so here are our jet structures. This is the top hatch structure. But because in the gamma-ray burst you have things moving at relativistic speeds, then you have to take into account different Lorentz factors depending on how fast the ejector is going. So your top hat jet actually gets spread out to be like this. Although you can't see this, but this is from here to here is two orders of magnitude, and so from here to here. Here is two orders of magnitude, and so from here to here is six orders of magnitude. Okay, but that's where the spread comes from. And so here's a power law jet and it spreads out like that. And then here's a double Gaussian structure to mimic the cocoon style. So the cocoon is out here somewhere or something like that. And so there's a double Gaussian. We also had a single Gaussian, which is basically just this going all the way down without the second Gaussian. Anyway, here's the base factors. There are a lot of numbers here, but let me just highlight. But let me just summarize. Let me just highlight the main result, which is these numbers here. So, all these numbers here are relatively small in terms of base factors. But, what is clear is the trend, right? So, the way to read this table is that it's this jet structure versus this jet structure. So, Gaussian in the numerator, top hat in the denominator, power law in numerator, top hat denominator, double Gaussian top numerator, top hat denominator. So, if this is positive, So, if this is positive, it means that the numerator is preferred, which means that all these bubbles are preferred over the top hat function. So, if you like, this is observational evidence towards a structured jet. So, actually, we should not be thinking about, well, the observational evidence is still low, but still, this is just combining observed data to tell us that actually consistently, Consistently, the structured jet is more likely than the top-hat jet. Okay, and then we can do from this analysis because it's hierarchical analysis, we can look at the different parameters. So again, sorry, this is very small, but we can get things like the opening angle or the core angles for the jet. For example, here's the Gaussian, and here's about six degrees, so this is roughly 5.5 degrees is where the peak of this is, plus or minus a few. Plus or minus a few. Here's the rate estimates, and you can't really tell, but I'll show, I'll highlight that in a minute. Here's the parameter for the relativistic beaming. Here are two parameters for the luminosity function, which I won't worry about. But the thing that is maybe more interesting is just to look at the legend because all these contours are just like a mess. The point here is that we can do the analysis for just one event, 170817, or 1908. 170817 or 190421 or we just take the that's R, yeah, that's right. We just take all the old rates information and use the rates information and combine the rates information to get the posteriors. Or we can take these three and combine them together and get this all thing in blue. But yeah, so let's look at the rates. So this is the prior, and the prior is basically GWTC3. Okay? And so we put this prior in, and depending on which And depending on which jet structure model you prefer, then you can see that in all cases the error bars are improved, which is not surprising because you've assumed more information. Nonetheless, it also points to the fact that if we have better inference on jet structures, that actually we can have better inference on the rates, which has implications for other things, for example, chemical enrichment of the Of the Milky Way. So that was just with gamma-ray emissions. We are, oops, what am I doing? Yeah, so we are moving towards incorporating other bands because we want to have multi-messenger stuff. So we have X-ray afterglows, actually, which are basically included. And Kilonovi, which I will talk about more in a moment. And to mix it all into this big pot here. And as Michael mentioned this morning, there's this NMMA thing. I forgot what N sex for. What's NM? Nuclear? Nuclear, sorry, yes. So this is a very similar analysis. And honestly, I'm not tied to any code base. This is just what we wrote. And I'm willing to work with anyone to do it, whatever. The other thing is that what I'm about to do, what I'm about to talk about with the Kilonova stuff, actually uses a slightly different code base, which is. Yeah, there it is. Okay, sorry, your bottle's in the fake. I can't see you there. Yeah, which uses Redback. So if you are, Redback is another framework for multi-message inference. Okay, but ultimately this is my vision, my dream, if you like, that we can take all these bands, put it in the pot, we do jet structure inference, but we also do EOSs, like for example what Michael was talking about this morning, and chemical enrichment, and so on and so forth. Okay, so. Um okay, so let's just talk about kilonovi because this is the next thing we want to add. But we are doing it in a slightly different way. So we are um with kilonova models, there are lots of models on the market. And well, we have this idea that basically we want to make we want to make a generator that is, well, actually, our first motivation for this was to make. Our first motivation for this was to make a rapid generator that could make predictions for what kilonova-like curves could look like given masses and lambdas, because masses and lambdas are what we get from our posteriors, our just BNS posteriors. And then we have this question, you know, EM bright or not EM bright? Well, we could just give them a curve, basically. And the curve will embed all the masses and lambdas, inclination angles if we want to. Inclination angles, if you want to, and distances and everything as well. So, the idea is to take this, predict all the light curves, and then maybe add it as extra info for loads. But this, by the way, because we are going, you're using a flow, and I'll mention this in a moment, we can also go backwards, which means that we can put light curves in and get masses and lambdas out. And this is a way of also doing multi-messenger analysis. Basically, a way of putting the kilonov light curves into this overall interest. Kilonoval like curves into this overall interest. Anyway, so what I'm talking about is something my student was doing, Xiaofay. Also, in collaboration with all these people, including Nicole, because something is an expertise in Redback, and we're using this module called Redback. I want to say, full declaration, that there's already this thing called KilonovaNet, which will bet you Kilonova-like curves with machine learning, in this case, variational auto-encoders. It's led by this person who I don't know how to say his name. It's led by this person who I don't know how to say his name, but one of the co-authors is Zoha Doctor, who until recently was a member of the collaboration. So it's a nice tool. Actually, it's part of Redback, am I right? Yeah, it's inside Redback framework. So you can use it to generate this. However, the difference between this is that this will depend on, let's say, kilonova parameters. So the ejector mass, the dynamic of the winds, the opacities, and so on and so forth. Whereas what we're doing here is we're Whereas, what we're doing here is we're relating the BNS parameters, or the BNS estimated parameters, to what could be the like curve. And so it's slightly different from that, apart from just using a different kind of machine learning. Okay, so what do we do? So the training data is we take this skin thing called the BNS model, which is incorporated in Redback. It's basically based by, made from Matt Nickel and Co. And yeah, so firstly, we need to convert the masses and lambdas into We need to convert the masses and lambdas into the equivalent ejector masses and ejector velocities. And then we have all these other parameters that we don't really, we can't infer from the VNS. Oh, actually, this we can, but at this point when Xiao Fei wrote these slides, we were not considering the inclination, the viewing angle yet. But that's just. So all these other parameters, we just simulate, give it to the flow, and the flow can do the mapping. And this will just map into the GT. This will just map into the degeneracy, the uncertainty associated with the prediction. So, if you like, this is one curve for this set of parameters, these different masses and lambdas, and then if you vary all these other parameters, you get a bunch of curves that look like this. And so, let's say the truth is probably somewhere between this lower line and the upper line. And that's basically what we want. We just want a band and say that your truth is somewhere in that band. Somewhere in the band. I mean, yeah, let's just cover that. So, and also we simulate these values of the different opacities in uniform distributions in these ranges and so on and so forth. So, all these are encoded into the training data, and then we train the data with these things. And we use this thing called the flow. So, the flow, the normalizing flow, rather. The normalizing flow, rather, and we didn't explain it pictorially. So you have some complex distribution, which is like our light curves. And then we take the light curves and we put it through the flow, and the flow tries to map all these light curves into a multivariate Gaussian distribution. And so the way the flow tries to do it, two minutes. Oh my God. All right. The way the flow tries to do it, The way the flow tries to do it is to make this as Gaussian as possible. So that's the loss function. And we use this thing called blast flow, by the way. That's just another bunch of ways, which is, for those who use flows, is a layer on top of n-flows. All right. So all the light curves go in here. They go into latent space. And then when we want to generate, we just say give the numbers, give it masses and lambdas, and we get the light curves up the other way, which is basically what happens here. Which is basically what happens here. So, if we give it masses and lambdas, then we get the light curve here, and we get a band basically. Here's the range we predict the light curves for this particular band. And the outcome is that, yeah, this is what we had for one of the predictions. This is unseen, so this is not part of the training data. What's more, this equation of state is not part of the training data. But it's still, see, the point is that the original data, which is a black dot, clearly lies between the uncertainty band, and so the original data is data. The uncertainty band, and so the truth is consistent within the uncertainty band. Of course, the sharp-eyed ones amongst you will note that the uncertainty band is of the order of two magnitudes in astronomy units, which is very large. So we want to do more things to firstly improve the training, but secondly, see if we can figure out what the degeneracy could be and to make it work. And right now we only predict in one batch. Right now, we only predict in one band, but we can get this to predict in multiple bands spectrum, which is what we're working on. The last thing I want to say, which I'll go through very quickly, because again using flows, but with masses and lambdas, we can also predict the equational state. So here, instead of having light curves, instead of having light curves in your training data, we now have equational states with corresponding masses and lambdas. Then we can actually get it to predict. Then we can actually get it to predict a band, this blue band, which is uncertainty associated with the equation of state. And you can see that this is consistent with the LBK result. This bit here, just ignore it, please, because this is just an artifact of how the data was created. And you can see that the LBK one has a drop that way. We decided to drop this way. But yeah, so the idea now is we combine everything together and try and make life happy for everyone. And I think I will stop there. Oh, wait, no. As a prelude, we combine this. As a prelude, we combined this with a dingo-like flow thing for BNS, and we did the same thing with different stuff, and we got it to work as well, which is we're quite happy about. And that's it. I have one more slide, which is an advertisement, but maybe we'll take questions first, and I'll talk about them. Thank you. A couple of minutes for questions. Yeah, I have a question. So, if I understand well, your goal is to release like Your goal is to release like curve uh to help for the strategy. But uh my question is do you think the product and the project is like curve or more education like for big time? Why like curve in fact? Because in like a many incentives you have them did? Sorry. Yeah. So why in Leica? At Lycurves because that was what we started with. Now if you think that's a better product Now, if you think there's a better product, we should talk about it. I'm happy to talk to you about it. Yeah? So, when you generate a predicted light curves, do you marginalize over equations of state uncertainty? So, do you fix it? At the moment, we fix it. So, if you like, by giving it all these like curves that are generated with all these range of parameters which we don't know, we only condition it with these numbers. Give it, we only condition it with these numbers. We are trying to marginalize over all these uncertainties, basically, which is where we get this big band of uncertainties from. Right now, we only give it one EOS, but in the future, we could give it a whole bunch of EOSs, and effectively, again, we get the band, which will marginalize over the different EOSs, effectively. I have another quick question. So when you use the BNS radius, measured from GW, that already had That already has the information from the toolbox that you use on the other part of your approach. Can't you just estimate the rate inside and marginal? Is there any double compared to that? Yeah, no, that's a good question. I thought about this before, but no, we don't because, well, what is done is that the difference is the same thing. The difference here is that the improvement in the rate here is not because of double counting, it's just because you are picking one of these models, and so you're just constraining the range that the models could be. And that's basically where it would come from. We could start from scratch as well for the BNS and do the whole thing that the LBK did to get the same rate. I agree with you. But we didn't do that because we were just trying to reduce our calculations. But there isn't any double calculus. No, I don't think there's any double calculations. Okay. Very quick. Okay. Um after the talk. Okay, yeah. Thank you, Sian. Well, Chris is walking up here. Yes, he is. Where are you? Yeah. Giara Maldi happening in Glasgow next year. Registration is open, and so please think about registering. We're trying to get the registration fee as low as possible. So right now, your main So, right now, your main aim is just to register your abstract so that we can have the info. And then later on, depending on what you think about the price and whether you get a talk or not, then you can actually do that later. But what we need to know now for us is how many people are coming. So, please think about registering at no commitment rate. Yeah, thank you. You were gone and we couldn't remember who we supposed to chase, so we did microbiome. 