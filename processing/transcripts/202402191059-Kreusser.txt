Generally speaking, the interface of applied analysis, numerical analysis, optimization and data science and machine learning with various applications. But today I will talk about the specific topic of how we can use dynamic systems in generated modeling. So kind of just as a general overview about general modeling, there are different types of approaches which one can consider to run probability distribution from a finite set of samples. Set of samples. So, first of all, there are kind of likelihood-based approaches where kind of the idea is that we want to explicitly model the probability density and maximize the resultable likelihood. So, common approaches in this field are, for instance, variation in auto-encoders, normalizing tools, and auto-regressive models. However, they also come with some kind of difficulties, and this is the reason why implicit approaches have been suggested. Implicit approaches have been suggested, where kind of the general idea, as the name implicit, kind of suggests, is that we want to sample without knowing the density. And here, one popular framework in that direction are generative adversarial networks. And last but not least, more recently, score-based models have been suggested where the key idea is that one wants to estimate a density rather than the probability density function. To function. And all these approaches kind of come with different advantages and disadvantages. So, in terms of those likelihood-based approaches, they can be in fact very restrictive because these are kind of those explicit modeling approaches where we need to kind of approximate or estimate some kind of density functions. And so, this in particular means that we need. This in particular means that we need to guarantee that our densities are non-negative, so that they kind of make sense. And in addition to that, we also need some appropriate normalizations. And this can actually result in quite a lot of instabilities when training these types of approaches. Then this also motivated kind of looking at different types of approaches and in the context of those. In the context of those implicit approaches, guns became very popular. However, again, they can be very unstable when training them. And last but not least, more recently, score-based models have been suggested, which have actually been very successful in terms of many generation tasks. And they do not require And they do not require tractable normalizing concepts, for instance, as in the case of explicit approaches. And in fact, they are also based on dynamical systems, which makes them of interest to kind of study in more detail to understand how these types of approaches actually work. And what I forgot to mention in the context of generative modeling, those Wasserstein guns actually were initially very popular. were initially very popular because they were motivated by the Wasserstein distance. So as a mathematician we kind of asked ourselves the question whether this motivation of approximating the Wasserstein distance actually really makes sense and whether we can investigate this further. And this is kind of part of today's talk and the other part will be kind of related to those score-based diffusion models in the context of chemical. The context of can we understand better how different notions in that context actually fit together. So, to get kind of started with the topic of Baselstein generator of adversarial networks, the disjoint work that cover people and in particular my PhD student, Jan Sch√§nzug, who's kind of just finished his PhD at the University of Cambridge, did a lot of those experiments I'm going to show next. So, in general, the key idea So, the key idea when kind of studying generative ethno-cell networks is that we have two neural networks, one generator and one discriminator. The generator has some random noise as an input, and its aim is to generate realistic data. Then, on the other hand, we have a discriminator whose key aim is to distinguish true and generated data from each other. And so, as you can kind of see, those And so, as you can kind of see, those two neural networks basically compete against each other. So, the generator wants to produce as good samples as possible, which are very close to the true data, even for barefect data, whereas kind of the discriminator wants to kind of distinguish those two and generated data from each other. And so, kind of writing this down a bit more mathematically, here the key idea is that we have two neurons. Is that we have two neural networks, a generator which I call GCETA, which lives kind of on some latent space and its range is some data space X. So we have kind of our noise as an input and get some data as an output. And we denote the associated generator distribution by p theta. And we hope that this generated distribution is sufficiently. Distribution is sufficiently close to that target distribution which we want to approximate. And then on the other hand, we have the discriminator, which is again a neural network where we kind of train with respect to twice alpha. And here the key idea is to distinguish between samples drawn from training data and samples which were generated with Which were generated with the generator. And to kind of formulate this a bit more mathematically, one could say that the discriminator should have some output as associated with a probability value. So for instance, output one could be associated with real data, and output zero could be associated with the generated samples. And so this kind of already indicates, on the one hand, we kind of Indicates on the one hand, we kind of the generator is trained with respect to some base theta, the discriminator is trained with respect to some base alpha. So we can also phrase this as an optimization problem with respect to both alpha and theta. So more specifically, when kind of looking at the discriminator, the key idea, if you're kind of saying that real samples should have above, should be associated with. Should be associated with 1, and fake samples should be associated with 0. This means that if we draw some x from our target distribution, we want to maximize the discriminator objective. Whereas if we have fixed samples, so we kind of take z from our underlying noise distribution and consider g theta of z, which are kind of our of set which are kind of our tech samples and then d alpha of text samples should be minimal. And so this is kind of the key idea on how we want to determine those parameters alpha. And then we can kind of phrase this as part of the so-called vanilla gun, which is kind of the first gun proposed by Goodfellow in twenty fourteen. In 2014, that kind of the first term here is kind of just an additional log, but this doesn't really make any difference, is associated with kind of this maximizing the probability of real data. And the second term over here is associated with minimizing the probability for fake images. So that's why this to kind of all phrase it as a maximization problem, we just consider the expectation of a lot. The expectation of the logarithm of Ohm minus the alpha evaluated for fake images. And so since discriminator and generator are competing with each other to kind of evaluate now the generator's objective, we basically just add another optimization in front of it, namely since they are kind of competing against each other, the generator's objective is to minimize with respect to It should minimize with respect to those weights etc. And in terms of rephrasing this, so we have kind of this generator's objective over here. And what turns out actually to be quite nice is that one can also rephrase this generator's objective as the Jensen-Chen divergence between the target and the generated distribution. And the generated distribution and solve kind of this standard problem for the vanilla gun. However, training the vanilla gun comes with some difficulties. First of all, it is very well known that the optimal discriminator leads to vanishing gradients. And so in any kind of optimization problem, it's not desirable to have kind of vanishing. Desirable to have kind of vanishing gradients. And secondly, there's actually another issue: namely, since we have seen that the minergang can basically be rephrased using the Jensen-Schengen divergence, there's an issue related to looking at the Jensen-Schengen divergence in general, because the Wasserstein distance, for instance, we know that if we have two distributions. Two distributions which have kind of slightly different supports but are converging towards each other, the Basserstein distance actually decreases, which would be kind of very desirable. Whereas for the Jensenchen divergence, once you have disjoint supports, it's in fact maximized. So this kind of motivated in fact the introduction of the Wasserstein distance, because if you have kind of just slightly different supports, then the Then the results of using those standard vanilla guns might not be desirable. And this is actually the reason why this optimization problem using the Yankees and Chen diversions was rewritten in terms of the Basserstim-1 distance. So basically, one minimizes with respect to theta the Basserstim-1 distance between the target and the generated distribution and looks at the And looked at this optimization problem instead rather than the classical vanilla gun, in the hope that now all the problems can be circumvented and everything is working much better. However, when kind of phrasing the problem using the Baselstein distance, what you can already see immediately is that basically now we have an additional constraint, namely that we look at one Lipschitz functions and functions and this kind of comes with additional problems when formulating the optimization problem. So more precisely in fact there are two challenges which have to be overcome when looking at this Fassersteigants objective. Namely first of all it's impossible to optimize over all absolute functions so one needs to come up with suitable approximations on the state we are optimizing over and We are optimizing over, and secondly, generally speaking, we are interested in the solution of full probability measures. However, when kind of running any simulations, we only have access to finite samples. So it's also not kind of clear immediately how kind of going from full measures to finite samples actually kind of influences the underlying. kind of influences the underlying methods. And so this kind of raises the question when kind of looking at the standard Wasofstein-Gann algorithm, to what extent are we actually minimizing the Baselstein distance or are we actually minimizing something different? And so kind of to look a bit more into the details on how Baselstein-Gunn algorithm was proposed, it basically consists of two loops. So the outer loop aims to determine the optimal basis. Determine the optimal base theta, and the inner loop is for the optimization with respect to those parameters alpha for the optimal discriminator. And to kind of deal with this assumption that we have to optimize over all one Lipschitz functions, one can kind of introduce some auxiliary functions. So the first function v over here is kind of our original objective, and then one kind of introduces. Objective, and then one kind of introduces another constraint enforcing that the gradients of our discriminator should, roughly speaking, be around one. With the hope that once we optimize here, our objective V is roughly speaking close to the Wassers Deim, one distance between the target and the generator distribution. Distribution. So, to kind of see this in practice, Jan run some simulations, and what you can see here is we focus on fixed, finitely supported distributions, where we considered different coefficients for the constraint that the Lipschitz constant should be equal to 1. And so, if the coefficient is very small, for instance, 0.1. Small, for instance, 0.1, you can kind of see that the reciting Wassenstein distance was kind of around 4, and in other cases, it was kind of significantly smaller than that. Then one could kind of estimate what is actually the Lipschitz constant in those cases. And in fact, when the coefficient was very small, the Lipschitz constant was kind of much higher. And even after normalization, kind of roughly. Normalization, kind of roughly speaking, in all those cases, the Rasserstein one distance was in the order of 0.8. However, the correct value would have been 41. So it's already kind of visible that the approximation of the real Masserstein distance, which can explicitly be computed for quite nightly supported distributions, wasn't really approximated very well. And so this can be related to seven. So, this can be related to sample complexity issues and kind of motivated us to look at a specific scenario. Namely, one can it's kind of been proposed by Reid and Bach that the center of Assassin distance concentrates very well around the expectation and that one should rather look at kind of the associated expected. Expected Wasserstein distance rather than the Wasserstein distance itself, and also link this with the intrinsic dimension. And for the data sets we considered, that would be kind of in the order of 20 and study those associated experiments. However, also in this case, when kind of looking at these more mathematical foundations and run the simulations, we would kind of hope that the expected Basserstein distance. That the expected Basserstein distance is about zero between samples from the same distribution. However, it turned out that if you kind of study the number of samples being larger and larger, then for kind of 10,000 samples it was still above three. So if you wanted to kind of extrapolate this a bit further, then it actually turns out that if you want a very small buzzer. A very small Wasserstein distance, we kind of needed something like 10 to the power of 50 centers, which, of course, is not practical at all. So I guess we have to kind of stop hoping that we are going to approximate the Wasserstein distance very well, even though it kind of was originally motivated that this should be the case. And in fact, then the question arises. Then the question arises: what do we actually see if the Wasserstein distance is, in fact, not kind of minimized very well? And so here you can, on the left-hand side, you can kind of see samples both from the target distribution, like basically real images. Then, here in the middle, you can see kind of an average face, and on the right-hand side, you can see You can see a geometric K-means image. So, of course, both those images here in the middle and on the right-hand side don't really look very nice. But if we kind of look at the source-tated samples and measure the expected Wasserstein distance, then you can see that the kind of desirable images here on the left-hand side have an expected Wasserstein distance of 50, whereas kind of the average one of 47.9. 7.9 and the geometric K-means is even kind of in the order of 39, so much better than the bonds in the left and in the middle. And in fact, we could also show analytically that those geometric k-means one actually minimize this associated optimization problem rather than the real images which one puts. Real images, which one puts a poof. And so, kind of the question which still remains is why does the Basserstein still kind of work reasonably well? And in particular, what might actually be going on to explain better that we obtain these kinds of findings? And in fact, kind of And in fact, one should note that the Wasserstein one distance for any kind of image can be kind of regarded as some kind of L2 distance for certain permutations of the pixels of our images. And in particular, since the Bassershine distance is so strongly kind of related to the A2 distance and A2 distance. L2 distance and L2 distances don't really capture any perceptual distance, which can also be kind of seen in the following examples. So here in the left-hand side and in the middle, we kind of have two very similar images which are kind of slightly altered by cropping them. So, like, visually, picture number one and two look very similar to us. However, if you measure the source. Measure the associated L2 norms and compare it to some kind of artificially constructed image, which looks completely different. It turns out that the L2 distance between the first and the second one is bigger than the L2 norm between the first and the third one, even so, kind of perceptually they are so different to each other. And we believe that this might be one of the Underlying reasons why, when kind of studying images, there might kind of be these problems arising. So, kind of just to sum up, looking at different types of experiments, we noticed that the Wasserstein guns, generally speaking, fail to approximate the Wasserstein distance very well. And there might actually also be some fundamental underlying problems because of the underlying Because of the underlying edge wrong when looking at these types of problems. We also studied some kind of regularization approaches, but it turned out that even when studying kind of non-Wasserstein distances, one could actually obtain comparable results so that in the end there's nothing kind of special about Wasserstein guns as opposed to standard vanilla guns, for instance. Now, kind of moving towards score-based diffusion models. Towards score-based diffusion models, which have become very popular in the literature recently because in generation tasks they've been very successfully applied. And in fact, they also have some mathematical foundations. And here, in fact, also Tiyo, who is in the audience, and Jan, who was working on the other project, did a lot of those experiments and the analysis. So, kind of the So, kind of the key idea here is that we look at score-based diffusion models, which can be described by forward STEs. So, the key idea is that we want to go from data to noise using some kind of STE where we assume that the drift F and the diffusion G are to be known. And in fact, one can also. One can also associate this with a reverse STE, kind of going from noise back to the data again. So that's kind of the key idea is motivated by Anderson's theorem that one can associate a forward process with the associated reverse process. And in fact, there are lots of other descriptions in that context as well. So one can also look at the associated probability flows. Associated probability flows. And in fact, since we're kind of taking samples from these types of STEs, one can also look at the associated mean field equations and write down the associated Fokker-Planck equations for this type of problem, which kind of gives us different types of formulations, either based on SDEs, ODEs, or PDEs, to study these types of approaches. So, more specifically, when kind of looking at When kind of looking at the particle dynamics, we can write down forward SDE, where we assume that f and g are known, and we kind of consider some kind of initial distribution for our problem. And then we have a marginal density P where basically our samples X at any time T are drawn from. Any time t are drawn from. Then using Anderson's theorem, we can look at the associated reverse STE and in fact kind of both the forward SDE as well as the reverse STE have the same distribution. And last but not least, we can also look at the associated probability flows, which kind of gives a different type of formulation of this problem, which might be desirable, for instance, in the concept that For instance, in the concept that generally speaking, ODEs are much easier to solve than SDEs. So, this might be very useful for computational efficiency. And like kind of those forward and reverse SDE, the probability flow again has the same distribution p. So, this kind of shows how these three different formulations are connected with each other. However, there's kind of one issue when. There's kind of one issue when looking at the reversed STE and the probability flow, namely that we require the knowledge of the cradient log density, both here as well as here, which generally speaking is not known. So we need to approximate this first. And so for this, one can use neural approximations. So the key idea here is So the key idea here is that one could look at a score matching objective. So we have some kind of parameter lambda depending on t in front of it. And then we want to compute, find our score S theta in such a way that the norm between gradient of density minus S theta is minimized. However, in this context, However, in this context, this would require us to know xt drawn from this density xt at time t for all times, which in fact is not given. And we don't compute the gradient of the log density directly. However, alternatively, Directly. However, alternatively, we can minimize some different objectives, namely the denoise and score matching objectives, where basically the key idea is rather than looking at samples Xt from the density P X T T, we kind of have some initial condition X naught drawn from the initial condition and then consider the sociated conditional probabilities which we can kind of compute using. Compute using, for instance, the Provet SDE. And then, kind of, the denoising score matching objective can actually be computed explicitly due to those conditional probabilities. And in fact, it turns out that these two objectives are related in a very nice way, namely that the denoise and store matching one and the store matching objective just differ by a constant. Objective just differs by a constant. So, in fact, it's perfectly reasonable to consider this noise and score matching objective to optimize this problem. And so, this is kind of already a very nice step that we are able to find the associated scores. However, it turns out that this S C. This S theta is not conservative. However, since it's supposed to be the gradient of a log density, we would expect it to be conservative. But numeric experiments basically show that this is not the case. And so alternatively, one can look at some associated potential models and kind of rewrite this problem so that using kind of those Using kind of those potential models, we actually see that the gradient of the potential model is indeed approximating the gradient of the log density as well, which is kind of very desirable. So we can kind of focus on this new formulation. And so what we obtain now is all the desirable ingredients we need. So we started with the reverse STE using the Muro approximation. We can now look at Now, look at the associated reverse approximate STE by kind of approximating our gradients of the log density. And similarly, we can proceed with the probability flow ODs and their associated approximations as well. And last but not least, when kind of looking in the continuum setting, we can look at the Fogger-Planck equations as well as the SOS. Equations as well as the associated approximate Fokker-Planck equations, where again the key idea is that we need to consider all appropriate approximations, as previously as well. And in terms of the analysis, it turned out that rather than looking at the densities itself, it makes sense to kind of study the associated log densities. However, we can also write down. However, we can also write down the forward Log-Foker-Planck equation as well as the source-stated approximate Log-Foker-Planck equation. So this kind of gives us a rather big shoe of equations which you can now study, which are basically all related to each other to some extent. So the question is now on how we can actually use this in the context of score based diffusion models. Go-based diffusion models. And in particular, one question which arises is: to what extent those probability flow ODEs and the solution of those SDEs are actually related to each other. And one way one can actually enforce those relations is to introduce some additional terms in our loss function, enforce. Loss function enforcing that the log-Foker-Planck equation is satisfied. And for that, one can kind of introduce a log-Foker-Planck residual. And this was kind of also the key statement which we kind of proved in the end. So we're under rather weak regularities, assumptions. We were able to show that provided this format blank residual is sufficiently small, then the Wasserstein two distance between the solution of the approximate probability flow ODE and the approximate verse SDE is actually very small. And kind of in terms of the proof, the key idea is that first one needs to associate how the approximate STE solution is related to the neural network solution one obtains. And then, as a second step, one kind of has to associate the solutions of the Of the probability flow ODE and the reverse SD with each other. And so, in terms of the numerical experiments, one can just kind of choose any function f and g which kind of satisfy those rather weak assumptions. And for instance, in the case of f being equal to minus x and g equal to 1, this results in an Ornstein-Ullenbeck process. Process and then again, as kind of just explicitly, one can write down all the associated equations. So, in particular, in the context, since we studied the log-fungabland residual, one can write down the log-fun-abdagger equation in that context and then kind of write down the associated loss function, which is considered, which is kind of the sum of the denoising score matching loss as well. Loss as well as kind of a second term enforcing that the log-Foker-Planck equation is satisfied. So, more precisely, this R tilde is, as kind of previously, our Locke-Frog-Planck residual weighted by some factor WR, which regulates to what extent actually this Locke-Froger-Planck equation is satisfied. The equation is satisfied. And this actually is also demonstrated very well in numerical results. So, for instance, when looking at target distribution ranging from Gaussian mixtures to circles to a checkerboard, one can kind of run the associated simulations. Here, on the different rules, indicate the value of WR, which was kind of enforcing the regular. Of enforcing the regularization with the Loch-Poker-Black equation, and here on going from left to right, we see both the different types of underlying target distributions, both for the SDE as well as for the probability flow ODE on the right-hand side. And what we kind of observe is that if the Log-Fogger Planck residual is not very well satisfied. Is not very well satisfied. So, for instance, if Wr is equal to zero, then there's actually a big difference between the approximate reverse STE solution and the approximate ODE solution, no matter which type of target distribution we are considering. Whereas, once we are kind of enforcing it, so for instance, if WR is equal to 10, then the STE and the ODE solution. The SDE and the ODE solutions consistently look very similar to each other, no matter which target distribution we have. And this, in fact, also coincides very well with the analysis that if the residual, the lock-focker-blank residual is very small, which is kind of the case if WR is big, then ODE and SDE solutions basically coincide. Solutions basically coincide. It is kind of this bottom row. So, kind of just to sum up, so as part of our work, we did a systematic investigation between the different types of score-based model solutions, in particular, focusing on the densities of the reverse STE and the approximate probability. approximate probability flow ODE. We proved a discrepancy between the SDE and the ODE solution using the Fokker-Black residual and the Wasserstein distance observed kind of substantial differences in the numerical simulations between the ODE and the SD solutions, particularly when the this residual This residual was not satisfied, and kind of introducing that residual allowed us to close the gap between the ODE and the SDE solutions. And yeah, this was kind of just some example of my work in the broad field of applied analysis and for differential equations. And in particular, kind of we went also for micro to macro when kind of looking at. Macro and kind of looking at both those SDE and ODE approaches as well as the Fokker-Planck equations and looked at different aspects in terms of the numerical analysis and optimization as well as typical foundations for machine learning. And yeah, thanks a lot for your attention. So thanks time for questions. Questions? Can you talk about the architecture you're using for the protection? Since Teo did most of the simulations, maybe he can talk about this in more detail. But if you differentiate, I mean, then you have to differentiate in order to so you're using double the derivatives and uh yeah so it's a it's a field that you differentiate and we It's a filter that you differentiate and you filter. Should I have to ask another question? Or it's okay to ask another question on this? So the problem that we may have with this is that, you know, the... Okay, so your multi-layer perceptor is a good approximation to a function, but it doesn't mean that it's gradient. It's a good approximation to a function, right? Yeah, so we have smooth activations and smooth. Not always enough, correct? Yeah, so do anything special to make it work? I have a lot of experience with. Make it work. I have a lot of true stuff. So we use a smooth Realu as activation, and I think it happens to be very big, like international problem. And we run this sort of another experiment and we observe this difference. Beyond that, we don't do anything special apart from adding this block configuration. More questions? Yes. Yes. Very nice talk. So I have a technical question, but also I'll explain why I still want to ask it anyway. So I like, for example, in many parts of the talk, for example, so there's a WashesThen 2 error bound between the ODE result and the SD result. So is there any way we can go there and take a look at that? For example, I'm wondering about the dimension. I'm wondering about the dimension dependence. Do you have any ideas like that? It doesn't really depend on the dimension at all. Well, that's interesting. Like there was definitely no lack of you mean so the dimension may be fitted inside the constant, maybe yeah, like indirectly it's kind of hidden behind the constant, but kind of we didn't kind of But if you didn't kind of videos, you check in terms of how it depends on I think it's just indirectly hidden in there to the extent that you don't have anything like undesirable like as the dimension grows to becomes very large this would cause very big problems. I don't think it kind of depends on it such a Depends on it such extremely. So, once you discretize, you are bound to have some dimension dependence. But anyway, so the reason I ask is because there are a lot of great theoretic results analyzing the generation quality of square-based diffusion models. So, most of the results are based on SDE, so reverse SDE, and its digitalization. There are some results based on reverse ODE. Some results based on reverse ODE, the probability flow, but the results are worse. The analysis is harder, and also the dimension dependence of the result is worse. So there are just not enough technical tools, but I feel like this could actually provide a tool to do the analysis if there is some good way to actually dig out the dimension. So the short result. Yes. So for the first part, have you looked at the tails of distribution, the distribution that you approximate? Because you brought in concentration of measure, which means that if the tails of the latent space are mismatched, we use multivariate Gaussian to reproduce the Produce the tails of the generator, you're going to use very ugly non-leak sheet functions to be able to do that. So in many cases, folks try to adjust the latent space distribution to have the same tail behavior. But I don't know what it means in terms of images. Maybe well, we did like we basically just considered mito bad quote and dispersion. Multi-bed Gaussian distributions for the latent space, we didn't kind of consider any type of other distributions. The only thing we kind of changed was mainly in terms of looking at different data sets, but that kind of so consistently works for different types of data. But the latent space was everywhere identical in all the simulations. So uh in terms of of tractability for uh the second part, uh how many dimensions do you think you can go up to with Focus Planck? Well what do you mean in terms of the analysis or in terms of the simulation? In terms of the simulation. Also in well there's this is kind of all kind of indirectly enforced just through those um Enforced just through this Pok√©mon Planck residual. However, kind of the key question is what you actually kind of want to use those results for. So even if you consider higher dimensions, then the question, this kind of just indicates, if you kind of consider this additional log-fun-Kardan equation, that you enforce that ODE and SDE solutions are kind of identical to each other. Of identical to each other. However, if you kind of wanted ODE solutions anyways, then you could also kind of just study the ODE solutions themselves without looking at SDE plus forger-blanc residue. So I agree with you that there are kind of issues when kind of increasing the dimension, but I think the key underlying question is which type of problem we actually want to kind of start with. Of problem, we actually want to kind of study. But in principle, it can be extended to higher dimensions, whereas it's just more difficult to kind of visualize how the solution looks like. Computationally, if you're working with like images, then compute the Fokker climate operator of your neural network in 10,000 dimensions of things. Um in practice it you can do not yeah I was injured in the approximation but yeah thank you. Any more questions? Can I say how great these discussions have been? It's it's fantastic. Let's have more discussions like that. More discussions like that. There will be time at the end of today just for discussion and nothing else. So we've tried to build in lots of discussion. Okay, so next is lunch, and we've got quite a nice lunch break, and we will re-conclude here at half past one. So thank you for all the money today.