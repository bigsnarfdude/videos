thing in this in this line of my work that i've done with uh with everybody so let me start from the beginning uh on this on the problem of estimating positions from distances it's a very classical problem which i'm using as a term here graphic graph embedding so given a weighted graph uh typically the weights in this setting denote distances or more generally more generally the similarities Similarities. So the node set is V, edge set is E, and then the weights are denoted by delta for now at least. And we also play with a desired embedding dimension, little D. The goal then is to find, at least roughly speaking, a point set of same size. So little N here is the same size as the node set. So a point set in dimension D such that the UQ and this. Such that the Eukian distances pairwise are close, and this is left to vague on purpose, close to the provided weights. And so more we can formalize this, so close here can be formalized as air-summer squares, for example. That's sort of the two, the go-to measure of discrepancy. And as you can imagine, this has been proposed before, and in fact, it's one of the first proposals in the In fact, it's one of the first proposals in the literature and is known as the metric scaling. And this thing here is typically called the stress. Okay, so just so you know, we can't be too naive here because this is not a convex problem and it's also a high-dimensional problem very quickly, even dimension d equals two, which is the most common dimension for visualization purposes that are at least used in practice, which is what In practice, which is where those tools really come in often, then this optimization problem is in dimension two times n, so even dimension two. So it's a very high dimensional non-convex problem. So this problem, which again, I've used the word graph embedding, is known as multi-dimensional scaling in statistics or in the related literature and particle psychometry. Geometry, graph drawing, graph realization, Euclidean distance metric completion, sensor network localization, remote engineering, the number of terms, and there are important connections to other branches. In particular, the last one, which is the one I had looked into more, the other ones are in particular, this one is mathematically much more sophisticated. Okay, so what So, one of the first methods for solve, at least attempting to solve this problem, is classical scaling, and it doesn't really tackle it through minimization of the stress. But it turns out it's exact when it's possible to solve the problem exactly, but it requires all the distances to be provided. And that's a method that dates back to the 50s. It's called classical scale. The terminology here is confusing, and I'm still confused. Still confused, but that's how I call it at least. And so we have some work. Oh, I forgot to compile this. Okay. Somehow I didn't compile here, but there was some previous work on obtaining perturbation bounds for classical scaling. There were previous developments by Sibson in the 70s, late 70s. And the 70s, late 70s, and the seal by Tin and Down more recently. Guys, this is how it looks. I don't want to spend too much time on this, but that's essentially if you start with the center point sets, Y, and there's a radius and a half width, which we use for stability purposes. So we want a point set that's fairly thick and not too thin. And the distances are the delta. And the distances are the delta edges, we are provided with all of them. And the problem is exactly solvable in the sense that the deltas corresponds to Euclidean distances. They correspond to Euclidean distances. Now, we're not provided with the deltas, we're provided with lambdas, which are, well, as I said, is a noisy version of the set of deltas. Okay, so it's measured in this particular way. It's not too important, it's more or less what you're thinking it should look like. And then we just say, like, And then we just said that classical scaling, when we provide with those lambdas instead of the deltas, find some point set z1 through zn that comes close to the original point set. Of course, that's up to a orthogonal transformation. And that's quantified in that bound. So that's the bound that we got that I'm plugging in here. So the morale here is that if we have an idea of what go on how to An idea of how to estimate missing distances, which is the object of the rest of the talk, then we can sort of use this sort of bound. Sorry, we can apply then classical scaling and then use this sort of bound to obtain a bound on the accuracy of the entire method. Okay. So let me move on to estimating distances when some of the distances are not available. So in which case we cannot apply. In which case, we cannot apply classical scaling directly. We have to fill in. If we want to use classical scaling, we have to fill in the distance matrix. Okay, so there is actually a branch of math called rigidity theory that actually looks at this question of when it is when a graph with the probability distances can be embedded uniquely. And I'll leave it at that for now. So, in terms of methodology, So, in terms of methodology, there are a number of methods that have been proposed over the years. One of them is to use graph distances to fill in the distance matrix. In other words, to use graph distances to estimate the missing distances. And again, once we have that, we can apply, once we have filled in the distance matrix, we can apply classical scaling. So, this is the oldest method in the bunch, and it dates back to Kruskal and C. To Kruskal and Siri in 1980, even though it was rediscovered a few times since then. Another one is to embed a clique. Actually, those have variants, but one of them is to embed a clique by classical scaling. So a clique is a complete graph, and meaning all the distances are subgraph. All the distances are available, so we can apply classical scaling to embed those points. This is assuming. Embed those points. This is assuming that we have, say, at least D plus one points in the click, and that we want to embed dimension D or something like that. And then once the click is embedded, it serves as an anchor, and you sort of grow that click by sequentially positioning the other nodes. So that's sometimes called trialteration. Then another one, this also has variants, but something like that. So something like this. So you embed instead of one click and that you grow. Instead of one click image that you grow, you embed all possible clicks and then you sort of quote-unquote synchronize them or align them in one way or another, either sequentially or all at once. That's another class of methods that actually all of them have points of contact with manifold learning. Okay, then there's also another method which seems to be popular called SMACOF, in part where Called SMACOF, where the optimization is conducted directly, but actually indirectly, but via measurization. So you detect the stress and derive a measurization, which is then applied to decrease the objective with each step. And then there are semi-definite relaxations of the problem that have also been suggested. Again, there's a point of contact in manifold learning in the form of maximum variance unfolding, which actually Variance unfolding, which actually was proposed by the same authors simultaneously as a method for positioning. Okay, so I'm going to focus on graph distances, which is the simplest method among them. And even though it's simple, and it is much simpler than say semi-finite, and they're actually points of contact also with SDP methods, including MVU. methods including MVU, which I won't say much more here, but otherwise it's the simplest method to look at. And it turns out that it's optimal in some circumstances. So imagine we have points in space. So this is a realizable situation. And we are provided with the distances where there's an edge or a segment between two points here. Between two points here, and then imagine we want to estimate the distance between those two endpoints. So, the square point here on the left and this other square point on the right. This is a shortest path in the graph. And you see in this particular example, it comes relatively close to the actual shortest path in the Euclidean plane, which is what we're trying to estimate here. So, which is here depicted by the purple line. Okay, so there's hope. Okay, so there's hope, and that we want to know a bit more about this. So we have the definition of graph distances, which is what we're going to use for estimating or filling in the distance matrix. Firstly, take two nodes, i and j, in the graph, and simply you look at among all paths, meaning you know chains where pairwise there's a link or an edge. Otherwise, there's a link or an edge between the corresponding nodes, and you sum the weights along the path, and you look at the path and minimizes that sum. And that's the graph distance between i and j. Okay? Sometimes called shorter path distance as well. All right, so we want to see how things behave, not in the abs necessarily abstract of a general graph, but in the context of points in space. But in the context of points in space, which is really what we are into here. Just to clarify, so you want to estimate the Euclidean distance or the graph distance? So we want to estimate the Euclidean distances. So it's a situation in which the graph actually, the nodes corresponds to points in space. We don't know where they are. And we want to estimate their pairwise distances. Once we have their pairwise distances, we can apply something like classical scaling to actually embed and actually. To actually embed and actually find points in space. Okay, thanks. Yeah. So, okay. So let's look at the situation in which the graph is a neighborhood graph. So meaning that we are provided with an edge between i and j if underneath, we don't know the points themselves, but it corresponds to the situation where the distance between i, x, i. The distance between i xi and xj is less than or equal to some r. Okay, and here we only have the graph, we may not know what all r is, we don't know where the points are, and so on. We only have the graph and those values here, the delta ij is when delta ij is less than equal to r. So there's some censoring in statistical parlance. And here's a very simple, relatively simple result to prove where we have, so it's a again, I'm talking about the relizable case. Talking about the relizable case. So we have a point set x1 through xn in our in some Euclidean space. And the crucial quantity that drives how precise the graph distances are as estimates for the actual distances is this epsilon here, which is measuring how dense the point set is in its convex hole. Okay. So So, and that's a limitation of this methodology: is that if epsilon doesn't go to zero, like for example, when the point set doesn't fail a convex set, then the method is biased. There's an irrecoverable bias there. I mean, there are things you can do, but to allow to modify the method, but in principle, the method doesn't behave well in that situation. Right. So that's the quantity of interest here. Right, so that's a quantity of interest here, and um okay, so when epsilon is small compared to r, so r again is the radius of connection, then well, this inequality here we always have because in the plane, well, actually in the Euclidean space, the straight line is the shortest path between two points. So, this inequality is always there. And this upper bound, and this is because also a path here will connect those two points, okay? And this inequality is less trivial. Inequality is less trivial and it looks like this. So there's a factor that's on the order of one plus something times epsilon over r squared. And yeah, and C1 and C2 are some universal constants you can come up with, but it's not so important. Okay, so that's a simple result. It nonetheless improves some existing bounds, and I'll come back to that a bit later. I'll come back to that a bit later. Okay, so that was in the plane and how graph distances behave there is relatively situation relatively simple. How about on a surface? So that has been used for manifold learning in this famous algorithm proposed in 2000 by Tinman, Sylvan Langford called the ISO map. And actually, I can tell you in words what the method does is it does exactly. Does it do exactly what crucible and series method does? But here in manifold learning, instead of being provided with some pairwise distances, we are provided with points. The assumption is that the points are near a surface or on the surface of low dimension. And let's say dimension is provided, little D, and then we want to embed the points in dimension, a Euclidean space of same dimensional D. That's the problem. But you see, it's different because we are provided. But you see, it's different because we are provided with points as opposed to distances. So, the idea is that we assume that the surface on which the points live is smooth, but the wise non-parametric. And so almost by definition, the distances we're going to trust between the points are the ones that are small, because small distances on the surface are close to the corresponding Euclidean distances in the ambient space. Distances in the ambient space. So we do that, that we actually do a thresholding. So it's not a censoring because it's by choice, but we do a thresholding in which we only say trust some small enough distances. So now we are in a situation in which we are as if provided with distances, and we applied the method of crucial and theory, and that's exactly what IsoMap is. They didn't formulate it this way. I don't think they were aware of this existing method. They were aware of this existing method at the time, but that's exactly what IsoMap does. Okay, so in this context, the distances are estimated with the purpose of doing manifold learning, in other words, for embedding points that are in higher dimensional space believed to be on a small dimensional surface, which is unknown. Okay. Okay, and you can also imagine motion planning on the surface in robotics. That's another example. Okay, so how to define source paths on a set? Just imagine the set being a smooth surface. And in that case, it's simply, you know, among all curves that are in that surface, on that surface, that join points x and x prime, which you just. Which you just compute the shortest one. And that length, the length of that shortest one is the distance between x and x prime on the surface. Okay, so now the problem of estimating distances on the surface, we estimate them based on the sample on that surface. So same setting as in manifold learning. And so the surface would be a dimension L D. Surface would be a dimension L D and ambient space big D, a dimension big D, and the goal is to estimate the intrinsic pairwise distances between those points. Once we have that, we can, say, for example, apply classical scaling to obtain an embedding in a desired dimension. Okay, so big delta here denotes the graph distance in the R-ball neighborhood graph. Graph distance in the R-born neighborhood graph. Here we have to construct the R-born neighborhood graph. And epsilon has very much a similar meaning as before, except that now it's not the density in the convex hole generated by the points, but it's in the surface that the points are supposed to span. Okay, so that's what epsilon here is measuring now. And from the people that proposed IsoMap plus another person, Bernstein, and the same year they have this preprint. And the same year they have this preprint that somehow remains unpublished where they obtain this bound right here, this upper bound. And so the upper bound is in epsilon over R. And there's also a lower bound. This one requires some smoothness of S. And the lower bound, it depends on what, so this is sorry, this is an a lower, this is a lower bound on G, sorry, and this is an upper bound on G. And this is an upper bound on G, and it's in R squared instead. And with some, with a collaborator, Thibault Lubwick, we actually took on this problem, we actually extended in a different direction. I'll mention in a second. We also recovered the same result. Just we were out of curiosity, but we basically have some known similar work, in fact, of doing some on motion planning in the plane. In the plane, in dating back to the 50s, and we also analyze how close the actual paths are to the source paths are that you get from the graph which embedded are polygonal lines, how close they are to the actual, if you want to call them geodesics on the surface. Okay, I won't say much more here about this, but just let me just mention that the same paper studies curvature control. Studies curvature constraints, shortest paths. So, this is a situation which I'm looking at a notion of distance on the surface, but where the curvature of the path cannot be too tight, and which is partly motivated by applications in robotics. All right, so let's continue nonetheless in the setting of manifold learning. So, if we combine the results. We combine the results. So, this is, I think, this one here with another one. So, this is in a situation which, very special situation, which as the surface is isometric to convex domain. So, it's as if you take like a convex piece of paper. So, you take a piece of paper, you cut it in a way that's convex, and you fold this without tearing or having the paper touch itself and without stretching it if it's possible. And we are stretching it if it's possible. Then in that case, we have this upper and lower bound. In particular, there's a square here. There was this, without this assumption, there would be no square. That would be the best we know. Okay. And so if you optimize over a little r, remember that we have the control over a little r here in the manifold learning situation. So points in space that we provide it with that are assumed. Provided with that are assumed to be on the surface of low dimension. Okay, so we have those two bounds. And if we optimize over little r knowing everything, what we get is this bound right here. So we have a proportional bound of order epsilon. And epsilon is typically log n over n to power one over d. If say, for example, the points x1 to xn, in fact, are drawn iid. I are drawn IID from the uniform distribution on S. And then S is a dimension B. Okay, so the question is, is this rate optimal? And it turns out that that's not the case. So in recent work with Alan Chou, who is a student of mine, we show that we can get, in fact, a proportional error of size epsilon instead of epsilon, sorry, epsilon squared instead of epsilon. Sorry, epsilon squared instead of epsilon. And also, this rate is minimax optimal. Yeah, it's minimax optimal. So the idea is very simple is just come up with a surface approximation, which we call here S hat. So G hat is our estimate for the metric. Our estimate for the metric and s hat is going to be an is going to go through, it's going to be the metric associated with an approximate approximation to the surface, which we call s hat. And we have a couple of ways of doing it. One is non-constructive, is a bit cleaner, but is again non-constructive. And the other one is constructive, but this one requires going into mesh constructions, which took us a while, rather complicated. Complicated literature, at least for me and for us to parse. So here is a torus. So we have a coarser sample from which we get a mesh and we compute now the shortest paths on the mesh. So that's, I think the green one is the truth, I think. The blue one is the reconstruction, it's hard to see. And the red one is the rougher one that corresponds to finding the source path in the neighborhood graph. In the neighborhood, in the neighborhood graph, and on the right-hand side is the same thing, but with a finer mesh for a finer sample, sorry, based on a larger sample. Okay. And this led us to propose a variant of isomap, which we call mesh isomap, where instead of computing distances based on the graph distances, we compute the distances on a mesh reconstruction of the surface. And so that seems to achieve the optimal weight epsilon squared. I say seems because there are some issues at the boundary that we're not 100% clear about, but it seems at least plausible that the boundary doesn't have a global impact and that the rate holds. But in the paper, we don't state that as a theorem, rather as a discussion. I'm almost out of time. I'm almost out of time. Let me just mention this last setting, which is also on estimating distances and also using graph distances, but in a different framework. And let me just show you the framework and then I'll stop there. So here we only provided with an adjacency matrix, so a unweighted graph. And it's a noise in the following sense. So actually, it can be noise. It doesn't have to be noisy. Actually, it can be noisy, doesn't have to be noisy, but can be noisy. So, here's the model. So, X1 through XN, as before, are actually in the manifold learning they were known, but in the graph embedding problem, they were unknown. That's assuming a situation in which a graph embedding problem is solvable exactly. So, anyway, so we're back to that. X1 to Xn is a point set that's unknown. Given that point set, The item item i and j, which are you know abstract placeholders for the corresponding points xi and xj, wij is equal to one with probability a function, decreasing function of the distance between xi and xj. That's the model. And the goal is to recover, to recover the points at, of course, up to an orthogonal transformation or possibly even more. Transformation or possibly even more. In fact, it's scaling in our situation, also, because we assume something that's fine is unknown. And it turns out that graph distances are, there's some literature around that, and graph distances can be optimal. So yeah, so let me okay. So the simplest case is in which there's no randomness. So wij is equal to one if xi and xj are within distance r and zero otherwise. So it's distance r and zero otherwise. So it's if the x i's are random, that would be a random geometric graph, the sort of typical understanding of that other concept. In that context, for example, using graph distances, we get this rate and it turns out it's minimized up small in a certain sense. And then we go beyond this with a little bit more work. And then here's proof of concept that it works numerically fairly well. All right, let me stop here and thank you for. Let me stop here and thank you for your attention. Thanks, Eddie. Any question? Do you need to know the dimension of the underlying manifold when you do this? So in the manifold learning principle, yes, there's work on estimating the dimension and being a discrete. Being a discrete, you know, it's a discrete parameter, so it's not too hard to essentially estimate it relatively well in standard settings. But there's a there's some work on that, yeah. But in principle, yes. In the graph embedding problem as well, in the sense, but in graph embedding prime, what people do is they you can go down all the dimensions until you get some sort of distortion, for example, or you just embed in dimension too because you want to see it on the screen, which is not necessary. Which is not necessarily a good reason, but um, but the truth is it's actually used a lot. I think it practices actually used like this the most, but so dimension two or possibly three, because that's what you can handle on the screen or on paper. So, yeah. I don't think those methods are used for as dimensionality reduction as pre-processing for, say, regression or classification. I had the impression that people don't, at least nowadays, don't even think of doing that. For doing that. Okay, thanks so much. Thank you. Thanks.