NRS and Universit√© de Paris, and he'll be speaking on differentiation in probabilistic coherent spaces, reconciliating differentiation and determinism. Go ahead, Toma. Thank you very much, Rick. Thank you for this kind invitation to this beautiful workshop. So I've added this subtitle to my talk because I have an impression that the direction where the thing is heading now, but I'm not completely. The thing is heading now, but I'm not completely sure. So, oops, what's going on? Yes. So, this, I will essentially be focused on the Prohibistant PC, which is a model introduced by Girard and then that we developed with Marcon Danos and that we use essentially with Christine Tasson and Michele Pagani to develop models of programming languages, especially to prove full abstraction properties. Full abstraction properties. So it's a model of classical linear logic where non-linear morphisms are very smooth, they are analytic in this a bit as explained yesterday by Erika. And so they have all derivatives. But it's not a model of differential linear logic. And this is mainly due to the fact that it's not a pre-addictive category. And actually, you don't have an isomorphism between the coproduct and the... the the the coproduct and the but the the whole talk will be around this uh this uh this factor so what is a polysequent space it's uh it's uh a pair made of two things a set which you should understand as a collect of the set of indices and and the collection of valuations which are valued in non-negative real numbers and this set of valuations is is uh is subject to the following conditions it is downloaded close it is uh It is downward closed, it is closed under labs of monotone sequences, it is convex, and it has a technical condition to avoid intrinsic coefficients. This can be also formalized expressed in terms of bi-orthogonality condition. And I will use this notation here, this EA for denoting the distribute valuation, which is zero everywhere, but for A where it is equal to one. So, the notion of morphism is very simple. A morphism from x to y is simply a matrix indexed by the product of the web, which maps any vector of px, any valuation of x to evaluation of y by usual matrix application. So, I use this notation for the application of a matrix to a vector. So, as I said, it's a model of. So, as I said, it's a model of classical analogy. And one very interesting feature of this model is that you have a lot of constructions such as general recursion or general fixed point of types, which are not always available, which actually are not available in models of differential numerology usually. So, what are the main constructs in this model? Are the main constructs in this model? I will just a brief summary here. So, you have a unit which has a singleton as index set. We have the space of matrices. So, the web is a product of the webs, and it has variations that take all matrices. As a special case of this, we have the orthogonal. So, the orthogonal is a dual, if you want, it's just a space of valuations on the same. Valuations on the same index set which have a scalar product less than one with any element of px and then we have a tensor product that we can define that way. Important thing here is that if you have a valuation of x and a valuation of y, then you can take the tensor product of these things and you get a valuation of x tensor y. It's defined as usual. And then you have a product for which evaluations is just the same thing as the capital. Valuations is just the same thing as the Cartian product of the valuations, and the co-product, which is not the same thing as the product, as I told you. What you have in the coproduct are the pairs lambda x, one minus lambda y for an element of px and element of py, but they must be chained by some lambda, by some probability lambda. So you have some sense you toss a coin, a bias coin, and then you take x in the base component, if you get a head and You get head and while the right component you should get for you should get tail. So it's not the same thing as a product. And the thing which is very important for us is the exponential. So the bang of X is as web, as usual, it's as web, the multi-sets of the element of web of X. And you have to think of them as multi-exponent. I think this is very standard now. And then if you have evaluation. And then, if you have a valuation x, then you can consider the valuation where the index are now the multi-exponents, and where the values are the valuation taken to the corresponding multi-exponents. And then a morphism from bank x to y, a morphism in the classic category, is just a matrix indexed by the multi-sets of x and the point of y, which maps any element of any valuation of x to valuation of y by this. By this application, which is using this notion of pronunciation of evaluation to multi-sets. So, an important feature here is that what you get is a power series, but this power series is converging on the rule of P of X, but usually outside P of X is diverging. I plan to give an example. Perhaps it's a good idea to give a kind of To give a kind of flavor of what's going on in this model. So, here, if you have a term in some programming language of type boole depending on the Boolean parameter, then you can interpret it as an analytic function of this kind from the sub-distributions of Booleans to the sub-distribution of Boolean, because I didn't say that, but P of one plus one is just exactly the same thing as the sub-quality distribution. The sub-proliferity distributions on the Booleans. And so, for instance, if you consider this program here, you see what it does. What it does is that it retraces a Boolean that you should consider as a bias crown, and it processes it. And if you get different results, then it finishes by giving the last result it obtained. It processes twice. And if you get different results, it returns the last result obtained. And otherwise, it restarts and redoes the same. It restarts and redo the same thing. Then, in the model, the interpretation will be a function which satisfies this recursive equation, and actually it's a list function in some sense. And then you can see that it is the function written below. And it's easy to see that how it behaves. When you feed it with a total Boolean, with a Boolean theta true plus one minus theta four, so it's a Minus theta force. So it's a total Boolean. It's a sense that it's a coin which always gives you a head or tail. Then it finishes with either with a fair coin or it diverges if you have credit with a coin which is completely biased in the sense that it always returns true or always returns false. So it's a program to transform an unfair coin into a fair coin. So if you look at the So if you look at the total convergence of probability of your program and you plot it, you have this. Remember that R is the global quality of convergence of the partial coin, if you want, and theta is the underlying total coin. So you see that for total coin, you have a constant quality of convergence, which is total, which is constant equal to one for theta different from zero, one, and it becomes. Different from 0, 1, and it becomes 0 at when theta is 0, 1. And inside for S strictly less than 1, you have this regular shape. If you compute the derivative, you find this. And you see that the derivative is going to the infinite on the corner, which is very natural if you look at the fact which is written below, which is an interpretation of the derivative in terms of expectation of the number of times. Expectation of the number of times the program is using its arguments. That's a terror that you can prove. It's not very difficult to prove. That the derivative, well, actually, it's a little bit modified, but essentially the derivative is the expectation of the number of times the program will toss the coin. So, of course, if the coin is very biased, it is very unlikely that you will get two different results from your toss choice. And so the computer time, computational computational time becomes very large when you go to very biased points. So this is to say that in this model, it makes sense to compute derivatives. It's related to execution time. There is also some hope to use this kind of derivative to do learning, to have the semantics of learning. semantics of drama but these problem these derivatives are problematic because essentially when you when you do differential calculus at some point we need to use the Leibniz rule or similar things and then we'll have to compute the sums of two things and this is why the notion of tangent categories and differential categories relies very much on additive On additive categories as five and that's simply here we cannot do here we cannot do that because simply our category is not additive. So this is also related to the fact that the derivatives, when you compute them, they give you values which are not in the model. For instance, if you take the function x to the n, which is a perfectly quite morphism from band one to one, then if you compute the derivative, of course, if n is strictly greater than one, it will take. If n is strictly greater than one, it will take values greater than one. So it's influenced morphism from bound one to one. But if instead of computing f prime of x, you compute f prime of x times u, then you get something which is always less than one. And this is a general phenomenon that if you take, we can define the notion of local derivative for which this phenomenon would be generalized. So I define. So I define given a PCS X and a distribution for this PCS. I define a local PCS which has the element of the web of X that you can add to X by some possibly arbitrary low shrinking. And then you take for the valuation of this, all the valuations that are here that you can add to X. To x. You can actually actually this PCS has as valuations exactly the valuation that you have in Px and that you have that you can add to X and stay in Px. Okay, and it's a category piece, yes. So now if you have a function, if you have question, please interrupt me. Then if you are given a function in the classic category like this. In the classic category, like this, okay, so it's an analytic function, then we can define a differential or derivative which will be a linear function from the local space at x to the local space at f of x. And this f prime x of this f prime x, when you apply it linearly to a u, which is an element of the local space of x, then it's simply defined as a linear part, as a u linear part of. part as a u linear part of f applied to x plus u so if you do the computation you get this result which is not surprising and what happens here since because everything is positive you have that that f of x plus f prime of x dot u is uh is stands between f of x and f of x plus u and so it is and since f of x plus u is in p of y then this f prime of x dot u stands in the local space of y at f of x. f of x. So this looks like a tangent category situation and I was looking for things in that direction but actually it's in some sense it's much simpler and it seems that the relevant structure can be described by a very simple functor. Well, I call it S for not calling it T for avoiding it speeding identification, but of course it's very Identification, but of course, it's very, very close to the concept of strand line category. So, I have this functor s from Pico to Pico, which does the following: it maps a PCS X to a new PCS, whose web is just two copies of the web of X. And the element, the distributions of this web are the pairs of actually look at the last line, it will be more easier to understand. The element of P of S X are exactly the pairs of are exactly the pairs of valuations on the two copies of x such that the sum belongs to to px so it's a it's a it's a global constraint on x and u if you want that that the sum must be in p x and it is a functor that's very easy to see because if you have a morphism from x to y so it's a it's a linear morphism it's a matrix from from the px to from x from x to y you know then you get you get a matrix from sx to s y just by by by copying the the t in the two components okay so in this cells which are not in the same component you put zero and you copy t for when you are in the same component and what happens simply is that you have this equation okay and since since t is linear of course this is this is uh this gives you a mapping from uh from image of image of xu which is in psx would be a tx tu which is in psychique by the anti of t and the one uh one things which uh departs this from uh tangent categories is that here there are no differentiation involved in this frontality so of course uh if you have a if you have a vector of if you have two vectors of of px you px you can uh you can take a kind of probabilistic combination as i did before so you can take lambda x plus one minus lambda u or some lambda n 0 1 and of course you will get something which will be in psx because the because psx is convex as i told you but it's not true that but in p in sx you you you have uh in psx you have uh pairs on some of the pairs which are not of that shape Some evapors which are not of that cheap. For instance, if x is a direct product, is a direct product of x1 and x2, then, and if x1 is a vector of pi, if x of i is a vector of px i, a valuation of pxi, then you can take these two valuations, you see, where you have taken zero on the side, which is not, you put x1 in the left component and x2 in the right component. Component index to the right component. And these two things are vectors that you can add. And you can add them without any deviation of probability. This is, of course, related to the fact that the S printer commutes with arbitrary units actually, so it commutes with products. It commutes with E because it has a left adjoint, which is we are in linear logic. So everything, as soon as you introduce something, we will have a Thing we will have a kind of dual. So there is a dual to this S, which is this functor here defined by this equation. And it's actually the bi-orthogonality closure of the diagonal of the x-axis. Okay, so there are a lot of structures on these functors, so which some of them are similar to tangent categories. So here, for instance, we have injections. So here are injections on. injections so there are injections on both sides actually in tangent categories you would have injection only on the on the on the on the left side in some sense but here we can inject on both sides we can project on both sides also okay and these things combine as you would expect and and this this means that sx stands between somewhere between the co-product and the product The coproduct and the product of X with itself, but it is neither the coproduct nor the product in general. It is important to notice that the pairing of pi zero and pi one is a model for some reason. And then you have another morphism which is very important, which is a summation morphism from Sx to X. They just take a summable pair and it computes the sum. Now, there is a possible definition for saying that two morphisms can be summed up, because as I told you, we are not in an additive category, so usually I'm not allowed to add two morphisms. But I can now, I have a definition for two morphisms being summable. I can say that F1 and F0 and the products are summable. If I can find a morphism from Y to Sx, such that this diagram commutes. such that this this diagram commutes and then i will i will use uh i will i will use uh f0 plus f1 for for for sg this will be the definition of the sum of f0 and f1 of course you see what i'm trying to do okay i'm i'm i'm describing some structure which are present in in pcs but i'm trying to find some categorical axioms which will allow me to to generalize the situation so this this would be the kind of definition that i would like to have in the Definition that I would like to have in a general categorical description of this scenario. Okay, so what about differentiation? Now, differentiation turns out to be, well, as in you see in differential linear logic, differentiation is related with exponential. So in some sense, it's a structure of the exponential. So it is a case. The exponential. So it is the case in differential linear logic, it will be the case here too. And what kind of structure? Actually, a kind of distributive law between the bang and the S functor, the summable pair functor that I described. And actually, we have this. So we have a morphism from bang S6 to S of bang X, which satisfies this diagram. And it is given by, it's very easy to describe. It's very easy to describe. You only have to observe that the web of bang SX is simply the same thing as a pair of multisets from X, two multisets from X. And the web of S of Bang X is either a multiset on the left side or a multiset on the right side. And then this distributive law is given by this equation. So, I will not read it for you, you can read it. And so, it is when once you have a kind of distributive law like this, you can lift your functor S to a functor from the classic category to itself. By this standard definition, you say that I call it D because it's related to differentiation. I simply say that if F is a morphism from ang x to y, then I find df as S F applied to. As SF applied to the distributive row, which is exactly what I need to turn SF into a morphism from bang SX to bang to SY, sorry. So to a morphism from SX to SY in the classic etc. And the fact that it is a functor is simply a rephrasement of the chain rule. Well, sorry, what I didn't say is that this is really the most important fact. This is really the most important fact. Before at the beginning of my talk, I defined what is the local derivative of a morphism in the category of Royal sequence bases. And so now using this definition of differentiation as a functor acting of the classic category thanks to this distributive load, I have this equation which means that. I have this equation which means that exactly this decomputes the derivative, the local derivatives that I wanted to get. So there are some things that you can prove and which would be probably taken as axiom, the fact that if you compute this distributive law with weakening, because bank X is as a structure of commutative polymonary, where we call the counit, we call it weakening and the Unit equality weakening and the core multiplication equality contraction because it's related to the corresponding rules in linear logic, as Rick explained yesterday in his very nice talk. So if you compose this distributive law with weakening to which you have applied S, you get zero, which is not surprising because it corresponds to deriving a constant function. And if you do the same with contraction, what you get is this. What you get is this commutation where here, or you see this, nothing very surprising. The only interesting thing is this L morphism here, which is a kind actually a kind of a lax monoidal structure on S. Which we can axiomatize it as follows. We can say, because it is true in PCSs, that if we take P0 tensor P1, If we take p0 tensor p1, remember that p0 and pi 0 and pi 1 are the two projections from sx to x. Okay, so we can tensorize pi 0 with pi 1 and pi 1 with pi 0, and we get two morphism from sx tensor s y to x tensor y. And now I observe or I require that these two morphisms are some angle in the sense I defined before. Then what happens is that this L is characterized by these two diagrams. If you compose L with pi zero, you get the product of pi zero with pi zero. Get the product of pi zero with pi zero. And if you compose with pi one, you get the sum of these two things, and this sum makes sense because of this preliminary observation. Otherwise, I wouldn't be able to do that. But that's really important because it's meaning that the sum I'm required to do for using the Leiden's flow is possible in the model. We have also kind of Of trends that you can define using the first injection that you can combine with the monoid IT I'd explained before. It's a kind of strength, but of course, the notion of strength applied to a monad, I think, normally. But the functor S is not a monad, so well. So, well, one has probably to adapt the notion a little bit. It is certainly commutative, but one has also like here, one has to also understand what it means exactly particularly. But what happens is that using these trends, using this, yes, these trends, we can define partial derivatives by combining the morphisms in this way. I'm afraid I've been a bit too fast. I hurried at the beginning of that talk. Now I'm rest with five minutes because I write to the conclusion. So this is clearly very close to the idea of tangent categories. Also, there are some discrepancies I pointed, I stressed during the talk. And more more specifically, I see some connection with this paper by Roden GS and Ann Roy. In GSN annually. With the main difference, I try to list the main differences that I can see. The first main difference is that there is an algebraic structure on objects, which is a way of we have this possibility of turning a summable pair into a vector of x, whereas in a tangent, sorry, no, in tangent. sorry no in in in tangent the tangent category situation what you have is is a t2x to to tx where this where this t2x is defined in terms of pullback which is uh the the the possibility of acting adding vectors in in the in the tangent space okay here what we can do is something much more uh drastic which is directly adding uh vectors to to element of the of the base space in some sense and uh And this is a very, very big difference. The other difference is that it is not a left additive, we are not working in a left-added category. In the tangent space, in the tangent, as far as I understand, in tangent categories, it's important that we are in an additive category because this operation here is essentially addition of tangent vectors, and there are no limitations. Tangent vectors, and there are no limitations on the kind of additions that you are allowed to do. And there is, so this seems to be more restriction or strange properties. And this is a very positive outcome of this, which is now that we can do differential calculus, if you want to let me put some quotation mark around differential calculus, in a category where we have arbitrary fixed point operators and. Fixed point operators and which are very also recursive types, which a priori seems to be difficult in an additive world, because in an additive world, we have typically this morphism, which maps x to x plus u or even u. And such a thing tends not to not to have a fixed point unless you accept integrated coefficients or idempoton coefficients. Coefficients. There might be also a connection with Joey situations where you have partial sums, a bit like what we are doing here. Another, I wanted to insist on something else, which is the kind of derivatives we are taking. What we do is that we consider programs as analytic functions acting on not on real numbers considered as data, but on real numbers. Data, okay, but on real numbers describing probability distributions on some basic data. So that's why, for instance, we can speak of an analytic function from Booleans to Booleans, as in the example I showed you. And it makes sense to take derivatives even at this type, whereas we don't see very well what it means to take a derivative with respect to Boolean. And this is quite different from, at least as far as I understand, from the kind of derivative. From the kind of derivatives which are taken in differential programming languages, which are used in machine learning for implementing gradient descent method and this kind of things. So the hope is that we will be able to have a uniform setting where we have a differential calculus and also a linear logic and lambda calculus and some tweak points and Some checkpoints and at the same time, some kind of deterministic things because, well, this is a possibility of generalization. But one funny remark is that this setting that I described seems to apply perfectly without any, almost directly to current spaces, to the old model of current spaces introduced by Girard many years ago, which is a model of deterministic computation. Model of deterministic computation, but a domain-theoretic model of deterministic computation. But this model seems to have the right structure. So, we have a S-functor and we have a strength, so we can compute locally the differentiation in Forehand spaces. And this seems to mean that we have a notion of differential calculus which is compatible with repetitivism. Okay, so I can stop here. Okay, so I can stop here, I think. Thank you, Toma. So, before we get to questions, let's unmute and give Toma a big round of applause, please. You can also give him an electronic applause by hitting the reaction button. Oh boy. Okay, so are there questions? If you have questions, you have to hit the raise hand button because I can't. You have to hit the raise hand button because I can't see everybody. So I was wondering, differential interaction nets, can they be done in this setting? Well, no, because we don't have any more co-contraction and co-direction. The point is precisely here, because the way you encode a co-contraction at the category. At the categorical level, is essentially by it is based on the fact that the Cartian product and the and the co-product inside. Okay, so because you essentially what you do is you you apply you have the let me write on the slide, yes, you have the co-diagonal like this, okay? And then you observe that this is the same thing as the product. So now the co-diagonal becomes something like this. Now the co-diagonal becomes something like that. And then you apply the bank functor to this. So you get something like from x with x bang x. But this is the same thing as bang x times bank x. So this is going to bang x. So you get something like this, which is co-contraction. But you see that the way you define this co-contraction is really based on this identification, which is no more true here. Here. So that's why it's really a local, it's really a local differential calculus. So when I need to rewrite completely even not only the rules, but even the constructors of differential linear logic. And probably if that's something I'm trying to do now, which is developing a linear logic for this kind of calculus. And what seems to be the case. And what seems to be the case is that you have to introduce a new modality in the syntax which will correspond to s and uh but it seems to work. I have already a set of rules and it seems to make sense. So it will be a completely new calculus, but uh yes excellent. Okay, let's hear from JS and then Jonathan. The S functor seems to have a very like a ring of dual numbers. Like a ring of dual numbers feeling to it, except you also have an addition that goes from SX to X, which you don't necessarily have for a ring of dual numbers because that addition map is not a ring homomorphism. So do you know what's sort of happening in this scenario? I don't know. I observe the similarity, but I have to dig it more. But I agree with you. In our paper, the fact our S functor was the ring of dual numbers, and that was the key to constructing our tangent category. So there's definitely a link there that we do. Jonathan and then Marie. Oh, I was curious if the only thing that fails The only thing that fails with differentiation is the addition and sort of needing infinite coefficients. It seems to be, at least from what I can see, it is really what the main problem because you really in this model of PCSEs, you have really non-trivial differentiation that you can compute. And the only restriction is that you have this restriction that the linear argument you can do. that the linear argument you feed your differential with has to be in the in the local colon, which is really related to the failure of addition, of general addition with the object. So I would say that it's really well as far as understand, but I can see I understand this future. It's the main problem. I see. So there's this notion of what's called a differential restriction category with With suits. And in there, you can do fixed-pointy kind of things if you need. And the addition is sort of fine on finite numbers of things, but you could iterate it and do all those kind of, it would become undefined. Restriction categories are sort of ways to deal with partiality, I guess. And I was wondering if maybe you thought that might give you like having a restriction on top of or it kind of. On top of uh, or incompatible with these with this setting, might make it perhaps perhaps possible. I the point is that the morphism I have are defined everywhere on the domain which is fixed once and for all, so which does not depend on the morphism itself, if you see what I mean. Whereas in the setting you mentioned as impression, but it parting all that the that the domain of definition depends on the morphism, no? Depends on the morphism, no? Right, yeah, usually. Yes, here really you see in the example I showed you, there are even more dramatic morphisms where you see you really cannot extend the morphism outside px. But on px, it is perfectly different. And on the interior of px, if I can say, it is even analytic suit. It seems to be different, but well, can I? Seems to be different, but we're connected. Marie, go ahead. Hi. So if I understood well, the logic corresponding to the categorical structure would be differential in a logic without contraction. And do you without co-contraction? Yes. And do you have any idea of what would that be in the Of what would that be in the differential lambda calculus setting? Is that meaningful? The fact is that what I told you today is extremely frequent to a point you cannot even imagine. So there are a lot of things that I don't know, especially this kind of question, which are very natural. So for the time being, what I'm trying to do is cleaning up the categorical axioms and the categorical axioms and uh and uh defining a linear logic and then i will i will i will try to to go to to differential lambda to to lambda calculus but it seems i mean it seems to be more reasonable to start with linear logic because things are more automated sometimes but i'm pretty confident that there will be a yes there will be a differential on the calculus associated with this but it will probably be quite different from what we we are used to because there was a Because there was a Lambda calculus behind the probabilistic current spaces. Absolutely. Absolutely. There is the probabilistic Lambda calculus, if you want, which is the end. But the differentiation operators, I mean, if you want to write them, so if you want to define, so you can extend this calculus with differentiation, that's no problem. But then you have to define the differential. The differential substitution terms, okay, and this requires to write to use the limits flow. Okay, and see, if you are not careful when you do that, then you can go into problems because you cannot take as granted the fact that when you do a liven summation, if you want, that the two things will be biometrically added if you want. Usually, it's not. Be added if you want. Usually it's not the case. So you enter the point of the S of the S functor. Okay. Oh, thank you very much. Thanks, Marie. And let's all thank Toma again for a great talk. Thank you, everyone. Thank you again.