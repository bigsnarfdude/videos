We're going to be talking about computing gravitational wave posterior probability distributions, but doing so incredibly quickly and doing it with machine learning approaches. So I will get started. It's the longest title I think I've ever written for a presentation. Let me see if I can find my slides again. There they are. Okay, so. They are. Okay, so the overview: I'm going to just do a very, very brief introduction to cover some gravitational wave basics that are key for this presentation. I know you've had lots of gravitational wave talks already, so I won't dwell on that. And then I'm going to just dive into a particular machine learning sort of tool called autoencoders and the basics of autoencoders. And then I'm going to go into a specific type of autoencoder and a specific. Specific type of autoencoder and a specific implementation called a conditional variational autoencoder, which we call a CVAE, and we have a particular implementation which we've called vitamin. And I'm going to risk trying to go through the mathematics of it, which is not too difficult. It's a little bit maths heavy. I'll try and stay light and not overrun. And then I want to talk about some results and then just summarize. And I wanted to remind myself there to just people can. There to just people can please ask questions at any time. I don't know what I'm happy with people shouting out, but you can type in the chat and I'll try and answer questions as I go along or wait till the end. Either way. And that's a lovely picture of where I'm from. I'm at Glasgow University at the moment, although I haven't really worked in the office for two years. I guess that's the same for most people. But that's our lovely university building on a nice sunny day in Scotland. So, first of all, So, first of all, the problem, and the problem being specifically gravitational wave parameter estimation, and the plot on the left-hand side, oh, sorry, on the right-hand side, is just an example of one representation of a gravitational wave posterior probability distribution, a joint distribution on M1 and M2 of a compact binary coalescence. This is taken from one of the O2, I think, O2 LIGO, and Virgo. I think O2 LIGO and Virgo papers, where it's just showing for a particular event, GW170104, in comparison with a bunch of other detections, and just showing just a representation of the kind of thing we're trying to get here. This whole talk is about trying to get probability distributions on the parameters, specifically of compact binary coalescence signals. So, again, I won't dwell on this. Again, this slide is really just historical, just to everyone likes to. Just to, everyone likes to see the wibbly gravitational wave polarization diagrams, and just to show you the kind of waveforms I'm going to be talking about here. I'm talking about specifically our paper is on binary black hole waveforms, and this is the classic GW150914, as if you could ever forget the kind of waveform we're looking at there, where it's the classic chirp in spiral and merger signal seen in both the LIGO detectors in 2000. Seen in both the LIGO detectors in 2015. One thing to note as well here is just the typical time scale of binary black hole mergers in the tens of solar masses. We're talking about order one second, a few second long signals in our time series, as opposed to binary neutron stars where there would be many tens, if not hundreds of seconds long. And if you didn't need more reminding, here is just a lovely animation of 10 of the detections that were made by the That were made by the LIGO Virgo collaboration. I can't remember how old this is. We obviously now have the O3B results just published with a total of 90 detections. Sorry, my alarm's going off for some reason. And again, just to sort of hammer home what I'm talking about today, it's just going to be binary black hole signals where we have the in-spiral, classic in-spiral signal of the increasing amplitude and frequency followed by the Followed by the merger and in spiral. And things that these are being shown at a slow, slowed-down rate. You can see the time ticking in the top right-hand corner, but they're typically lasting of order a second. And here is my main point, the main motivation behind this, and one of the problems with being fancy and being overlaid on top of my slides, as I do cover up some of the information on my slides, is that the time it takes for us to analyse time it takes for us to analyze our detections. We have an absolutely excellent, the whole collaboration, the whole gravitational wave community has this excellent infrastructure for making very fast, rapid detections of our events and being able to do some quite sophisticated parameter estimation within of order a minute of the signal arriving. And this is an example on the right-hand side of the GRACE database, GRACEDB. Database GraceDB, where you can go and look at the most and greatest gravitational wave detections open to the public, and you can get things like sky maps and distance estimates for various sources and figures of merit of how likely they are to be binary black holes, binary neutron stars, neutron star black holes. But the one thing that you can also extract from this database is the amount of time it takes, it has taken the collaboration to run standard Bayesian sampling approaches. Bayesian sampling approaches to estimate the full parameter set for these systems. So that's a whole for binary black hole signals, that's 15 parameters. I won't list them because I always forget some of them. But it's beyond just having the sky position and the distance, it's the masses, the spins, the polarization, inclination, angles, sky position, and so on. And if you look at that table of events when we looked at this mid-03, it was taken between Mid-03, it was taking between six hours and five days to estimate the parameters fully using standard Bayesian sampling approaches like nested sampling, Markov chain, Monte Carlo, and different variants of those. Now, the ones that take five days, there's various reasons why they take five days. There are some cases where there are technical problems, but there are some cases where it just takes that long to search the parameter space to get the posterior distributions. And as I said, this is not to be, that's full parameter estimation. Said, this is not to be that's full parameter estimation, not to be confused with the rapid parameter estimation that is seen on the right-hand side of these plots on the right-hand side here, which can be done in a border minutes. And so, it's really important that the astrophysical motivation is to find where these objects are on the sky, how far away they are, and other properties like the likelihood of them containing a binary neutron star component, which would then imply that there is potentially electromagnetic radiation that we could point to. Electromagnetic radiation that we could point electromagnetic telescopes at to learn a lot more about the system, just as we did with the famous GW170817 binary neutron star merger. And the earlier you can do this, the more valuable information you can get. So that's my sort of motivation side. And now I just want to go through some basics of autoencoders. If you're all very happy with machine learning, you can switch off now. But I'll try and give a sort of basic view of autoencoders. View of autoencoders, and I like to think of autoencoders trying to squeeze your data through the eye of a needle and then actually expand it back up to practical scales. Underneath this is a representation of what's called the MNIST data set, which is a bunch of handwritten digits, many, many thousands of handwritten digits, which people use in machine learning for quantifying the effectiveness of their approach. So I'll be referring to those in a second. I also just want to say, I'm assuming a lot of Say, I'm assuming a lot of knowledge for the next couple of minutes about machine learning. I'll be assuming you know what a neuron is, a layer, fully connected and convolutional layers, activation functions, etc. The standard terminology in machine learning. But don't worry if you're a bit lost on that. There's a lot of maths to get stuck into. And you can just think of the machine as these black boxes that do specific tasks. And I'll try and outline what each black box is doing, what its inputs and outputs are. Inputs and outputs are. So a basic autoencoder takes data and it consists of, an autoencoder consists of two networks, and the first network takes in some data and re-represents it in a different abstract space called a latent space. So one way of thinking about this diagrammatically is at the top of this diagram here, I've got one of the MNIST data set samples. It's a handwritten It's a handwritten 3. It's a 28 by 28 pixel black and white image. Imagine you throw that into a network, a neural network, which we call the encoder. And the output of that encoder is simply a reduced representation of the data that we call represented in this latent space. This can have any number of dimensions you want, but really to be encoding something, to be compressing the dimensions. Encoding something to be compressing the data down, you want that's typically the size of that space, the dimensionality of that space is smaller than the dimensionality of the data. And when I say dimensionality of the data here, I mean this image is a two-dimensional image, but it actually contains of order 700-800 actual pixels. So it's like a 700-800 dimensional input space that gets mapped down here to some lower dimensional space. Then that lower dimensional space representation is then fed into the decoder network, a separate network. Network, a separate network, and its job is to expand it back into a 28 by 28 image. And what you do is, once it's gone through the encoder, into the link space, and then into the decoder and out, you compare the output with the input, and you construct a loss function, which becomes minimized when the output is very, very similar to the input. So you're trying to squeeze the data into a smaller space, expand it out without losing. Without losing detail in your original data. Now, in this case, you can see we've lost some detail. It's a bit fuzzy in that lower image, although it is behind me, isn't it? There you go. It's a bit blurred, a bit fuzzy, because we've had to squeeze it through this latent space. And so this is a sort of form of compression. And the loss function is minimized. You construct a loss function, something like the mean square at a distance between the input and the output. The input and the output. So the loss function is minimized when the output looks as close to the input as possible. So that's just your basic autoencoder, compressing data down and expanding it up. Then if you make the add the extra word variational to your autoencoder, then what you're doing is it's the same setup, except in the latent space, you're treating it a little bit probabilistically with a little bit of fuzziness. Instead of having the encoder, Instead of having the encoder encode to a specific location in your abstract latent space, you get it to predict the properties governing a distribution in that space. And here is just a 2D example. Now we're saying that the latent space is two-dimensional for this example. And we're having the encoder predict two numbers. Well, we're having the encoder predict the parameters that describe a Gaussian. The parameters that describe a Gaussian distribution in this. So, this would be a two-dimensional mean for a location, and technically, you can also have a four-dimensional covariance matrix that describes that Gaussian. In this case, the little black bob there is quite symmetric and doesn't look to have off-diagonal elements of the covariance matrix. But it's predicting a distribution. You then get the machine to pick a random location in that distribution, which then gets fed to the decoder. Which then gets fed to the decoder and gets output as the new image. And you do the same thing with the loss function, with one minor change, which I'll talk about in a minute. So as I say at this last bullet point, there's an extra loss component that keeps the latent space Gaussian when it's averaged over all the inputs. It's designed to be Gaussian by default because you have the encoder output the parameters of a multivariate. Parameters of a multivariate Gaussian. But what you want to also enforce is that when you do this over many, many input digits from your particular space, in this case, it's the MNIST handwritten data sets, that when you do loads and loads of these, the overall distribution of all the latent space locations remains Gaussian. And that gives you some extra power, which I'll explain in a second. So, in this next sort of slide, Next sort of slide: what we generally see when you do this is that when you input something like a handwritten 3 and you train this network by defining your loss function and then iteratively training your encoder and your decoder to minimize this loss function. You find that, for example, threes will live in a certain place in the parameter space because 3s will look quite similar and the network will. Threes all look quite similar, and the network will learn: oh, threes, I'm going to put the threes over in the top left-hand corner. And sixes, they look different to threes, so I'm going to put them somewhere else in the latent space. I'm going to make a distinction between where they live, such that when you sample from those spaces to go to the decoder, you're giving it locations in latent space that are indicative of the type of handwritten digit you want to be output. And you might also imagine that the sixes are going to live quite close. Are going to live quite close in the latent space to the eights and the fives because sixes look quite similar to eights and fives. They have very similar properties. But also the threes often look like eights. So the threes might live next to the eights. And the encoder the whole network during training sort of sorts out where it places all of these in the abstract laten space. When you're doing, this is what I was trying to say before: that what you're trying to enforce overall is that of. Overall is that obviously the threes live in a certain space, the sixes live in a certain space, the twos live in a certain space, but overall, on average, over your whole training set, it forms this Gaussian, this very well-modeled zero-mean unit variance Gaussian in the middle of your latent space. And again, I'll get to why that's well, that's useful because once you've trained your whole system, you can in some sense throw away your encoder and just sample yourself. And just sample yourself from the latent space to generate any handwritten digit you want and just use the decoder. So on the right-hand side here is supposed to represent a trained network situation where I've thrown away the encoder and now all I have to do is sample from my latent space from the known distribution that I trained it to have. And I will just be able to reproduce random handwritten digits however I want, whenever I want. So then we step on to the next word to add to this long list of words in the name of these things. This is conditional variational autoencoders. So this is where you now add an extra conditional aspect to the whole game. And in this case, on the left-hand side, you're passing labels along with the data. So you know that the three has been written and it is actually a three. And so you tell the encoder this is a three. And so you tell the encoder this is a three when you are encoding, when you're training. And you also, when you're doing the decoder, you also say, please pick a point in the latent space. And also, remember, I want a three. So the decoder takes two inputs, as does the encoder in this case. But it still outputs an image of a three if you asked for one. But you can, doing this training method, what you can do is you now have this condition. What you can do is you now have this conditional ability, instead of just having a trained decoder situation where you can just sample from the latent space and it gives you a random digit, but you don't know which one it's going to give you, this time you can say, I want a seven, please. I want a six, I want a three, and it will give you examples of those specific classes if you train it in this way. So if you give it these labels both in the encoder and the decoder stage. So, the little bit of maths that goes along with this is that the best way to think about these things, and in fact, the only way to think about the next bit I'm going to talk about is in terms of probability distributions. And thinking about these encoders, in this case, the encoder specifically, is really modeling a probability distribution, like I said before. In this case, in the conditional case, you're giving it the X and Y inputs, where X would be the image data and Y is the Image data and y is the label that goes along with that data. And you're having the encoder network basically output you the parameters of a probability distribution in the latent space Z. And then you actually sample from that latent space. But the encoder is really modeling this conditional probability distribution in the latent space conditional on the input data and its label. And in this particular case, the decoder is not modeling a probability distribution, it's just modeling a function. It's just modeling a function. It is outputting a function of the label y and the latent space location. And your overall loss function that you're trying to minimize in this case, the left-hand side term for this loss function is just the mean squared error, basically, between your output of your decoder function, f of z and y, minus, or the mean squared error between that and the truth, the x. That and the truth, the x, which is the input image you put into this decoder. And you try and minimize this squared difference between these things averaged over this joint distribution on x, y, and z. The other bit of loss term that I sort of hinted at is that you also have this thing called the KL divergence as part of your loss function, which in this case is the KL divergence between your latent space distribution, the PZ given x, y, and the one you want it to have. And the one you want it to have, you want your latent space distribution to be Gaussian. So you just have this KL divergence term, which is minimized when the encoder distribution, this PZXY, is equal to a unit variance zero mean Gaussian. And so minimizing all of these things allows the encoder and decoder together to produce you relatively representative images, representative of your input data. Representative of your input data, but also keeps your latent space well behaved so that you can use that later on to sample from it and generate anything you want from your input training data. So that's sort of the background. What I want to do now is talk about the work of this guy. This is my now ex-PhD student because he just passed his PhD viva last week, I think. So he's now Dr. Think. So he's now Dr. Hunter Gabbard. And the bulk of his PhD work, the second half of it at least, was working on how to implement a form of conditional variational autoencoders for gravitational wave inference. And he decided to call the code Vitamin because of VI is variational inference, and we haven't yet worked out what the T-A-M-I-N stands for in our analysis. So, to try and put things in context with what I've been talking about before, about the sort of the basics of conditional variational encoders, I want to just define some of the parameters of interest and try and match them onto those variables I was talking about before. So, in this case, the data we're measuring, the equivalent to that handwritten image, is actually our noisy gravitational wave time series represented on the right-hand side. Series represented on the right-hand side by the dark blue data in this diagram behind me. And the dark blue data in this case contains gravitational wave detector noise, simulated gravitational wave detector noise, plus a binary black hole signal. And what you're seeing is a whitened version of that. But we are using sort of we're using data pre-whitening from the advanced gravitational wave detector noise PSDs. Noise PSDs, power spectral densities. We're also simulating three detector networks: the LIGO, Hanford, LIGO, Hanford, LIGO, Livingston, and Virgo network. And we're also generating lots and lots of training data and simulation data where we're randomizing all 15 parameters, and they're all listed here, of our binary black hole signals. So masses, distance, time and phase of coalescence, the sky position, the inclination polarization, and the six spin parameters. And the six spin parameters, three spin parameters for each of the black holes. And we are defining our signal parameters just like our labels in the previous case. Our labels, X, are these 15-dimensional vector of parameters that we have associated with each of our signals that we simulate. And what we really want to do is we want to get a machine, a CVAE, to be able to give us samples. Be able to give us samples from the posterior distribution p of x given y. It looks very simple written there, it's a very, very simple thing, but it's not so simple. We want p of x given y, and we want to do that really, really quickly, ultimately. And all of the work from the next slides on is from our paper, which we've had around for a while now, but it's just recently been accepted in nature physics, which I'm very proud of Hunter for achieving that. Achieving that. So this is where we start. We start with this lovely diagram on the right-hand side, but what I'm going to do is I'm going to try and mathematically motivate that lovely diagram over the next couple of slides, because this is the construction of our conditional variational autoencoder. And each of the components, each of the triangular components, is a neural network. And each of the circular components is part of the loss function that we're going to compute. Part of the loss function that we're going to compute. And we start off, though, in getting to that diagram with just the expression on the left-hand side, which is the cross-entropy between the true posterior gravitational wave probability distribution, p of x given y, and what we're going to call our approximate posterior probability distribution, r of x given y. And this is just the integral of the product of p log r integrated. log R integrated over the whole of the parameter space, the X parameter space of the signal. And if we can minimize that, when this is minimized, it means that the minimum, it's minimized when R equals P. So this forms a nice loss function because when we can achieve the lowest value of this, then our approximate distribution equals the true posterior probability distribution. Now I'm looking at the time and I'm wondering how much time I have to go through all of the maths. Through all of the maths. I expect I'm probably going to rush a little bit, but I'll supply the sides for people to go through. But also, you've got the paper reference there, which explains everything. So what we first do is we generalize that loss function. That loss function was written there before, but just between one posterior distribution and one particular approximate probability distribution. But we want this to work over all possible realizations of gravitational wave signals and noise realizations. Signals and noise realizations. So we take the ensemble average of this over the likelihood p of y given x. And when you put that in, you get an extra integration over dy, the data. And if you rearrange this and use Bayes' theorem, you end up with this third line, which is just a couple of integrals. We have the prior p of x, we have the likelihood py of x, and we have the log of our approximate distribution. Our approximate distribution. We're also going to model our approximate distribution according to that lower equation. We're going to model this as the integral of the product of two distributions. And this is where we bring in our latent space. So it's the product of r of z given y, which is a distribution in the latent space, in the z space, multiplied by r of x given z of y, and then all integrated over the latent space. Integrated over the latent space. And what we're going to have is we're going to have both of those R distributions modeled by neural networks. This next part is maybe well known to many of you. It's about deriving the elbow or the evidence lower bound as it's called. I'm going to just leave this and not go into the details because I'll get myself tied up and I'd rather get to the results and be able to take questions. And be able to take questions at the end. But this goes through a derivation where we introduce another distribution called Q, again in the latent space, and conditional on the data and its parameters. And what we're trying to do is come up with an inequality. And what we find by rearranging all of these equations is that this thing in red, the red brackets, which is known as the evidence lower bound or the elbow, we can have an inequality of that quantity with. Of that quantity with the log of our posterior distribution, our target posterior distribution that we want, this r of x given y. So we're starting to build up the pieces to put all this back together. And I do apologise for going a bit fast over this. It was a bit nonsensible of me for thinking I could get through it all. If we then, on the top right here, I'm just quoting the result from the previous slide. And if we go back to the cross entropy derivation that we wanted, this ensemble average of our cross entropy marginalized over all possible noise realizations, y. And then we plug the elbow definition into this in place of log of Rx given y. We end up with this inequality where the cross entropy is less than or equal to the thing on the right-hand side. And you rearrange some things, and you end up with the thing. And you end up with the thing at the bottom here, which is a bunch of probability distributions multiplied by this final bracket. And what you'll see on the next slide, again I apologize for going a bit too quick, but I'm not going to slow down. If you all accept this as the truth, what I've got is that exact same expression in grey at the bottom for reference. This is the expression now. I want to just look at this. This is the integral. This. This is the integral over the x data. This is the parameters of our system. It's the integral over the realizations of noise or realizations of our data y. And it's also integrated over the latent space z. And inside this integral, we have this bracket on the right-hand side, which I'll get to. But we're actually just these pre-factors of the prior on X, our prior distribution on our astrophysical parameters that govern our signals. We have P. Our signals. We have py given x, which is our likelihood, the probability of measuring our data y given x. And we have this q, which we called the recognition function. I didn't say what it was called before. But this is a probability distribution in the latent space conditional on the data and its parameters. And the thing is, we can sample from all three of those distributions. So we can convert these three, this three, well, it's not a three-dimensional integral because. Three d well, it's not a three-dimensional integral because the x is a 15-dimensional parameter, and y is many thousands of parameters, and z is also a few tens of parameters. So it's a very high-dimensional set of integrals, but we can sample from each of these from the p of x, that's our prior, we can sample from the likelihood by just generating variations of noise, and we can also sample from the q distribution by just throwing samples into our q network. And then what we're left with is the Monte Carlo. And then, what we're left with is the Monte Carlo integration of this right-hand side term. And this is where you can build up the picture of how we construct that diagram that I had a slide or two ago. And I'm, again, I'm annoyingly covering part of the diagram. I'll try and move out of the way a little bit. What we have is at the top of this diagram, we have the labels X and the data Y. And we simultaneously throw those data into these two. Those data into these two networks, the Q and the R1 encoders. So let's look at really at the pink one, R1. This just takes in as input the Y data. This just takes in the noisy time series gravitational wave data. And what it does is it's going to be trained to output a set of parameters that describe a Gaussian distribution in the latent space. And we're calling those parameters these Î¼r1s. But we also have simultaneously the recognition network, the Q network, taking in the signals, the data, and the parameters, the true parameters of the signal, which we're allowed to have during training. That takes those both inputs, and it also outputs a distribution in the latent space, in the Z space. And in our loss function on the left-hand side, we have the ratio of the pink and the green terms inside a log and also averaged. Also, averaged over Z. We're drawing from the Z distribution in this average. And so that left-hand side actually turns out to be just the KL divergence between those two distributions that are output from those two encoders, which we can compute quite straightforwardly. The second term in blue is the output, well, it's evaluating the probability of the output distribution in the physical space. In the physical space, in the X parameter space, and it's evaluating that at the location of the true signal that went in at the beginning, the true parameters that was input at the beginning. And so this is just evaluating the log of this Gaussian, multivariate Gaussian distribution that is being modeled by the R2ing decoder now. And that gives us this L term in our loss. So we have two components to our loss. And we call them the KL divergence term in the latent space. And we also have this L, which we call the reconstruction. And we also have this L, which we call the reconstruction loss. It's how well the encoder and the decoder manage to get back to the true set of parameters at the end of the day. Now, I may well have not convinced you that that's how we get the maths, but one of the beautiful things I thought about all of this was that this starts, this is an ad hoc. We start off with a well-defined cost function or loss function. We do all the maths, we turn the handles, and then it leaves us something that looks It leaves us something that looks like a set of probability distributions which we can just get neural networks to model. And then the whole idea is you train this beast to minimize this whole quantity. Because if you can minimize it to its absolute best case that it can be minimized, then you are, in a sense, where you are, in effect, making your target output distribution from this whole thing equal to the true possibility. Whole thing equal to the true posterior distribution associated with the gravitational wave measurement. So that's the training side of the network. And just for completeness, when we actually run this on real data, we throw away the Q network. The Q network was only useful for training. And for testing, all we do is we throw the data. If we get some data from a gravitational wave detector, a time series data Y, we throw it into the first encoder. That first encoder generates a That first encoder generates a distribution in the latent space. We take that latent space location and throw it together back with the Y data into the decoder, and that outputs a sample in the physical space. And if you repeat that over and over again for the same piece of Y data, because of the variation that's involved in the latent space, you won't get the same answer each time. You actually get a new population of samples in the physical space, in the X space. The physical space, in the X space. This is your gravitational wave parameters. And those samples, if you accumulate enough of them to make a nice histogram, that is the gravitational wave posterior distribution on the physical parameters of the system. So I'm going to jump now because I am running out of time just to some results. I'm not going to talk about the networks. That's the sort of technical aspects of how we did the networks. It was convolutional neural networks that we were using. And I'm also going to skip over the loss function. And I'm also going to skip over the loss function because it's just a bit technical. And you can just see that we are minimizing that green curve during training. But here, this is our sort of money plot, which has changed over the last couple of years because we've increased the number of parameters, and now we're doing the full 15-dimensional parameter space. But one thing, if there's anything to take away, what I want to sort of have people take away is that there are three distributions being plotted on top of each other. Distributions being plotted on top of each other here. One of them in red is from Vitamin from our new conditional variational autoencoder approach to parameter estimation. And the other two are from standard existing Bayesian samplers used widely in the gravitational wave collaborations. One's Dynasty and one's PTMC. And one thing to note is there is very, very good agreement between Dynasty, the most trusted, and The most trusted and the only one to be fully accepted and reviewed in the collaboration at the moment, and vitamin, our approach. And PTMC does very, very well, but there are some cases, some dimensions where PTMC doesn't agree with either of the other two approaches. There's an example on the right-hand side. That is the time series of binary black hole there that went along with this. And there is a sky map comparing all of the sky maps that came out of the three analyses. I'm going to very briefly go over the fact that we do very rigorous tests to ensure that we are both statistically consistent. Here is a PP plot, and we are very diagonal as diagonal as every one of the other samplers we tested. We did another, there were other two samplers we also included in the comparisons. And we also compared distributions pairwise between PTMC and Dynasty, well, everything with Dynasty in this plot. So vitamin with Dynasty, PTMC. In this plot. So vitamin with dynasty, PTMC with dynasty, CPNS with dynasty, and MC with dynasty. And we computed this JS divergence, which is a measure of how similar the distributions are. And we looked at it on a dimension-by-dimension view. And you can see here the JS divergence is very low. And the blue distribution is the vitamin with Dynasty, which is consistent in its agreement between other samplers as other samplers are with other existing samplers. Existing samplers. So we're very much in the mix and producing the same quality of results as other samplers. And here is one of the final sort of pieces of information I want to get across is the speed. All of this would have been a total waste of time if it was just as fast as the existing samplers, just a new fancy way of doing exactly the same thing. But I want to stress: whilst it does take of order days to weeks to train this machine on a single GPU, Machine on a single GPU. When you've trained it, you don't have to train it again. Well, you don't have to train it very often unless the noise characteristics of your detector change or you want to search for different signals. But when you run it in testing mode, you can generate 10,000 posterior samples for a given gravitational wave detection in under one second. This is six orders of magnitude faster than existing samplers. Existing samplers work Existing samplers work on a, they're not pre-trained, they take the data at face value and then they explore that parameter space. With these CVAE approaches, with these machine learning approaches, you train it up front, and then when you want to run it, it runs individual samples in milliseconds, and you can get full posterior distributions in less than a second. It actually takes longer to make the plots than it does to actually analyze the data in this case. So, I'm just going to summarize very quickly. Machine learning can provide a direct replacement for Bayesian parameter estimation. I think this has been known for a while because this work and others have been out there in the literature for a couple of years now. And it will enable real-time multi-message astronomy. So, being able to get full posterior probability distributions in less than a second, and potentially, in fact, I mean, it's not too hard to imagine that we can do this pre-merger as well. Imagine that we can do this pre-merger as well. So we can train on signals that have not yet merged and be producing dynamic posterior distributions in the final few tens of seconds of mergers. There are many, many challenges which we've got to solve a lot of these, but other people working on this stuff have already solved for their analyses, which is dealing with real detector data and dealing with longer duration signals, because the real interest here is when you have. Real interest here is when you have neutron stars because they're the real thing you want to turn your telescopes to look at very quickly. This analysis is applicable to any kind of Aayesian analysis where you want to do posterior distribution generation, not just gravitational waves with time series input. And I should say there are other solutions available that do very similar things in the last couple of years. And it's very interesting and very powerful work on normalizing flows. That paper there is just directly to the normalizing flows paper that there are. Directly to the normalizing flows paper, but there are the group at AEI does excellent work with normalizing flows to do exactly what we're doing here as well. And I'll just end there, and I'm sorry that I've taken so much time, nearly all of my time. Well, let's thank Chris for the nice talk. Are there any questions online or Online or here. Judith, you could go ahead. Great talk, by the way. I was wondering, so in the step where you train the network, you have these two inputs to the encoder during training. So one of them is the signal itself, and the other is The signal itself and the other is the signal injected in noise. Could you please explain that step one more fact? Which two distributions do you compare during training? So I wasn't clear. Sorry. So this step here on the left-hand side, we have the two encoders. The Q encoder takes X and Y. Now X is just the parameters of the signal. It's just the 15 parameters describing the binary black hole signal. The binary black hole signal. Right? And why is the noisy time series that goes along with that signal that was injected into that data? So the queue encoder gets two of these. I was saying this. So then one is just the 15-tuple number that describes the data. And the other one is actually the strain time series may be sampled at 4K, one second data. Second data, so they go into the network, and then so then, but both of them map into this maybe, I don't know what's the dimensionality of mu Q or mu R1, but you take the KL divergence there and that's it. And that's very interesting. That's exactly right. So we actually use a 15-dimensional latent space as well, which is motivated by the dimensionality of the problem. The problem. Although we find it doesn't really make much difference if we change that too much. And that KL divergence is sort of part of the loss function, it's tempering the loss function. You would ideally think you want both the KL and the L to go down together, but what we find in practice is the KL divergence goes up and the L goes down but faster. So the overall sum of the losses goes down, which actually means, and this is one of the things we didn't realize for a while. And this is one of the things we didn't realize for a while. You don't expect both encoders Q and R1 to be producing the same distribution in the latent space. They're not supposed to, because they have different information given to them. So the R1 encoder only has the noisy data, whereas the Q actually is told the truth as well. Thank you so much. I'm sorry about the noise. I'm outside in a cafe and maybe sorry about that. Maybe sorry about that. But thank you so much. Great talk. Cheers. Any other questions? Yeah. Hi, Chris. Thank you for the talk. So what kind of noise are you using when you train? Are you training on Gaussian noise? Yes, sadly, we're training on Gaussian noise because, well, there's two reasons for this. One, we always like to Reasons for this. One, we always like to start with something we know. It's also only a fair comparison with the existing samplers if we use Gaussian noise. They have likelihood functions that are defined assuming Gaussianity in their data. We do want to join our competitors in user injecting into simulated noise. It's one thing I wanted to just stress, though, thank you for asking this question. This is where these approaches can beat and do better in terms of the quality of results in comparison. Of the quality of results in comparison to existing samplers, because we don't make any assumptions about the noise other than what training data we give it. So, if you do all your simulations and injections into O3A data for training for O3B, you are making an assumption that the noise properties are similar, but you are including all of the nastiness in that data as well and having the network learn that nastiness, that deviation. Network learn that nastiness, that deviation from Gaussianity, which the existing samplers have difficulty in modelling. So, do you expect to train when you go into production mode, right? For example, do you expect to train on real noise? For example, if you had to apply this on O4, then you train on O3 noise, and do you think you will need to retrain periodically? Or you simply go with Gaussian, anyway, is it good enough? I think retraining periodically, if the noise properties are changing, noise characteristics are changing, is one thing. You can also do what our competitors have done, which is to make this whole thing also conditional on PSD estimates. So you basically take a PSD estimate from the first previous minute, and you use that as a conditional piece of information as well. Information as well. Now that deals with the changing PSD, doesn't change with the changing glitch characteristics of the data. So if that happens, you might want to retrain your network. But you can always be retraining in the background on a single GPU over a week and then just upgrading to that new trained version. Also using transfer learning would speed all of this up. So even though it takes a week or two to train from scratch, if you already have a trained network and you just now want to train on some slightly Just now want to trade on some slightly different data. If you start from the pre-trained network, I think it would only take a fraction of the time to learn those new minor characteristics. So that's something that we expect to be running in the background and being updated all the time. Yeah, I think the glitches probably won't be a problem, right? There will be somebody cleaning the data for you. But the changing of the PSD, that if you can do that conditional, That if you can do that conditional, that would be really good. Yeah. But also with the glitches, I think somebody will be removing them, whether they'll be removing them in less than a second, because that's when we are analyzing the data straight after HFT comes in. So we might have to rely on these networks to do the first stage of that if we want to get super low latency posteriors out. Any other questions for Chris? If I could ask a quick one, I mentioned in the chat, but I was wondering. Right, yeah. I was wondering, does the reconstruction loss function matter? Meaning, if you change, say, cross entropy to means, mean square, or maybe a better question is, what was the motivation behind? Is what was the motivation behind choosing cross-entropy? So, the motivation is because the very clever computer scientists came up with this way before we did, and we have just used their tool, right? So, this is a standard thing in computer science to start with that cross-entropy, follow through all the maths, and then you end up with the expression behind me about how to calculate things. I think it's possible for you to start with a different loss function. If you can construct one. If you can construct one that theoretically, when it's minimized, you have also achieved the true posterior, but then also that thing is tractable and can be computed in a sensible way, then that should be fine. But we haven't looked at another way of doing it because it seems like far cleverer people than us had decided this was the way to do it. I see. So, this elbow only holds true. Elbow only holds true if you start from the cross entropy loss? Well, I'd say if you start from the cross entropy loss, you make use of this elbow inequality to do this calculation. I'm not saying that if you had another loss, you wouldn't also have an elbow term in there, but it depends which one you would start with. I see. Okay. Thanks again. Thanks again. So we have a question here. Go ahead. Hi, Chris. This is Sean. Just a quick question. If you are doing this for BNS events, so where does the cost enter? Is it the Y is now bigger? Sorry, Musk. Is the Y is now longer. So the dimensionality of the latent space is going to go up. So it is really the Monte Carlo integration that's going to take more time. The Monte Carlo integration that's going to take more time, right? So, what I mean to say is that the training is going to take more time, not the final result. It's going to still be subsecond, right? Right, I'll try and keep this brief because there's a lot to unpack in that. You're quite right. The challenge for BNS is hard. The signals are longer, but the latent space doesn't necessarily get any bigger because the latent space is trying to encode the relevant information about the signal. Now, BNS signals are longer. BNS signals are longer, but they don't contain more information than a binary black hole signal. They still have 15 parameters, or maybe a few more if you've got some tidal components. So our lane space wouldn't actually change, but you're right, the input data would get significantly longer. And again, our competitors, keep saying collaborators, our competitors are making great headway in being able to compress that data into more manageable forms for input into the network. Work. And that's really going to be the secret is because you can't take 100, 200 seconds worth of time series sampled at 4 kilohertz into these things. You will just kill the GPUs and you will never be able to train this in a sensible amount of time. So you have to be smart. And the reason we did buy black holes is for that reason. We wanted to start on the simplest problem. But it's also quite a hard problem in itself. Great, thanks. Breaktoff. Great, thanks. Great talk. Thank you. Good to see you, Shao. Anyone else have a question? We have one more here. Hi, great talk. I was wondering, when you are in the testing mode and you have a fixed time series, how many samples do you have to take? So you get like enough information. Like enough information about the distribution, Australian distribution? Aha. We just go by the standard rule that the Bayesian samplers tend to produce around 10,000 samples. Some of Bayesian samplers, like nested sampling, you can't ask it how many samples. It just gives you a certain number of samples at the end. And they're usually of order 10,000. For us, we can just ask for as many as we want. But it just comes down to how. We want. But it just comes down to how accurate you want your Bayesian histograms to be at the end to make those lovely corner plots. If we wanted to make them super, super smooth, then we'd just do 100,000 points and it would take us 10 seconds instead of one second. But the general consensus that the rest of the community has used is of order 10,000, which gives you nice enough posterior distributions. I hope that was the question. Question that I should have answered. Yeah, that was the question. Thank you. Thanks. Anybody else have a question? No, not here. No one in the chat. Okay, well, with that, let's thank Chris again.