Yeah, okay. Thanks to everyone for coming and thanks a lot for the invitation. Yeah, my name is Yung Fu and I'm currently a postdoc at UC Berkeley. Before we start, I would like to apologize for the thing that I'm going to talk about today. It's not something particularly new. It's a joint work with Ling Jung-Li, and we did it three or four years ago. In terms of myself, I do discrete probability theory. Myself, I do discrete probability theory and I have done like various different topics. And I just found for this workshop, this project is still the thing that's most relevant. And also, I find for many of the audiences here, I haven't got the chance to present this, so I hope you can still find it interesting. Okay, so let's start. So, first, I'm going to talk about the background, but since you are many experts here and also I'm mainly experts here, and also Jin talked a lot about Anderson localization already, so I'm going to be a bit quick about this part. So, historically, this model was named after Philip Anderson, who first proposed and found the localization phenomenon back in the 50s. Namely, you just want to analyze how a quantum particle evolves in a random environment, and that is modeled by this random Schrodinger equation. By this random Schrodinger equation. So, in the left-hand side, it's like a Planck constant and imaginary unit. And in the right-hand side, you have this random Schrodinger operator. So, here it's in the D-dimensional lattice, it's a discrete lattice model. And here, this operator is written as a minus Laplacian plus delta V, where Laplacian is a discrete Laplacian. And this delta is just some positive real number, the disorder strings, and this V is the. Strings and Zv is the potential, which is taken to be random. So it's a function on Zb and is IID random at each vertex in width. So as I said, this models quantum particle, so the size of the particle profile, which evolves in time, and this random potential V just models the environment where this particle holds. And the localization phenomenon observed by Anderson is that when dimension Understand is that when dimension is one or two, or when dimension D greater or equal to three, but the disorder strength is large enough, your particle will be trapped in a small area. So one way you can formulate it like this. So this quantity is, well, this square of the psi t at A is the probability that you can find the particle at location A at time t. So this is just the expectation of the square of the distance of the particle. Distance of the particle. So the condition says that the expectation of the square root of distance remains uniformly bounded as t goes to infinity, so it does not go to infinity. Whereas there is also a delocalization phase when dimension degrees 3 and the disorder strings is small, then quantum particle would behave diffusively. So that if you look at how far it goes away from the origin, it has like square root growth. Square growth. So if you look at the expectation of the square of the distance, that should grow linearly. Well, these are like physical predictions, and there have been lots of experiments in the past decades of different types of materials at different dimensions where this localization phenomenon is observed. So these are all classical things. And now let me tell you about the mathematical formulation. Mathematical formulation. So you basically need to analyze this random shorting operator and its spectrum. So it's a classical result that almost surely for this random shorting operator, the spectron is the sum of these two parts. So the first is the spectrum of the minus ablaze, and the second is the support of delta times v. So what that means, times V. So what that means, remember this V is taken to be IID random at each vertex in the D-dimensional lattice. So the support is just the support of the random distribution at each vertex. And usually people assume it's compact supported. And since this derivative is just some real constant, you can just factor that out. So it's written this way. And the notion I'm going to use is this spectrum localization. So it says like So, it says like for any interval contained in this spectrum, which is deterministic, we see that spectrum localization happens in this interval if you precisely have the following statement. So for any generalized eigenvalue u, namely, it satisfies this eigenvalue equation, but has at most polynomial growth, so it's not necessarily L2. Necessarily L2. But for any such generalized eigenfunction with eigenvalue in this interval, this u is actually a true eigenfunction and has exponential decay. So if this statement is true, we say that the localization or spectrum localization holds in this interval y. So that's a notion, and in particular, this implies that the spectrum of this operator H in this interval. This operator H in this interval I is pure point. Whereas, for like a classical theory of how you can decompose the spectrum of the self-fuge operator in a Hilbert space, it can be written as a union of these three parts, the closure of the pure point spectrum, which contains all the actual eigenvalues in L2, and also the absolute continuous spectrum, the singular continuous spectrum. Again, the physical meaning is that this pure point spectrum corresponds to localized states, whereas To localized the states, whereas this absolute continuous spectrum corresponds to like a delocalized state such as Brahma-Wigs. Well, the implication is that if this is true, then this interval I is disjoint from the absolute continuous singular continuous parallel spectron and is contained in the closure of the pure point spectrum. Okay, so that's a basic notion, and then you have this phase diagram. So, this is a phase diagram. So, this is a phase diagram in dimension d greater or equal than 3. So, the horizontal axis is the energy level, and the vertical axis is the disorder strings. So, if you want to look at the spectrum for any fixed data, you just take a horizontal size. And the spectrum area is given by this region. Remember, like we know that almost surely the spectrum of this operator is the sum of these two. So, when the third height is this. So when the is zero, you just have this interval from zero to 4D. And as the tai increases, this thing just grows linearly. So that's a practical thing. And the phase diagram people believe is that there is some like a mobility edge, so that below this curve you have absolute continuous spectrum, whereas above this curve, you have pure point spectrum, with a fairly good meaning that you have like a You have a conductor here and an insulator here. And more precisely, it's like for Dirk High is small, you take a horizontal slice, then near the edge you would have localization, and in the bulk you would have delocalization. Whereas when delta is large, let's say you take a slice here, then in the host electron you just have delocalization. So that's a phase diagram believed in dimension vertical equivalent string. In dimension vertical equivalent three. And in dimension one or two, this is different, and people just believe that in the whole picture, you would have localization everywhere. So that for any third time, the spectrum is always pure point. And here are some of the results. So the first wave of the study goes back to the 80s, like represented by this work of Freud and Spencer. And let me just give some summary of what are the things known. So, in dimension d equals one, more or less like everything is known, so that people can prove for any sort of non-trivial potential v and potential v and also any like positive data, you have organization in the host very far. Whereas for dimension d greater than two, the region above this red curve is proven, so people It's proven, so people show that when disorder strength is large enough or for any fixed theta, but you are close enough to the boundary of the spectrum, you always have purified spectrum localization. For dimension equals 2, while people believe it's localized everywhere, it's unknown what's happening below. And when dimension goes to 3, people haven't really proved any delocalization and also don't know where this transition places. Also, don't know where this transition precisely happens. Actually, that's something Jungmen has just talked about. Okay, so now one thing in this works from the 80s is that they need one key assumption, which is that the distribution of this IID potential is continuous. Well, historically, like in this first few works, they need to assume that for this IID potential, That for this IID potential, the distribution density needs to be upper bounded. But later in this work of Kamonakling and Magnemizif can sort of generalize to like holder distribution functions. It's not necessarily bounded density, but if you look at the distribution function, it still needs to satisfy some continuous properties. And so, a particular question left is that, okay, for 1d, everything is known, but for dimension d greater or length 2, is the localization sort of universal? Namely, can you go beyond this particular continuous assumption? And there, an extremal case is the Bernoulli case, namely for each vertex A, the random potential equals 0 or 1. And the potential equals 0 or 1 with half probability, and that's IID Bernoulli potential. And they have some physical meanings, such as if Timothy alloy-type materials. And so this is the Anderson-Beruni problem. And the main breakthrough on this problem was achieved by Baugen and Koenig in 2005, where they In the five, where they instead of working on the Mattis model, they started a continuous space model of Rd. So, what is that? Well, if you write out the model, it looks the same as the discrete lattice model. So, you have a minus Laplacian plus some potential V. But here, since you work with a continuous space, this Laplacian is a continuous space Laplacian. And this V is taken in a way that's. Is taken in a way that sort of simulates the lattice setting so that in each lattice point you just put some bound functions. So this besides some bound function, so you just put a bound function with probability, say, one half independently at each lattice point. And then you want to ask for this operator, can you still prove the localization near the edge of the spectrum and the disorders? Spectrum and the disordered strings is large. And that is achieved in this paper of Bolting and Kini. So they showed that in dimension d greater or equal than 3 for D greater or equal than 2 for this model, for this operator in the L2 of RP space, almost surely you have spectral localization near the edge of the spectrum. So, the next major breakthrough in this topic is by Jen Ding and Charles Smart in 2018. So, they went back to the original lattice model, and there they showed that in the two-dimensional lattice, the spectrum localization holds near the edge of the spectrum. And for the IID pernovices. Potential. And here is our result. So we work on the 3D lattice setting. And in the 3D lattice setting, we prove spectrum localization near the H. Well, more precisely, if you take your operator H in the 3D lattice, which is minus L Passion plus I differ Loudie potential, we show that for any disorder strings, we can find some small enough epsilon, such that only surely the Such that almost surely the spectral localization holds in the interval zero and the epsilon. So that's the result. Any questions so far? Okay. So for the rest, I'm going to give you some of the ingredients involved in this proof. I would need to say that for this problem or this topic, For this problem or this topic, it is something that has been developed. Is there any way to make the absolute moment to you? That's a good question. So I think from our method, if you work hard, you can get some precise quantitative, but that's probably a bit far from what is expected of people from, at least from our measure. Yeah, no, I mean as a function of delta, exactly absolutely. I think the proofs are our point of view, because it's just the base case. So I think you just make some explicit estimate of the base case using the principal eigenvalue. And I think it's poly sort of for free. Sort of for free. I mean, this may not be short, so the final definition you also expected to do the curl as a solver program. Maybe it's good. I want you to keep something. Thanks. Okay. Yeah, so yeah, as I was saying, this is. This is some topic that has been developed a lot since the 80s, and there has been heavy machinery in solving such kind of problems. So I'm probably not going to give you a comprehensive outline of how this is done, but to outline some of the key ideas and the focus, mainly on what's different from continuous space to the display space, and also what's different from the 2D lattice to the 3D lattice. Uh 3 lags. Okay, so okay, so next. The general framework we use is the one that developed by Froley and Svenzer from the 80s, which is this multi-scale analysis. And in this multi-scale analysis, what you do is like you look at some like a finite box, you restrict your operator in this finite box. Operator in this finite box, you need to estimate the sort of the relevant there and then we connect up this resonant using some arguments and then do it from one scale to another scale and eventually obtain the exponential decay of the original resolvent. And in doing so, the key thing bear in mind is that you need to find the density of the states and namely giving. And namely, giving an interval, you need to show that with high probability, there is no eigenvalue in a small interval. And here, a key step is this thing called Wechner estimate. And the original form of it is pretty clear, and it is stated this way. So, for any self-adjoint operator, let's say in a finite-dimensional space, where you can basically think Where you can basically think of it as a symmetric matrix. And then you add a diagonal matrix to it. So the diagonal matrix is a thing, and these diagonal entries are taken to be IID of each other. And then if this IID random variable, their distribution density is bounded by lambda, then for any interval j, the probability j, the probability that there exists an eigenvalue of a plus v in this interval j is upper bounded by this. So this is something you can just prove using perturbative arguments so that you continuously change the value of v and you just analyze how the eigenvalue would be would change as a function when you change v. So that when v just increases at a given speed, you lower bound the speed that the eigenvalue changes. The speed that the eigenvalue changes, and then you can get such an estimate. But here, the key thing is that this potential should have distribution density bounded, and that's sort of the reason why in the 80s, they first need to assume that this random potential density is also bounded, as they use this Wegman estimate. This is something not available directly in the Berlin case. The Berlin case, and that was the first difficulty people faced. And what's done in this paper of Bo Ging and Kinik is that they managed to obtain some weaker Wagner-type estimate using induction arguments. So by a weaker Wagner-type estimate, I mean like they get some worse bump, but still suffices to prove the localization. And they did in a multi-scale induction way. In a multi-scale induction way, and sort of integrate that into this multi-scale analysis framework. Okay, so therefore, now we have this general framework of multi-scale analysis from Ford Spencer and Paul King and Kenny. And there, in order to prove this so-called Wigner-type estimate, you still need to run some kind of perturbations so that you perturb your potential V and Your potential V, and then you need to analyze how the eigenvalue changes. So then you can show that eigenvalue in this interval, if you perturb it a little bit, then the eigenvalue would get out of the interval. Then you get some upper part on the probability that the interval contains some eigenvalue. Okay, so basically you have some equation like this: so Laplacian u plus vu equals lambda u, and you want to perturb this v and You want to perturb this v and see how it affects the value of lambda. And here, a main issue is that let's say you want to perturb this potential v at some vertex x, but if your function u at this x is 0, then perturbing this v would be useless because changing v at this point would not change anything here, so that your lambda in particular. Your lambda in particular is also fixed and not changed. So, therefore, one task you need to do is to show that for u satisfying such an equation, you can still find many places where this u is non-zero. And if you can do that, you can run this perturbative argument and see how lambda is sensitive to perturbation of v and for that. And for that, the key thing you need is the so-called unique continuation principle. So, this is one that is a very simplified version, but the spirit of the thing that used in this paper was again Kenyan. Well, basically, in Rd space, suppose you have Laplacian u plus v u equals zero. In particular, here this Laplacian is a continuous space Laplacian. And then the claim is that if u is not identical. If u is not identically zero, for example, it equals one as an origin, and also this v is sort of bounded, then you will have that in any open ball, this function u is not identical to zero. Okay, this is a very like a clear version, but what's actually used is a more quantitative version. You need some lower boundaries on what the value is here. But this is something. Is here. But this is something that is true in the continuous space setting, as it's some elliptic operators, and you can sort of have this unique principle holds in any dimension without any issue. And that's what's used in Boogey and Kini to obtain continuous space per newly setting results. So, the main issue with the discrete lattice setting is that this unique continuation principle, or at least some analog of it, is not precisely true. And here is a counterexample, which can be found in a textbook of Sweden. So, there, let's say you consider the two-dimensional lattice. Lattice. And in the 2D lattice, you take your function u such that it's zero of the diagonal. So it's only non-zero in some diagonal. And along this diagonal, it takes value 1 or minus 1 alternatively. Okay, so this is your function u. And then if you apply Laplacian to it, you would get 4u. So you would have the Laplacian u minus 4u equals 0. So this particular For u equals zero. So, this particular say that you can find a v that is uniformly bounded, and this is true. But on the other hand, if you think about this function u that you get, it is zero almost everywhere. So, if you can find like arbitrarily large box where this u is identical to zero, but this function u is not zero everywhere. So, this gives you some sort of counter-example, like something like this. Something like this actually not be true, so that you need to do some further things to like work with the disquiet-lighted setting, and that's sort of the thing that prevented Bogging and Koenig to work with the complete discrete setting. Okay, so let's step a bit back, and as there are already some counter examples, so like something precisely like this is not going to be true. This is not going to be true, but can you just have some weaker estimate, but still suffices for the purpose to get the localization? And this is something sort of figured out in this paper of Fing and Smart. So in the D-dimensional lattice, if you can prove the following, then you can fit it into the framework and get the localization result. More precisely, in More precisely, in Zd, if you have a function u at v satisfying this equation, and the u is not identically zero, such as u at origin equals one, and also you assume that this function v is bounded, then we hope to have the following estimate. So, if you consider the set of points where this u is non-zero, and you want to show that in some And you want to show that in some large box of n to the d's, the number of non-zero points is at least n to the d over 2 plus epsilon. Okay, so this power of n to the d over 2 is like the square root of the number of vertices in such a box. And the argument sort of comes from some combinatorics like Sperner's lemma. And as long as you have such amount of non-zeros, you can still rock some perturbation and show that your eigenvalue can jump out of a given interval by perturbing its potential V. But again, I need to see that actually you need some more quantitative version of this. So instead of just being non-zero, you need to count the number of vertices where it's greater than e to the. Where it's greater than e to the minus Cn. But this is a cleaner form that I just want to show you. And this is a dream that you want to do. And if you have that, you are more or less solved. But unfortunately, this is not precisely true, as you can see from the counterexample by Smith Nana in the previous slide. So here, this is a two-dimensional. So here, this is a two-dimensional case, and you can construct some u and v such that this function u is non-zero just in some one-dimensional subspace. So this means that at least in two-dimensional signal because of d equals two, you want some n to the one plus epsilon power of non-zero sets. But unfortunately, in 2D case, that's it's not possible. And so you may ask how DNA smart did in the 2D setting. Deanna Smart did in the 2D setting, where they sort of get around of this by resort to some randomized argument. So they, as this is not true, but they prove that this bound holds with high probability when the potential v are like IIB random. But in the 3D case, we can obtain the desired estimate, and this is what we prove. So we show that in the 3D lattice, In the 3D lattice, if you have function u and v satisfying this, and also u is non-zero at origin, then this non-zero set in a n cube has size at least n to the 2 minus epsilon. And that is enough for a purpose, right? So in 3D, you need like n to the epsilon. Need like n to the 1.5 plus epsilon, and we can get 2 minus epsilon, so that's good enough to get the result. Okay, so for the rest of the time, I would like to explain to you some of the ingredients involved in proving this bound. Okay, so the first key ingredient The first key ingredient that I want to mention is this thing called 2D Luvier theorem. And this is from a paper by harmonic analysis people. So in this paper, they proved some like Julius theorem for discrete lattice harmonic functions in the 2D setting. And one result in their paper is the following. So if you Is the following. So if you have a 2D lattice harmonic function u, and it satisfies that if you consider the non-zero set in some like n-by-n box, that the size of that set is smaller than some epsilon times n squared, so that the density of the non-zero set is smaller than epsilon. Then, if you look at a smaller box of, let's say, n over 2 times n over 2, then Number two, then this function u must be identically zero. Well, in other words, if you know that this function u is not identically zero, the smaller box, in particular, say it's just not identically zero at all, it's not zero at the origin, then if you look at the larger box of n by n, then the set of non-zero points should be at least the size epsilon times. At least size epsilon times n squared. Okay, and how they did that? Well, that's a sort of very like harmonic analysis argument. But one key ingredient used there is some sort of polynomial argument. So I can explain to you through this figure. So if we take some such rectangle, a tierti rectangle in the 2D lattice, a key property. A key property of the 2D harmonic function is that if the value of this 2D harmonic function at these black points are already determined, then you can inductively determine the value of this harmonic function in the rest of this rectangle. Let's see, for this point, it can be determined by these four other points, and then you can inductively do it line by line. So, from this, you can actually have some polynomial structure from that. Well, more precisely, let's say you are given the value of a harmonic function at all of these black points. And also, let's say in these two lines, the function u are identically zero. Then, if you look at the third line, then you have like, let's say, x minus x, x minus x alternative. Minus x, x minus x alternatively. And then if you look at the next line, then you would, if you write it out, you will find that the harmonic function here is sort of linear. And then if you look at the third line, then you'll get a degree two polynomial. So if you look at the case line, you'll get a degree k polynomial. And for a degree k polynomial, you know that either it has at most k0s or it must be identical to zero. So this is a way that you can sort of look. This is a way that you can sort of lower bound the number of nano zeros for such a harmonic function. And this kind of idea is also used in this paper of DM Smart as the precisely working 2D lattice set. Okay, so this is one key ingredient. And the next key ingredient is some very basic combinatoric observation. Observation. So it's the following. So if you have a, let's see, such a function u satisfy this, and at some vertex x, suppose that your u at this x is non-zero, then it cannot be that for all of these six vertices, they are all identically here. So I can draw it. So it's So it's it's it's a very simple statement. So let's say you have like the 3D magnetics and this is your vertex x. Then you just walk one step and look at this point which is x plus 1 0 0 and then you look at this equation around this point. And so it cannot be that the function u is identical The function u is identically zero at all of these other six points. But not zero at this point, because otherwise this function at this point of x plus 1.00 would not be true. So this gives you a way that if you know that this function is non-zero at one point, you can find one other point that is also non-zero. And then you can keep working on this, and you can find some sequence of non-zero points. But in general, But in general, this is just a combinatorial fact that if you consider the non-zero set, the set must satisfy that its intersection with any such seven points cannot be that it just intersects at one point and does not contain any other point. So, this is a very basic combinatoric effect of this Leonardo set of this function. And then And then one thing you might think of whether you can just use this to deduce use some like combinatorics argument to sort of deduce some lower bound on this non-zero set. At least for the three-dimensional case, it's still a bit unclear to me whether this alone is enough. But, okay, at least in even dimension, this is not enough because you have these counterexamples. But also, in But also in high dimension, this is also not enough. I think like Charlie has some counterexamples, and Ling Jring and I also have some counterexamples. And there is also a recent paper by a master student of Lokunov where he explicitly writes out some counterexamples, saying that in high dimension, this set can be much smaller than n to the old. Yes. Oh, yeah. Okay. However, this comethal effect is not completely useless, and this is a key new thing that we sort of did. And we used these combinatorial arguments to sort of decompose this non-zero set of this function u to some like lower dimensional structures. Well, the idea is that you need. Well, the idea is that you need to look at some plans, for example, x plus y plus equals zero, and then you restrict this non-zero set to, so first you restrict this lattice to such a plane, then you would get a triangular lattice. And in this triangular lattice, if you choose the plane appropriately, then this function u would satisfy some relation, and then you sort of reduce this problem to some 2D lattice setting. And finally, you need to. And finally, you need to sort of assemble all of this 2D lattice, triangular lattice results to the original problem. Okay, so more precisely, so if you think about this plane of x plus y plus z equals zero, its intersection with the straight lattice would be a triangular lattice. And then if the plane is choosing appropriately, you would have that in this triangular lattice, if you come with this function u, in any such smallest triangles, the three numbers at these three vertices would sum up to zero for this function. Well, if you imagine that this happens. You imagine that this happens if you look at the, it's a 3D space, you look at the plane, like parallel to this plane, but like one level and two levels below them. If u is identically zero there, then you would have that u at these three points must add up to zero for any such small triangles. So to speak, you would have something like this. So for you want to analyze such To analyze such a function u in the 2D triangular ratio satisfying this kind of relation. And then here is the thing that we showed. We can, for such a function u, you can lower bound the set size of the non-zero set in a m by m box for such a function. And this is sort of proved by adapting the arguments in this 2D harmonic function case. Function case. And then using that, we get some lower bounds on the non-zero set in some such structures. And the next thing is that you need to assemble these lower bounds at different such triangular lattices together. And that's done in a multi-scale and quite combinatorial way. And at the end, you would get the lower bound of the non-zero set of this original function u. And And also, we need to do one more step, which is to upgrade this thing to a quantitative version so that replace this non-zero by something greater than e to the minus c times n. And that itself further involves some combinatorics arguments. Okay, so that's all about these things. Finally, I want to mention some further things that I'm thinking about. First of all, it's natural to ask what can you do in high dimension. Well, there are some people arguing that 2D and 3D are the most physical dimensions, but it's also interesting to look at the high dimension. On one hand, it's like a theoretic point of development. And in particular, for this result of walking and kinetic, they could do. Without the Balkan and Kinnick, they could do any dimension greater or equal than two simultaneously. It's a bit embarrassing that we, in a discrete lattice setting, we have to do one dimension at a time. So we actually have some arguments, like a four-dimensional or five-dimensional, but that's by sort of generalized idea we did in 2D and 3D. And in particular, you need to do some dimension reduction with some combinatorics arguments. Just like how you reduce the 3D, that is. Like how you reduce the 3D lattice problem to the 2D lattice case. But ideally, we want to find some more systematic treatment so that you can hold the dimensions together. And there's another question of quantum percolation, and this is sort of related to what's happening in the large disorder case. So, for example, for the things in the 80s and also for Polk and Koenig, what they did, they proved this localization near the edge of the spectrum. Near the edge of the spectrum, but people usually believe that these arguments can be sort of adapted also to deal with large disorder in this. But in the discrete lattice setting, there is an additional subtlety which comes from this quantum percolation phenomenon. And for that, you can think of this extremal case, namely your dirt hat goes to infinity, so that your potential v takes zero or infinite at each vertex independently with probability one-half, and you ask. Have and you ask now, you end up getting like this site population cluster, and you ask what's happening with the Laplacian here, whether that has localization. And this, I get the feeling that this is some deep problem as it's sort of the so-called threshold of classical population versus the threshold of quantum population. Or quantum calculation. Namely, if you have like an infinite cluster, the calculation cluster, can you still have some delocalization phenomenon? And this is something that has been open for a while. That's all I want to talk about. Thank you very much. Questions? Uh, so you have this very nice triangle theorem that you reduce the 3D unique continuation to. Is that triangle theorem deterministic or is that one actually sort of probabilistic and then somehow after averaging you get... Oh, we work with deterministic. Okay. Yeah, so this thing is deterministic, just like the 2D harmonic lattice, 2D square lattice harmonic function. Oh, so this is for harmonic. Oh, so this is for harmonic functions, no potential. Yeah, this is just a function. U in the triangularity is satisfying that in every small triangle it's added up to zero. Then you can lower buttons. So this does not have any but you need a quantitative version, right? Which is not zero. So what is the quantitative version you need? You have an e, you must have some, not just it's not zero, but it has to be bigger than something. Yeah, yeah, it's bigger than e to the. bigger than something. Yeah, yeah, it's bigger than e to the minus constant n. So it's exponential. Is the exponenti big or e to the minus n or? Yeah, some it's e to the minus constant n for okay, some constant. If you can have that, that's something. Yeah, so actually, like to upgrade to this quantitative version, we sort of lose a bit of this exponent here. So when you're updating something like I don't remember 1.7, 1.6, using some kind of Horix arguments, but let's say enough. Let's say enough, lucky in the street. Does there theory also true for this triangle letters? I think so. I think that should be. Here it doesn't work. Because some of the people relied on the uh explicit formula of the person kernel, right, on the on the square. How to do it from the Yeah, so not exactly from that. So in this triangular matches, you can still have some like. You're talking about the second half of the proof, right? Because there's two ingredients. There's this ingredient, which is this kind of lower bound, but there's a matching upper bound you need to prove a little bit. But that's just some Fourier analytic result, and it's easy to show for the last the second half is easy. I mean, you know. I forget which half, you know the Haldane series. Did you know the algorithm uh three circle? Yeah, yeah, the three circle part. The three circle part is just only three circle models rely on. Uh yeah. I like the last problem with the population cluster. So I think you said uh what the problem was, but let me just uh remember if I okay, so you have this this probability P, and so you're saying, uh, so classically there's a transition to infinite cluster that's and so the problem, the question would be if there's a higher p, so that up to that higher p, So that up to the hierarchy you have localization and above the hierarchy it's a spectrum or something? Yeah, so the problem is that, okay, so in the classical calculation there's a P C, right? Above this P C you have infinite cluster. The question is if there's any higher P such that P greater than P C you have still have infinite cluster but the say the Laplacian here still have uh localization. Right. That's a question. No, no, th right but but is there like an expected result? But is there like an expected result? I mean, so you think there is a hierarchy, but I'm asking, is there a hierarchy up to which you have a position above which you have these spectrum? Is something like that expected? I think physicists have disagreements. That's what I heard. This is what physicists call the quantum calculation special. So some people actually expect exactly that picture that you have one. Physicists did this in the 80s, so they have to do it. And there are disagreements? I got the impression that some physics think that there is a quantum percolation ratio that is different from the classical one. That's reasonable in three dimensions, but maybe not in two. Oh, okay, I see. I mean, in two, you might expect always to see localization. I think so. Unit any methodological result? No. Nothing. So, but you can. So if you go a little bit beyond the classical perpolation, then you presumably still have, maybe prove localization, right? No, no, but then you have finite clusters. Then you have finite clusters. No, no, I mean. You mean about? Above. Yeah. No, no, but there's not even an idea of if you have one minus one over a million at your basis spectrum, not even that. No, no, no, no. And then I have two measures. I think the most reference I can find about this is in physics and also goes back to the 1980s. Uh y yes, yes, that's that's also true. Given this basic statement is also true in that uh but it's just a matter of yeah how you can put this into the like so this is a basic building block we use, but there are I think the main hard part or the main technique part is how to use this to analyze the I was asking you because you were drawing the the conjecture kind of vertically. It's like you were expecting the absolute continuous spectrum doesn't really increase when your disorder increases. Oh, is it uh that's what I expected to be or is it like just a I don't you mean like whether the slope here is this way or yeah it's like the red zone at the localized region. Rentum at the localized region, I expect it to grow as a smaller growth. I want them to get them. Yeah, it should have like a max, like a just hatch cloud. Also growth? Yeah, the AC spectrum growth. I see. And you were mentioning that the edge polynomial. This is known on trees. I think this the kind, the shape that you go. This is known if you don't do this on D of course not in that because that's a conjecture, but on trees it's just I see, I see. If there's no more question, that's going to be a good idea. So at some point you're going to get a normal like minus nine. 