Okay, so motivational slide, but I think Sam already kind of took this point home. Reality looks like this, and we are faced with reality as modelers. We look at reality and we acknowledge already that it's a lost cause. We're never going to be able to model. We're never going to be able to model or get the electronic structure to the kind of accuracy we would like for a system, realistic system like that one, where the characteristic length scales are in the hundreds of nanometers. How can we do that? Well, so far, I think it's an open problem. And orbital 3D FT is definitely one way to attack this problem. But there are many other ways to attack this problem. Density embedding, machine learning. This is something we are actively working on. And we will hear in this work. Will hear in this workshop? We'll hear some very good other alternatives based on linear scaling methods and other methods, which I will not talk about. I will just simply focus on what we did. And hopefully, that's still quite interesting to you. Okay, so the three alternatives to DFT that we develop, the first one, orbital-free DFT, is something that we will discuss later. It's something that we will discuss later, but essentially you have it's a theory of maps. You have the system of interacting electrons. These giant circles are electrons, and these little wires are the interaction between them. The system of interacting electrons can be mapped onto a system of non-interacting electrons that map has a cost, and we will see later what that is. The system of non-interacting electrons can be mapped on a system of non-interacting bosons. Of non-interacting bosons. And that also has a cost. That map is theoretically exact, or it exists, but computationally, the cost is to, every time you have a map, to make one approximation. Okay, if this is either too approximate or not quite there yet from the point of view of the computational models, then you can consider something like this, which is kind of an average between cone. Of an average between Kohn-Sham DFD and orbital free DFT, you essentially say, look, I can take small systems like molecules, consider them to be subsystems, and use consham DFT for the subsystems, and just account for their interactions using orbital-free functionals. This is achieved by this energy expression, which is also formally correct, meaning that there is That there is a assuming that the subsystem electron densities, Ni's, are Vs representable, then there exists a combination of Ni that gives you the total density and through this energy expression also the total electronic energy. So, this is formally exact, just like DFT is, and computationally gives you access to something in between orbital-free and conscious DFT. Again, I will not talk about this. Again, I will not talk about this today. We've also been working on machine learning. The questions that we wonder there is: what should we learn? What is the more appropriate or even ethical thing to learn? It turns out that actually we can have endless discussions, but in my opinion, yeah, you really don't want to make models that are specific to certain quantities. That's not ethical. What you want really is to learn something. What you want really is to learn something that is general and has packed in it a lot of information, such as the electronic structure. And what is the electronic structure you choose? It can be the wave function, can be the electron density, the one-body reduced density matrix, you name it. Okay, now we have heard from Sam about orbital 3DFT and what that is. Here, I just want to point out a few things. Here, I just want to point out a few things. We understand we have to find approximations for the single particle kinetic energy, and to do that, if you do that in an efficient way, then you can afford to just simply brutally minimize the DFT Lagrangian. You can just minimize that Lagrangian and get, as the argument of that minimization, a ground state electron density, and as a Density and as a value for the energy associated with that ground state electron density, you get also the electronic energy. Great! This allows you to reach very large system sizes. And nowadays, we have approximation for TS that are good enough to even look at some semiconductors. Of course, metals generally are accessible. Semiconductors, not so much, but some of them, yes. Not so much, but some of them, yes. There are some big problems. They have mentioned second-row elements and other elements also in the transition metal block are inaccessible because of the need to use pseudo-potentials. These are in orbit-free, have to be local. There is a problem lying there. And also, whatever approximation you come up with is approximate. So you will have you'll hit some sort of accuracy limit at some point. Accuracy limit at some point. So we have worked hard, and you'll notice recurring names here when Hui being one of them is here, and Shuosheng, another one of them, who is online listening. And essentially, what we worked on is to try to get better approximations to this single-particle kinetic energy density functional. And if you look at the landscape of density functionals that have been Of density functionals that have been mentioned by Sam, you really have two possibilities. Okay, the third one is the local approximation, but that's just a Thomas Fermi. You have two possibilities. If you think about what gives the value of the potential, meaning the first functional derivative of the kinetic energy functional in point R, a semi-local functional will give you that as a function of infinitesimal neighborhoods. Infinitesimal neighborhood of points around R. But if you use a no if you employ a non-local functional, then you have access to a finite neighborhood of points around R. And that's where the non-local functionals really are able to capture this non-locality more efficiently than a single point functional that only depends on the density gradient. And of course, you know, you can already imagine there are discussions here that one can have. Imagine there are discussions here that one can have about what's the right thing to do. Well, the can I say our workflow has hinged on these four points. Number one, we come up with a model second functional derivative for this kinetic energy functional. And we use this equation where this is formally correct equation. This is a formally correct equation where the second functional derivative of Ts is equal to the inverse of the con-Sham response function. Well, you can make an approximation there and say, okay, we can use the Lindert function for this. Then you notice the Lindert function has a Fermi wave vector dependence, which has a density dependence. You can then say, well, how am I going to handle that? We have heard from Sam how it can be handled using average densities or ad hoc. Average densities or ad hoc values for these average densities. One can also say, ah, maybe we can just write it as a density field, right? Position dependent, Kf, but also you can go beyond just a simple position dependence. We have stopped here. The group of Yan Ming Ma and the group of Emily Carter have gone beyond, although with some complications, and maybe when Hui will touch on them later. The fourth step is. The fourth step is to do a functional integration to go from the second derivative back to the first derivative. And the first derivative, really, that's all you need to obtain an electron density. The functional derivative can seem impossible, right, to actually enact, but if you do it in certain ways, actually is doable, is approximate, but it's doable, and it delivers. But it's doable and it delivers better results than if you don't do this. Okay, something that Sam mentioned, which I think is correct, two-point functionals or non-local functionals are slower than one-point functionals. So what do you do then? If you do this minimization brutally, every time you need to find the step or the direction in density. Or the direction in density to go, you have to, the minimizer, you have to then know the derivative of the energy with respect to the density, that's the potential. So you have many potential calls during a minimization of this kind. Well, this is why if you just use a non-local functional just like that, and you give it to the optimizer, let's look at a block of silicon bulk, you know. You know, with 30 hours on a single CPU, you can at most do 20,000 atoms. Now, the chemists in the audience would say 20,000 atoms, that's a huge amount of atoms. But that's nothing if you want to model reality. So what is that you need to do then? Well, you have to do this in a smart way, rather than giving the non-local potential at every time, at every minimization step. Minimization step, you can just give it once in a while or in a SCF type way. And we have done that. We achieve essentially the same results as the brutal minimization, but at a much lower cost. So in 30 hours of CPU, you can do 100,000 atoms. Okay, that's already much better. Still not quite where we would like to be, but much better. Okay, this is, I think, all I'm going to say about density functional development. That's the functional development, except to say that it is useful. Functionals that we developed actually can be applied to do something that is useful and predictive. One of them here is the surface energies of semiconductors. You can read all these numbers, the take-home messages, the maximum deviation with respect to Kuncham is 24%. And you can say 24% in a surface energy. Oh my God, is it bad? Well, actually, it's very, very good. 24% is. Very good. 24% is nothing for surface energy. So the trends are recovered, meaning low surface energy to high surface energies. Our functions do a good job there and deviations are not bad at all. Surface reconstruction, we did it for silicon 111 and it looks good. Here I'm highlighting the atoms where the geometry from orbital free, this right here is orbital free, the left is concham. You see some. The left is Khonsham. You see, some geometries have been somewhat symmetrized, but overall, I would say the results are good. This is one example. Will orbital 3 always work for surface reconstruction? Chances, no. But then you can say, will DFT in general always work for predicting surface reconstructions? And I think also there the answer is probably not, right? So there is a level of approximation here as well as in. Is a level of approximation here as well as in conventional DFT that you have to be cognizant of. Okay, here there are some muscle flexing applications which we can talk later, but here is probably one that it's the most interesting to me. If we use the one-teeter functional, so this is not a non-local functional with density-dependent kernel, it's not like the fanciest functional, but still a non-local functional. We can do a million atoms in A million atoms in 30 hours on a single CPU. This is a block of aluminum. Is it impressive? Probably not so impressive, but when you learn that this was done with a Python-only code, then this starts to become quite impressive. I believe Xue Sheng in the group of Yan Mingma has run calculations with 100 million atoms. Okay, so I think this segues nicely into this need. I think we were discussing it this morning also with Guido. How useful is a method if the method is not available? Methods have to be available, they have to be coded in efficient codes, and they have to be coded in a way that if Be coded in a way that if student X writes the code, student Y can understand that code. These are really important prerequisites to our science. So we decided in 2017 to make the difficult step to move away from Fortran and just simply code in Python everything that we do. It turns out Python is not that bad. Most of the machinery ends up being done in C anyways. Ends up being done in C anyways, so it's not too bad. In particular, these are all the software that we do, that we develop. We're not going to talk about all of them. Suffice to say that DFTPi is the one that handles Orbita-3 calculations, and we maintain it regularly. You want to do a million atoms with the Orbital-3? Go ahead. You can do it with the FTP. We have all sorts of no-local functionals there and no-local exchange correlation functions. And no local exchange correlation functions as well. No hybrid functions for obvious reasons. Okay, how do we do that? Do I look like someone who codes all the time? No, I don't code all the time. I end up coding maybe one month of the year, right? But I do so surrounded by people I can learn from. These are hackathons that we organize. And here, just to give an idea, who comes to these hackathons? Well, okay, my group, but also other groups. Group, but also other groups. This is a Hakadoni at Rutgers. Last year, we had six groups. This guy, I think I talked about the code INC yesterday with somebody. This is a GPU-only DFT code. Javier Andrade coded that. Javier Andrade also did most of the TD DFT implementation in Octopus. This guy here, I chopped his head off, but this guy, Volcker Bloom, main developer of FF. Main developer of FHI AIMS, he routinely comes to our hackathons. This is something that I think, as a community, we should pay more attention to. We can learn from each other in terms of coding, I'm sure, and we can actually solve problems if we come together and try to advance science together rather than competing with each other so much. So, here's an idea, just throwing it out there. Why don't we organize an orbital 3DFT hackathon where we just pick a problem in orbital 3DFT and try to In orbital 3DFT, and try to come all the way, I'm not saying to solving it, but at least to advancing a solution for it as much as possible. Here I'm throwing some ideas what we can do. We can attack this problem of pseudo-potentials. Why not? We can attack the problems of non-equilibrium or TDDFT. Why not? Just an idea. This could be the logo for that hackathon. We strangle a wave function with a We strangle a wave function with a snake. I should put a spider that strangles the wave function. Okay, I promised I would talk about... Am I already 20 minutes in my talk? It's a big problem. I promised I would talk about orbital-free TDDFT. This is a list of articles that have shaped my understanding of that field. And of course, I also throw in my stuff, but I think you guys. But I think you guys should be aware of a couple of articles that are really important. One of them is: if you are into applications, this article by Kalman Varga is really good. And it applies orbital-free TDDFT to solving an actual engineering problem. These articles by Neuerhauser and Alex White, really, really important for understanding the non-adiabatic. Non-adiabatic nature of the orbital-free functionals in TDDFT. And finally, this article by Fabio de La Sala, really important for understanding the connection between orbital-free TDDFT and plasmonics. And this is something I will discuss also in my talk, because really is interesting. Okay, so orbit of 3 TDDFT, how do we even think about it? Well, we start with this ground state, right? Everything starts with the ground state. Everything starts with the ground state. We have this equation which we saw before. Now we take the functional derivative, we put it to zero. And Sam already showed this equation here, which is the Euler equation of dft, orbital free dft, but written in a Schrodinger equation way. This phi b is a boson wave function that if you square it, you get the electron density. So what is this Pauli potential, which comes in a Polypotential, which comes in addition to the conchamp potential. Polypotential is just the difference between the kinetic energy, first derivative, and the von Weizacker potential. Now we see the clear maps that we are exploiting. The Conchamp potential maps from interacting system to non-interacting system. The Pauli potential maps from non-interacting system to non-interacting boson system. Now, you wonder. This is the boson Hamiltonian. This is the concham-Hamiltonian. And throughout, I'm going to call them Fermion-Consham, this one, and this one Boson-Consham. So the Boson-Consham, we can also consider virtual orbitals, yes? And for the Fermion-Consham, we can also consider virtual orbitals. We do this all the time. These virtual orbitals give us a glimpse onto the non-interacting wave function of that. Interacting wave function of that system. In this case, sorry, non-interacting, sorry, the chat lag, non-interacting response function. In this case, for the fermion-consham would be the fermion-consham non-interacting response function. For the boson one, would be the boson-consham non-interacting response function. So, the very first thing we can do is to take some soy systems and check the band gap, the band gap in this framework. Framework. So silver 19, we get this, sodium 55, we get this from the Fermi-Concham system, and these calculations are all done with the LDA exchange correlation function. The Boson-Consham, if you use Thomas-Fermi Pauli potential, you get this gap, which is generally bigger than the gap of the Fermi-Consham. And this is generally the case. We also did this with the OEP, meaning the exact Pauli potential, finding that also in the case of Finding that also in that case the gap is bigger. Of course, you can say, wait, what is the right gap? These are non-interacting concham gaps. They have no, can I say, immediate physical meaning. You can wonder about what the wave function looks like. Well, the concham wave function, the first root of the concham wave function, you would think is similar to the first root of the root of the um of the boson uh the first eigenfunction eigenfunction of the boson conchamp system but because the pauli potential is positive semi-definite it turns out that this first wave function of the boson is much more spread out than the first eigenfunction of the conchamp system as you would hope that would be the case okay non-interacting excitations what do they look What do they look like? Do we understand them? In concham, fermion concham, we have a manifold of occupied orbitals, a manifold of virtual orbitals, and we consider a number of occupied times number of virtual possible excitations. This is what we do. What about this boson-consham? Well, you have one occupied orbital and a manifold of virtuals, and you consider just a number of virtual excitations. So very different systems, yes? They have very different response functions. Very different response functions. Let's look at what kind of excitations we're looking at. Well, if you have an interacting system with the response function chi, if you start from a reference determinant, you can reach any number of excitations. Single excitation, double excitation, triple excitation, any. If you use the fermion-consham system, you are stuck or constrained to single excitations. This should not worry you. It's still formally exact. And we'll see why. In the Boson-Consham, different situation. Every time you do an excitation, you're actually taking all of the electrons from here to here. So you actually have collective excitations. Okay, so physically, they look very different. Okay, now we want to solve TDDS. Now we want to solve TDDFT, so we go to the Dyson equations. And I'm hoping you are broadly familiar with these Dyson equations. Essentially, for TDDFT, fermion TDDFT, you have this Dyson equation that connects the Counchamp response with the interacting response. And you have the exchange correlation kernel and the potential, Coulomb, Coulomb interaction. FXC is the XC kernel. For the boson system, you have an additional equation that connects the Additional equation that connects the boson non-interacting response to the Fermion non-interacting response. And there is a Pauli kernel that connects these two. They're both defined formally as functional derivatives. And they're both frequency dependent if you want this theory to be exact. Of course, we never consider frequency dependent, or seldom consider frequency-dependent exchange correlation kernels, right? Because you have seen before, exchange correlation kernels. Exchange correlation kernels are such that the manifold of single excitations for the Fermi-Concham system is then translated due to the frequency dependence of the kernel to representing all possible intervening excitations that will exist in the interacting system. And for the Pauli kernel, even more important to have a frequency dependence, because we are this FP, what it does, it recovers single Does it recover single excitations from this collective excitation? So it maps the collective excitations down to the single excitations. Fine, yes. Okay, so how do we go about approximating this Pauli kernel? Here, of course, I'm not going to talk about the exchange correlation kernel. That's a whole story that we live there. But for Pauli kernels, really, you invert this Dyson equation, and you just put Yes. And you just put Lindard functions there. I mean, let's put response functions that we can analytically write, right? There is a Lindard function for the boson concham, there is a Lindard function for the Fermion Consham. And then what we do, we focus on only the non-adiabatic part of the kernel, which is just the frequency dependence of the kernel. We do so in a way that we can actually code it. So we take the limit. Actually, code it so we take the limit as q goes to zero and omega goes to zero in the audience. Some of you may know that the Linder function has an order of limits problem. You cannot take the limit of the Linder function for both Q and omega going to zero because you'll get different results how you go to that limit. It's non-analytic in that point. So we choose these dimensionless variables to take that limit with this choice that I use. With this choice, the limit is unique. And we end up with this in second order in omega. It's a bit gibberish, right? A bunch of stuff here. What does this mean? Well, if you want this to become a density functional, you need to put Kf of R here. Q will be some sort of derivative of the density. And omega, what's omega? I don't know. So definitely, if you want to put this in a TDDFT code, you can definitely. A TDDFT code, you can definitely do it as with this, with some paying attention to how you do it. If you want to do this for a real-time TDDFT, then you have to convert this into a time-dependent potential, which will become current-dependent. And we have done that. What kind of physics does this give you? As I mentioned before, it recovers this collective to single-excitation coupling, which is what I think physicists. Coupling, which is what I think physicists call plasmon-to-electron hole pair coupling. Okay, and this I'm almost done. Let me give you some examples. So we coded this, we run it, it's all in DFTPy. MG50, where is it, this guy here? If you just use Thomas Fermi with your Pauli potential, just Thomas Fermi, you get this very bad dashed line. Very bad dashed line. The solid line is TDDFT reference, fermion TDDFT reference. If you improve your Pauli potential, adiabatic part of the Pauli potential, meaning the non-frequency dependent, non-explicitly frequency-dependent part, using LMGP, which is the non-local functional that we developed, you get much better peak positions. If you add this non-adiabaticity in the polypotential, Adiabaticity in the polypotential, you essentially recover the spectral envelope. The same story for gallium arsenide. And look at this gallium arsenide nanoparticle. This is a fairly big nanoparticle. You would need a supercomputer to do this using regular TDDFT. We just did it with one CPU with DFT pi. We get this spectrum. You have, you know, as usual, I'm showing energy on the X and oscillator. Showing energy on the X and oscillator strength on the Y. Get this spectrum. You see that the peak is well represented, the shoulder is well represented. We are missing this peak, this experiment. We are missing transitions between bands. The Pauli potential that we use is from the uniform electron gas. Uniform electron gas is not gallium arsenide. Okay, so we don't reproduce the bands of gallium arsenide, right? And that's fine. That's fine. But I want to conclude with an analysis. What are these peaks? Are these peaks? Are they all plasmons? What are they? So, here for MG8, a simple system, I just take two peaks here and run CASIDA TDDFT and check the transition density using orbital free TDDFD. This is what this first peak looks like. This looks like a plasmon to me. This is what the other peak looks like. Looks like a plasmon to me. And these are the boson-cons-champ excitation contributions. Excitation contributions to them. You see, you only have about three. Three excitations will give you this. If you had to do a plasmon using regular TDDFT, you get hundreds of contributions, maybe even thousands of contributions. So this is a very convenient way to represent plasmons. Same story with silver 20, is a little tetrahedron of silver 20. I'm going to go faster because I'm running out of time, so I apologize for that. The natural question is: What is a plasmon? Is a plasmon just a collective excitation? Are all collective excitations plasmons? But there is this nice paper by Christoph Jakob where plasmons are defined as those excitations that are strongly dependent on the electron-electron interaction. So as you modulate the electron-electron interaction, plasmons will change energy. Will change energy while valence excitations will stay flat. This is lambda, and this is the excitation energy of that particular state. Now, this is interesting because now I can check whether what I get from the boson TDDFT, from the orbital TTDFT, whether they're all plasmons. I would hope that they would be all plasmons, given what I think of plasma, think of plasmons as collective excitations. Excitations. So we look for the shift. What you see here, different lambdas, different the same Mg8 spectrum we saw before. Forget this part here, it's just noise. Here, yes, we do see a shift. So this definitely is a plasmon. Most of them are plasmons, they're shifting. What is it? But there are some that don't shift much at all. Don't shift much at all. And then the natural question is: are there collective excitations that are not plasmons? Maybe it's too provocative of a question, maybe it's a stupid question. Now, this result is done without this non-adiabatic polykernel. Should we expect the picture to change when we use a non-adiabatic polykernel, when you include this coupling with the single. Coupling with the single particle excitation. So, I think, yeah, should stay tuned and thank you for this. And if you'd like to try the FTPy, there is a Google collaboratory for DFTPy. I understand we would have to all fly out of China to use this, but maybe not. So, anyway, thank you.