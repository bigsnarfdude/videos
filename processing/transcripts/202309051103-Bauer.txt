Started and so was not able to travel. Actually, our classes didn't start yet because we had a hurricane, but they were supposed to start. So, so far, the internet works very well today, and I hope this will continue. But the last days we had some troubles with internet outages because of the hurricane. So, if I'm gone suddenly, I very much apologize. So, this talk today, I wanted to, I plan to talk about curves and shape graphs, but after planning out the talk, But after planning out the talk, I realized that I might have light a little bit in the title and that I will focus mainly on curves, although I hope that I will be able to talk a tiny bit about shape graphs in the end. And similar as the last talk, this is again mostly work together with Nicholas, who will give the next talk. The idea of this talk is it can be somehow seen as maybe as somewhat as a part one of the talk that Nicholas will give. Of the talk that Nicholas will give afterwards, where he will deal with much more interesting but also much more complicated data, and I will deal with the simpler case of curves here. So Laurent in his talk already talked a little bit about this outer and inner or intrinsic metrics, but I wanted to use at least like two minutes in the beginning to kind of visualize the differences again from what we have seen in the last. Again, from what we have seen in the last talk and what we will see in this talk here. So, if we want to put a Riemannian metric on the space of curves or surfaces, there are kind of two, I mean, there are several other ones, but there is two type of models that I want or that I put against each other on this slide. And the first one is outer metrics or deformation-based metrics, which correspond to the image on the To the image on the left on the bottom of the slide. Actually, this is taken from Darcy Thompson's book, which we already mentioned, was also mentioned in Laurel's talk before, which is to me really one of the kind of founding fathers of many of the ideas that we use here still. And so, what you see here is kind of a curve, like think of the outline of these shapes as curves, and a curve is deformed into each other where the whole ambient. Where the whole ambient space is deformed, like the whole ambient space in which the curve sits is deformed. And what one was doing is one puts kind of a Riemannian metric on the space of deformations of the ambient space and uses this to induce a metric on the shape itself, which corresponds to these so-called outer metrics. And the approach which I kind of will follow in this talk, and also which will be that kind of approach that Nicholas will talk, take in his talk in. Nicholas will talk, take in his talk in his next talk, is that instead of kind of deforming the whole amian space in which the object lies, we just deform the object itself and kind of it moves without deforming the amian space, which corresponds to the picture on the right-hand side. And this is often called intrinsic metrics or also elastic metrics on spaces of curves and surfaces. Sorry, as there is some relation. Um, as there is some relation to linear elasticity theory, and I believe Nicholas will also mention this in more detail in the next talk. So, kind of as an overview of the talk, what elastic metrics, they really provide kind of a nice and effective approach to endow spaces of curves, shape graphs, and surfaces with Riemannian metrics. And kind of the idea of these metrics will be that we will define an invariant metric. Define an invariant metric on the space of parametrizations and then quotient out this reparametrization action, which will require us to solve a registration or matching problem, which is kind of the most challenging part of the program that I will show you. Okay, so I want to go now a little bit into mathematical details, but I try most of the talk to rather visualize the concepts instead of going in any technical. Concepts instead of going in any technicalities. However, it still requires me to introduce some notion in the beginning. So the main space that we will deal with in this talk, at least in the first part of the talk, is the space of curves. I'm sorry to interrupt, but I think the dirty spa on Zoom that is hiding your screen because you're sharing. Sorry, this thing? Now we can see. Is it better? Sorry, I thought that Zoom usually puts this away immediately. Okay, thank you. So, the space that I want to deal with is the space of curves. And how do we model a curve or how do I model a curve just as a smooth mapping from the circle for closed curves, or if it would be open curves from an interval into Rd. And they require a small condition on the curve, namely that it's regularly parametrized. Regularly parametrized. This is not really too much restriction on the shape that I can represent, but it's just a restriction on the parametrization. We want to view this space as an infinite dimensional manifold. And in fact, it's kind of the simplest example of an infinite dimensional manifold because it's just an open subset of a vector space. Okay, I know that at least to me, it was kind of difficult when I first heard about infinite-dimensional manifold, and it was kind of a difficult time. Infinite dimensional manifold, and it was kind of a difficult concept for me to visualize because the concept of a manifold that I had in my mind was always kind of a sphere or something, and now it's suddenly talk about infinite dimensions. So what are kind of the two main things that we need to understand if we want to work with Riemannian geometry on a manifold? You kind of want to know what is a path in my manifold and what is a tangent vector. And what is a path in my manifold? I'm now on the infinite dimensional space. Now, on the infinite-dimensional space of curves, so a path in my manifold is just a path of curves. So, at each time point, it is a curve itself. And what we saw here is we saw a path that connects the circle to this skull-shaped figure that was again taken from Darcy Thompson's book. And what is a tangent vector? A tangent vector is formally a derivative of a path, or in other words, it's a deformation vector field that sits on my curve. That sits on my curve. So, on each point of the curve, it is just a deformation direction and the deformation strength. If I want to do Riemannian geometry, I want to define a Riemannian metric as a first thing. So what I want to do is, given two tangent vectors, I want to assign a real number to it. This is essentially what a Riemannian metric is. It's an inner product on each tangent space, and the tangent vector is just such a different. And the tangent vector is just such a deformation field. Why is this a convenient concept? We have already heard about this a little bit in the last talk: namely, once I have a Riemannian metric, I get a distance on the space, the so-called induced geodesic distance, which allows me to define a similarity measure between different points in my space, so between different curves. Okay, so before we define every Manny metric, we will start. Riemann metric, we will think a little bit of this: is the right notion for a shape? Namely, is the notion of a curve really the right notion for a shape? And I mean, I would not ask this question if the answer would be yes. The problem is kind of visualized in the following image here. So we see the circle and we see two different parametrizations of the circle into Euclidean space. And the color coding here should visualize the parametrization. So the violet area of the circle. So, the violet area of the circle is mapped to the violet area of the skull, and so on. And if we want to use these curves to describe, for example, the outlines of objects, then we would like to identify these two curves with each other. However, as immersions, they are completely different objects. So if I just put some metric on the space of immersions, the geodesic distance between these two skulls will be different. So this requires us to quotient out. us to quotient out this group action. And Laurent already talked a little bit also about this in his talk. So if you think about this talk, we had kind of a two step quotienting, we only have a one step quotienting because we start directly on the space of parametrized curves and then we have just to get rid of what is sometimes called shape preserving actions, group actions. Okay, so how can we do this in remote? So, how can we do this in Riemannian geometry? So, I think this picture kind of really explains the whole setup quite nicely. So, what I like to think of is the shape space is this quotient space below, where we somehow forgot about the color coding of our curves. So, the color coding visualizes the parametrization. And so, we have on the space of immersed curves, we have kind of two directions. One direction is the fiber direction, which kind of goes along the Which kind of goes along the fiber, where we do not change the shape of our object, but we only change the parametrization of our object. And then we have a second direction, and the second direction is kind of the shape change direction. So when we want to solve matching problems on shape spaces, we will not match points in the space of emergence, but we will match fibers later. So in Riemannian geometry, if we have a metric on this top space, what is kind of the assumption? Space, what is kind of the assumption that we need of this metric to descend to a metric on a quotient space? It is a relative kind of natural and simple assumption. We only need the metric to be invariant under the group action that we want to factor out. In our case, this is kind of the reparametization group. So if we reparametrize our base point curve and our two tangent vector by the same reparametrize, we want that the value of our Riemannian metric does not change. And if this is the case, And if this is the case, then it will induce both a Riemannian metric and thus also a geodesic distance on this quotient space. Okay, so let's try to define a very simple metric. And this is a very simple setup. We have seen essentially that a tangent vector is just a function with values in Rd. So given two tangent vectors, at each point we can just take the inner product in Rd. This defines me a function on a circle. Defines me a function on a circle, and now it seems kind of natural to just integrate this function on the circle up, which is just the standard L2 metric on the tension space. This, however, doesn't work out or doesn't work out for our purposes because what we obtain is not invariant. How can we see that it's not invariant? I mean, if we try to use every parameterization, then just standard calculus says, U substitution formula says that this doesn't work out. Model says that this doesn't work out. Okay, so the next kind of guess or try that we could do is we just instead use an invariant version. So instead of just integrating with d theta, we integrate with respect to arc length. So we integrate with respect to the norm of C prime C theta. And now a simple substitution rule shows you that this obtained Riemann metric here is really invariant under my reparometrization action. Um, reprimetrization action. Unfortunately, although this seems very natural and seems as a good basis for shape analysis, this also doesn't work out. And that's by now kind of a classical result that has been first shown by Michael and Mamford in 2005 by now. They showed that the geodesic distance of this Riemannian metric is degenerate. So, this is a completely nice and well-defined Riemannian metric. And in finite dimensions, if you have a Riemannian metric, this Dimensions, if you have a Riemannian metric, this always really defines you a true distance. However, in infinite dimensions, it turns out that the induced geodesic distance can be very bad behaved, namely it can be completely degenerate. What do I mean with this? It has been shown that for this invariant L2 metric, the geodesic distance between any two curves is zero. This obviously renders it as relatively useless for shape analysis, as it says all points are the same. It says all points are the same, and I cannot distinguish between anything. Now I said that this is a purely infinite-dimensional phenomenon. And you might ask, in practice, I will discretize anyhow. So I'm on a finite dimensional space. Will this really matter? And the answer is, yes, it really matters. And kind of the answer is a little bit also the proof for this result. I don't really want to show you the details of the proof because it's rather technical, but kind of the idea of the proof is. Of the idea of the proof is so, if you would try to minimize the geodesic distance between for this L2 metric and you discretize your curve by just n points, and so what I try to minimize, so what I will show you is the minimizer that I obtain if I try to translate a straight line. And I try to minimize this with endpoints. And what happens is I grow this teeth out until they reach the top, and then these teeth get retracted. And it turns out as you increase. Turns out, as you increase your discretization, it will be more and more cheap, and the geodesic distance becomes smaller and smaller and smaller. Actually, because of this, this is actually how the proof was found. So when David and Peter tried to work with this metric, they just never thought that this could be degenerate because this kind of was very counterintuitive. And they just tried to find some numerical examples. And this is what they figured out. And because of this, they started investigations of what could. Investigations of what could be the reason for this ill behavior. Okay. So if this is not a good metric, what can we do? Kind of a standard approach to strengthen my metric is to include derivatives into the definition of the metric, which leads to the so-called class of Sobolev type metrics. And to make it invariant, we just don't use standard derivatives, but we use arc length derivatives. Use arc length derivatives. And in fact, if you define this inner product by this, then you get again an invariant Riemannian metric. And furthermore, you really get an invariant metric that has a well-defined geodesic distance. What do I mean with well-defined geodesic distance? You can show as long as you include at least one derivative, the geodesic distance is non-degenerate and a true distance. Okay, so this means at least. So, this means at least it should be somewhat a sensible basis for doing shape analysis. And in fact, and I will not be able to talk about this today, kind of since then, I would say in the last 10, maybe even 15 years, we have really put some effort in really investigating like the theoretical properties of these metrics in a lot of details. And it turns out, especially if you have an H2 metric or stronger on the space of curves, it has kind of old. The space of curves, it has kind of all the nice properties that you kind of could wish for. It's like a geodesically complete metric, you will get minimizers between any two curves. And at least from a theoretical point of view, everything is very well behaved. Okay, so this seems like it should be a good basis for doing shape analysis. And so what we want to do is we want to minimize the geodetic. We want to minimize the geodesic of calculate the geodesic distance in practice for two given curves. So, what does this require us? So, if you have two curves C0 and C1, as I said, we do not want to match the curves, but we want to match the fibers. So, this requires us to minimize over all paths, and we can fix the point in the first fiber due to the invariance, but we have to minimize over the whole fiber at the target. So instead of having C of one. So instead of having C of 1 being a fixed curve C1, we have to assume that C of 1 is somewhere in the fiber over my curve C1. Kind of obviously, there are no explicit solutions available, and so we really need for numerical solutions. And a standard approach is just to discretize everything and try to use standard optimization techniques. I think I have five more minutes, hopefully. Yes. Okay. Hopefully? Yes. Okay. So there are approaches. We represent curves and reprimatizations in some discretizations. It could be either beast lines or piecewise linear curves. And then we formulate the geodesic boundary value problem as an unconstrained high-dimensional minimization problem. So what is the main difficulty here? Kind of like discretizing the curve is relatively easy, but discretizing the reprimatization group and in particular the action of the reprimatization group becomes a bit more complicated, although this has been. A bit more complicated, although this has been done. But in addition, solving this exact boundary value problem can also become quite problematic, especially in the context of data with noise. So what we usually do instead is we consider a relaxed boundary value problem. For this, and this will play a kind of a rather big role in the next talk. So for this, we want to define some similarity measures. To define some similarity measure that can be rather coarse, but that at least tells us if we hit the right orbit. So, we would like to have something that is completely blind to reprimatization and that just tells you if we hit the right orbit. So, if C1 only differs by my curve from us, like my endpoint only differs by my curve by every parametrization. If we have this, this allows us to. Have this. This allows us to reformulate the endpoint constraint using this prevalence and reparametrization plan similarity measure, which so far doesn't help us much yet, because this is still very much a constraint optimization problem. But in a second step, we can just relax it and say instead of solving the exact problem, we just have an initial constraint. We want to start at C0 and we just add this endpoint constraint to our energy function. Endpoint constraint to our energy functional using a Lagrange multiplier. And this can now be kind of optimized really straightforward. And what I want to show you in maybe in the last two to three minutes are at least some results. So the very first one is a very simple example where I just want to show the difference between a parametrized matching and an unparametrized matching. This is, I guess, again, in a way, so we move it up here. It up here. And so you see on the left, we did a parametrized matching where we did not minimize over the point correspondences, and there is kind of some mismatch in where the curves go to. Whereas on the example on the right, we have already optimized over the point correspondences and get kind of a better matchup. So, my next example, probably almost the last example, I just want to show you. Last example, I just want to show you how this can be used to do some very simple classification. And this is again a toy example and not in any way a real example, but we used 54 shapes from the Surrey Fish database, and they were from kind of different classes. And they calculated the pointwise, the point-wise, the pairwise distance matrix between all of these points, and then did some automatic MDS-based clustering. And what we get. And what we get is kind of reasonable clusters. So, actually, if you would compare this to the real ground roof, because these are fishes from different species, I think there was one mismatch between these 54 points. And maybe one more example that I wanted to show you. This is a second example of clustering. And what we see here. What we see here is a comparison of the exact approach and a little bit also the problems of the exec approach that I mentioned as in comparison to this relaxed approach. Namely, what we did is we did, this was from the Chemia database and we chose, I believe, 20 or maybe, no, maybe 40 shapes from the Chemia database. And we added some noise towards it. And if we solved the exact matching problem, although it was in general still okay, there was This was in general still okay. There was quite a bit of mismatch because this noise really dominated a lot in the geodesic distance. However, if we used the relaxed approach, kind of the similarity measure that we used had quite a bit of noise canceling performance in some way. And so the clustering that we obtained was much better. Okay, this is really the very last example that I want to show you, which comes very much back to the beginning of. Very much back to the beginning of this talk where we talked about this inner and outer metrics. And this is again an example, a comparison between a matching of two curves with both of these approaches. So in the second index-trinsing, we use exactly an LDMM-based model that LaRoa explains in the first talk. And in the first approach, we use our intrinsic metric. And this really explains in some way the fundamental difference between these two methods. So, what we see here is we see to pump, pump. See, here is we see two pumps that move that change position. And in the intrinsic model, it really uses essentially only local information of the curve. And so the one bump doesn't see the other bump. And so it can just move past each other. And it doesn't even is not disturbed if they overlap or anything. I think they overlap a little bit if you if you look at the second to third picture. However, in the extrinsic model, where you assume that the whole surrounding space That the whole surrounding space kind of gets deformed with each other, you cannot just move these bumps past it. Instead, kind of the bump gets crushed down so that there is enough space for them to move. What do I want to say with this? Essentially, I want to say I don't think that one of these two models is superior to the other model. It really depends on what type of application you have in your mind, what type of model you want to use. Want to use and yes, I was, of course, way too slow, but since it is lunchtime, I will stop here and just say all of the things that I talked here about are open source available. I talked mostly about closed and open, mostly about closed curves, which is an open source code that is available on GitHub. Unfortunately, I did not get to shape graphs. Did not get to shape graphs, but they would be also open source available, and I would be happy if someone would be interested to use them. Yeah, thanks a lot for your time. Thank you very much, Martin for the back. Questions? Yes. Hi, next talk. I was wondering about the in terms of practicality. In terms of practicality, if you try to do it because the transport you have to do is pairwise, how does it scale? So if I have a million shape, one million shapes or two million shapes, very easy to extract from time-lapse. Sorry, I didn't understand you perfectly. What's the question about speed? Yeah, I was wondering for your shape graph matching, so you showed it on relative sport data sets. Like now they have either one million shapes from time to time to data set. I would say there's certainly, I mean, it is not for free. I should say this. It's certainly not for free. It's really solving optimization problems. So doing pairwise distance between 1 million shapes, I would not necessarily want to do this. But this doesn't mean that there is no chance. I mean, for shape graphs, we didn't do this, but at least for curves, also together with Nicholas and one of my graduate students, Emmanuel Hartmann. Students, Emmanuel Hartmann, we experiment with using neural networks to learn the shape distances. And as long as you have at least some homogeneity in your data set, I mean, if there's completely different shapes, at some point it gets to a, I mean, gets difficult, certainly, but if you have some homogeneity in your data sets, learning these distances worked out surprisingly well. We didn't follow up on this. We didn't follow up on this much, but I mean, my answer would be: if it's really necessary, we probably could do one could probably do something here. Yes, certainly curves are much easier and much more able to do than what we will see in the next talk. Surfaces, I think if you have one million surfaces, you are in troubles. But curves, I think you can do quite a bit. Thanks for the very nice one. Could you comment on it? comment on dynamic manifolds, time-dependent manifolds. Time-dependent manifolds? And so you mean you have like is the like you assume that your manifold is of a fixed dimension or do you also want to change the dimension of the manifold and the topology of the manifold? Oh, that's much more complicated. I'm thinking of the manifold that's evolving continuously in time. Then, most of the things that I talked about today really generalize to this setup. This was actually kind of my PhD thesis. The theoretical results that I did not discuss, a lot of them really work only for curves, and we are working hard on getting this in a general case, but we fail so far. But just the whole setup, and at least some of the theoretical results work out exactly in this setup and really. Work out exactly in this setup and really independent whether it's a curve or a higher-dimensional manifold. Questions? Yeah, could you also apply that to non-closed curves? And would it change something specific? So there is some funny, like there is some funny thing that changes. Is some funny thing that changes for non-closed curves, which concerns the completeness of the space with this Riemannian metrics. There is still one thing that you can always leave, but in general, everything that works out for closed, besides this, everything that works out for closed curves works also for open curves. Also, numerically, there's really no difference. Thank you. Hope to see you soon. Have a good lunch break. Bye.