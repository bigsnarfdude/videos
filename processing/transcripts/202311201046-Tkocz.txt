It's a wonderful workshop in such a lovely place for having me here. I'm gonna talk about my joint work with Alexandros Escanasis and Philip Mayar. Sorry about the funky title. I hope it will become clear soon. So throughout this talk, oh, and my apologies to Lad and probably several other people here. There will be no comments. There will be no convex bodies in this story. My sincerest apologies, but the story is inspired by some convex bodies. So just bear with me. Epsilon 1, epsilon 2, da da da will be a sequence of IID identically distributed independent random signs. So random variables which are just uniform on the two point z minus one one. Point set minus one point and C1, C2, da da da will be IID random vectors uniform on two-dimensional unit nuclear sphere in R3. Okay, so these guys are real values, random variables, these are random vectors in R3. Alright, so the start. Alright, so the story begins with two results, which I'll happily recall here. First is Charac's inequality from 76, which says that for every n, at least one natural number, and for every vector a think about there's a choice of scalars, a1 up to a n, whose squares add up. n whose squares add up to 1. I'll just write it in this way. We have the following inequality. The L1 norm of the weighted sum of random signs is never smaller than what you get from two non-zero scalars of equal value. That's the first result. And the second result is the already mentioned Is the already mentioned Ball's cube slicing in equality, which I'm gonna write in a way that will resemble this. And I'll say that is really the cube slicing result in a second. So here we take the weighted sum of these guys on this. weighted sum of these guys on these spheres. These this is a random vector in R three. We take its Euclidean norm, we take its reciprocal, so this is negative first power, we take the expectation and this is never bigger than what you get from the same extramizing vector, two non-zero coefficients of equal value. Okay? And the reason why well, reason. Well, reason. Some geometry is hidden here. This quantity, this probabilistic quantity, is nothing but the volume of the central section of the unit volume Q. Okay? This is just equal to that. I think this beautiful probabilistic formula for the sections can be traced back to the paper of Koldovsky and Kenig. Alright. Um all right, so we've got these uh these results. Um they follow the theme of uh the so-called sharp kinchin type inequalities. So let me just remark that these are examples of sharp kinchin type inequalities which are broadly speaking just in a quote. Broadly speaking, just inequalities about comparison of L P L Q norms of some random variables or classes of random variables. I'll just write it broadly like that. We look into the comparison of L P L Q norms on the probability space up to some multiplicative constants. So what we've got here is the L1, L2 case. The L1, L2 case. The L2 norm here is hidden in the constraint we put on this choice of scalars. We ask the squares to adapt one, which amounts to saying that the L2 norm of the sum is one. If you do this very pleasant calculation, L2 norm of this sum is nothing but the sum of the squares of the closure. Of the coefficients, then you get 1. So here's L1 norm. This is just a constant which you can find: 1 over root 2, and times 1, which is like L2. So let me give a name to this inequality. Let's call this S, let's call this P. So S is saying that the L1 norm is at least 1 over root 2. is at least one over root two, the L2 norm. And similarly, B can be interpreted as follows. The L minus first norm, so if we just take the reciprocal of this inequality, then it becomes a lower bound on the L minus one norm of this sum. And actually, we get the same constant. This turns out to be root 2 if you do the calculation. If you do the calculation or remember cube slicing, reciprocal goes to one over two, and the same constraint, I didn't write the assumptions, the assumptions here are the same. The constraint tells you that the L to norm of the sum is also one. It works exactly the same. And to motivate the story, so Like to motivate the story, sort of I the point I would like to make here is that this type of inequalities, since we care about sharp constants, they are known only in a handful of other distributions. It's not that we know this for a very broad class, we know it for some very special situations. Let me just mention without recalling precise references in the interest of time. So, such inequalities Such inequalities known only for a handful of other distributions. So for instance, we can go into higher dimensions and handle vectors uniformly distributed on spheres. This is actually intimately related to Intimately related to this uniform distribution on the ball. There is this very nice rather, well, there's this class of type L random variables which come from statistical mechanics. We know these inequalities for Gaussian mixtures and we know them also for marginals of uniform distributions on L people. Uniform distributions on L defaults. Okay. And these are essentially. So the point I'm trying to make here is that for distributions which are nice, have some good structure like rotational invariance or some other things, we can do things. And today I would like to present, I think, a first present, I think, a first result which goes in a slightly different direction. I would like to say that actually these inequalities continue to hold for all distributions, provided that the distribution is sufficiently close to the distribution of a random sign or random vector uniform on the sign. Okay? So So just loosely speaking, our main result, and then I'll be more precise. Our main result is that S B false for and that's small talent, so let me Alright. When the particles mess here, I don't need this. When these scalar coefficients are not too large, L infinity norms, so they are all bounded by 1 over root 2. Our main result is under this scenario, these inequalities continue. Scenario: These inequalities continue to hold for all distributions sufficiently close. Now, make this precise answer. All distributions sufficiently close to in the Shar case epsilon, both case. So, that's just so that I don't know, if you want to remember one thing from If you want to remember one thing from this talk without any details, the point I'm trying to make here is we are hardwired to these inequalities. If you wiggle the distribution a little bit, the inequality continues to hold. And now for the details, I'm just going to focus on the easier case here of random signs. I'll just write a precise result. Result for this case, and just mention briefly maybe some because there are further technicalities going on with handling those spherically symmetric random vectors. So let's just stick with random signs. This is fairly easier to write. So the theorem is that if I've got a sequence of IID symmetric random variables, yeah, Variables, yeah, symmetry means that minus your random variable has the same distribution as your random variable. If I have IID symmetric random variable such that and here for concreteness we use a Wassenstein two distance. So if Wassenstein II distance of the distribution of x1 to epsilon 1 is not too big, we get some explicit concepts. We get some explicit constant 10 to the minus 4, then ShareX inequality still holds. Oh, now I should write xj. So we have the same inequality for every okay, and I'm going to mention this assumption here. So note that what So note that when the dimension is 2, there's basically optosymmetry is just one unit vector satisfying that. So for n equals 2, that assumption is... the inequality holds trivially, it's equality. So it gets interesting only for n at least 3, so I'm going to write it in this way. So for every n and for every a such that We have this boundaries inequality for all distributions sufficiently close to Rademakers. Okay? And here I shouldn't really be so annoying. This is something super concrete and super explicit, of course, for such a simple distribution as a random sign. This is nothing but the L2 norm of absolute value. Of absolute value of x1 and distance, to measure the distance between absolute value of x1 and constant random variable. Something very cool. Alright. And we have an analogous result, similarly hide-wired. So let me just write it here without the details. There's one more extra technical assumption you have to make. I'll try to get back to that at the end. One extra technical assumption we make to work it out. Maybe it's not needed, comes from our Comes from our proof method, which I would like to discuss. All right, so some proof of ideas. Okay. So the key, the starting point here is The starting point here is Hageruk's approach to classical kinchin inequalities, so to these sharp LP LQ bounds for random signs. We use Fourier analytic formula Agaro's Fourier analytic approach. Let me click approach So what is this about? I'll just write well treat it as a black box it's not too difficult to derive this but in the interest of time I'll just write the end result. So if you've got a symmetric random variable here which is a sum of independent random variables like we've got here one lower bound you can put on One uh lower bound you can put on the L one norm is the following. You can sort of d separate those aj's in the following way. And you take the sum of aj squared times cx of aj to the minus two, where this function cx comes from this Fourier analytic formula. The Spourier analytic formulae, this is a special function which is defined as follows. You take 1 over pi integral on the real line 1 minus phi of t over root s absolute value 2 ds. This is integrated against the weight t to the minus 2 dt and phi x is the Fourier transform of your distribution. module distribution. Px of t probabilistically is just the characteristic function. Okay, so Hagarov came up with this beautiful lower bound. It gives a very actually slick a nice proof of Chareck's inequality. So now let's talk about it. So the first thing I want to get a The first thing I want to get across, these proofs, actually, okay, another thing you should take from this talk is that this result and the proofs, this is, as one of my co-authors who shall remain nameless here likes to say, they are obvious. This is all super easy. I'll try to convince you that this is easy. Okay, so. Okay, so if you have this lower bound and you want to end up with this at the end, let's note that. So the crucial remark here is to note that if this function at s is lower bounded by its value at 2 for every s at least 2, then we are done. We are done. Why? We are done. Why? If I've got this, under the assumption that the L infinity norm is bounded away from 1 by 1 over root 2, what does it mean? It means for this guy to be at least 2. This is at least 2 if and only if a j absolute value is at most 1 over 2. So my input here is a number. Input here is a number who is at least 2. If I have this lower bound, I can just lower bound term wise, these guys by psi of 2. So I get from Hagerup's inequality, I'm going to get the sum hx squared, psi x of 2. This is a constant. These guys add up to 1. So I end up with psi x of 2. And if you backtrack. And if you backtrack how this function comes up, actually Hagarup's estimate is tight in this case. This value of the special function recovers that L1 norm. Because some square shows up and things like that, I skip those details. You get the right thing at the end. Just from such a property of. Just from such a property of your special function. Of course, the crux here is that how do you deal with such a function? So let's talk about it now. When we have random signs, okay, when we talk about the random sign case, the characteristic function is just the cosine. Okay, very easy Fourier transform of the random sign is the cosine function. And then your special function, actually Heigero managed to very cleverly evaluate. Manage to very cleverly evaluate an explicit formula for this. This integral is doable by using that cosine perioding and things like that. This turns out to become this very explicit function. You can write down formula using the gamma function, you get this. The point is that you can now have an explicit formula, you can analyze carefully. You can analyze carefully your function. Here we just care about this property, but we can actually observe much more. So, this is for the random side, sorry. This function turns out to be strictly increasing and it's concave. Beautiful function. Looks like that. This is Cs of T. So, of course, it enjoys this property. Not only that, it enjoys Not only that, it enjoys this property for a very strong reason, namely that the function is monotone. And that's basically the end of the argument here. Because now what you can try to understand is that, well, if the distribution of x is close to distribution of a random sign, maybe the special function is close to this function, because here I have a very good reason why the property volt, I should be alright. I should be alright. So that's the idea. That's all there is to that result. So now, when I'll skip the details, of course you have to do some careful estimates here, but when the law of x is approximately the law of random sign, it's believable, I hope, that the characteristic functions Will be close to each other. Then, as a result, these special functions should be close to each other in some sense. You can put some pointwise bounds on these things. And not only that, you can also be tempted to look at the derivatives. Why not? Here the derivative, we know this function is strictly increasing, it'll be strictly positive, so we'll have some wiggle room. So we'll have some wiggle room to play with. This is believable. This is believable. So then the picture you're going to get is: all right, I have this property for my C epsilon. Here is 2. Here is C epsilon 2. Okay? And now what will I know about my function? I'll know that maybe I won't know that it's monotone, like here. Maybe that's too much to wish for. But I'll know they will. Much to wish for, but I know they will be close in some range, and close to 2 is not enough to know that they are close because, you know, it may drop below this orange. But I know that the derivatives are close as well. So this this uh I'm so I'm trying to plot in purple C x will be monotone close to two, which will give me the statement as well. Will give me the statement as well. So here I'll go for the claim that the function is monotone close to 2, and then when my estimates on the derivatives aren't good enough for large values of s, I'll go for just the claim that the functions are closed. I already have some gaps. And that's basically the end of the argument. Of course, now you have to make all these things precise, but this is some technical work. Technical work. Alright, do I still have some time left? How am I doing? Fine? Oh, doing well. Alright. So maybe let me mention briefly now these symmetric random vectors. It just gets slightly more complicated because we don't have such a good access to the special function. Okay? So all of this can be repeated. All of this can be repeated. In the spherically symmetric case, you work with random vectors. So, in principle, you have to write here integral over R3 from your Fourier analytic formula. But for x, you have rotational symmetry. Actually, this integral can be reduced to integral on the real line. Okay, for the special function, as in Ball's proof, I mean, you don't have to talk about these C's. They are intimately related to uniform distribution on the interval, of course, hence the q. So you come up with such an integral formula for your special function using this Fourier formulae. That function has been of course well understood to a certain extent. You don't have an explicit formula, but Bohr's function. So now I'm going to plot sigc. It looks Okay? Uh it looks like that. Uh well this is this is what you can get uh let's say from numerical simulations about this function. The point is that it's not just beautifully increasing all the way. It increases then it decreases to some asymptotic value. But fortunately this asymptotic value, here let's call it C, that's a limit at infinity, let's call it At infinity, let's call it sig C at infinity. The value at 2, let's say 2 is here, is separated from this important one. This is sig C of 2. And now your special function from the perturbed random vector around the spherically symmetric one comes in. And initially, you still followed this strategy, but you also made up some problems and improved. You also may have some problems at infinity because these estimates get somewhat weaker. So, there's one extra assumption needed that allows us to control the decay of this function, so that we can argue that the limit at infinity is very close to the Poitet function relative to the one for which we know everything is good, right? Here we know okay. All right, maybe. So maybe instead of talking more rubbish, I'll just stop here. Thank you so much. Thank you very much. Questions? Of course, I will use this approach to detail in the approach. So, how should I put it? I'll just be frank. I guess we were lazy. It just becomes so much more technical because special function for LP norm is not so explicit and easy to work with. And actually, even the proof that this property holds for this corresponding special function in the LP case is very different. Very difficult. But you know that. He makes difference between P less than two and bigger than two. Yes. Ah, ah, for P less than two. Yes. For P less than two, it is easy. One should roll up the sleeves and get to work, I guess. We haven't done this. Of course, that's yes, it's a great comment and suggestion. I was wondering if your theorem could be made like a little bit more explicit if you talk about maybe the density of your X1 and the H-inverse norm of the density. Somehow your condition on the density was in terms of this W2 distance to the data. Yeah, I mean I guess it depends what quantities you like to work with. But that would give you a concrete example. I don't know if you can give concrete examples. That's what I'm saying. Give a concrete example. Like here is a With example, like here is a distribution for which you know that this works. So so any distribution was, you know, I have to know I think it gives a rather large family of distributions by this condition. Square is 13 squares of them to the minus. That's just our condition. And for spherically symmetric random vectors, you have a similar thing, plus one extra. Like we need a file moment. You see, here we don't even need to assume that any integrability beyond L2. To push through this asymptotic behavior, we do some asymptotic Gaussian approximation. We need a higher moment. It's just some extra technical assumption. I do press the sync button.