We'll be speaking about the long-time asymptotics of the parabolic-Anderson model in a hyperbolic space. Thank you very much. Thank you for the invitation. It's a great pleasure to visit this very nice place. So my talk today is about understanding the long-time behavior of the parabolic Anderson model in the hyperbolic state, the standard. In the hyperbolic space, the standard hyperbolic space. So, this is based on joint work with Wei Jing Shui from Peking University, and also I will also talk about some very recent progress with Wei Jing and my PhD student Shikwan on the almost sure of importance. Okay, so the basic object of study. Object of study in my talk would be the so-called parabolic Anderson model. So this is essentially a simple heat equation, but the underlying generator is Laplacian plus a random field, a random potential. And here we assume that Cass C is a mean zero stationary Gaussian field. Gaussian field. Gaussian field, stationary means that its distribution is invariant under translation. And we assume that the sample functions of the Gaussian field is sufficiently regular. So my talk is not related to existence, uniqueness, or regularity issues. So we do not consider singular fields. So we assume that the field is sufficiently nice. Field is sufficiently nice, you can assume that the covariance function is sufficiently smooth. So the equation is solved in the classical sense for every fixed realization of the Gaussian view. You just solve this equation in the classical sense. I mean, it is very standard that the solution has a Feynman tech representation, which is given by the expectation of the The expectation of the exponential of the integral of the Gaussian field composed with the Brownian motion starting at x, where the Brownian motion is independent from the Gaussian field and the expectation is taken with respect to the Brownian motion. So, the basic questions that is of interest in the talk is about In the talk, it's about the long-time behavior of the solution as t goes to infinity. So, there are two types of questions one could ask. The first one is what is the exact long-time growth of the solution as t goes to infinity? And if there is an exact first-order symptom, then one could also ask what we can say about the second-order symptom, the fluctuation of the solution. Of the solution with respect to the growth. And for these types of questions, of course, one can ask the question in two scenarios. One could ask the question for moment asymptotics, which means you take the pth power of this notion and average it. And one could also ask almost your asymptotics. So you fix a realization of the C and ask. Of the C and ask the same question. So, in the Euclidean case, this is well known due to two fundamental work, one by Gardner Koenig for the moment asymptotics and the other by Gardner Kovnik-Muchanov for the almost sure asymptotics. So, both were published in 2000. So here is the statement. So let sigma squared denote the variance of the Gaussian field. So kappa is defined by, sorry, this is chi. Chi is defined by this explicit quantity in terms of the patient of the covariance function. So this quantity comes from solving some variational problem. Variational problem. We'll come back to this point later. So the moment of syntax says that the p moment of the solution, so here I use this notation, this bracket means taking expectation with respect to the Gaussian field. Just to distinguish it from the expectation with respect to the Brahman motion. Sorry, what was the initial condition for this? Constant one. Constant one. Yeah. So the moment the symptom is the following. So the leading growth, the growth of the solution is like an exponential of t squared. So basically this is exactly the moment generating function of the Gaussian field. But that's not too surprising. Not too surprising. Actually, there's a fairly straightforward and robust argument to obtain the first order symptotics of the moment. It's really just a few lines of proof. I could show you afterwards. So the second-order symptotics says that the fluctuation is of order t to the 3 over 2 and with the fluctuation. The fluctuation exponent is this kappa here. So we will see where does it come from and how is it related to some variational problem. The almost sure of symptomics is quite different. So with probability one, the solution grows like in the order t times square root of log t. log t. There's also an explicit constant in front of it. So this constant times the square root of log t comes from the growth of sample functions, the growth of the maximum of the Gaussian field in the form of radius t. The growth of Gaussian field in the Euclidean space is of order square root of log r. It's of order square root of log r, where r is the radius of the ball. And the fluctuation is of order t times log t to the fourth, one over fourth, with also data that explicitly. Is this some kind of like limb soup as t goes to infinity? No, it's an exact limit. So, yeah, so for example, you normalize. You move the first part to the left-hand side, take the logarithm, and divide this by t times log t to the one-fourth. It converges to this number. So that's the precise statement. It's an exact limit. It doesn't sort of fluctuate, it just no, it doesn't. No. No, not yeah, no. No, no, yeah, no. Uh so here so the basic question in the talk is um if one changes the underlying geometry um would it lead to uh some uh different asymptotic behavior of the parabolic model? Um so there are two obvious directions one could go, right? One could ask the question in in positive curvature, or one could also ask the question in negative curvature. We somehow choose to consider the hyperbolic space as an initial exploration because we somehow think that the hyperbolic space is more different from the Euclidean than positive curvature to Euclidean. There are several aspects of geometry in magnetically curved space that is not so much different from Rd. For example, it is quite typical that. For example, it is quite typical that they have polynomial border growth and polynomial heat kernel behave, and also the large-scale behavior of Brownian motion is quite similar to the Euclidian case. Is that assuming it's a non-compact non-metability? Yeah, non-compact, yeah. Okay, because S2 has rather than There is a long list of work related to parabolic Anderson models in non-Euclidean spaces. For example, I think so recently Ouyang Cheng and Victor did some work about moment estimates for parallel in its model on compact Riemannian antiques. And also, there are some works for And also, there are some works in the discrete setting, for example, on trees, on random trees, the Gautenwasen trees, by Hollander Konek Santos in 2020, and on regular trees by Hollander Wong in 2023. And there are also some works in the sub-Riemannian setting on Heisenberg groups by Workgroups by Fabrice, Chen, Sammy, and Jing, and also metric measures basis on fractals by Fabrice, Chen, Huang, Ouyang, Tim Do, and Huang this year. And there are also a series of works in the singular case where you need the regularity structure or parallel control calculus to. Control calculus to define a solution. For example, the work by Anton Yosha, who was driver in 2017. And there are more subsequent works. This is far from being a complete list, just to name a few. Okay, so before talking about the parabolic Anderson model, maybe I should first give a very quick I should first give a very quick review about the basic hyperbolic geometry. Maybe this is too elementary to many of the geometers in the audience, but it could still be quite beneficial for a quick review to non-experts like me. So the upper half plane, so let's just take the upper half plane as a basic model. So under the So, under the canonical parametrization, the metric tensor has an explicit form. And under this metric, the up half plane is a complete simply connected Riemannian manifold of curvature, negative 1. And there's a very explicit description of geodesics. So these are, here is the upper plane. The geodesics are given by semicircles where both ends are perpendicular to the x-axis, and also including those vertical straight lines, which can be built as, also be viewed as semicircles with infinite radius. And the hyperbolic distense function is also very explicit. It's given by this formula, where this distance is the Euclidean distance between two points on the upper half plane. So one could talk about the Laplace-Deltrami operator, so the hyperbolic Laplacian, which takes a super explicit form. And also the Bollinger form is the Lebesgue measure divided by y squared. By y squared. And the group of orientation-preserving isometries is this projective special linear group, which is SL2R, the space of determinant one, unit determinant matrices, modulo, plus minus identity, and the group action on the above planes via Hubiel's transformation. So let's name a few. Name a few properties, a few global properties about the hyperbolic space that are quite different from the Euclidean case. The first one is the volume growth. So in contrast to polynomial growth, in hyperbolic space, we have an exponential volume growth. So the volume of the ball with radius r grows like e to the r. And also the And also, the bottom spectrum of Laplacian is strictly positive instead of being zero. This is also quite different from the Euclidean space. And this will lead to the property that the heat kernel decays exponentially in the wrong time instead of polynomially. So this is also very different from the Euclidean space. But there's another remarkable point. But there's another remarkable point I didn't mention here is that actually in the long-term behavior, apart from this exponential decay, there is an additional polynomial correction, which is 1 over t to the 3 over 2. So this additional polynomial factor is Polynomial factor is the same in all dimensions. So there's an algebraic reason about this factor, which is quite different from the Euclidean space as well. Right, so this is a uniform upper and lower estimate of the heat terminal. Okay, now how about Brownian motion in the hydrogen? Brownian motion in the hyperbolic space. So, by saying Brownian motion, we mean the Markov process generated by the plausion. The first thing is that, so the Brownian motion is transient, it will diverge to infinity. And actually, the distance that it travels up to time t is of order t. This is something. The t. So, this is something also quite different from the Euclidean case. So, in the Euclidean case, the Brownian motion travels like in the speed of square root t. So, it travels a distance of root t on average. But in a hyperbolic space, it travels like in the order of t as t goes to infinity. And one can also prove a central limit theorem. So, if you look at the fluctuation of the Fluctuation of the distance that the Brownian motion travels with respect to the average, with respect to t. And then the fluctuation is Gaussian and with in the order of square root of t. Now, here is a quite famous and a fundamental result, which might not be that Very standard comparing to the previous properties. But this is a very elegant result. It's Sudoman's theorem. It says that let's just take the Poincaré radius as a model, for example. So you consider the Brownian motion starting from the origin. So previously we know that it is diabetic, it is transient, so that the radial component will converge to infinity for so long. But Suleiman's theorem tells you that the angular component is actually also stabilizes. So in other words, eventually the hyperbolic grounding motion is going to converge to a definite point on the boundary of infinity. Infinity. So in the Pointer at this case, you can just think of the Brownie at infinity being the unit circle. So the Brownian motion is going to converge to some definite random point on the unit circle, almost surely. So this is equivalent to saying that the angular component of the Brownian motion converges almost surely. So this is quite different from the Euclidean case. In the Euclidean case, for example, in dimension time, For example, in dimension higher than three, we know that the Browning motion is also transient, but the angular component there will not stabilize all those shown. But it is possible that you have different sub-sequence of times where your Brownian motion explores different directions. But on the hyperbolic stage, it will eventually stabilize at a particular point. And this property will give rise to the construction of To the construction of a rich class of bounded non-constant harmonic functions on hyperbolic space, which do not exist in Euclidean spaces. So basically you just solve the Diricheray problem. So given any continuous function on the boundary, you just consider this function. You run the Brownian motion until You run the Brownian motion until it eventually stabilizes at some point on the boundary, and you look at the expectation. So, this function is a harmonic function whose boundary value is, is a bounded harmonic function whose boundary value is y. So, you can't really do this in the Euclidean space because in the Euclidean space, essentially, the boundary at infinity essentially collapsed to a point. Essentially, collapse to a point. So if you do something like that, eventually you only obtain constant functions. But in negative curvature, it has a rich class of non-constant bounded harmonic functions. Okay, let's come back to the parabolic Anderson model. So we take the hyperbolic space as our underlying background. Background space, and we consider exactly the same problem. So Laplacian plus the Gaussian potential. So we assume that the potential is mean zero, invariant under isometries, which means that the distribution of the Gaussian field is invariant under actions by the isometry. And it is sufficiently regular with a sufficient. Regular with a sufficiently regular covariance function. So, in the same way, we have this Feynman Keck representation in terms of the hyperbolic Ramian motion. So, the first question, of course, one may ask is whether there are examples of interesting Gaussian fields, invariant Gaussian fields satisfying this problem. So, this is a simple construction of a class of examples. So, suppose we begin with the white noise on the isometry group. So, this is a random measure. So, it's a family of random variables indexed by sets with finite volume with respect to the HAR measure. With respect to the Haar measure, and it's a Gaussian family, mean zero, and the covariance structure is given by this one. And so this is called a white noise. Once you have the white noise, you can integrate the white noise against L2 functions. So you could just, so let's say, suppose f is a smooth function with a sufficiently good nice decay of infinity. Then you could just consider the Consider the convolution, the integral of my f against the y-noid. So that gives you a sufficiently nice Gaussian field. And the covariance function, you can easily work that out. It is given by the L2 inner product between fgx and fgy. And you can easily see that this is invariant under isometry. So it is a function of the distance of x and y. So this is the first part of the result that I want to present, which is about the moment of symptotics. So define H to be the cumulative generating function of the Gaussian field. So Ht captures So Ht captures the leading growth of the field, and beta t is the fluctuation scale, which is t to the 3 over 2. So the moment of syntax we found is a little bit disappointing on the one hand, in the sense that the result is exactly identical to the Euclidean case, not just about the scale. About the scale of the growth and the fluctuation, but also the constant, the exponent. But on the other hand, it is a little bit surprising because, as I mentioned, this fluctuation exponent is defined in terms of Euclidean, in terms of solving a variational problem in the Euclidean space. So, in other words, it is a little bit surprising because even with Even we begin with hyperbolic geometry in the fluctuation of syntax, the exponent does not feel the underlying geometry. What it actually feels is the Euclidean geometry. So this guy is defined in terms of some variational problem associated with the Euclidean Laplace. So actually what's happening is that in this fluctuation, a syntax In this fluctuation, asymptotics, what you see is the geometry of the tangent space at the starting point, which is Euclidean. I will explain heuristically why this is the case. So to do so, I need to maybe say a little bit more about the fluctuation exponent. Suppose we fix a base point on the manifold, so we can talk about. So, we can talk about geodesic polar coordinates. Rho is the geodesic distance to the base point, and sigma is the angular component. But let's define two functionals. The first one is called j. It is explicit, but it it comes from the limit it it comes from the limit of uh the tumulant generating function of a suitably uh rescaled version. Suitably with scaled version of the Gaussian field. This comes from explicit computation. Let's also define something called the Dons-Keveratan function law, which is essentially the Dirac trade form of the, associated with the Euclidean Brownian motion. So it is a function of probability measures. So previously the j this j is also a function of probability measures. A function of probability measures. So μ is a compactly supported probability measure of net. So if my measure is absolutely continuous with respect to the LaBeade measure on the tangent space, and whose radar negative derivative can be written as phi squared, where phi is an H1 function. H1 is the standard software space. Then it is just the energy. Then it is just the energy and intin otherwise. So, this is just like the Dirich trade form. But for the Euclidean Brown emotion, well, you can think of it as the Brown emotion on the tangent space. And what we essentially proved is that the fluctuation of symptotics, so if you normalize the solution by the correct first-order growth, and you take logarithm and you divide it. And you take logarithm and you divide it by this correct fluctuation order scale, then the limit will converge to this number, which comes from solving a variational problem. So you minimize this functional overall probability measures. But this functional is defined under Euclidean geometry. So that's, so in other words, the So that's so in other words, the fluctuation exponent does not feel the underlying geometry. It only re recovers the Euclidean exponent. And this exponent, because it's just the same as the Euclidean exponent in Gardner-Koenig's work, so it is known that we can compute it explicitly, which is given in terms of the second derivative of your covariance function. So I'll come back to So I'll come back to explain heuristically why you end up with Euclidean geometry. But the most surprising part comes from the almost sure symptoms. So apart so l uh so let's assume for simplicity that uh the covariance function is compactly supported. Function is compactly supported. This is not a very important assumption. It can be relaxed. Now, the almost show asymptotics is quite different from the Euclidean case, but this is the only thing we can prove so far. We can't really identify the exact asymptotics. So we can show that the solution grows like t to the 5 over 3. So there's an upper bound of the limb soup and the lower bound of the limb in by some deterministic constant. And you could recall that in the Euclidean space, the growth is like t to the square root of log t. But in the hyperbolic space, you have a much faster growth. Well, you might, so this is not. So, this is not entirely surprising if you think in terms of the growth of the maximum of the Gaussian field. In Euclidean space, the maximum of the Gaussian field on the ball of radius r grows like the square root of log R. But in hyperbolic space, the Gaussian field on the ball of radius r grows like square root of r because of exponential volume growth. Volume growth. The volume growth is very different, so it will lead to a very different growth property for the sample functions of the Gaussian field. But even with this observation, there is actually another part of the surprise which can be described as follows. So, as I said, it is well known, well, one can prove that the sample functions of the Gaussian theory The sample functions of the Gaussian field grows like this. So this is I think this is sharp. So the growth of the maximum of the Gaussian field on a ball of radius r is like a square root of r with an explicit coinstar. I think this is optimal. Now, as we mentioned, we know that the hybolic Brownian motion travels at distance Motion travels a distance of t. So that's actually only the two-dimensional case. In a d-dimensional case, the hyperbolic Brownian motion travels a distance of e minus 1 times t. So it is of order t. So up to time t, the Brownian motion travels a distance of order t with the root t fluctuation, which comes from the central theorem. Now, if you look at a ball of radio. If you look at a ball of radius of order t, then from this growth property, you can see that the Gaussian field, the maximum of the Gaussian field, is growing like square root of t. So now let's look at the Feynman Keck formula. It is given by the composition of the Gaussian field with the Brownian motion integrated up to time t. So up to time t, we know that the Brownian So, up to time t, we know that the Brownian motion travels a distance of t with very high probability. And at the same time, the peak, the maximum of the Gaussian field is of order square root of t. So you may naively expect that the correct growth of the solution should be t to the 3 over 2. So that's an aggregated guess. So the surprise is that this guess is actually not true. Is that this guess is actually not true? The reason is that it is true that with very high probability, up to time t, the Brownian motion travels a distance of order t. And at the same time, your Gaussian field is also growing in order from t. But even we know that the Brownian motion travels like a We know that the Brownian motion travels like in the distance in the order of T with high probability, but it doesn't mean that it is impossible to travel further. It is just because it is just that the probability that the Brownie motion travels faster or even further than T is very small. The probability that it goes much faster than T is very small. But on the other hand, if you allow the Brownian motion to travel beyond the motion, Beyond T, your Gaussian field is going to pick up something larger, right? Because your Gaussian field is also growing when your radius grows. So the probability that the Rami motion travels further is very small, but at the same time, the Gaussian field can pick up larger values. So there is a competition between these two. Scenario. So eventually, it turns out that it is possible to further extend this scale of distance until you reach a point where the sacrifice of the probability defeats the gain from the row of the Gaussian view. And that scale is, after computation, is t to the 4 or 3. So this turns out to be the correct scale of localization to pick up the correct growth of the symbol. And if you plug in t to the if you allow the Brownian motion to travel a distance of T to the four over three, then you will get the the five over three uh rows uh. Roles in the first order products. So, this is what we are currently trying to understand. We are not so sure whether there is an exact first order syntax or the limb the limb soup and the limb inf are different. So this is something uh we are not so sure. But We are not so sure. But we have some evidence believing that it is the second case. And this is the more interesting situation if this is the truth, because then we can proceed to ask the question about fluctuations in quality. And there are also evidence to believe that the fluctuation is in the scale of t to the 4 over 3. Yeah, but at this point, I can't really. Yeah, but at this point, I can't really say more which one is the truth. If it is the first one, it is also very surprising because this is different from the Euclidean case. In the Euclidean case, we do have an exact first-order symptom. Okay, so for the last maybe five minutes, I'll quickly talk about some ideas. So for the moment of syntax, So, for the moment of syntax, we basically follow the main philosophy of Galway and Kohling in the previous case. The underlying principle is actually very simple and natural. So first of all, if you apply a rescaling argument plus the Feynman Keck formula, it is not difficult to work out that if you normalize your solution by the first order growth, then you can, it emits a famous Can it emit a Feynman character representation, but in a suitably rescaled form. So here, beta t is the fluctuation scale. Cassiet is a suitably rescaled version of the Gaussian field. So Wt is a hyperbolic rounding motion starting at the origin, the base point. But a crucial comment is that the natural Is that the natural, the correct way of looking at these rescaled objects, both CT and WT, is that they should be considered as fields and processes defined under curvature t to the minus a half. So in this rescale picture, your geometry is also rescaled. So it is no longer the hyperbolic space of curvature one, it is of curvature. one it is of curvature t to the minus a half which which goes to zero as t goes to infinity so that explains why in in the in the second order syntax in the limit as t goes to infinity if you do not see the hyperbolic geometry you see the Euclidean geometry so let's be a little bit more specific so let's take the expectation on both sides then Then one can rewrite this renormalization in this form. So here, Lt is the occupation measure process of the Brownian motion. And Jt is the cumulant generating function of the rescaled Gaussian field, we normalized by this beta t. So there are two key points. The first So there are two key points. The first one is that this Jt will converge to some limiting functional J. So that J is the one that I showed in the earlier slide. And another key observation is that this process, this occupation measure process will satisfy the large deviation principle, whose rate function is the Is the Domskada time function. Now, once you believe this, then it is just a standard application of Varad Hans lemma. Because we are looking at an expectation like this. If you know that this L has an L D P, and you know that this J converged to some functional, then you know from the broadcast length. know from the Bradhan's lemma that this limit will exist, which is given by the infimum of this function node j plus the rate function of the L D P. So this is very standard L D P theory. So that's how we get the fluctuation exponent formally. But there are some technical difficulties. Technical difficulties to implement this simple philosophy. The main difficulty is that one cannot directly prove such an LDP on the non-contact manifold M. Because the Donska-Vara-Pan theorem essentially only works on compact spaces. So you do need to apply some sort of localization argument. You can't just directly You can't just directly do it on the whole space. And another thing is that in the current setting, if you look at this process, L, it is parametrized by T. So it's not a single process. As time changes, the curvature also changes at the same time. So we're talking about two limiting procedures. So you want to send time to infinity. So you want to send time to infinity, but at the same time, your geometry is approaching the flat space. So there are a few technical issues there. So, for the almost show asymptotics, maybe I will just quickly mention why do we end up with this choice of This choice of localization scale that I mentioned before, which should be a 4 over 3. Maybe I'll conclude my talk after this. So we recall that the maximum of the Gaussian field on a ball of radius, let's say kT. So KT is the scale of localization that I want to determine. So this is the growth of the Gaussian field. So let's think about the localization. Let's try to consider the following localization. Before exiting the ball of radius kt, we first let the Brownian motion to reach the peak of the Gaussian field. So the peak of the Gaussian field is somewhere very close to the boundary of the ball. So you let the Brownian motion You let the Brownian motion to reach this peak under time delta t. Delta t is a small amount of your total time length. And then after that, you require you ask your brand in motion to stay there, to stay near the peak island, to pick up the maximum of the thousand feet. So this will lead to the following simple. Simple lower bound, right? So we localize on this event. So the first part means that you ask the Brownian motion to reach the island at time delta t, and then you require it to stay there before exiting the ball. And then you split this exponentially into two parts. So for the first part, you just bound the Gaussian field from below by negative of its maximum, which is negative of Max, which is negative of c times the root kt, multiplied by the time, the total amount of time here. And then, so the probability that if you require the Brownian motion to reach the island, which is of distance kt to the origin, then this probability is of this order. That comes from the heat kernel estimated from the Kernel estimate in the hyperbolic space. And then the main contribution comes from this part, where you require the Brownian motion to stay in that ion to pick up the maximum of the Gaussian field. So what you pick up is e to the t minus delta t. That is the amount of time you require the Brahmin motion to stay in the ion. And then when it is in the ion, then you pick up the maximum of the Gaussian feed, which is HKT. HKT. So you get this lower bound. Now, the point is that you're trying to figure out what's the correct K to choose. So you want to choose K as large as possible, whilst this part is still the dominating part. And you expect that the other two are somehow negligible comparing to the main contribution. So you try to choose your KT as large as possible. KT as large as possible until this competition is reversed. So, in other words, this is the constraint of choosing KT. And if you think about this inequality for a few minutes, it's not too difficult to work out that the optimal choice of my delta T and K T should be, your delta T should be proportional to T, and at the same time, your Kt. And at the same time, your kt should be of order t to the 4 over 3. So that explains why the correct localization should be of order t to the 4 over 3. And once you've identified this, then it is actually possible to also prove an upper bound under the same scale of localization. And I think that's precisely it. All right, with that accurate to within three seconds stopping time, we do have time for one or two brief questions before we start with another talk at 420. How much your idea and that does depend on the constant purpose so you know it's also bounded by two. Okay, it's also bounded by two constants. Yeah, um okay, uh so the current so the moment of symptomics relies uh quite uh I'm not I wouldn't say that it is of fundamental importance, but it relies on the invariance of the Gaussian field with under isometries. So you do want your space to have sufficient stimulus. Sufficient symmetries. So I would expect that it is reasonable to generize these symmetric spaces. But for a general manifold with variable curvature, if you don't have enough symmetry, some part of the argument will break down. But I'm not saying that the result will be different. It is just because of it is just part of the method. Because if it's just part of the methodology, maybe we'll be affected. Yeah. How about modular co-finite error groups? Yeah, that's. Some symmetry in it. Of course, of course. Actually, another question we are thinking in parallel is whether, I mean, can we Can we uh consider the problem on on compact regimen surfaces? So in modular uh suite, basically in compact uh antifones. Um we do believe that uh the moment of symptotics would remain the same, but there is some technical challenge in getting the fluctuation caught. Because of the rescaling argument, there will be some issue in There will be some issue in the rescaling argument. You can't really talk about it. I mean, it is a bit strange to talk about dilation along geodesic contact manifold. For the omoshur symptotics, I think you would expect very different phenomena. Because, for example, on a compact space, For example, on a compact space, the Gaussian field is just constant. There's no growth in the Gaussian field. So for that part, I believe the result would be quite different. For example, the first order symptomics I would expect that it would just be T. Decrea a a function on the compact uh space. The states. You integrate exponential of integral from 0 to t of f of a Brownian motion, but your f is a continuous function of compact states. It can't grow more than faster than t. I have questions, but we only have two minutes before the next speaker, so I will maybe try to catch you at coffee, and instead we can. At coffee, and instead, we can thank you once more. Come down, fast turnaround. I'll press the button in preparation for pressing the button again.