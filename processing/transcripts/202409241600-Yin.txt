Today I only tell you just a bit more a very interesting property we found recently. And so I will try to avoid any detailed techniques and notions. I will just try and do very basic things, just play around with the matrices and also the rank about on the matrices. And so So let's start just with matrices. Basically, we're back to linear algebra. So we have two matrices. We know the matrices for the Rankine matrices is sub-additivity, which means we have such inequality for the sum of two matrices. And of course, this quantity, the sum of two ranks, is quite not optimal, far away from optimal to give a bound for the rank. To give a bound for the rank of A plus B. But we can do the following that we can improve. Just with very simple argument, we choose some lambda to make this decomposition, kind of decomposition. We make A plus B to be A plus lambda plus B minus lambda. It's the same thing. And then we're using some additivity of rank to get another bound, but now it can be. Another bounce, but now it can be smaller, and of course, you can take you because this holds for arbitrary lambda, lambda just complex numbers, so you can take the informal. And of course, when lambda is not the eigenvalue of A and B, then this rank is always maximal. So you can take the, you can choose a number only to be the eigenvalue of B or the eigenvalue of minus A. Minus a and also you can look not just a union, click actually the intersection of these two sides because if you already have one rank getting maximal, then already you can you cannot get a smaller rank for the for this quantity on right-hand side. So you can, in the end, with just a simple argument, you can get a slightly improved upper bound for the rank of A plus. upper bound for the rank of a plus b and then the question is can um can this be uh somehow optimal so but let's first uh rephrase this question in the following now we fix matches in a and b because the previous arguments somehow we are trying to keep the information about the eigenvalues so now we just fix matrices and then we The fixed matrices and then we rotate B by unitary matrices, then AB will be in very general positions. So when we can look at the superma of this rank, A plus U B U star, then we can ask how about this upper bounds, this superma. And by the previous argument, we know this is bounded by this quantity. And then the question I want to tell you today is that can this Tell you today is that can this be optimal, which means this suproma is equal to this? Is that possible? And moreover, I want to tell you that now I want to look at more complicated polynomial. Here, I only look at summer, so it's very simple. In this application of this probability, I want to tell you that we can actually get upper bounds for any polynomial p. P evaluated such matrices A and B, and B is rotated by arbitrary unitary matrices. And also, I will show you that in some sense, this upper bound is optimal in a dimensionless way. Okay, so this is the question I want to show you that somehow can be done by favorability, which is quite Done by favorability, which is quite surprising because we are not this is not the question we are asking probability, it's uh it's kind of a byproduct of some property we found in our research. So the main result is the following. So it's uh it's from we I refresh our May results in the following. So so we are actually studying some autumns for the free random variables in probability. Inferability and we find some universal property. And this universal property is more or less can be stated in this way. So I only now just concern for matrices. We have two matrices A and B. And then inferability, we have some construction. We can make a free copy of two matrices. And then when we rotate this B by unitary matrices, and then this rank is always. Rank is always smaller than the rank of the same polynomial evaluated at this free copy of two matrices identified by A hat and B hat. I will tell you more about how this quantity can be dealt with. And moreover, this quantity on the right-hand side can be approximated by random matrices. I will tell you more how it's quite direct and simple arguments. Arguments. So, in this sense, this quantity is just somehow imagine you have random variables in favorability. We can do like what you have for the classic random wearables. We can make this random variables free, free independence. So, this would be free product analog to the tensor products. And before we continue the story, I want to make a few remarks. Make a few remarks. So, first, to just make things simple, I only write all statements in two variables, but all results can be generated to finite many variables. And there's maybe some other interesting generation is that actually we do very general statements. This AB are not necessary to be matrices. They can be any normal operators in finite. Any normal operators in finite fundamental algebra. But today we won't go to this general setting. It takes too much technique stuff. So also I only look at the polynomial. This can be also generalized to can be this inequality can be done for the non-communication functions. I also won't talk about this today. Also, when you have such things, we know for the user rank, you have this rank. The this rank neutrality theorem. This you can write rank as the total size of the matrices minus the eigen space, the dimension of the eigen space. And the right-hand side can do the same thing. So this theorem can be rephrased with the help of the dimension of kernel space, eigen space, and then the this free copy of matrices will give you a lower bound for any. For the dimension of the eigen space for a given pinna and for abitary eigenvalues. Okay, so how we have such phenomenon for some free copy of matrices. So again, I will give you maybe a brief introduction of repability. You already have to see this yesterday, but let's just Just record. So we have an abstract setting, we have something called a non-computer space, which is just a complex unit algebra A, and then with a linear function of tau. And we add to be this tau to be unital because this will be our conditional expect will be our expectation of random variables. So we call elements in the edge bar the non-competitive random variables. And when A is a star edge bar, And when A is a star algebra, we need this to be linear function tau to be a state. And the typical example is that matrices. This is the first really non-tentative probi space. We can understand matrices as random variables. And then the moments are given by the normalized trace. And the most important or common examples is the finite bond algebra. The finite fundamental algebra. It's because many interesting random wear buttons inferability are living in this space. But we don't need too much about this today. And I will tell you what the property and we need to see this universality. So, but this framework I have to put here because when we have such framework with functional matchup, we can define an analytic design. And it's an analytic distribution Î¼A associated to giving normal random variables, which means just the normal operators in the fundamental algebra. And with the spectrum theorem, we can define a distribution associated to the random variables. And this EA is just the projection values, measures given by the spectrum theorem for normal operators. Yesterday, you already see this match actually can be determined by the mixed. I mix moments like this in this way. And I really need this setting because then we can talk about rank. Now we have, because in the fundamental bar, we have all the projections, which will be the identity function and the boreal stats. And then when we have a random web bar, we can look at the projection to the closure of the image of A and also the projection to the kernel. A and also the projection to the kernel of A, and this will give us the rank and the dimension of a kernel of A. Or the way you can also shift A, and then we'll get the size of eigen space. And then the projection is always in the fundamental. And then we can take a trace. And this number, then we define it as a rank and dimensional coloner. And we have this rank new entity theorem for the finite fundamentals. And in this And in this setting, we get we have a very classic notion in classical probability theory that we have atom, which means that we have a single atom and the measure of single atom is positive, then it's called atom, and this corresponding to the eigen size of eigen space. So it's just dimension of a kernel. And then we can write every, we can write autumn, the size of We can write autumn, the size of autumn, as quantity by involved rank. So, but in the following, I won't talk about too much about dimension of the angle space or the autumn. We'll focus about the rank, but everything can be rephrased in other way. Okay, this is the framework we have, and then the basic examples, of course, matrices. If I have mattresses, If I have matrices, then we have an analytic distribution to the x, then we know it's determined by the moments. Then it's easy to see this is the average eigenvalue distribution. This delta is just indicates this direct mass at the lambda i and the lambda i are eigenvalues of x. And then this atom is purely atomic and then the Atomic and then the signs of atom is just eigen space. Also, in the following, then when we talk about the rank of matrices, I will always assume it's normalized. I mean, we have the trace, it's unital. So when we get a rank, it's always a number between zero and one. So the rank in the previous definition, it's normalized, kind of normalized rank. But so I will not write these parts, but we can. Not to write these parts, but we can understand this is a rank. If it's a matrices, it's where the in the previous definition is just usual rank divided by the size of matrices. Okay, and then the first really important example is that the following. This will be a key tour we will use later to see this universal. See this universal property for the rank. So we call a unitary random variables, which means that it's a unitary element in some star per bit space. We call it high unitary if it satisfies this moment conditions. And then you can show this the analytic distribution associated to this element is the high measure on the spectrum, and the spectrum will be the circle. So we call it the high unitary. So, we create the high unitary random wearables. And a typical construction is actually kind of a motivation example for such notion that we can look at any discrete group and we can associate a group with following non-committed property space. And the H bar A will be taken to be this group H bar, which is the linear, finite linear convolution. Linear finite linear combination of the indicate function on the group elements, and we have a canonic trace on the this group h bar, which just sends this indicate function to one if g equal to e, and then the others are zero. And then you can consider element here as random verbals. And if you choose some group element here, and if it's totally free, which means Here, and if it is torsion-free, which means the order is infinite, then it's a high-unitory random variables. And then we have this notion of independence. It's a non-cumbitative anonym of the classical independence in the classical theory. And yesterday, you already said the definition is just asking the vanishing of this mixed moment when we have. This mixed moment when we have this element, this product coming from alternatively come from different sub-edge paths. And in particular, we can talk about the two random wearballs are free or freely independent if they generate some free independent sub-ethras, which means you have a rule to calculate the mixed moments of these two random variables. And I'm not, I won't use And I'm not I won't use the definition. I will tell you some other property that we were used to get to see our universality. Okay, and then I want to tell you that there's a typical construction of a free product, like so it's analog of this tensor product when you have tensor map this product of Product of two measures give you independent random errors. We have free products. So if we have such a non-campus space, a circuit, and we can construct something like this. And this AI circle is just the kernel of the linear functional. And you can see this is an infinite sum of a lot of stuff. I'm not going to give you the explain this construction in detail. This construction in detail. What I want to point out is that this construction is quite different than like a tensor. It won't stay like a finite dimensional if we're starting with finite dimensional stuff. So if we have two matrices, now we want to make a free copy to get our upper bounds, then they are basically leaving the free product of the matrices. But then the free copy of AB won't be Of AB won't be matrices anymore. They are some operators living on some operators on some infinite dimensional hyperspace. And then the problem is that the rank of the free copy of AB is not given by the user, cannot be given by the user rank or matrices. It only can be somehow calculated through such reproduct of the nominated streets. So it's So, it will be some very non-trivial quantity and objects due to this complicated construction. But just something about rank we can tell you. So, at the beginning, actually in the 90s, not far away after Verkulasku developed the theorem of free probability. He proved that if you have two free independent self-joint random variables, Shall we joint random variables? Then he prove that the rank of the sum of xy smaller than one if and only if there exist some numbers such that this quantity also strictly smaller than one. And so you see this, you already see these things at the beginning, right? And the non-trivial part, I mean, one direction is actually trivial. You can do the same argument because. Can do the same arguments because this rank is also subadditive. If you know it, then you see one direction is quite trivial. But the non-trivial part of this theorem is that actually it's equivalent. And moreover, he proved that this rank of sum of xy actually equal to rank of x minus this lambda, which exists, such that we have this equality and it's equal to this. And it's equal to this sum of two ranks. So, actually, the original statement is using the atoms. So, the needs to ask X, Y to be self-joint, then you can define measures and then you can talk about atoms. But you can refresh everything in rank, then you have this formula. So, then we can combine this with the argument we have in the beginning, but now, of course, we need to somehow restrict our matches. To somehow restrict our matrix to be Hermitian. And then you look at the rank of A plus UBU star. And when you see this equal to is bounded by this quantity, and now this theorem tells us this quantity is nothing else but exactly the rank of a free copy of two Hermitian matrices, A hat and B hat. So now I'll give you a special example of the theorem we prove in general. The theorem we prove in general. So now, if you take a polynomial to be just x plus y, you will have this arbitrary rotation and take a rank, you are bounded by the free copy of two matrices. So why this? So it's I want to then I want to show you it's not a coincidence. There's something behind this inequality, but in general, if you take an arbitrary poinomial, at the moment, I think we don't know how to somehow. We don't know how to somehow figure out this quantity, so we cannot do this. Figure out the upper bound and then prove exactly the same as this thing, and we get this in quantity. I'm not doing this ever go directly, taking some arbitrary polynomial and look at the rank of polynomial of A and U, B U star, and bounded by same polynomial, evaluate A hat, B hat. I will go directly to that without saying this. So, how is this possible? Then we need a number, a very basic factor in the flepability. If we have a high unitary random variables, we can construct with the help of groups. We're just asking the moment to satisfy this moment conditions. And then we took any random variables, x, y, and we ask x, y free from u. And then we look at x, u, y, u star. Then you can see just from the definition, it's there. from the definition it's they are they are free independent and in such a way this u y u star won't change anything about y so the distribution of each verb here x y are preserved but then they are free so we can think this is a free copy when we say free copy in the in the following we just imagine we are doing some operation like this so it's kind of similar to what we want to to bound right so we have a met two matches a and b and we Two matches A and B, and we rotate it by arbitrary matches u. So now we imagine we have two matrices A and B, and then we can look at the free copy of two matrices in this space. We take the free product of matrices with just the integer groups, and there's a free high entry. And we make conjugate U with 22b and then there. To B, and then this is the frequency. And then, what we really want to show is such inequality. The antipolynomial evaluated AUBU star should be smaller than this rank of polynomial. Now, the rank is quite a non-trivial, but there's a way to see this rank. I will tell you in the following slides, and then we need to evaluate this P at A and U B U star. B u star. Now that's something not that nice. That we have a very concrete matches here, A, B, and this also concrete scan when it matches U, choosing arbitrary in the UN3 group. But here we have something abstract. So what we are doing next is that we will rewrite this U also in a metrics form. So I mean the following. We have another technique or basic stuff inflability. Basic stuff influenced. We can start with any random wearables or operators. We can build a metrics model, which gives you the same thing, but just nothing more, but just write it in a metrics form. We are doing this for our unitary random variables. So if we fix high unitary, there's no need to be free and just the high unitary random variables, and we can construct m square random. Square random wearbus, we did it by uij, and then we put in a matrix form. We can we can we did it by u hat, u hat. Then this u hat is again a high unit random, high unit random variables. You can do this arbitrarily. Actually, you will get back the same distribution. Then we do it for high unitary. And what's more is that when we have this entry are free from some Are free from some h bar for random variables, then this matrices will be free from the matrices over this h bar m and in particular if we look at the matrices and then this this one is uh if we take m to this scanner management and then this this u hat construct from this free compression technique always free from the matrices, then we can replace. Then we can replace this small u by this hat, capital U. So I won't go to the explicit reconstruction of this free compression to bear these elements, but I tell you they are the two basic property, and then we can replace our U by our small U by this. And there's another thing is that this entry here is very nice. entry here is very nice the former algebra isomorphic to the polynomials of n-square formal variables and there will be this isomorphic also will preserve rank i will explain this property in the following and that's what we need to see this universality so the first two just to remark that first two is uh it's a common knowledge interoperability and but the the three the third one is non-trivial we The third one is non-trivial. It's proved by another joint result with Tobias Meyer and Iron Speischer a few years ago. But I will explain what's the meaning, but without giving too much about the detail and how to prove that. But then we will see how to why we have this university. So now again, we have matrices and we just scanner manuals. So they were sitting in. So they were sitting in side of some matrices or some maybe from the match bus. And where we can, where this UIJ living, compress our U to get some entry UIJ, right? Then this living this space. And our results said the entries are actually. Actually here I should take just M to be scanned values. Things clear. So these things, because scalar is always free from anything. So when we have when we've done this free compassion for high entity, this engine are automatically free from scalar, free from one. And then this guy is always free from AB. Is always free from AB. That's the second term of previous number. And then now we just plug in our u hat, replace the u, we give the same quantity here. Because it's again, this is just a high entry. And we know this gives us the same rank. And then we need to prove this. And now you see, then the form in the left and right hand side are quite. The left and right hand side are quite similar. What you need to do is now just mapping each element here to some numbers. So we have a unitary matrix here, right? It's again by n matrices, but our numbers. If you map each such object to numbers and you're proving this mapping is somehow have a rank decreasing property, then we have this rank inequality. This rank inequality for every unitary matrices. So, to do that, and it's another notion, it's also not complicated, it's a very basic notion that generates the notion of rank. So, if we have m matrices, let's say it's over the algebra of polynomials, so just so then we can define this notion called the. And this notion called the inner rank, we dealt by row A, is the minimal integer such that we have such matrix regularization. So we look at all possible ways to write our square matrices into two rectangular matrices, P and Q. P is R is N times R, and the Q is R times N. And the smallest number R is the inner rank. And then you can show that. Can show that this notion has the basic property that to net it behaving like usual rank. So then I want to remark that if you look at definition, you can also see that if you have a homomorphism from, let's say, polynomial to anything, or just homomorphism between two rings. Two rings, then this rank will be decreasing along the homomorphism because this notion actually can be, you can see, easy can be defined arbitrary, arbitrary ring, and then along some homomorphism phi. But now we only need to take the homomorphism from polynomial to a ring, maybe by the evaluation. And then this is have a rank decreasing because when you have factorization, Because when you have factorization over formal variables over match over polynomials, then you just evaluate them or take this homomorphism, you automatically get a packetization in the same shape. So this is always ranked decreasing. You can also check, if you take the scan minimum matrices, that this inner rank over the complex number is exactly the usual rank. So now we are. now we are very close to the universality because now we can we can we can do the following we just choosing n by n formal variables then we look um then we look at um homomorphism by map each x i j now it's just formal variables you can evaluate anything you want we evaluate this x i j to be the entry of the given uh unitary Unitary matrices, and then it's a homomorphism. Then, of course, we have this rank decreasing. And then our problem is remain, our problems tend to the following inequality. We need to show this inner rank over a formal, inner rank of matrices over formal variables is smaller than the point of some matrices over this. Over this UIJ compressed by the high entries. And this actually, the third term, the third term for previous compression number, and we actually can show this quantity. Now they are living in some phonema edge brass, and this analytic rank actually equal to this inner rank. Of course, you can see that this also is kind of evaluation of a general matrices or pinomination. General matrices or polynomials. So it's always smaller than this quantity. And then when we have this inequality, it's automatic autumn. But we prove in a very different way using something called dual system in free probability. I won't bother you about the detail of these techniques. So let's see, we have such equality, then we get our rank inequality. So puts everything together. So it puts everything together. Then we have matches A and B. Now we replace this high entry by this at U here. And then we know this rank of user polynomial at these two matrices is smaller than the inner rank of same polynomial. Now evaluate at A, B are just the user matrices, but then we have this formal, these matrices or formal variables. These matrices or formal variables xij. And the good things is there that for unitary matrices, u star is nothing but the u inverse, the u inverse. So we can replace this u star by the u bar inverse because we have something formal. We don't have the star structure here, but we can consider u inverse. And this will go to the rational functions building from these formal variables. So there are some. The formal wearables, so there are some technical issues, but I can tell you that this with this free compression technique, we can prove this in quality. And also actually, the technique is just there's inverse, but we can get around this inverse by something called a linearization. By something called an unionization. This also mentioned in the yesterday talk of Marma. And then we can promote everything here to, again, just some polynomials. And then everything is kind of a homomorphism. And we can prove this if you were going from Xij to this Uij here, it's also a homomorphism and they keep this rank. And this is what this compression lemma tells us. And then we have Lamma tells us, and then we have this rank smaller than this rank. That's why we have this interesting upper bound for the user matrix rank for arbitrary polynomial. And then I will using just maybe I will explain why this is kind of optimal. Then the other, this optimal part is done by. Optimum part is done by the random matrices. So, this is we are using this result in random matrices. We take a high unit random matrices, which is just the matrix value, random variables that's sampled according to the high matrix on the M by M unitary matrices. And then when we can look at any deterministic matrices, we can actually kind of fix. Be kind of fixed x and y. But in general, we can assume these two sequence of matches converge in distribution to some matches, mu x, mu y. And then we all do the same thing like this. So now we just conjugate. I'm sorry, this should be this something wrong about. I want to write here the UN. here the un y n un star so forget about this this should be the un y n un star then the un y n un star it's sympathetically free from xn so it's just again we're using some unitary stuff and conjugate to some elements then they were somehow free or simple totally free from the other matrices and then which means that now we have this quantity converged Have this quantity converge to polynomial evaluated as some free random verbals. So, this is the precisely meaning of the SM33 when you have such construction. And then this allows us to do the following. So, now we fix two matches A and B. So it's the rank we want to look at. And then we can compose, we can. We can constrict this B tensor IM by some high unity random matrices. And we know this table is free, asymptotically free, which means they converge to a free copy of A and B. And then we can take a polynomial, but not that polynomial, it will be a two by two matrices of the interest poinomia in such a way. Then we make this object subjoints. Then we can talk about the Then we can talk about the measures. And then we can follow from the previous result that we know this measure converts weakly to the measure of the polynomial, this two by two matches of polynomial, evaluate the A hat, B hat, and then we can conclude something about atoms. And this from atoms, we can conclude something about rank, which means we can find some matrices, converge to the upper bound. Converge to the upper bound. And that tells us somehow if you are not, if you want an upper bound that involves not involved dimensionless, then you should always get this free random wear bus. So now I sum up the major results for matrices and B in the following. For matrices and B in the following, just we look at some set like this. You look at the matrices of all dimensions, but you keep the information about the eigenvalues, eigen space. And then the supermo of this quantity were given by this quantity of the this rank of polynomial evaluated the free copy of these two matrices. Also, you can also calculate this quantity with the help of this inner rank. So, this will be a pure. This inner rank. So this will be a puny edge break stuff. And that's that's that's the main story. And in the okay, have maybe one more remark about this result. That's is that this upper bounds is optimal only somehow in a dimension. Only somehow in a dimensionless way, if you fix the dimension, then if you look at the rank of polynomial or some matrices, but fixed dimension, you probably cannot get this quantity. Here is a simple example. So if you look at a polynomial like this, you can check this polynomial vanishing on all two by two matrices, but you can find in higher dimensional. Dimensional matrices that this quantity is strictly larger than zero, which means they break this inequality. So this inequality is somehow only optimal in a dimensionless way. And in the last, I want to give you more examples of this upper bound for the rank of matrices. Rank of matrices. I already told you that this sum of rank is bounded by this quantity with just a very simple argument. And this proving to be exactly the rank of two free random verbals. And actually, we can remove the limits of self-jointedness. So we actually find another way to get this. To get this rank of free random webbers. And the methods are very different than the usual method inferability, which involve, I mean, like the WebCoLescu and Berquid result using analytic tools, but here we can do it very edge-break and we actually find more quantity. Quantity. I mean, we actually find more polynomials. We can calculate out all exactly this rank for some three random verbals. So here it's commutator and non-commutator. And we figure out this quantity. It should be the rank of the three random verbals in this per two in commutator and non-commutator. And then automatically our And then automatically, our general result tells us it can be the optimal bound for the just commutator and the anti-commutator of two matrices. So I think I finished quite fast. Then that's I will stop here. Thank you.