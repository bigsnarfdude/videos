Um we were discussing is based on having two samples. Um but for example Lydia showed an example where she had a third sample in the middle and it might be well to my mind it would be sort of seem obvious that if you have more samples you get a better idea of what that map should be. So is there a way of including more samples? There is with a Wasserston very center. I I don't know if that would have been your answer but go ahead. Okay, so one thing you can so is my understanding correct that you you you you're assuming you have multiple distributions. We're assuming you have multiple distributions. Yeah, so I think of it as saying I want to go from P to Q, but via something else. I don't know something in the middle. I don't know about the via part, but if I had three distributions, one over here, one over here, and one over here, and you would want to find a midway between these two? No, no, no. So, say, for example, I have your triangle at the top there representing s equals 0 and the square representing s equals 1. But now I know that the double bump. But now I know that that double bump represents S equals 0.5. Is there a way to go to make sure that I go through the double bump before I get to that? You see what I mean? Yeah, yeah, so absolutely. So if so if I understand correctly, you can just sort of extend this picture if you want. And just start here and you end there, but you want to go sort of like this. Like I started to find two maps. Right, exactly. Because I mean, you have to go there because it's sort of one of the distributions that you know is true, right? And so the only question is, you know, how do you go from here to there and how do you go from there to here? From there to here. Except we want it to be smooth at the midpoint. You can't be two maps, it's got to be one map that's smooth at the midpoint. The map either way will not necessarily be smooth, though. So that's not a property of transport. So if I create a hole over here, so my distribution is a mixture of two disconnected, two distributions with disconnected support, the map from here to here will at some point have a discontinuity because they'll have to send some mass to one half and one mass to the other half. Some as to one half and one mass to the other half. So the map will not necessarily be smooth, even if we have this constraint. I think you can also think about it: if you're going from point one to point three, the geodesic may not cross through point two, ever, so there may be no possible smooth map. The gradient G may never pass through that direction. But there might be a smooth, non-geodc map. There could be a smooth, non-geodic map. Because if the geodesic from A to C does not pass through B, there is no solution. There's no solution. That's right. There is no optimal transfer solution, but there could be a longer path that is smooth and passes through the limit. That doesn't solve the model job here. That's right. Yeah, sure, but that's probably simple. Well, it depends what you want, right? If you're not sort of smoothness, but you won't bother this, that's coming and saying I haven't finished. I will not finish my finish. So, to me the more interesting, I mean this is extremely interesting, of course, but the more interesting problems here, at least a few of us here, is how do I transport a certain event, like all its bells and whistles, that is conservation laws, into a different event which also fulfills some physics by going through the space of all probable physics. All probable physics realizations of my matrix element. And so, how do I inject in that loss function that cost that you have the matrix element weight in a way that I know that I'm going from here to there, transporting stuff in a way that maximizes, in a sense, my probability density of realizations of real physics, because I don't want to go and take the shortcut and go through unphysical points. Points? It's a good question. It's one we ran into when we were working on this application, and the way we circumvented it was to choose an estimator of the transport map, which when you evaluate, so we estimate the transport map here and then evaluate it over here. We chose the estimator such that its output over here would be supported on the signal region points that we had from this distribution, but just with different weights. So, our way of circumventing this in practice was to This in practice was to make sure the map only takes me to points that I know are physical, points that I've seen already in the data. Just such that they assign different weights to these points. So I agree. In the sense that... But it doesn't fully answer my question, because although you may go through points that exist in the space, you don't know what their relative probability of occurrence through some matrix evidence is. And so you're jumping for points that may be high or low probability, and you don't give a Or maybe any long game but card for that. But then I could say the same thing about any other background modeling procedure: that I produce some weights on some points over here, and those aren't necessary. I'm not questioning the validity of the application. I'm just thinking broadly of a way to do this that preserves the information on the market side. Yeah, so I agree. It may be a question of choosing the cost function correctly. I'm not sure. Yeah, it's a good question. The discussion keeps getting just a very quick comment on the previous thing. So, basically, I think there's like two problems here. It depends on whether you have an ordering for these templates or not. So, in Lydia's case, there was an ordering, in which case this makes sense. And indeed, then you kind of have results two different optimal transfer problems, one from one to two and then two to three. And that's not the same as point from one to three directly. So that's when you have ordering, but then you might also have problems when you don't have ordering, in which case it's Don't have ordering, in which case it's like you know, then this point of the right picture, and then you would kind of be looking at what Twitter and kind of Washington's time manifold or the manifold of this optimal transfer solution. So it really depends on whether you have ordering or not. I think that's sort of the kind of the defining feature of what you want to be looking at. And then depending on the answer to that, then you might have to choose what methods you're going to work on. So I think I have something that follows. Something that follows more or less where Tomasu was going. So, if you go back to your slide with the beautiful green arrows going to the region of users, so in many cases, the background composition is different on the different sidebands. So, like you have different types of processes, or you have them in a different amount. So, what happens in this case? Does the method have some? Do does the method have some notion of clustering and then it will because it will tr take the shortest distance it will morph let's say I'm going to say jets and B plus jets because this is four B and it will morph each one of these clusters or will create a mapping which is specific to each one of these clusters or it will just you know everything goes it won't take into account this clustering structure I mean ultimately we we just learn a map that's defined over We just learn a map that's defined over the entire control region. And this is just a deterministic map, to be clear. This is. I mean, it depends on the stochasticity of the data, but I mean, it's just a map that's defined over this entire space of collider effects in the control region. So I understand that you do it in a sort of holistic way, and you do it once. What I'm wondering is whether, underneath the hood, it is actually learning two separate mappings because it figures out that there are two separate populations that you can't say more of. More of? It's a good question. I think it depends on how singular they are in ET. Right, right. The question is: imagine they are very different. It's like really two jets and B plus jets, or four jets and I don't know, two B plus two jets. Is it able to actually figure out that there are two things happening? If the cost is such that it's very hard. It takes a long distance to get you from one class to the other class. One class to the other class, but within each class, the distance is because it's very different, right? You're thinking about exploiting. No, I'm thinking, of course, every time we get a beautiful tool, we try to use it for it. No, no, exploiting the underlying information that inside the transport. Yeah, it figures it out. So, because that would be, I think that would be an interesting thing to investigate. If it can figure out just the fact that you have the optimal transport, can figure out that there are different populations. Can figure out that there are different populations being transported, like one takes the plus, the other takes the other. So, this picture is really misleading in the sense that it's really done in the 12-dimensional space. So, do you want to say something about the topology of these two sets in the high-dimensional space? We've had this discussion about Swiss G's versus Guji, so maybe you want to say something about that. Yeah, I'm not sure if that'll answer Andrew's question, but Andrew's question, but yeah, I guess it's unclear when you go in the 12-dimensional space that we're in, these points are very far apart. We have a cursor dimensionality here, and these, you know, there aren't that many events that we're able to work with. We're working with on the order of 100,000 events here and 10,000 events here. So it's a relatively small data set for a twelve-dimensional space. And these there isn't really any clustering going on between these points, I would say, in our application. But nevertheless. How do you know? I'm just saying these points are very far apart, we would expect. Did you do something like some dimensionality reduction technique and then saw that there are indeed no clusters? I have done these early on. I have done these things early on and there wasn't any obvious clustering structure. But either way, that's not answering your basic question, which is you could be in a situation where there is clustering structure. Situation where there is clustering structure. And then it comes to the question of, for example, as a simplification of your question, what is the optimal transfer plan between two Gaussian mixtures? For example, like that's a basic question which one would like to be able to answer. And you'd like to be able to say that one component of the Gaussian gets mapped to one and the other gets mapped to the other. And that will not be what happens in general. At least for natural cost functions that we're used to using. That's a good counterexample. Yeah. So unfortunately. Unfortunately, I think you may do this one-dimensional picture. I'm just trying to figure out how this error study, what they really would do. Am I correct that the left part of the left control region would occupy the first part of the signal region and the right part, the second part? You say this is the same like horizontal morphing, so it would mean that the left part goes to the left part of the signal region? That's a good question. Question. Well, I think, you know, again, if this is really just a 1D slice to a much higher dimensional space, then it's not clear what does it even mean half of the signal reaching? Like, which half? Yeah, yeah, okay, but still the left would go, let control would go to somewhere. Let the region in the signal arrangement. Yeah, this does go back to what Mikhail said, that in the 12-dimensional space, we actually expect these control and signal regions to be very, very mixed up. The way we define them in our problem is through a two-dimensional projection. Is through a two-dimensional projection, which looks at, you know, we have four B-jets in each event. You imagine each pair of B-jets arising from one Higgs boson in our application, and we know the invariant mass of the Higgs, and that allows us to draw a two-dimensional mass plane, and that's how we define our control. It is simply possible. You would just use we cannot do better than do a fit fitting of this left co the whole region, like one secular region. And then you know then that right you And then you know then that right you will that but we have just discussed before that for background shape fitting. You will fit the whole spectrum and then you get the result for the for the transfer effect of the yeah so I do think you're right that in this case the optimal transfer plan will just map this monotonically up to whatever the right point is such that the mass is preserved. Yeah that is true. Are you only going to uh So it's not even obvious to me that if you have a mixture model, whether or not the thing is that you want to have separate transport plans for each component. Because here what is actually happening is that just the mixture coefficients are changing. So the actual distribution of the mixture stays the same, but you're changing the relative weighting. I agree. I guess even in the case where the mixing weights were the same between the two mixtures, I still don't think in that case it would be the case that the components did not have to each other. But in general, I think it would be an interesting But in general, I think it would be an interesting kind of benchmark or toy model to think about just causal mixtures and if you take this as the kind of our training wheels, I think. Yeah, so unfortunately it's quite hard to study optimal transfer between Gaussian mixtures, and a few folks have tried to do this in the literature. And there are some papers of people who slightly modify the optimal transfer problem in the special case of Gaussian mixtures. And anyways, I won't go into it, but I agree with you, but it is actually quite hard to. I I agree with you, but it is actually quite hard to to study what this thing does for this thing. I I'm just gonna say, isn't it the case? Maybe I'm wrong, but if you're using the MONGE formulation where you cannot split density from the source to target, then you're guaranteed you won't get different transfer maps because if your transport components share support, then all the density of shared density at any point has to be mapped to another point. You're right. This is, yeah, that's a great observation. So that's an interesting feature of, oh. Interesting feature of these optimal transfer maps is that no mass can stay in place. And this goes back to your electrostatics problem interpretation, right? So yeah, that's exactly right. It's a feature or plug. I don't think yeah, but that's the duplication, right? I mean, none of this knows about physics. The physics knowledge has to be injected. And only then, if that can be done in the right way, then it starts being meaningful, right? Being meaningful, right? Ah, but actually, the projections of those dotted lines in 12-dimensional space are physics motivated to say calculated the invariant mass in order to draw those control regions. Why not use those invariant masses? What do you get from the other 11 dimensions or 10 in this case? Because there may not be anything, any extra information in the variables. I'm not sure I follow. I'm not sure I follow. So you're saying I should use the other dimensions to define the controlling signal just? Why not do it in one dimension? Well, you probably have at least two invariant masses from the two. But why not use those variables instead of 12 directions and forgets? Ah, so okay, now these are the question of why are we estimating a background in full dimension? That's right. In the first case. You actually wouldn't be doing this because you would want the curse of dimensionality whether you like all statistics. That's what you would get from full data. The reason we're doing this in full dimension is because if you have a background estimate in full dimension, Is because if you have a background estimate in full dimension, then I can form a classifier in full dimension to discriminate background from signal events. And once I have such a classifier, I can use the output of this classifier as the discriminant that I use in the final signal analysis. Like when I form bins for my final likelihood, I would form bins along the output of this classifier. And we think of this classifier as the most powerful discriminant that you could use for this signal analysis. It's the one that most separates signal from background. And if I can train this classifier on the full-dimensional space, On the full-dimensional space, I heuristically expect to have the most power. I incur a cursive dimensionality, and maybe there's a trade-off there as to how many dimensions you use. But that's kind of the guiding principle here to that hopefully we get a more powerful. We need insane amounts of polycarlo to fill in those five-dimensional distributions. I agree, because that signal is going to be an 11-dimensional ridge. I agree. I agree. So there's a trade-off to be done there, but I think it's not obvious that you should go all the way down to two. Yeah. Can I ask a question about your multivariate CDF and quantiles? So, what if your reference density is different from my reference density? Can we ever agree on the CDF and quantiles? No, and there's actually a huge ongoing disagreement between some statisticians on this point. So, there's this guy, Mark Kellan, who uses this reference distribution. It's a spherical uniform distribution. And there's some other folks in the literature. Some other folks in the literature that want to use the uniform over the cube. And these give different quantiles, and there is a huge disagreement. I've been to many conferences where I see these guys fighting about which reference distribution to use and things like that. So no, you're absolutely right that there's some arbitrariness in the choice of this reference distribution. From the point of view of doing this for sample quantiles, I told you that the sample quantile will be based on this finite grid. You could say that there's some. You could say that there's also some arbitrariness in one dimension when we use the grid 1 on n, 1, n, 3 on n, etc., to define ranks. You could say that I could have chosen some other grid between 0 and 1 and 1. I agree. Yeah, and some folks are using the Gaussian for this, which is not even completely supported. So I agree there's some arbitrariness over there. Yeah, so yes, the last thing. Yes, I was thinking, have you tried doing this in the latent space of an outlet coder? That's a great question. Our physics collaborators who have been working on this, that we've been doing this background estimation problem with, they were thinking of doing this so that we no longer have to use this optimal transport distance between collider events. And instead, we'd find some embedding of those collider events into some Euclidean space so that then we could just use a Euclidean distance between them. Them. And yeah, so this is something that we've started trying to work on, but we don't have results yet. But I agree, that would be a huge computational benefit because it's very costly to do it the way we're doing it right now, hierarchical optimal transport. Questions related to this? So first, can I use it to extend the way a little bit, or does it, you know, can I move to S1.1, or does it quickly become One, or does it quickly become completely insane? Like, can I imagine, if I have two distributions, right? Can I imagine going slightly beyond, or does it somehow very, sometimes interpolations are very bad extrapolations, right? So the question is how stable is it for if I go a little bit beyond, and I imagine the case guy wants to extract a little bit. And the second question, which may or may not be a label, is how sensitive are you to uncertainties for For additional distributions. I modify Ethereum certainly in my institutions. If I change the input a little bit, it doesn't somehow blow up. I think, let me maybe answer the second question first. And I think the answer there is: aren't they similar to what the answer was for Lydia, right? Depending on where you put your basis samples, depends on how impactful your uncertainties are going to be. So, first of all, the notion of uncertainty here is sort of an uncertainty on And uncertainty on sort of the family of functions that get you from A to B. You would have sort of a second parameter in your function that sort of parameterize your uncertainty that if you dial it between, let's say 0 and 1, you sort of get different vectors out. And if you follow along these different vectors, you sort of span the family of the final, let's say, target distributions that you get, right? And in general, how that behaves depends very much on what the structure is of the uncertainty. What is the structure of the uncertainty and of the source and the target distribution? So, I don't think there is any magic bullet here that somehow makes all these things go away, right? This is something that's inherent somehow in the problem. And then the first question about extrapolation. So, you mean like sort of going this way or going that way a bit further? And from a mathematical perspective, it's it's at least you know, in Euclidean space it makes sense, because there you have a vector in, you sort of march along straight lines and nothing stops from taking one more step, right? And that you stop taking one more step, right? In practice, I think it can sometimes lead to strange outcomes. Which is, for example, if you have, let's say, a very narrow peak and you sort of make it be a very broad peak, it's sort of easy to go from narrow to big, but then if you go from big to narrow, you sort of go a bit too far, you sort of blow up, right? Because then you hit that singularity where everything coincides at the same point. And sort of this leads to a more general, so this is sort of one manifest. So, this is sort of one manifestation of, let's say, a more general phenomenon that if you sort of went away from Euclidean space and you sort of go to that situation that Tura discussed, where you have collider events and they don't necessarily live in Euclidean space because there are certain physics and symmetries and so on. So you have a manifold in general in which you can do optimal transport. And then these lines, they continue to be straight, but then they are straight according to the metric on that manifold. So these things are geodesics in the end. And you know that geodesics can have an end. You know that geodesics can have an end. There might be a point where you cannot arbitrarily continue geodesic. You will hit a similarity at some point. So, this is what happens, for example, in this case. But again, it's very popular, problem-specific. You'll see some extra place on Thursday. So, we are doing it in pretty much work. And we all find each other this day. As long as you don't go too far, it kind of works fine. And if you start going too far, basically, at the end of that GOS, it can be run into fault. I think it depends on how close your end point already is to the edge of the magnetic. Already is just of the edge of the manifold. That tells you how much further you can go. Could you go again to the slide where the two side panels are maps? So without knowing anything about optimal transport, I can construct a map on the left that takes the control region, the left control region to a signal region or have a signal region, which is um it's then I can apply it to the right-hand side. Side. But I don't understand. Okay, so now that corresponds to optimal transport. What I don't understand is why if it's optimal for transport, it's optimal for estimating the background of the sequence. It doesn't need to be. It doesn't have to be. It's just an educated guess. You should think of it. So first of all, again, I reiterate, we're doing this in full-dimensional space. So to your former point, it's not so obvious how to do this mapping in full-dimensional space. Yeah, it's an algorithm. But why why should I consider it to have any optimal properties of Optimal properties background estimation, especially when David Andrea talks about, makes a point that you got different components going on. So again, the starting point here was that we wanted to propose a method that made a distinct modeling assumption from the status quo at the moment for these searches. And using a method based on transport of this kind achieves that goal. Now, the question of whether you want to use optimal transport or a different transport map is up to discussion. If a Discussion. If a physicist had information about the problem that would inform a particular choice of a transform map distinct from the optimal transform, they should use that one. But in the absence of a canonical choice, we believe the optimal transform app is a natural one because it is parsimonious. It is the one that minimizes the extent to which I'm moving points from one end to the other. So, you know, you could cook up a transform app that takes points all over the place and lands them over here. But it stands to reason that we would want something that's a little bit more. To reason that we would want something that's a little bit more parsimonious. So it's just a proposal. Ask another question? Yeah, sure. I don't mind leaving. I'll go back to the cue. Go. Okay. So I got a little confused about what the claim was for this ordering. Let's just take a square. Yeah. So it's saying are people who claim they can order the points in the view of the square? They're not claiming that they can order them. They're claiming what the claim is the following: is that if I pick some. Is the following is that if I pick some reference points, okay, so I have this data, these are your data points in the square, let's say, and I pick some reference points over here, and I fit the optimal transport coupling between them. And now I'm just going to define these as the ranks of these points. Okay, that's just a definition. And now the question is, is this a useful definition? And the answer is that it is in the sense that it has allowed statisticians to generalize classical non-parametric rank-based tests, which have a beautiful property called distribution freeness. Beautiful property called distribution freeness, which is a very nice property, but it's kind of become an obsolete topic because it's one that used to be limited to one-dimensional data. And the observation is that this definition allows one to generalize those tests to a general dimension while retaining the same efficiency properties of the one-dimensional counterparts of these tests. And that is the sense in which this proposal seems to be very attractive as a definition of multivariate ranks for statistical purposes. You could come up with a bunch. Physical purposes. You could come up with a bunch of other probabilistic definitions for CDFs and quantiles in general dimension, and that has been done for decades. But those don't lead to tests with the same efficiency properties as one-dimensional counting qualities. I've done a lot of work on combining two p-values. So one p-value is flat between zero and one, the other p-value is flat between zero and one, so the flat at the bottom gets to the flat in the unit squared. And then you want to map from the unit squared to the to the flat in 1D. To flatten 1D unit. And it's well known that this is opposite. There's a zillion ways to do it. Fisher had one eye. And that all also all applies to from 2p values, you get all 1p values, so it's basically use it for any but what I'm saying is that once you have this notion of rank, it allows the construction of test statistics which have certain very desirable properties that used to be limited to one dimension. Properties that used to be limited to one-dimensional tests. And so I'm talking about the construction of new test statistics using this. And I can't go into any detail about this. I'm happy to do that. Imagine a self-revariable transformation in which we transform multivariate space into one-dimensional space using the Westernstream distance as the transformation variable. And you get total discovery distribution. Yeah, maybe kind of combining what Bob was saying. So maybe in a fully data-driven way, it's not necessarily the case, but certainly for the situation where we have template morphing or decluttering, we have a simulator. So there is a true answer. There is a true evolution of the density. So we were mostly limited by computational power. So if we had unlimited computational power, we could just simulate. Computational part, we would just simulate all the different values of our simulator. We would see how the distribution moves, right? And so somehow we're not connecting the physics. But then in this like fully data-driven, you know, where you don't have a simulator that generates these distributions at all, I agree, you know, you need to yeah, maybe it's interesting, but to keep in mind that in many cases there's a true Yeah, no, I completely agree with your point. So I think actually that raised the point. It again goes back to this: like, you have ordering. So the problem you're describing is the one where you have a parameter for the simulator, and then it's a function of this parameter, you get different distributions out of it. But maybe it's actually like, that's more like a kind of a conditional density estimation for instance that you're really interested in the conditional density as a function of this parameter. And that might be, it maybe even leads to things like plus standard. Maybe even leads to things like quasi-stand regressions. That's more like a regression, really. You're regressing on this para picture. And there are actually like optimal transfer form formulations of regressions called quasi-stand regression. So I am using... Victor has worked on that. So I think it would really actually be like a pass-time regression part. I think we could have in fact. But there's still the point that we haven't injected any physics here, right? As long as you don't specify the cross-section. I think that was Lucas's point, right? That was Lucas's point, right? That in principle, this is a method that's solely agnostic to physics. Which, you know, I don't know the answer. Is there a cost function that makes this thing follow the true evolution of the simulators possible? And if so, how does it look like? I don't know. And again, our hope was that by using this cost function, we're somehow getting a little closer to that goal. But of course, even though you're using this physics-informed cost function, that doesn't mean that the geodesic that you end up with satisfies these properties in terms of the motor. Okay, now we need to go to coffee. Okay, now we need to go to coffee. You is this you want to go? Yes. Go ahead. Can you go back to the working slide of uh I wanted to ask if there's plans for some sort of benchmark. So the same way that the media showed a lot of cases, because the case that you show here is a case where, let's say, it's a Gaussian and the mean and the width are changing at the scale time. Does optical, optimal transfer? Optimal transports always give the same answer as, let's say, traditional methods when we know what the traditional method should do. Maybe a little kind of discussion we had before. Is there a benchmarking program in the sense of doing benchmarks and comparing them? So, the case of one-dimensional distributions is very special. And for this case, it turns out that whatever optical transfer is. You know, whatever optical transfer is doing here, the top row is always identical to the horizontal morphing of Lidia, where you sort of average the CDS. So that's because of the CDFs. Yeah. So it will always give the exact same answer as that CDF average. But that's sort of a special case of one. And then sort of the beauty of this is that it sort of provides a framework within which you can generalize the higher dimensions. Where it's not obvious how to do that if you're just with the CPS. Okay, so by construction in one dimension, it will always keep the same as well. Independent of the cost function, actually. Independent of the cost function, actually. So all of those jobs are. The cost function doesn't matter at all. No, it doesn't. In one dimension, it does. As long as it's convex, it'll always be there. We should keep going off.