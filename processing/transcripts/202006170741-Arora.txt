Yeah, it's great. Okay. Yes, good. Okay. Hi, everyone. I am Arshia Rora and I work at Memorial Sloan Kettle and Cancer Center. Today I'll be talking about multi-omic supervised integrative clustering or Mosaic on the mouse castellation data set. So a little background before I said I'm going to use a supervised integrative clustering approach. Integrative clustering approach. So, first of all, why do we need a supervised integrative clustering approach? With the advancement of sequencing technology, and the audience here knows more than that, that we tried collecting a lot of molecular information on patients. So let it be DNA methylation, copy number, single-cell sequencing, flow cytometry, so on and so forth. So, the first step when we have this molecular data work. Molecular data, what we do is that we do some sort of an unsupervised clustering and see to get an idea of the heterogeneity in the population. So that's the first step. But there are, apart from the molecular sequencing, we also collect some other information on the patients. Most of the time, we have the time-to-event outcome or like overall survival and progression phase survival, or we also have the, in a clinical setting, the response to a treatment, which is often. The response to a treatment, which is often categorical. Now, what I am suggesting is that instead of just doing unsupervised clustering alone, how about if we do supervised clustering and try to have clusters which are guided by the outcome of interest, may it be time to event or categorical data? So, Mosaic is actually inspired from SurfClust, which is now available on bioRxive and under review with a journal right now. So, SurfClust actually takes into account time to Actually, it takes into account time-to-event data, but today our main interest is that how can we integrate categorical outcome data? So that leads us to mosaic or multi-omic supervised integrated clustering. So going more into that, I would like to explain a little bit with the simulation example, why do we need such an approach? So let's say this is an example of my typical data set where I have some features that are associated. That are associated with the outcome of interest. There are some features shaded in the blue over here that are molecularly distinct but are not associated with the outcome of interest. And the remaining features are just noise. So the way I'll go about simulating such a structure is that I'm going to enforce a three-class structure, total 300 samples and 100 samples in each three class. And I'm enforcing this three class by different Gaussian distributions. And to have a competing class. To have a competing cluster, I'm just going to simulate a same number of features with three Gaussian distributions, but I'm going to permutate the features. So this essentially breaks the association between the clusters. And then the remaining is just noise. Now, what happens if you run an unsupervised approach versus a supervised approach like Mosaic on such a data set? So we see that if I just do a simple k-means on such a data set, it's not able to distinguish our true. Distinguish our truth, which is these 100 samples in each cluster, really well. Whereas, if I run mosaic on it, it can get the ground truth more cleanly. So, what's happening here is that I have a supervised approach. And in that supervised approach, I'm essentially providing information of the outcome of interest as weights. So, let me show you how the data set changes. So, this is an unweighted data set that went into k-means clustering. K-means clustering. And then, if I do mosaic on it and I'm essentially providing information about the outcome, then we have information about the outcome, then that essentially reduces the noise. And then we have, if you see a big peak of zero here, and that's essentially all the noise has been converted into zero, and then we can do a clean clustering. Since 90% of our features were anyways noise, I think this is a good cleanup. Up. So, very quickly about the mosaic workflow. So, the first step that you do is that you prepare your input data, you pre-process it, you standardize across features, which is really important. And then once you have processed your data, the meaty part of Mosaic is to get these weighted distance matrices. And I'm going to talk in later slides about how I get these weighted distance matrices. But one thing I would like to mention is that you don't need to. That you don't need to have overlapping samples in this data. The different, whether you can have one data or different data types, the different data types can have non-overlapping samples between them. So then you get the weighted distance matrices, and then you combine these weighted distance matrices. In this, I'm just taking the average across the query-wide distances across the weighted distance matrices. And I think this is an unbiased estimate. An unbiased estimate because let's say if a data type has two samples with a very small distance, which means it's very close together, whereas on the other hand, another data type is putting two samples very far apart, then essentially, if we take the average of those two distances, it's somewhere in between the two spectrum. So, once you have the combined data matrix, you can just run mosaic on it. It's a little bit about these distance matrices. Let's say you have to find you have an A and B pair of samples, and you need to find the weighted distance between them. These A and B are measured across P features. So the weighted distance can be given by this equation. And the good part about this equation is that the W here, which contains all the weight information, is a P by P diagonal matrix, where the diagonal elements of this matrix are the weights of the P. This matrix are the weights of the p-features. So, if you take the square root of this weight matrix and you incorporate it up front in your data, then you can just take regular Jacobian distance and that essentially contain the weights or the weighted information according to your outcome in your data already. So, now how do you calculate these weights? I think that's the next question. So, I'm calculating these weights like shown in this slide. Like shown in this slide. So, I'm going to go back to the simulation structure again. As I said, I have three clusters over here, and each weight, each feature, each J feature has a weight for each C cluster. So the weight is essentially cluster-specific versus the population-specific likelihood ratio for each feature for that cluster, for samples belonging to that cluster. So, when we have, so in this case, we're going to. So, when we have, so in this case, we're going to have three weights for each feature, and essentially the final weight for that feature is going to be the maximum weight for that feature for whatever cluster is maximum for. So, I know I'm doing supervised clustering. So, the results that I'm going to show in the next slides, they were all cross-validated. I performed a five-fold cross-validation for 50 rounds for k equals to 2 to 7. So, now a little bit about So, now a little bit about the mouse castellation data set. Oh, before that, I forgot to mention one point. I think I want to clarify this about the weights that let's say if a feature is not associated with the outcome, then this ratio is essentially one. And if you take log of one is zero, so that's how I am, that's how I'm dealing with the noisy features, and that's how we see the shrinkage. Features, and that's how we see the shrinkage effect of non-informative features, essentially. Sorry about that. So, starting with the single-cell mouse castellation data set, I would like to tell you that I've already used SurfClus, which was the time-to-event outcome data previously on a TCGA data set and a flow cytometry data set. So, this is my first time applying this to the single-cell data. So, I would really like some. Data. So, I would really like some feedback on how to deal with this Herculane task of single cell data types. So, as the previous speakers have mentioned, we have these 13 data types measured for about 826 cells. Six of them were from accessibility chromatin data type, and then another six for methylation, and then one for RNA data type. They were measured across different features, and they had variable missing. Had variable missing across them. So if you see RNA here had no missing values, which is one clean data we have to work with. I did one filtering where I just removed greater than samples that had greater than 50% of the features. I removed them. So just moving ahead, I want to show you some results. So first of all, I ran Mosaic with the stage. The stage distribution looks like. The stage distribution looks like this. It was run for all the 13 data types for five-fold cross-validation. So once you have your mosaic output, it is measured across these two metrics, the adjusted mutual information and a standardized pool within sum of squares. Adjusted mutual information is just showing you concordance between two class labels. And higher the AMI or adjusted mutual information, better the concordance is between the class labels. And the SPWSS is essentially a gap statistic, which is a point of inflection. And if you see that point of inflection, that's the optimal K for K that have maximum variation. So in this case, if you see the RNA data is really on the top, which is green in color, all the pinks are methylation data types, and all blues are chromatin data types. So you see RNA is really informative of stage with AMI. Of stage with AMI on top of other data types. So if you see here, it's really maximum for k equals to 5, and so is the point of interaction as also for k equals to 5. So I'm going to show you the mosaic clusters for k equals to 5 for RNA. So if you see here, I have this is the heat map of top 500 features of five different glasses that I picked from Mosaic. And if you see the cross-tab. And if you see the crosstab between the stage, you see that cluster one and two are a little mixed. So I'm assuming these cells might be in a little bit transition. That's why it's a little mixed cluster. But cluster three, four, and five are clean, have clean stages of E4.5, 6.5, and 7.5, sorry, and 5.5. So moving forward really quickly, because I really want to show you integration results. If you see what's the difference between the k-means really, See what's the difference between the k-means really is. We see that if you just do a k-means on RNA data, you will get AMI of 0.34. Whereas, if you run Mosaic, which is target, which is specified for an outcome, you get the adjusted metal information of 0.55. Here, whereas the AMI is really high for lineage, that's because lineage is the dominant feature here, and it's not really honed in for an outcome-like stage. And even in the mosaic solution, the lineage AMI is. Mosaic solution, the lineage AMI is quite high. And I think to some extent that makes sense because the stage and lineage are so correlated with each other. I also ran Mosaic on other 12 data types. And this is the chromatin data type and methylation, six segments of this circle plot. And each ring over here is mosaic cluster. The first ring is mosaic cluster, so there are four clusters here, and the first ring is showing the four clusters. First ring is showing the four clusters. And the subsequent rings are tracking stage and lineage. So, as I said, more AMI is indicative of more information towards stage. So, the highest feature in ACC that is indicative of stage is ACC DHS. And for methylation, it's actually ACC promoter gene body and so on. So, I took the first top five data types that had really informative towards. Really, really informative towards stage, and I integrated them. I did not find many overlapping pitch features between them, and that's really showing that orthogonal information is present in these data types. So I'm really curious to see how integrated results look in this case. So this is the same adjusted mutual information metric that I'm measuring on the integrated now. So integrated is the black curve over here. It's on the top, but it's closely tracking RNA. So more. Tracking RNA, so more work can be done here. And this is only the five data types that I mentioned in the previous slide. So, really quickly, if we see the integrated crosstaps between stage, RNA, and lineage, we see the AMI for stage with these four clusters is 0.53, whereas for lineage is 0.33. So, that's actually finding more solutions that are really honed in towards stage in this case. So, then I very quickly So then I very quickly, same story here. I also ran this with lineage, just this with lineage with these five class types, and RNA is really outperforming the other data types. And again, I'm just saying since it's honed in towards lineage, the AMI is really high and it's really targeted towards that. So very quickly about the conclusion slide. Mosaic finds supervised clusters with an outcome if you have an outcome in mind. Outcome, if you have an outcome in mind, where k-means might give us mixed results. And you also, if outcome is also correlated, supervised clustering is much more efficient and it helps in sorting out different signals. Mosaic can run with missing data, and that's how I ran with it in this case. However, interpretation should be made carefully. And I'm actually looking for feedback here when I wanted to impute this data. I saw that a lot of research there out there to deal with missing data. To deal with missing data. And I was not sure which is the right approach to deal with it. So I'm open to suggestions. Mosaic reduces computation space because it reduces, since it deals with weighted distance matrices, it's reducing the sample by feature to sample by sample. It is efficient in dealing with noisy features, as you saw in the simulation example. And the code to mosaic is available on my GitHub repository. Finally, there is a lot of There is a lot of work to be done, as I said, imputation of missing data. The stage has a temporal relationship, and I think we've discussed this before in previous talks. So perhaps I can have some sort of an ordinal relationship. Really curious to see if I can model some sort of a joint modeling approach of stage and lineage. And I think there is some room of improvement for integrated to improve the some references. Thanks and questions. Time some questions. Thank you, Ashi. So, we have one question from Emrit who asks if Mozai can perform either verbal selection or some verbal importance. Yeah, you can. It's not really targeted for variable importance yet. For analysis of this data type, I just took the top 5,000 variable features, wherever data type had more than 5,000 features. Features, but uh, yeah, it's you can go down and see the weights how they're looking like to get an idea since it's based on a likelihood approach, but it's not targeted towards any feature importance yet. Is there any way you could change the distance depending on the type of data? I mean, they have different distributions. I mean, all the modalities have you know different distributions and everything. Yeah, so um. Yeah, so I strongly recommend that you standardize your data, like do a standardization across features so we have a good continuous data to deal with. In case of our data, I know we have a lot of proportion data, the 12 data types that of methylation and accessibility, they had proportion data. So I actually used the folded square root transformation and then standardized them so that I can use them for calculating weights. Can use them for calculating weights. It's not really optimized for binary or categorical data right now. Yeah, but it uses Euclidean distance because it has good properties to use multi-dimensional scaling noun string. Okay. Do we have any other questions from the floor? I may ask another question about how the missing values are handled. Yeah, so for now. Yeah, so for now, I just kept the missing values in there yet, because even though I did have a version where I imputed the missing values, but since these data types, especially with the ST and MT-seq data, they are from such different modalities, I was not sure, really sure what's the good imputation approach. So I just left them there like that with missing data. So they ignored. When they're missing, they ignored? Yes. They're missing the ignored, yes. Okay, yeah, yeah, great. Yeah, um, yes, um, I think, um, yeah, you could have a discussion with Al. We've tried also to weight this different weight also on the features. He also tried some supervised analysis. So, yeah, definitely scope for discussion here. Thank you very much again for your talk. We're going to have 10 minutes break.