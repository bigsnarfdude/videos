Okay, fine. So, yeah, so my talk today is going to be on some open questions that are related to the law of proximal brand conjecture, but I really think that among these open questions, some of them are quite fundamental and should have wide applicability. And so I'm excited to share this with you. But we'll start with the Lauda Proxima Rank indicator itself. It deals with It deals with the randomized communication complexity of a function, which I'm denoting as RCC. Epsilon is just an unspecified constant. Let's think of it as one-third. And the conjecture says, the conjecture is based on this fact that for every single communication function f, the randomized communication complexity of f lies between the logarithm of the approximate rank and the approximate rank itself. Now, this first, the lower bound is quite simple. It's just it follows from the fact that if you have a communication protocol of cost k, it gives you a matrix of rank at most 2 to the k, which approximates your function f. And in fact, it's slightly stronger than this. This decomposition is a non-negative decomposition. So you could actually replace this with a non-negative version of approximate rank, and the lower bound will continue to pull. Lower bound will continue to hold. The upper bound, at least the most elegant upper bound, was from this recent paper of Antara and Rizwan. And it says that there, well, it basically states the upper bound, but it's again stronger than this as well. Because as stated right now, all you need is a randomized communication protocol, but the protocol that they give is actually a deterministic one-way communication protocol. So it's a much stronger result here. But it doesn't give us. But it doesn't give us a better randomized communication protocol that has, say, square root of approximate rank cost. So, in the deterministic case, we know that there is a deterministic protocol around square root rank, but in the randomized case, this is still up more. And the conjecture itself says that the cost is polynomially related to the logarithm of the approximate graph. This is what the conjecture says. And this was refuted a few years ago. We showed that there's a function for which There's a function for which the cost is actually polynomially related to the approximate rank without any logarithms. And if you look at the function, the particulars, it shows that the cost was at least the fourth root of the approximate rank. So the main question, the first question that one could ask here is where, what is the best upper bound on the cost? Can you get a square root approximate rank, or can we get a lower bound that's better than the fourth root of the approximate rank? And this is the Right, and this is the first open question that we'll be seeing in this talk, and there are some interesting questions related to it. And the second question is about the non-negative approximate RAM. So it's true that if you give me the logarithm of the approximate RAM, we can't tell what the communication cost is going to be just based on that. And it's also true that if you give me the approximate non-negative rank, we can't tell based on that either. But if you had told me what the maximum of the approximate rank Of the approximate rank of the function, sorry, the non-negative approximate rank of the function, and the non-negative approximate rank of the negation of the function, is that is still open. It's possible that the randomized communication complexity is characterized, it's polynomially related to the logarithm of the larger of the two non-negative approximate ranks. So that's a natural open question here. Can we refute what I will call the approximate? It's already been conjectured before, but It's already been conjectured before, but to just give it a name, I'll call it the log approximate non-negative wrap condition. So to get to what these open questions are that I claim are fundamental, let's first see how we can actually come up with a counterexample to the plain log approximate random conjecture. And the idea is quite simple: think of this as your input space, all the inputs to your function. Input space, all the inputs to your function lie over here. And let's take some subsets of this. I'm calling them g1 to g5 here. And we can also think of these subsets as functions. So g1 here is the indicator function of the subset g1. If x lies in this, then the function outputs one, otherwise it's zero. And if we take the or of these functions, of these gi's, since these gi's are disjoint, you can actually write it as a summation. So even though we're writing it as a summation over real numbers, the function f that Real numbers, the function f that results from this is a Boolean function. And this is really good for coming up with a function with small approximate rank, because since rank is a sub-aditive measure, the approximate rank of f cannot be much bigger than the approximate rank save t. It's at most like t times that, ignoring some small error-related corrections. And the log approximate rad conjecture would imply that the cost of computing f would be something like Computing F would be something like log t times the cost of computing g. But it seems reasonable that we should actually need omega t communication to solve this because there are t different functions. I mean, it sounds like an omega t lower bound should be able to come up. It's not that simple though, because if this was deterministic communication, we can actually prove that you can't get an omega t buster. You can show that it will always be polym log t. That it will always be polym log t and cost of computing g if this was deterministic. So, the question is now: how do we actually show a lower bound for this function f? And this by itself is a quite interesting function. It's been studied a lot, what's the communication complexity and core of functions. But generally, in those settings, these GIs are acting on different inputs. And if they're acting on different inputs, then these GIs cannot be destroyed. So, this is an interesting variant of this source. interesting variant for this source. Now, so how do you show this lower bound? So to start with, we'll be working with query functions. So we'll look in the query world, we'll find functions that are just sums of simple query functions. And the simple query function, an example of it, is a subcube or a subspace indicator. And subspace indicators are easy because we're looking in the setting of randomized priority discharges. Our eventual goal is to take this function f and Take this function f and show that f composed with XOR is hard for randomized communication protocols. But we're going to split it into two steps. First, show that f is hard for randomized parity decision entries, and then show that f composed with XOR is hard for communication protocols, because that way we can get two papers instead of one paper. Now, a randomized parity disc entry is just, it's exactly what it sounds like. You can take parities of your inputs and query those, and you can think of that as just making a leap. And you can think of that as just making a linear query in the space f2 to the n. Now, this is the counter example that we used. So it was a function that we called sync. And it was basically you take a complete graph, you have one input bit on each edge of the graph, and the input bits are going to, based on their values, it's going to orient the edges of the concrete graph. And we're asking if there is a sink in this graph. There is a sink in this directed graph. And we can see that only one of these vertices can possibly be a sync. You can't have two sinks in a complete graph. So it's actually a summation of these functions. One function is asking is V1 a sync, the other one is asking is V2 a sink, and so on. And these functions are all very simple. They're basically subtlest. That's asking, are all the edges coming out of B1 0? Each of the functions is something like that. Like that. And it's also simple in the sense that each of these functions are only looking at a subset of the bits. It's a function of a small subset of the bits. So first we'll go over how we come up with the lower bound here, and then we'll see can we extend this lower bound to settle some of the open questions that we had earlier. And the lower bound here is at a high level very simple. So in case when we're looking at randomized priority disintegrations, we would be caring about subspaces. For communication, we'll care about rectangle, but both. Communication, we'll care about rectangle, but both of them follow really the same framework, so you can focus on whichever you want. And it starts with this observation that in order to compute this function, the structures that we have, I'll just stick to subspaces for now, you should be able to come up with small, with large subspaces. That's a subspace with a few, which comes from only setting a few parities that is biased against B1 being the same. Against V1 being a sync. If you can't come up with a subspace that's biased against V1 being a sync, then you're not making any progress in being able to tell whether the function value is one or zero. And if you actually come up with such a subspace, you can prove that if you look at your subspace, but restricted to only these bits that are relevant to V1, then that restricted subspace cannot be maximal. So that's what I mean here by slightly small. If you look at, say, the end. If you look at, say, the entropy of the inputs in that subspace, it can't be n minus 1. There has to be an omega 1 loss. So n minus 1 is just the number of edges coming out of here. So it has to be n minus 1 minus omega 1 if you're being biased. And if you want to actually solve all of sync and not just the question, is v1 a sync? Then the same property must hold for many of your vertices. So if you're actually following, if you're actually solving sync, you're finding subspaces where Finding subspaces where, for many of the vertices, your subspace restricted to those bits have less than maximal entropy. And in the final step, we use Shearer's Lemma because what we've got till now is we've got a subspace for which many of its restrictions have submaximal entropy. And Shearer's lemma says that all these sub-maximal entropies are going to add up. And it happens because each of these, so if you look at the bits related to V1 and the bits related to V2, they share. And the bits related to V2, they share only one edge in common, so they're nearly disjoint. And because they're nearly disjoint, all these sub-maximal entropies add up, and so it's going to actually have far less than maximal entropy. All right. So we'll be seeing this again in a slightly different context, but this is all the background that's needed. Now I'm going to start with getting to our other problems. Are there any questions here? So the first one was: Can we do better than the fourth root of approximate rank? And this is in work done with Archie and Dunkit. And here, we're going to use this very nice structure called a subspace design, which was used as a pseudo-random object and it's been very useful in coding theory. And it's got a quite simple definition, if you parse it correctly. Parseparately. So it's a set of subspaces such that, so that's here S1 to SK, such that if you take any small subspace, so W here has small dimension, then W should only intersect a few of the SI's. So in a sense, it's a set of subspaces that is very well spread and that it's hard to hit if you're trying to hit it with small subspaces. Quite straightforward definition. And if we take the duals of these subspaces, so I'll be taking the duals of SI's and the duals of W, although I'll be still using the same letters. So if you take the duals, what you get is a set of subspaces such that for any large subspace, that is, it has small codimension, it doesn't, if you look at how this subspace biases SI, so you're changing the universe from your two to the end inputs to just the subspace W. Subface W. And when you do this, W doesn't notice any difference from the universe. The size of SI inside W is the same fractional size as the size of SI inside the whole universe. This means that it's not able to tell anything about the same. And this is exactly what we wanted. This is saying that if you've made only a few queries, then for most of your subspaces here, you've not been able to tell whether you're biased against it or not. And so this immediately gives you a randomized priority to say. gives you a randomized parameters in pre-roll one. So it's a very simple class of functions. Take a set of subspaces that forms a subspace design and ask is my input in any one of these. That's already hard for randomized parity to synchronize. And so the obvious open question is when we compose this function with XOR, can we find a communication robot? And there are very efficient subscribes designs that are known to exist. Designs that are known to exist, and if you use these, um, these are all randomized probabilistic arguments, so we don't actually have explicit self-based designs. But if you use such a sub-space design and prove an omega and lower bound against it, then it actually would improve the gap that we had. It would improve the lower bound and say that we can actually get cost as large as the cube root of the approximate map. So, if you think about what we said on the previous page about the randomized parity dissenting, Page about the randomized priority disintegrating heart, we've kind of answered this question, but in the randomized priority disintegrity world. And now the only question left is: can we lift it and make it the similar result in the communication complexity world? As of now, this is still very much a communication complexity problem. It's not the fundamental question I said it was. But this is a natural question that we can ask. Can we get an analog of Shearer's lemma for subspace delights? Because that's what worked for us in our earlier. Work for us in our earlier example. Now, it's worth noting that for these subspace designs, you kind of have to so you've got these subspaces: S1, S2, S3, and so on. And you define the subspace by telling what priorities are set in it. And so, if you look at the number of priorities set, If you look at the number of priorities set, priorities that are set, I'm not used to writing links, so this is weird. You can see that it's omega n priorities are set here, omega n priorities are set here, and so on. So if you look at the overlap between S1 and S2, this is also going to be quite large. You can't have all the overlaps being just one bit like it was earlier, if that even makes sense. Because these are subspaces, we can't talk about bits in the first place. But if you were to like, Place, but if you were to like force yourself to talk about bits, you still couldn't justify saying that these share only one bit in common. So, the shears lemma setting really isn't applicable here, but nevertheless, we do have a lower bound in the randomized pipe PDCMP case. So, clearly, there's some sort of share-like result that's coming out of this. And so, the question is: the conjecture that's actually fundamental is this one. If you take a distribution. This one. If you take a distribution over 0, 1 to the n that satisfies, okay, so first I have to describe what A restricted to a subspace means. And that's quite simple. So S1 has some number of parities that are set. In the case of a subcube, there were some bits that we were looking at. And so when you were restricting to a subcube, it made sense. Just look at those bits. And in the case of substances, there are some priorities you're looking at. So again, it's quite natural that if you're restricting. Quite natural that if you're restricting a set to a subspace, you're just looking at the parities of the bits and you're ignoring the rest of the input. And since there are codimension of T many parity set, this entropy can be at most co-dimension of T. And now we're asking, if a distribution A satisfies that for most of these subspaces, it actually does lose some amount of entropy, can we say that the overall distribution has to have lost a lot of entropy? Distribution has to have lost a lot of energy. The exact analog of what Shira's lemma would have given us. And yeah, it's quite fundamental. There's no communication complexity. There's a nice notion of spread out subspaces and projection. It's very natural. Any objections? So, what if you just take A to D is random? Great. So, if A is random, most likely you wouldn't get Most likely, you wouldn't get random. I think this would automatically fail. Your coordinates, the entropy of the Adriatic skips to the subspace would actually be large. Here, the coordination. Yeah. You wouldn't get like a omega-1 separation for this. If I include every point, what would you want to call it? So, good question here. So, I have a question. I'm over Zoom. Am I misreading your notation or misunderstanding something? Because if T is the entire space, the co-dimension is zero and something is weird. All right, so if T is the entire space, I was confused about this, but there isn't a problem. I remember that it wasn't a problem. Oh, yeah. So if T is the entire space. So, if T is the entire space, then the T's are not coming from a subspace design. Okay. It took me a while to figure out how to plastic. So, it really is co-dimensional and not dimension. Yeah, so this A restricted to T is a set of bit strains of length coordination of T. Sorry, just on that, couldn't you add the full space to any subspace design? I mean, it wouldn't be interesting because you just counted it as one of the H. I'm still confused. Okay, that's fair. I think I'll have to break this down to get up. Think you know? Can we come back to this in the end? I would like to put dimension there and stuff. So if t is a whole space, there aren't any constraints. So whatever you're right. Wait, if I was to mention that's like the maximum energy. All right. So it's the fact that this is saying you're restricted to the whole space, and that sounds like you're not restricting it at all. Sure, yeah, I'm not restricting. Sure, I'm not restricting it, and I'm saying I'm using concepts. Yeah, but then I think that's a problem with the notation here because if T is the whole space, this should just be the string of length zero. According to my definition of if you take the whole space as your space here, then you're really not setting any heights. Sorry, yeah, it's fine because you just ignored that. It's fine because you would just ignore that sub like you you can add the whole space to a subspace design but it but it wouldn't be interesting because you're just not included in the menu and in the in in the list of subspaces yet to satisfy so it'd be fine sorry about that go go ahead on the other s that's all right i'm i'm still confused so it's fine if you are um yeah we can come back to this at the end we'll it's it's a fascinating question but yeah this But, yeah, this ends the first open question that I had. And there's this second open question, which is not really related to subspace designs, but it's related to the same goal of getting a better separation. So consider this simple function that takes in, so this is a query function, but it takes two n bits as input. We split it into n bits that we call x and n bits that we call y. And it outputs one if and only if x is a cyclic shift of y. X is a cyclic shift of Y. So this is X. If you cyclic shift it by 2, that's just pushing every two bits forward. And this also shares a property that Singh had, that if n is a prime, then only one of these i's, so if n is a prime, then barring some edge cases, x can only be one shape. Edge cases, x can only be one shift of y. So basically, the claim is that if x shifted by i is equal to y and x shifted by j is equal to y, then that means that x shifted by j minus i would be equal to x. And this is saying that x is periodic. And if n is a prime, the only periodic inputs that are there are the all zero input and the all one input. zero input and the all one input. You just can't have a period other than that. And so this also shares the property that it has really small approximate rank. And it would be interesting to prove an omega n lower bound for the randomized parity decentry complexity of this, but we don't even know that. So this isn't a subspace design. And we had some trouble trying to figure out its randomized parity decentry complexity. So this, I think, is a very interesting open question. And I should mention this was. And I should mention this was inspired by another function which had a small approximate rank because of the same periodic type of property. But I felt that this was a neater formulation of such a function. Okay, so in the last five minutes, I'll show you another open problem, which is related to the approximate non-negative RAM. So the problem that we had in our function sync was that sync was indeed a sum. Sync was that sink was indeed a sum of simple functions, but the negation of sync was not. The negation of sync was a very complicated thing if you wanted to write it as a sum. So we can rectify this easily in three steps. So what we'll start with is we'll take our full input space of 0, 1 to the n, and we partition it into the ones of the function and the 0s of the function. And we try to ensure that both of these are actually partitioned. Partitioned into a few subcubes. So that's the first step. And yeah, then we just define f saying that if it's on this side, it's one, and if it's on this side, it's zero. So now the good thing is that f is a sum of simple functions, and the negation of f is also a sum of simple functions. So all that we need to do now is to prove that it has large randomized parity decidity complexity and you can't. Because any such f turns. Because any such f turns up, it has a fairly small randomized pirate, the same complexity, and this follows from a result of these people. I really don't know how to pronounce that. Does anybody? So the statement is that if f inverse of zero and f inverse of one can become covered, not even partitioned, but covered by just c subcubes, then you can actually find a small size decision. You can actually find a small size decision tree computing f. And if you can find a small size decision tree, you can actually find a small cost randomized covering decision tree. And this sounds quite similar to some results in communication complexity, like P is equal to N P intersection to NP, but there are some differences. And so this rectification didn't work as long as both parts were unions of a few distrained subcubes. But if you replace subcubes by subspaces, If you replace subcubes by subspaces, we don't know that this direction won't help. So it's possible that you can actually have these being subspaces, and F could still be hard for randomized parity disease entries. And my conjecture is that it isn't possible, but it's still interesting to see how one can go about proving this. The negation of the statement has a particularly nice form. Nice form, which doesn't deal with functions. It just says that if you partition the full Boolean hypercube into subspaces or affine subspaces, I meant affine subspaces throughout this talk. I'm just used to calling them subspaces. Then there, yeah, so the negation of this is exactly stating that there's an efficient randomized parity based in p that computes the function on input x. It tells me which of these subspaces does my input lie in. Very simple function. Very simple function defined based on a partition of the Boolean hypercube. And the question is: are all such functions easy to compute for randomized paragraphs? And the fundamental conjecture that comes out of this is my conjecture that not only is there an efficient randomized parity design entry, but there is one that comes from a small size parity design. So if you have any partition of 0, 1 to the n into subspaces, then you can actually find a tree-like partition that refines it and doesn't have too many more parts. That's the same as saying that there's a small randomized, there's a small size parity. From that, you can find a small randomized parity. And there's been analogs of this in communication complexity. We know that if you can partition matrices into rectangles, then you can actually find real partition. And it's a very interesting question to ask. The same two per subspaces in the Boolean header. And that is my talk. Thank you for listening.