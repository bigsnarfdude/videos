I'm delighted to be here today, although, of course, I'm sure it would be nicer to be there in person, but I appreciate the opportunity to speak from here. So, what I'm going to be talking about today is an extension of an idea of doubly robust regression-based estimation to a high-dimensional setting. This was work that has been conducted as part of his doctoral research by my student, Chris Bian, who's been co-supervised by Sahir Batnagar, with some input. Batnagar with some input from my collaborator, Susan Shortreed, and another substantive collaborator from the School of Nursing, Suzy Lambert. So what I'll be talking about today is precision medicine, so treatment, effect, heterogeneity, and how we can leverage that heterogeneity from a statistical standpoint. And then I'll talk a bit about what is known as D-Walls or dynamic weighted ordinary least squares, which is a really appealing. Squares, which is a really appealing, intuitive, easy-to-use method of estimation that relies on complexity scores as well as an outcome regression model to perform a form of doubly robust estimation. And then I'll talk about how we can incorporate different forms of selection, in particular penalization to really target a more interpretable and easily implemented treatment strategy, as well as how we can implement confounder selection because sick. Confounder selection because selecting confounders is a somewhat different problem in that selecting confounders is not really in the traditional domain of selection, I would say, that we are not really aiming to get at prediction performance, but rather we're aiming to get at unbiasedness or causal performance. That is, we're trying to select variables that will help us reduce imbalance. So it's a slightly different objective to the traditional sort of selection that. The traditional sort of selection that we do. And then I'll finish off by briefly looking at a case study of a web-based stress reduction tool that was put out in a pilot of 50 cardiovascular disease patients. If at any point there are any questions, please don't hesitate to interrupt me. This is my first time giving this particular talk, although of course there are elements from old material. So if anything, Old material. So, if anything is unclear, please don't hesitate to ask. So, I always like to start with this quote, which is from not so, so very long ago from Sir Austin Bradford Hill, who I think is particularly well known for his work in medical statistics on, well, in causal inference and indeed his criteria, if you could call them that, for causal assessment. But he's also given some other really important lectures. Some other really important lectures in which he talks about a number of different things. And this one I particularly like, where he says the medical statistician recognizes and is familiar with the pros and cons of that difficult question. Should a fixed dose be given or should it be allowed to vary with the apparent needs of each patient as judged by the clinician? And I would argue that that's not quite the question I want to ask. I don't want to have the clinician making those judgment calls, but rather I think what Calls, but rather, I think what we would like to do as part of evidence-based medicine is provide some statistical input from large volumes of patients, large amounts of data, to try to say how should those doses vary. And in the causal inference literature in particular, this is a bit of a change from the usual question. Much of the causal inference literature is focused on average treatment effects. So, what happens when we marginalize over the covariate distribution in a population? Covariate distribution in a population. There have been other questions that have been looked at. Our last speaker, for example, was looking not so much at averages but at tails. We can look at quantile treatment effects. But again, these are typically done in a way that is averaging rather than conditioning on covariates. We may actually want to condition on those covariates to see if there's treatment-covariate interactions or treatment-effect heterogeneity and leverage that to target the treatment towards. That to target the treatment towards individuals. So, when I'm talking about precision medicine, I'm referring to tailoring medical treatment, or it could be other forms of intervention. This could be an educational intervention, a preventative intervention, to the individual characteristics of an individual. This doesn't mean to the individual level down to a unique person. I'm not trying to, for example, take a biopsy of a tumor. Take a biopsy of a tumor and try out hundreds of different chemotherapies to find the one unique drug for this patient. But rather, I'm trying to classify individuals into subpopulations who will differ in their response. But I may want to subclassify on a very large set of covariates. This could include genetic or genomic markers. It could include different physical characteristics, biological characteristics, lab measurements. But what we're trying to do is But what we're trying to do is find that sweet spot in the middle of not too much treatment and not too little, right? The Goldilocks level right in the middle that gives the most therapeutic benefit for the least cost and the fewest side effects. So I'm working in a setting where we have many different treatments or doses available. I need to try to choose between these. And I may work even in a longitudinal setting. So patients may be switching from one treatment to another over time. Another over time. And the settings that I tend to work in, not exclusively, but largely, are settings where we have confounding. So I need to account for the fact that clinicians are making decisions about their treatment as a consequence or in reaction to the patient's condition. And of course, there may be other factors too, such as the physician's habit or their large or small hospital, things like that. But there is a confounding element to this. So I then need to. So, I then need to try to learn from the data I have what information is useful from a patient's profile, whether that's at the macro level of clinical characteristics or the micro level of genetics or genomics, to try to determine which patient to give to, or sorry, which treatment to give to a patient at a particular time in the course of their disease. So, when is this useful? Well, it's useful when patients do be It's useful when patients do behave differently, when they respond differently to the different treatments that they are given. So, one particular example, and not the one I'm talking about today, but one where I do a lot of work is in the treatment of depression. In depression, there are over 20 different antidepressant drugs that have been licensed for use in the United States. They all have moderate, modest average effectiveness, but there is a huge variety. But there is a huge variety in how individual patient responds. And so there is this large variability or heterogeneity in response that is not well understood and hasn't been well explored. So this opens the door to asking, well, are there specific patient profiles that might benefit more from one treatment than another? Different patients respond differently. Can we figure out why? Another reason why we might want to adapt treatments is. Want to adapt treatments is if patients differ not only between you know one patient to another, but the patients themselves may evolve over time. So their response to a treatment might change over time. That could be because we're talking about a chronic disease, which is a relapsing and remitting condition, or because the types of therapies that we are given, giving to a patient, the patient may adapt to those treatments. So, for example, pain medication, sometimes a patient adapts to it and requires a higher Adapts to it and requires a higher dose, or they may have a viral disease, a chronic viral disease, such as tuberculosis or HIV, where there may be a developed resistance to the drug that they're giving. So we need to think about how the patient is responding over time. We may learn a lot from how a patient responds to a given treatment so that what we give a patient now tells us how to treat them in the future. And indeed, if we can do this, And indeed, if we can do this, find the right patient or the right drug to the right patient as a match, this could improve compliance. It could avoid side effects. Again, avoiding treatment fatigue, which reduces compliance, and avoiding higher costs, either to the patient or to the healthcare system. But we also want to avoid under-treating because we want to make sure we're hitting that right therapeutic target. So, in the simplest form, So, in the simplest form, we can think of adapting treatment as something we do when we have a treatment-covariate interaction. So, if we look at this figure here, what we see on the very far left is a situation where I've got some outcome y. I'll assume this is quality of life or something where bigger is better. And I have some covariate along my x-axis. This could be a symptom score, for example. And I have two treatment levels. And I have two treatment levels that are represented by the solid and the dashed line. So we can see on the far left here that there is no interaction between my treatment and my covariate, my symptom score. No matter how many symptoms a patient has, the difference between these two treatments is constant. And if bigger is better in terms of my outcome, then this plot here tells me I always want to prescribe that solid level treatment. It is always better to do that. To do that, if I shift over to this figure in the middle, I do now have a treatment-covariate interaction, but over the range of the symptom score observed in my population, there is still no need to tailor the treatment. The solid line treatment still dominates. At the lowest level of treatment, it's not as useful as at the high level, but there's nothing to suggest that anywhere across the range of my population, Across the range of my population, it's better to give the dashed line treatment. It's only when we have what's known as a qualitative interaction, where these lines actually cross over the range of X that I've observed, that there's a benefit to tailoring the treatment. And so what we see here is that at the high level of symptoms, my solid line treatment is better, but at the lower level, it's the dashed line treatment. And where those two lines cross, that's the threshold that tells me which treatment. The threshold that tells me which treatment I should prefer for which patient. So, these variables that exhibit this qualitative interaction are known as prescriptive or tailoring variables. So, when I apply certain types of estimation procedures, what I get then are adaptive treatment strategies. These go by a number of different names, but the idea is that I am adapting the treatment that I give based on these tailoring variables. Tailoring variables. Okay, so I adapt it to the patient in front of me, and I might need to change that and repeatedly change the treatments or adapt my treatments over time. Now, there's been a huge interest in estimation and inference for these adaptive treatment strategies, both in statistics and the computer sciences, for almost two decades now. There are a number of different estimation strategies. Some of them are regression-based, so familiar forms of regression, or for those Forms of regression, or for those who know this field, Q learning is probably the most commonly applied. It's a form of sequential regression. G-estimation is a term mentioned often, but used rarely in practice. What I will focus on today is the D-Walls approach. But then there's an entirely different class of estimators that I won't talk about today, known as value search estimators. These tend to work on a classification basis rather than a regression basis. Rather than a regression basis, they have some advantages over the regression-based estimators, but they also have significant drawbacks, which include that they are much harder to communicate with clinicians, and they're generally a little more complex to implement. So the goal of the research that I'm describing today is to really talk about how we can do this selection. Most studies collect a huge volume of variables. If we are helping somebody Of variables. If we are helping somebody design a study, we would tell them to collect anything they could possibly dream was relevant. But it's often the case that the many variables collected are not all useful. Certainly in this case, they may not all be useful for tailoring treatment. They may also not all be useful for reducing confounding bias. So different variables may have different purposes, and some of them, it turns out, may not be useful at all. But we do want to be selective. But we do want to be selective in the variables we use because if I have a simpler treatment strategy, it can improve statistical efficiency. I'm not wasting my data estimating parameters that are really zero. And it also leads to treatment rules that are more practical. If I can tell a doctor that they only need three variables to determine the optimal treatment, it's a lot nicer than telling them they need to come up with a risk score, for example, of 20 or 30 variables. It's easier to implement. Variables. It's easier to implement, easier to understand, benefits all around. So, what we wanted to do here was introduce methods of variable selection to choose both the most important confounders, but also the critical tailoring variables in an adaptive treatment strategy analysis within the doubly robust framework of dynamic weighted ordinary least squares. All right, so I'll start out with the single stage setup so that you can see just. Single stage setup so that you can see just how straightforward this is, and then show you how we extend it to a multi-stage or longitudinal treatment setup. And then I'll talk about how we incorporate the penalization and the selection of these two different types of variables, the tailoring variables and the confounders. So in the single stage setting, I have a very simple framework. I have some pre-treatment covariates X, a treatment received that I'll denote by A, and an outcome Y. That I'll denote by A and an outcome Y. And X could include a huge number of variables. Some are confounders, some are tailoring variables, others may be irrelevant. So if I have this simple single stage setting, I only have one treatment decision to make, and I'm going to focus today just on a binary treatment decision. So A will be a 0, 1 variable. I could think about proposing a simple outcome model, a model that includes A model that includes this part in red here, a contrast function, sometimes called a blip function. This involves anything to do with my treatment A. So the main effect of A and any treatment covariate interactions. And then of course this component over here, which is really the main effects of all other covariates. Certainly I want to include the main effects of anything that interacts with treatment. I may also want to include additional covariates in that blue component. Additional covariates in that blue component. Okay, so I've broken this outcome model into two pieces: the blue piece, which we can think of as the impact of patient history at the zero level of treatment. If treatment is, you know, placebo control, that would be at no treatment. If I have two active treatments, that would just be at whatever is the baseline or the zero-coded treatment. And then in red, I have the impact of the active treatment on the outcome. So its main effect. On the outcome, so its main effect and interactions. And simply in specifying this outcome model, right, and focusing on that contrast, this piece in red, I can learn about the optimal treatment strategy. Again, if I'm assuming bigger is better in terms of my outcome, then what I want to do is maximize this component in red. So if treatment is coded 0, 1, then this is maximized whenever. Whenever the thing that I'm multiplying by treatment is positive. If the thing inside the brackets here is positive, then I maximize my outcome by assigning a equals one. If what's inside the brackets is negative, then I maximize my outcome by trying to get rid of that negative quantity. So I take a to be zero. And through that, I end up with a very simple linear decision rule, which simply says whenever this linear combination is positive, I give level. Combination is positive, I give level A equals one, otherwise level A equals zero. So it's giving me this simple linear decision rule. Okay, so moving forward, I'm going to assume that I have correctly specified this contrast function. Now, what dynamic or ordinary weighted ordinary least squares or D walls does is it takes that outcome function and it's going to make it more robust. One of the things we One of the things we worry about a lot in causal inference is whether we can correctly specify our outcome processes, because those are usually biological processes that we may not understand well, and they are almost surely not linear. Okay, so what we want to do is make our analysis more robust, and we often do this through the use of a treatment model or a propensity score, which I'm showing right here. So with that propensity score, I want to create some weight. I want to create some weights. And I can do this in a variety of ways as long as they satisfy this balancing condition that I'm showing you here. And this balancing condition is satisfied by a number of different weights, including weights that we know of as the inverse probability of treatment weights. So our very familiar standard causal weights. We also get this condition satisfied when we use what Michael Wallace and I originally called the absolute value weights, but they are now more. Absolute value weights, but they are now more commonly known as overlap weights. And there are a handful of other weights that also satisfy this condition. So, as long as we satisfy this condition, we can use these weights in a standard regression framework to achieve doubly robust estimation. Okay, so D-Walls is great because of that. It's doubly robust. It's easy to explain to clinical collaborators because we're working in this regression framework. There have been a number of extensions. There have been a number of extensions. So, we can do this DWALS type approach with multiple treatments, so multiple categorical treatments with continuous doses. We can handle censored outcomes. And being regression-based, we have a lot of nice tools. So, we have standard residual diagnostics, as well as other model validation tools. And we have a package that implements a number of these options, although not all of them. There are some challenges and limitations, though, one of which is that we have somewhat limited results for discrete outcomes at present. And to date, we don't really have any nice data-driven variable selection methods. So that's what we aim to address here today. But first, you're going to say, well, that all looks awfully easy. So what's the challenge with adaptive treatment strategies? And the main challenge comes into the multi- So in a multi-stage setting, we go from this one treatment interval setting to having several treatment intervals. And for those of you with a causal background, I need to emphasize that this is not a causal diagram. This isn't a DAG. I'm showing the flow of information, but if this were a true DAG, I would have many, many more arrows here. Okay, so the idea is in this case, for example, I have three different treatment intervals. Prior to each, Intervals. Prior to each interval or each decision, I have measured covariates. Okay, so these covariates again could be confounders, they could include tailoring variables, they could include irrelevant variables, and then I have some final end of study outcome, where again, that outcome could be something like a time two, it could be an accumulation of events over the course of the study, but I will assume for now that bigger is better. Is better. So, again, what I want to do is try to estimate this, but I need to be careful, right? Because if we think about sort of a standard wisdom in, for example, a randomized trial setting, we know that we shouldn't condition on post-randomization variables. The same principle applies here. If I'm trying to learn about A1, I really don't want to condition on anything that lies between A1 and Y. One and why. So I need to be very careful in how I implement this. And the way that we do this is through a backwards induction or dynamic programming approach. So what we do is try to reduce this complex multi-stage problem to a single stage problem, which simplifies implementation, but I will briefly mention how it also brings in additional theoretical challenges. So to make this a single-stage problem, I'm going to start at the very end. I'll start about learning the End. I'll start about learning the last treatment to see what's optimal there. And I'm going to make this look like a single stage problem by collapsing everything prior to the third stage treatment into what I'll call H, or my history. I can take whatever pieces of H are relevant and I can fit a single stage problem. Easy peasy, I know how to do this with my weighted regression. But now I need to learn about stage two. So, what am I going to do then? So, what am I going to do there? I want this to look like a single stage problem. So, it's easy enough to collapse everything prior to treatment level or treatment A2, my second stage treatment. I can collapse that all into a history, no problem. But how can I create an outcome that is fair, right? An outcome that will not force me to condition on anything that happens after A2. Well, what I do is I create what's known as A. Well, what I do is I create what's known as a pseudo-outcome. And this pseudo-outcome is actually an estimated outcome. And it's an estimate of what is the best possible outcome that I could have given the observed history of H2 and A2, but assuming that each individual was treated optimally at the final stage. Okay, that's what I want to do. And then everyone's on an equal playing field. Whatever happened to them up till now, we'll treat them. To them up till now, we'll treat them optimally thereafter. And so we don't need to condition on their later treatment because everyone will be treated according to the same, hopefully, optimal estimated rule. Okay, so that tilde term, that y tilde, is the key to this backwards induction. It's called a pseudo-outcome. And the way we calculate it is pretty straightforward. If what I estimate to be the optimal treatment in stage three is the treatment a patient actually Is the treatment a patient actually received, then their pseudo-outcome is just their observed outcome. They got the best treatment they could have, nothing for me to do. If, however, what I estimate to be their optimal treatment is not what they received, then what I'm going to do is take their observed outcome and add something to it. And specifically, what I'm going to add is that contrast function, that gamma function that tells me how much better the patient would have done under the optimal treatment strategy. Under the optimal treatment strategy. So, anyone who we think was not treated optimally, we bump up their outcome a little bit and we do another stage one analysis. And we can do the same thing at the very first stage in this three-stage example. So we just keep moving backwards, creating pseudo-outcomes along the way. Now, simple to do, simple to, fairly simple to explain. Statistically, this comes with a number of challenges, one of which is that Of challenges, one of which is that in doing this pseudo-outcome, we actually end up putting parameters inside non-differentiable functions, either within maxes or within absolute values. And as soon as things aren't differentiable, the math gets ugly, right? We can't rely on standard asymptotics. We end up with estimators that are not asymptotically normal. We have some approximate fixes to this, but it does lead to an estimation procedure that is not totally trivial. Totally trivial. Okay, so it's not bad, but it's not quite as straightforward as it appears. Okay, just to quickly cover my bases, because I've sort of focused on the how-to, but I haven't told you what we need to do this. We do rely on some identifiability and estimation assumptions, including SUFA. So, the standard assumption that my treatment is there is only one version of it, and what other people receive. Version of it and what other people receive doesn't affect me. I'm affected only by my treatment. I assume that the data contain everything I need. No unmeasured confounding, no measurement error, no selection bias. I assume the positivity assumption so that, you know, for any combination of covariates, both levels of treatment are possible. And then I'm going to assume correct model specification of some quantities. I assume my contrast is correctly specified. I assume my contrast is correctly specified. And I also need to assume that at least one of my treatment-free models, the part in blue, or my propensity score model, are correctly specified. Finally, because I'm fitting an outcome regression model, it turns out I need to make the assumption of strict hierarchy. Okay, so what you tell a student as soon as we get beyond one or one variable regression, we say, ah, if you've got the interaction, make sure you put the main effects in there. Put the main effects in there. It turns out we actually require this. We often tell our students it's a good idea. The D-Walls procedure requires it. So, if there is a covariate that's a tailoring variable, we need to include it as a main effect. And so this last quantity is probably the most important when it comes to any form of variable selection. We need to take into account that we can't simply select our treatment variables and our main effect variables or And our main effect variables, or sorry, our tailoring variables and our main effects completely independently, we need to preserve that hierarchy. So, how are we going to do this? Well, we wanted to do something fairly straightforward and certainly build on the existing body of knowledge in variable selection out there. So, we wanted to use something like Lassell with an L1 penalty and an objective function that looks something like this. So, my objective function includes. Objective function includes a squared error loss function. So I've got my y minus my treatment effects, my main, sorry, my main effect of treatment, my main effects of covariates, and these treatment-covariate interactions. Okay, and I want to penalize both the main effects. I don't need millions of X's in there, but I also need to do some penalty on my tailoring variables so that I can simplify that tailoring rule. But just using this, But just using this as I have it here doesn't assure that strong heredity, right? There's no connection between the size, which are my tailoring variables for the interactions, and the betas, which are my main effects of the covariates. So I need to find some way to tie those together. And we accomplished this by reparametrizing the squared error loss function. So we've reparametrized it in such a way here that the A way here that the beta is now a parameter that is also in front of my interaction terms. Okay, so now what I'm going to do is I'm going to penalize this tau function and the beta, but you can see that anytime a main effect of a covariate is shrunk to zero, that will also wipe out the interaction so that it's not possible to have a variable appearing here. A variable appearing here as an interaction, but not here as a main effect. Okay, I have tuning parameters, two of them. I have lambda here, which controls the degree of penalization, and I have this alpha, which really controls how much I'm penalizing the interaction versus the main effects. We usually, at least in simulations, what we did is we estimated or we chose the tuning parameter lambda by. The tuning parameter lambda by cross-validation. Alpha we kept fixed in our simulations, but we did some little checks, and it didn't make a huge difference to bias and variance. It did make some difference to our false positive or our selection rates. And it turns out that this approach can be performed sequentially. So, in the same way that I was doing a sequential estimation in a multi-stage setting, I can do sequential estimation with penalization starting at the last. Penalization starting at the last interval and working our way to the first interval. Okay, so that's how we select our tailoring variables with this penalized D-Walls procedure. Okay, but again, how do we do that cross-validation? We need a selection criterion because I'm not actually assuming normality or working within a specific likelihood. I'm just working with a loss function. What we found worked well was what is known as a value information. Known as a value information criterion. So we're selecting our tuning parameter based on an estimate of how well our population performs or how good their average outcomes are when they are given our estimated optimal treatment strategy. This value information criteria performs well. It performs better than a BIC-like estimator. But it has to be noted that this is really only estimated. This is really only estimating the value or the expected outcome under an optimal treatment strategy if my propensity score is correctly estimated. So there's a little bit of a mismatch here because I'm using a doubly robust procedure, but my selection is based on something that is singly robust that says I must get that propensity score right. So I'm going to be really quick. I know time is running short, so I don't want to cut too much into your lunch time. To cut too much into your lunch time, but just to say, performance is really good in simulation. We did a number of simulations in what I would consider a moderate or modest scenario where we had not very many covariates, you know, 20, 50. But even in really difficult settings, so where we had 400 covariates and a sample size of 200, we still had excellent accuracy in terms of selection and the treatment recommendations. But what we Recommendations. But what we did find is that the selection induced some bias. And so, if we want to really do a better job of estimating those parameters in our treatment model, we recommend refitting. So do the selection and then refit on the reduced model to avoid that bias caused by shrinking variables or shrinking coefficients of variables that we retained in the model. This is something that's well known in the penalization literature. Okay, so we figured out how. Okay, so we figured out how to select our tailoring variables. Should we do the same thing for confounders? The answer is no, right? As I mentioned at the start of the talk, the goal of selecting confounders is to achieve unbiasedness or to reduce confounding. The goal isn't to get phenomenal prediction, right? So I want to achieve balance in the covariance between my treatment groups. The propensity score is one way to do that. Okay, so my goal when I'm doing propensity Okay, so my goal when I'm doing propensity score building is to create balance. I don't need an amazing predictor of treatment allocation. That might actually induce positivity and make my estimation worse or less efficient. So I don't want instruments in there for sure. So what we did was we borrowed a method proposed in 2017 known as outcome adaptive lasso. And the idea in outcome adaptive lasso is much like a standard adaptive lasso. Adaptive lasso is to do an initial fit of an outcome regression model and take the estimated coefficients from that outcome model as my weights in an adaptive lasso. Except here, I'm taking the outcome model weights in an adaptive lasso for my logistic regression of the propensity score. So, what this accomplishes is it says any variable that has a big impact on the outcome, I'm going to make it harder to shrink it out of my. Going to make it harder to shrink it out of my propensity score. So I'm simultaneously taking into account the relationship of a covariate with the outcome and the treatment. So I'm really taking into account whether it is a confounder and not simply very predictive of either the outcome or the treatment. And what we found again in simulation is that combining this outcome adaptive lasso with the penalized D-walls really improved the accuracy of our value function estimations or expected outcome. Function estimations or expected outcome under the optimal treatment strategy. It also slightly improved accuracy of the tailoring variable selection. It didn't make a big difference there, but probably that was coming from the slight gains in efficiency. All right, so just briefly to discuss our stress reduction analysis. Cardiovascular disease, like many other chronic conditions, has the potential to create significant stress. To create significant stress, we actually looked at a number of different cardiovascular diseases. So, really, that should have an S on the end of my CBD. So, my collaborator Sylvie Lambert and I, we conducted a pilot sequential multiple randomization trial. So, we actually had several stages of treatment to look at the feasibility and potential effect size of a stepped care approach. As this was a pilot study, we had only 50 patients, which sounds like a reasonable number, but because we Like a reasonable number, but because we had multiple randomizations, it actually didn't leave very many in the specific different arms or pathways of treatment. So, what we're focusing on in this analysis is just the first randomization. Okay, and in that first randomization, I'm going to look at the website only group, which is A equals zero. And I have a typo here that should be a one. My active treatment here, my A equals one group, is the website plus a weekly telephone. Website plus a weekly telephone coaching group. So it's a more aggressive intervention. Okay, so A equals zero is website only, A equals one is website plus individual level coaching, which is more expensive, of course. And our goal was to minimize a depression and anxiety scale in these patients. Now, you're saying, well, where does this causal business come into it? Why are you doing confounder selection? We measure tons of things on these. We measured tons of things on these individuals, and what I'm showing you here in red is that in this study, we actually did have some imbalance in some covariates. We had many more covariates. I'm just showing you a selection here. But we did have some imbalance in the physical health of our participants. We had some imbalance in terms of education and employment, in terms of some of their comorbidities, particularly obesity. Obesity. You say, gosh, Erica, did you do a terrible job in this study? No. As with any randomized study, if it's small enough, you will see some imbalances in some variables just by chance, and that's what we see here. So the question is, can we ignore it or should we treat these as confounders? Because clearly these are covariates that could affect a patient's stress or depression. And while they didn't affect our treatment allocation, didn't affect our treatment allocation, they are uneven between the treatment groups just by chance. So we applied the two methods that I showed you earlier, the penalized D-walls with the outcome adaptive lasso to select covariates into a propensity score. Of the 16 potential tailoring variables that I showed you on the previous slide, seven were selected as being important to tailoring, including baseline depression, age, physical. Age, physical component scores, and so on. And so we get an optimal rule that looks something like this. What we found is under our estimated optimal rule, the number of symptoms or the symptom score was about 14.8. Okay, that's not great. It's not terrible. But if we compare that to treating everybody with one treatment or Everybody with one treatment or the other. So, if we gave everybody the always coaching, so everybody gets the most expensive intervention, the website plus the individualized coaching, their DAS score was at about 15. So, interchangeable, really, with what we saw. A little bit worse, but it's significantly more expensive, right? So, they're a little bit worse, and we have the Bit worse, and we have the extra burden or the extra cost of paying for more individuals to get this one-on-one coaching. If we gave everybody the simple website only with no individual coaching, the average DAS score was 18.5. So we're seeing quite a few or quite a bit more symptomology of depression and anxiety. Now, if the covariate imbalance was ignored and I just said it's randomized. Ignored, and I just said it's a randomized study, forget it. I'm ignoring that. We get a slightly different optimal rule, which yields a slightly worse symptom score. Okay, so it makes a little bit of a difference, but not a tremendous difference. So these adaptive treatment strategies or tailored treatment strategies, I think, have enormous potential to improve clinical practice in a way that is actually based on data. As much as I love my clinician, I don't know how many people she sees who are. I don't know how many people she sees who are exactly like me with my particular patient history, my particular family history. So, rather than having her make guesses of who is most like me from her patient pool, I would love it if there was some large resource of patients and an analysis that could help guide those decisions that she makes. Okay, but we do need lots of data to learn about these adaptive treatment strategies. We're trying to learn about interactions, they are generally Interactions, they are generally not well powered. So, for that reason, observational data tend to be attractive. They tend to be cheaper, it tends to be easier to get a large sample size, and often it's a more generalizable or less selected population. But of course, many of the large observational databases we get come with many variables we don't need and confounding structure that we may not understand well. Trials will give us the randomization, but even then, The randomization, but even then, balance could be imperfect. And we often have more measurements or more variables than we really need. So there's a lot of work to be done here. Just a few examples. You know, I want to investigate the criterion that we're using for a cross-validation. You know, it doesn't really make sense to use a singly robust criterion for a doubly robust approach. If we don't think we can get that propensity score right, why on earth would we? Propensity score, right? Why on earth would we use something that assumed we did to do our cross-validation? And probably one of the most important directions to pursue will be extending these to a discrete outcome setting because it is very, very common for the measurements that we have in these sorts of studies to be discrete. You know, was there remission or not? Even the DAS, I mean, we treated it as continuous, but it isn't continuous. It is a count. So I think I've almost made it. So, I think I've almost made it right on time. I will just acknowledge that all of the heavy lifting in this talk was done by Chris. Any mistakes that have cropped into these slides are my own and not Chris's. This is the sort of second big project in his doctoral work. Again, the co-supervisor, Sahir, my collaborator in Seattle, Susan Shortreed, and Sylvie Lambert, the nurse who is really the pioneer of these adaptive stress interventions. And this work. Interventions. And this work was funded by NIMH, the FRQ, and CIHR. Thank you so much.