Let me start weighted connectivity documentation. It's the problem I will be talking about in this talk. About in this talk. So, what is the problem about? So, in this problem, we are given some graph, vertices, and edges. So, that's the black graph that I'm talking about here. This is what we denoted by G. Vertices and edges. Some graph, and we'll denote by k the edge connectivity of our graph. So, our graph is k edge connected for some integer number k, which will be at least one somehow. And then we are also given some. Given some extra edges. These are these dashed blue edges here in this picture. And I will often call these extra edges these dash blue ones. I will often call them links to distinguish them from the edges of the graph G. So talking about edges, I would usually mean the edges of our graph G here. And the best blue edges are the links, I will notice it lengths by L. And these links come together with. Come together with they have costs, they have the non-negative costs, which are given by this cost function C here. So these are the red numbers next to the links here. And so our task in this weighted connectivity augmentation problem is that we would like to increase the edge connectivity of our graph. So if we had a K edge connected graph before, we would like this to be a K plus one edge connected graph. And we can do this by adding some of these links to our graph. And we want to do this at the minimum cost. So we want to find the minimum cost. So, we have to find a minimum cost of the lengths that we can add to our graph in order to make it K plus one edge connected. So, increase the edge connectivity by one. Good. Okay. So, this is the general weighted connectivity augmentation problem. And we'll talk about algorithms for this. And the first thing that we should observe when talking about this problem is actually, if we want to solve this problem, or we want to find approximation. This problem, or we want to find approximation algorithms for this problem, we don't actually have to look at this very general problem, but it's actually sufficient to look at the special case of it. Namely, it's sufficient to look at the special case where the graph that we are given is just a cycle. So we don't have to deal with arbitrary graphs, but if we can solve the problem for cycles, then we can solve it for general graphs. So it's an alpha approximation algorithm for cycles where we just have this cycle with this two-edge curve. Have this cycle with this 2-edge connected and want to turn it into a 3-edge connected graph by adding some of the links. And we'll actually have an algorithm for the problem in general. And the way one can show that it's sufficient to look at these cycles basically follows from this tactus representation of the minimum cut. So this is a result by Dinets, Katanov, and Lomborov, from already quite a while back. Already quite a while back. So they showed this so-called cactus representation of the minimum cuts of the graph. And the minimum cuts of the graph play a particular role for you because, of course, in this problem, we would like to increase the edge connectivity of our graph by one. So this basically means for all the min cuts of our graph, where we don't have k edges right now, we need to add one additional edge covering this cut. So we can think of connectivity augmentation also as the problem of cutting. Also, has the problem of covering the minimum cut of the graph by links that we are adding. And that's why this min-cut structure is relevant to you. And in the weighted setting, one can show that this using this character structure, one can reduce the problem to a cycle. So this means that the remaining part of this talk, I will always focus on a problem, but we are just having cycles, and we would like to augment this to a sphere. Yes? In the cactus representation, though, you don't only have cycles, right? You also have this. only have cycles, right? You also have this restructure. Yes, so the yeah, yeah, maybe you can sketch this. So in a cactus, you can basically think of, this is basically like kind of a tree of cycles that are somehow attached to each other. So you can have some cycles like this. And the idea is basically what what you would do with this is you would turn this into have the cycle here. Let me mark this this vertex here in red. This vertex here in red, I will do just like I would have two copies of this red vertex. I would just keep the other edges like this, and I will connect them by a link, of course zero. And this then gives you an equivalent instance. So with this idea, you can kind of turn the cactus into a cycle by kind of unfolding it. And this way you get two cycles. But you can also just have trees, or you can have trees dangling off of it. Are you just repeating? Are you just recapitulating? Actually, in this case, you were kind of getting like two technical ways. That's yes. Any questions about this? Okay, good. Then, yeah, let me say if you would about approximation algorithms, what do you did we know about this? The approximation algorithms for this problem. So, what has been known for a long time are That's been known for a long time are true approximation algorithms, and there are various ways to do this. So, there are some specialized techniques for this problem, but then also these general techniques for network design problems work. So, for example, primal dual gives you a two approximation, and of course, like this integrative rounding algorithm, say Jane's algorithm, would also give you a two approximation, as for many of these networks and problems. And yeah, so in this, also in this weighted connectivity augmentation problem, like for many. Connectivity augmentation problem, like for many of these network design problems where this J technique works, we have the situation that we cannot get the true approximation easily by these standard techniques, but getting below a factor two can then be fairly difficult. And actually, prior to this work, we didn't know any approximation algorithm better than two for the general problem. But a lot of work has been done on special cases, also by little people here in this room. So, for example, one special So, for example, one special case that has been heavily studied is the tree augmentation problem. So, tree augmentation is just a special case where the graph we are starting with is a tree. You know, I'm having a tree instead of a cycle. You can think of this as tree augmentation as being the special case where our edge connectivity is one, or you could also say special case with the edge connectivity is odd, but this is a special case of the general connectivity ausimation problem. Connectivity auspication problem. An unweighted tier just means that all the links that we're having, they have all cost one. So for unweighted tree augmentation, there's been a long line of work where the commonly best approximation ratio is 1.393. And then there was also unweighted connectivity augmentation problem. So this is then the unweighted special case of the general connectivity augmentation problem. There we only got the two special There we only got the first better-than-two approximation algorithms in 2020 by Arnold Booker, Fritz Anbouni, and I put Straval Em√©lie. But now the best known approximation algorithm is also here, 1.393 approximation. And then even more recently in twenty twenty one, together with Rikhut and Closen, we got the first better than true approximation algorithm for True approximation algorithm for the weighted tree augmentation to then commonly best known approximation ratio is 1.5 plus epsilon for an arbitrarily small epsilon. And in this talk today I want to talk about getting this 3 half plus epsilon approximation ratio even for this more general weighted connectivity augmentation. So problem. So this will be Problem, so this will be a generalization of this weighted tree augmentation algorithm here, which gets this three-half positive trial approximation. So that's the result I want to talk about. And so let me start before getting into details, giving you a very light, probably a bit break, but a brief idea of what the general approach will be that we'll be looking at. Be looking at. And so the main ingredient of our algorithm, the main two ingredients, are first of all that we'll be looking at some kind of directed version of the problem. We'll explain in a bit more detail later. But you can always see here in this directed version of the problem, the links will not be undirected anymore, but they will have somehow directions. And you'll see later on that this directed problem has somehow very structured solutions. Problem has somehow very structured solutions, and this will be very useful for us. And then, besides introducing this directed version of the problem, the second main ingredient will be that we'll be looking at exchange steps, where we are given, say, a direct solution for a problem, and then we would like to exchange some of the directed links by a set of undirected ones. So, here you can see in this picture, we throw away these orange directed links. Throw away these orange directed links, and we would somehow replace them by the green ones. And the idea is that we will somehow exchange links and hopefully, in some sense, make our solution cheaper. I will get to explaining in detail what this exchange step means, this directed solutions. And then so one way of using these kind of ideas to get an approximation ratio better than two, although not the half-plus epsilon. Half plus epsilon, but something better than two would be then that, as we'll see in a moment, we can obtain two approximate directed solutions, which are somehow very structured. And then we can, in some greedy way, imply such exchange steps, where we exchange some directed links by undirected ones, and thus improve on our true approximation and get something better than two. So, using starting with a directed solution and Starting with a directed solution and then doing edit exchange improvement steps would be one way of getting to a better two approximation for this reactor's epsilon. They need to be a bit more involved algorithm, but this is somehow the high-level idea. And the main focus will be now on discussing what are these direct solutions, what are like the structure behind them and what are these improvement steps and how can we implement these exchange steps. These exchange steps. Okay, yeah. So now, yeah, let me start with talking about this directed version of this weighted weight augmentation problem. So the main difference between the directed version and the undirected one, well, the first one, of course, our links will be directed now, not undirected anymore. And now we have to talk about what is the solution to this problem. Have to talk about what is the solution to this directed weighted ring augmentation problem. And what we will do in the directed version is that we will fix an arbitrary root vertex R, just any one, and then we will try to cover all the minimum cuts of the cycle. The minimum cuts of the cycle look like this, this can see here. This is like a two-cut, these are like these intervals along the cycle, cuts that contain just two edges of the cycle, and we'll always represent them as a set not containing. Represent a real set not containing the minus site. And in the undirected version of the problem, we wanted all to cover all these two cuts by links. A link just covered this cut whenever there was a link like crossing the cut. And now we still want to cover all these two cuts by links, but now a link, directed link, is only counting as covering the cut if it's entering the cut. So if they're leaving the cut, I'm not covering it, just they have to enter. So that's the best. Enter. So that's the event. Same as before, we still have to cover all the cuts, but now only entering links. So it's covered. Okay, and once you introduce this directed version, this actually gives you a very simple way of getting a true approximation for this problem. So what can we do? So what we could do is we take our instance, we simply replace every link by two directed links going the link. Two directed links going in the two opposite directions, each of them having the same cost as the original link. Now it turns out that this directed version of the problem is solvable in polynomial time. So one way of doing this is you write down the standard LP values and will turn out this is integral. This is one way of doing this. We come to a different way of doing this later. There are a couple of ways of solving this, but one easy one. Solving this, but one easy way is to write down VLP. And now that we solve this directed version, we can just forget about the orientation of the links and get the solution to our undirected problem again. And this is at most by a factor of two more expensive, because in the worst case, an optimal solution to the directed problem can be twice as expensive for the undirected problem, because we might have to fire the link in both directions. So that gives a very simple truth approximation for this. For this. Now we would like to get now these, yeah, we've seen this directed solution there, and these directed solutions are a bit messy to work with. So we introduced one concept which we call shortenings of a link. And so shortening of a link is here, as you can illustrate it here in this picture. So if we have this blue link here, which is going from there to Blue link here, which is going from there to there to this here, this is directed link. Then we'll call this red link a shortening of the blue link. And a shortening, you can think of this as just like moving the tail here closer to the head of the link while not going along the side where the root is, but in the other direction. And the point of this shortening is that the main idea is that whenever this shortening is entering a two cut here that we would like to cover, then this That you would like to cover it, then this two-cut will also be covered by the original link itself. So this means that the shortening is just a weaker version of my original link. So this also means that I can, whenever I have a link in my instance, I may assume that the shortening also exists and it is not more expensive than the original link. Because if in the end I install the shortening in my solution, I can always replace it by the original link and still have a solution. A solution. So we may assume that all these shortenings exist, and that's something we'll do. And once we assume that all these shortenings exist, it turns out you can only move the tail. Yeah. So it's important because also, for example, if you move the head somewhere else, so if I have moved the head, like I would have something. Would have something. This is my original link. And I would look at this link here, for example. This link here would cover the two-cut, which just contains this one vertex here, which obviously the other link, which had a different head, didn't cover it. So this can't work. But if you just move the tail, you can actually observe that every toothpaste that is covered by the shortening is also covered by the original link. But the creative modes are very controversial. Yes, we're moving away from the root, that's good. Yes. So that allows us to assume that all these shortenings exist. Yes. Okay, so we'll, in the following, always assume that all these shortenings are part of our instance. And the nice thing about this, the main reason why we're doing this, is that once we have all these This is that once we have all these shortenings in our instance, we can prove that there always exists an optimum or that, yeah, we can look at solutions that are non-shortenable. And non-shortenable means I cannot take any link and replace it by a shortening while still maintaining a solution. So I take a solution, I can shorten the link, make the one link shorter, you still have a solution, I would do this. Or so maybe you can remove the link. So in some sense, these are kind of some kind of. Googling, so in some sense here, these are some kind of minimal solutions. And one can show that these non-shortenable solutions have a very nice structure. So you can see a picture here on the left. So you can always see that this looks like a very special kind of solution. Actually, non-shorts of the solutions will always look like this. Namely, they have the property that they are an aborescence rooted at our root R, that's the first property. First property, then they are planar in the sense that if you embed this thing in the canonical way, by say drawing a straight line between the two endpoints, if you draw the whole thing as a cycle, then this will be planar. So you don't need any cross-segment links here, and that is the reason for this. And the other thing, property that we have is that every vertex has at most two outgoing lengths. Going links, and namely, one that is going to the left, and the other one that is going to the right. And this is something one can prove. So one, yeah, I mean just one simple example where we have, say, two links here going here in the same direction from this vertex, then what I could do is I could replace this link here. I can show that I can replace this by this shortening here. By this shortening here, this length by this shortening, and they together still cover the same cuts as the original two lengths. So, this would be just one example for establishing one of these properties. And yeah, one can show that one can actually look at these cases, these things, that one can actually show that we always have all of these properties, non-tropical solutions. So, this means once we look at these directive solutions, we always may assume that we have this very nice structure, which will Have this very nice structure, which will be very helpful in describing our exchange steps later on. And by the way, this also gives you a way of solving this directed weighted ring augmentation problem exactly. Told you before. So basically, you can compute a minimum weight aborescence once you added all the shortenings and just to solve the problem. So that's another way instead of solving the LP. Okay, good. So that's what I'm saying. Good. So, this was all I wanted to say about these direct adsorptions for now. So, we have these nicely structured direct adsorptions, and we can get this nicely structured direct adsorption, which costs at most twice as much as an optimum solution. That's what we need to remember for now? Okay, so now we want to get back to our improvement steps. We want to exchange directed links by hopefully two percent uh of undirected ones. Of undirected ones. And the first question that we are going to ask is: if I add a particular set of these green undirected links, then we have to ask the question, which of the directed links are we able to draw? So there might be different possibilities, so we will fix one particular way of doing this. And we would also like to get a better understanding of this. We add a particular set of undirected links, which directed links can I drop. links, which directed links can I drop in order to analyze the algorithm and in order also later to find, be able to algorithmically find good exchange steps. So if you found an optimal directed solution, there are examples where two is the right type. Is there a simple one? If not, don't worry about it. Basically Basically something like maybe you could do something like here is the root, you have some link here of cost zero, and then you have some other link of cost one, and then you need to have the optim solution has cost one, but you need to have one enterprise link here and here, which each of them has cost one. Each of them has cost one. So that's the cost two. I guess good, okay. So this is now what we will focus on next. Given a fixed set of these undirected links that we are trying to add to our solution, let's try to understand. Our solution, let's try to understand which directed links can drop from our solution. Okay, and so this is not, there is not necessarily a unique set of links that we can drop. There might be different possibilities, and we will fix one particular possibility. And the way we will think about this is that we will introduce the concept of PET responsibility. Cut responsibility. So, what we will do is, if we have our direct solution for each of these two cuts, we will make, there might be several links in our directed solution, the two orange ones and the red link that are all covering this cut. And we will make one of these links, these directed links, be the one responsible for covering this cut. And so, the formal definition of how we define this cut responsibility is Is that we would say this link here, this red link, L here, is responsible for this cut C here, this link from U to B is responsible for this cut because first of all, of course, it should be covering this cut. And the second condition here is that no directed, no link on the path from R to U would also cover this cut. And so, for example, this orange link is not responsible because if I look at Responsible because if I look at the path from the root here to the tail of this orange link, then there is another link, namely the red one, that is also entering the cut. So the orange link is not responsible for covering the cut. And using the structure of these aborescences, of these directed solutions, one can actually show that this means that now when we define this for every cut there will be one unique responsible link. Responsible link. So I will denote direct solution F and the directed link L this set R of L will be just a set of all the two cuts for which my link L is responsible. Can you explain why the right orange link is not responsible? Yes, because if I look, I mean this is entering the cut, but now I should look at the path in the aborescence from the root to the tail of the orange link. And on this path, And on this path, there's the red link. So this is, in some sense, yeah, the red link is the one which is in the adorescence closest to the root windows that's covering it. So we are always considering paths which I will not talk about. And we are looking at paths in the adorescence. In the evorescence. And then we have paths. And that gives us a unique link that's responsible. So this way, now for every cut, we fixed one. Now, for every cut, we fixed one unique link which is somehow responsible for covering it. Okay, and now we are able to define what we call the drop set of a component. So, if I have a set of green links, I want to define a set of links, directed links, that I can drop from my solution once I added the green links. And the way we define this is simply that we say a directed link can be dropped from my Link can be dropped from my solution whenever every cut that this link was responsible for is now covered by one of the undirected links that I added. So every link in some sense has a list of cuts for which it is responsible. And if all of them are covered by the new component that is added, then the link can go from the solution. Because then we will, in this way, we'll make sure that still every cut is covered if it's. But still, every cut is covered by 12 piece links. And after when I give it all the cuts that I was responsible for, every cut there will be one link that's responsible for. And if this wasn't covered by my new component that I added, then responsible link is still there. Oh, yeah, a mixed solution just means every link, every cut is covered by a directed or undirected link. So an undirected link. So an undirected link just covers counts whenever it just crosses the cut, and the directed link counts as covering whenever it's entering. And yeah, so it's still a solution in that sense, that every cut is covered. Okay. Now this was yeah, this was like the way we actually define these drop sets. It turns out that this definition with the spec responsibility is not the most useful definition to work with in the analysis of the algorithm. So we also give some characterizations of this. So if we are looking at some components, so some set of undirected links that we would like to add. Undirected links that we would like to add to our solution, and we have our directed solution here, and we would like to understand: can we drop this link from U to E here? And so here, the gray vertices here on the right side, these are the descendants of the vertex B in the blue abores. And they are forming an integral here. This is no coincidence, this comes from the structure of these solutions. So the grey ones are the descendants here. The descendants here. And then what we can do is we can look at what we call the link intersection graph of this component. So what this means in this link intersection graph, we have a vertex for every link in our component. So for every undirected link here, we have a vertex. And these two vertices, so for example, the green vertices corresponding to the green links here, they are connected. To the green links here, they are connected by an edge because the corresponding links here are crossing in a geometric sense. Or we also have an edge, for example, between the dark green and the red edge here, because like here the vertex endpoint also counts as intersecting. So whenever the links are intersecting in some geometric way, we're having the same endpoint also count. We have an edge here. So this is the link intersection graph. And then one can prove. one can prove that the vertex, this link from u to v is in the drop of this component, so it can be dropped once we add this component, whenever there is a path in this link intersection graph from a link incident to the vertex v. So this means here from this bound vertex to one of the link that is incident to one of the right To one of the right vertices over there. It's one of the vertices that is not the descendant. So, for example, the green link here would be incident to one of the white vertices here, and we have a path from the brown vertex, which corresponds to a link incident to V, to a white vertex, to this dark green vertex here, which is incident to white vertex, so vertex that is not a descendant. That is not a descendant of V. So, this is one characterization that turns out to be very useful. And one particular one thing that one can derive from the designer, so we basically is the following. Because one could one could write this out, but it turns out that But it turns out that this is somehow more useful to work with in the analysis of the algorithm. Or actually, maybe the next lemma will make it a bit more clear what's one useful consequence of this. So one consequence of this is if we look at a link set K, which is connected somehow in the link intersection graph, for example, these two green links, they're always like kind of everywhere. Links, they're always like kind of somehow they are intersecting. If I would like to understand what's the drop of this set of green links here, then I one consequence of this previous lemmer is that all the links, if I want to find a set of links that I can drop when adding this green link set here, I can look at all the endpoints of my green links here. These are the red ones. And now the link. And now the links that I could potentially drop for my solution are links that are entering one of the red vertices. So I could potentially drop links that are entering one of my red vertices. And what the previous lemma implies is that I can actually drop almost all of these links, except for maybe one. Namely, if this red vertex here, for this one, I cannot drop the end incoming link because this Incoming link because this red vertex is the least common ancestor of the other red vertices in this abores. And this basically comes from the previous lemma, because all these red vertices are somehow connected to each other by this link intersection graph. And the previous lemma basically told us we can we can drop all drops the n incoming link here unless all the other red vertices Unless all the other vertices here are descendants of this vertex. That's where the previous lemma somehow comes into play. So this somehow tells us that if you look at this component, we know that we can drop all of the incoming links into these red vertices, except for possibly one, namely the one entering the least common answers to. And that's the only thing we need to be careful about. And that's the only thing we need to be careful about. And this somehow turns out to be a very useful characterization to work with also algorithmically. Basically, we just, if I have some, also if this k is not connected in the link intersection graph, I can just look at the different connected components. And then basically, for every component, I just need to look at which vertices does it connect. I just drop all the entering links, except for possibly one name of the release common answers. Possibly one that we will release common access to in this directed double essence. And this turns out to be the way more useful characterization to work with algorithmically. But yeah, we can show that this is equivalent to this concept of the cut responsibility, which was maybe more natural to introduce the whole thing. Yes. So one clarification. So if this link intersection graph had many components, then for Many components, then for each component, you'll not be able to drop one edge as in. Yeah, it might be that for each connected component, yes, yes, exactly. That happens. Okay, so yeah, this is how we define these dot sets. So, once we now So we now kind of understand, once we fix one set of green links, one possible set of links that we could add to our solution, which links can we drop. But now, of course, the question is, we know if I decided for one green set of links, I know which links I can drop. At least we have some fairly good understanding of this. But now the question is, of course, what would be a good set of green links to add? So which one should I add? Lengths to add. So, which one should I add? And this is, of course, useful to know if I decide for this one, I know what is a piece somehow for my solution. Okay, so that's the next question we'll try to answer. And so we will try to add here some link sets. And we'll not allow to add arbitrary link sets here, of course, because the best thing we could do, of course, is somehow try to say, add the optimum solution. They add the optimum solution, and we could drop everything. And this is not something we'll be able to find algorithmically, so we'll somehow restrict ourselves to a reasonable set of possible sets of green links over which we will be able to optimize efficiently. And the kind of sets that we allow here as these green link sets to add is what we call a thin component. And there are a couple. And there are a couple of basically equivalent definitions of this. Let me start by explaining one quite geometric interpretation of this. So I will now introduce the thinness, the concept of the thinness of some set of undirected links. So here is a set of undirected links, which might be a contemporary component. And we would now like to define what is the thinness of the set. And the way we define this. And the way we define this is that we look at triangulations. Namely, we have our cycle, and then here we introduce these black vertices somehow, which are always between two consecutive original vertices. And then we can look at triangulations of the convex hollow of the black dots, basically. So here is an example of such a triangulation. And then we will Uh and then we will count will count uh the number of links that are crossing the sides of these triangles here. And we will say that a link set is alpha thin if there exists a triangulation here such that each line here in the triangulation, each side of such a triangle, is crossed at most by at most alpha many lengths in k. A different way of working. A different way of looking at this, you could also think of these lines here in our triangulation. You can interpret them as two cuts of our graph. So, for example, this red line here, this corresponds to the cut here, this red cut here, separating the vertices from V4 to B7 from the rest. You can also kind of see geometrically. So, in that sense, you could also interpret this triangulation as an inclusion-wise maximal language. And inclusionwide maximal laminar family here, where basically each set is the union of two smaller sets. And all the signals are contained in there. This would be a different way of looking at these triangulations. So we could also say that they exist, inclusion raise maximal lamb in a family, such that every cut contains at most alpha many links here. That's the very same definition. And another definition, which is up to Definition which is up to some factors we have. Also, again, equivalent as one can also show that this thinness is up to a constant factor, the same as the tree width of the link intersection rather here. This is another way of defining this in the equivalent way, but actually, so in the paper we have a comment on this, but we work more with this definition here of the laminar family. Here of the Lamia family, this turns out to be the most useful for us to, most simple one to work with, but it's all equivalent. If you like tree with, that's also the same thing. Yeah, and so the main motivation behind this definition here is that we are able in some sense to optimize over these set of sync components basically by some kind of dynamic program. Basically, by some kind of dynamic programming approach. So, you can think of some dynamic program which is in some sense going along this laminar family and trying to build up something component. Of course, we don't really know this laminar family, but the main important thing is we can somehow look at partial solutions for every possible true cut. And because these two cuts, I mean, they're intervals of the cycle, they're polynomially many of them, podelically many of them. So, this is something. Many of them, so this is something one can hope to do with a dynamic program, and then you kind of try to combine all these two of them. Like as we take you, yeah, this laminar family, as we said, is the union of two smaller ones, you kind of combine them along this laminar family. This is like roughly the idea where this kind of definition was helpful. If you're trying to optimize over this, you can start trying to build dynamic programs of that kind. Uh and uh using this kind of idea, one can prove uh the following optimization theorem. So we can prove that if we have such a nicely structured directed solution, we can find if we have yeah, if alpha is some constant, then we can find the best alpha thin component in a sense. Component, in a sense, and the best here means that we are minimizing the ratio between the cost of the component, so the cost of what we can add to our solution, and the cost of the drop, so the cost of what we can throw out of our solution. And here again, also in this dynamic program, the one thing that, one reason why this works here is this definition of thin components that allows you to build this up nicely, but you also need this code dividation. This code degradation of the prop with this least common answer to thing in order to be able to handle this objective function throughout the dynamic program. But essentially the proof of this optimization theorem is done some technical but otherwise fairly standard dynamic programme somehow, which is trying to uh build things up along this limit of heavily which we we don't fully know yet, but we can kind of guess uh in our dynamic programme. In our dynamic programming. So, this is to prove this is dynamic programming. This is the one key ingredient, technical ingredient that we need for building algorithms here. And then, so this now tells us that we are in some sense able to find the best component that exists here for doing exchange steps, but this doesn't tell us that good exchange steps exist at all. So it might be possible that even the best possible exchange step is just pretty bad. Step is just pretty bad. So, for example, if you would only restrict to adding components containing just a constant number of links, which would be a very natural thing, then you could just enumerate all possibilities. If you just try to look at such exchange steps, it might be that we just don't make any progress at all, and you're just stuck as factor two. So the second main thing is we need to prove that there exists a good component that we can use. Good component that we can use to make progress, and for this we prove what we call the decomposition theorem. And this is actually where most of the work is in theory, so that's the difficult thing probably to prove here. And we explain what the statement here is saying and why this is useful. So the setting is: yeah, we have some fixed epsilon, this will just appear in our proximity. Xylon, this will disappear in our approximation guarantee here, and this will somehow fix the thinness of the components that we will be looking at. So we'll be looking at components that are somehow on the order of 1 over epsilon 10, which is some constant, which is important for applying our iteration theorem. And then the setting is we look at such a non-shortenable direct solution, bruising, our true approximation. And then we also look at the optimal solution and the reason why it's. Solution and the reason why I look here at the outcome solution is that right now we are just trying to prove existence of good components. We've already shown that we can find the best component in a certain sense, so we just need to prove that it exists so we can work with the optimum solution and we don't need to worry about algorithms right now. It's just about existence. Okay, and so what we prove is that it's possible to partition the optimum solution into Solution into thin components. Like here, you see this green optimum solution is partitioned here into three components: brown, blue, and the greenish, green, yellow one. And now what we can prove about these thin components is that if I sum up, if I look at what's the drop of these components, so how much can I drop for my solution if I add these components, then the sum of the drop. Components, then the sum of the drops of these components will be essentially the cost of my directed solution up to some small epsilon. And this is, in some sense, the best thing we could somehow hope for. So, in general, the best thing we could probably hope for is that each of these directed links here will be part of the drop set of one of the components. That would happen, for example, if it would allow to take the outcome solution, then I could just drop every link and then sort of total. And then so the total if it would be if the upcoming solution would be a feasible component, I would just get that the cost of the drop of this component is precisely the cost of my whole direct solution. And this tells us that we can almost get this up to an arbitrary small epsilon. And so the reason why this implies now that good components exist is basically a simple averaging argument because in total, summing over all these components, these components Over all these components, these components are cheap because they are partition of optim, so they just cost as much as an opt-time solution. But if I look at the at the, if I summing over all the components, if I look at what I can drop, this is essentially everything in my directive solution, which at the beginning, in the worst case at least, is twice the cost of out. So essentially, on average, one of these components, at least at the very beginning of my algorithm, for all of the components here, Here, the what I can drop will be roughly thrice as much as what the cost of what I have to add. So at least the beginning will be this shows that we can make progress by some simple averaging and unit. Okay, so I will not get into the proof of this here, but this is like the main technical statement that one has to prove to get the results. Okay, good. So now I'll let me So now I'll let me at the end comment a bit on algorithms. So one thing that I already kind of explained very briefly before, one thing that one can do is one can apply this now in a 3D fashion. And this leads to a 1 plus ln2 plus epsilon approximation. So this is basically 1 plus ln2 plus epsilon is a bit less than 1.7. But this is already Bitset. But this is already significantly PLI2. And so, what this algorithm does, it basically applies the optimization theorem really, all the time. So, we start by computing a non-shortenable directed solution to our weighted ring augmentation problem. So, this nicely structured directed solution. This costs at most twice odd. Yeah, so that's the two, that's way two and the L12. That's where the two in the L12 is coming from. And then you just readily apply the optimization theorem. So you just iteratively find the component that minimizes the ratio between the cost of what we add and the cost of what we can drop from your solution. And then you do this exchange step as long as you're still making progress and you always move the drop from your solution. Yes? Of your solution, yes? What do you mean iteratively? Because you explain what happens if the starting solution is a ramp, but you add some components and then you have a mixed solution. Yes. Basically. Yeah, basically you still apply the whole thing to the original starting solution, but in the later iterations, you will not get any gain from being able to drop the same one again. That's kind of what. That's kind of what happens. So you could think of like applying the optimization theorem to the same directed solution, but for all the links that you already removed sort of in the previous iteration, you can put cost zero on that. So you don't gain anything from being again able to drop the link. This doesn't help. You already know that we can drop this, so you just don't get any extra reward for this, because otherwise we just do the same thing again. So this means later durations, after some time, we will make less. Durations, after some time we will make less and less progress. So, this argument that we find something where we gain a factor two applies only in the beginning, which of course the cause can then resolve the problem actually. Yes, yeah, you can just work. And after some time, somehow what happens is that the ratio between what you can add and what you can remove becomes worse and worse over time. At some point, you don't make progress anymore, and you don't always get the Anymore, and you don't always get this exchange rate of a factor of two, because otherwise that's also why you don't end up with essential with the pitas in the end, but you only get this one plus log two here. So the precise analysis is very similar to other kinds of these Velatic VID algorithms applying for the tree augmentation before. Also, say for signal tree, there's a Silikovsky Velatic video algorithm. So once you have this Once we have this optimization theorem and decomposition theorem, the analysis is basically the same thing, just that the notion of components and drops is always something different in every context. But anyways, the analysis is really like the same. What happens here? Okay, and then, yeah, I promised you at the beginning that we can also get a three-half plus a zylon algorithm. And for this, one needs to use a slightly more involved. One needs to use a slightly more advanced algorithm than this greedy one, namely, you can use a technique which we previously applied for weighted 3 augmentation, which one can also apply for the stein entry problem to kind of recover the best known approximation ratio for stein entry by a non-LP-based algorithm. This is a kind of a local search algorithm, and so very roughly the idea. So, very roughly, the idea is that one weakness of this relative 3D algorithm is that once we added one of these undirected links to our solution, we will never be able to improve it again. So, even if at later iterations we add some components that, as we mentioned in your question earlier, that might somehow allow us to drop the same link again, it might cover something that we already previously covered. We'll not be able to drop these links. And this is what this. These links. And this is what this local search thing is, in a sense, trying to fix. And so the main difficulty here in being able to drop undirected links is that our decomposition theorem doesn't work for dropping undirected links. It really needs that we are able, also the transition theorem, really needs these direct links. So it's kind of hard to analyze this. And the idea of doing this is basically Doing this is basically that we think of every undirected link in our solution. We think of this as being sort of the union of two directed solutions and then of two directed links here. So actually this is the link we actually have in our solution, but we kind of think in the analysis for the algorithm we think of this as being like union of these two directed links. And then the idea is that using our decomposition theorem, we are able to analyze when we are To analyze when we are able to drop these directed links here, and we know that once we got rid of both directed links, we know that we are actually able to drop the undirected link from our solution. So, in the analysis, we are looking at this because that we can analyze. We know when both kind of halves or both direction of the length disappear, then we can also get rid of this one. And then, yeah, one can design some kind of local search algorithm. We need to be a bit careful. We need to be a bit careful in designing the objective function so that you see, for example, some progress, already if just one half of the two links, one of the directions of the link disappears, but not both of them, and that point of solution doesn't get cheaper, but it's still some kind of progress. But using these kind of ideas, one can construct a local search algorithm, which, yeah, as I mentioned, is also can be applied to ge augmentation. Applied to H3 augmentation, also to stein entry. There also has to be what can actually also apply this to, for example, to set cover. There was some recent paper by Anukam and Edis and got for set cover for small sets some improvements or something based on this local search technique. But again, like the main technical ingredients to analyze this are the very same. You can use some kind of optimization to over with a dynamic program and use the very same. Program and use the very same decomposition theorem here. Okay. One more question. Yes. So your first research, can I interpret it as saying if I start with a non-shortenable directed rap solution that is an alpha approximation, then it is a one plus lana? Yes, yes, exactly. That's exactly what's happening. And actually, yeah, such a thing can actually be useful for, so an analog statement also votes for 3 mutation, for example, and it says. For example, and this has actually been useful for forward augmentation, where you have just an underrated graph without weights and doesn't have to be connected, we want to augment this to a two-edge connected graph. And there in some yeah, joint work with Fabrizio Granduni and Afric, Giovanni Medi, we actually used this kind of formal form of this. uh this kind of formal form of this to get better true approximation algorithms for this uh uh for this problem using this relative 3d algorithm. Um so this can actually be useful, this former version. Yes, very good point. But that's yeah, exactly what happened to you. In general, we cannot find any nicely non-shortenable directed web solution of cost less than twice than R. But if you are able to find such a thing, you can get even better approximation out on this. It's in general doesn't work, but in a particular setting this might be useful. When you add an undirected link and think of it and the analysis has these two directed links, do you then take your directed solution and re-shorten it to get one of these structured directed lists? Yeah. We shorten it in order to be able to apply the decomposition theorem and the analysis, but we somehow still keep track Somehow, still keep track to which original undirected things this was coming from. Yeah. Okay. Yeah. So then, yeah, let me conclude with a brief summary. So in this talk, I told you that there is, yeah, I was talking about this result that you get as we have perceptional approximation algorithm for this weighted connectivity augmentation problem, which is also the first time we beat the factor two. Time we beat the factor 2, we've seen this directed version, which is somehow a very highly nice thing about this was that it had these very, very nicely structured solutions, but it can give us only two approximations. But we've seen this concept of cut responsibility in order to get a good handle on which links we are actually able to drop and understand this and get a unique choice for this. You've seen this careful definition of these. This careful definition of these seven components, which is crucial to be able to prove the optimization and the decomposition theorem, and to find the second or set up the right definition theorem, both statements are still true. And then, yeah, the main technical part of their skip theory was the proof of this optimization theorem, more like a dynamic program, and the more interesting part would be the decomposition theorem. And then the very end, actually sketched how these results can be used in relative Calls can be used in relatively technical search algorithms. So it's a brief summary. Yes, with this I would like to stop here, so thanks a lot. Thank you so much, Vira, for the very nice talk. There are some questions during the talk. Time for some more questions. Are there any? Does this seem to rely on starting out with the cactus representation? Does that mean it doesn't sort of automatically produce directory install? Because you can't go from zero to one? Yeah, it it, I mean, yeah, from zero to one would be, and from going from zero to one in that setting would would be like like computing minimum spanning tree. Uh, that's yeah, it does that doesn't give you this sort of the complicated thing, but the Sort of the connectivity thing, but the zero one case is like, yeah, maybe undivided case of connectivity augmentation. So if I understand this right, there's no better than two integrality, gabal E, or other functions. Yeah, no, we don't know this. Also, for even for the special case of weighted tree augmentation, you can write down You can write down for tree augmentation, yeah. You could also write down like the natural LP. Tree augmentation is like every basically one cut of the tree has to be covered by one link. We don't know whether this has an instability or better than two. We don't know whether we can write down any vulnerable time solvable relaxation down there. So, in particular, like Like, or like here, one might think like in signature, of course, you could write down this kind of component LP. But it's which was kind of nice here. And even though, like, these, it seems like even though one can apply these local search techniques to sign a tree and get the same thing, it's not clear, at all clear how to write an LP down, use a component LP here for the tree augmentation. We don't know how to do this. We don't know how to do this. And the main reason here is somehow that in this decomposition theorem, this decomposition theorem, this decomposition depends on our two approximate solution, which in signatory is not the case. So in signatory you can kind of get these components and then you can later this whole thing doesn't depend on like the minimum spanner tree from which you're trying to drop things, which is kind of the analog of our directed solution here. Directed solution here, but here the decomposition, the partition has to depend on the solution, the directed solution, and that makes it hard to write down a similar component-based LP. This is one partial difference here. So, relatedly, for this wrap problem, the lower bone is example for the integrality gap, the natural LB is uh I think for the for the cycle thing, um Cycle thing, I'm not quite sure. Also, for tree augmentation, for sure we have the lower bounds and we don't know what it is anything. Yeah, so for tree augmentation, we don't know whether integral TQF is somewhere between three half and two. So just comparing towards the LP, you cannot get uh better approximation outcomes uh than this, but we don't know what it is. Uh that is, but we don't know where it's going to process the issue. That's open. But yeah, even like coming up with any idea only solvable LP, say some component LP is sort of unclear how to do this. Was this nice to draw on the background? So do you think that's possible? 