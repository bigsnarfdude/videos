And yeah, really quickly, I want to acknowledge my collaborators on this work. So we have, so actually Kevina Meyer, this was her, most of this work was her PhD thesis. And yeah, she did like really awesome work. And then we also have the other two collaborators from Potsdam and the Free University. And for the university. So, what I'm going to talk about fits very well the first two talks we had today. I'm also going to talk about cancer. I'm going to focus not at all on the model and not at all on the data, but on how to bring both of them together. So if you ask me very complicated questions regarding the model or the data, I might not even be able to answer them because I was not the person in charge of that. But I'll try anyway if you have some questions. If you have some questions, so the goal was similar to what Madika has already been talking about: is kind of use a model to make some prediction of how the patient is reacting to a specific drug. Here we're considering chemotherapy treatment for a small cell lung cancer, and they have actually several models in my group that have been studying for several years. And yeah, we use the most advanced of that, and essentially what the model outputs is one of the key outputs of the model are neutrophenore neutrophil cell counts. And what you're essentially trying to study is if the patient is ending up or is having some form of neutropenia. Of Neutropenia. So, what you basically reach too low in your Newton counts, this is categorized in five stages. You have grade 0, where everything is fine, and then grade 1, where you might be affected by the treatment already, and the cell counts are starting to go down, and it goes down to 4, which is a life-threatening level where you really need to be in the ICU. And yeah, your immune system is getting compromised. Is even compromised, and so you do want to somehow. So, the target is for the because we actually don't model how the treatment is affecting the cancer, we would like to do that, but the model doesn't give us this information. So, the model is being used as, first of all, to kind of manage the situation that the patient actually doesn't end up in grade four, to find out when the patient. To find out when the patient has recovered, to actually give him or her the next dose, because obviously you want to go up in your cell counts to administer the next dose. And also finding the right timing because it seems to be a huge schedule issue. You bring in people and then they're not ready for treatment yet and it's hard on them because they're not feeling great and going to the hospital is always a risk. So this was one of the reasons they developed these models. Of reasons they developed these models. But in terms of how we're actually tackling the cancer cells, you also there's an underlying assumption that if you do go down in neutrophil count, you also assume that you do go down in cancer cells. So there's a balance you want to achieve where you actually don't want to stay in grade zero because you assume then you're not really attacking the cancer at all. So the ideal area you want to be in. Area you want to be in is kind of here between grade 2 and 3. Although there's also no specific number, if you would ask the physicians, they wouldn't tell you, oh, you want to be in grade 3. Where they want to be ranges depending a little bit from position to position. So in our kind of consideration, we said, okay, anything between 1 and 3 is good, 0 and 4 is kind of bad. And what we actually What we're actually trying to do is first of all prediction, but second of all, also modeling for precision dosing. So, we do want to kind of give a good recommendation what dosing to administer at certain time points. For now, we assume that we have fixed time points, so every 12 or 15 days you dose the patient, because you assume at that point, on average, the patients have recovered. In our new asset, In our newer studies, we actually also tried to have the time point of dosing be variable, but at the original study, we didn't attach that. So this is a graph of like we wanted to make a graph on what we think is state of the art and what's out there basically. And the most simple thing that basically was given by the education. Or was it the FKA approval was to give 200 milligram at each dosing time, and if you end up in grade 4, you reduce the dose by 20%. And then later on, there were people who did a clinical study to refine this a little bit and also tailor to specific variants of the patients, such as age, gender, I think body surface area. Body, surface area, and a few other things. And so, this would be the most, this would be right here where you would literally look at a table, okay, this is the gender, this is the age, and then you would come to dosing. But it is not very much deviating from this original give 200 milligram and then just go down twenty percent every time. Down 20% every time you reach grade four. And on the other end, state of the art would be to actually collect data where you would say, okay, I have this model, I collect data, and for my specific patients that I'm treating right now, I can actually find the parameter within the model that is tailored to that specific patient. And according to, then I use this parameter to do the predictions, and according to a specific cost function, that it would be, for example, to stay between grades one and three. You then calculate the optimal dose. And so this was what was out there. In practice, of course, they don't do that yet, but there's papers out where they suggest to do that. They suggest to do that. But the problem with the map estimate is that they usually just get a point estimator of the parameter. And I will go into this in detail. So we kind of said, okay, let's look at it with data estimation. This is just for the prediction for now. And if you use an ensemble method, what you have is you have a much better idea of what the tails look like because on average, this flammer, what you would get from This parameter of what you would get from the map estimate would be very good on average, very robust, but for all the people who kind of deviate from their average, would not be that great. And that's already a lot of people because there's lots of variability. I mean, you could see it in the previous slide. Some people react completely different. And these are actual outputs of the model for different patients. And these are not even extreme cases. So yeah, that was one thing we tried to do. That was one thing we tried to do, and we also wanted to say, okay, if you just have the simple model, no data at all, you could also do something like reinforcement learning and try to find the optimal dose, kind of what Malik was also suggesting. So we tried to tackle the problem from different alleys and then when we did this and this, we realized why not put them together and see if it brings us an even better outcome. And I will go a little bit more into detail. So, as I said, that was state of the art, just really quick again. So, depending on the model, you can just calculate this best parameter theater that is used to kind of describe the specific patient better. And then you have some kind of cost function. And one issue of this cost function is as well: like if you think this is basically grade 2, 1, 2, 3, you use this utility function. Use this utility function to kind of calculate the best dose. This can be good and bad to have a continuous function to do this. Because obviously you kind of want to, like the question is, do you always, for example, want to be in the middle here? So to find the right utility function is not always easy. But yeah, it can have, depending on how you choose it, it can have bad big effects. But one big issue what I already mentioned is that there is this high variability in the tails and you would never model like those patients would be completely ignored with this map estimate. But if you actually had some ensemble members that would represent these tails, they would kind of be factored in. They would kind of be factored in when you do the optimal dosing calculation. Because you would also, like, you would compute the model forward for all these particles, and also the particles and the tails would be evolved in time, and that would also show you maybe more extremer outcomes that could happen. And this was if you use that information rather than the Rather than the point estimator, you can achieve much better results for the whole, like not only on average, but you can also kind of get the people that are more on the outskirts of what you expect to happen to get better results when cheated. And you can also kind of model the uncertainty through it. So this is already a really nice way to do it. And practically, I call this data. And I call this data assimilation here. The map estimate is also data assimilation. To be very precise, this answer would be kind of particle-based data assimilation, and the other answers would be variational data assimilation. If you write a math here. Yeah. When you write y1 to n, is n people always? Or is n the sequence of data? That's data idea. So then, where's the I think I'm confused about. I think I'm confused about the population and the person. Are you always fitting? For a specific person, I do all of this for one person, and then I have a different set of like particles I have are essentially particles of different parameter values that could all be kind of tailored to that specific patient. And instead of just having one that I then use, I have this whole range, but it's all for the specific patient. But it's all for the specific patient. Okay, so the left is a point estimate of a personal parameter, and the right is a probabilistic estimate of a parameter for a patient. And in the previous slide, was that population or was that also per patient? It can be done both. So it can be for per patient where you just look at the entire time interval that you have data up till then, and then you do it for that specific patient. Specific patient, but the problem is, even if you, okay, maybe explain a little bit confusingly, but even if you do it for one patient, there could still be variability because you only have very little information up to that point yet, because this could be just like the second dosing time, for example. And then you don't project this variability you could have for one specific patient as well. I don't know if that was what confused you. So, this also could also just be one patient, but then you have just one parameter and you don't have a choice of parameters that you kind of update because then you really narrow down what could happen to the patient. You're basically just looking at the map for that specific patient and don't consider what else could happen. Now So, this was basically the map approach and the DA approach. And one thing we want to do now is also use the RL approach, where we basically use the information of the state that is given by the model. And it's a little bit complicated how we do it, but you essentially project that state because it's actually concentration values, but we want to then project it to actually. To then project it to actually just the grains. In fact, actually, also the history of the grains, but let's ignore that for now. And what you're trying to do is basically for each of the states you could be in, which is basically just the grades, you assign a reward or penalizing value to it. And obviously, if you're in grade 4, you want to give a negative reward, and if you're in between 1 and 3, you want to give a positive. One and three, you want to give a positive reward, and you do that over time in each time step. And summed up over all of these rewards, basically. And you also put a discount factor because something that's far in the future you might not care about that much because it might be too far in the future and you could deal with the fact now that somebody's in grade four, even though the future might be good in terms of reducing the cancer cells more. And what you're really trying to do is approximate this like well in Approximate this, like in general, what is happening in reinforcement learning is you have to have this expected value that is basically the expected value of the outcome that will, like over a time it will, conditioned on what state the patient is in now and what dose you're administering. And if you can estimate this for a specific strategy, the way you kind of dose. Way you kind of dose, you have an idea of how well you're doing, and what you basically want to do is find a strategy that is maximizing this value here. And since you often cannot actually analytically compute this expected value, what you just do is you do it empirically by generating lots of samples, which you can do obviously because you have a model. And then you can optimize over this with respect to a certain strategy pi. Certain strategy pie, which is just telling you what action or what dosing you perform at each step. And there are many different variants of how you can do that then to find the optimal policy. And one we used was Monte College Research, which is actually heavily building on the version we used, Monte Coloured Research with Apple Confidence Bounds, which is heavily based. Confidence bounds, which is happy based on the thing I was talking about on Tuesday, where you're basically trying to say, okay, if I don't have enough knowledge about being in this state and then giving the patient, like the patient is in grade 3 for example, and I don't know what will happen if I give him 180 milligram of the drug, do you want them to explore this more? Because you don't have a lot of knowledge about what will happen to the patient then. Happen to the patient then. And as soon as you have a lot of knowledge, but you realize this is not really a good action to do because the outcome is bad, you then go for the actual good actions. You balance between exploring what could be potentially a good dosing choice. And if you have collected enough information, you basically want to stop and just go through the best one. And it's called Monte Carlo's research just because, like, let's say that that's up there, that's the state the patient is in. Up there, that's the state the patient is in, and then you perform an action, and then you reach a different state in terms of metropoenogrades. You can then choose again between different dosing decisions, and since you can visualize this with this kind of graph, people call it multi-colored research, but essentially it's just how you because what you do is how you choose those actions is not always the same. Is not always the same throughout the graph because the further you go down, the more you want to explore, and this kind of how you do this. This is basically called multi-counterage research. So, just to give you an idea what's happening, so in the very beginning, as I told you, so these are the different, so this is the starting state, for example. And just to show you, so what you see here are always the grades over time. So, what you actually have is a state, it's not just the grade that. Actually, have as a state is not just the grade at the current time, but also the whole history of grades. And what you see is, for example, the first state having no grade at all, just in the initial dosing time, this is explored a lot. But then if you have very extreme states, this has not been explored at all at the very beginning of learning. If you have, for example, only seeing 100 samples or so. And this would be that the patient has, throughout the entire time, grade 4, for example. 4, for example. But at the very beginning, what the algorithm would do, it would explore these extreme states, although this, whatever led to that state, cannot be a good dosing, like it would, for example, if the patient is in grade 4 all the time, you would not give the highest dose, for example. But in the beginning, you would still explore what you can do because you use a model. But obviously, never do that on a real patient. And you can really see, so I can go. So in the beginning, as you see, you explain. As you see, you explore all the dosages here as well for grade 0, 0, 0, which is also considered bad. And then the more samples you gather, the more you actually shape towards only picking the sample from the dose you actually consider to be a better idea. For example, the person in grade 4, he being just a grade 4 at the very beginning, or the grade 4 all the time, he would never go to You would never go towards this anymore. You would hear what you can actually see here, it's a little bit more clear. Here it would give us a small dose. Here, if you end up in this state, this is already so bad that none of them are a really good idea anymore, which is a really bad situation. But here, this is after again now, I think, ten thousand samples you can see. Now it's clearly the algorithm realized, okay, this is the best um idea. Okay, this is the best idea. Am I done? Two minutes. Two minutes. Okay. I didn't realize I was already talking so long. So we also mix this with data assimilation, data coming in all the time, and then you first pre-train with the model of reinforcement learning. And then you do actual data where you already know that your dosing decisions are good. And every time a new data point comes in for the patient, you also tailor the reinforcement learning strategy on the A part to the specific patient. Specifications. And just to show that everything that really was a good idea, this is the map estimate. And this is the DA approach and this is the combination. And this is the window you want to be. And you can see that, I mean, here you were already kind of in the right area, but the tails, there was too much variability in too many patients. Variability, too many patients were basically not treated correctly because the ideas considered the high variability. And here as well, you see this is grade 4, grade, sorry, grade 0 and grade 4. And you could see that you can essentially really reduce the times per 30 people end up in grade 4, especially compared to the, which is the blue stuff, is the standard stuff. So you could really reduce. So, you could really reduce the number of people being in grade four, which is very good because this is always a live-streaming situation. And since I'm running out of time, it's also very nice to kind of study between covariates groups, these are different dosing, yeah, how well you, because you also get information on how well a different dose, not only the optimal dose would be. Not only the optimal dose would be, but also some of the other dosages, and how this differs between covariance groups. And that's actually interesting to study how different covariance groups react to the different dosing intervals. And yeah, that's it. Thank you. So, how complicated are the underlying models? I wouldn't say it all, but as I told you, I'm not just a set of ODEs. But I mean, there's significant delays involved, which are local partners. I I didn't catch that whatever. So there are delays, right? Because the cytotoxic effects are on the Yeah, but that's on the universal cells and what you're actually doing. Yeah, so the longer the treatment, also the severely more down you go and that comes. Yeah, that's included in the model basically. I mean the thing is, one thing we really want to do is actually have something that also models the cancer cells because right now The cancer cells, because right now we're just saying, or we're like down in neutrophil counts. That must mean we're also technically cancer, which I think is very unrealistic. It would also be interesting, there's several side effects that would be interesting to model because people get the nerve damage. And there's many, many things they're all very to go into them it would require a lot of detail, but there's many things we can explore further. There's many things we can explore further from time points, and we also can actually consider our points of different covariates or other models through the reinforcement learning DA scheme. But the bigger problem then is you have to also do some kind of parametrization of this q functional because right now we actually basically have a matrix because we don't have that many states. But as soon as you go higher than state space, you also need to do some parametrization for this. And for now, we're also using a particle smoother. For now, we're also using a particle smoother, which we cannot do if we go be a very high dimensional there. And then adding more derives, it would say. Yeah, but you could do it. I mean, we're currently working on all of these angles. Another thing would be interesting, because right now we have a reward function that gives specific, there's basically a negative value assigned to being in grade four, but this is really just like how do you balance being in one grade over the other. This is really subjective how this is chosen. Subjective how this is chosen. And one direction we're looking into is going into the direction of inverse reinforcement learning, where you basically find this reward structure you should be using on the basis of data from actual physicians having performed the dosing. Excellent. All right, let's see. We'll see ego.