Thanks for the invitation and the kind introduction. Today I will talk about this self-interacting approximation to Martinus flood of diffusion long-term limit. As you will see, it's recent progress on the line of research on the wash system grading flow to understand the training of neuron. Okay, so let's first talk about the motivation. So, neuron. So, neural network analogy, everybody knows that it's indispensable in all kinds of applications. So, in our talk, in our framework, we only focus on this simplest one, the stoolian neural networks. It's nothing but a parametrization in this particular form called desired functionality. The universal representation theorem tells us this kind of parameters can be found, but it does not tell you how to find them. Does not tell you how to find them. To find it, typically you need to solve an optimization problem to minimize the training error. So between the real value, the label value, and the output of your network. But this is, well, no convex optimization on a very big, well, very high dimension space. Yes, go ahead. You say in m in the number of neurons, and yeah, you can choose a number of neurons, entire parameter, yes. It's higher parameter, yes, right? Hyper parameter. The width of the yeah, it's a waste of two layers. Yeah. So, uh, well, to analyze this complex optimization problem, one observation brought up in the recent literature is that, well, you can divide n and pass n on the position c, and then in this way, the output of the neural net is in the form of an inferior sum. So it explains. Empirical sum, so an expectation with respect to empirical measure. And now you rewrite this optimization instead of minimizing on the real space, high-dimensional real space, you take the informal on the space of probability measure. For here, it becomes infinite dimensional. The advantage of this reformulation is that now the potential function of f is a function of a probability measure. It's a function of probability measure. It's a convex function on the space of probability measures. So it's a convexified problem. We can call it the mean-field convexification of this optimization problem brought up by the training of neural net. So now our objective is to solve this kind of collective optimization problem. So to do that, the first thing is to understand the first of yes, when you say E of M, that's the condition of Z. It's independent. Independent, yeah. It's independent. Independent. Yeah, the neurons value and the data value are independent. So first step is to understand the first of the conditions. So you need to introduce the proper directive. So this will be the same directive as in the literature who may build the ms. So the flat directive or the linear directive defined by this Taylor expansion on the space of probability measures. And we can And we can understand it through two examples. First, well, in this community, everybody knows. So, for linear examples, if it's just an integration with respect to a function L, then the linear directive is just L plus a constant because it's defined up to a constant. And the second example: one step further, you can consider this fancy metrical functional, a nonlinear function composed with this linear integration. Then, in this case, you can. integration, then in this case, you can compute the linear diagram by the shared rule. First, take a diagram of five, and then use a result of the example a. Okay, and in fact, in most of the applications, it's enough to understand this example B. Okay, so also you can define the so-called intrinsic directive, the Leon's directive or L directive, or Washstown gradients, all kind of name. So you notice in this top DMF, it's just taking another gradient. It's just taking another gradient being x on the linear algebra. So, why this kind of linear diagram is important? You can see from here. So, front, so in the following talk, we will consider optimizing problem regulated by relative entropy. So, H is a relative entropy and sigma is the temperature, the size of the regulation. And first, the theorem is simple, it tells you that it's a theorem is simple it tells you that it's a it's a legal uh regulator so when temperature goes to zero the minimize the minimum converges to the on the initial minimum no bias so the linear diagram is important in this sense so it tells you uh it's a first order inequality or tangent inequality it tells you that while liking the real space the improvement of a convex functional can dominate on the improvement on the tangent line On the tangent line. Okay. And here it's a linear diagram for this potential function function f, and the red part comes from the calculus of vibration for the relative entropy. And this inequality tells you immediately the first order condition. So, in fact, for m to be the minimizer of its regularized energy, free energy, you can call it free energy, then Then the linear diagram of this free energy, okay, in this notation, must be a constant. Because once it's constant, then the integration on the right-hand side here is zero for every m prime. In other words, for every m prime, f sigma n prime is bigger than fm. The m is the minimizer. Okay. And also, no, this is about the sufficient condition and necessary condition can be proved by calculus of valuation, classical thing. Classical thing, and if you don't like this constant cost by definition up to constant, then you can take another direct being x so you can note it as the intrinsic directory for the free energy and as a notation. Okay, so now we understand the first condition. Now we can move on to connecting it with the diffusion. So in fact, this first of the condition has a direct link with the Faucon equation because, well, it starts with Because, well, you start with from this first-order equation, you rewrite it, and you find this one. And this one tells you that m star, the minimal, must be a stationary solution to this Fault-Planck equation. In other words, it is an invariant measure for the corresponding diffusion process. And the corresponding diffusion process is what we call the nuclear Langevin process. All right, so it's a mapping model of diffusion. You have nuclear dependence on the drift. You have neutral dependence on the drift. And also, it's a long-run type of dynamics because the gradient is still minus gradient, just intrinsic gradient on the water stand space. All right, so now we know that the desire is a minimizer, the stationary environment measure for this diffusion. Then, one question, a natural question is to ask whether the marginal distribution can converge to this environmental measure as in the market. Environmental measure as in the Markovian case. This is not non-Markovian, it's a marking method of diffusion, but we can ask this natural question. Which is true. So the main observation is classical in the research of gradient flow. So it's a so-called energy dissipation. So in fact, when you compute the evolution of the free energy along the dynamic of neutral non-trading, the marginal distribution of The marginal distribution of movement one. Then, first, you use the definition of linear direct. You have this, right? And then you replace the DCMT by the Focus Planck equation. Focus plan equation can be written in this compact way by definition. And then you just formally do an integral by integration by part, and you get all you want. Okay, this is so-called energy distribution. It tells you that the free energy always decreases. You that the free energy always decreases along the living field long time until the point when the red part is zero. And the red part is zero is the first order condition, meaning that it will bring you towards the minimizer of the design problem, okay, of the free energy. All right. And of course, you can, well, we can make it rigorous. So under the assumption that F only admits one single To one single critical point. Then we can prove that the model distribution of Newfoundland electron really converges to the minimizer of the free energy in the sense of watch time distance. That's the first paper five years ago with Kaitong, Lukashen, and David. And there are following research by two groups almost at the same time, Nganda and Supreme. Nikanda, Sukuzi, Suzuki in Japan, and Shida in Sweden. They realized that, well, with the help of a logo-life inequality, they can prove if f is 1x, so at least slightly stronger, then under the sense of relative entropy, the convergence is exponentially quickly. And the convergence rate can be explicitly estimated by the log stopulation constant. Log stopulation constant, all right. So, in this, we mainly focus about the simulation. So, um, the initial Large Man dynamic is maximum diffusion, so you cannot simulate it on your computer because they are empty and objects of infinite dimension. What you can do on your computer is to sample the corresponding particle system. So you replace the Margin distribution MT by the empirical flow. By the imperial law of the particle system, and particle system. And then, of course, you hope the simulation you do on your computer is close to the real answer. So you need to dominate the control error between the particle system and the mutual system. That's the so-called propagation chaos result. And the classical propagation chaos result reaches in this way. It tells you the error at time t of the two system can be dominated by a constant times. Can be dominated by a constant times the error is times zero. And in particular, the classical result in the classical result, the constant is in the form of exponential CT coming from groundwall inequality. Okay. So when t become bigger, this estimate is useless because constant is too bad. And unfortunately for us, we do need t to be big because eventually we want to set them. big because eventually we want to set sample m star well once we sample m star then we can evaluate the neural network okay we record the motivation so we need to sample m star and the strategy is to sample m t for t big enough and if t big enough then this propagation of chaos results is not enough for us and ideally what we pursue is so-called a uniform time propagation of chaos in the sense that the constant does not depend on t. And well, with a few exceptions in the literature, most of the results of this uniform income population chaos require small mean fuse interaction condition. More precisely, they ask the league's constant with respect to the m variable to be small enough. Small enough compared to the temperature signal. Okay, and this is a better number. This is a bad assumption for the application to neural net because it doesn't make any sense. It tells you that neural net can only evaluate, can only evaluate a very small function like constant. All right, so we want to get rid of this kind of adoption and what we can exploit is the convexity of. So that's what we do in the in the work with the We do in the work with Sumbo and Fan two years ago, three years ago. So we assume that F is complex and some technical result, technical assumption. Quite general, we can prove this kind of uniform time propagation class when there's no type of gift. So L is out the delta N. So basically it tells you there this long time the propagation. you there's this long time the propagation calculus estimate and then you combine it with the short time the finite count classical uh classical uh propagation calculus and then you can get the uniform time propagation count and also if you work harder you can replace the one time two distance by the relative entropy so you get a more strong propagation purse uniform in time Propagation paths, uniform in time. So, they have a constant C there plus the C uh C, yes. Yeah, this should on the right should be the O1 because eventually you should divide by N here. All right, so uh today in today's talk, we uh so So, we have seen this particle system. Okay, it's very good. But in this tweet, we want to introduce something new. So, some new algorithm or I can say just for fun. Okay, so the end-party system of the refuel launch map, if you look at it, in the sense of a numerical algorithm, is very simple. So, we evaluate this, it's just the gradient descent of n variable plus the Gaussian noise. Plus, Gaussian noise. Okay, it's nothing material. It's very easy to apply. But it's well, also, it's very direct, right? So you think about whether we can exploit this mathematical structure and do something new. So that's what we do here. So introduce so-called self-interaction diffusion. So what is that? So the motivation, the introduction motivation is that the n-particle simulation is expensive. n-particle simulation is expensive so at each time you need to sample n particle evaluated at each step right so it's sort of expensive especially n is big and then we record the literature so in the classical markovian case so for example classical anchoan diffusion we can also use an occupational measure to approximate the environment measure not only the marginal convergence to m star but the occupational measure star but the occupation measure this random measure also covers m star almost surely right and the advantage of that is to sample marginal distribution you need n particles but to sample the occupation measure you only need one particle and recall its history that's the difference so in the recent work of our father Tung Kai and Futan and his colleague they observed that in fact That in fact, for marking rather diffusion, like this one, you replace this margin law by the occupation measure of history of the frequency. You consider this kind of diffusion, self-interacting diffusion. Then you will have the similar result to the classical one. It tells you that the occupational measure will also converge to M star, the environment measure of the marking result of. Environment measure of the marking rate of diffusion, but under the assumption that the mean field dependency is small, so the diffuse constant on this variable should be small enough. So again, our motivation is to get rid of this small mean interaction assumption and exploit the convexity of our potential. All right. So, what are we doing here? It's a similar idea. It's a similar idea, but modify a little bit. So we consider this mean field lamp one and replace the margin distribution by Pt. And Pt is indeed an occupation measure, but the occupation measure with exponential weight. You can write either in this differential form or this explicit form. But explicit form, you see it's an occupation measure, just with exponential weight. Well, if you say that occupation measure is a uniform. Say that the occupation measure is a uniform frequency of your memory, then this one is the weight that is with the exponential weight. It means you have better memory for your recent past and worse memory for the past. Okay. And the lambda is the so-called intensity of your memory. If lambda goes to zero, then it converts to the uniform memory. Okay, it's the initial one. And we're interested in the regime for lambda decimal because eventually we want to go back to the Because eventually we want to go back to the classical computation brand. So, well, but you can ask this question. Well, okay, you say you introducing self-interacting diffusion, but it's not good. Why? Because the TT is the occupation by the objects of infinite dimension. And incomputer cannot do that. You cannot sample this guy. So what you will do is consider this symmetrical form. This symmetrical form so f a this is our example b okay, and since f is convex, we ask its phi function is convex, real function. And we have already computed the linear diary, and we can also compute the infinity derivative for this example, symmetric example. And then you replace this infinity diary in your self-interacting diffusion and get this explicit form, and then you realize that. And then you realize that the x depends on pt, but only through the integration with respect to l. So you don't need to record the pt, but only record the pt integrated with l okay, and that's what we call yt, a finite dimensional object. And now the, well, you can also write down the dynamic of yt. And now the couple xt and yt is a finite dimensional Markovian process. Okay. Which you can sample on this computer. On this computer. So, the objective now is to prove the environment measure of this Markovian system is close to the Markov environment. Not exactly equal, but close. Sorry, what is capital phi and L? It's here. So, you can say it's symmetrical. And in the explication of Of neural nets, phi will just be a square function, and L will be the error, the difference between label and the output on issue data, what's linear and non-lead, right? Yes, inside of the linear iteration. All right, so uh so now. So now, I said about the objective now to achieve this target, we have two steps to go. First is to prove the self-interaction efficient itself converges exponentially quickly to environment measure. And second step is that to estimate the bias between the self-interaction environment and the one of the maximum dynamics. Okay, two steps to go. The first step, I go quickly. The first step, I do quickly. So, for this kind of Markovian diffusion, one of the only tricky part is the y dimension here is degenerate. There's no bright motion, but we can still do that because on y dimension, it's dissipated. So, the minus lambda white. Okay, so it's a force thing to the center. So, uh, with some well. Well, easy, uh, well, a general assumption on this drift B. So you even don't need to be launched by just the general B. You can prove this kind of contraction result. On the washer-style distance, you have this contraction result for the law of X-TenoH. And you can explain, well, you don't need to read the detail, just tell you that this convergence. Detail just tell you that this convergence rate can be estimated explicitly. And since there's this contraction, then by Barnett fixed point theorem, you can prove there is a unique environment measure for this thing and it converts to this environment measure at the speed of exponential speed C. And you can, well, we have this remark. In fact, the C is of the size lambda, it's all lambda. When lambda goes to zero, Okay, when lambda goes to zero, this rate will disappear, and that's why we consider positive lambda. We don't start with lambda equal to zero. We want to go back to lambda to zero, but we cannot start with that because if we start with lambda equal to zero, there's no rate, no rate of convergence. Okay, and now let's denote the environment measure of the self-interaction diffusion as rho. We want to now prove rho is close to the Low is close to the environment of the mean field that may fail. Okay, to prove that, so let's go a little into detail. So, to prove this, there are steps to go. The first thing is to observe that it's a two-time scale diffusion, this X and Y. In what sense? In saying that, since we consider we are interested in the region where lambda is small. The region where lambda is small, so the evolution of lambda is much, much slower than the evolution of x. Okay, that's the so-called two-time scale. And in the extreme case, lambda equal to zero, then y does not evolve. It stays at a constant, small y. And you only need to consider this kind of labeled x equation. All right. And let's denote the m hat. And let's denote m hat y as the environment measure of this label diffusion. And we also assume that m hat y, this environment measure, satisfies also inequality. This we can have very simple sufficient condition. It's not our realistic assumption. Okay, so what do we expect? We expect that the environment of our self-interaction. measure of our self-interacting diffusion is close to this m hat y the conditional rule of this environment measure is close to m hat y because m hat y is the extreme case when lambda equal to zero and here it's lambda small so we are expecting they are they are close to each other and that's exactly we can prove by some energy estimation we can prove that in this sense so the related venture between So, the related mention between this row given by row is the environment of self-interacting. If we compare this, and M has Y, environment of this label diffusion, then the relative entropy integral by the marginal distribution of rho of y is of the size of lambda. Okay, well, lambda is unit equals, so it's small, small value. So, next step, next step. So next step or next step is to prove if we can prove for each m for each y m hat y is almost m star the mean few long time environment for each one okay then our task is finished because then we can prove law for each y the condition law is almost the m start that's what we want okay so in this sense we can say that So, in this sense, we can say that the self-interaction long time limit is close to the mean fuel long-time limit. And to prove this, the red one, to prove this, well, that's the most tricky part of the proof. We construct a probability measure satisfying these three conditions. So we construct a new one. If you look at these three conditions, the first one, the second one, we only need to take Second one, we only need to take mu equal to row. Okay, mu equal to row, then it's automatically satisfied. And the only new part for this mu is to satisfy this spatial position. We can construct this. And if you can control this, then we can continue the argument. Let's predict, let's assume it's and now that place the complexity of f The complexity of F comes in. In fact, we only use the multinicity of the linear edge here, but to construct it, we use the first data. Using convexity and the tangent inequality, we have first ideas. And now we use the or we get this inequality. You remove the central part, you get this inequality. And now, uh, you call us the marginal distribution of mu and row algente. So, you can put those guys here to get a mu also like this. And second, since we are only considering a symmetrical case, so can we can write this linear diagram explicitly, indicate this. And now it's time to use the second property of mu, but we can replace this guy by simply y. By simply y. And then it's time to use the third property of this mu to replace this new integration. And the third property tells you this kind of integration, using u and using rho are the same thing, give you the same evaluation. Okay, so you replace mu by rho. Okay, you rewrite this inequality ideas. And then you take out the marginal distribution of row y, just rewrite it. Let's see happen. And now you use the result we already proved. You reduce the result we already proved. The conditional law of row given y is almost the same as m hat y, as we have proved in the first step. Replace it. Look at this guy. And now, uh. And now, recall the definition of relative entropy. In fact, it's nothing but this symmetrical relative entropy between m hat y and m star, which is positive value, non-negative value, and offside of lambda. It's small. It's positive variable and small. And if it's small, it tells you these two matters are in fact close to each other of the size of lambda. All right. So eventually you prove the sequence of approximation, and then you can conclude that the marginal row of rho on X is close to M star. So mean few run limit. So we can make it rigorous. So that's the result. So well, rigorous. Well, rigorously, so it's estimated in the washstand two distance or the total vibration distance. The bias is offsize lambda. Also, you can consider the bias on the y dimension. It's also lambda. Well, square root of lambda. So finally, I want to talk about, simply talk about annealing. So in the previous slide, we see that the conversion rate depends. Depend on lambda, when lambda is small, the convergence will disappear. And the bias also depends on lambda when lambda is big, the bias is big. So, what we want to do is to reduce the lambda for the meaning, right? And well, we can do that. So, basically, we can construct by hand a knitting fixed white constant knitting and achieve this kind of convergence rate without bias. And well, this is the record of the motivation. The record of the motivation. So, for the self-interaction diffusion to sample, it only need one sample one particle, but not n particles. And that's the result. So, you see that for lambdas, smaller lambda, the bias is smaller, but in the beginning, the convergence rate is slower. And eventually, you can use the knitting scheme in plan. Scheme in black, so will outperform any kind of fixed lambda scheme. Okay, that's just so-called self-interacting fusion. So, well, one single remark. Since this is a workshop of Stockholm again, so here is nothing about game, but what we can do is to adopt this technique to the study of exotic potential mifogens. Exotic potential. Potential monotone we get and everything will well under some technical function, everything will go so we can trend the gain only with one agent it's trend with against himself. So it's again that you play with your past. Then we'll also lead you to the new field equity. That's also