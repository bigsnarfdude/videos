So I'm going to talk to you today about how we use black box deep learning methods trained on a variety of different kinds of genomic data to understand the syntax of non-coding DNA and to understand non-coding genetic variation. So jumping right in, what do I mean by syntax? So as you know, transcription factors are DNA binding proteins that bind DNA in a sequence-specific manner. Often combinatorially, they form Combinatorially, they form complexes, they cooperate with each other, they compete with nucleosomes to get access to DNA. And of course, evolution, you know, since they rarely operate independently, evolution has worked on these sequences and there often is hypothesized syntax. By that I mean specific or soft rules of arrangement, composition, the role of affinity, density, and so on. Density and so forth, and how this sort of syntax affects cooperative binding of transcription factors and effects on downstream molecular effects like chromatin accessibility and eventually, hopefully, gene expression. I'm going to focus on two main types of assays today, protein DNA binding maps like ChIP-seq and ChIP-EXO, as shown right here, for a variety of transcription factors. And of course, chromatin accessibility maps from ATAX-seq, DNA-seq, and single-cell ATAX-seq. And single-cell attack-seq. So, jumping right in, you know, the way we try to model this or try to formulate this problem of trying to understand or decipher syntax of regulatory DNA is we transform the learning problem into a supervised machine learning task. And the idea is that if you're given a regulatory profile, let's say from a protein DNA binding map or promote an accessibility map across the genome, you can imagine the You can imagine the experiment effectively labeling every base in the genome. And of course, the genome is often quite large, millions of base pairs, and so, or billions of base pairs, depending on which genome you're working with. But effectively, if you bin or tile local bins across a genome, you get a pretty nice training data set of several millions of riddle sequences. Let's say, Riddle sequences, let's say 100 to 1000 base pairs, and each of them gets mapped to some labels. The labels are coming from the experiment, and these are often binarized based on some statistical model of noise, which allows you to call peaks or significant signal. Or even better, is to avoid the binarization and to actually use some kind of normalized version of the signal itself. And so, the goal is then to kind of learn a mapping from. Kind of learn a mapping from these sequences to these labels. And obviously, there have been plenty of approaches in this space over many, many years. As I mentioned here, so the traditional approach is to model this data at a reasonably cool resolution, you know, 100 base pairs, 50 base pairs, and so forth. But over the years, you know, assays have been getting more and more high resolution. And just to give an example, like, you know, a cartoonized version of a chip exo experiment, you see these. You see these beautiful footprints, read pileups flanking the binding sites. And the precise shapes of these profiles and the magnitudes and the spans, they have pretty exquisite information about the protein DNA contacts and the syntax that's encoded in the sequence. So it's a bit of a loss of information if you take this sort of beautiful high-resolution single-base per resolution profile and you summarize it into one. And you summarize it into one scalar over 100 base pairs. The same is also true for chromatin accessibility experiments like ATAC-seq, DNA-seq. You know, transcription factors will often create footprints, either deep ones, shallow ones, or have some impact on the overall profile. If you go to historomarks, there's even more complexity of landscapes. And so, you know, what we decided to do is sort of abandon the whole idea of summarization at some kind of lower resolution and really model the data in its And really model the data in its rawest form. So, in 2021, which I think was last year, since I've lost track of time, we published this paper called BBNAT with Ziga Abcek and Julia Zeitlinger. And what this model does is it tries to change the output formulation a little bit. Rather than mapping a sequence to a scalar, it tries to do a base-by-base mapping. So, it's a sequence-to-profile model. And what it's doing is taking Model. And what it's doing is taking 2KB pieces of sequence and trying to map them to the central 1 Kb of read counts directly. So unprocessed read counts. You just map the reads to the genome. You get the five prime ends. You count them on the plus and minus strand. You directly model read counts. There are pros and cons of this approach, and I'll get into each of those aspects. But essentially, it's a simple model. It's a convolutional neural network with residual dilated convolutions. The goal is to. Convolutions. The goal is to obtain a full receptive feed of about 2 KB. So the model really sees every output base, every output position, gets to see effectively a 2KB region. And the key tricks were in the loss function. So what we did is we modeled the data with two complementary losses to capture two complementary aspects. One is the total coverage, so a scalar value, corresponding to the total number of reads over the entire sequence. Over the entire sequence. That's similar to what people have done before. But then we added another loss function, which is trying to capture the base resolution distribution of these reads across the entire genome. And for that, we use a multinomial loss. It makes sense because you have reads falling on the genome and they get distributed based on some probability distribution over each position as a function of sequence. So we fit this model. And one thing that I'll One thing that I'll refer to multiple times is: since we model the data in its rawest form, we don't do any normalization of any kind. It's very important to correct for various confounders, assay biases, control tracks, GC bias, whatever it is, right? So we explicitly have a way of incorporating control tracks or any kind of bias tracks or covariates. And effectively, the model is trying to explain as much of the signal using these covariates. Much of the signal using these covariates and then fitting the residual simultaneously to the actual sequence, such that when you now take this model and do stuff with it, like interpret it, for example, you can account for the biases. You can unplug this bias component, and then hopefully the sequence component will only capture the biology. That's basically the idea. We trained this model on OC4 SOCS2 nanograph and KLF4, four important pluripotency factors in mouse ES cells. I'm not going to spend too much time on this because the I'm not going to spend too much time on this because the paper is published, but you can take my word for it for now. Based on these two anecdotes, these are two enhancers with combinatorial binding of the four transcription factors. You can see these are in test chromosomes. So we train on one sub-subset of chromosomes. We make predictions on the rest. And it really works really quite well. At single-base resolution, we get very nice predictions. We even get a bunch of denoising. And if you quantify this chromosome-wide, a profile prediction. Profile predictions are often on par with replicate concordance. So we're pretty happy with the performance. But you don't really care about performance. Michael Hoffman had once said that, like, I think I never met a biologist who did experiments on just half the chromosomes. So predicting data you already have is not particularly useful. I think what we're more excited about is that if the models are able to make these predictions at such high resolution, These predictions at such high resolution, they must have learned something about syntax from raw sequence. And so we focus a lot on interpretation, and we have a suite of tools that we use to try to pull information out of the model, dive into the secrets encoded in the millions of parameters. So the first thing we do is we've developed this method called DeepLift several years ago in 2017. It's a simple approach that tries to take predictions of a neural network and recursively And recursively pass messages, which are similar to gradients. They are finite difference versions of gradients called multipliers. You push these through the model all the way back down through standard back propagation, and you can very efficiently derive contribution scores of every base in any sequence of interest with respect to a specific output. And we can do this for the count head and the profile head. Remember, we have two heads: one predicting total coverage and the other predicting the base resolution. And the other predicting the base resolution distribution. So we can also decouple how bases are driving overall signal versus bases that are driving shape. So that's a nice advantage of using this sort of complementary formulation. So this works quite well. If you take distal enhancers, like here's the awkward distal enhancer, here you see kind of nice binding profiles. You can obtain these deep lift scores and they will highlight context-specific motivation. You know, context-specific motifs like the auxox motif driving all four transcription factors, this motif right here, specifically driving NAROG, and then you see these KLF motifs kind of specifically popping up for KLF4. And so you can kind of use this to, we have methods to do motif discovery as well. I won't talk about that today, but you can obtain really high resolution annotations of precise sequences and bases driving or predicted to drive binding. Predicted to drive binding of these different factors in the same cell type. You can imagine doing this across many TFs in many cell types and obtaining really nice projections of these profiles onto the sequence. So now you can almost obtain base resolution interpretations of what is the causal signal likely driving binding, which is nice. But I really want to focus on syntax, as I said before. So, you know, annotation of motifs. You know, annotation of motifs and so forth is nice, and our methods certainly improve over the previous state of the art. But I think the most powerful aspect is now we have this in silico machine, which is extremely accurate at taking sequences and predicting really high resolution profiles. So let's use it like a simulator, as an oracle, and ask it questions. So, if you have a black box and you can't really interpret its parameters, you can desperately try to do that, or you can just ask it questions. That or you can just ask it questions and hope that it answers the right things. So, we decided to take the easy approach, which is just ask the model questions by designing sequences systematically and letting the model tell us interesting answers. So, here's one sort of such simulation where we insert like a narrow motif at this position, Oxox motif at this position, and we specifically want to test just the marginal effect of spacing of these two motifs. These two motifs. And you're seeing the model is trained on Nanog, and the model is trained on OP4 binding. And so, as we change the spacing between these two motifs, we can graph the response of NAOG and the response of OP4 explicitly to spacing, accounting for all other. So, we integrate this over many, many different backgrounds, hundreds of different, thousands of different backgrounds. And what we see is something quite very interesting. You see the OP4 in red. You see that Oct4 in red really doesn't care about the spacing syntax at all, it just does its thing. Nano, on the other hand, really cares. You see an almost initial linear to an exponential increase in occupancy. And then as you get closer, the two ODs get closer, you also start seeing a beautiful periodic preference. And that periodicity happens to be exactly 10.5 base pairs, which is a helical tone of DNA, telling us that. Telling us that Octsorts and Nano Motifs prefer to be on the same side of the helix, which likely drives cooperativity. We can replicate this experiment, of course, in genomic sequences too. This is sort of a version of in-silico genome editing. This is more like an in-silico reporter experiment. Here we are taking actual, again, the OCFO enhancer, and we can take predictions from the model. We can interpret the exact sequences. We can interpret the exact sequences driving the predictions. We can then perform systematic mutations. So we scramble the OXOX motif and see the model predicts dramatic drop in OCFO binding and Nanox binding. We can mutate the Nano motif and see that OCFO doesn't care. Nanog actually gets a hit, but you can still see some residual signal indicating that OXOX can recruit Nano even if the Nano motif is actually missing. And if we integrate this. And if we integrate this information across the whole genome, we see the same sort of asymmetric cooperativity mediated by sequence syntax manifest. It's of course noisier because we cannot eliminate just by averaging observed genomic sequences that are biases, meaning biological biases, and we can't exactly marginalize the effect of just the syntax. So, in the simulation experiments, synthetic sequences and the in vivo genomic sequences, they provide two complementary approaches to try to. Complementary approaches to try to verify syntactic relationships to make sure you're not going out of distribution. Because in simulations, you can do whatever you want. If they don't hold up with genomic sequences, it gives you a bit of a check whether you're testing something that really isn't observed much in the genome or is out of distribution. So we can learn all these syntactic relationships, which is very nice. Some of them are really short range. We hypothesize that these are due to direct protein-protein interactions. To direct protein-protein interactions, and others are pretty long-range in the context of local syntax. That is, they can extend up to like 150 base pairs, which is really far away to have direct contact. So it's likely nucleosome-mediated or chromatin-mediated cooperativity. And there are really interesting transfer functions as a function of just spacing. Of course, you can then simulate affinity, you can simulate density, you can actually study all the axes of syntax using this. Of syntax using this sort of framework. And we've been doing this now for thousands of transcription factors. We hope by the end of the year to have a pretty solid atlas of syntax for many, many TFs based on publicly available data. Of course, this is all hand waving without actual experiments. So Julia was amazing and her lab did some really exciting CRISPR experiments. We induced mutations at this enhancer containing nanoorgan SOX2 binding sites. Here's a SOX2. Nanog and SOX2 binding sites. Here's a SOX2 motif, here's an NAOG motif. These are the model's predictions for the Y-type sequence. The model says if you take this TT and mutate it to an AG, you will see a pretty big effect. This is the predicted effect, and then this is the observed effect in the actual CRISPR experiment. This sort of verifies the medium-range SOX2-SOX2 cooperativity because you can see not just some attenuation of signal at the SOX2 motif, but also at this nearby footprint, which is another SOX2 motif. So you can see some attenuation there. So, you can see some attenuation right there. We can invert this and do the same mutation but measure nano binding. And again, the predictions of the model land up really nicely. You can see a pretty large-scale effect of mutating the SOX2 motif on nano binding. You can then mutate the nano motif that's nearby, and you can again see a relatively smaller local effect that captures the nanog-nanog cooperativity, and then you can mutate the nanog motif and look at effect. You can mutate the nano-motif and look at effects on SOX2. The model predicts no effect, and the data shows no effect. So, this again verifies some of the asymmetric cooperative effects that the model was predicting. We also found that the model's representations generalize very nicely to other experiments. So here, what we do is we freeze the model, we take the final layers representation, and we fit it a simple linear model to attack-seek delta attack-eekk. Delta attack C fold changes after trans depletion of OC4 and SOX2. And this is other data that we did not generate previously published data. You can see that the binding representation, a model trained on OC4 and SOX2 binding, just through a linear transformation, generalizes very nicely to delta taxi, right? So predicted versus observed fold changes post-October depletion or SOXFOR depletion, we get very nice correlations. And also Get very nice correlations, and also, if we apply the same representation layer to train a linear model and MPRAs, we again see very nice correlations. Again, this is all on test data. Okay, so we are fitting a model, but we're holding out sequences and then making predictions. So, this tells us that the binding syntax we're learning is not just some useless syntax just driving binding, but has no impact downstream. It looks like the representation does transfer to other readouts. So, what we've recently done. So, what we've recently done, so this is published work. I'm going to talk now about mostly unpublished work. So, we recently extended this model to ATTAC-seq and DNA-seq. And you might think, well, this should be really easy. Just apply the same model. It should work. That rarely ever happens. Every assay is unique and has its own little kinks. And so, we have to always spend a few years massaging a model to get it to work. The biggest problem here for us. The biggest problem here for us was: unlike chip exo data, which is really clean, and even chip-seq, for example, even though it's chip-seq is pretty dirty, compared to attack-seq and DNA-seq, even though the attack-seq DNA data often are really beautiful, high quality, high read depth, the effects of enzyme biases are much more pronounced. I'll show you in a few slides. And this has been documented before. And this has been documented before, so it's not new, but it was surprising how much effort we had to put into kind of eliminate this bias. And I'll tell you a few stories about why that is the case. So as you know, like attack DNAs also generate beautiful footprints for transcription factors, but I just want to give this cartoon example of how bias can really screw up your signal. Let's say this is the true accessibility that you're measuring, trying to measure latent accessibility. And let's say there's an enzyme that only like produces signal. Only like produce a signal at C's. This is, of course, just a hypothetical example. If you use this enzyme to study accessibility, you would get a signal that looks nothing like the true latent accessibility. And, you know, DNAs and ataxic biases have been characterized before using sort of simple motif models, either K-mer models or motif models. You can just take naked DNA, treat it with TN5 or DNAs, and With TN5 or DNAs, and you know, just design, I mean, learn a position rate matrix or a K-mer model over all those positions, and then you can use it to predict an expected cleavage profile just based on sequence composition. And then you can like do an observed by expected or whatever to get some kind of footprint probability. All the deep learning approaches that have been used to fit ataxic DNA's data almost entirely work on either binarized data at low resolution. At low resolution or continuous data, but at low resolution again. I don't know of methods that have actually directly applied this to model footprinting at single base resolution. So we tried some of these bias approaches, bias correction approaches, these PWM models and sequence models, sorry, K-mer models. They did not work. And I'll show you what I mean by they did not work. So we thought there's something off with these. So, we thought there's something off with these bias estimations. And so, why not, instead of learning some simple, you know, underparameterized model for bias, why not just train a neural network to learn bias? So, why not do that? So, we just did it. So what we did is we take the DNA's attack data, and instead of modeling the peak regions, we model the background because the assumption is the background is going to contain either a completely uniform distribution of reads, which is unpredictable from sequence. Of reads, which is unpredictable from sequence, or it contains a pretty predictable distribution of reads, which would be some form of TN5 or DNA's bias. So that's what we do. We just take the background regions that are outside peaks, and we fit a much smaller, simple neural network, because you don't expect bias to be super complicated and have like very complex syntax. But what's really interesting is, in the interest of time, I didn't want to go too deep into this, but when we interpret this model, Uh, when we interpret this model, the bias model trained on chromatinized background, uh, we actually identify uh eight motifs for taxi for tn5. It's not one little motif. The primary recognition code really is the classic PWN that people have built before, but there are at least eight other pretty different looking motifs that capture TN5 bias. So it's not a simple, you know, one motif recognition code. And that's one of the reasons why we found that using those motifs to do bias correction. Using those motifs to do bias correction in a neural network doesn't really work as well. Okay, so what we do is now we have this bias predictor, right? It takes any sequence and predicts what enzyme bias, background enzyme bias will look like. So now we can take any sequence of interest. We can actually shove it through this bias predictor. We can predict bias at really high resolution, denoising and desparsifying it. And now let's say you're trying to fit this model to a tax-seek data or DNA-seq. To attack-seq data or DNA-seq data in peak regions, you can predict the background bias, which is sort of convoyed with the actual signal. You can now fit a profile-to-profile model. It's a very simple linear regression. It tries to pull as much of the signal out as it can. And then you, in the second stage, you fit the residual with an actual BPNET neural network. And it's a bigger BPNET model with more filters. That's it, nothing else different. That's it, nothing else different. So, this works actually really, this works well. We again get high correlation of count predictions, and we also get very nice predictions of profiles at single base resolution. This is the Jensen-Channon divergence distribution of Jensen-Channon divergence between observed and predicted profiles. This is JSD between replicates, sorry, pseudo-replicates. This is JSD between observed data and randomized data. And this is JSD between observed data and average profile. Data and average profile across all peaks. Okay, so you want to be as close to this and as far away from this, you're pretty close. So it's pretty nice. So let me show you actually how this look, what this looks like at single-base resolution at actual enhancers or promoters. So we're going to look at this particular promoter for a good reason. It is a CDCF binding site, a gold standard. We know what CDCF footprints look like. So I'm going to start with that. Apparently, this is what a CDCF. Apparently, this is what a CDCF footprint looks like in a tax-seq data. This is the observed data at single-base resolution with no correction. If we fit a Chrome VPNAT model directly to this data without any correction, we actually get very nice predictions. You can see it really looks very similar to the observed data. When we interpret this model, we see the CTCI motif and a pile of garbage around it lighting up. Initially, we didn't know what this was, and we thought, oh, it's noise in the attribution scores. And the attribution scores, and we tried all kinds of fancy attribution priors and so forth. We can then, we now have this bias model, so we can predict what the TN5 bias profile would look like. This is what it looks like. It looks actually ridiculously similar to the actual attack-seq profile. You'll notice that if you interpret it, all this garbage nucleotides jumping around is exactly mimicked in the bias model, except for the CTCF motif. So now if you regress this bias. So, now if you regress this predicted bias out of the signal and you fit the residual to the sequence, you end up with this gorgeous latent footprint. You can see it hugs this, and when you interpret this corrected model, you just kept the CDCR motif. All this other noise just disappears. So, the attribution noise is not attribution noise, it's actually bias in the profile. The model is learning it. So, we suspect actually a lot of instability. Actually, a lot of instability and so-called attribution noise that we've previously observed and others have observed is likely coming from actual experimental biases in the signal that are a function of sequence. And if you fit these models without worrying about bias correction, you are going to get a lot of these spurious features lining up. With a very powerful model, you don't have an option. It is going to learn whatever you give it. So this is really nice, a beautiful correction. And you get these really nice denoised footprints, a single base. Denoised footprints at single base resolution at individual loci. These are not aggregate footprints. This is a footprint that you derived using the neural network after denoising at an actual promoter. And this is just showing like what you get with hint attack and probias, previous methods that do corrections using PWM models or K-mer models. HintATAC really doesn't work very well, at least in our opinion. You can see the profiles look pretty similar. You can see the profiles look pretty similar. There's some partial correction, so it does sort of attenuate. You can see the bias a little bit. CDCR motif lights up a little better, but there's still bias. Tobias actually works pretty well, but it does a pre-correction, which means it cannot actually impute missing signal. The neural network has two advantages. It corrects the signal and it can fill in missing information because the signal is very sparse due to sequencing depth. But the model predicts the expected value at every position, and so it can actually fill in. Position and so it can actually fill in the blanks. So, actually, you can see that the Tobias predictions are quite similar to us. For example, this spike that you saw on the right, in our bias-corrected predictions, moves to the left right here. And you can see Tobias moves it to the left as well, right? Exactly aligning our predicted position. Similarly, this one, right? So, if you really look at the footprint, it's much sparser, but it is really capturing the same bookending locations. And you can see again, the correction is. Locations, and you can see again the correction is much cleaner. The CTCMotive lights up, there's better attenuation of the bias, but it doesn't fully eliminate it again because the recognition code for TN5 is not a single PWM. It's more complicated. So we're just kind of, again, standing on the shoulder of previous giants, right? But using a neural network to do a better job. Now, of course, you know, CDCF always looks great. Know CDCF always looks great. So, I wanted to show you what happens with other motifs. Here, I'm showing you some examples of four transcription factors across three cell types for TAC-seq. NF-kappa-B, which is very specific to LCLs, GATTA specific to K562, HNF4A specific to HePG2, and SP1, which is common to all three. And you can see if you don't do bias correction, you look at these profiles of single-base per resolution, they actually look very similar. They actually look very similar. The bias totally dominates footprints, right? Completely dominates. As soon as you do the correction, you see a really nice cell-type specificity. This is the predicted latent footprint for NF-Kappa B. Here's a weak footprint for GATTA, which has low residence time. Here's a deep footprint for HNF4A, which has high residence time. And here's footprints for SP1. And you can see that in the cell type where the TF is not expressed, you see nice flat lines indicating very nice. Flat lines indicating very nice correction. Now, another really nice proof of the pudding is DNA-seq and attack-seq are supposed to capture chromatin accessibility, but if you look at them at single base per resolution, they look completely different, right? So here's attack-seq at single-base resolution. This is the typical smooth track you see in genome browsers. This is the uncorrected predictions if you don't correct for bias. Again, we see very nice predictions. This is the corrected predictions after bias correction. Corrected predictions after bias correction looks totally different, right? You see this really sharp footprint lining up area. There's also a dip right here. If you match this to DNA's data, you see that the DNA's data, even without correction, actually matches these footprints that we're picking up in the attack seek, right? So it looks pretty nice. After bias correction, DNAs also does have bias. It is not as bad as TN5. So the profile doesn't completely get reshaped, but there are subtle changes. But there are subtle changes which are important because otherwise you still get spurious predictions. But after correction, you again see these really nice footprints. Qualitatively, what happens is these footprints really start aligning with each other much better. And the most beautiful aspect is even though the footprints don't look exactly the same, which they shouldn't because DNA is much smaller than TN5 and they have different chemistry and so forth. Once you interpret the models on the bias-connected data, they are Bias-corrected data, they are literally identical. Like 95% or 98% of the answers we look at, you pick up a very, very similar syntax, or at least annotations across the board, at least in the central regions. There are differences in the flanks because ataxi captures nucleosomes differently than DNAs does and so forth. But in the core regions, you pick up very, very similar syntax. Now, I want to show you also what happens if we change sequencing depth. Happens if we change sequencing depth because these models were trained on very high depth profiles: 500 million reads, 100 million reads, and so forth. Usually people do not sequence data this deep. And especially nowadays with single cell adaptic data, you're lucky if you get 5 million reads in a pseudobulbic sample with a handful of cells, right? 500 cells, 1000 cells. So, can this approach actually work there, or are we just stuck? So, we try to do some experiments. So, we tried to do some experiments to test this. So, we started off with really deep data, 500 million reads, and we fitted a model. This is the observed data. This is the predicted bias-corrected track. This is the interpretation. This is the beta globe in LCR, the very famous enhancer that we actually, it's one of the main figures in one of the ENCODE papers because a lot of the motifs are conserved in this sequence as well. So, you start with 500 million read data, you fit a model, you get predictions of the Fit a model, you get predictions of the corrected footprints and the underlying attributions. You can do the same for 100 million reads, the same for 15 million reads, 25 million reads, and 5 million reads. This enhancer barely has like six reads at 5 million, but you can still see that the predicted bias corrected profile looks very similar to the 500 million one. And the underlying attributions are also really quite clean. Ask me why the gamma lights up stronger later. So, this is really nice, and we actually see this genome-wide. So, we can quantify the degradation of signal. This is the Jensen-Shannon divergence between the observed data at 500 million reads and the observed data at different sequencing depths, right? So, this is 500 versus 100 million, 500 versus 50, 500 versus 25, 500 versus 5. You can really see this dramatic degradation. This dramatic degradation of signal. If you do the same thing for the predictions, it's much, much better, right? You're seeing degradation as you should. It's not magic, but you can see the power of imputation. Like the model can impute missing information to the expected values, and you actually get very minimal degradation. And just to show you that this actually works at the level of individual footprints as well, if you take CDCFs, PI1, and FYB, GABB, these are not CTCF, these are very different motifs. Very different motifs. These are the footprints at 500 resolution, sorry, 500 million reads, at 100 million reads, 50 million reads, 25 million reads, 5 million reads. So you're seeing degradation, but there is definite sensitivity. You can still pick up all of these motifs. You can pick up footprints. What you lose, and this goes back to this, why the GATA lights up much stronger here, is you lose the relative power of different motives. There isn't enough signal in the data for the model to. Enough signal in the data for the model to really rank the relative contributions of motifs. It does learn all the motifs, but it learns a little bit more of its hedging its bets, you know, sort of getting the exact syntactic effects and the relative contributions of different motifs very accurately. But we can live with that. It's better than nothing at 5 million reads. So, this is quite nice. And so, now we can actually use this to prioritize mutations and variants. And as you know, you know, a large number of genetic variants associated with diseases lie in non-coding elements, especially common variants associated with various diseases from GUAS, the strong enrichment in regulatory DNA. So, what we can do is try to use our models to predict effects of variants and in a slightly different way. Our models really predict. I'm always really predict profiles as single base per resolution. Usually, people predict variant effects or quantify variant effects by comparing course resolution counts, again, scalar values over some region around the variant. We don't need to do that. We actually have single base pair resolution profiles. So we can look at the two orthogonal components of the signal. What effect does a variant have on the overall coverage? And what effect does a variant have on the shape? These are two interesting different. Shape: These are two interesting different questions because, as I'll show you, you can actually infer really interesting observations from the shapes of the profiles. So, these are predictions from the model at a very well-validated QTL. This is a variant in LCLs that affects pi1 binding, which is a pioneer factor. It affects chromotin accessibility, and it affects histone marks, and it affects expression. So, it's one of those great examples of a variant propagating its effects through multiple layers of. Through multiple layers of regulatory readouts. This is the reference allele, the C allele. You can see it produces a moderate profile for binding. This is predictions actually from BPNAT models trained on SPY-1 binding, trained on ATTAC-seq, sorry, DNA-seq, and trained on H3-K2NAS and acylation with different receptive fields. So the C allele produces a moderate profile of binding, a moderate profile of accessibility, and a decent profile for acetylation. For acetylation, we can switch to the G allele and you see two things happening. Of course, you see a dramatic amplification of the signal, but you also see very interesting changes in the shape, right? So notice how this valley right here hits at this position. And once the mutation is activated, like once it switches to a G allele, you see two cliffs. You see one cliff right here, which matches the previous cliff, and a completely new cliff, the much stronger cliff, is actually right here at this position, right? Actually, right here at this position, right? So, something happened, and the entire shape of the profile changed. It isn't just like a magnitude jump, it's not like the same profile just kind of like increased, you know, stretched out. There's something that changed the morphology as well. So, if you interpret the sequence, what you see is something really interesting. You see that there is actually, if you take the C allele, there is a weak SPI-1 binding site right here. It's a low-affinity binding site. And you can see kind of it's even weaker in accessibility. Of it's even weaker in accessibility and has pretty much not much of an effect on histone marks. And there's something else, actually. There are other motifs to the right driving the histone mark signal and the accessibility signal for the C allele. This SPI1 motif is really weak. When you switch the G allele, you create this really strong SPI1 motif when the two cooperate to drive binding, and they dramatically amplify accessibility and they amplify histone marks as well. And this is what explains this. And this is what explains this huge difference in the profile shape: this weak motif driving this cliff and this really strong motif creating this new cliff. So, you can see the power of looking at the sequence profile. You can get much more interesting insights into syntax and how it drives profiles. This also means we can actually derive new metrics for variant effects, not just ratio of counts, or some kind of Poisson negative binomial p-value. We can actually look at Value, we can actually look at changes in profile shapes. Each of the profiles is represented by a property distribution multinomial. We can do log likelihood ratios of multinomials. We can do Jensen-Shannon divergences. These are going to capture differences in shape. And so we do that. What's really exciting is if we now use these new metrics, shape-based metrics for variant effect prediction, we're validating here against DSQDLs. So the models, this is a DNAs model and ATTCK model. Models: This is a DNA model and attack model. You can see that if we compare to SVMs, which are actually really quite good, they're pretty hard to beat, to be honest, particularly because of their stability. If we use our models, which are really fancy, and we use the count head, the actual count head, we don't see much of a gain over the SVM for DNA-seq. We see a bit of a gain for TAC-seq. Remember, the bias really helps. The bias correction really helps. However, However, if we use the profile shape metric, you see big boosts in AUPRC. A pretty solid jump. It's not minor. And so we're now using these approaches and we see across the board pretty strong effects of this. I'm out of time, so I'm not going to waste. I'm not going to take more time. I'm going to quickly just say that we've applied this to single-cell attack C data and human cardiogenesis. And human cardiogenesis, we can really apply this to pseudo-bulks of rare samples, rare cell types. We can score demo mutations, you know, again, reference alternate alleles, and we can get really nice predictions, a single base resolution from single-cell attack-seq pseudobulb profiles. We can interpret the underlying syntax. We can really predict which cell types are driving effects, we can localize specific phenotypes of congenital heart disease to specific cell types. Genital heart disease to specific cell types. For example, we see really strong enrichment in cases versus controls only after using the model. We see this specifically for structural defects in arterial endothelial cells and capillaries. We see very strong enrichment convergence of predicted non-coding effects from de novo mutations and coding mutations affecting the same genes. So you see very nice convergence of these across different patients. And finally, we can perform CRISPR-X. And finally, we can perform CRISPR experiments in IPSC-derived endothelial cells, where we actually knock out those enhancers that are predicted to affect gene expression of very important cardiac genes, and we see pretty solid effects. So with that, I'll just summarize that I think I showed you that basic resolution neural networks can predict bulk and single-cell profiling experiments at really high resolution and accuracy. They can be queried. They can be queried. They're not black. I mean, they are black boxes, but you can ask them questions. They can reveal novel, subtle syntax properties. They can decipher regulatory variation. They can be used to prioritize non-coding mutations. And as I showed you, you can also use them to design editing experiments. And I hope that these models and all the work that many of you and all of us are doing will start forming in silico, foundations of in silico platforms for biological discovery, hypothesis generation, and model-driven. Hypothesis generation and model-driven iterative design. I hope we'll move beyond single anecdotes we publish in our papers and design systems that can really extract the beautiful stories that are encoded in these models. There's a lot that they can tell us. And of course, none of this is possible without my lab members who are amazing. They've really built this over many years and the extensive group of collaborators who provide us the data and the support for validation and the funding sources.