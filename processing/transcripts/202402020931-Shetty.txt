Alright, hi everyone. This is a unique situation where I like, I think I know everybody in the audience, so that puts me a little bit more at ease than I would be usually. But yeah, I wanted to start with thanking both Tanya and Yanon for inviting me. Oh, thank Yanon for a lot more than just that. As you'll see, I did my PhD at Penn Statement with her, and this is joint work with her and Jue, who gave a really nice talk yesterday, so you guys can see it too. This is a part of my discussion. This is a part of my dissertation. This is one of the chapters, and it was robust estimation for non-ignorable missing data. Before I get into the robust estimation, let's just briefly talk about different types of missing data. So broadly, and I know most of you are familiar with this, so this, pardon me if this is just repeating a lot of stuff, you know. Missing data broadly, you can classify it into two types. Classified into two types, ignorable and non-ignorable. And from what I've heard, this is sort of similar to the informative versus uninformative censoring. So the ignorable missingness, here at the probability that observation is going to be missing, is either completely random or at random, in which case it depends on some observed data, some fully observed data. So an example of that is it's shown that if you ask people or you give a survey out about People, or you give a survey out about depression severity, men are less likely to respond to it, which means you'll have missingness and depression variables, and it depends on gender, which is fully observed most of the times. And on the other hand, you have non-ignorable missingness. This is a little bit trickier because the missingness can depend on unobserved variables and including the variable that has the missingness in it. Missingness in it. So, a classic example is income. If people have really high income and you ask them what their income is, they're less likely to disclose it. So, you're going to have missing data and income, and it depends on what that potential value of the variable would have been. Another, it's the same case with things like depression, because people who have high levels of depression are less likely to answer a survey about depression, right? So you're seeing missing data in the variable, but that missingness. But that missingness has a pattern that depends on that variable itself. So, a lot of work has been done with the ignorable missingness. Non-ignorable missingness, it's a little bit trickier to handle, so it's still sort of an ongoing area of research. So that's the one that I'm going to focus on today. So, just to give you sort of an example of a data set that I will use, for this particular paper, we looked at the Korean Labor and Income Panel Study data set. Labor income panel study data set. It's got about 2,500 observations. What they're interested in, what they were interested in, was the income in 2006, but there was about 35% missing data in it. What they had in addition to that was income in the previous year, gender, age, and education. And all of these were fully researched. So can we find, say, like the mean and income in 2006? Say, like the mean and income of 2006 using these other fully observed covariates. And mathematically, the data set that I have used for this paper usually has the following couple of components. We have y, which is going to be our univariate outcome, and that will be the only variable that has the missingness, so it's only missing outcome. And it will have non-ignorable missingness. That's the assumption we're making. Or that's the setting we're studying. Or thus the setting we're studying. And then we'll have a fully observed set of covariates, which I'll denote by x. And then we'll have r, which is a missingness indicator. So r is 1 when you observe y, 0 when you don't. And like with the sensor data models, you have two components. You have your regression model, which is the relationship between your y and x. And then you have a propensity model for missingness or missingness propensity, which is the probability that. Which is the probability that r is 1 given everything else. We typically use pi to denote that function. Under ignorable missingness, you can often sort of simplify pi into just a function of theta, some set of parameters, or theta and a fully observed set of covariates x. Whereas for the non-ignorable setting, you can't sort of simplify this any further, because it depends on y itself. Depends on y itself. It can depend on either the full set of covariates x or a subset of these full set of covariates. So there has been some work with non-ignormally missing data. Usually what you have to do is decide how you want to model those two components, the regression model and the perpetratory model. Parametric assumptions were sort of the early works in the area. Sorry, sorry, did you? Sorry, sorry to swear. No, you go. But since both of these models involve missing data, they're really hard to verify if they're actually valid models or not. And so parametric models, they're probably a little too inflexible to consider. The best case scenario would be if you could just let both of them be non-parametric, make no assumptions, or minimal assumptions. However, Robinson Brutope, way back in 97, showed that that. Way back in 97, showed that you can't have identifiability. Both the models are not remarkable. And so then there were a series of works where either the regression model was parametrically modeled or the propensity model was parametrically modeled. And they do well, but again, you're assuming a parametric model, which is still pretty inflexible. So if you have the most specified model, you're not going to get consistent estimates. So the paper that we sort of build on is shadow. We sort of build on is Shao and Wang in 2016. What they did was they assumed a non-parametric model for the regression, but then considered a semi-parametric model for the propensity score, which is just adding a little bit more flexibility than you would if you merely had a parametric model. So, this is the form of the semi-parametric model that they suggested, or they have. Have in their paper. One of the key parts of this is for this model to be identifiable, you need to be able to split your set of covariates into two parts, u and z. So x needs to be split into u and z, where your regression model can depend on all of x, but your propensity model should be independent of z given the other two variables. Sorry, this should be a u. So r should be independent of z given u and y. Be dependent on z given u and y. And this is necessary for identifiability. So z cannot be an empty set. You need to have some variables that you can kind of, they're called shadow variables or instrumental variables. So you need to be able to make that like, yep. Income example, could you tell us how you install that income example? Yeah. So what we do here is we use the monthly income in two. Use the monthly income in 2005 as U and then different combinations of the other three as Z. The idea is that if you're looking at missing data in 2006 income, 2005's income probably does a good enough job explaining that. And then these other variables just affect the Y directly. To be honest, I didn't make that choice. I followed what Shaw Wong did, but it's pretty intuitive in this case. But that But that is actually kind of an ongoing people still haven't quite fully figured out how to choose shadow variables, but there are different suggestions. Yeah. Does the Z, does the shadow variable have to be informative or can it just be like a fabricated variable for the sake of? No. So what is important is while Z should not be in this propensity model, it has to be in the regression model. Because otherwise, Regression model. Because otherwise, then Z is just not a part of your data set, right? It's just an extra variable. So it must be an important variable in the regression model. That is actually one of the ways you can choose Z, is if it's not important to Y in the regression model, then it's probably not a good version of the variable. And then, sorry, any other? Okay. And then, so specifically the semi-parametric model that they proposed has two parts. Proposed has two parts. The parametric part is a function of y. So h is a known function of y with some parameter beta. And then the dependence on u is through a non-parametric component g. So g is a non-known function. So essentially what we need to do here is you want to estimate beta, you want to estimate g, and then the overall goal is essentially to estimate some function of your outcome. So, like the mean of the outcome, the median of the outcome. The outcome, the medium of the outcome, right? That's the true goal. That's what you want. And beta and g are just sort of, they help you in estimating that final quantity. And so Sean, I'm doing something. Sorry, did you, sorry if you talked about it. What was that last thing, that h of the beta 1 can be equal to the h of the beta 2? Oh, you just need, sorry, I didn't talk about it. It's just an identifiability. It can be a trivial function of y. Okay. Where? Okay. Why. Okay. Where? Okay, yeah. Okay. There are some more identifiability things, but I'm just skipping over those. We can talk about. So what they did was they essentially proposed some estimating equations based on the fact that this conditional expectation goes to zero. And then for each like candidate beta, they have to go and estimate g hat for each observation. So as you have Observation. So, as you have a lot, if you have a lot of observations, this becomes quite a computationally complex process because for each candidate beta, you go estimate n of these other variables, go back, estimate beta, do the same thing, right? And so, what we found was it really takes a long time to run their code. And so, our goal was to, one, use semi-parametric methods to estimate the parameters in this model. And what we found was when we did that, And what we found was when we did that, we actually didn't need to estimate g of u. So our estimates for beta are consistent even with like a misspecified g, which actually saves us quite a lot of computational time. So that was like the objective of our paper. That's kind of like what I'm going to talk about in this talk is overall our goal is still to estimate the mean of the outcome or some function of the outcome. Some function of the outcome. And we'll do that by estimating beta first and do both of those consistently without estimating g. Okay, so yesterday Jwe talked about like the semi-parametric estimation, and I've talked to a couple of people. We've decided it's not the most easiest thing to talk about. But I did exactly what he did, right? So I found the Neeson's tangent space, found the orthogonal, and then projected my score function for beta onto it. Score function for beta onto it. What that gives me is an efficient score function for beta. And it has a complicated form. We don't really need to pay too much close attention to all the little terms in it. But essentially, once I have this, I can now build an estimating equation. I just sum over the efficience core function over all the observations equal to zero. And when I solve this, the beta that I get back is going to be a consistent estimator and efficient too if I knew every single To if I knew every single component in there. Components to pay attention to here is we do have g in this efficient score function, that non-parametric component that we don't want to estimate, right? And it also pops up inside of A of U and D of X. And then we also have these conditional expectations of Y given X and 1. And when I say X and 1, it's X and R equals 1, which just implies that this is essentially. Which just implies that this is essentially a conditional expectation of the absorbed data. So that's a full data problem, right? I have all the information required for that conditional expectation. So let's first tackle the fact that this g is showing up in the efficient core function and we want to try to avoid estimating it, right? So what we found, and this was sort of like a nice surprise to us, honestly, is if we plugged in a working model G-star, A working model g star into our efficient score function. Let's call the new one SEFX star, indicating I've replaced all the G's by G stars. The resulting new estimating equation, or this new function, still has mean zero. So what that means is if I build a new estimating equation with this SEFF star, I'm still going to get consistent estimates for beta. There will be some loss in efficiency. Loss in efficiency, but they'll still be consistent. Yep. Are you required to get the propensity score model correctly? No, because G is a part of the propensity models. So I just need the parametric component. So that was a nice little surprise that we found, and we're like, yeah. And then the other part is a lot easier. We have these expectations. We have these expectations, conditional expectations, but like I said, they're all conditioned on r equals one, so they're a full data problem. So you can use any method to estimate them. And I'm just denoting them by E hats to indicate that we do that. And so if we finally have this new estimating equation, which has both the star and the hat, so you have G star in place of G and all the hats. And that's what we'll use, right? The E hats, again, you can do them a couple of ways. The one in the paper. A couple of ways. The one in the paper, like the three conditions we looked at of the paper, was if they were known, which obviously we don't, so that's like the baseline. And then we can estimate them parametrically. So we derive what it looks like if you estimate the conditional expectations parametrically. But the case that I'll focus on is the non-parametric estimation. So we use like another L. Watson kernel estimator to estimate that E hat. We looked at the asymptotics for that number. We looked at the asymptotics for that non-parametrically estimated version. So this is beta tilde hat. It's still with the G star, and then all the expectations are non-parametrically estimated. And it's root and consistent, asymptotically normal. There is a loss of efficiency that comes in both from the fact that we have the G star and some from the fact that it's not parametrically estimated. But if G star were equal to G, then it does achieve the semi-parametric. Then it does achieve the semi-primetric efficiency bound. So all of these red parts would disappear, and what you're left with would be the efficiency bound. So we're still getting consistent results, even though we're plugging like a random working model for G star. Yes. So the K would disappear or it would. Yes. Because the K is just E raised to negative G times isn't a constant, so this term would just go to zero. And then all the these stars would also disappear. Stars would also disappear. So we've estimated beta, but like I said earlier, the overall goal is to learn something about the outcome, right? So some function of the outcome. And so let's say the simplest case, we're interested in the mean of the outcome, e of y. How do I estimate that once I've done the beta estimation? So you can split it into two parts, the observed data part and the non-observed part. And the non-observed or the missing data part, right? I'm just using Bayes' rule here. The first part is easy to estimate. This is observed data, so the expectation of y given r equals one is simply the mean of all the observed yi's. The missing data part, I have a conditioned on r equals zero. First, I can kind of write a condition on x as well. What's important is we can write this expectation of y given x and zero in terms of In terms of condition, two expectations condition x and one, which again, once we can write this as a condition on x and one, it's now a whole data for all time, right? Again, it's easy to estimate. I've just sort of left the math there, but if anyone's interested, it's just algebra. And then I can plug it in, so now we can also estimate that missing data part using only what we've observed. So these are expectations. Right, so these are expectations that we have discussed estimating already, and that's just R. So I'm just putting the two components together so I have an overall estimator for my sample mean, outcome mean now. And what we found is, first, note that this does not actually involve G at all. So as long as I can plug in. So, as long as I can plug in a consistent estimator for beta, the resulting estimator for E hat is also consistent for a working model G star because it just essentially gets all the properties, nice properties that S has, and G is just not in it anymore. It's just independent of G. So it turns out not only can we estimate beta consistently, but also like the function of an outcome consistently without having to. Without having to model ng at all. And again, you can generalize this to like any function of y and x or something that looks like this, which would be like the median of y. So any big function of y and x. So I've shown you the theory, and I'm just asking you to blindly trust me. So let's do some little simulation just to see if it actually works. So this is a pretty simple simulation setting. The shadow area. Setting. The shadow variable z is just binary 102 with equal probability. We have a two-dimensional u just to see if what happens in these scale dimensions a little bit. And they're normal given z, and then y is also a normal variable that depends on u and z. And then the missingness mechanism, I have a linear function of y and a quadratic function with u. And the working model I've used, instead of like the true g is a Instead of like the true g is a quadratic function, but I just use a linear function to see what happens and if it should make sense. So these are the results for estimation of outcome mean, E of y. I have the oracle, which is if we knew all the data, what would be the sample mean. I have naive, which is complete case analysis, so only looking at the observed whys. SW is the shallow-wong paper of the people who proposed the perpetrating model, and then our method. To the proof density model, and then our method. Note that all of these are multiplied by 100, so they're not actually, I don't actually have eight bias for negative 0.1 beta. And so naive estimator, the complete case, as expected, is biased because you're only looking at the observed y, but the missingness depends on y. So you're missing some information, right? And that bias doesn't go away as sample size goes up. The Oracle obviously does well, but that's. The Oracle obviously does well, but that's like the golden standard. As for our method in Shan Wang's, we do really well. Our biases are usually a little bit lower. Standard deviations in this case a little bit lower. But in fairness, we're just pretty similar or better, one of those two scenarios. As for inference results, our coverage probabilities are still close to 95%, and we do a good job of sort of estimating the standard errors as well. Note that I don't. Note that I don't have inference results for Xiao Mong. That is because their method really takes a long time. So, like, when we got to 1,000 sample size, our method for like one iteration, one simulation took like 10 minutes. Theirs takes 16 hours. And so we decided that we were not going to spend our time sort of running their model for a long time. But this also goes to show that our method really does, it does equally well, but it saves you so much. Equally well, but it saves you so much time, right? So, simulations-wise, things looked good. Going back to that real data set that we were interested in, again, as a reminder, why that we're interested in is the monthly income in 2006. That has the missing values. And then you have four barriers: income in 2005, age, gender, education. 2005, age, gender, education. And Sarah's already asked this question, but how we split our covariate set was: U is the income in 2005. We think that if you have that, then the missingness is independent of the other three variables. And then we just use different, so these are all discrete of categorical variables. So we used different combinations of these three variables as our shadow variables and looked at what happened when you changed them. Couldn't change them. And I just plugged in some working model. Honestly, like the first model I put in gave me these values. So it's just a linear function of u. So these are all the different combinations with those three shadow variables. Like what it yep. But a working model, so it really doesn't matter what you should do. Yes, I mean obviously like depending on how Obviously, like, depending on how bad it is, your variance will go up. But to be fair, like, when I did this paper, I didn't have to search for a working model at all. I would plug something in, and it mostly worked. So, if you were, like, if you wanted to do something a little bit proper, you could technically run a logistic regression and use that initial model you get as your starting point. Just so you're not doing something too crazy. Yeah. Um, so. So, again, I'm just like the naive model where I'm only taking all the observed data into account. Long it all, which I haven't discussed before, it's one of those papers that did a non-parametric regression model, but a parametric propensity model. And what we notice is their method is a little bit more sensitive to your choice of Z. So their estimates go from like 183 to 196, depending on what C you use. Depending on what you use. Whereas for Shao and Wang and R methods, our estimates are really close. They're pretty stable for the most part. In this case, our standards are slightly higher, but I mean, they're all very comparable. And more importantly, ours runs much faster than theirs, right? So it's sort of like a trade-off. All right, so, yeah, sorry. Can you go back to the table, please? Are these the empirical standards or the estimates that? Are these the empirical standard errors or the estimate standard errors? Estimate. Oh, sorry, I'm correct. Okay, I was just gonna say, but yours is so fast, can you bootstrap it and then get back to the right standard errors? But no. Is this from the real data? Yeah. And they're empirical standard errors? Well, I guess sorry, they're bootstrapped. Oh. Yeah, yeah, yeah. They're not empirical. But they're they're estimated, yes. I'm sorry. I wasn't trying to make this up. They're estimated using their esybottics. No. No. Bootstrap. That's the first thing. I was like, I didn't use the asymptotic variants. That's fine. I was just like, but you're so fast. You can get whatever. Do you have an idea why your scene's more efficient than the 2014 ones? Do those make more parametric assumptions? Wouldn't we think that that's better? That's a really good question. Like a good thing to me. Think right. I'm trying to remember whatever I remember about this model. It's actually a very similar model to ours, except they don't have the semi-parametric bit in it. Like even the U is five, I can think of X. Really good question. I'll have to think about that. You're right, that there shouldn't be as worse as they do. Do or their spoon stop, you know? Do you have? I don't know if they have the red band version of something. No, that was Shang Wang. It's about the parametric version. This one. This one has a parametric propensity score, a propensity model. And you trust the parametric propensity. Yeah, so that would, I mean, that's why there are. Yeah, so that would, I mean, that's why their biases are a little off, but standard deviations. And like if a parametric model is misspecified, could that show up? Is it some side that maybe something's wrong? Mostly biased in the bias. Yeah. And we don't know the true values, right? Yeah. Yeah. I might have to do some digging. I'll get back to you. I might have to do some digging. I'll give back to you. Any other questions? Because this is the end of the talk, so if you have questions, it'll be a good time. But yeah, just to summarize, we essentially try to do some estimation under the context of non-ingrollable missing data. We do find ways to do consistent estimation while sort of reducing the computational complexity because we don't have to estimate this non-parametric component. Our estimators do well, they're typically better. Do well. They're typically better, but at the worst case, at least as good as the other estimator. And yeah, hopefully, because it reduces computational time, that means people can actually use it. I guess step one for that would be to build an R model, R package, which I haven't, but that would really help with that. Yeah, thank you. I have a really old version archived if anyone's interested. It's currently submitted paper. But thank you again to Tanya, Yanoa, all of you for being here. Tanya, Yamala, all of you for being here. I'll take any questions I have.