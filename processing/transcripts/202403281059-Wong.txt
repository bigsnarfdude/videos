So thank you very much organizers for giving me the opportunity to visit BAM for the first time. And pleasure to talk about this ongoing work with my student, Madu. And this is a very concrete object, and I hope you will like it. So we're talking about, as I said, a very simple problem, which is transport between Gaussian distributions actually in discrete time only. But the new thing here is that we have the But the new thing here is that we have the adapted constraint or bipartisan constraint. So the overfill is the following. So as we all know, the Wasserstein distance, I only talk about the order 2 Wassenstein distance here, is nowadays very commonly used in distribution only, reverse problems, and other things. And the key thing here is that usually we think of Gaussian distribution as like a geometric. Distribution is like a geometric thing, and the state space is just outn. But now, suppose we think of the distribution as the law of a stochastic process. So x, x1 up to xn are the values at time 1 up to time n. And if we think of x and y are stochastic processes, and let's say we still use the Wasserstein distance, it is not always the best thing to use. And why is that? It is because the Wasserstein distance does not look into the flow of the input. Long look into the flow of the information of the two processes. So, when we take the flow of information into account, these two several versions of transport, but the one we use here is the bipausal or adapted optical transport. So, this will give a generalization of the Watsonstein distance that we call here the adapted Watts-Stein distance. So, this theory has been developed for quite a few years already, and there has been a lot of Already, and there's been a lot of theories, but not many very concrete examples. And so, we consider here the simplest non-trivial case, which is, okay, what is the adaptive oversight distance between Gaussian distributions in discrete type? And so, in this talk, we give the answer and we talk about several properties that may be of interest. So, before we do the adapted case, let's go to the classical case because we modify. We modify the water side distance for the Gaussian theories. So, just to fix the notation, suppose I let μ and μ be Gaussian distributions. So, I let the means be A and B, and the covariance matrices be A and P. So, as we all know now, the Oserstein distance between Gaussian distributions is separated into two parts. First, we have the square difference between the mean, and then we have the burst boss. And then we have the Burstwassenstein distance, which is the distance between the coherence matrix. So, this is the formula that I think we all know. And so, this distance is attained when we use the optimal transport map, which is affine. So, this is this funny matrix, which will transport the covariance matrix from A to B. So, if A, this matrix, and then transpose of A, we will get the matrix. So, this matrix is symmetric and positive. Symmetric and positive definite because we know that in the project transport, the optimal transport map must be a convex free. Okay, so and then in the one-dimensional case, when n is one, so this reduces to a line with positive slope. And so this is simply the monotone. Okay, so so I I will use uh you know this for the like a transport, you know. If this is the W is the Watson stein and this is this is the initial measure, this is the final measure. The initial measure, this is the final measure. Okay, so now let's visualize this a little bit. And we show here two Gaussian distributions of the ellipses. And here on the left-hand side, this is below. And we have the green corresponding to the eigenferral decomposition. So we have the first eigenfactor and the second eigenfactor. And this is mapped under the linear mapping to the right-hand side. So you see something like this. And the red one is mapped to this one, but the blue one is mapped to that one. No one is mapped to that one. Okay, so this is the action of this map. Alright, so the main thing here is that because the matrix is symmetric, and suppose you look into the image Y1. So in general, it is the means, okay? And then you have, you know, from the matrix multiplication, you have the effects of x1 and the effects of x2. So what this means is that when you're going to transfer to y, y1, it depends on both x1, which is the present. Both x1, which is the present, and the future of x2. So, if you think in the attempted case, this does not make sense because we need the future of x to get y. So, in general, unless the matrix is diagonal, this transport, the Wolfsonstein transport, is always not adapted. Because you need the filter of x to get y. So, I'm also using a notation, so t plus 1 to n, it means t plus 1, t plus 2, up to up to n. Okay, so this leads to the Okay, so this leads to the so how to you know avoid it or hook into the how to avoid this we do this by using the adapted optimal transport. So to because of the interest of time I will just give the literature review to the end and here let me just say that the causal constraint in the in the coupling has a long history actually Lutga has a Markov construction back in the 80s and the first The first, or let's say the current formulation that we use here is due to LASO in 2013, and a lot of people have worked on this since then. Okay, so suppose now X and Y are stochastic processes with both mu and nu. And we want to consider coupling pi between two. And what is the causal constraint? Consider the pi, so x and y are now jointly distributed as pi. I want to say. Distributed as pi. I want to say that if you consider an event for y up to time t, so b is now a subset of tau t, and suppose you condition on all the x, okay, then actually I don't need all the x. I only need x up to time t. So this is the causal constraint from mu to nu. This is a symmetric condition. And one easy way or one concrete way to get this is to go to the determination. is to go to the deterministic case where y is a mapping of x. Then the mapping has to be, well, as long as angular in a sense that each yt is a function of x1 of the xt on. So this is the Monge case. This is the Monge case. And if we find that pi satisfies the causal constraint in both directions, then we call this the bicausal output. So this is the constraint. And the thing which is useful here is another equivalent constraint. Useful here is another equivalent condition for pi to be bicausal. So, what this means is that we decompose the joint distribution in the following way. We first go into the time one marginal of x and y, and then we condition on the first time to get the joint distribution, the conditional distribution at time two up to time n, up to time m. And the condition is that each conditional distribution is a coupling of the corresponding margins. Corresponding margins. So, this way it is basically the mark of construction of a glitch here, and this is the bipauso condition, which is the one we can use. And if you use now in the transparent problem, we restrict the complexity by causal, we get the adaptable size transfer. Okay, so now to get let's let's for intuition, let's consider first, you know, possibly some optimal case, which is the Wolfson-block coupling. The Wolfson-Block coupling or the multifarried quantile transformation. So we know that from the earlier theory, we know that this coupling, which can be constructed for any two distributions, is adapted, was assigned to optimal under some modelistic conditions. But here, let me consider the special case for Gaussian distributions. And it turns out that not only here, but also in the general case. In a general case, it is more useful to look into not the coherence matrix but the trousky decomposition of the matrix. So now let me let A to be L L transpose and B is M transpose and L and M are lower triangular matrices with positive diagonal entries. So this is uniquely determined for any coference matrix. So what is the non-Ozenboard component or also for the synchronous component in this case? What it does is the following. What it does is the following. So, suppose we consider X, we can realize the normal distribution in this way. So, first we have the mean, and then we have a normal noise. So, f1 is f1 t is id standard normal. And then you multiply by the velocity matrix, and this will give you the correct coherency matrix. And you do the same for y. You do the same for y. So, y is b plus m epsilon. The key thing here is that we use exactly the same noise for both processes. So, that is what is. For both processes. So that is what is called the synchronous coupling. So when we do this, we call the resulting law the KL coupling between new and new. And you can realize this as a deterministic coupling because you can just express the epsilon in terms of X in terms of epsilon and then you go to Y. So the matrix now is now M times L transpose and as you will see it is a lower triangle. So this is why it is bipartisan. So, this is why it is quite possible. And when you plug in this coupling to get the expectation, you get the so-called log versus block distance between the two distributions. And it becomes like this. So, you still have the same difference for the mean. So, most of the time you can just let A and B be zero for simplicity. And now we have a new distance between matrices. Let's call this the KL distance between A and B. So the difference is that we have this new term. Is that we have this new term, and actually, this is exactly why the reason why the velocity decomposition is useful is because we can realize that this expression is nothing but the provenous norm squared between the two matrices. So, it becomes Euclidean once you look into the Cholasky matrix. Okay, so now here is the picture. So, it is exactly the same two bargain rules, but we port it differently. So, instead of It's differently. So instead of the eigenvalues, I use a standard basis. I use the standard basis on the left-hand side, and now it is mapped to the right-hand side in this way. So what is the interesting thing now? We remember that at both times, the coupling is modeled, right? So at time one, what it does is that you project it down to here, and then you map this to the right-hand side by an increasing linear map. So that is why the red line here is still pointing. Red line here is still pointing to the right. It is because it is monotone at time one, and at time two is also monotone. That is why the blue line is pointing upward. The blue line is pointing upward. So it's kind of something like this in this case. Okay? So you see that just to give you an idea about the numbers. So in the Wasser-Stein coupling, you get around 1.65 for these two distributions. And the Keral coupling gives you a higher number. And we'll see that for this pair, the Kelpa. That for this pair, the chair coupling is not optimal, and the depth of water side distance is a bit lower, so 2.2. So, another remark before we continue is that with two time steps, any distribution is marked. So, this is to say that the Kel coupling is not always optimal, even if the two are marked. Okay, so how to get the optimal coupling? It is to modify this a little bit. Modify this a little bit. So now instead of using the same noise, let's consider a noise for X and a noise for Y. But now we allow that they are correlated. So the noises are still independent at different times, but at the same time, so at each same time, we can allow a correlation coefficient rotate between them. Okay, so this is very natural. And also the same idea was used for coupling solutions to STES, Markov STES. Okay, and you plug in this coupling and you will see that. And you plug in this coupling, and you will see that you get something like this, and then you still get the matrix L transpose M, but now you get the correlations, you get the correlations, and now you get to choose what correlations to use. And so this is the diagonal, and obviously what we want to do is that when the diagonal entry is positive, you want this to be small, right? So you want to choose this to be plus one when the diagonal entry is positive, and you want this to be negative one where the diagonal entry is negative. One where the diagonal entry is negative. So we call this an adaptive Watson-Stein coupling if the correlations are chosen in this way. Okay, and it is actually possible that the diagonal entry can be zero, can be zero. And in this case, it is not unique. Okay, so any questions so far? Okay, so now the claim is that the main result is that this is optimal for the adapter was diagnosed. Okay, so this is the theorem. So, you know, for any two Gaussians, we have this. So, it's optimal under the adaptable satellite coupling or any version of the adaptable satellite coupling. And this is the cost. And so, the difference is that in the Baris Warsaw distance, we modified now the last term. But now for the diagonal, we have the L1 norm to take into account the sign of the diagonal instance. So, this is the main formula that will be the takeaway. And so here are two remarks. So, if the diagonal entries are all non-zero, then the sign is determined by this formula. And so, the optimal coupling, as can be seen from the proof, is unique. It will be the adaptable surst coupling. And otherwise, it is not unique. And I'll give you an example. And we can also see that now, because if M, so because when M So, because when M is the same as L, the diagonal entries are always positive. So, when you change L to M, and when M is very close to L, then you still get positive diagonal entries. And so, this is saying that when M and L, or A and B, are very close to each other, the KL coupling is optimal. So, it's locally optimal, but not globally optimal. So, here's an example for the same pair of distributions. So, this distribution was chosen. So, this distribution was chosen such that the first diagonal entry is negative, and that's why you should choose the first time one coupling to be the anti-model coupling. And this is why now you do this graph, and now the red line, which was pointing to the right here, is now pointing on the left after the image, in the image. In time 2, it is still the monotone component. And you can actually see this. It's always the case for all pairs because for any A, B, the last edge. Any AB, the last entry of the diagonal is just the product of the last diagonals of L and N, which is always positive. So no matter what happens, at time n, you always do the monodo coupling, but depending on the entries, you do either the monotone or anti-monotone coupling in the previous time state. So this means that you choose the last noise to be the same, for example. Okay, so here's an example where you have non-uniqueness. So I choose my L to be this. Choose my L to be this, so 0, 1, 0, 1, 1. So when you multiply, you get this A. And I choose M to be the by flipping the sign of this entry, and B is now like this. And you can show that the diagonal entries for the first one, it is zero. So let's see why, in this case, it is not unique. Or let's say, yeah, let's see why, you know. Okay, so why is that at time one? It doesn't matter. It is because of the following. It is because of the following. At time 2, as I said, we should always use the same notes. So that is why let us represent x2 as x1 plus epsilon 2 because this is 1. So what x1 and then x1 plus x2 in this case. And for y, we have minus y1 plus epsilon 2. Okay, so we are using the same noise at time 2. So now suppose you do the distance between x and y at both times. So this is time 1. At time 2, you now plug in this value. Plug in this value, you plug in this value, you will see that x12 gets canceled out, and that is because we are using the model coupling at time 2. But what happens is that when you simplify, you get x1 minus y1, and then x1 plus y1. So, interestingly, the cross-champ x1, y1 is cancelled out in the sum. And so, it doesn't matter how you couple x1 and x2, and x1 and y1, because it always gets cancelled at time 2 after you use the model of count. Use the model problem. So, this is pretty interesting, at least to me. So, this is the plot. So, it is the another. So, these two now represent these two normal distributions. And the top row gives you the coupling when you use the KL coupling. When the time one is the same sign. And in the other case, you get this. So, either you do this or you just flip an interesting. Or you just flip, and interestingly, both couplings give exactly the same optimal cost. Okay, so let's see quickly why is this optimal? So we use the dynamic programming principle, so in this paper, which is in the last slide, which gives you a way to iteratively solve the bicausal problem. So it's pretty intuitive. So what you do is that at time n, you just let the value function to be The value function to be the cost function between the two paths, x and y are the paths. And then you do the conditional expectation basically by coupling the conditional loss. And you find the one which is minimizing, and you do this for all time steps. And then at time zero, you recover the adaptable style distance as well. Okay, so the problem now becomes a sequence of one-dimensional transfer problems. But in general, this is pretty messy because the science can be very complicated. The signs can be very complicated depending on the paths. But we cannot use it properly without special structures. So, the key way is that for the Gaussian distributions, we do have a lot of special structures in this. So, what are those? So, the first thing is the conditional distribution of the Gaussian. And again, the Cholesky decomposition is what makes this attractive. So the so this is the representation of x when you fix let's say you you decompose the times into time 1 to time t and then t plus 1 to time n. Okay, so t both t is 1 to t, t prime is t plus 1 to n. And this is the representation from the knot. Okay, so this is the Cholasky matrix now in the block form. In the block form is still lower triangle. So you can see now from this easily that So, you can see now from this easily that the conditional law given xt, so equivalently, if I fix epsilon t, it's still normal, it's still normal with this expression. The key facts are that actually, the first is that, you know, they have the mean with this form. The mean, the coefficient is in this form, and you see that these coefficients do not depend on x. Okay? They only depend on t. But x only appears here, so it is linear in x. Linear in index. And more importantly, in the conditional coherence, it is simply, the L matrix is simply the lower right one from the trailer CD deposition. Okay, so this is why I said here. And from this, you can see the following lemma, which is that, so I just think of it as a time consistency result, which is that it will end you a Gaussian, as opposed to do the adaptive process time trotten for the whole process. Okay, and now you fixed the And now you fix the initial path, xt and yt of the time t, and you consider the conditional distribution for the rest. It is the same as if you start with the conditional distributions, and then you do the adaptable setting property, each in the two. So this is a time consistency property. And with this result, we can now plug in to the DPP, and you can show by direct verification that this is true. So so it takes a while to do it, but you know, we can be done. While to do it, but you know, it can be done. Okay, so now let's consider some consequences and also some open questions. So, of course, the first thing we want to ask is what is the geometric structure coming from this adapter-whossess line distance. And I have here the notation. So, this symmetric plus N is a set of positive definite matrices, and this L is low triangular matrices with positive diagonals, so namely the chorus T matrices. So, it's well known. So it's well known that, at least from a paper by Asuka Takatsu, that on the space of coference matrices, the Beres-Waster sine distance is the Riemannian distance. And the jobistics on this space would be the McCain interpolation. So you just do the transfer map and then you linearly interpolate the transfer map. So in terms of the covariance matrix, now if you restrict to the normal distribution, you get if you transfer from A to B, for example, then this is the covariance. Then, this is the coference matrix at time t. t is between 0 and 1. Okay, and now suppose now you do the KL coupling, then you get by doing the same kind of interpolation another geodesic, but now you replace this T by the transport map from A to B. And this will give you the covariance matrix. But because, as we observed before, that for the KL coupling, it is actually the It is actually the Euclidean. Euclidean in the Cholasky matrix. So the Cholasky under the Cholasky decomposition, the geodesic is simply linear interpolation of the Cholasky matrix. So it's just a straight line. And so the symmetric matrix, so under the KL geometry, it is isometric to this phase under the mapping from A to the L matrix. And okay. And okay, the adapter was a strain coupling, you can now replace this by the signs. The P will be the signs if this is well defined. However, this is not always the judicial signal because you can show by some examples that if you do this for two time steps and you look at the sign for s and t, it may not be the same sign for the time series to time one. So it's pretty interesting. Okay, so let's see an example. So I have two examples. So, I have two examples. I have the beginning matrix. So, let's say A, A0, and this is A1. And I want to interpolate from 0 to time 1. The machine, the usual interpolation is in red. So, it's the overlapping, you'll get the usual one that you see. But if you do the adapted one, you get something like the one in blue. So, actually, in this case, at some point, the coupling actually becomes, I mean, the matrix becomes degenerate. So, it becomes rank one at this time. Generate. So it becomes rank one at this time because of the negative sign. So it's pretty interesting. And then if you simulate other examples, you can see that sometimes they are very similar, and sometimes they are not. So in other words, the geometry for the KL coupling is actually very easy. But the geometry of the adaptive Wasserstein is kind of open because of the degeneracy in the L1 distance. However, because the careful However, because the the careful point is locally optimal, so for usual purposes I think that the care proponent is good enough, but I don't have a good proof yet. Okay, so let's see another graph. So this would be the uncertainty set. So let's see what I want to do. So I start with some matrix A, A naught, and what I do is that I have a judicial coming from A, okay, and now at some point it will hit the At some point, it will hit the Wasserstein ball. I'm using the two Watson stein balls, one for the adapter-wasser stein, one for the Beris-Wasterstein, and I'm using the same epsilon for both. And so the adaptor was steinball is smaller because the Beris-Waster sign distance is smaller. So if we hit, let's say, something like this. So let's say that it, so this is for the adaptive Woosset sign, and this one is for the Voissance time. So at some point, So at some point they hit, you know, here, right? And then I will plot the ellipse when it hits. So this is why for each, I'm using the same directions for both graphs. So this is why, let's say for this ellipse, you see that this is now more extreme because this point is farther away from the origin. So the point of this property is the following. So you see that I have now 10 ellipses, I think. And you see that for certain And you see that for certain directions, they are very close to each other, like here. So, for example, you know, maybe for this one, this, this black, dark, gray one, the ellipsis kind of similar, right? For the similar locations. So, it means that this direction is more or less the same for the two. But there are also other directions, like maybe this one, where the intersections are more farther away from each other. So, maybe in this graph, maybe this ellipse, you can go a bit further. You can go a bit further to get the bound for the voices. So, at this point, of course, we can see from the, I mean, everything is computable from the formulas, but I don't have a good intuition about why this is the case and what is the implications in the net ADL. Okay, so now let's just conclude. We studied the adapted OT in discrete time between Gaussian processes, and we don't need any conditions. As long as the two matrixes are non-degenerate, we are fine. And the optimal And the optimal coupling is the adaptable system coupling, and it gives this adapted burst or sustaining distance. Okay, so it seems to be interesting and useful, and there are some interesting questions that are not solved yet, the geometric ones. And there are some ongoing and future problems that will be of interest. So for example, let's grab realize my entropy and consider this uh problem instead. And I strongly believe that the coupling must be Gaussian, although I don't know how to prove this yet. Although I don't know how to book this yet. And similarly, for the barycenter. And with so, so an advantage of, let's say, Gaussian distributions is that we can compare with them. So that is why maybe with these formulas, we can start to do some calculations and applications. Let's say, suppose we do some kind of filtering, you know, LQ type control, then everything is still Gaussian, and maybe we can do things properly. And there are also variable questions like, okay, if you have continuous time Gaussian processes, you know, not necessarily. Time there are some processes, you know, not necessarily similar to games, then how do we come with them often? So, I think there are some interesting questions. So, thank you very much. But the references, I don't have time. But anyway, yeah, let's go to the questions. Yeah, the uh the problem between time lines the the two business is a time line. noticed the time is problem and therefore with this construction of another construction or attractive one can reduce to the distance of the conditional distribution and then it already so one could have also a simple other possible candidates for doing analysis for example it was difficult one has also access to uh this conditional isolation This conditionally suggests make these other things. Yeah, that is for sure, yes, yes. But in the DPP here, we really use the Gaussian properties. And maybe I didn't mention it clearly here, but basically what happens is that in the pair function, you always reduce to one-dimensional W2 problem between two one-dimensionals. And you really need these coefficients such that it will correspond to the coefficients of the Coefficients of the diagonal entries. So maybe with elliptical, I need to see the formulas to know if this is possible. So you were mentioning the paper where SDEs are compared, right? And I think they actually use a similar strategy in the group, right? Where they just discretize the time and then for each time step they do an analysis like this. So from your results, Analysis like this. So, from your results, have you tried to actually extend? So, I think they just look at SDEs in one dimension, right? So, you would naturally get something for D-dimensional version where, as you say, the correlation may be based on also not always the synchronous coupling of the law. Again, it's a good question. I haven't thought about this because this is still ongoing. And another interesting thing. And another interesting thing is to consider Gaussian processes, let's say still in discrete time, but factor further. So, this is more complicated than this one. I was thinking about the elliptical phase. So, it seems that from the earlier making, like Moogress said, you just need to condition the pass. And I didn't get a point where you're saying that you're essentially using Gaussian. Somewhere that actually couldn't see. Because when you simplify the... Okay, when you let's say you because what you do is to do the induction. You plug this formula into this formula and you do this. And so I really need that I can, it becomes like a linear quadratic function and then I can do things. But I haven't tried the useful case, but maybe it is. But I haven't tried the ethical case, but maybe it is possible. I am not exactly sure. My impression is that it will give you the same. But if it's in the same class, mu and mu have to be in the same class, like T3, something like that. If they're in a different class, of course, I think it's class. We need more than the same class, because for example, we need that these things that do not depend on the path, for example. So I don't remember exactly the formula for the difficult, but we can think about that. 