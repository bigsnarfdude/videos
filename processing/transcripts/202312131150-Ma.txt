And today I will talk about one of our recent work in the sub-seaming in large networks. Okay, so let me first present the motivating example. Just before pandemic, actually, I came to not Hangzhou, but Huzhou is a very close city here to attend a small symposium. And one of the researchers from Ant. the researcher from and research financial group from alibaba present uh their this uh alibaba this uh transaction network okay this network is really huge essentially they have about billions of nodes indicate uh which represent the identity that can have a transaction and then uh they have more than hundred billions of transactions occurred if there is a Transaction occurred. If there is a two identity transaction occurred, there is an edge being drawn between these two nodes. And then the thickness of this edge indicating how many transactions have been ever occurred, recorded. Okay, so it's really impressive to have such a large network. Okay, but this network, I talked to this researcher obviously because of. Researcher, obviously, because of a commercial secret, we couldn't guide this network. But very recently, I work with one of my colleagues in the biochemistry department in UGA. We are working on a so-called curated proton-proton interaction network. Essentially, my colleague compiled this large data set about a proton. So, each node in this network is a proton, and if there Proton, and if there is a being verified interaction in the literature, and he draws a line. And then our purpose of analyzing this large network is try to see whether there is any potential interactions not being recorded by the experiment, how we can predict it out using certain RVC. Data out using certain algorithms. However, the immediate challenge we're facing is that we are even very hard to visualize this network. Here, I just present a very, very small, tiny network we extract from this large curated network. In this curated network, actually, we have about half a million nodes which represent the represent the proteins and a lot of also a lot of curated protein protein interactions. Okay, so this is the motivation for our analysis. For large network, the first thing is sometimes it's even very hard to visualize this large network, large network. And then second thing, if you can hardly visualize If you can hardly realize, it's even difficult to analyze. And then, third, is right now the network most of the time is not using by itself, but using as an input for, for example, for the GN graphical neural network. And then if you input a huge network and then the calculation propagated during these deep learning layers, even become more challenging, make the computation even more challenging. Make the computation even more challenging. Okay, so what we want to do is from this LATA network, our aim is to extract a sub-network. Hopefully, this sub-network can represent a certain essential characteristic of this last network. Okay. And we call the sub-sampling is essentially all kind of method we can get. We can get, okay, for extract a small network from this last network. Okay, we call it sub-assembly. It could be fixed step procedure, it could be random procedure, and so on and so forth. Okay. And here I want to talk about three settings. Okay. The first setting is relatively easy. This network is large, but we still can load into our computer memory. We can do all kinds of computing. can do all kinds of computation the only thing is that taking the perform all kinds of computation is uh time consuming but you still can do it okay uh this is a is a is it's quite like our protein protein interaction network we can still load into our computer memory okay we can still do it but it takes time to do it but the second is uh setting is more challenging is this huge network is so Is this huge network so huge you couldn't even load into your computer memory? You can only load in piece by piece, okay? A small block by block. Okay, this is an even more extreme challenging setting, okay? And third setting, obviously, is something between. For example, maybe you can load in some of these things. You can you know the overall network, how it looks like. For example, we know. Look like. For example, we know there are 10 billions of nodes and we know some simple statistics, but doing the calculation is still challenging. So, in this talk, I will focus on the second setting. This huge network couldn't even load in into our computer memory. And then, how do we analyze this network? Okay, so hopefully, the setting is very, very clear. Okay. Okay, so then we want to our sub-assembly have certain properties. Okay, the first property we like is we call it local to global. Essentially, you sub-sample this large network. So you must computing certain importance index. How guide you to select this small network, right? This sub-network. So this small index either knows Index, either nodes or edge, okay, they are local features, but contains the global information, even though you can compute locally, but hopefully it has carried out some of the global information of the whole big network. Okay. Second of all, and then since this huge network couldn't even load into our computer memory, so what we want to do is you do is you don't you don't need to compute the important important index of all the edges of all the nodes of this huge network before you do anything right so these are the two desired properties okay so uh in the literature how we analyze uh a large network okay typically uh we doing this so-called uh matrix reproduction Matrix representation, right? We're starting with the Zation C matrix and then maybe calculate the degree matrix and eventually using Laplacian matrix and then we're doing all kind of numerical linear algebra, calculate the eigenvalue, blah, blah, blah, something. Okay, this is a lot of research being done in this area. Okay, so the second way is in the last 20 years or so, okay, there are some approaches. Okay, there are some approaches. Essentially, I want to make this discrete matrix balance of this benefit of this discrete matrix into a smooth function. Essentially, even though you have this network, okay, and then you have something like this adjacency matrix. These adjacency matrix, and then you postulate a convergence process, and then eventually you end up with the smooth function. Okay. And then this smooth function, if this matrix is a dense matrix, the JSON C matrix is a dense matrix, this is a so-called graphon. If this is a sparse matrix, then the limit is called a graphics. Anyway, so then you can analyze this graph on graphics. And the good thing is. And the good thing is that now you can have calculus. You can utilize a lot of powerful calculus tools to do a lot of analysis. But instead, in this work, what we are going to do is we want to embed this network into some manifold and then analyze this from the perspective of manifold. Okay. So. So, the thing is why we want to use manifold because manifold has a good, nice property, is that there is no predetermined coordinate. So, it gave us a lot of flexibility to choose these coordinates. And second of all, we need to ensure any object we define globally on the manifold do not depend on a particular coordinate. The first is a chance, the second is a challenge. Is a challenge. Okay. We need to ensure both of them to do. Okay. So now let's look at the very elementary geometry. Okay. Whether we can find some local features that help us to have some global information and so on and so forth. For example, we know for the plane circle that the radius is a local feature, but this helps us to distinguish all kinds of plane. Of plane circle. If this radius is the same, then the two circles are identical, right? And then also this local information about this radius also carry out the global information because the circumference is a 2Ï€ times this radius. So essentially, this elementary geometry tells us, yes. Tell us, yes, there are something local features you can use. Okay. And then, because once again, in the manifold, we don't want to depend on a particular coordinate. Okay. One of the key index being used in, especially in Riemann geometry or differential geometry, is this so-called curvature, especially for these plane curve. Okay, so if this plane curve So, if this plane curve can be represented as a function of a certain covariance, for example, t, and then the second derivative is typically defined as a curvature. Okay. And depending on whether you want to have a positive or negative, you can take absolute value and so on and so forth. Okay. So this curvature is a very useful thing to be used. Okay. And then curvature also have these two. Curvature also have these two properties: that is, two curves are identical if their curvature are the same at every point. Okay. And also for a simple closed curve, the integration of this curvature equals 2 pi. Okay, so the curvature does, even though it's a local characteristic, but carries global information of geometry manifold. Okay, now the tricky thing is how do Now, the tricky thing is how do we define the curvature in high dimension? Okay, so let's take a look at these three figures. Okay, for example, this is a pyramid. Okay, this is a very flight object. And then if we look at a triangle, this is, we learn it in the elementary geometry. But if it's a sphere, we still have this triangle. Okay, we find something interesting is that. Find something interesting: is that these two sides being turned inside itself, okay? And then the third side, if we measure the third side, this third side, the lens actually is smaller than the third side if it's in a plane figure. Okay. And then in this figure, obviously, we see the third side is larger than this thing. And then using this integration, we can define the curvature in high. The curvature in high dimension. In the Riemann geometry, they defined something so-called a section curvature. The basic idea is suppose there is a point P, and then there are two tangent vector, okay, each of them with a length epsilon. We call these tangent vector as x and y, but we draw a line along essentially it's a Essentially, it's a geodesic, okay, along this object, the surface of this object. We want to measure the length of the third side. We call it L of epsilon. And then using Taylor expansion, we can show that this L of epsilon, the third side length equals epsilon times this. This essentially is if it's in a plane curve, it's essentially in Euclidean space. And then this is the length of the side, okay. The length of the side. Okay. However, if it's an arbitrary manifold, it equals one minus something. This is, there is a, there is this thing. This thing is so-called a section curvature. Okay. And so if this section curvature is positive, and then you get the third side is smaller than its counterpart in Euclidean space. Otherwise, you got if it's a this guy is negative, then you got. This guy is negative, then you got the third size is longer than the third size, okay, of this counterpart in the Euclidean space. Anyway, this thing is so-called a section curvature. And then we because this section curvature depends on two sides, okay? And then one way to generalize this is we can do this so-called RISC curvature. Essentially, this RISC curvature is. Essentially, this recursion is integrated out the other side. We consider all the possible side of Y, all the possible. Now we focus on X and then we integrate out this thing and eventually we can define this so-called recurvature. And then RC curvature is about the same side, but if you want to define the different sides, still want to have different side, you can use this identity to define the different side. The different side, okay. What is this Risk curvature thing? Okay, uh, essentially, the Risk curvature measure the degree to which the geometry determined by a given Riemann metric might be different from that of the Euclidean space. Euclidean space you can think of as a very flat, very smooth space. And then all the other manifold is rougher or either rougher or smoother than that. Okay, so this. Okay, so this essentially is recyclation measure that. And also another thing you can think of this recyclature as a Laplacian counterpart of this. Remember, in the matrix thing, you always calculate the matrix Laplacian for the adjacency matrix, right? Okay, but this is essentially the counterpart in the manifold. Okay, so we will focus on this. So, we will focus on this. Okay. Now, the thing is, we embed this our network to a manifold, and then eventually we want to calculate this thing for this network, for discretized network. Okay, so we need another approximation. Okay, so let's first look at this. Okay, this is a so-called olivia-reasy curator. Essentially, it's a rougher. Essentially, it's a rougher approximation of original recurator and proposed by this UK mathematician Olivier in 2006. So the basic idea is very simple. Suppose we have a circle, and then there are some uniform density putting on this. We want to move this circle from one place to another. From one place to another, okay. In this place, the center is we record as X. In another place, the coordinator we suppose it's Y. Okay. So how much work you do to move one plate to another? So obviously, this the total work you need to move this one is you're using the optimal transport. You can calculate that this is a watches and distance, right? Okay. But anyway, so eventually. And anyway, so eventually you can calculate if you move this one circle from one place to another, and then essentially the Taylor expansion looks like this. Okay, this Delta is essentially the Euclidean distance between these two centers. And then it equals something very rougher. This is a REC curvature. But there are some also some massive constant before it. Constant before it. Okay. So we don't care about this thing. Okay. Let's just ignore the higher order smaller terms. We only focus on this. And let this thing equals this what this is the total work, okay, equals water stand distance. And then we call this thing, okay, as olivia receiver curvature, essentially. Okay, so here, So here gives the so-called olivia recyclation. Olivia recyclature, after you absolve this equality, you got this one minus, this is the distance, this is the Euclidean distance, this is the Wasserson distance. And then now you can put it back into a discrete of this network, and you can calculate these alleviaries curvature for each. This olivia rigid curvature for each of these edges. Okay. And then this, when you calculate this, it only depends on the local two nodes. Okay. It does not depend on the overall network. Okay. This is a nice thing about this. Okay. Now let's take a look how these things work in a small network. Okay. We just have a small network. Okay. Small network with essentially two clusters or two communities. We calculate the olivia received curvature for each of these edges. And then what we find is within a network, these olivial receipt curvature tend to be positive. And then between the network, these olivial research curvature tend to be negative. Okay, this is a good observation. And using this observation, now we can. Now we can do. Oh, and then how do you calculate the vast distance? I will skip this part. Okay, it's not much. Okay, this is another, we generate another one. So we have two major community and we have some little bit of scattered notes. Okay. And then once again, we can see within the Once again, we can see within the community, the olivary risky curvature tends to be positive. And then between the community, these olivary curvature tend to be negative. And then we have our algorithm, very simple. So we randomly start from certain edge in this large network. And then we explore this whole network by looking which Looking which edge gives me the largest difference for their olivary security. And then we move on to the next edge. And eventually, hopefully, we draw a sub-network that have all these edges. Okay. Have these edges. And then we stop somewhere. So these essentially our algorithm is very simple. Okay. Very exploratory. Explorer 3. Okay. But turns out it works. Okay. So we try some real network example. Okay. How we try it? We have this network. We want to draw a sub-network. And our goal is once we draw this sub-network, and then we apply this community detection algorithm to it to see whether it still can detect the same number of community as. Of community as this original large network. Okay, so this is our goal. So we tried three data sets. The first data set is relatively simple. It's so-called political books. Essentially, there are hundreds of books saled in Amazon. It's a political books. If the same customer buying these two books, which all align, okay, on these on these two. on these on these two uh two books and then these roughly have 100 books and have 150 edges okay no and then if we apply this community detection method okay the total time is uh is about one second okay very simple and then using our algorithm uh with first the subsample and the subsample we use a proportion okay we only subsample Proportion, okay. We only subsample 10 percentage of the edges in the regional network, okay? And then we apply these our community, the community detection method on this sub network. We find, okay, actually, we repeated several times. We find, okay, we gather this exactly as we detect the oldest communities, okay? And then obviously, the time being substantially reduced. Being substantially reduced. And then we also compare with some other algorithms. For example, there are snowball algorithms that can also help us to identify this sub-network. Okay. And so on and so forth. But turns out this works very well. And also we try the other two data set. This is a political block. This has a hundred thousand of political blocks and then Blocks and then there are about 15,000 of edges, something like that. And then in this part, we extract 5% of edges from the regional network. And then once again, we applied our algorithm and we identify all these communities. And the last one is a little bit larger. It's with a quarter million of these. Of these parameter literature, each of nodes represents a publication. And then the edges, there are about more than millions of edges in this PubMed network. And once again, we only extract 2% of this original network. And then we apply this community detection algorithm once again. Algorithm once again is very accurate. Okay. So this is pretty much the result. Okay. So once again, this is a work already published in ICLR in 2023 and it's worked with several of my students and Hui Min is my former students already now is assistant professor at the BU and with Ven Shen and is supporting Vision and is supported by NISF and NIH grants. Thank you.