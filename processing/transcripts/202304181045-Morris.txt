Welcome back. Our next speaker is Kirsten, Kirsten Morris from Waterloo. She talks about spectral asymptotics of operator density. I think it's originated by your work on root locals, yet. Yeah, we are excited. So as Timo just said, this is part of my work with Birget on On my work with Birgit on root locus, which is an engineering term. So I actually spent some time translating this talk for a mathematical audience. And this was actually the title. We did a research in PARS here in BAMS, I think eight years ago now. This was the title. And some of the results I'm going to talk about now we got while we were spending this week there. So it's kind of appropriate to talk about it here. Here. Here we are looking happy because we spent a very nice week in Banff doing mathematics and doing some hikes. But it does not tell the photographs, does not tell the whole story because at the end we did publish a paper, but there were a lot of problems that we were unable to solve. And I think the people in this room maybe have the, I think if anyone does, these are the people that could solve some of these open problems that we have. Solve some of these open problems that we have. So, I'm going to highlight that in this talk: what we did, but also what we weren't able to do. So, just starting with a standard description of a system, I have an operator on a Hilbert space, and a big assumption is that not only is the spectrum non-empty, but it's only isolated eigenvalues, each with finite multiplicity. I'm not assuming that I'm not. That I'm not assuming a gap condition here, right? But we are excluding things like where the eigenvalues cluster, which does happen only beam with television damping and some kind of delay equations. But it does include a lot of operators that arrive in applications. And there's an important assumption for infinite-dimensional dynamical systems, which is the spectrum you can relate with sound check. Is that the spectrum, or in this case? Is that the spectrum, or in this case the eigenvalues because there's nothing else in the spectrum? Determine the growth or decay of the semi-group. This is automatic for finite dimensions, but although it often holds an infinite, not always. This is important because without that, the spectrum doesn't really tell you what's happening with the solution. So, what I'm considering here is I have some control on the system. So, that's blue. So, U of T is a control variable. So, U of T is a control variable. We have some determination over it. And then I have another function, Y, which in applications would be typically a measurement. So we made a big set of assumptions here, which is that those control and observation operators, B and C, are bounded. And not only that, the input and the output spaces are scalar. So I have a single input, single output system. So then, of course. system so then of course that means that e is in fact which will disappear soon but e is actually a number right okay um so actually because of you know well because of that then we can this will be helpful sometimes identify the operator B with an element lowercase B of the Hilbert space and the operator C is also identified with an element with C. My idea here is what you're doing so you Is what you're doing for your control variable is some constant times the output, which happens. This is motivated by applications. This is a common engineering thing to do. The simplest control in a system is just to feed in a constant multiple of a measurement, right? And then you want to see what happens to my dynamics with this simple control. In fact, sometimes for some systems, it's enough just to put what's called a constant gain on it. Called a constant gain on it, right? Other times, not, and then you have to think about a more complicated controller. So, with this very simple kind of control, which not people can think of perturbing the system, I'm left with, right, we now have a system with these dynamics, right? So, I've changed the dynamics of the system. So this leads to what's called a boot logist. Very common concept of engineering. It's taught in like third year, I think, in pretty well every electrical and mechanical engineering program. So the idea is you're going to consider the iodine values. And because, actually, because we're only considering compact B and C here, the fact that we assumed A. The fact that we assumed A to have no essential spectrum is kind of beside the point because you can't move essential spectrum with such a perturbation. So if it did exist, it wouldn't go anywhere. So because these are bound in finite rank, the perturbed spectrum is also only point spectrum. And we can consider then a family of curves in a complex plane, which show you what happens to the spectrum as I increase the control. is I increase the control, the feedback K from zero up to, say, infinity. And this family of curves is called a double. So the big question is, and the tricky question, is what happens to the spectrum of AK as K is increased from zero all the way up to infinity? And the question at infinity is particularly interesting. Part of answering Part of answering this is zeros of the system are really important for studying these perturbed spectrum for the root locus. And I think there's a few people here, I think, who know what zeros are, but most people don't. The spectrum and the I values for dynamics are key, right? But if you're doing control, zeros have the same importance. If you have, if a system has stable zeros, if the zeros of a system has a Stable zeros. If the zeros of a system are in the left-hand plane, it makes it much easier to control. You can do sort of almost anything with regulation. Here's a formal definition using this state-space form. So the invariance here are, you see it's a solution to a kind of a generalized eigenvalue problem. So when it has a non-trivial solution, that's called the zero of the system. They're called invariant errors because they're invariant under feedback. Because they're invariant under feedback. If I put feedback on the system, the system doesn't change. Actually, an example of sort of importance of like stable versus unstable zeros is if I tried to, I can't balance this one because they're really pointing. But it's much easier to balance something looking at the tip and looking at the bottom. And I've noticed this because I watch people learning to skate on campus sometimes. And there's a tendency when you're learning to skate to look at your feet. Skate to look at your feet. And that actually is a much more difficult control problem. It makes the zeros, you can make model yourself into play. It makes the zeros go into the right half plane and it's very difficult to balance. It's actually much easier to balance if you look straight ahead. That moves the zeros of your system into being stable and it's much easier to stay up. So there's a sort of a practical example of the importance of variance. So we're going to say it's minimal if the spectrum, intersects the spectrum and the zeros is FB, which is fairly common. It's not hard to show. This was done sometime ago. If the system's minimal, then the set of invariance arrows is also countable. Variance errors is also countable and having a finite accumulation point. So it's a consequence of the assumptions we made. So the first point is that even for infinite dimensional systems, the good locus, which you remember, is the eigenvalues of this as I move k turns out to be, we're going to assume here, the mirrority, which makes things simpler. Each n, right? So each n corresponds to one of the original eigenvalues. Each of them is a continuous non-intersecting curve. So none of these curves intersect with themselves. And the limit of each curve, actually intersect, like one of them could intersect with another one. That's quite common. But at the points of intersection, the multiplicity of the eigenvalues is maintained. You don't lose anything. Anticipating if you don't lose it. But what happens in the limit? In the limit, each one of those curves converges to either a zero or it goes off to infinity. But where infinity does it go? So in particular, you want to think about, like, ideally, you'd want it to go off to infinity, negative real infinity. That would be good. Positive real infinity would be bad. You're going to actually, as you press. In a state, as you crank thinking of a control system, that would give you as I increase the gain, the system goes unstable. And it's just really, I think, an interesting question. Where do these other loci go? So, yes, the remochus yields information about the spectrum of the perturbed system, right? So, K can be a control. Sometimes also, it's a parameter, right? So, for what range is a parameter. So, for what ranges of parameters do I keep stability and the spectrum determined growth function, which is satisfied for a lot of systems. I'm going to get rid of the case. So, D being not zero is rare in practice and it's also not mathematically interesting. This was done a long time ago, and it's in fact. time ago and it's in fact in the original this results in the original edition of Kirtman's work which is a textbook it's a minimal system with E not being zero with both things not as a little fine each branch moves a whole to its zero so there are no asymptotes in this situation so you know exactly what's going on so once you know what the zeros are you know where all those blue blood hikes are The root locus as k goes from infinity is the same as, you can see k, this thing k 1 plus d k inverse is another number, so I can replace it by k tilde. So it's the same as k tilde going from 0 to d inverse. And it's also pretty easy to show that the zeros in this situation are given by the spectrum of this operator. So then each branch is going to move into it. That's pretty straightforward. So what's interesting is a situation where it's not where D is zero, which is the common situation. So I'm just going to go over quickly finite-dimensional systems, which is well known. It's a topic in the undergraduate pediatric. Finite minimal system that I treat. minimal system matrices when I define these operators which gives us sort of the character which gives us an expression basically for the characteristic so because it's minimal these are co-prime and then the perturbed system is going to be at the roots of this polynomial this is a matrix so this is a polynomial this is also a polynomial This is also. The degree of D is going to be equal to the degree of N plus some number R. This is called the relative degree. So D is 0, R is positive, all the 0. The eigenvalues move from, again, the eigenvalues A to the invariant zeros, but then there's going to be R is introduced. This analysis is much more straightforward because N and D here are polynomials, so we can kind of do a fairly simple. So we can kind of do a fairly simple analysis with some basic complex variable stuff. See that I have our asymptotes, and actually, because there's a minus one over here, the angle of the asymptotes, so it's like I don't know if you're in baby, because I think it's like minus one, minus something. We'll show you a couple of things. So here's a problem. This had been non-zero. It's written out in order D here. Written out N over D here. And you can see what happens here. The X's are the eigenvalues of the original system, and the zeros are the older zeros. So this eigenvalue goes from here down to the real axis, and then out, this eigenvalue goes up to the real axis. You see, they meet here, right? So you'd have a double eigenvalue at that point. Another system. Okay, so now that's And out. That's got two eigenvalues. I don't know, we've got one zero, so it's relative to only one. I'm going to have one as you totally, it's going to have an angle edge and 80, so it's going to go off along the real axis. So again, one goes from negative value up here up to negative infinity, and then this one goes down. This one goes into zero. So actually, although this one is stable, this is going to actually minus one, right? So you can see that actually as it cranked K up, It cranked K up, I'd be sort of increasing the settling time up to a point, but then past that point, I'm decreasing the decay rate of the control system because now that what's going in there's no zeros, I've got that's two binding values, no zeros, I've got two asymptotes, so you get 180, so each one goes out at 90. So one goes up there, and then one goes up there. So, that is finite-dimensional systems. The immediate question then is: seeing as we know exactly what to do with a finite-dimensional system, why not just work with approximations? Apart from all the complexities, which Matt talked about with calculating Heigen values for the conventional systems, the zeros are even worse. So, even if I had a problem like the need equation, like with the equation, replay comp this actually isn't bounding part. Even when you get a good job on the eigenvalues, the zeros are often really bad. So this is a problem. I know what the zeros are. They're all real, like the eigenvalues are all real and negative, and the zeros are actually all real and negative. And I did a nice model approximation and I hit these really bad zeros. That's the whole question of relative degree for infinite dimensional systems is quite different than for finite dimension. It's mostly kind of pepper. So is the zero approximation harder than the pole approximation then? I don't know if it's harder, but it's not. I think people just, I wonder if people just haven't really studied it. Yeah. Because it's a generalized eigenvalue problem. Because it's a generalized eigenvalue problem, right? So I suspect that people like people like you and Matt would do a decent job on it. But if you think it's sort of a simple-minded way of doing it, then they're wrong, which is what I did here. Because what I did here, this is what I did the obvious way here, which is that I approximated my PDE, the heat equation, using the Fourier truncated Fourier series. Truncate into Fourier series, and then I get a finite dimensional system, and then I calculate the zeros, right? That is clearly not the best way to calculate the zeros. Actually, that's a question there, right? So, in a sense, you want to kind of go back and do it more directly. Actually, any other questions? This is a returning delay system. So this is, we have some sort of specific results for different systems. This is sort of some as in type results on it. I'm not going to scroll at. Here's a simple, so-called simple, a delay equation. And in this problem here, I have D is actually equal to the adjoint of C. So it's what we call complicated, which also means. Which also means in this case we can say the relative differences. So there's the eigenvalues typical for infinite dimensional systems. I have the eigenvalues going on to infinity and then the eigenvalues. This is like a simple system. You can have a calculated document where I want. Then the root locus is the roots of this equation. They have a similar pattern. It stays in the left half plane as long as K gets bigger than A. This is an example where you could use it, right? I could put K on the system and use the feedback to stay away from the system. Anyway, they all go into the left-hand plane. But this problem has no zeros, so there's no invariant zeros, okay? And all of them go off to negative infinity. Kind of weird because relative to greed one, you think. Because relative to grade one, you'd think if this was finite-dimensional, I'd have one less zero than the whole. Example, things are different in infinite dimensions. So all the i's and toes go off to negative infinity and they go off to the real part goes off to negative infinity. So they're going off somewhere in the left-hand plane. Another problem. Actually, that should be a point there. Simple, relatively simple equation. A heat equation on one interval. And again, I have obligations, so I have B is equal to the edge weight of C and relative to V1. It's minimal. It turns out that all the poles and zeros are real. Well, you know, sorry, I can say. Well, you know, sorry, I can say eigenvalues to change the language. So all the eigenvalues and zeros are real and intranace. In this case, each branch of the rib locus moves to the pole from zero. So there are no asymptotes. So this was actually kind of observed by people working in this DL that when you work in a PDE and you calculate the numbers, it seemed to always It seemed always that the zeros of a self-adjoint, when the generator was self-adjoint, like a diffusion problem, the zeros would interlace with the eigenvalues. So this was one thing we did manage to propose, is that if we have a self-adjoint operator on an infinite dimensional space, which is collocated, then each branch, the real locus is entirely on the real line, and each branch moves from an eigenvalue to zero. Zero, and we have what's called total zero eigenvalues. Toll would be like the engineering group. Zero is interrelated with the eigenvalues. This was kind of nice. So we proved something that everybody believed. What's very interesting, right, is the system is relative degree one. So if this was finite-dimensional, you'd have one asthmato. But because I'm in, one of the consequences maybe you get the dimensions is no asthma. In this case, it's no asthma. In this case, it's no as a job. It's the previous one that an infinite number is because Malik Zelig got actually around the same time. They looked at a very specific type of problem. He said, I did more, sort of, more general operator theory approach. They looked at A being a diffusion operator, like the heat equation, but on a Like the heat equation, but on an arbitrary region. Boundary control, and then again, complicated observation. And they got this, so now they've got, and this is with, so now the feedback is unbounded, okay, because it's boundary control, but they've got the same behavior that the root loci and the pole and the eigenvalues and the zeros. The eigenvalues and the zeros interlace and the root locus moves. So there's kind of some hint there that there's maybe a more Hint there that there's maybe a more general result. Their proof here was quite different. The proof here is actually really using the differential equation and the series expansion and so on. So they got kind of a tight result on where everything was. Yeah, there's generalizing. I think there's some idea of generalizing this to more general B and C. Okay. So another class. So another class again, skew it, right? So self-adjoints, and then you say what happens if it's skewed joint, right? So an undamped wave equation. So we've got an undamped wave equation. So there's my B, and then C again is the adjoint of the disjoint. This one is, again, this is simple enough that you can calculate exactly the eigenvalues and also the zeros. The eigenvalues and also the zeros, and this little picture of them. What's happening here, right? So, the eigenvalues are imaginary, we know that. The zeros are also imaginary, and they interlace. So, again, we have this interlacing. This is another thing people have observed. Again, undamped kind of wave-type problems, the eigenvalue is zero every time anyone down. And what happens is, in this case, most of the time. In this case, most of them move from the IV value to the zero, and then you've got one pair that goes down with that, and then you've got one. Okay, so now I've got an asymptote. I've got one asymptote going. So, great, you can see, okay, so all three of these examples have collocated B and C. The behavior at infinity is actually no asymptotes, one, or an infinite number of asymptotes. Can end up with as well. Another thing that's interesting here is it looks like when you look at that, that's a zero, and there's quite a decent distance there between the zero and the eigenvalues. The eigenvalues are even, right? We know that. The eigenvalues are evenly spaced. The zeros seem to be getting quite close to the eigenvalues, which is in fact more generally true. So, yeah, this is generally true. So, what do we show for skew adjoint? The zeros and the eigenvalues are imaginary and we get the interlacing property, so we're able to prove interlacing for general skew adjoint allocated systems. And we know that it stays in the left half plane, okay, but in general, we didn't get. But in general, we didn't get much more than that. We assume a very effective finance QA joint system. This one is second order system. Assume it's a tiny model. And then we do get a little bit more. The entire real axis is in general part of the root locus for this point of 15 foot construction. See, the entire real axis here is in the blue locus, so that in general is So that in general is that in general true for these kind of ways and the second group you get is thought and stable. Which makes sense, that's actually what we saw in that picture again. Yeah, you can see the little circles here, right? So these are the little. Here, right? So these are the loci. So you can see the real part on them is getting closer. It's always in the left half lane, but it's getting closer and closer to the real axis. So the fact that you only have stronger asymptotic ability is kind of shown in that picture. But yeah, so this would be a general truth, for instance, a library plate. Okay, but what we don't have is what happens to the asymptotes in fact. I guess it's the conjecture. Kind of what we believed was what I showed you in the first picture was generally true. That except for one, there's only one for a skewer joint system. You get only one, you get exactly one asymptote, somehow the other branches converge to a zero, but try as we. It's converged to a zero, but try as we might, we could not get that. Great, someone to finish this up. I think these are like really good sort of operator pencil type questions. And we started looking at pseudospectrum, and we got some interesting results from looking at pseudo-spectrum in these problems. So epsilon pseudospectrum is The epsilon pseudo spectrum is numbers where the difference between S and Az is less than epsilon for making Z and not finite or something. So in finite dimensions, these are all little walls, right? But in infinite dimensions, they're only going to be little walls, or you only know that they're little. Okay, it's not the respectable results. This is an old result which I made for the next slide. So I showed you a little while ago when I had a non-zero D, you can write the zeros in terms of a different operator. It's zero the invariant zeros of the system are the eigenvalues of A minus B. The eigenvalues of A minus B, B inverse C. If I have a system, it doesn't have to be complicated, but I can say this is, I can call this relatively one. You can write the zeros in terms, again, in terms of an operator. So this is the operator K. It's in general not bounded. Everything's this new one A has to be KZ. This gives you. And this gives you the eigenvalues of that operator and the kernel of C gives you the invariant zeros. Also, we have an expression for the corresponding eigenvalues. Eigen functions. Okay. And this one. So we have a little list of technical functions here. I get them all again. We only took the big ones. We need that the eigenfunctions of We need that the eigenfunctions of A infinity form a Reese system. So even if A is a Reese, you know, even if the eigenfunctions of A form a Reese basis, there's no guarantee that that happens for this new operator that I got from an unbounded perturbation. Okay, so let's assume they do. And I have a nice C that's in the divine A, right? I'm going to write the invariant zeros. And we'll s to one da da da da, indicating our corresponding item functions. Corresponding eigenfunctions, right? So let's go with the zeros. And for any epsilon greater than zero, there's an n, so all n bigger than capital N, zero, right? So basically what I'm, what this is technical, what I'm saying is that the zeros are in the epsilon, they're in this pseudo-spectrum of A, which is kind of an unexpected. Which is kind of an unexpected. So, on that picture I showed you where the zeros were getting closer to the poles, that is logical all those assumptions generally true. Yeah. Because of all the assumptions, we're something like three lines of difference between AZN and UNZN. So this is exactly. I get that some eigenvalue of that operator. Between these two operators is that. And I assume C was in the domain of the adjoint, I get the floor and I get that side. They form a resystem. It means you're weakly converged into zero, which means that side eventually. Here's another problem, right? These are, I think, technical assumptions. So So I think I would believe this is more generally true, right? So, but to prove it, it's a kind of question. That all those assumptions functions coming in system and C being yeah. Then another technical one, sort of a similar result, so now it's collocated. So now it's collocated. So I'm assuming moreover it's collocated. Just different assumptions. Collocated A is negative semi-definite. And I have a very nice B, right? So it's in the domain of the data. Then we get kind of a uniform result that therefore it's uniformly close to some I know. It's I guess similar to the previous result of a little a little bit more simple. Yeah, lots of assumptions. I don't think A being negative semi-definite and B being exclusively. If A is self-adjoint, so that in pseudo-spectrum the union of these absolutely wrong, so does that does the previous self-defense? Yeah, the normal part, yeah, we'd be able to use that correctly. Exactly. And also for this one, right? So skew joint. So A again is normal, right? This is actually the same kind of result only A species. Okay, what do we have? Perturbed, right? So you can think of it as perturbing the spectrum of an A with only point spectrum and finite magnitude of perturbation. They're well defined as certain focus, they're well defined as smooth curves in the complex chain. That's probably branch of the smooth curve that goes to zero or infinity. That's a typo. That's my non-strict improper system doesn't have any asymptotes, so that's not interesting. We did show, we did prove the poll zero. We did show, we did prove the pole zero, the eigenvalue zero interlacing property for collocated self-adjoint and advanced second-order systems. Does it collocation? This is another question, right? Does this interlacing property hold for more general types of systems? We got some results, right? So the asymptotes, but pretty narrow, right? The general asymptotic behavior. The general asymptotic behavior of the spectrum is really still wild because it's really short. Zero is being in the pseudo-spectrum is kind of like an interesting result. This ties in, right? This is some kind of a generalized eigenvalue problem. Beta, if you have beta not zero there, and alpha over beta. zero there and alpha graders exactly the invariant zeros so that's what we're solving right developing this so that gives you the invariant zeros but we get yeah the root locus of different kinds of infinite dimensional systems have totally different behavior infinity void surprise and by looking at pseudo spectrum you got some results right is there something more we can get from the pseudo spectrum or looking at the spectrum of this Looking at the spectrum of this type of cell, it's just the asthmatic behavior of the spectrum. That's a number of questions you have more, you got. So, in the center of the big one, it's what's happening in infinity. Proving that the eigenvalues go to the zeros, general skew adjoint A and complicated. I really believe this is true, but yeah, it's only showed that for that subclass. We've only shown it for that subclass, the second order persistent. What happens when it's not collocated? I mean, I think handling follows through the spectrum of infinity, maybe keeping A, B, self, and joint skew adjoint, but looking at more general. And when we'll look at non-unbounded B and C, then it's very interesting to consider, of course, non-point spectrum, because if you've got a non-compact derivation, you can move spectrum. 