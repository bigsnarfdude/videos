Very nice paper by Bill Helton as well on in the real in the case of R D instead. So and one of the things I'll mention in here are sure complements and various extensions. And well, in fact, Michael Jury has already spoken about this to some extent when he talked about realizations. Talked about realizations. And then, what we'll see throughout is that there are certain basic tools that are used throughout these constructions. Although, I mean, obviously, there are variations on these things and generalizations of them, but they're GNS type constructions and uses of Hanbanach separation theorems as well. Right. Okay, so let's go on to the first. Let's go on to the first thing, which is just as a sort of a background, which is the scalar-Fair Rees theorem, which says if I have a trigonometric polynomial with complex coefficients, and if it's positive, then it can be factored as a square of analytic polynomials. In fact, to the same degree as Q on a unit, and again, they're outer. And again, they're outer functions on the unit disk. And there are various proofs of this. I don't need to tell you what outer means in this case, because I'm sure you know, but probably the proof that most people are familiar with comes from the book by Reese Naj. And then there have been various generalizations that were done that culminated in the late 60s. That culminated in the late 60s with a version by Marvin Rosenblum where the polynomials were operator-valued. So here is again stated now in Marvin Rosenblum's version. And of course, the definition of what it means to be outer in this case changes. And we have to assume in this case something which actually works out to be the same as in the scalar case, which is that the closure of the range. Is that the closure of the range of F is an H2 space? Right, so okay, so we have a trigonometric polynomial. We associate to it an interpolitz operator as I've drawn here. And it turns out that the polynomial is positive, if only if the corresponding turpletz operator is positive, strictly positive. And then Then it turns out that there is actually a very nice, simple proof of Rosenblum's version, which could be taught in any basic functional analysis course. It's a variation on Rosenblum's proof. In fact, you just essentially first start off by, since we're assuming that Q is positive, the interpolates operator is positive, we factor T as F star F with any factorization. It could be the square root of T, for example. For example. And then, since T is turpulates, of course, it's equal to S star, T s, where S is the shift unilateral shift operator. And in other words, that tells us that F star F is equal to F S star F S. And so there has to be an isometric W so that F S is equal to W F. Okay, and then what you do is you write W using in terms of its mold decomposition. And the fact that And the fact that Q is polynomial is a trigonometrical means that if you take T and multiply it by a sufficiently large power of the shift, it becomes analytic and so then commutes with the shift. And then you can use that to show that the unitary and using the fact that the unitary part of the bold decomposition maps its corresponding subspace to itself, you conclude. Space to itself, you conclude through applying high enough powers of the shift that n has to be zero. And therefore, this w is itself a shift. And then a little bit more work, a little bit more, also an elementary argument, shows that the dimension of the space L is less than or equal to the dimension of H. And then we can take F to be analytic on H. And its range, of course, by the construction is going to be dense. By the construction, it's going to be dense in H2 of L, and so therefore it's going to be outer. And then it's easy to see also that the degree has to be the same as the degree of Q. So that's basically a quick proof of Rosenblum's theorem. There are alternate ways of doing it. And as I mentioned earlier about short complements, let me just give you a very brief definition of these, at least in the case where an operator is positive. So if we have a two by two matrix operator. matrix operator matrix A B B star B C the short complement supported at H so in other words in the on the one one corner is the largest positive operator that can be subtracted from that corner so that the resulting thing is still positive now there are various ways of calculating that sure complement so for example if that if the 22 corner happens to be invertible then To be invertible, then the formula that you get for it is that you can write it as a minus b star c inverse b. This connects very well with what something that Michael Trury noted, that you can then get this factorization. His, I believe, was like this as well. And then in that case, this This connects to the realizations that he spoke about. In that case, the operators that are here were replaced by functions. And you get a realization. Now, in a single variable case, there is a sure complement argument that gives you a factorization. What you do is you use the fact that the let me see if I can. Uh let me see if I can write okay so if you use the fact that the that the that the operator is is uh that you're starting off with as turplets you take the short complement on the at the very top left corner uh then you end up uh with something so if I have an a minus x I have minus x, I have b star b x. So if I what I first do is I take the the what I should have said is I start off with t 0, t 1 star, t1, and so on, up to t n star, up to t n and then I break this up into blocks and this will be now try Now tri-diagonal. So this will be my this thing here will be my A, and this will be the B here and B star. And then the sure complement of that corner there has the property that if I look at A minus X B star BX, that that's going to be greater than or equal to zero, if and only if this turplitz operator is greater than or equal to zero. And then you can use that actually to get a factorization. To get a factorization of the turpless operator. Now, the nice thing about this, clear this somehow. Okay. The nice thing about this is that this works for more than one variable. So if I have two or more variables, I can do the same sort of thing. Sort of thing. And now the A's and B's are actually turplets operators of turplets operators. And the problem is that when you break things up into blocks like this, the counting is off a little bit. So you only get an approximate solution, the larger the blocks are that you choose. But if the thing happens to be, if the If the original operator happens to be invertible, if it's greater than zero, which corresponds to q being greater than zero in the multivariable case, then it turns out that you can perturb the original, this new thing that you get to get a solution to the original problem. So, what that tells us then is that every strictly positive polynomial in In several variables can be factored, unlike the case for the single variable case. And then it turns out that it's possible to use a Cayley transform to say something about rational factorization of operator value polynomials over Rn. Now, it turns out that the scalar case of this in the several variable case is actually Uh, case. It's actually a special case of theorems by Schmunken and Putinar. So I will introduce those next and mention a few, uh, some uh some terminology. So I'll be a little bit sloppy in what I'm saying here, but if you have any questions as I'm going along, please be feel free to ask. Uh, so a semi-algebraic set here, what we mean is something that can be described by a finite set of polynomial inequalities. And this will be, say, over Cn or Rn. A quadratic module will be those things that can be written as sums of squares in a sense, but now you're multiplying by. So actually, I've got I've got, oh, let me try it. Okay, so here you're multiplying the polynomials that define, there should have been a pj there, that define the semi-algebraic set by Fj star and Fj, where Fj's are, FjL's are polynomials themselves. themselves. Okay, so there's also a notion of what's called a pre-ordering, where instead what you do is you so these things will be obviously be positive in this case because the pjs are positive on that set and squares are positive. For the pre-ordering, what you do is you now consider instead products of the pjs. And again, since the pjs are positive, products of them will be positive on this set. And then again. Set. And then again, you do the same sort of thing about conjugating by the x in this way. Obviously, pre-orderings are examples of quadratic modules as well. And we'll say that quadratic modules are comedian if, say, the coordinate functions in this case, the x i's, let me take some sums of the squares of those and subtract. There's a large enough n that you can subtract n times the identity minus that and end up back in. And end up back in the quadratic module. So, Schmuttkin's theorem says that if you're on a compact symbol of grades. Hello, is that me or is that somebody else there? Oh, well, anyway, I don't know where the noise came from. Anyway, so if you have a So, if we have a compact semi-algebraic set, any polynomial which is strictly positive on net is in the pre-ordering. So, it has this way, we have this way of writing every possible, strictly positive polynomial there in terms of the polynomials that define this. Now, in the case of the Fair Reese theorem, the polynomials would just be the ones that define the Just be the ones that define the, say, the detours. Putinard's theorem is similar. Again, it's about strictly positive polynomials, but now about when they belong to the quadratic module. And that will happen if the quadratic module is Archimedean. And one way that that Archimedean. And one way that that's often written is instead you just happen to throw in those this polynomial that I wrote here as one of the defining things in defining the semi-algebraic set. If you make n big enough, that doesn't really change the semi-algebraic set. Okay, let me clear this again. This again, oops, okay. Right now, the last two theorems were primarily, at least there are a number of different proofs available now, and they're, I think, originally primarily analytic. Certainly, that was the case for Schmutkin's theorem. But they all, at some crucial point, rely on something called Graveen's theorem. Called Grubine's theorem, which it doesn't have any known analytic proof, and here's a statement of it. It says that, and again, here we're thinking of x as being detuple variables here. So we'll say that f of x is greater than zero for every x in ks. That's equivalent to that there exists t and u in the pre-ordering so that one plus t times f equals one plus t. plus t times f equals one plus u and uh this turns out to be uh um uh important in uh the proofs of sorry sorry turns out to be turns out to be sorry i hear an echo of myself okay sorry okay so i'm not sure why that's happening yeah okay so let's see that's that's that's craving That's Kirvine's strichtpositief Stellensatz. Now it turns out that there's a mutricial version of this, which is due to Chimprich. And so here we'll write MR of Rx. So this will be the matrix value polynomials, real value matrix polynomials R by R. And then the And then the sum of squares I've indicated in this way, and where the FJs are polynomials. And as we did before, we defined the quadratic module and the pre-ordering. The S hat here just indicates the products of all the polynomials that were used to define the semi-algebraic set. The semi-algebraic set. And Chimpuch's strict positive Stellensat says that essentially the same thing as the original version did, but now we replace, so F again here is matrix valued. We have T is in the scalar pre-ordering, and U here is in the matricial pre-ordering. And we can write. tritial pre-ordering and we can write one plus t times f equals one plus u. The reason I mention it this result is that, well, one thing, one direction is easy to prove, but it's the other going from two to one that's challenging. And what Chimprich did was he used sure complements. So if f is positive, what you do is matrix valued, you take You take in the one, one corner, you look, let's of capital F, let's call that little f, and the other things was indicated here. Little F will be scalar value, and H will be now matrix value, but it was one dimension smaller. The sure complement of H is going to be H minus G one over F G star. Remember, we're assuming here. Remember, we're assuming here that capital F is strictly positive, so little f is scalar value and strictly positive. And so, what happens is that we can factor capital F in the way that I've indicated here, where H tilde is this sure complement. Now, the sure complement that we've got here has a 1 over F in it, so it may not be a polynomial, but there are various clever algebraic manipulations that can be done to get rid of the term. Manipulations that can be done to get rid of the denominators, and that's what he does. Okay, so let's come back to the Fair Reese theorem, but now a non-commutative version of it. And so let's assume now that we have G is a finitely generated discrete group. And I'll write the generators as G0 to Gt, where G0 is the identity. I'll define an involution. I'll define an involution on net to be the takes g star as a g inverse. And the algebraic group algebra for g will be script C here. And I'll write S for the semi-group generated by these generators. And then I can define in a similar fashion trigonometric polynomials. So these will be finite sums. Finite sums over G of the form that I've indicated here. We're now the PGs are allowed to be operators. So I take tensor products like this, and then the analytic polynomials will be precisely those where the elements from the group are actually from the seven group. And a trigonometric polynomial is self-conjoint, of course, if Of course, if p of g star is equal to p of g with the star in the top, and then we'll say it's positive or strictly positive if every irreducible unital star representation of the group, the extension of pi to this algebra, where we tensor by the identity on V of h, the identity representation on v of h satisfies pi. Of h satisfies pi of p is either greater than or equal to zero or strictly positive, we strictly greater than zero. Okay, and well, it turns out that very much like in the first talk for the meeting, we had we letting omega represent the set of all such irreducible representations, we think of that as the points in our space. And then the so we have this non-commutative space in which our polynomials are defined, and then there's something called the Gelfan-Rykoff theorem, which then ensures that there are enough irreducible representations to separate this group, the elements of the group. Now, we also have hereditary trigonometric polynomials. These are defined as polynomials of the form that I've indicated here, where the stars are on the left here. Stars are on the left here. And the W's are in the semi-group. And I'll write H sub H, little H, as for the self-adjoint elements of this. Now, real polynomials will be sums of things that have the following form. They're either sums of things that look like identity tensor A, where A is a self-attoining. As a self-adjoint operator, or W2 star, W1 tensor V, and W1 plus W1 star, W2, tensor V star. So something plus is conjugate in a sense. So it turns out that the first is obviously going to be a difference of squares since all the joint elements are in B of H. And using the fact that W star W is equal to always equal to the identity for any W. To always equal to the identity for any w and a g, then we can write the other thing as the difference of squares as well. And so consequently, you get that the self-adjoint elements are the difference of some of this cone, which are there of the positive cone, which consists of squares. Okay, so the square of an analytic polynomial is a hereditary trigonometric. A hereditary trigonometric polynomial. That's the way I'm defining it. And then it turns out that sums of squares will obviously be in this cum C. And in analogy then with the multivariable Fairy's theorem, we have that strictly positive trigonometric polynomials are sums of squares. And the way that you see this is that Is that that well, first of all, you show that this cone is arc median. And then you get the following theorem, that if, again, G is a finitely generated discrete group, P is a strictly positive polynomial on it, then P is going to be a sum of squares of analytic polynomials. And there are There are a few observations I'd like to make about the proof of it. Essentially, it just uses you look at, again, we have C for our sum of squares. Here we write H as the difference of the sums of squares. Then we look at the things on the n-dimensional matrices. And we have And we have this order unit, which is the thing that we use to show that the Comu is Archimedean. And then we can use this to actually define norms on CN closure. And then we use the Choi-Effros theorem to get a matrix order structure. matrix order structure that turns this into an operator space, an abstract operator space. And so it'll be completely isometrically isomorphic to a concrete operator space. And then, well, we can then use what we do is we assume that we have something that's a Something that's a p which is strictly positive, but that's not in that cone that we started off with, and then we use some sort of Han-Bannock theorem to separate that and find a separating linear functional. And then we use that separating linear functional to build a representation which is positive on the com, but not positive on the P. But not positive on the P, and so P couldn't have been a positive polynomial in the first place, giving us a contradiction. Right, so anyway, this is just to try to give you an idea of some of the tools that are often used in this area. And in particular, we see that this is the way that often that the hot. That the Hanbanach theorem is used for one. And it's sort of again a version of the Fayerise theorem that I mentioned earlier. In fact, if I take G to be a commutative free semigroup on, say, on n generators, it gives me back the theorem that I mentioned earlier. Right, so let's now look at Scott McCullough's version of the theory. McCullough's version of the Fairies theorem. Now, with a difference here is that in the previous theorem that I gave you, I assumed that the polynomial was strictly positive. And what Scott was able to show was that if G now is a non-commutative free group in D generators with an identity E, so that was what I was calling G0 earlier, and S is the free semi-group with these generators, then in fact, we have that. In fact, we have that we can do better than what we did before with being able to just factor strictly positive things. So, as before, we have hereditary words. We have the inverses on the left from the semi-group. And so we write the hereditary words, V, inverse, W, where V and W or an S. And then if we restrict to words of length N, Restrict to words of length n, we'd indicate that by sn and h n. And then what we do is for the representations, well, essentially what we're doing is we're evaluating on unitary operators. So we have u1 to ud will be a detuple of unitary operators. And then the forward w, which is g word w which is g j1 up to g j k we write u w for u j one up to u j k and then we can evaluate in the way that I've indicated here for H equals V inverse W in the in HN Now as before we'll again write C for the algebraic group algebra and then we can define trigonometric and analytic Define trigonometric and analytic polynomials as we did before. And then such a polynomial, if it's a trigonometric polynomial, will be positive if when we evaluate it on any detuple of unitary operators, that we get something that's greater than or equal to zero. So, Scott's non-commutative Fair Reese theorem says that if f is a positive trigonometric polynomial, so in other words, non-negative, essentially is what I mean here, that of degree n, and it's in d variables, and if we set r equal to the sum of the various degrees for the various terms, then Various terms, then f is going to be equal to bj star bj. So, in other words, it's a sum of squares, and it's going to be a finite sum of squares. When d equals one, it turns out that we just get back the usual Faris theorem through the Berlin's theorem. Now, the way Scott proves this is he uses some of the tools that I mentioned. Is he uses some of the tools that I mentioned earlier? So if L of Sn are the bounded operators on the Hilbert space, we'll let that be bounded operators on the Hilbert space of orthonormal basis indexed by Sn. Remember S is our semi-group. The operator T on this space is said to be triplets. If we'll write it as T V W, this corresponds to a word that looks like V inverse W. And then we note that if we multiply V and W on the left by GJ, of course, since G J comes from a group, then V inverse, G J V inverse times G J V W will just be V. W will just be dw. So it has this turpletz property. So these act like the group elements here act, or the elements that define a semi-group act like shift operators. So we'll denote the turplets operators in this space by Tn, so these finite dimensional things. Things and then the ones that are matrix valued, mk valued, we'll write as mk tensor tn and so if t is a function from h n to m k we get a turplets operator in the way that I indicated here and uh and then uh what Scott does is he uses a version of Kara Theodore's interpolation theorem. Theorem to extend any such t, which is now, remember it's only defined on this tn, he extends it to tn plus one. And so that it agrees with the original t on mk tensor tn. And continuing in this fashion, he builds a positive kernel q on a Hilbert space banned by the elements of s tensored with ck. And then from here, the proof follows a familiar GNS or Steinspring representation construction. You define an inner product in this way in terms of this positive kernel, drawing out by the null vectors. You look at the left regular representations. These turn out to be isometries and then can be extended to unitary operators. Extended to unitary operators. Then, if you look, then you can define a map phi, which acts in the way that I've indicated here, where Vx is equal to X tensor, the identity. And this will be a completely positive map on Tn. And then what you do is you extend that to by the Arviset extension theorem in order to finish off the proof. So that's just a brief sketch of. So that's just a brief sketch of Scott's proof of this non-commutative version of the Fair Erice theorem. And again, the advantage of it is that it's for non-negative attributive polynomials and it's over this particular group. He also in that paper does a real version. So if you real version. So if you have if you look at if you look at a polynomial which is for all d tuples s of bounded self-adjoint operators, you have sw tensor aw is greater than or equal to zero. Then again, you have a factorization as a square. Or a sum of squares. And the proof is very much like what I was just discussing before, but now he works with non-commutative Hancock operators rather than durable operators. And in place of the Caratheodori extension theorem, he uses a non-commutative version of the Curto and Fialco flat extension theorem. And then it turns out that the left regular representation. It turns out that the left regular representation sends the generators to bounded self-adjoint operators. Now, there's an alternate, there's another version of another real version, and this one is due to Helton. So if we start now with, again, XJ's as being generated as some semi-group S, and YJs as formally the adjoints of these, we consider words that are mixtures of these letters. Words that are mixtures of these letters in A. And then we look at finite sums, PWW with the obvious involution. And here these are scalar valued. And now AD will be the subspace of all polynomials of degree at most D. So, in other words, are consisting of words of length at most D. Of length of us d. And if we're given a d-tuple x1 to xd of operators on c and d, we get a representation of this algebra by which sends the xj's to x, capital xj and the yj's to their adjoints. And we'll say then again, as before, that the polynomial is positive if a p evaluated at this is greater than or equal to zero. Is greater than or equal to zero. And so Helton's theorem then says that if I have a polynomial in AD, which is greater than or equal to zero, then there's a finite number of Rj's so that P can be written as the sum of squares of these Rj's. And again, Kara Theodore. And again, the Cara-Theodori theorem plays a role in this getting a bound for the number of terms that are involved. And Hambanach separation theorem also plays an important role in the proof. Right. Now, what came after that was, well, Bill. Bill Helton and Scott Cole have worked quite a lot together in this area. And so there was one of their most important results is a characterization of convex free semi-algebraic sets. So the setup is very much like what I was just describing in Helton's theorem. We consider matrix value polynomials in freely non-computing variables, x1 up to xd, in molution defined as. Inolution defined as before, and evaluations, as I mentioned, but now with the X's coming from real symmetric and by n matrices. And if P will be a free symmetric polynomial, we'll assume that at zero, that P of zero is greater than zero, and we define a set script D of P of N as being the D tuple. So n by n matrices. The d tuples of n by n matrices over x for which p of x is greater than or equal to zero. So at each level, for each n, we have a collection of these matrices. And then the union of all of these we denote by d sub p without the end here. And we'll say that this d sub p is convex. Sub is convex if each if at each level the set that we have is convex. There's a special case of this which is particularly interesting, and that's when p happens to be linear, and then p of zero is the identity, and the inequality l of x is greater than zero is often referred. X is greater than zero is often referred to as a linear matrix inequality or LMI. So the Helton-McCullough theorem in this case says that if, well, there should be, there's a, sorry, there should be R missing there. It should be free. P is a free symmetric polynomial and D of P is convex. There is an L, which is the dimension of the space on which it's finite, on which capital L will live on. And capital L is half line linear with L of zero equals to identity. And it turns out that this set D sub P is actually D sub L. So all of these All of these convex-free semi-algebraic sets are then described by linear matrix inequalities. Now, this has, again, sort of the feel, because the L's live on lots of levels, this in some sense has the feel of a Crane-Millman theorem. So you can think of the L's as describing. So you can think of the else as describing bounding things at extreme points on the sets DP. And just to say a few words about the proof, well, again, the Hanbanic theorem plays a role, but in this case, now it's a matricial version of the Hanbonic theorem, which is due to F. Russell-Winkler. And then in then in in place of the of the so we we use these uh uh L mi's or these uh linear terms here in in place of the linear functions in that theorem now uh it turns out that uh that there's also uh a way of describing A way of describing positive polynomials in this case using some of these results. So, under the same conditions as on the last slide, we'll say that the script dp is bounded if there's a capital C so that C minus X J star X J is greater than or equal to zero. Now, it turns out that in their original, in their paper, they assumed boundedness for DP, but then maybe. DP, but then I was reading there's a dissertation by Creel, who's a student of Marcus Schweikhoffer, who claims that, in fact, that the boundedness is not needed in their theorem. So that's why I didn't state it there. Anyway, they've also proved, Helton and McCullough have also proved. Helton and McCullough also proved a version of Putinard's theorem in this context. So if we start off with a P as being a free symmetric polynomial, DP being now bounded. And if Q is now a positive polynomial on DP, then there are polynomials PJ, SJ. are polynomials pj sj rk tml so that q can be written in this fashion essentially it's again a sum of squares now notice here that the that um that this this part is let's see the the this is like the part that's ensuring that the thing is our comedian this this cone positivity cone that we're dealing with uh and and also it's important uh and and also it's important to notice here too that the that the assumption is that the q is strictly positive on this domain dp in order to uh to state this theorem uh so as i was saying it's an analog of futnard's theorem from the convex or from a commutative setting and as you might expect there the the Expect the tools that are used are similar variations on the tools that we've seen already. They're versions of the Hambonic theorem that are used and the GNS construction. Another important thing that's worth noting is that convexity of the domain is not assumed. So DP is not necessarily convex. However, in that case, because it's not convex, this strict positivity is really essential here for this to work. Here for this to work. Now, on the other hand, if you do assume that dp is convex, but now not necessarily bounded, it's possible to come up with an even stronger result. So here's a result due to, again, Helton and McCalba now with Igor Klepp. And it says that if P is now a free symmetric polynomial, and DP is convex. Polynomial and dp is convex. If a polynomial of q is concave on dp, so in other words, it means that minus q is convex, then q has this very nice representation as essentially as a sum of squares. And the special case when p is the identity, so that dp is everything, just Is everything just gives you back Helton's theorem? And even though I haven't stated it here, it turns out that it's possible to give limits on a number and the degrees of the polynomials that are involved. And again, just to remind you of what it means for the polynomial to be concave. Now, it's also interesting to look at the case when Q is scalar valued and Q of zero is the identity. And q of zero is the identity, then you get that it's rather restrictive in this case. It turns out that q of x has to look like this, it's essentially quadratic in this case. Okay, well, the last little bit I want to talk about, I don't know how I'm doing on time here. Maybe I'm going too fast, but uh. I'm going too fast, but are about trace polynomials. So, this is something that hasn't, the other things that I've talked about so far have been mostly mentioned in other talks, or the framework that I've been talking about has mostly been mentioned in other talks, but trace polynomials have not so far. So, a trace polynomial is a polynomial which is, again, in symmetric, non-commuting. In symmetric non-commuting variables. And along, we take those along with traces of their products. What I mean by traces of their products is that when we look at evaluations of them, we take the traces of the evaluations. And then so here's an example of such a polynomial. You take f equals x2 x1 squared x2 minus the trace of x1 x2 times x1 cubed. Okay, so there's also the notion of what's called a pure trace polynomial. What's called a pure trace polynomial. So, this is a trace polynomial that only involves traces. There are no, there are no, there are no free variables. So, for example, if I were to take the trace of f above, that would be an example. So, I would end up having trace of x2 x1 squared x2 minus trace x1 x2 times trace of x1 cubed. trace of x1 cubed, that would be an example of a pure trace polynomial. Now what's interesting about these polynomials is even in the case of pure trace polynomials, they don't necessarily have to be commutative. So for example, we know if we're just taking products of two or even three things, trace of x1, x2 is trace of x2, x1. And we can do permutations for three, but if we get up to more than three, then Then we won't necessarily have equality here when we do our evaluations. So there's still a level of non-commutativity involved with these pure trace polynomials. Now, there's a very nice recent paper by Klet, McGron, and Volchich, where they prove various theorems related to the ones by Helton McCall, which I mentioned earlier. Earlier, characterizing or giving positive Stelling's data for trace and pure trace polynomials. I'm not going to state any of these theorems because it would require introducing too much notation at this point, just to say that they're there. And one of the interesting things to mention at the end here is that, in fact, there are connections that were made actually some time ago. Made actually some time ago. I think this goes back to a paper from 2008 by Klepp and Schweikhoffer that found connections between these positives Gellenzeitz and classes of trace polynomials and the Kahnemetting conjecture, which, as we know, was recently proved to fail. Now, I haven't read the, I have to say, I haven't read this paper on the common betting. On the common betting, on the failure of the common betting conjecture. But Scott tells me that, in fact, there was some use made of these results, these earlier results on trace polynomials by people like Klept and Volchich and others. Okay, well, I think I'll end it there. And thanks. 