To speak at this great conference. If you saw an earlier version of the program, or perhaps a printed version of the program, you saw I had announced that I'd be speaking about the top eigenvalue of random trees. I changed my topic for quite a sad reason. So, last Friday, I learned that a member of my research group, a summer intern from ENS, graduate intern, Deadlock Conrad Falkin, had Fauqui had died unexpectedly. A circumstance there still in theory seems to have died in his sleep in the night. So that was quite a shock. We had a great summer research group going. He was a brilliant, young, enthusiastic, chaotic guy. And he was a lot of fun to work with. I was hoping to have him back for a PhD. And so Sunday night, I decided. So Sunday night I decided that I was going to switch topic and talk about our summer research project to honor him. Today I just learned yesterday is also his funeral. So I sent this photo to his mom. She's probably going to come and visit Montreal next summer. I hope that I can honor his enthusiasm for math and his work by giving a talk about what we worked on. About what we worked on. So that's the sad story. I hope you like the math that's to come. And my apologies, you know, normally I give myself a little more lead time before I talk about a new project, especially work in progress. So if it's a little bunky, please read. And as in any And as in any of my talks, please feel free to ask questions if anything is unclear, or if you're curious about some final. I'm going to just give a sort of extremely brief situate this work in what is sometimes called network archaeology. So network So, network archaeology is, I would summarize it as the sort of branch of our field which seeks to infer structure, let's say, latent structure of a network. From a single snapshot. Okay, so you might have some network that either grew in time or was created with according to some rules, and you'd like to understand what those rules were, an example. What those rules were. An example that many of you have seen would be the stochastic block model, where you see the network, but there are in fact communities, and you'd like to understand what those communities are. The example that I'm going to jump right into in this talk is this SI model. So the SI model, S stands for susceptible. I stands for infected. Okay, so this is a model of, say, viral spread, a very simple model. So you have a graph G whose size will be N, which I'll mostly assume is finite, but you can also do this on infinite graphs, and I'll say a little bit as we go about how to. And I'll say a little bit as we go about how to modify the kinds of questions that I'm interested in in that setting. So we have an initial infected node at some root. It might make sense to call this the source of the infection, but root will be convenient for me. And so at time zero, all other nodes are susceptible. Okay, and then the dynamics are just that at rate one, each infected node infects each neighboring Each neighboring susceptible not. Okay, so let's just run a quick example. Right, so maybe let me label the nodes with their timestamps, infection times. So the root in this case would be labeled. The root in this case would be labeled one. One of its neighbors, you know, at rate one, each of its neighbors are infected. So maybe this node is infected second, then the infection is equally likely to spread along either of these two edges. Maybe this is node three. This could be node four. Maybe now there's the current boundary has size four, right? Each of these edges is equally likely to spread the infection. So maybe five, six, seven. 7. I don't know. I should really now, I should be drawing which edges the infection spreads across, because if I write 8 here, you can't tell whether this spreads from 4 or from 7. So maybe this is 8, and the tree that was just grown was this. So this creates a sequence of trees. Freeze where T1 just consists of the root with no edges. And then Ti plus one will be Ti plus some edge Gi, where Ei is a uniform edge from the edge boundary. Edge from the edge boundary of TI. Let me just throw out that partial TI. Okay, is the model clear? Okay, so Junyang used in sub R for uniform random. I'm using N sub U. Okay, so T n then, the final tree, is The final tree is maybe I'll note that this depends on the series. Imagine we add just one at a time. Yeah. Can you infect multiple neighbors? You just take one edge? So at each edge has a vertical. So given the tree Ti, the next edge to be added, there'll be a uniformly random edge connecting the current vertices. The current vertices. So, this set here, this is a uniform random element of the set of edges V, W, such that U is a vertex of Ti and V is a vertex of Ti and W is not a vertex of Ti. So the final tree, Tn, will be a random spanning tree. be a random spanning tree of G. Okay, and in fact, due to the way I was choosing these uniformly at random, it's actually also distributed as the first passage perpellation tree on G from R if I put IID exponential one passage time. Exponential one passage times. So that's another way of thinking about this model. Okay, and the question of root reconstruction is how well can the root R be recovered? Yes, from observing this final tree TM. Yes. So I might be confused about the notation, but I mean, certain trees are going to occur with different probabilities from other trees. So it's not, when you say it's IID x1, it's. So this, I mean, this is really a parenthesis. I'm not going to. I'm not going to use this perspective in the talk, but if you just think about fixing a given root, you put weights on the edges, and then you form the tree, which is just the union of all the shortest weighted paths to all of the nodes in the graph. And that turns out, if the weights have this distribution, they have the same distribution. This is true. Okay. Okay, so to give this question. Okay, so to give this question I mean to make this question a question we can study mathematically I need a little bit more precise. Again, there's two sort of ways I want to do that. So I guess I need a new board. Boards are like space in your apartment. It's like no matter how many boards there are, you always want just a little bit more. Okay, so let's say if we're given some parameter m, this is the number of guesses we're going to be allowed to make for the location of the root. And some function s, which is a function from spanning Trees of T into sets of M possible guesses. This is my notation for subsets of the vertex set of size M. Okay, so this is rules for guessing a set of size M of the vertices having observed a tree. Okay, and then I can ask, well, first, let me define the error of my function. I'll define two possible errors. Function, I'll find two possible errors. One is the probability that the root is not in the set that we guess where we've taken a random root. Okay, so if the root is chosen uniformly random, I ask what's the probability that my I ask what's the probability that my guessing function manages to guess the root among its guesses. And another thing I could ask is, so that's the sort of average case analysis. And then the worst case question would be, what's the probability, what's the sort of, sorry, the maximum error probability, this is the, sorry, the probability of the break's not in. And now I can ask, what's the maximum probability that Probability that my guessing algorithm fails over all vertices. Okay, so for a given guessing algorithm, that's two kinds of error, the average case error and the worst case error. Okay, and now I want to look at the best algorithm in both cases. So I'll ask about About so epsilon m of g is the minimum of epsilon s over algorithms and epsilon m plus is the minimum of epsilon plus of s. Okay, is the question clear? So this makes sense for So, this makes sense for any graph. I'll start to look at specific kinds of graphs almost instantaneously. Do you care if the algorithm is efficient or not? No. I mean, the algorithms I'll end up talking about is Talking about, it turns out, will be efficient. But in this, I mean, as formulated, there's no requirement. Okay, so let's look at a very special case, which is where G is a clean sizer, the complete graph. And in this case, there's a theorem. Um and in this case there's a theorem from uh Seth Dubeck, uh Luc Duploi and Gabriel Lugosi from 2014, which says that, so informally, uh big O1 guesses suffice to find the root. And this is for the average case or? So in a symmetric graph, it doesn't matter. So for because Kn is vertex transitive, yeah, it's the same for all vertices. So I can be more precise and say that for all epsilon, there's some m such that for all n, Such that for all n, the error, which can be, as you said, either the average case or the worst case error, is less than epsilon, if you're allowed to make any guesses. And I think I'll probably end up just in terms of time telling you more about this proof than about the proof of. Then, about the proof of our result, which I'll state in a moment. The ideas from here are important input to our arguments. But before I even tell you our result, I'd like to reformulate the problem a little bit in the special case of the complete graph. It's so symmetric that, in fact, you can kind of couple these tree growth procedures for all n. These tree growth procedures for all n in a nice way. And it's using something that briefly came up in the lightning clip talks. So, an equivalent model for your head in is the random recursive free model. Okay, the only insight you need to understand why this is equivalent is that if at a given time we've grown some partial spanning tree of Kn, right, then we said that the next edge that will arrive is a uniformly random boundary edge. Okay, but that means that, I mean, every virtual And that means that, I mean, every vertex over here has exactly the same number of boundary ideas by symmetry, so that means that it attaches to a uniformly random existing vertex. Okay, so if I'm labeling vertices by timestamp, like I did in my drawing, then the new vertex, you know, if there's, if I've already, you know, have spanned k vertices, then the vertex of timestamp k plus 1 will just attach to a uniformly random timestamp basis entry. Okay, so in this setting, and that can then be sort of projectively extended to an infinite sequence. So this is, I can divide a sequence, Ti, i at least one, where T1 is just vertex, the vertex with timestamp one, okay, and then t T, sort of inductively, tn plus 1 is tn plus a random edge e, and e will be sort of the vertex with timestamp n plus 1 attaching to a vertex u, where u is a uniformly random time stamp for one at 1. Okay, so now if you want to ask about root reconstruction for this tree, I've been Root reconstruction for this tree, I better hide the labels or else you can just see which was the root for the labels. So then I can let Tn star be Tn with its labels permuted according to sigma n, a uniformly random permutation. And then what the, you know, what the theorem says then is that for all epsilon, there's some m such that for all n we have an algorithm s such that Such that the probability that one is not in so sigma and inverse of the set of nodes that S guesses when it observes T and star is less than possible. Doesn't do them. Sorry? It doesn't do them. It doesn't bother. Thanks. Okay, and the reason I went through this slide referred to. And the reason I went through this slide reformulation, one is because it's going to be useful for me to use the random recursive pre perspective in a second. But two, what was two? I've been warned against this. Never say how many things you're going to say in the press. Oh yeah, two is that this perspective sort of also works on infinite graphs. Like if you wanted to do the same thing on an infinite three-regular tree, I could just grow for Could just grow for end steps, show you the tree with its time steps randomized, and then make you guess the tree. So that's one way to formalize that this can also be difficult to choose. Okay. So now I can state our results and Can I ask, is the algorithm easy to get a thing up with? I'll talk a little bit about their algorithm. I mean, basically, they analyze the MLE. It turns out that for trees, you can write down the MLE pretty explicitly. They don't quite analyze that if they analyze the relaxation. I might say a bit more. Okay, so what we've What we proved, and this is a subset of the people that were on that slide: me, Sasha, Tasman, and Theodore is that you have a similar result on random graphs with given degree sequences. So if I fix a reasonable Sequence of degree sequences. So dn is going to be dn1 at the dn of some length which tends to infinity. Okay, then Okay, then order one guesses suffice. So, in the sense that for all epsilon, there's m such that for all n, if g n is a uniformly random connected graph with a degree sequence. Then epsilon m of g n is less than epsilon. Okay, so I don't want to tell you what reasonable means because we're still working out exactly what we can do. I think probably a sort of third moment assumption on the degrees will be enough. I think you need at least 2 plus epsilon. I don't think it's at least. At least things get substantially more complicated if you only have two moments. There's something that goes wrong. But I won't, you know, for this talk, just don't ask me what these means. Okay. So, you know, we can't cover the special case that GN is the giant component of GMP, very supercritical. Critical P. Okay, and that's why I wanted to allow the size to be not necessarily exactly n. And I'll say the theorem is false in general with epsilon m replaced by epsilon m plus. Okay, so the basic point is that there can be bad local So, the basic point is that there can be bad local configurations which make it impossible to find the root with order one guesses. But if you let yourself have a one start, then that doesn't happen. Yes? And the reason you don't want to say what reasonable means, but just a sense of it, you're saying it's like roughly typically average degrees constant issue. You need bounded average degree. You also need to know that in this first passage percolation, you don't hit nodes of very large degree to quickly. I will. Yes. Just to make sure I understand. This is a you see the graph also. You see the graph. I don't think we need that for our algorithm, but that's the model that I'm thinking, yes. Okay. So let me give you a proof idea for this theorem, and then we'll see how much time is left. And then we'll see how much time is left. Any other questions? Yes? Is this with pi probability over GX? Yeah, good. So this is, we could probably do that. For now, I was thinking averaged over both GM and the tree. But I don't think it would be much more work to do what you said. Yes. If you quantify these rates of convergence, do you get the same rates as well? So I think, I mean, that's related to this question about the relation of m to the error. I think we actually, well, in some, if the degrees are bounded, I think we actually do a little bit better than their work. Actually, I think in all cases we do better because we have another project where we improve their analysis to improve the rate. Their analysis to improve the rate guarantee that they get. But the methods are similar enough that, in principle, sort of what you get on both sides should be the same. Okay. So any other questions? So I'm telling you the proof idea for the feedback to Heroy-Lugosi result, and I'll summarize this as. Summarize this as nested polyurons. Okay, and I'm afraid I need now another reformulation, a reformulation of the random representree formulation to explain this. So let me tell you what the Ulam Harris labels are. Okay, so this is just a sort of nice way of tracking tree growth. So I'm giving myself very large notes because I'm going to write two things inside of these. So the root is labeled with the empty set. Its first child has label one, its second child has label two, its third child has label three, and then And then we just tack on to these strings for the children. So this would be the notice label: 1, 1, 1, 2, 1, 3, and so on. Okay, so strings at depth, nodes at depth k will be labeled with strings of integers of letter k. Okay, and so v, you know. A node V1 up to VL has parent V1 up to V L minus 1. So that should be enough to sort of understand the Ulam-Harris labeling and tree. And so now I can think about encoding the random recursive tree growth process just by saying that the initial root is at the root of the Ulam-Harris tree. Paris tree, and then I'll always add children to this diagram from left to right. So the first note to attach to one is always the vertex. The timestamp two, it'll go here. Then maybe three comes along and attaches to one, so it's a second child at the root, so it goes in slot two. And then maybe four attaches here and five here and Here, and I don't know, six here, and seven here, and eight here, and so on. Okay, so we fill, you know, we have children go down, and then amount of children, siblings filled up to it. Everyone's okay with this theorem? I need this basic thing, so I'll space it. So now I want to I'll write Tn of V. So this is for V by N Elon Harris label. I'll let Tn of V be the subtree Of V be the subtree of Tn rooted at V. Okay, so in this example, I guess we're at timestam eight. So the sub tree of T8 rooted at node one, two is this tree five, six, seven. Okay, so I just extract the subtree. Okay, and now let's think about the first step of the tree growth process, of the random tree process, always just attaches node 2 in this specific location to node 1. Okay, and now let's think about how the subtree rooted at this node 1 evolves in time. In time. Okay, so well, at time one, it just has, or sorry, at time two, when there's two nodes in the tree, it has one of those two nodes. And in general, what's its size at time n plus one relative to tn? Okay, well, at time n, you know, we attached You know, we attach to a uniformly random node. So the probability we attach in this subtree is just proportional to the size of the subtree. Okay, so Tn plus 1 is Tn1, and then it grows by 1 is the probability Tn1 over 1, and it doesn't grow otherwise. But this might look familiar. This is just Polyerin dynamics. And so you can prove straightforwardly by induction that the size of Tn1, the size of this subtree here at time n is uniform on one. Uniform on 1 at 10 minus 1. Okay, whenever I write uniform on an empty set, it's just communion for that to mean 0. Okay, so if n is 1, then this is just 0. Okay, and then Martingale convergence theorem tells you that Tn1 over n converges almost surely to a uniform zero length. Okay. Let's think about what happens at the subtree of label two. I'm not going to do all the notes. So actually let's think about what happens in this whole subtree over here. Okay, so this is, we looked at TN1, this is the complementary tree. Complementary tree. Okay, so sometimes nodes attach over here, and what happens over here doesn't change. Sometimes nodes attach in here, and then when a node attaches in here, it just attaches to a uniformly random timestamp from the timestamps that are available to it. Okay, so that just looks like a scaled, you know, a time-change version of the original process. This tree over here has the same structure as the whole tree. You know, it's just every node has infinitely many children. You attach to a uniformly. Has infinitely many children, you attach to a uniformly random one that's already filled. The first node that's filled is always this node 2, and then the proportion of nodes that attach on either side of this cut evolves like a polyurn, just like over here. It's just a slowed down polyuren because not every new node attacks in this circle. Okay, so TN2, this is Tn2, the size of Tn2, I can't say over n anymore, but over, let me call it, just to be consistent with my notation. So I want to compare it to the subtree of the overall root, where you only look at nodes at the children starting from the second. So let me call that Tn empty set greater than or equal to 2. equal to 2. So this ratio, the ratio of the size of the 2 subtree within this subtree, will also converge almost surely to a uniform 0, 1. And the limit will be independent of the this length. And this same structure sort of repeats inductively in two ways. You sort of, so what this tells you, this implies that the size of Tn2, oh by the way, this denominator here, right, what is it? It's n minus the size of Tn1. Okay, and so this, the denominator is approximately. is approximately 1 minus some uniform, the uniform from over there times n. Okay, so the denominator is like a uniform times n. This ratio is converging to another uniform, so this implies that the subtree rooted at 2 will have size which converges almost surely to a product of 2 uniforms. And the uniforms are now related. And the uniforms are now related. And if you sort of do this a bit carefully, what you learn in general is that if you look at a node, V1 up to VL, so that's a label in the Eulon-Harris tree, then The size of the subtree rooted at v, the proportion of nodes that fall within this subtree will converge almost surely to a random variable which is distributed as the product, a product of uniforms. And the number of uniforms in the product is the sum of the entries of this vector. Okay, so basically the idea is that you lose a uniform factor each time. A uniform factor each time you go across or each time you go down the country. Okay. So, the way I think of this, or a nice way to think of this, is that there's some, you have a, you know, total mass one if you renormalize at the root, and then that mass sort of flows down the tree and splits at each level, and the way it splits is according to this sort of uniform splitting. So, think of that kind of mass flow. Okay, and now you know, the first couple edges, that split will be relatively bad. That split will be relatively balanced. Maybe it's a third, two-thirds, maybe it's 90%, 10%, but it's very unlikely to be one in a million and the rest. Whereas if you've gone very far down and across, then the amount of mass that's flowed there will be very small. So that suggests a good root-finding algorithm. Okay, so let's just, so given the parameter m, this is remember our number of guesses, let's find the edges e1 up to e m minus 1 for which tn with the edge removed is most balanced. Okay, so we find the k minus, the m minus one most balanced cuts in the tree, and then we just return the set of end points of those edges. And it's an exercise that. It's an exercise that this set of edges is always connected, so it forms a tree, so it has K endpoints, or M n points. And so then it's not too hard to show that the probability that the original root fails to be in this set is less than epsilon if we take m to be sort of positive. Sort of polynomially large epsilon of this. And the proof of that, you can imagine just trying to run some union bound, and that works. I mean, if this is the level of precision you want, then that's tape. Okay, any questions about that? So this is the nested polyverse idea. Okay, the bound it gives is not as good. So this root-finding algorithm is very easy to understand. It's natural given what I described. It's not the MLE. The MLE does better. By the way, the MLE, basically, if you want to know what's the probability of a given root, you have to think about all of the possible increasing labelings of that tree starting from that root. Of that tree starting from that loop, because those are just the ways, the sort of possible execution paths of the animal gold tree algorithm. Okay, so you get some product over some tree sizes with automorphism factors, and you can start getting trade analysis. And the analysis, by the way, of the MLE in the Broad Librosi basically consists of throwing away the automorphism factors and hoping that you haven't put them away. That's That's because that was a little bit rushed. I didn't actually intend to say that, so don't worry about it. But I'll just say that MLE, they proved that MLE yields a sub-polynomial algorithm. So that is to say, you can take m to be epsilon to the minus little o1. And in fact, they show In fact, they show you can take M to be the MLA succeeds if M has the form constant times log 1 over epsilon divided by log log 1 over epsilon. Okay, so this, they beat, they beat polynomial by a little bit. Okay, so before I even get to Anything about our proof? I wanted to say a little bit about how tight this bound is, because we've also been working on lower bounds. Okay, so with this heuristic, that there's this flow from the root, and the cuts that result from that flow are balanced near the root in some sense. Or balance near the root, in some sense, and unbalanced further away, you might think that if you want to hide the root, to make the root an unlikely guess, a good thing to do is to make that flow just go away from the root. Don't let the mass split at the root. Send it all down one branch and hope that that hides the location of the flow. Okay, so a heuristic is to best hide the root. To best hide the root and put all flow down one branch. Okay, so maybe that means something like that the early part of the random representative tree process just builds some long path, and then all remaining nodes attach at the far end of that path. And by analyzing that, And by analyzing that kind of example, the same set of authors showed that for any algorithm, if you want to have the probability, you know, it suffices to look at the MLE, because it's the best algorithm. So for any algorithm, if you want to have the failure probability less than epsilon, then you need m to be at least exponential in. Exponential in root log one over epsilon. And so, with the other people in that picture, we showed that So, this is basically type, in fact, the lower bound. So, MLE succeeds when M is, so we lose a log log factor in the other direction. Okay. So this, so the lower bound that was proved by Evector-Boyle Grossius looks tight. I think it is tight. If you run this process, instead of the random recursive tree process, you do the same on a bounded degree tree, like an infinite free regular tree, for example, then we can get rid of this log log factor. So then the lower bound really is tight. There's some combinatorial terms we don't quite yet understand how to control and unbound. Yet, understand how to control an unbounded degree case. Yes. When you say the MLE and you're doing M guesses, you're just taking the M best. The M most likely. But is that necessarily optimal if you have M guesses? It's optimal for one guess. Couldn't there be some anti-concentrated strategy that's better? I don't think so. I think it's not too hard to see that the best guess conditional on the first guess. Guess conditional on the first guess failing is the second guess that all against the second guess. Okay, so in the remaining, I don't know, five or ten minutes, I'll just say a tiny bit about what we do for random graphs of given degrees. Of giving degrees. Okay, so we're going to construct G N, I mean, yeah, by the configuration model, which I expect most of you have seen, but I'll be a little bit precise because I want to actually couple that. To the SI model. Okay, so remember in the configuration model vertex so vertex I has um Has half edges attached to it. I1 up to I. The degree of node I was denoted DNI. The sequence was DN1 up to DN of some and. Okay, and then you choose a perfect matching of the half edges. Of the half edges and hope to get a connected path. And a sort of key point about the configuration model that's useful to us is that the way you choose this perfect matching is very flexible. You know, if I can do this in an online way where I start from whatever. I start from whatever half edge I like and then choose a uniformly random pair for it from all the remaining half edges. But I can make the choice of which half edge to explore in an online fashion as long as its mate is always chosen uniformly at random from what substitute. Okay, so then the coupling. So I'm always going to have, you know, after k steps, I'll have some. I'll have some partial graph with vertices V, edges E, and half edges H. So here HK is the unpaired half edges incident to the graph we've constructed so far. Okay, and then TNK will be the SI tree that we built, restricted to GNK. Okay, so we'll maintain that as we grow the graph, we grow the tree, and any time a new edge joins the Joins the currently constructed graph. Sorry, anytime a new vertex joins the currently constructed graph, that vertex will also join the tree via the new ISA. So rather than writing that in detail, let me just run a quick example. Let me just look at, you know, imagine that the degree sequence is all threes. We have a random three regular graph. So at 21, we have the root, it has three half edges incident to it, and then we have this big reservoir of other half edges. Big reservoir of other half-edges sitting down here. So we'll choose one of these half-edges with however we like. In fact, we'll always choose a uniformly random half-edge. And then we'll choose a matching half-edge down below and match them. Okay, so this edge found something new, so it gets added to both the graph and the tree. Okay, so the new graph. Okay, so the new graph looks like this, and let me draw tree edges in yellow. So this is our root. Now here we still have this big reservoir of nodes. And maybe the next path edge we choose at random, I guess I should have, to be consistent, my drawing should have been like that. Consistent, my drawing should have been more like that. Maybe the next half edge I choose is here and here. Okay. So now I end up with this picture up top, and both of these edges found something new, so they get a Found something new so they could add it to both the graph process and the tree process. So maybe the next half edge I choose is here, and then it's uniformly random matching edges within the graph. So then I'm starting to create the graph structure that's not part of the tree structure. I now have this picture. Right, so these two. Right, so these two edges are tree edges, this edge is not a tree edge. And I continue, right? So as long as I use the rule that the growth rule I use is I always choose a uniformly random available half-edge from the tree that I've currently constructed. Then the growth dynamics is exactly. Then the growth dynamics is exactly what I want. If that half edge goes to the remainder of the graph, then it was a uniformly random attachment point from among the eventual boundary half edges. So this coupling constructs a tree and the graph at the same time. Eventually, it has to, you know. Eventually, it has to, you know, if I had a random three regular graph, eventually a lot of the steps are not going to grow the tree because you have to create all of the cyclid structure. But for the first long while, this exploration will look quite tree-like because there are not many half-edges up here, and there are lots of half-edges there. So that is just about all I have. Not all I have time to say, but let me just conclude the proof sketch by saying that we analyze the coupling in three phases. So the first phase is the first, say, n to the two-fifth Two-fifth steps, and this is when the growth is tree-like. And what that means is that if you track the evolution of the mass splits in the growing graph, you again have a sort of poly-earn-type structure. It's not exactly the same as the one in the random recursive tree, but you do have this nice splitting of mass, which, if you are allowed to just stop before you've built any cycles, Before you've built any cycles, then you would have an easy time using the previous rivers, or the previous framework, let's say. Okay, then there's a second phase where you can ask how these mass splits change now as the first cycles start to occur. And because there aren't very many cycles, the growth of the subtrees stays quite deterministic. Stays quite deterministic. They're sort of, if I look at the subtrees close to the root, they have lots of half edges. The proportion of the time that you grow them by attaching down to the remainder of the graph behaves well. And so in this phase, the splits remain concentrated around n to the two-fifth values. Values. And then, you know, these values actually need, if you want to get sort of optimal moment conditions, you need to push this very close to root n and this just above root n. Okay, and then for the remainder, the final phase, when there's lots of cycles that start to form, you have to use the first. So this is done using sort of standard martingale concentration type stuff. But the martingales, you know, the probability that a sub-tree grows. The probability that a sub-tree grows is proportional to its number of available half-edges, and that number eventually is zero at the end of the process. So you want to use, in this phase we can use the fact that certain values are large that just stop being large by the end. So we need a second kind of argument, which is just a moment method argument to finish off. Okay, so I plan to finish with a bit of time to spare, but there's just one minute and I do want to say one more thing, which is that the quality of the estimator that you get is really determined by this early growth phase where these splits are formed. And so the error probabilities that come out actually are just determined by what they would be on the corresponding kind of limiting tree. Of limiting tree model. So everything else is just sort of, I mean, the bounds you get out of the remaining part are tending to zero as n as the size tends to infinity. Okay, and so if you're interested in this dependence of epsilon on m, this part all vanishes, and all of the key information comes from this first step. And that's also why I think the quench thing should be very different. Okay, that's it. Thanks very much for being here. That's it. Thanks very much for your questions. Are there any more questions or comments? So in this frequency