All right, I apologize. I was expecting chalkboard, but I don't have the space, so I'm going to make do with the Zoom whiteboard today. Yeah, so I'm going to be talking about a bit of work that's for a paper that Kwa, myself, and Qua's postdoc, JC Saunders, have been working on. And this is primarily a result that Qua has put together and I've kind of reviewed. And I'm summarizing and presenting to everybody here. And presenting to everybody here. And I am going to change my share just to kind of show you the theorem. Actually, I might just put it in the chat so you guys have it. If I can get it in there, because it's a mouthful and I don't want to write it out by hand. So if I can find it. All right. So that's just a statement of the theorem. Oh, you guys. Of the theorem. Oh, you guys can see all that. I wasn't sure about that. Okay. And just to get started, what we're doing is defining the Artin Maser Zeta function on what is the positive characteristic torus and just the definition of this function. So if f goes from x to x, topological spaces. X topological spaces to itself. And we're going to first define this nk of one function that's going to be part of the definition. So nk of f is going to be the number of fixed isolated points of this function f. Sorry, f to the k, I should say. So, like, you repeatedly apply this function f, and then we count the number of fixed, specifically isolated points. So, we don't want like fixed subspaces or anything really. Then the Artin major zeta function is going to be zeta of f of z is e to the, and then this. E to the and then this huge sum k equals one to infinity. You count the number of fixed points and then divide that by that k, whatever iteration we're at, and then z to the k. So essentially, this is the function that we're going to be looking at and kind of figuring this out for this positive characteristic torus, which I haven't quite defined yet. So we do have a classical, not totally classical, but a result from Classical, but a result from 2010. So this is back, Lau, and Pascunus. Basically, on the real torus, so F is an endomorphism. And so the real torus, just if you're not totally familiar with that, T is just R modded by Z. R modded by Z, so it's sort of just like the decimal part of your real numbers. Then F is rational. Sorry, is that Zeta? Yeah, that's a typo. Yeah, so then your Zeta function is actually, it turns out it reduces to a rational function. Very nice. And then there was a conjecture in. A conjecture in 2014. So that's Bell Eiles Ward. Essentially saying that as long as we have this automorphism, I maybe won't write this out. So on a compact metric abelian group. And so something I'm kind of missed in pointing out here, we do need that for this to be defined, all of these should be finite. Doesn't really work otherwise. Otherwise. But yeah, so if we have this property that the number of fixed isolated points is finite for all k, then what the result would basically be is the zeta function. So theta, again, that's compact metric abelian group. Then the zeta function is either going to be rational or Or admits a natural boundary. And what we've found, or what Faceliquaz found here, is that that doesn't seem to be the case, especially for this positive characteristic torus. So, all right, first, next thing I'm going to do is just kind of define that positive characteristic Taurus. Characteristic Taurus. So we build up the integers first of all. So F is going to be basically any finite field. We would just say the size of F is going to be Q, so it's going to have characteristics, some key. Characteristic, some p, q should be like, I don't know, p to the k for some prime p. And so the integers are basically polynomials with coefficients in f and the rationals naturally are just going to be polynomials over polynomials. Sorry, that's R, R for rational. Sorry, the reals are going to actually. Are going to actually be the completion of these, or sorry, the what we're going to get is this set here. So sort of like Laurent series with the one over t expression. And so the integers are basically, yeah, polynomials. The rationals, what they'll look like, kind of like our rationals in decimals. Rationals in decimals is it'll have these chunks with like one over T's coming in, but you'll get like repeated patterns, just like you'll get with decimals of rationals. And then like the reals of the decimals are kind of just any decimals, like strings of decimals or no patterns. And same idea with the positive or the positive characteristic real numbers. You just get these strings of one over t's with no defined pattern. So it just is kind of any set that looks like Is kind of any set that looks like sums. K equals one to infinity. Sorry, I'm reading the wrong sum there, even though I should know what this looks like. So for some upper bound M, we'd have a AI to the TI such that the I's are in. The integers and the AIs are your coefficients in that finite field. So they kind of look like decimals. They're not really quite the same thing. And then we also are going to define a norm or like a distance metric on it. So we get a valuation. So ZF is not the valuation rating of time. Evaluation rate of language and RS, right? Uh, I don't work as much with the like, yeah, okay, yeah, so where was I? Uh, so yes, RF. So we get this valuation that's kind of just sort of like the highest power of your expression, so but the negatives. So v of zero is going to be infinity, and otherwise v of And otherwise, v of any of these sums is going to be negative m, provided that that lead coefficient is not zero. And then that's going to give us a norm or the absolute value. Sorry. Yeah, you're right. Yeah. And so then the absolute value of any x is just going to be q to the negative of that power. So even that kind of looks like the norm that we would find on like our decimal representations, but that's basically the limit as far as how these numbers behave like our integers and real numbers, primarily because it's non-Archimedean. And so we get all sorts of problems when we try to actually add things together and do math on it. Add things together and do math on things. Okay, so that is the buildup of the reels. And then naturally, the torus is going to be defined essentially the same way. So the positive characteristic torus is just the reels modded by the integers. And so we can just think of that as those strings of powers of t. That is those strings of powers of t where it's like one over t, one over t squared, etc., etc., with coefficients in f. So, again, it kind of just looks like the decimal part of the terms. So, getting back to the actual function that we're working with, we have this nk of a that we're trying to define, and this was in that Art and Mazer-Zader function, and so we need. Maser-Zader function, and so we need to actually find a way to count this. But there is already a known result for this positive characteristic torus, which I've got written right here. It's actually really simple and clean. So for any B in the D by D matrices with coefficients in Zf, and these are going to be our surjective endomorphisms for the finite characteristic torus. Basically, what we get is that What we get is that the fixed points of just applying B once is actually just the determinant of B minus the identity matrix. So you can find that result, I think, in a few places. But basically, it's kind of interesting in that if the determinant is, yeah, so if like lambda equals one is an eigenvalue, you would get no fixed point. So I say eigenvalue. Would get no fixed points. So I say isolated fixed points because you actually get like a space. So it's not quite isolated. But otherwise, this does work out. And then that means that for us, n k of b is just going to be the determinant of b to the k minus i. That looks really bad. I'll fix that. So this is going to give us a nice way to count this. It does mean that we're just looking at the products of the eigenvalues. Looking at the products of the eigenvalues of lambda to the k minus one, essentially. So, this is going to be yeah product. And I guess there will be, if we're in D, D And sorry, I need my N in there. So, yeah, so we just kind of take the product of those eigenvalues. Of those eigenvalues. And so, really, what we're all that we're going to be looking at here, I do need the absolute value on that. Because we're working in these numbers where it's like polynomials and the size of these polynomials are based on the degree, we're really only super interested, or the only things that kind of pose a challenge to us for figuring this out is if the degree or like the size of these lambdas. Or, like, the size of these lambdas, if that highest power is exactly one. Because if the highest power is bigger than one, then this is just actually equal to this, right? It's going to be the same thing. But if the highest power is less than one, you would actually get one, because then you subtract one, and now we have, you know, that just lead term as a coefficient. But if the highest power of the lambda is exactly one, Power of the lambda is exactly one, so that means that if the lambda equals one, this is going to create problems because subtracting one from it may or may not vanish that lead constant term. So it becomes a little bit dicey trying to figure out what the size of this thing is. And that's kind of where all of these calculations are coming in. All right, so how we're going to basically split up any of these lambdas into the part that is exactly a constant. So this is kind of what we're going to do for every lambda or every element. So if any element has size. Any element has size one, and this is going to be in k, some finite extension of r sub s. So that's kind of where our eigenvalues are all going to lie. What we should be able to do is deconstruct them into these two parts. So this part here is going to be either have size one. Size one, or it's equal to zero. Sorry, not size one. It's either a constant, I should say, or a root of unity. And then these parts here are going to have size less than one. So there's really two ways that we can get to essentially this result: that this is a thing that we can do and we can uniquely do. In the paper that we're writing, Kwa has kind of done it by introducing, I think, I don't know how to pronounce this, Tuk Mueller representatives. Does that sound right? What's that? Yeah, the take-mueller representatives and kind of using maybe heavier algebra than I'm usually familiar with, but it totally works out. I've actually done a little bit of work in the rest of my dissertation on looking at In the rest of my dissertation, on looking at the representations of algebraic components, and I'll just kind of jot down what they actually look like. So, if you have an algebraic element, it actually ends up looking like I is in some particular set S. S has these unique kind of properties that are not quite the same as the polynomials we're looking at. But then we're getting, so we get something like this. So we get something like this, where the AIs live in a finite extension of your field F. And that F set, it kind of looks like elements of the form. It'll be something like A, maybe I'll use B because A's are used. B over N to the P K where the P is that. Where the P is that we already got P, N is going to be the degree of the minimal polynomial of the element. And so there's a few other restrictions on there, but as you can see, it's not the same structure at all because those denominators can get infinitely small. So it's not quite like a Laurent series of any form at all. Of any form at all. And yes, it's a little bit more chaotic, but you kind of get the same result coming from this, and that we can write this out as a sort of series. And then we should be able to get the same absolute value on this, where you just sort of take the highest power of t that shows up. And there is another property of this s. I said there's a few other restrictions. S always has a maximal element. Uh, S always has a maximal element. Just there's other restrictions that really aren't that necessary for putting this piece together. But that does allow us to essentially, I should, not just S, but every subset of S. Specifically, but yeah, so that means that even if like our lead coefficient is zero, we just get some subset, it'll always have a max element. So we could define. It'll always have a max element. So, we could define this absolute value very similarly. You just take the highest degree that shows up, and it would essentially give you that same kind of Q to the valuation, Q to whatever that number is. So essentially, we get these pieces that if we're kind of looking at this deconstruction, if this thing, if our lambda that comes up from this has an absolute value of one, that means it starts with a constant. One, that means it starts with a constant term that comes from some extension, a finite extension of f, and so that constant term is going to be essentially this piece here, and then the rest of it is trailers with negative powers of t, and then they'll all come there. And there's only one way to deconstruct it that way if you view it as that series. So, quas demonstration doesn't go into that breakdown at all, it relies a little bit more. Down at all, it relies a little bit more algebraically. My explanation kind of loses a lot of that depth, but I think it's easier to see. So, but yeah, so essentially that's how we're going to figure out what those products are going to be, because we'll basically just analyze what happens to that root of unity part and figuring out what happens on that, especially when you view things as these series, when you take it to. View things as these series when you take it to powers. The only part that will ever, ever contribute, if you take this to any power to that constant term is when this is zero. All the other ones are just going to keep getting pushed further down the series. All right. So, something else that's missing from the theorem that I gave you guys is what order modulo P means. So, it has something to do with. So, it has something to do with this breakdown. So, I'm going to write that again. So, for any alpha is alpha 0 plus alpha 1. So, I do the order modulo p and basically in the multiplicative group. I know there's probably a better notation for this, but it's just the multiplicative group of, yeah, so this is just the roots of unity along with zero. So in that multiplicative group, so the order modulo p of any element in this extension is going to be just the order of Of the constant part, the alpha sub zero. So that's the part that is actually just a root of unity or zero itself. And again, so that's going to be really important for us because if we take this alpha or any of our eigenvalues to the power of some k, that means it's going to become one, or this piece here is going to become one. And then when we subtract one, that constant part vanishes, and you're just left with trailers in that series. And so its absolute value is going to be less than one at that point. So yeah, so that was part of that big theorem, which I'm going to. That big theorem, which I'm going to pull up again, and it's also important to note that this is the smallest positive integer n so that essentially we get this alpha to the n minus one is less than one. And again, because it just eliminates the constant part, and all you're left with is smaller powers. So. So I wanted to switch over. Where is my yeah, so I wanted to get that statement of the theorem here. Yeah, so I did put that in the chat, but this is the theorem and it's quite a lot to write out, so I'm just going to show it here. But basically now, if we take any A, so like this matrix multiplication for elements in our positive characteristic torus. Positive characteristic torus. So we're going to define this R of A is going to be the max of one or the absolute value of the eigenvalue. And so for this, yeah, this here in the other part of the paper that we're working on, JC had actually shown that this is the entropy of any of these maps. So that's a lot more analytical than all of this, but it's kind of just a nice result that the two things kind of tie together. Result that the two things kind of tie together. So, the product across all of the eigenvalues of either one or the absolute value of the eigenvalue. And again, that's what we're interested in. If it's a big eigenvalue, we don't care about subtracting one. So, we're going to split up the eigenvalues into mus. So, these are going to be the ones whose eigenvalues are exactly roots of unity. Exactly, roots of unity. And so that means that like we don't have those little trailer chunks. So in the breakdown of like the bracket zero and the bracket one, the bracket one is nothing. There's no less than part. And then the etas are going to be ones that have absolute value one. So they start with a constant term, but they're not roots of unity. So they have like trailer, like little other chunks in their series. And And yeah, so the Mi's are going to denote the orders of the mu's modulo that P. So again, that means that the power that we have to take the constant term to get to one. And then similarly, the N's are going to denote the order of the etas modulo p. So like we only care about those constants. And this is going to break down into really two cases. So the first case is that essentially every n. That essentially every n has an n that goes into it. And if that's the case, it's going to be actually fairly easy to compute this product. This kind of looks a lot worse than it is. But what we end up getting is one minus that R of A, Z to the negative one. And then this double product, basically, it just kind of runs through all of the different sets of the I's, all of the different sets of the things one through M. Through M. And then plus or, yeah. And then R. So this capital R, which is defined down here. And it's just, yeah, this kind of messy function here with a lot of least common multiples. It looks worse than it is. But otherwise, if we don't have that property that all n's have an m going into it, then this series is actually, it's going to converge in the open. It's going to converge in the open disk that's bounded by one over that R of A against entropy. And actually, the function is going to be transcendental. So what this is actually going to allow us to do is generate a counterexample to this idea that all of these functions should be essentially rational. Because as we can see here, the denominator of the powers of things could potentially be roots or sorry, be one over something. So we could get like algebraic. Something so we could get like algebraic expressions that are non-rational. So I can do just kind of a quick example of that. So I'm going to switch over to So just as an example, let's say we are in our function will just be our sorry field is F7. Our map is going to be 6003. 03. Oh, do we have a question? Yeah, I was asking if case B can happen in the radius of convergence one case. Yeah, that would be, I guess, if either all eigenvalues are less than or equal to one. Oh, yeah, that's the circle. That's the circle. Sorry, yeah, I was thinking. Yeah, okay, thank you. Sorry. That's all right. Yeah, so just a really simple example of how. Yeah, so just a really simple example of how this kind of breaks that conjecture. So we'll set up this. So, I mean, the eigenvalues of this are obviously six and three. So, what we end up getting is just two mus. So, there's no etas. There's no anything with that. So, we've got, yeah, mu1 is six, mu2 is three. The absolute value of both is exactly one. And so, this is going to give me mu one is six, mu two is. Mu2 is two. Wait, what did I do here? Oh, yeah. So I think that's right. Maybe I've made a typo there. And then the powers. So this will be M1 is two. And this will be M2 is three. Is that the right example, Ko? I think I might have a typo. Because I had my M's where. Yeah, so I think what I wanted was, oh no, I think it's three and three, right? No. What's the element of order three that I need here? Five? Two, two. Yeah. Two also short of three. Okay, there we go. Then that works. Yeah, yeah, two cubed is yes. Okay. Yeah, so what we're gonna get is this Zeta A. This zeta a of z is going to be so, yeah, our r of a is the product of the max of either one time or the absolute value of the eigenvalues, which are all just one. So I think, yeah, this might actually be exactly that case. R of a is going to be one. So we end up getting one minus z to the negative one. And then so the product, if we go back to that theorem, essentially you just kind of take all. Theorem essentially you just kind of take all of the groupings of it. So all of the one element groups would be the just lambda one, just lambda two. So we end up getting one minus z squared to the half, and then one minus z cubed to the third. And then on the bottom, you divide by the number of sets with even elements. But yeah, it basically breaks down to this if you plug the things into the function. And this is not rational, it's going to be algebraic. You can obviously generate lots and lots of these. And I think probably almost everything you choose will generate something kind of like this, as long as you have that all of the ends have some m that goes into it. And by default, this does. So essentially, if you have anything, everything has a. Everything has roots of unity eigenvalues. You'll probably, unless somehow all of the powers reconcile themselves, you'll somehow get some like algebraic expression here that's not rational. So yeah, it's kind of a simple example. I just want to spend the rest of the time going through kind of an overview of the proof of the theorem. The case where the n's have the m's that go into it is not that. Is not that complicated. You sort of just kind of see what happens to those lambda minus ones, and they're actually really easy to handle in this case. So I'm going to grab that theorem, which is, yeah, so part one, part A, I can't remember what it was. So yeah, we had R of A is the product of the maxes. So that's again over all of the eigenvalues, repetition if necessary. So we get just basically this result that the number of fixed points. So remember, it was the determinant of a to the k minus the identity. And that's entirely determined by those eigenvalues. Entirely determined by those eigenvalues, so we end up getting these products where we just sort of break it down into either the ADA or the mu cases. And so the A's are going to be defined by either zero. By either zero if the MI goes into the K or one otherwise. And so the A's are going to be coming from those mu's. Remember, the mu's were the eigenvalues that were strictly constant. So if the m goes into that particular k, then lambda to the m, or sorry, lambda to the k is going to be zero. So sorry, lambda to the k is going to be one, which means. Lambda to the k is going to be one, which means that lambda to the k minus one is going to be zero. It means that this whole determinant there is going to wipe out. Otherwise, it's going to be some non-zero constant. And so that's why we're going to get a one for the value of that eigenvalue, for the absolute value of that eigenvalue. And then the B's are going to be the parts that had the trailers, essentially. So for the Bs, we are going to either have So, in this case, when we subtract one from it, we'll never get exactly zero when we subtract one because there's those little trailers. But if that constant term of these eigenvalues, if its degree goes into the k, then the Then the constant term is going to wipe out, and you're going to be left with essentially the trailer. And the Frobenius map is going to raise it to the power of basically some power of P, depending on how many P's are in that K. So that's going to just sort of push things further down in that rest of that series, which kind of makes sense. If that thing doesn't go into the K, so yeah, if the degree doesn't go into K, then you're actually going to be left with a constant term. And so the Be left with a constant term, and so the absolute value of these ones is going to be a one. So that's where these numbers are kind of coming together. And basically, the proof of the first part is that if every n has an m that goes into it, then anytime this happens, this will also happen, which means that this doesn't matter pretty much for that scenario. Pretty much for that scenario. And essentially, it dictates that this value entirely comes from the A's. So we really, really only care if the M's go into K or not because, yeah, essentially that property there. So it makes it really easy to calculate because we only have to keep track of the A's. And so just kind of the rest of the proof is really just going to be rewriting that sum. We end up getting. We end up getting so one minus RA Z to the negative one. Sorry, no, I'm just rewriting the statement here. I'm a little bit unorganized. Yeah, so we go. Yeah, so we get the sum of k greater than one of nk a over k z to the k. So, yeah, at this point, we actually only only care about that scenario of what's going on on the left if the mi's go into k. So it just kind of breaks down to k is greater than or equal to one, and m i we do not want to go into k, otherwise it vanishes for any of Any of the i's because if any of the m's do that, the whole thing's just going to vanish, and then we can turn it into the r a part. Yeah, so just the r a part in this expression. Oh, I think I'm missing that. Sorry, yeah, I'm missing something here. The RA part is the chunk that you get from the eigenvalues that are That are shock, I would just rewrite this. So yeah, sorry, what I've written here actually only has the eigenvalues with absolute value one. So what I was missing was RA in the expression to the k. Yeah. So that the RA part is gonna be all those other eigenvalues that we hadn't really considered, the ones whose size are bigger than one. The ones whose size are bigger than one, they're also obviously going to contribute to it. And so, just to kind of get back to this, all we would have is the RA. Everything else just kind of goes to one in this. So we really only care about those RAs. And so we get this expression here. And then basically what we do with this, I'm not going to go through the math, but it's just a bunch of inclusion-exclusion principles on this property here. And then you get just all those sums. And then you get just all those sums in and out, in and out. You end up generating a bunch of log functions through these sums, and then the logs wipe out with the e, and then it just kind of fixes itself. So it's, yeah, it's kind of just messy, but yeah, you end up with a bunch of logs. It all works out. It's not that bad. The other side of it, though, part B, certainly gets a lot more complicated and it needs a little bit more algebra behind it. Behind it. Yeah, so before going into this, I'm going to have to kind of go through a little bit of a side step. So we're going to look at something called D finite series, which I have got right over the page. Okay, so I'm going to introduce something called a D finite series. Something called a D finite series. And so we have F of Z in C of Z is D finite if there exists N greater than or equal to zero and polynomials say A naught through Say A naught through an in C of Z, so the lead one is not zero so that basically this equation of the derivatives is identically going to be zero. So another way of viewing it is the derivatives cover a finite. Derivatives cover a finite dimensional space. So, we're going to kind of use this property because we're going to use this property of definite spaces to essentially show that this thing that we had is actually algebraic. And so, a nice result that we get is that if something is algebraic, Then it's also definite. Kind of just the explanation of this is that if you just if it is algebraic, that means that we can kind of write some equation P0 plus P1F plus PDFD equals zero. And then really just how it goes is you take the And then, really, just how it goes is you take the derivative of this, and that allows you to write f prime as a rational expression of all of these f's in these polynomials, p. And then inductively, you can essentially show that f prime, f prime prime, all the way through to the dth derivative of f are all rational expressions of f in all of these polynomials. And then that kind of allows you to set up a Of allows you to set up a rational expression in this, but this tells you that they are linearly dependent, and essentially it just kind of shows that like you can make it zero somehow by just kind of reconciling the denominators and going from there. So that's not too hard. Then there's another proposition from that is that if our function, specifically, if we can. Our function specifically, if we can write it as a series, like what we're about to do with these expressions here, is if f of z equals, so this is a series that we're going to generate, c i z i is d finite, then there's some s greater than zero and more polynomials. Going to And I'll just use maybe Q so that we can generate that this really nice equation. We plug in the K Z K minus one. Yeah, so that what we're going to be able to do is actually set this up with this value of k that kind of corresponds to those coefficients. Basically, the result of this is to just take that d finite equation. Just take that definite equation and take derivatives on it. And then when we take those derivatives of this, yeah, so you just take the series and actually like apply those derivatives and you just sort of match coefficients. Sorry, you match like powers of the Z that come out of this and it just kind of generates these coefficients for each of those powers of Z. Coefficients for each of those powers of z and uh k oh i think uh uh any k actually uh large enough k so yeah so for all like large enough k and basically k just has to be big enough so that when you take those derivatives some like none of some of them don't wipe out so it's it's basically like i think n I think, and in that case. But that's going to allow us to set up this equation that we're going to use and essentially do a p-adic valuation on the equation. And it's going to actually allow us to really control everything. So I think I've got like 10 minutes left. So that is right there. All right. So the equation that we're going to do is. Great, so the equation that we're going to work with essentially is going to be fixing those CKs to be something very manageable. So, what we're going to do is assume that our zeta function is algebraic. And remember, this is the case where there is some n that has no m's that go into it, so we'll just say like n1. Doesn't really matter which one it is, so we'll just say that. We're going to assume the zeta is algebraic. That means then that f of z, which is going to be zeta divided by the derivative and then times z. And what we are actually going to get out of that is the sum the derivative divided by The derivative divided by, oh, yeah, you're right. Yes. Just miscopied. So this is going to be the sum from i equals 1 to infinity of the nk. So that means that this function here is also algebraic. And so again, that's just like going back to that exponential function and taking the derivative. And then that means that f of z over ra, which is going to be Which is going to be the sum, and this is where we're going to get those C's. And that's going to, each C now is going to be NK over RA to the K. And if we go back to the way that formula was built, this is actually just going to be the products of the A's and the B's. And then to the P V P of K. So it's just going to be those A's and B's. And so essentially what we're going to show is that this thing can't be algebraic because if this is algebraic, then it must be definite by the results right before. And that's going to be a problem. And it's going to be that long particular equation that's going to kind of mess this up. So, what we're going to do is let Q be the product of the etas. And let Q1 be just the product of the etas that are actually going to show up in here. So this is the one. And so, yeah, so these are the Adas that. So, these are the ADAs that essentially are going to show up in this product over here. And the other Qs are like all of the Adas. So Q1 is obviously a little bit smaller, potentially smaller than Q, maybe not. So again, you can show this using algebraic machinery. But since, remember, these ADAs are like the trailers when we take away the constant part, what we do. Away the constant part, what we do have is that if we sort of define that extension of the norm, is that the absolute value on all of these are going to be powers of p to the something like this for all of these. Is that right? Yeah. Yeah, so all of those values because. Yeah, so all of those values because that seem right kind of feel like they might yeah yeah yeah that should be good. And what's that? Oh, so our um r specifically it's some r so some integer r. If you use all the algebraic reasoning, r would be like the degree of the minimal polynomial or it would be the inertia degree if you're looking at the extension that If you're looking at the extension that contains all of the eigenvalues, yeah, so it would be the inertia degree of that over this. But they're going to be kind of powers of this thing. This is going to allow us to control the p-adic valuation on these Q's, which is going to allow us to kind of control the p-adics on the C's, which is going to be kind of nice. So, what we're going to do is let Let yeah, and particularly what we're going to get out of this is that the p-adic evaluation of Q is going to be bigger than one, same with the Q1 as well. So because of those results, all right. So what we're also going to get is that if I look at the C's, so I'm going to shrink this down a little bit. Shrink this down a little bit. Is that every CK is, if I look at the p-adic valuation of it, it is going to be bounded by the Q's, particularly to this expression here, because remember, the CKs are kind of. Expression here because remember, the CKs are kind of just some product that has something to do with it, but it may or may not have all of those Qs in there. So the CKs will have less of those ADAs than the Q does. So it gives us a nice little bound. I think I kind of got to wrap up. So I'll just kind of give an overview of what we do with the rest of it. What we're going to do is look at this equation. So essentially, we're choosing K equals N1PL in that. P L in that result that we had from the D finite equations. Yeah, so plus dot dot dot. Okay, so we're going to basically look at this particular equation here. So the result that we had before. So the result that we had before was that for k large enough, this will always be true for some fixed set of polynomials p naught through ps. So we can fix those and then we can actually just kind of choose l large enough to sort of get any results that we want from this. So this CN1PL, what we're going to have here is that the piadic norm of this Piadic norm of this, sorry, the piadic valuation of this is going to be q1 to the PL. So those are going to actually just be the exact same thing. I think I need the P on that. Does that look right? Yeah. And the other ones are all going to be bounded by the cues. And then all those polynomials as well, all of those polynomials are going to essentially be just determined by the constant term at the end when we plug in these expressions here. So these are also going to be bounded when we do the p-adic norms. And so what it Uh, and so what it does kind of give us is that this here is going to be bounded. And since this can get exponentially like doubly exponentially large, what that means is that we can make this piece here exponentially small. And so that, what we're going to do is use that to kind of control the product equation. The product equation. So the product equation is going to be essentially over all of the places. So for any x that's non-zero, we have this equation that if we take all of the evaluations, like all of the norms of x over all possible places, the product should be equal to one. So what we're actually going to do now is look at Actually, going to do now is look at all of them for x equals exactly that polynomial chunk. And there's only finitely many infinite places for these maps. And all of those are going to basically be defined by pretty much something to do with the P naught or the degree. Something to do with the P naught, the degree of the P naught. So they're going to be bounded by some constant multiple of the degree of the P naught when we set that in. So those are going to be bounded specifically here. So something like this. So not that, it's a constant multiple of that, but in particular, it's polynomial growth. Polynomial growth. On the other hand, all of the infinite, or sorry, all of the finite places, we can also bound those because it's some polynomial. So it's just really going to depend on the denominators of the coefficients. So we can kind of create some kind of fixed bound for all of those. And so when we do this product, basically, This product, basically, the only thing that's really going to mitigate how small this is getting in that product is these polynomial growth infinite places. But on the other hand, this one can get exponentially small. So as we just take L larger and larger, it will basically just make this so small that it's going to override this eventually at some point. And then that will essentially say that this thing can be close to zero as we want. So it cannot possibly be one. So it's going to contradict that product equation. And that means that this equation can't be true. That means that this thing could not have been definite, which means back, back, back, back, back, the original expression was not algebraic. So it's transcendental. So that's basically the proof. Sorry.