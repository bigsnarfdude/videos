Thank you. First off, let me thank the organizers for the invitation, but also for putting this great workshop together. It's my favorite kind of workshop where I learn about things I am interested in. I didn't know I was interested in. I'm leaving with lots of new ideas. And hopefully, I'll leave some of you with that too. So, I'm going to talk today about the idea, it's very similar to Sirkin's talk in many ways. It's the idea that Circans talk in many ways. It's the idea of using randomness as a way to probe primarily our folklore and conjectures and things that we have when thinking about algebraic objects we care about. And so I'm going to talk about it in the context of syzygies primarily. Anything original in this talk is joint with Jay Yang and Caitlin Boons, who's a Caitlin is a PhD student of mine who's finishing this year. Okay. Okay. Which way? Okay. It's counterintuitive, which is going to be. So before we get to Syzygues, I want to start with, I don't know, an inspirational idea of a way in which randomness can help us think about problems and where I think it's maybe helpful. So the twin prime conjecture, it's an old conjecture, and it's something that I don't know, I believe, and I think maybe everyone in the audience believes, maybe not, but many people do. And you ask yourself, why do you believe it? Why do we believe it? He asks yourself, why do you believe it? Why do we believe in the twin prime conjecture? And there's kind of two reasons I do. I mean, one is you can look at the data, and I only went up to 10,000, but if you count the number of twin primes, it kind of keeps going up. And they've tested it to really preposterously high numbers, and it seems to be growing. But there's a second reason to believe it, which I think is worth exploring. It's going to be related to what I'm getting at and very related to the theme here. At and very related to the theme here, which is you can build a random model that predicts it. And so you need a kind of model for how to predict how many primes there are. And so the prime number theorem tells you that for a big n, there are roughly n divided by log n many primes, smaller than big n. And so you can say, and this really put this in quotes for this to make any sense, that the probability that an integer between 0 and big n is prime is kind of roughly. When big n is prime is kind of roughly 1 over log n. And of course, that makes no sense at all, right? I mean, an integer is prime or it's not. There's no coin flip involved, right? It's just, you know, so it's very obviously a kind of heuristic or whatever. But that doesn't mean it's not useful. And if you take that as your heuristic, you can start making guesses about the number of twin primes of a certain size. And that leads to the conjecture that there should be infinitely. The conjecture that there should be infinitely many of them. And it's an especially interesting model because it not only predicts finiteness versus infiniteness, it even gives you a kind of quantitative estimate. And this estimate here is a little wrong. There's some local factors, arithmetic factors you have to take into account to adjust to make it a little, you know, you have to be a little more rigorous if you want to get a good quantitative estimate. But I find this random model, even though it's founded on something that's kind of not literally true, I find That's kind of not literally true. I find that at least as convincing as the data, or maybe in combination. That's where it becomes especially convincing. But as a thought experiment, I think it's interesting to think about it's a variant of the twin prime conjecture where data would be impossible, right? So how many, do we expect primes where p and p plus, so a preposterously large integer are both prime. And again, there are conjectures to this effect that there should be, this set should be infinite. Should be infinite, and what the density should be, and so on. And when we get to something like here, where we're utterly unable to compute data, random models versus the data, I mean, the random models really become the foundation of building, at least for me, building that folklore, that intuition, and those sorts of conjectures. So, all right, I mean, nothing I say today will be as interesting as the twin prime conjecture. But the idea, I think, is going to be at the heart of what I'll try to get at. Of what I'll try to get at. It's this idea of how are we going to use random models to gain insight into areas where computational data is unavailable. So, I don't know why it's. In quotes, because if you uniformly select the integer in that interval, it seems okay to say the probability is graph 57, you know the answer. Yeah, but it's for an individual. I mean, it's not a probabilistic thing, right? For a single number. So, I guess. It has to do with independence. Where the heuristic. Exactly. That's where I think when you use the second statement with saying the fares, you're treating them independent when they're massive. But even talking about it as a random variable is. Well, I don't know if you can. Well, you can use sampling of the variable. Okay, okay, okay, that's fair. So maybe it's the end of the team. Regardless, okay. All right. So the area I want to explore is syzygies. That's an area I've been very interested in. I'll give a kind of quick summary. I'll give a kind of quick summary. The idea is: we have an algebraic variety sitting inside of a projective space, and it's defined by an ideal. And the first syzygies are just the generators of that ideal. The second syzygies are the relations among those generators, and the third syzygies are the relations among the relations, and so on. So, you can phrase this many ways. You can phrase it as tour groups, or you can phrase it in terms of a minimal free resolution. Or you can phrase it in terms of a minimal free resolution. It's not, you know, if you know what these things are, I think many, many in the audience do, then I don't want to dwell on it too much. If you've never heard of syzygies before, I'm not sure in 60 seconds I'm going to give you much more. Other than the point is, from an algebraic variety, they're invariants that people care about. And I'm going to encode them as follows. Beta ij is going to be, we count the number of syzygies and their degrees, and then we. Degrees, and then we put them all into this collection, this table. So we have at the end of the day is a finite collection of integers associated to an algebraic variety, and we want to understand the properties of those integers. They show up, they capture geometry in various ways. All right, I guess, so why study syzygies? Just very quickly, they show up in lots of areas. I mean, what's a good picture? So, primarily for me, they show up in algebraic geometry. They capture, you know, famously through work of Mark Green and Green Lazarusfeld and many others, they capture invariants of algebraic varieties. And then you can use syzygies to capture really subtle geometry. But many in the audience here have used them in other contexts. I mean, they're used to measure complexity of Roger basis algorithms. So they show up, they're very related to computational algorithms. They're very related to computational algebra. In some sense, Macaulay 2's core function, the original Macaulay's core function, originally was computing syzygies. That was kind of what it was built to do. They show up in combinatorics. If you study Stanley Reisner ideal associated to simplicial complex, you can get kind of combinatorial invariance from that. I was just talking with someone the other day who was explaining how they show up in algebraic topology, in the study of certain variants. In the study of certain variation of certain mapping class groups, they show up in a lot of places. I mean, they're kind of a fundamental algebraic thing. You can associate to a set of polynomials. Anywhere polynomials show up, there's potential that syzygies will be irrelevant to them. Alright, so the next thing to know about syzygies is that they get complicated really, really fast. So let's do the kind of basic example. So let's do the kind of basic example. Imagine my initial variety is projective space of some dimension, and I'm going to take the kind of simplest embedding. I'm going to embed it by the default Veronese. That's kind of the classic one variety and another. How much simpler can you get? But if we start asking which Veronese embeddy tables do we actually know? Where do we know the syzygies? This is kind of a disheartening chart. So if you take P2 and you embed by the D. And you embed by the default Veronese, we know it up to about d equals 6. And even 5 and 6 were only kind of recently computed. And I guess only in characteristic 0, I have to be a little careful. Yeah. Even there, we don't have a proof of it. I think we just have numerical data. So there's even some potential for error. But we know it up to about d equals 6. Past that, it goes haywire. And you can see the number of variables is getting... You can see the number of variables is getting big. If you go to P3, things are even worse. We don't know anything, you know. I don't know how well this is showing up. There's little X's on the ones we don't know. So we don't know anything past D equals 4. And I didn't check, but I think if you go to P4 or P5, we don't know anything past D equals 2. I mean, even D equals 3 is probably, I don't know, maybe we know it, but certainly it peters out very quickly. The number of variables gets massive, and it just, the computation becomes. And it just, the computation becomes unmanageable. So, this is an area where there's no computational data. We're not going to be able to get good computational data for large-degree varinases. And so Aynan Lazarsfeld had this idea, which was maybe we should study the asymptotics, right? This is a situation where we probably aren't going to be able to get absolutely perfect information, but maybe we can nevertheless ask questions. Maybe we can nevertheless ask questions about the asymptotics. So, their setup was: they took a smooth algebraic variety and embedded it by kind of start with a very ample or an ample divisor, and then you scale that by D, and you get more and more embeddings. And you're kind of letting D go to infinity and asking, maybe we can't figure out all the details about this Betty table, but maybe we can kind of get asymptotic overarching behavior. And they raised this question around 2006. And they raised this question around 2007 and proved a few beautiful theorems and proposed some conjectures. But so, you know, it's kind of we're looking at large-scale behavior as opposed to understanding the specifics. And even that's hard, but there was some progress there. Their questions set off an array of work. I kind of summarized it down here. Let me see, what are some interesting ones? I mean, in some ways, their question was similar to what Mark Green asked. Was similar to what Mark Green asked in 84 about curves when this whole thing launched. He studied smooth curves under these sorts of embeddings. So that's kind of a proto-result. Let's see, there's just an array of different results here. There was this Konka, Junka, Kabitska, and Welker did it analyze a kind of combinatorial analog of their setup, where instead of re-embedding by increasing the ample things, they took something combinatorial. They took a simplicial complex and subdivided and analyzed the Betty numbers of the Stanley-Reasoner thing under iterative subdivisions. And they proved theorems similar to what Ein Lazarusfeld proved. I don't know, let me see what else. Juliet's thesis was about a related question where instead of having an ample divisor, there was some semi-ampleness built in, so you kind of varied the geometry in some way. Me and Me and Juliet and Jay and a few other people did some attempts to generate new data, some sort of parallelized large-scale linear algebra to try to build up some of the data like on the previous slide. That was actually super cool. And then Park recently had this result. So Ain and Lazarsfeld proved a sort of asymptotic non-vanishing, roughly which numbers are non-zero, and Park recently proved their. And Park recently proved their conjecture about roughly which ones are zero. But there's an array of work on kind of large-scale structure of what's happening here. And I guess maybe I'll highlight some of the overarching questions. So in the area, in this asymptotic systides area. So there's kind of qualitative questions. This is what Ayn and Lazarsfeld started with, which are, can we say more or less which of the Betty numbers are non-zero? And that's basically between And that's basically between Ayn and Lazarsfeld and Park. We have a pretty good answer now for that qualitative question. For the other ones, we don't. So quantitatively, if we know it's not zero, can we say roughly how big the entries are? Can we say something about the function, you know, ij to the corresponding Betty number? Is it polynomial in i? You know, can we bound it? How does it behave? And so on. And so on. How does it depend on D? That tends to be kind of wide open. If you start plotting some of them, these are actual plots of Betty numbers as you vary D, you kind of see that some of the Betty numbers, there seems to be a sort of normal distribution looking thing going on. There is a conjecture to that effect. What's that? I haven't checked, probably. So, yeah, that's a good question. There's a problem with the tails, it's hard to tell. There's a problem with the tails, it's hard to tell. Yeah, yeah. I would bet so, though. Are they log concave? Oh, explicit generating functions? No. I mean, there's almost no data, right? It's really very little. So this is part of the Betty table of P2 for D equals 4, 5, and 6, I believe. Or maybe it's 3, 4, and 5. No, 4, 5, and 6, I think. So it's, we don't have much data, right? It's kind of P2 up to D equals 6. kind of P2 up to D equals 6. D is not very large there. All right, I mean there's kind of arithmetic questions. So this is one I'll come back to at the end, but it's known that the Betty numbers can depend on the characteristic, and we can ask about that. When should we expect that? Can we measure that? And then there's overarching structural questions. What's going to control the answers to this? Ein and Lazarsfeld's work, the only thing that mattered in their work was the dimension. That mattered in their work was the dimension. That was the only invariant that really mattered. But as you get to more refined questions, we somehow expect that other variants will come into play. And I put an example up here just to kind of fix how wild this can get. I mean, so let's embed P3 by the 100-uple embedding, and it's a big old projective space. And we don't have the answers to any of these on the knowns. So we kind of have a decent answer to this one. Decent answer to this one from Heinrich Lazarusfeld, but we don't know the actual answer. So we kind of know some that are non-zero and some that are zero, and that accounts for most of them, maybe 99%. But there's a twilight zone, to steal Serfin's phrase, where we don't know what's happening, even in these examples. We have no clue on a question like this, how big these numbers are, what digits it has. And we have no clue on these sort of arithmetic questions. Sort of arithmetic questions. For D equals 100, should we or should we not expect it to depend on the characteristic? How many primes will pop up? So we're going to get 100 is, it's only factors are two and five. Are those the only interesting ones? Is seven going to come into play? Who knows? These sorts of things are wide open. All right, let me pause a sec for questions. These are the mysteries. This is what I want to probe with a random model. But these sorts of overarching mysteries of how do these Betty numbers behave? Of how do these Betty numbers behave and things that I said. So, are there in this array, Ij array, I guess, recurrence relations saying how these different ones relate to each other? No, no, I mean, they're basically, it's what they are is they're core groups. So they're the homology of a certain complex. And, you know, it's just an omod image. There's no obvious reason why one is related to the next. Is related to the next. And there's examples, you can find all sorts of examples where the Betty numbers aren't even unimodal, but they go up and down. They can be kind of arbitrarily non-unimodal. And so the idea here is that somehow that goes away in the case where you study something simple like projective space and with incredible positivity. You know, d equals 100, all that stuff should kind of wash away and you should get predictability. But there's no. But there's no known examples of that. I mean, except for maybe P1. Sorry. Other questions? It's a good question. For the primes, you're hoping for a finite list for changes? Yeah, yeah. I mean, my hope would be a finite, or maybe just, you know, roughly how many. Is there at least one? Can we kind of, like, like with the twin prime thing, could we guess? Could we say for d equals 100, there's roughly log of 100 primes where it breaks? I mean, that would be my dream. Where it breaks, right? I mean, that would be my dream. I guess you're asking, like, which primes will divide the entries of the various members? Yes, but yeah, yeah. The maximum minus how many of them will be, it'll be some of them. Exactly. But if you try to do the estimates, it's... They're deterministic. Well, but it's hard. If you try to do the estimates of the size of the matrices, it's not at all clear that it should be any at all, right? So it's, you know, because the ranks are. You're all in characteristic zero. If you resolve them over the free resolution. Over the free resolution of any integers to be tortured in things, which are dealt with over complex numbers. If you really begin to do that, this way you look at over the integers and you look over the point as interesting production. I don't like the word bad. Yeah, yeah, yeah. But it's not at all clear. So we'll see an example later. The first known example is the numbers won't be what you expect, I think. Other questions? Great. I guess you might also get, I mean, you know, not exactly in this case, but like there's this work with Claudio where you get really fractal behavior at different parameters where you're answering all LIPO questions. That's right, yeah, and actually it's really wild. And that's kind of what that is. That's kind of what I expect, that you can build, you can build, once you find one cycle, you should be able to, one p-torsion cycle, you should be able to reinvent it kind of further and further. But talked with Claudia about it. Talk with Cloudy about how to do it. But that's what I expect. All right, so again, this is just a place where experimental data is not going to fly. We're not going to get experimental data about large-scale D. And so a random model is, it seems to me, a kind of good avenue for generating insight or intuition about what should happen and these sorts of things. These sorts of things. And so, in my kind of overarching idea, it's, you know, we're going to use a random model here. You know, I mean, I think, just thinking of this, as I wrote this talk, that there's interplay between results and random models and all, you know, there's interplay between all these ideas, but this is the one that I'm kind of focusing the talk on, primarily using random models to probe and generate new insight. And I wrote down here, I wrote this little And I wrote down here, I wrote this little tiny thing because when I practiced it, I kept forgetting to say this. So, this is an old idea, right? This is not my idea. This idea has been around forever. It's huge. I learned about it from the number theorists, where they developed these models for kind of what random abelian groups look like and use that to talk about class groups of number fields. It's modelous conjecture, the Cohen-Leinster heuristics, that sort of stuff. It's very much, if you've ever heard those phrases, very much this idea. They use a random model to get conjectures. Random model to get conjectures about all right. So, what's our random model? So, so let me now get some randomness actually on the board. So, what Jay and I did was we looked a random model that was attached to an Erdős-Reni random graph, and it goes as follows. I note I'm citing both us and this multi-author paper, which actually their paper came out first. Which actually their paper came out first, even though it was published later. So in their work, they did a whole slew of different random models, and they had all sorts of parameters. And so one of their families of models specializes to ours. And we were interested in just one and probing it. But so there's overlap there, too. Alright, so here's the one we did. We wanted to start with vertices and then some attaching probability. And so you get. Probability, and so you know, you get some Erdős-Reny random graph by attaching some edges, and then we want to make it into a flag complex. So, whenever there's a click, we're just going to fill it in. So, you know, I only did this because I didn't want to try to draw tetrahedron, so that's my thing. And then to any simplicial complex, you can attach a square-free monomial ideal where the number of variables lines up with the typo, that should be a V with the number of variables. Typo, that should be a V with the number of vertices. And so you get a square-free monomial ideal, and then I can ask about the Betty numbers of the ideal, right? So it's a way to produce a kind of random object and then ask about its synchronous. So without step four, they've studied random synchronicial complexes, built that way, right? Yes, yeah, yeah. So step three would be like work of Matthew Kala and people like that, right? Who studied all Who studied all, and that's where we came up with the model. We saw those papers. Their buddy numbers are different. Yeah, they're doing topological vending numbers. Yeah. So that's beautiful stuff. And that was kind of where we came up with the idea. We thought, let's use that model. That looks like a great model. A lot's known. And we kind of knew we'd be able to leverage that, right? So that was one of the ideas. All right. Let me pause and talk about a whole bunch of work in this area. So there's this. So there's this multi-author paper I mentioned where they do all these different models and establish really the foundations, I'd say, of studying random monomial ideals and the basic threshold phenomena. So how does the dimension of the ideal or the co-dimension of the ideal vary with the parameters? What are the threshold phenomena there? You know, projective dimension, colon-Macaulay, so a lot of that gets explored in this initial paper. Then there was this following. Initial paper, then there was this follow-up paper that Sirkin told us about where they go more into the homological invariance. How does the syzygies, the free resolution, the projective dimension, how does that vary with the parameters? A little about that. And then there's, let's see, this one we haven't heard so much about. Jay mentioned it. Silverstein, Wilburn, Yang. What they do is they take, so this initial paper shows the threshold phenomena for when the Kroll. Phenomena for when the Kroll dimension jumps. And what these guys do is they show within each bucket of Krull dimension, they kind of stratify by degree. So what's happening is you're crossing threshold where the degree is getting larger and larger. And as the degree hits infinity, that's where the Kroll dimension then jumps to the next one. So they do this beautiful kind of sub-stratification of the initial paper. And then these last ones, Bonner G. And then these last ones, Banerjee Yagershwaran, Dr. Min Newman, and Engstrom Borlik, what they're all doing is studying variants of, I mean, random edge ideals, things kind of variants of our model in some sense, but going deeper into the, using deeper combinatorics to get deeper results. So Dr. Man Newman, in particular, I think they used some really interesting and deeper combinatorics to improve. Deeper combinatorics to improve, to really sharpen up what we know about the syzogies of these things. So, sorry, I'm just gonna go ahead and do that. All right, so we're gonna make this work. What we need to do is we need to, I mean, this is an overarching model, we need to set the parameters in such a way that we can make the case that studying the random Betty table is going to be able to. Betty table is going to tell us something about the projective space for D very large, right? So, in other words, what we have to do, we have to ask ourselves: is there a way we can set the parameters so that there's a, I don't know, a similarity or they have similar features? And then if we're kind of, the sharper we can make that, the more confident we can be in the predictions that come out of it. And so that's where we're going to start. And I just want to point, I mean, the point here is what we're going to do, we're going to take Is what we're going to do, we're going to take known theorems about asymptotic syzygies, of which there are very few, but we're going to take those and try to set our model so that it has those features. So in other words, I'm going to take Einen-Lazarsfeld's theorem and try to create a model that echoes what we know from Einen-Lazarsfeld's work. That's the idea, right? So we're going from, we're building our model based on the known theorems, and then we're going to try to take it out for a test drive. Take it out for a test drive. Alright, so here's what Ayn and Lazarusfeld proved. What they proved was roughly that basically what they proved was that there's an elementary result that says, so here, let me show P3. So for P3, it's three-dimensional. There's an elementary result that says you're only going to get non-zero entries essentially in three rows. The only possibility of getting Three rows. The only possibility of getting it is in rows one, two, and three. And what their results showed is that if D is very large, basically every one of those entries is going to be non-zero. So here I did D equals 10, which is maybe not very large, but from the perspective of rows 1 and 2, it's looking pretty good. You can see, you know, sorry, the dots indicate non-zero entries, and you can see it's filling out almost a whole row. Filling out almost a whole row. And if I kept going, as D got larger and larger, they'd get fuller and fuller. And then this third row is catching up slower, right? The third row, we're going to need to let D get up to 20 or something before it really... Oh, so it's eventually all three complete rows in that particular way. That's right, that's right. What happens is you can measure the asymptotic size of these gaps. So the rho grows like d to the dimension, and then these gaps. Dimension, and then these gaps grow like d to something smaller. So, kind of in the limit, the row will overwhelm it, right? So, so that, and that, that's, but it's all estimates, right? It's all kind of asymptotically. So, we don't know, we still don't know exactly where it ends, but we know that the kind of gap is roughly, like, I think in this case it would be, so it's roughly like linear in D, quadratic in D, these gaps, and linear in D and quadratic in D. And so as D goes to infinity, And so as d goes to infinity, the whole row is cubic in d, it'll overlap. So earlier you said you don't know that, though, in general, for d bigger than 6. Yes. For b bigger than 3, I think. 5 would be. But you do know some of them. No, we know some which are non-zero, but we don't know the values. So we can produce non-zero syzygies. That's what Ain and Lazarsfeld do, I think, or we did in follow-up work. So all I'm putting is a dup. We have no clue what these numbers are. No clue what these numbers are. And there could be more non-zero ones you don't know about. That's right, and there could be more non-zero ones that we don't know about. But Park's result says they're kind of asymptotically it'll be right. So there can't be lots more. You know, it's, yeah. Other questions? These are great questions. All right, so what we want to do is we want to create, you know, we have this rule, right? It's for the rows one through the dimension. For the rows 1 through the dimension of x, we know that asymptotically all the Betty numbers are going to be non-zero, essentially. And so we want to produce a delta model where that happens. And so that's what kind of the main result of me and Jay's initial paper was, here's how you do it. And here I actually, oh, I should have cited them. I'm actually citing, well, no, no, the way I'm citing it's fine. So if P is in this regime, and I don't think it matters so much what it is, then with high probability, It is, then with high probability, the Betty table has almost all of its entries non-zero in a certain number of rows. And so, I guess here you should think this looks like, what I'm saying here is, in this regime, the Betty table looks like an M plus one dimensional variety. That's what it's saying, right? So I have some parameters I can tune to say this delta. This delta regime is going to echo this theorem. That's the idea with me. By the way, I'll get to the fine print in a second. Let me say what we showed, what Dockerman and Newman did was show that also in this regime, it's kind of zero everywhere else, which was also a feature of the geometric case. So that was something Jay and I weren't able to do. So that was something Jay and I weren't able to do, that they were able to do with stuff. So it kind of slightly better. I'm going to note here, I mean, it's fun to sell your results and talk about how great they are. I mean, there's a lot of, it wouldn't be hard to poke holes in how good this, and how much these guys are similar to that one. I mean, there are several things that are known about the geometric case that aren't going to hold here. Going to hold here, right? So, so I don't know, NP conditions, or I mean, there's a whole bunch of differences. So, it's, I don't want to oversell, right? I mean, there's, there's, I don't expect that these models are kind of good enough that we could get really super granular information. But my hope is that they're kind of similar enough and there's enough uniformity in these sort of asymptotic pictures that we can capture overarching properties. And I'd say there is some. Properties. And I'd say there is some evidence to that effect. So, as long as we're studying kind of broad phenomena that we expect to be uniform across families, this sort of random model I think is useful. I think for something like really granular predictions, like exactly where these things drop off. Our model isn't there, right? That distinction makes sense. So a little bit like in the first slide, it's like the difference between asking about is the number of twin primes infinite or how many are there, right? Or how many are there? You need a richer model to get an actual, you need to take some arithmetic data to get an actual prediction of how many twin primes there are in most n. And this is more just like the first pass. All right, wait, so we just did that one. So I was using the results to get my model, and now we're gonna switch. And yeah. I don't want it to be a long question, but how did you prove it? Like, so did you just use use it? Like so did you just use use the wrong way. I mean it's it's r it's raw complex, so you're using the complex numbers and then with certain probability certain faces appear or don't appear and based on that you count the probability of tells you that the the Betty numbers are in bijection with the homology of induced subcomplexes of delta, and linearity of expectations tells you kind of what you Of expectations tells you kind of what you expect. And I guess, no, for this one, non-zero, we just need to find one. So then it's just, we just need to show that the expected number is strictly bigger than zero. So, you know, we just, we go, we find ones where, and, and you want, you want, what do you need? You need ones, the ones which are most likely to appear are like you want to find like one with the fewest vertices and a non-zero HI, basically, is that right? Or is it the ratio of vertices to edges? It's the fewest vertices. It's the fewest vertices. Fewest vertices and a non-zero HI, right? And so you want to say the probability that this arises is strictly positive. And I think we just compute that the expected value is positive and that's it? I mean, there's invariant stuff because you want to prove the probability of that, like probability that you get none of them is zero, but it's zero, and so you have to do it from the middle. Okay. It's not it wasn't crazy. Yeah, yeah. It's very it's very similar to s the method Sirkin mentioned about The method Sirkin mentioned about, you know, you do the linearity of expectations, then a Chevy show kind of bound on the variance. We're not doing any fancy probability here. That's great. All right, so wait, now let's take this let's take this car out for a test run, right? So what I want to do is, in the rest of my talk, talk about how the random model can maybe shape our intuition and give us some ideas. And give us some ideas. So I'll talk about p-torsion in the Betty table. So this is this kind of arithmetic sort of question, which is when is the Betty table different in characteristic P than it is in characteristic zero? The example, the first known example, was in 92, and it was that if you take P7 in the tuple embedding, that has three torsion. So I wanted to highlight that. That's the simplest known example. It shows it's. Simplest known example, it shows it's not that simple, even. You got to go up to P7. At least that's the first one we know of. And it's not clear why three, right? I mean, it's, you know, there's P7, which is eight variables. It's d equals two. I mean, there's, you know, it's the fifth Betty number if you look, right? Like, it's, it's, there's not any... What's that? There's no three there. There's no three, right? And it's it's not in any obvious way. I mean, certainly there is a three somewhere, but. Certainly there is a 3 somewhere, but not in any obvious way. So I'll say a Betty table where I don't reference the field. So I'm talking about projective space without specifying the field. I'll say it as p-torsion if the characteristic 0 is different than the characteristic p. And I'll just say if it depends on the characteristic, if that happens for some q, right? If I don't know which one. And so the sort of question that Jay and Caitlin and I were interested in was: should we think of Anderson's example as being the exception or the rule? If I look at the Betty table of Pn for D equals 100 or 1,000 or 1 million, should we expect that, of course it's going to depend on the characteristic? Or is this a kind of sporadic phenomena? Is it some weird thing that involves whatever. Whatever. Modularity relations among the parameters, and it can only happen sporadically. And I think there's so little data on this. I mean, there's like maybe two or three other known examples that there's not great intuition. I mean, my own intuition was it should have been the rule, but asking other people that that wasn't kind of a uniform intuition of people I talked to in the food. So we thought this is the kind of question where a random model might be able to give some evidence. And so that's what we want to do, right? We want to use our random model to probe this question. And so here's our main theorem. So this was not so long ago. So it was, we showed that if you set the parameter in a certain range, then with high probability, the Betty table will depend on the characteristic. The characteristic. And I'll note that the range of cases covered by those parameters, it covers every case where our model acts like an R-dimensional variety for R at least 7. So it kind of says in some sense that for P7 or above, you should expect, I mean, yeah, expect that dependence on characteristic. And I would say it's this kind of this. And I would say it's this kind of this new folklore, at least for me, I believe this, that dependence on characteristic should be the rule, not the exception. That's what we're kind of learning from the random model. It's suggesting that this dependence on characteristic is always going to be there. And so we made a conjecture, right? So our conjecture is that for R at least 7 and D very large, the Benny table should always depend on the characteristic. Characteristic. And so that's kind of the conjecture that we would draw from the theorem. Those are the cases covered by. Let me say a word about a sketch of the proof. So it's, yeah, let me see. How do I want to do this? So I mean, Hoxter's formula is this famous algebraic formula. Famous algebraic formula where you take the Betty numbers are encoded by the homology of induced subcomplexes of your complex. And so it amounts to studying, do certain induced subcomplexes appear? And so what we want to show is that for some Q, you can get an induced subcomplex which has Q torsion and its homology. You want to go seeking those and find them. And to do that, And to do this, we need to kind of know of certain finite complexes, simplicial complexes, that are likely to appear given this attaching phenomena, given this attaching probabilities. And it turns out a good way to do that has to do with the max degree of a vertex. The max degree of a vertex is going to control the likelihood of what appears, or it's actually, what's it called? What's the phrase Bolivov just uses? What's the phrase Bolovash just using? It's the essential density. Yeah. Is that the phrase yield? So, anyways, we just, it's, it's Bolovash proved a theorem on when is a subgraph going to appear in a certain random graph. And we need a mild variant of that because we need it to be an induced subgraph. It has to be the subgraph. Like an induced subgraph is you fix the vertical. An induced subgraph is you fix the vertices and you take all edges on those vertices. So like this is an induced subgraph, whereas this, the blue, would not be an induced subgraph of the initial guy. So it's just a minor variant, but it doesn't actually affect the probabilistic arguments too much. So we need this kind of minor variant of Bolovash's theorem. And that's what we use. We combine that with this explicit construction of Newman, which we have to alter a little bit to make a flag complex. To make a flag complex. But again, that's not a serious challenge. And this kind of captures why they're coming. You find something, and if the parameters are right, you should expect that with high probability, and that's what happens. Gibbons, torsion. Torsion kids. And so again, there's this question of how should we interpret the theorem? And I think it's worth, here we come back to the issue of how much do we believe in our model? How good is it, right? So it's, because the theorem we would So it's, because the theorem we would prove is actually for any Q, for any fixed Q, you'll have Q torsion for n sufficiently large with high probability. And so it would be saying not only do you see some dependence on characteristic, you're kind of getting any torsion you want over and over again. And that maybe is a little, you know, I don't know. We weren't quite confident enough to conjecture something like that, right? I mean, that's, you know, so. That, right? I mean, that's, you know, so again, there's some limitations to our model. We know that they're imperfect, and I'd be a little hesitant to make a conjecture that's too strong here. So I think we conjectured this, and we also conjectured something like the number of primes for which this has p torsion should be unbounded as d goes to inferiority. You kind of see things like that. But anything more quantitative would be exciting, but I wouldn't say that our kind of model is. I wouldn't say that our kind of model is good enough to festify that. All right, so my last slide, let me wrap with some comments about where we want to go, what the flaws were. So I think the first one is if we can improve the model by building in features that we know to be true or that we expect to be true, then we would be more confident in the prediction. Then we would be more confident in the predictions that come out of this. So I highlighted a couple things here. There's known NP conditions that give more refined data about Betty numbers that R or zero, and our model doesn't capture that. And Park proved a kind of asymptotic non-sorry, I said that worked. Yeah, asymptotic, no, I said that wrong. An asymptotic vanishing, that was a counterpoint to Einand-Lazarsfeld's theorem. And our model doesn't incorporate that. And so if... That. And so, if we could fine-tune it further to incorporate those features, I think we'd be more confident in the predictions. And I think also there are conjectures that are not known about what should happen. And if we could see that our model was capturing those conjectures, where the intuition came from somewhere else, that would also be evidence. I mean, it's not just incorporating known results, there's also incorporating features from conjecture. From conjecture. So there's a conjecture that Ain, myself, and Lazarusfeld had about normal distribution of Betty numbers. We saw a glimmer of it in those graphics. And if we knew that the models we were building captured that, that would give us more confidence. We kind of know that it captures it on the first row, but that's it. That was the result Jamie can prove. But so again, there's the opportunity to improve the model. And if we could do that, I think there's lots of interesting questions you could ask. Lots of interesting questions you could ask, but related to the talk, the kind of quantitative predictions on p-torsion. Can we say roughly how many primes do we expect to see where the characteristic differs? That's the sort of prediction that would be really fun to be able to make if we could get kind of more confident in the rigor of our model. All right, let me stop here. Thank you. Oh, yeah, yeah. Stop recording.