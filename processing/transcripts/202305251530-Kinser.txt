And I'll first tell you what is a quiver representation. So a quiver is just another name for a directive graph in the context of representation theory. And we define such a thing by four pieces here. So this would be the vertex set. This will be the vertex set. This will be the arrow set. And then these will be source and target maps. So these things, each of these eat an arrow and they pop out a vertex to tell you the source of the target. And a representation of a quiver. How's the size of my writing? Is that reasonably visible in brackets? Representation, Q consists of two pieces. And the things you assign to your directed graph to make a representation of it is Vx is a vector space. At every vertex. And then each phi alpha is a map from the vector space of the source of that arrow to the target of that arrow for each arrow of Q. And basically, if you take the reps of a given Q, you get the category of rep Q. So, if you fix your quivered, you can look at all the representations of the category in a natural way. I won't define morphisms or anything because we don't need them in the talk. But there's kind of an obvious way to do it. And if you don't want to think of this as some sort of totally new thing, you can actually just take your quiver and define a ring associated to it. These will be the finitely generated modules over that ring. So, anything that you know about, if you're familiar with modules, homomorphisms, direct sums, kernels, toe kernels, et cetera, you can do this all with representations in a very natural way. In a very natural way. Now, I'm going to be looking at some geometric things associated with quivers. So we'll usually fix a dimension vector. So a dimension vector is just an assignment of a non-negative integer to each vertex. So we think of it as the dimension of the vector space that we want to put there. And then with this, we get a representation variety. And this representation variety, you think of it as if you fix your vector spaces there. Oh, I didn't say it, but I'll always think of everything as being finite. So my quivers will always be, of course, you can be more general, but my quivers will always be finite-directed graphs. My vector spaces will always be finite-dimensional. And so I can, I'm sort of thinking of parameterizing. I'm sort of thinking of parametrizing representations with a given dimension vector. I might as well just pretend all my vector spaces are c to the d, as d varies with the vertex. And then I can represent the linear maps between them by matrices since we've assigned bases. And so you can formalize it this way. This representation variety will be the product over all the arrows of the space of matrices, which are of size. Which are of size the dimension of the target by the dimension of the source. Let's take complex-valued matrices. And you can see that this just parametrizes all the, you know, every collection of vector spaces with those dimensions and linear maps between them can be parametrized. Of course, it's very redundant because you can parametrize sort of the same representation in different bases. And then you have on this acting the base change group. So if we want to make Want to make think about what is that sort of natural notion of redundancy? We would look at change of basis. And what is a change of basis? Well, just GLD invertebral matrices associated to each vertex. And so that's a base change group, and it acts naturally on Quickly write a formula, but it's just if you think you have a collection of matrices and you have base change at the source and target, you have to put in your inverses in the right place depending on whether you want a left or right action, and you get a nice action. It looks something like a conjugation. So a couple examples, which we've already seen, of course, not in quibber representation language, would be if I take Q to be The n-loop quiver. So I've got one vertex here in n loops. And then, while your dimension vector is just a single, there's only one vertex, your dimension vector is just a single non-negative integer. And you see here that rep Q D is exactly D by D matrices. And we have N copies, one for each arrow. And G L D And GLD is just GLD, because there's only one vertex here. And how is it acting? Well, if you unpack it, it's by conjugation, because it's acting at both the source and target of each arrow in the natural way, so that's conjugation. And then another how do you and how does your D looks like it just from one vertex? Like just from one vertex to z? Yeah, so in that case, there's just a single, yeah, exactly. So there's just a single non-negative integer. So all our matrices are the same size. Maybe to make one that's slightly, still very elementary and closely related to joint spectra, if we could see it sort of hidden in, particularly in Professor Klepp's talk, a lot of this stuff is like hidden in there. Is we could also take so-called generalized chronic requirement. So we're going to take the same thing here in arrows, and we're going to make them go between different sources. And we're going to make them go between different sources and targets, so I'll call this x and y. And then say my d is like d sub x, d sub y. And then I can think of my representation variety as now matrices of dy by dx size, and then I have still n copies of them. And GLDX, here GLD is Here GLD now has two components, GLDX cross GLDY. Again, this acts in a natural way. Depending if you have a left or right action, it's going to multiply on one side and with the inverse on the other side. And so this is sort of the set setup in which you could talk about joint spectra in really either of these, but in quiver representation terms. Inquiver representation terms. And so, really, the key thing here is I'm going to go like, so first I give this sort of category, I mentioned there's a category, and I mentioned these are spaces. Just from the definitions, there's nothing deep going on here. But if you unpack some of these definitions, you'll find that two representations are isomorphic as objects in the subelian category. This is the same thing as saying they lie within the same orbit. And of course there has to be some for some specific dimension vector. So this D would be like for them to be isomorphic, they need to have the same dimension vector for this to make sense. So let's plus it there. All right. And so that's sort of the most basic, and there's nothing deep going on here, but if you unpack it, this is the basic interplay between the sort of categorical view and the geometric view. And then in the talk, I try to say a little bit more, go a little deeper into that. Uh go a little deeper into that. So here I'll talk about orbit closures. So there's orbits are basically just isomorphism classes. What gets more interesting geometrically is orbit closures and And so the basic problem we want to study is: I'll just call this like a key question. This is called like the one. So I think this kind of looking at this kind of thing was called an equivalence problem in Professor Klepp's talk. So I went back to Curto-Herrera conjecture and then the these things. So this is more like what he called the one-sided problems for orbit closures. So you're given For orbit closures. So you're given V and W of the same dimension vector. We want to determine, and this is kind of intentionally open-ended, how to determine if the orbit closure of one Of one, and let me make it this way, is contained in the orbit closure of the other. And that's just like a basic geometric problem. In this language, I'm going to give this, for the purposes of this talk, I'm going to define this for short as saying V is less than or equal to W. So I'm really putting a partial order on all the orbits, order of a closures, however you prefer to think about it, given by containment. Given by containment. And this is called, often in quiver literature, this is called the degeneration order. And in general, we wouldn't expect for all quivers that there's just some nice answer. This is going to be probably too hard for them to be just some nice symptom, the equivalence problem. You don't necessarily have just some nice simple thing. You can always compute it as too general. But it does have a rich history of kind of partial results. You know, what do we do? We might then consider restriction. So, what we do, we might then consider restrict what kind of quivers we look at, or restrict what kinds of VMW we look at that arise in some more natural situations or something, and try to get strong results this way. So this does have a rich history and representation theory, literature, representation theory of coverage and algebra specifically. And let me just give some names here. So, highly incomplete, because the disclaimer was a highly incomplete list, but these are some of the names of some of the most interesting results here. I'll put them in alphabetical order. There's papers of Baumgarts, Carlson was mentioned in Clepp's talk, Huyskin Zimmermann. Friedman, Malo, and Zoara. These are some of the, but of course there's many, many interesting results, some of the major ones due to them. One quick, just really observation in a definition. I'll use the observation to make the definition. If If V is less than or equal to W, then we can conclude in representation theoretic terms, this is not difficult, that the dimension of the homomorphism is just these are complex vector spaces. So the dimension of the homomorphism space from X to W is less than or equal to the dimension of the homomorphism. To the dimension of the homomorphism space from x to v for all representations of your query q. And I'm going to use this to determine, this is what's defined as the Hong order. You always have to be careful. It's reversal directing if you think about it, because sort of being contained in an orbit closure. Because, sort of being contained in an orbit closure is like a more specialized condition. So, that means some equations are solved, so you would expect dimension of homomorphism spaces to be going up because you have more homomorphisms when more equations are satisfied. So, this intuitively order reversing this. Yeah, maybe to help us understand the key question. So, let's look at the case that you have over there. You have X and Y. Let's assume we have only one map of a 1. Of a one. What does this question mean? Yes, if you have only one arrow, the orbits and their closures are determined precisely by the rank of the matrix. You can prove this. And then one will be contained in the other if and only if the ranks are equal. So in this case, the orbit closures are linearly ordered, so it's very simple. The post that is just linearly ordered by the possible ranks. If you go to two already, it starts to be pretty nasty to write it out. That's an issue. So even if you already go to a pair of maps, orbit closure, Go to a pair of maps orbit closure. I can give you an answer. In fact, it'll follow from Ethereum right. If you go to two, if you go to three, I don't think there's anything that you can say, like really concretely and simply. So it quickly gets complicated. So you can nicely say, right, have just one vertex and one loop, right? Yes, then that's also quite simple. That's exactly right. There's one vertex and one loop. In that case, you see, it's even, you know, there's something to be done there. You have a lot of conditions on the ranks of the powers. On the ranks of the powers of the matrix and their differences with the identity. So you see, it's not a simple answer, even for a single loop. Yeah, good comments. Thank you. So I wanted to say, so in general... The size of your dimension vector is like the number of the vertices, right? Yeah, exactly. There will be one dimensional system. So if you find like this degree D, right? So then this map, right? From the vertices to Z, what are the properties of this map? Of this map, right? It's just an integer. It's just sort of a list. Yeah, you just think of it like in practice, you would just write the integers over the vertices if you're doing some computations. The board is your friend. You wouldn't even really think of the function. You just kind of write them there. Yeah. Yeah. So it's basically you're just fixing a discrete invariant so that you have a finite dimensional space of possibilities is what you're thinking about. There's kind of nothing deep going on with dimension vectors. Just you want to fix some discrete invariant so you can have a finite dimensional space and parameterize things from the vector. Well, it's also a first. Tricines for me. Well, it's also the first invariant of equipment. Yeah. It's if you're more representation minded. If you have a representation of a single object, the dimension of that representation is the first invariant. Yeah, exactly. So it's just the first invariant you can call them. That's right. And they're more like representation to remind. It's a class in the Groton-Deep group. It's equivalent to that. Or equivalent without oriented cycles, I should say. That's not true. All right, so that's and then. So that's, and then it's pretty, you know, in general, the converse just won't hold. So this is pretty easy to go forward. In general, the converse won't hold. One quick comment is: if you replace these with equality, it isn't if and only if. That was the theorem of Al Slander, also mentioned in Professor Klepp's talk. And that's sort of what maybe inspire you to look at that, knowing that theorem of Alslander, to think, what if I just put things with an inequality, and that's fine. So the conversation is, oh, but just as an example where an example of where. An example of where you might, this is not generally true, but an example where you would get a converse. Is the theorem of Bongart? This is a pretty hard theorem. Is a theorem of Bolmgar's. This is a pretty hard theorem, so that's just an easy observation. Basically, you can prove it in the following situation, which is not easy. But just to give you a flavor of whether the converse might be true, is if Q is an extended Dinkin quiver, so for a quiver to be Dinkin or extended Deacon, I mean the underlying graph is. We just forget the orientations of the arrows. And so, kind of interestingly, this sort of simplistic thing does work in that situation, even when there's infinitely many represent, you know, this is not so easy. But this is just mentioned to give you a flavor. All right, so that's some basics about overclosures. And now I try to define rank schemes, which is the thing I've been thinking about. And the general ideas we're going to flip, so everything here has been sort of thinking of sets and like points in the space and their closure stuff, so we're going to flip around and think to functions. So we're going to flip things around and think of functions. And so some questions we might have when you think about functions on spaces are how to A describe how to describe A A orbit closures as zero sets and B ideals of all functions vanishing on orbit closures. So, these are two kinds of questions about functions we can turn around and ask. And so, to get to this, there's kind of a natural, if you unpack this condition, this might be like, even though it's only one way, a natural place to start looking for functions that vanish on orbit closures and candidates for answering these. And that's not obvious from the way it's written, so I have to make another definition. Another definition to do this. So, this definition, I think this is basically kind of, it falls out of the natural way of thinking of this and like kind of calculating homes by projective resolutions, but I think it's probably, I think the first time I saw it, I printed Reedsman and Zuara paper around 2013 or so. And so we're going to define a Q matrix M, and we'll just add some ad hoc notation here. To notice this, add some ad hoc notation here, but QMET is a matrix, not necessarily square, with rows and columns labeled by the vertices of Q such that an entry in a row labeled X. Labeled X and a column labeled Y has a linear combination, C linear combination of paths from Y to X. And so, actually, if you, if I, what is in Professor Clapp's talk called like something like Talk called like something like matrix-valued polynomials or something. I forgot about it. It's basically the same thing here. So I think you called the matrix-valued polynomials or something. This is equivalent to this, but this is why you can go for any quiver, basically. Because for the n-loop quiver, your path algebra, your paths are sort of like non-commutative polynomials in the arrows. And this is the same idea. Okay, so that's what a Q matrix is. And then you can evaluate. So this is just a matrix of the Clipper Algebra. Yeah, exactly. It's a matrix of the Claire algebra. Is the matrix of the cover algebra, but you want to sort of keep track of the low, the rows has to be sort of like for some commuter value, you might think they're actually representing projective resolutions, is what they're all possible representations of projective resolutions. So the evaluation of such a matrix at a representation, if you can all just do the natural thing, your paths are made up of arrows concatenated together, and your quiver representation has matrices associated with the arrows, so you just like plug in the matrices everywhere you see an arrow. The matrices, everywhere you see an arrow. And then this sort of the conditions here is set, it makes the sizes all make sense. So your matrices can be whatever size, and this will make the sizes make sense. So my V is the same notation as before. You have a linear map phi alpha over the arrows. You can represent it by a matrix once you've chosen. By matrix, which you've chosen bases. Again, I think in Professor Slepp's talk, this was what you got by like tensoring, we kind of evaluation, and you use the Kroniker tensor product, this will accomplish basically the same thing. And okay, so proposition from this, we can now translate the thing I had here about Pawn Mortar, the ones The ones we can translate basically the definition of HOM order to the following for VW of the same dimension vector, we can say that this home order between V and W is equivalent to saying that the rank of every possible substitution You have the corresponding rank for every possible substitution into all possible Q matrices. So that's another way to characterize that POM order. And so finally, I can make... So the substitution ends in being like a map from the direct sum of all the vector spaces themselves. Yeah, exactly, yeah. The other ones, and you might have repic, because you might have a repetitive. And you might have the thing because you might have repetitions, like your Q matrix has many repetitions or whatever. Yeah, exactly. And it's not necessarily the same things in the rows and the columns. All right, so with that proposition in mind, what do we, so I said I wanted to go to functions. Well, what do we rephrase things instead of like homes between equivalent representations kind of category? If I start to rephrase things in terms of ranks and matrices, someone like me then thinks to look at minors. That's how I get equations. If I want to measure ranks of matrices, I want to look at minors, and the functions, the minors that vanish. The minors have vanish. So, our last definition, we can get to what an array scheme is: is that, so, first of all, if we're given a dimension d, dimension vector d for q, we're going to need to now, for looking at functions, we need to get some variables. So, d is going to be the representation with independent variable entries. So you, same thing we did before, you fix your matrices and their size, but instead of thinking of like numbers in there, you just put an independent variable for each one. We have to call these like generic matrices in commutative balance group. And then we let, for a given representation v, we look at the ideal IV associated to be the sum over all possible. To be the sum over all possible Q matrices. And we take the R by R minors of the substitution of this matrix variable in there. What size minors should we take? We should take all of those minors such that the rank of m evaluated at Q is strictly less than R. And so you've cooked up a whole bunch of, this is sort of like a tautological way to cook up a bunch of functions which vanish on this. Which vanish on this. And then the spectrum of this quotient of the polynomial of the coordinate ring is the rank scheme of V. And so it's just cooked up. And so, let me see, so like a corollary to Baumgart's theorem. So, to rephrase what he had, it kind of might be equivalent in some sense. Sense. I think it's definitely just a corollary is that for Q extended deacon, we can answer the problem I put up there about describing as a zero set. You can say that the orbit closure of V is exactly the zero logus of this ideal. And what's sort of why isn't just why isn't that just always true? Well, the point is you can have like more things vanish than you would think. This is definitely a collection of functions that does vanish on it, but you can have more things vanish on it. You also might have more points that sort of are not in your orbit closure, like the vanished instead of this could be reducible in algebraic geometry language. So, this is kind of miraculous that this works out really nicely for. Kind of miraculous that this works out really nicely for extended thinking quivers. And it's sort of for a conjecture, kind of maybe a folk. I don't know, I've seen it written down because people have worked on it, and there's lots of pragmatists in specific cases, but like it would be for whenever q is Dinkin, this is ideal of all functions. I guess I guess when you have like a just like a directed A and quer this chair, this is a big, this is what people know about say shiver writing. Yeah, sure. That's what I was going to say. For the sake of time, that's right. I won't write it well there, but for the sake of time, I'll say that well, this is only proven in type A quivers. First, we'll actually put my Maggar for when it's all directed, and they use this connection with And they used this connection with Subrivarius due to Zelvinsky, and they kind of made it seem theoretical. Bobinski and Zuara used some very, very cool, very difficult Auslan to writing theory machinery to be able to flip the arrows around and prove it for those. And as far as I can tell, it's open in most cases beyond that. So let me just leave with a problem. So this is a conjecture that this would hold for Deacon, but I think when you go outside of Deacon type, it's just not going to be true for all V, but there's many situations in which we're interested in quivers, and they may be like all representations are bad. All representations are badly behaved or wild or something, but rarely are you actually interested in all representations of the quiver, right? You don't necessarily need all representations of it. And so a problem would be for a more general Q, find conditions on V such that, let me call it a star, somebody wrote it, find more conditions on V such that star. Such that star folds. And I need some shit on time. So I've done some work on this with Andros Lawrence in the case where the radical squared of v is equal to zero. We have some results there. And sometimes when, well, star doesn't hold, you can ask what more do you need to throw in there to make that true. And we found some situations in which you need to throw in some traces of some matrices. And so I think there's some interesting things we've done here with what's pretty basic questions about matrices and reminders and such. Questions about matrices and reminders and such, but it gets complicated when you start smashing them together. And that's a good place to start. Is there a maybe a single short question? Thank you.