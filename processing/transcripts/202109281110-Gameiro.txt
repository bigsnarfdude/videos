Okay, Mauricio, take it away. All right, thank you. Yeah, so I'll be talking about DSGRN and how to use it to design a bi-stable switch. I want to explain the details of how we do this, the computations for this, to design this bi-stable switch. So, some parts of the talk will be more. Some parts of the talk will be more technical than others, but I hope it's okay. So yeah, so let's, I guess we have seen in this conference, I mean, everyone here, I guess, know what a switch is, but what is a switch? The classical idea of a switch is you can think you have a mouse. I don't think you have a mouse, but okay. I don't seem to have a model, but I can two point. But you have this, you can think you have a like a differential equation, and you have a parameter which is the x-axis, which is the signal. And you are trying to find this for low values of the signal, you have monostability in the middle, you have bi-stability, and for high values of the scene, you have monistability again. And this region in the middle you have this bi-stability is important because. Stability is important because you don't want to have your switch to switch on and off back and forth. So you want to have this region where if you are on, you stay on, if you're off, you stay off. And how do we just a secure my So the goal is to, for some reason, I don't see my mouse, I'm not able to use it. Okay. So, the goal that we have is to we want to design three node networks that function as a Three node networks that function as a robust bi-stability switch. And we want to have this hysteresis where you have this modest biostability and then monistability again. And then robust here, robust means that we want this to be to function over a wide range of parameter values, and we want this to be stable in the perturbation. So if you make changes in parameter, this should function as a switch. So what so this is the motivation? So, this is the motivation what we want to do. So, I will describe what the SGRN is and then how we use the SGRN, as I said before, to design this switch, you know, these three node networks. So the SGRN that we're going to describe later, but for now, let me just yeah, the problems in my I'll do this. So, the SGRN, which is dynamic signature generated by regulatory networks, it can is a software or the method that can compute a coarse description of dynamics of a network. And this one important aspect of this is that this description of dynamics is valid for all of parameter space. And we're going to see the describe this later, but for now, we And let's just have this what the SGN does. And the SGN, it can also, we're going to see that it can also evaluate robustness across all of parameter space. And we're going to describe how we do that in the case of bi-stable switch on three-node networks. So then what we did is we ranked all three-node networks according to their ability to function as a robust bistable switch. And just to And just to quickly show, describe what we did before without details. So, we take all three node networks as the picture on the left. And then we consider one of the nodes, node zero, to be the input node that's where the signal comes in, or that you can think that's the parameter that you want to change to be able to. To get your switch. And node two is the output, that's when you're reading out your signal. And we consider all three nodes network. So it means we consider all possibility of edges between these nodes. The edge might be absent, maybe an activating or repressing edge. So then with that, you get like a total of 19,000 networks. But some of them are trivial. Some of them, there is no connection between the signal, the input node, and the output node. The output node, or there is not a path to go from the input node to the output node. So, eliminate those networks, we're left with 14,000 networks, and these are the networks that we computed with. And in these 14 networks, we compute this hysteresis score, and this is somehow kind of the percent of parameter values exhibiting hysteresis. And this is the, if you do this, this is. If you do this, this is what we can rank out these 14,000 networks, and this is the result that we're going to describe how we get this. So you have all these 14,000 networks, you can rank them according to these scores. As you can see, some networks have high or very high score, but most of them have very low score. So if you want to design a bi-stable switch, then it means most of these Twindle networks will not, according to this score, will not work. But some of the Will not work, but some of them seem to have a high score, so they might work. And then, and also, we will discuss how we test this, how robust these scores are. So, to do that, we will start now to describe the SAN network. So, I don't need to say this to this audience, but when you have a biological network, that is not the A biological network that is not the right model for the network, but there is not like first principles model for a biological network. And even if we select a model, then it's very difficult and expensive to obtain parameter values. And also, even if you choose specific parameter values, the dynamic can vary a lot when you change these parameter values. So the philosophy of the SJN is to not try to do that, not try to look at the specific OD model. At a specific OD model, but rather try to compute a coarse description of dynamics for a network that's valid for all of parameter space. So we do not pick parameter values. So we try to decompose the whole parameter space and compute a description of dynamics for all parameter space. And the description of these dynamics does not depend on a particular OD model. So we do not pick a particular OD model, but you try to get this. To get this course description of dynamics independent of a particular OD model. So let me try to more technically what we do. So let's say you have a network. And then for each node of the network, there is a quantity that associates that network with that node, let's denote that by Xn. So then the simplest, we don't want to write down OD model, but the simplest thing we can do is just this. Is just this assumption that if you have an edge from an activating edge from node i to node n, then if you increase xi, this will increase the rate of production of xn. And the simplest model that we can have for this is this switching function, this switching function sigma that has a low value L and a high value L plus delta. And this function depends on this parameter theta. depends on this parameter theta. There is a threshold theta such that if you are if the value of xi is below this param this threshold theta, then this the contribution of node i is this low value L. It means the xi is not active. And after theta, then the value of the contribution of xi is L plus delta. So it means this node xi is activating node n. node n and written in form of a function that's this step function here and the same thing we can the same thing for the same assumption or the the symmetric assumption for a repressing node so if you have a if node i repress the node node n then you have um the increasing x i should decrease the rate of production of x n and this just um uh means that we switch this uh switching function this step function right so you go from from uh on From on the left, it goes from low to high, and on the right, it goes from high to low. So, these are the basic modeling for the modeling of how one node i affects the rate of production or the rate of change of the node n. And then we put all this together to get the rate of change of the node xn. So, the rate of change of node xn will be given by a decay term, which is a constant decay. Which is a constant decay minus gamma times x, plus this capital lambda, which is the rate of production of this node. And this is lambda. The production is given by a combination of these switching functions. In the HN, we use like products of sums. So products are sums of this combination function, right? Of these switching functions. One example, if you pick this. One example: if you pick this simple network, x1, x2, then to node x1, there are two incoming variables, x1 itself and x2, and to node x2, the same. Now, so then to node x1, one of them, the incoming thing from node x2 is a repressing edge. So we put a sigma minus, so like a decreasing step function. Decreasing step step function. And here, usually what we do is we multiply the repressor by and add the attract the activating edges. So then we multiply sigma plus of x1 by sigma minus of x2, and we add sigma plus of x1 and sigma plus of x2 for the second mode. So this is the model that we want to consider. And notice that I'm not writing down a differential equation because we don't want to think of this as a differential equation. We just want to think of this as this rate of. You just want to think of this as this rate of change of the variable xn. So, this expression on the top will be the rate of change. And then, what we want to do is see how the rate of this kind of study the signs of the rate of change, see how the rate of change change signs. And the first observation is that if you look at this function, this rate of change xn, since it's given by these kind of sums of products of these switching functions. Of these switching functions, it only changes values if you are in one of the thresholds. So then, if you are off these hyperplanes where xi equals these thresholds, these out-edge thresholds from that node, these switching functions, they are constant. So, it means that this gives a kind of a natural decomposition of phase space. So, if you are inside one of these regions that are between two thresholds in each one of the axes, then it means that. then it means that in their dysfunctions the the rate of um the rate of production capital lambda will be constant so if you want to study this the sign of of the of the rate of change it's natural to look at the boundaries of these regions and that's what uh what the h does so we we get we get this decomposition of of phase space into these uh rectangular regions where this data these are the thresholds that determine the the switching functions where we switch from low to high Switching functions where we switch from low to high or from high to low according to whether the edges are activated or repressing. And we look inside each one of these regions in phase space and see what is the sign of the rate of change in there. And one observation is that since it's constant inside, you just need to look at the boundaries of these regions. In other words, so you just want to evaluate this, we want to determine whether the rate of change. mind whether the rate of change or whether xn is increasing or decreasing inside each one of these regions but then we just need to look at this at the boundaries of this rectangular region so you just need to look at this threshold value so we replace xn in the in the gamma n times xn we just need to plug in the value of xn equals one of the threshold so then we just need to look at the sign of this um and um one observation is that one observation is that uh in the the this function gamma the gamma n the which is constant inside each one of these regions inside each one of the so inside each each one of these regions it will be constant and the values that will be that it will take on will be a combination of these l's and l plus delta right the low values l or the low values l plus delta so it it only can take only a finite number of It only can take only a finite number of values. And for example, for this network, if you take this gamma one, which is the product of sigma plus of x1 and sigma minus of x2, then the values that the sigmas can take is sigma plus can either be can be only the values can only be L or L11 or L1 plus delta one. And the same thing for sigma two. So then if you thing for sigma 2 so then if you take all possible combinations of this value you get all you get these four polynomials these four values so these are the four values that that this function lambda can take and this is true in general of course if you have more and more combinations of more switching functions you get more more polynomials but you can always you have a finite number of these polynomials that this function can take on and if you then if you want to look at the sign of this rate of change minus gamma n times theta minus gamma n times theta plus capital lambda then you just need to consider the sign for every one of these values p so then one way to do this is if you just determine if you just look at these p's and determine all so we don't want to give values for these parameters l's and deltas so we want to analyze this for every possible values of l's and deltas but since it's a finite number of them we can just look at all possible orderings of this so if you look at this p0 So, if you look at this P0, P1, P4, we want to look at all possible admissible orders for this. And from that, we can then determine all possible signs of this rate of change. In this case, in this particular case, I have this P1 up to P3. All the possible orders are always P0 less than P1 less than P2 less than P3, or the other one you can flip the order of P2 and P1, right? Because P0 is when you have both. One right because P0 is when you have both L's, both low values, and P3 is when you have both high values. And P1 is in P2 is one where one of them is low L and the other one is L plus delta. So you can switch the order between them. So for every value of L and delta that you pick, you're going to get one of these two orders. So then with these two orders, now if you want to determine the sign of this, the rate of change, all you have to do is to take this value of gamma times theta and consider the value of gamma. And consider the value of gamma time state relative to this partial order. So then we get this kind of possible kind of signs for this rate of change. And here I changed the network a little bit just to have only one threshold. So the network, so we're looking at node one. So node one, we should have two input edges, the self-edge and the edge comes from node two. edge and the edge comes from the two so the polynomials you get are huge p1 p0 up to p3 but now we have only one threshold theta one and the the threshold that we want to consider this minus gamma theta is this out this threshold corresponding to the out edge so look at this network when you have only one out edge so we have only one theta that we need to consider the relative value of these polynomials according to this relative to this piece. To this, um, relative to this piece. So, if you look at all possibilities, so you have this possibility listed here. So, all the piece less than theta one, then p0 up to p2 less than theta one, less than theta three, and so on. So, these are all possibilities of signs of this, of the rate of change, right? So, if you plug in each one of the piece to the place of gamma one, then you get one of one of these, you fall in one of these inequalities and that. You fall in one of these inequalities, and that gives you the sign of the rate of change. And one important thing here is that we can make this into a postet, into a partially ordered set by considering the order of the thresholds when you change the threshold. So, if you look at this order all the way on the bottom, when you have all the p is less than theta one, this corresponds to the rate of change being always negative. The rate of change being always negative because it doesn't matter which p you plug in there into the rate of change, you're going to get the gamma, which is one of the p's less than less than theta. So, this will give us the and here I picked gamma to be one, so I remove gamma from this notation, but gamma is multiplying, should be multiplying theta. So, the order right all the way on the bottom is corresponding to where the signs of the rate of change is always negative, doesn't matter. rate of change is always negative doesn't matter what um what the value what p what value of the parameter you're using and the thing all the way on the bottom on the top is when the the the sign of the rate of change is always positive for every value of of the parameters and the things in the middle is the things where the the sign if you slide theta from right to left the sign of the the sign of the rate of change will change from all negative to sometimes positive sometimes negative and this gives us a natural order a partial order on this Natural order, a partial order on this list of inequalities. Can I ask a, can you remind me real quick what's theta? Yes. So theta is this threshold that you use to determine the, well, like if you go all the way back to here. Theta is this threshold that determines these switching functions, right? Oh, okay, great. Thank you. Right. So if you are below theta, you are. So, if you are below theta, you are either the lower or high value according to whether you are increasing or decreasing. And then we use the same thetas to then break up the phase space into this rectangular region. So, now if you are in one of these rectangular regions, you want to look at this data value to decide if you are moving to the right or moving to the left or up or down. Right, so you look at the sign of this, it means that when you plug in tape in here. That when we plug in theta in here, we are looking at the boundary of these rectangular regions where x equals one of the thetas, one of the hyperplanes. And then you want to determine the sign of the rate of change there so to decide if you're moving in the direction of increasing x or increasing y. Right. And also, so if you look at this set, so this set of inequalities, this gives us this semi-algebraic set. With semi-algebraic sets on the space of parameters. And we so this gives us one decomposition of all of parameter space. So we did not pick values for these parameters. But we consider all possibilities of orders, all possibilities of signs for this for the rate of change. And this will give us all the all of parameter space decomposed on these regions according to these inequalities. And if you change, And if you change only one inequality in this list of inequalities, like going from one to the next in this graph here, that's corresponding to a co-dimension one kind of neighborhood in parameter space. You have a region and a co-dimensional one neighbor in that parameter region corresponding to change one of the signs. So, this we use this to make a parameter graph. We call this a parameter graph. And And two of these regions in parameter space, two of these nodes that correspond to a region in parameter space, they are adjacent if you have only one sign of, one change of sign in these inequalities, which corresponds to a co-dimension one, being a co-dimensional one neighbor. So we have this, and we so this is the graph corresponding to node one. So we call this a factor graph of node one. So this is PG1 is the part, the factor graph, this graph that corresponds to node one. So then That corresponds to node one. So then we'll do that for every node. And one important thing that we're going to use later is that this is a partially ordered set. So then if you want to change parameters in your network, you can kind of try to think about changing, walking along this graph. So this is the factor graph at one of the nodes. And then the whole parameter graph is just the product of this factor graph. So now if you take, so this factor graph, when you look So, this factor graph, when you look at mode one, we're only looking what happens when we are for the threshold that is out the threshold of the out edge from node one. So, this will tell me the sign of x1. So, now if I look at the factor graph at node two, that will tell me all the signs of x2. And combining of this, we get this parameter graph, which is the product of this. So, this will determine. And this will determine all possible signs of this rate of change. And then, for example, for this, again, going back to this two-node network, so in this parameter graph, you have 1600 nodes or 1600 parameter regions. Each one of these parameter regions horizontal one of the set of inequalities. So the number of parameters will be 14 because you have three parameters per edge, the L, the theta, the L. L the theta, the l the l plus delta and the theta, and one parameter per node, which is gamma. So we have a 14-dimensional parameter space, and this 14-dimensional parameter space, the whole parameter space is subdividing these 1600 regions. And so you have this parameter graph with 1600 nodes. And if you pick one of the nodes in this parameter graph, that corresponds to one set of inequalities. Like this, and then this set of inequalities tells me what are the signs of the rate of change for that particular parameter region. So, if you pick one parameter region, then I know if you are inside one of these rectangular region phase space, then I know that inside that rectangular region phase space, I am between two thetas. So, I know which are the values of the polynomial speed that I'll be there. So, for That will be there. So, for that variable x, I know which combination of else and delta I'm going to use in there. So then I know which polynomial p I'm using there. So then I can, in that parameter region, since all the inequalities are fixed for that parameter region, I know if the flow is, if the variable x1 is increasing or decreasing and x2 is increasing or decreasing. So we can put these arrows on this region in phase space and decide if we're moving to the right or to the left. And decide if we're moving to the right or to the left or top or down, right? So, for each parameter region, for each node in the parameter graph, which is one parameter region, we can do this. So, then we can construct with this, we can construct this what we call the state transition graph. So, we look at, we pick one parameter region, one parameter node. We determine all the signs of the rate of change of each one of the variables at the boundary of each one of these regions, and then we can. Of each one of these regions, and then we can construct this what we call the parameter graph. So if you just label these regions in parameter space 0, 1, 2, up to 8, so then we can decide if 0 goes to 1, 4 goes to 1, 1 goes to 2, and so on. And we do one additional thing. So like here, if you look at so this these parameter regions, so if you look at these phase space regions five and eight. Five and eight, there is no edge going out of it. So everything is going in. So if when you have a situation like this where everything is going in, we put a self-edge to that region, to that region, to that rectangular region in phase space. So we have a self-edge to node five and node eight. So that's these two transition graph showing on the right. And this is for one particular parameter value, right? So for this one parameter value, you have this. So if you change the Parameter value, you have this. So, if you change the parameter value, you change the inequalities, you change the signs, you change the state transition graph. So, we call this the state transition graph, or also a combinatorial multi-valued map. So, it's a map that for each node takes you to one or more nodes. Now, once you have this state transition graph for this multi-valued map, we can then find the recurrent dynamics at the squarse level. So, we're just looking at this course level in this decomposition phase space. Uh, the composition of phase space. We're looking at what, um, so how things change from one of these cells to the other. So, this will give us this graph, this directed graph. And you can look for the recurrent parts of this graph, which this corresponds to the strongly connect component of this graph. So, if you take the graph on the right, you have this. If you compute the strongly connect component, the non-trivial strongly connect component, meaning the connect component with at least one edge. So, you get Component with at least one edge. So you get the four regions on the left. And then with this, each one of these strongly finite components gives us, or you have a recurrent dynamic, either like a single node with a self-edge with response to something like a fixed point, or like cycles when you have these loops of several edges. So this gives us what we call the Marx sense. And these correspond to recurrent dynamics. And we also get from the graph on the left how these Morse sets are connected. We get these non-recurrent dynamics. And this will give us, we encode this by this, what we call the Morse graph. So like in this case, we would label the three strong components on the left, zero to three, and then have this Morse graph in the middle. So that's how we structure dynamics at this coarse level using the STRN. Can I ask a question, please? Yeah, sure. So, you're trying to understand better how a region in parameter space, how a box in parameter space corresponds to a state. So, you call this graph of the parameter regions the state transition graph. So, could you tell me what the state is and what is the transition? So, what changes from one node to another node? Yeah, so the I may have gone too fast through this. So the transition is, so we have the we're doing this on phase space, right? And we have these regions. So we divide phase space based on these thresholds. So recall that these thresholds, theta J1, correspond to the threshold that tells me of the x-axis, of the X1 axis. The contribution of X1 to X2 is going to X1 to X2, it's going to be L before the threshold and L plus delta after the threshold. Right, so this is the phase space that is subdividing this rectangular region according to these thresholds. Is that part okay? Yes. And then we want to then look at each one of these rectangular regions. So now we are in phase space, and I want to know if I am in one of these rectangular regions, do I move? One of these rectangular regions, do I move to the right? Do I move up? Do I move down? Do I move left? And this corresponds to: I don't want to, we're not writing down a differential equation that we want to solve, but we're just looking at this, the rate of change of the variable xn. So if I look at the sign of this rate of change now, if the sign of the rate of change is positive, I'm moving to the right or up, and so on. If it's negative, I'm moving to the left or moving down. And since this This lambda is composed of these step functions, it is constant inside each one of these rectangular regions. So it means you just need to look at the values of x equals status. So you just need to look at the hyperplanes that correspond to the boundaries of these rectangular regions. So this gives me the signs. And then, so this is what we call the state transition graph. So we are in phase space, we have these rectangular regions, and the The transition is I move from one box to from one region to the next according to whether x1 is increasing or decreasing, x2 is increasing. So, in this example, that's for one particular set of parameter values, or maybe a domain of parameter values that give you this flow. Exactly, yeah. So, this is among these 1600 parameter regions, parameter nodes, we pick one of them. So, now we are in a one region parameter. Are in a one region parameter graph in the parameter space that I don't have value. I mean, we are not using specific values for parameters, but we have all the inequalities that determines that region. And then for that specific region, we have this kind of flow. Okay, so do I understand, right? You have a total of 14 parameters and a 14-dimensional parameter space, but that parameter space can be divided up into 1600 regions. 1600 regions that give you the total dynamic possible dynamics for this system. Exactly. Yes. Okay, but that depends, of course, on the network structure, which is the. Yeah, it depends on the network structure. So this is for this particular network. Yeah, for this particular. Okay. And we have to decide how, for example, if you are at the input to node one, you have. At the input to node one, you have x1 and x2. So you have these two switching functions, sigma one and sigma two. You need to decide how you want to combine them. Yeah. Either add or multiply. So for this particular case, we're multiplying the two when you add node one and add then at node two. That's equivalent to choosing and or or logic, I presume. Right. I think at least that's how we think of it. Yeah. So I think I understand that you are in phase space. That you are in phase space. So, then where is the parameter space? Yeah, so the parameter space is kind of hidden in here in this 1600. So, I have a so the parameter space will be all the possible values of theta and all the possible values of this L's and delta that gives me these switching functions. Now, we have determined all possible orders of these relative orders of these L's and deltas, and then also. And deltas, and then also the relative orders of theta corresponding to this. So now we have we pick one region parameter space, and one region in parameter space corresponds to one set of inequalities like this. So it means that we're not picking values for these parameters, but we're just saying I am on, I subdivide the whole parameter space into regions, and I am one of the regions where I have all these inequalities. I have all these inequalities in here. So, if we have these inequalities, then we can determine from these inequalities the sign of the rate of change just by saying if you are in one of the so now you fix one parameter region, meaning you fix one set of inequalities like this. So, I picked one of these 1600 parameter regions, and that gives me this set of inequalities that determine all the relations between the parameters. Now, if you are in one region in Now, if you are in one region in phase space, let's say you're in region four, you go in region four and you say, okay, so I am in this specific region in phase in parameter space, so I have all these inequalities. Now I want to look what happens in region four in phase space. So now in region four in phase space, I know that I am between these two thresholds on the x-axis and between these other two thresholds in the y-axis. So I know the contribution from x1 and x2 if they are L or L plus. If they are L or L plus delta. So I know in region four which polynomials I have, L1 times L1 plus delta one or something like that. So if I know that polynomial, then I can say, okay, and I look at the inequalities that gives parameter space, that gives the region parameter space. And then I can tell what's the sign of the rate of change of x1 and the same for x2. So then I know that I'm moving to the right or moving to the left or moving down. So in region 4. Left we're moving down so in region four you know that x1 is increasing and x2 is decreasing. Is that the way I should read this? Exactly, yes, yes. So if you're in region four, on both sides of the on both on both walls of region four, x1 is increasing, x2 is decreasing. And in region five, you're coming in, but what happens on the wall between region five and region eight? So there, so there I So there, so if you are in the wall between region five and region eight, in region five, the flow is decreasing. If you are inside region five at that wall, the flow is decreasing. And if you are at the top, region eight, the flow is increasing. So you can't go from five to eight because the way you formulated the lambdas, right? Basically, the and ors. Right. Right, because yeah, so the parental visual tell me you so there's a disconnect, there's a discontinuity in the flow field on the border between five and eight. If I'm a little below that, I'm going down, and if I'm a little above, I'm going up. Yeah, if you if you think of this as a flow, so that's why we don't, we don't, when you look at this this way, we don't want to look at this as an ODA. Oh, okay, don't call it a flow, call it a change, a rate of change. There's still a discontinuity, yeah. there's still a discontinuity yeah that's because of the that's because you're using i presume because you're using step functions because you're using step functions okay yeah so if you if you if you would want to look at the flow you'd have to look at the philip of system this kind of things right so there will be this kind of discontinuity there that um in this so this is the of the original dsgrn so in this in this version of the sgrn we don't try to resolve that we just leave it like that so in Like that. So, in a more recent version of the SGRN that we're still developing some parts of it, we make a finer complex where we can then resolve this kind of discontinuity. Could I say it a different way? Sorry to interrupt. Sure. I don't think anybody believes the step function is the right model. Right. It lets us do the algebra that gave the partial orders. Put any smooth function in there. Put any smooth function in there, and then that discontinuity will resolve itself. How will it resolve itself if I smooth out that discontinuity? Is the flow up or down? Ah, there will be a saddle in there. Ah, that's okay. All right. And that's where you're going to get the bicepility. Yeah. Yeah. Yeah, okay. Go to the next slide. I had a question on the next slide. Yes. Yeah. This region three looks like a node that's self-reproducing and yet it has arrows coming out of it. I thought you only put in a self-node if there weren't any arrows coming out. Yeah, maybe I should have used this graph. So this is just like a sketch of a possible directed graph. So this is not related to this, right? So in here. Not related to this, right? So in here, except you said that these self nodes are for regions that have no arrows going out. So you so you say, so is it just shouldn't have that self activation of node three? Is that the mistake? Yeah, I guess. So if I should have been more careful with the graph on the okay, that's fine. Everybody makes mistakes. So this graph does not come from the SGRN, but. Does not come from the SGRN, but like this is just like a general directed graph. So that's how you do. But yeah, in the SGRN, we don't get this kind of situation or anything like that. Okay. All right. So is so in this way we can find this recurrent region that we call the Mars sets or the Mars graph. So like in here, for this, again, for this simple network, if you take this Network, if you take this state transition graph or this multivalued map, then if we get these two Meier sets or this Mars graph with two nodes, and here, right back to the SHN land, so everything is coming in when you have this self-edged like this. And this will denote this by FP, just kind of meaning fixed point. We are not in the HN, we're not like saying that this is really a fixed point. Like saying that this is really a fixed point because they're not really doing solving the OD, we're just doing this course description of dynamic. But FP stands for fixed point. So you have these two FP at these two nodes 0, 0, and 0, 1. So these are the coordinates of these boxes. So, and we can do that. And that's for one specific region in parameter space, right? For one specific region in parameter space, yes, exactly. So we have 1600 up then. Of things, and we have this visualization too that we can use to visualize things when you have 2D and 3D networks. So, if you have, for example, another example, if you pick this three-node network, then we can compute the parameter graph. So, here is on the right is the parameter graph. So, this is. Parameter graph. So, this is all the nodes in the parameter graph. In this visualization, we can interact with this. If you click in one of the nodes in the parameter graph, we get the neighbors. And then we can see for it, if you click, if you select one of the nodes in the parameter graph, then you can see for that node in the parameter graph, what's the Mars graph and what are the Mars sets in phase space. And you can visually by clicking on the nodes in the Usually, by clicking on the nodes in the parameter graph, you can walk through paths in parameter space, which correspond to changing parameters in your system. So, and I think I'm kind of running out of time or out of time already. So, try it should be a little bit faster. So, I get to the tree to the design of switch, or I don't know, lesbi should stop. Should I keep moving? Should I keep going? Keep going. Yeah. So, and then so we can then inquire, ask questions to the SGRN. So, if you pick, for example, this one specific network, then you can say, like, you can look at all parameter regions and you can ask, can we find oscillations in this network? And then you can just look at for every possible parameter node in the parameter graph and check whether in And check whether in that parameter graph we have a Mohr stat that corresponds to an oscillation. And then you can check if in that oscillation all the variables are oscillating or some of them. So in this case, if you ask for an oscillation involving all the variables, the answer will be yes for 6,900 regions. And then one another important aspect of the SN that is really fast to compute. So you can compute this whole all the regions in the parameter graph for this networking just. Working just 30 seconds. And if you now look into one of these regions, you can, if you want, you can pick parameter values and then construct a model, let's say a hue model, and then do simulations. That's what is being shown here. So you just pick a hue model and do simulations for that hue model. How many parameters in that model? Here it's close to a million, 800,000 parameters. No regions. 800,000 regions. How many parameters? How many parameters? Oh, how many parameters? So it's just three times the number of edges plus the number of nodes. So 21, 29, 29, 29 patterns, 29 dimensional parametry space and 800,000 regions of parametri space. And can you visualize how those 6,900 regions are kind of? Related to each other in that 29 parameter space? Well, we can visualize as a parameter graph, right? So you can have this parameter graph that you can visualize and interact with it. And then if you select one of the nodes in the parameter graph, you can display, you can see all the inequalities that define that particular region parameter space. Space. And then, if you want to go to the neighboring region, you just go to the neighbor in the graph and see what's the neighboring region, and you can also get what are the inequalities that define that part of the region. So, in this sense, you can visualize them. You can see them in this way. We also know that it's contractible. We also know that, yeah, each region is contractible. The 6,904 regions they form a contractible subset of this 29-dimensional space or whatever. Okay, meaning it's one big region made up of these. It's not divided into separate regions in parameters. Okay, that's very interesting. Yeah. All right. All right, so now, but now I'm trying to go a little bit quick here. So now we want to go and we want to use DSGN now to design this bi-stable switch. So then there is the classical notion of hysteresis. Now, if you have what when you do the computation DHN, we have this discretized version of dynamics or not. And the classical DHN, the original DHN that we're looking at, it does not find. Looking at it, does not find the unstable fixed point, so you don't see the unstable part in the middle. But what we want to look for is monostability and then bistability, and then monostability again. And if you look in the bottom, so the way we encode things in the SGRN, we're only going to see things in this discrete kind of setting. So we're going to see, we want to recall that what we want to do is when you're looking at this particular design. This particular design of histories, we want to look at node zero as the input node. So that's the node where we want to change our control parameter. So, but the parameters correspond to node zero correspond to this parameter graph PG0. So then what we want to do is we want to look at this parameter graph PG0. And recall that this forms a partially ordered set. So we can move into this parameter graph from lowest node to the highest node. Lowest node to the highest node. We can try to look at for so what we do is we look for paths in this factor graph, PG0, and move in the direction from the lowest node to the highest node. So that's how we change parameters in this factor graph. And what we want to try to see is if we see this situation get shared or you have a mono stability for low if you're on the low end of this factor graph, bi stability if you're in the mid end. Biostability if you're in the middle and biostability and monistability again if you are on the high end of this of this path in parameter graph. So that's what we're looking for in the assuring. So everything is done in terms of this graph. So you just want to walk through graphs in this factor graph and see if we see this mono stability by stability and monostability again. So then and then so as I mentioned before we analyzed this for As I mentioned before, we analyzed this 14,000 networks. So the parameter space for these networks depends on the number of edges. It varies from 12 dimensional to third dimensional. And the number of regions in the parameter graph, so the number of nodes in the parameter graph varies from 27 to 93,000 medium. So it varies from very small to very large. And then with the history. And then, with the histoisis squared that we are computing, is we looking at all possible paths in this factor graph PG0, as we described in the previous slide. And from these paths, we see which one of the paths that will give us hysteresis, which one of the paths that will go from monostability to bi-stability, and then back to bi-stability for high values of the output variable. And then this quotient is. And then we, this quotient is the person, this percent or this quotient, this is the history score that we use. So, doing this, this is the history score that we present before. So, you can rank all these 14,000 networks according to this. And then, just one last thing. So, when you talk about how robust these things are, so if you pick one of these, each one of these points corresponds to one of the networks, for example, the first one corresponds to this network. And the first one corresponds to this network. One important thing is that we have this notion also when you look at these parameter regions, we have this notion of essential parameters that correspond to parameters where all the edges in the network are kind of functioning. So if there are these non-essential parameters that correspond to to essentially correspond to parameters where one of the edges in the network is kind of remove one of the edges. One of the edges. So, all this scoring is done with this essential pattern. So, meaning that if you pick this network, it means and think about parameter values for which each one of these three edges are having an effect in the dynamics of the network. So, to try to see how robust this is, we can do this kind of perturbation of this thing. So, we take this blue, which are the essential parameters. So, here we took the 14 first highest ranked networks, and in blue, it's Works. And in blue, it's the scores that we had before. The scores correspond to these essential parameters. And now we can look at neighbors of essential parameters. So it means we're allowing parameters to become non-essential going to the neighboring region. So this corresponds to kind of knocking out one of the edges of the network, possibly. And then we see that for some of them, this perturbed scores decrease. So it means that these are non-robust in this sense. So if you change the, if you move. Change the if you move in the parameter space from one region where you have a central parameter to the neighbor, you the square drops significantly. So, these are especially these first four networks that are these simple networks with just a loop, just three edges, those are highly non-robust. So, then we can look at all of these 14 networks and decide which one are the good designs. So, for example, here. Are good designs. So, for example, here, if you look at this 14, highest ranking network in the regional ranking, the first four and the last two are fragile because the scores drop significantly when you go from essential to the neighbors. The other ones, seven to ten, these are networks where some nodes they act both as a request and as an activator. We may not want to consider that. So, if you want to consider only networks that are have a high score, are robust and also have consistent edges. Have consistent edges, the consistent nodes that each node is always an activator repressor. Then these are the three net, the three best designs: 6, 11, and 12. And yeah, I think here is maybe a good place to stop. So we also did some numerical simulations to just using hill function models to kind of confirm this. So in the hue function models, if you pick on models if you pick on different values for the so these are the kind of huge function model that use with n is the exponent that is in the hue function and the the parameter we just add this parameter s into the first equation so doing this hue function simulations so we pick four of the networks and here on the bottom you see the dh n scores and on the on the other rows we see the scores that we get when we do these Hu-function simulations and of course if you take um n if you take n large equals 30 then you get scores that are closer the the simulation the simulated scores are closer to the SGRN if you take n smaller like four or closer to maybe to what a biological meaningful pattern would be then the the values do not match anymore but one important thing is that the relative rankings are still kind of maintained so the the the highest ranked network uh in the SJNs you are kind of highest ranked one Kind of the highest ranked one according to this to the simulations. And one important thing, another thing is that the simulations take a lot of time and the HGN is much faster than that. So let me stop here and we can thank you.                                                                                                                                  You kind of put a little extra vector. And if you're in some places, you're going to be still on the basis of attraction, so you're coming back to it and you reset. But in some other places, you make a whole Said, but inside of places, you may go and suddenly go to a base of attraction, a fixed point, and now you now you are right. So, I think it's again, I'm thinking about this in terms of parameters are fixed and the state space is being perturbed because if it's a kind of a temporarily, very short pulse. Got it. Got it. Yeah, no, I see what you're saying. Yeah, no, it's I'm super interested in what you're doing, Jay, because we've been thinking about doing this in the sales cycle for a while and trying to figure out exactly how to do it. Exactly how to do it. So, yeah, we should think more about this. I'm just closing with the last thought is that, you know, the challenge always here is that how do you have to translate the biology, kind of the question you have into, you know, kind of a query for this machine. A query for this machine, right? Like biostability is a clear query. A periodicity with certain phase kind of relationship is doable. But every time there's a new problem, we have to think about how to answer that question. I think that's right. And going back to Marcio's talk, you know, the reason that some of those networks were outlined with green, the question. Green. The question at hand there, the problem to solve was designing a three-node network that would exhibit biostability under all parameters or under a large number of parameter regimes. So he really wanted that thing to operate under a lot of different growth conditions and to also be able to build it practically. To also be able to build it practically. So that whole analysis had a very precise question underlying. And I think that's why some of those networks were highlighted while others weren't highlighted. We can build a network where a node acts as an activator and a repressor. It's just you're limited. I think that was network five or six that you. Five that or six that you kind of left out there. But I agree, Tomas. I think understanding the real questions that you want to ask, Jay, is the most important. And then being able to define those in terms that DSGRN can be used to answer them, or at least query them, or maybe get rid of just reduce the hypothesis space is one of the things I found the most valuable about this tool. This tool. Here's a set of experiments that are unlikely to work at all because my experimental space is usually larger than my bank account space. Yeah, I'd have to second that because the problem we have in regulation is we're getting better at predicting what's regulated, but we can't predict who's regulating it. And if we could narrow And if we could narrow down the hypothesis space about who's doing the regulation, that would help tremendously. Yeah, we'll talk some more about that tomorrow. Yeah, and let me just follow up on, and maybe it was just your phrasing, Jay, but you kind of said, oh, you immediately start about. Said, oh, you immediately start about what you could do. And I always think that doing biology is much harder than doing computations. And so, you know, the way I would think that this would start is that you say, oh, well, here are some possible networks, kind of like what John was doing. Here's what I would like to see out of this. Forget about the experiment. Maybe I'm thinking about it the wrong way. Forget about the experiments yet. About the experiments yet, and then say, Can you show us this? We could either say, Yes, this is something we can try to show you or not show you. We could do the computations. Those will be cheap. And then we can decide whether that's something that one should try to think about how one would approach experimentally, and maybe even look at it differently from the experimental perspective. But yeah, you know. Yeah, you know, running cycles on a cluster is, I think, much cheaper and faster. This is part of my answer to Leon, right? I mean, these computations are so fast that even stopping to think about how you would do it to make it faster often takes longer than it does to do the computations. And so, you know, it's we should take advantage of that. Take advantage of that. Taking notes as we talk here. Any other discussion folks want to have? This is this is good. This was very good. Jay and Jennifer, I have a very circadian-specific thing that would be great to chat about that we can do offline or here or whatever. It's up to you if you have a little time. I have a hard stop at 4 p.m. Eastern, but Eastern, but let's do it now. Cool, cool. So it's a bit of a wacky idea, but I think as you both know, and Tomas and Constantine and I have talked about this, we've been really interested in the budding yeast cell cycle and trying to understand, as you know, that we're feeling that there's pretty good evidence. That there's pretty good evidence for the central oscillating mechanism being transcriptional, very much like a circadian oscillator. As I'm watching people put topologies up, and I'm looking at, and I sent Tomas a direct message, but he isn't watching the chat. But as I start looking at topologies for human and what you've been putting up for Neurospira and And stuff that I've seen from Tina, I have this sense that the topological features of what we're seeing for the yeast cell cycle network are very eerily similar to what you're seeing for the topological features for the circadian clock. And in my wild, wacky world, I have this idea that they may have evolved from some common ancestor. Common ancestor in thinking really in a wacky way, since we're at a small meeting here. I could imagine way back when cells were crawling out of the primordial ooze and we didn't have good atmosphere that you would want to replicate your DNA or divide maybe at night when I'm not getting bombarded by radiation. I'm going to collect energy. Marred by radiation, I'm going to collect energy during the day. And so that you had this coupling between cell division and monitoring light-dark cycles. And then eventually, as we got a better atmosphere and you weren't being bombarded and you could collect light and do things better, as you went, that maybe what would be selected is your ability to divide faster, maybe divide twice. Maybe divide twice with one collection of energy, and so that maybe you split off those capabilities: one to follow light-dark cycle, one to actually do cell division. So that's like a kind of wacky biology, but remarkably, and I don't have the slides right now, but I'm happy to pull them up. And during the Cold Spring Harbor Circadian meeting, I was trying to write down networks in the way I write them down, which is temp. Way I write them down, which is temporally. And even the components are very interesting, right? So, white collar looks a lot like what we call SBF, which has a lot of inputs and outputs. The pulse of transcription starts at SBF. It turns on its own repressors that come back and truncate the activation signal into a pulse. And then at the very end of the cycle, those repressors. And then at the very end of the cycle, that repression sits on these complexes. There's actually layers of it that are put on. And at the very end of the cycle, there's a kinase and an E3 ubiquitin ligase that remove all of the repression that restarts that cycle at the end. And I'm thinking about, you know, casein kinase and the ubiquitin ligases at the end. It just seems like a It just seems like a really amazing coincidence that there's so many similarities between these network topologies. So maybe I'll put together a slide and we can just hold them up next to each other. But as I'm looking at white collar, I sent Tamas a message that said it looks, you know, white collar looks a lot like SBF. It's kind of central to the beginning of that network. So maybe, yeah, I'll. So, maybe, yeah, I'll pull together a slide and we can talk about it offline. But this is something I've been chewing on for about two years now and think that this is something we should really think about. Because right now, the circadian folks are over here, the cell cycle folks are over here. And I think in many ways, we're looking at the same things and we should be helping each other. And the networks really look really similar. Did I pull that together? I found the slide. Did I pull it? I told you I found the slide. Oh, do you have it? Yeah, put it up. Do you have it written? Do you have it written on ah, there you go. Right, so the complex in budding yeast actually duplicates its genome. So there's kind of parallel pathways there through SBF and MBF. And basically, at the beginning of the cycle, those things get turned on. We've unpacked that activated. We've unpacked that activator, that black box we've unpacked. But SBF has that self-loop on it. It has several different layers of repression coming from kinases and from transcription factors. What we think happens is SBF turns on, that negative feedback from YOX1, YHP1, and RM1 truncates that signal into a pulse because we've actually shown if you make Because we've actually shown if you make the deletion of all of those, the system goes to all on. Hmm. Interesting. All right. So we've actually shown that the repression comes from those particular nodes. So if you turn those on, it goes to all on. And then we think that pulse then gets transferred along this wavetrain that goes through HCM1, SFF, ACE2, and it's turning on phase-specific genes as it goes. Specific genes as it goes. And that's how you get all the phase specificity, this kind of dominoes effect that we see in the cell division cycle. And then at the very end, APC, that blue box, is activated by the kinase. APC is the E3 ubiquitin ligase. And notice that it's killing two of the repressors, probably three actually. YHP1, NRM1, and probably. YHP1, NRM1, and probably YOX1. And as well, it kills CLIB2, which is also a repressor on SBF. And that signals the end of the cycle. That doesn't happen until the end of the cell cycle. That's how cells know they're done. And then you just reset that clock so you're ready to fire again. But in many ways, I think the topology has some eerie similarities to the topologies I'm seeing. Uh, the topologies I'm seeing you draw. Yeah, certainly some of those sub-modules do. So, so you're saying that all of the cell cycle phasing is really has to do with the kinetics of transcription and translation for those intermediate nodes, the HCM1, SFF, and ACE2. Yep, they all have edges coming out very much in the same way you had Whitecross turning on other transcription factors. They're turning on a whole bunch of. Factors, they're turning on a whole bunch of other things. And in fact, we can delete some of those and not have a big problem. That SFF node is completely essential. But I think, yeah, the phasing that we see for S-phase genes and G1S genes and mitotic genes is largely controlled by different complexes. So SFF is controlling all the G2M complexes. G2M complexes, HCM1 plum2 TOS4 controlling the S phase genes, SBF and MBF controlling G1 and G1S genes. So some of them are just absolutely direct. What we know is we can stop the cell cycle, and this is the experiment that's haunted me for 15 years. We can stop the cell cycle by getting rid of cyclins. So the CDKs are the effectors. So the CDKs are the effectors of this network turning on S-phase and mitosis. So we can get rid of the cyclin CDKs, block the cells in G1, and this whole transcriptional cycle will continue. And the cell cycle progresses, or just the transcriptional program? The transcriptional cycle progresses, but the cell cycle is halted. So it seems like then the transcriptional cycle is maybe involved in, but not it's certainly not sufficient. So we think it's functioning as the underlying oscillator, but it doesn't, without the CDKs, you don't have the effectors to trigger DNA replication, mitosis, and all those kinds of things. So if I add back, for example, and we've done this experiment, if I get rid of the mitotic If I get rid of the mitotic cyclins, I can block the cells from entering mitosis, but I give an S-phase cyclin back, I will do round after round of S-phase without ever going through mitosis. So the cyclins are really effectors of this oscillation. So how do the cyclins receive input from this? Transcriptionally. They're turned on transcriptionally. Okay, sorry. So they are the direct output of. So they are the direct output of HCM1. They are the direct output of SBF, SFF, and HCM1 Plum2 TOSPOR. So Steve, when you say they're stuck in S or they cycle through S phase, so they're going from S to G2 back to G1? No. No, they're basically doing endocycles, Bill. So they will replicate a complete genome and then pause and then replicate another genome. So they'll go through, they'll do complete genome increments of replication. But they don't divide. They don't divide. What they do actually, and we give them their G1 cyclins, they'll continue to make buds. They'll continue to make buds. So they'll actually make bud after bud after bud after bud, and all those buds come out on schedule exactly how they would if the cells were actually going through cell division. But they never do cytokinesis, they never do mitosis because they don't have the right cyclins. So assuming that transcription is your timer, so Luis Tarondo has done network reworking to show that how a network Show that how a network works, and you can expand it in terms of clock context. So, if you were to add in another loop or somehow extend the time to make SFF, would you see the length of time to progress through the cell cycle length then? Have you ever done anything like that to show it's directly acting as a timer? Yeah, we've never, um, we've never added any, we've never added any. Added any extra nodes, but what we have done is unlike circadian oscillations, the cell cycle is very, very responsive to temperature. So you'll start down at cold temperatures, you'll start with a long period. You get to a maximumly short, or you get to an optimal temperature where the period's the fastest, and then they'll. Where the period's the fastest, and then they'll get heat-stressed and go slow again. If you eliminate the cyclin, so I can draw that graph with normal cell division. If I eliminate the cyclin, so I get rid of cell division, because what we thought is, oh, maybe mitosis is temperature-sensitive or DNA replication is temperature-sensitive, something enzymatic. If I get rid of the cell cycle and I, by getting rid of the cyclins, and I By getting rid of the cyclins, and I draw that curve again, it's the same for the oscillations that we're measuring in the absence of the cell cycle. So the transcriptional oscillations. So that suggests that that transcriptional apparatus or whatever's left after I get rid of the cell cycle is still following the same rules. So I draw the exact same curve. Teve, I have your paper, the PPT from 2016. The PPT from 2016. Oh, yeah, that's gosh, that's old. Yeah. So, oh, there's penis stuff there. Yeah, can you play? Will that movie play? Is this a movie? Yeah, that no, it looks like it won't. I'll see if I can pull something together and we'll, if folks are interested, we can talk about it after one of the other sessions. But yeah, this is something change the period length of the oscillator by. By you'd predict by changing the stability, I guess, of those intermediate things. The intermediate nodes, the HCM or... Right. Yeah. So we've, as I said, we haven't messed with stability of those. But what we have seen, this is the other thing, yeah, thanks for showing that Tomas. What we have seen, which is really interesting, when you start Contrasting, when you start making deletions in that network, you don't see a loss of oscillation. What you do see is a change in period. That's the other thing that suggests that we're in the timing mechanism. Can you go back to that graph? Yeah. How long is the cell cycle in this case? Cell division cycle under best case conditions is about 70 minutes. So, this, it seems like if Seems like if what you're thinking is that I would almost think of it as they added a piece in the circadian cycle to extend it, because the current circadian cycle is a transcriptual programming network, plus a negative feedback, and that negative feedback is extended because the negative arm has this really bizarre and long half-life. So it seems like instead of the cell cycle and the circadian cycle being or the cycle The circadian cycle being, or the circadian cycle being first, or anything, it almost seems like the cell cycle is what's been standard, and we co-opted it and pulled it out for circadian work and then just added in that negative arm. Yeah, it could be. And, you know, at this point, I'm way out on a limb suggesting they might have evolved from a common ancestor, let alone say who came first or how that actually happened. But I like your idea. And it's the interesting thing. And it's the interesting thing. Can you go back to that graph, Tomas? The interesting thing we found in this analysis is: if all the deletions made the cells go slower, you could just say they were sick. Some of them went faster. Some go slower, some go faster. So we found that very interesting. I would have assumed that pulling out a node would have made them all go faster. This isn't pulling out a node, I guess. This is deleting. Pulling out a node, I guess. This is deleting, not yeah. And a lot of these, and a lot of these nodes have redundant nodes that go with them, right? So, YAX1, YHP are similar. And then there's another node, NARM1, that's probably similar to that one. We had to get rid of all three to make the system go to all-on. Actually, we had to get rid of CLIB2 as well. So, we had to get rid of four repressors to make the system go to all-on. So yeah, no, I think it's something I've been very interested in and as well. I actually wrote down a table. I'll see if I can find it. I had it on my board where I wrote down all of the kinds of features that I could for a circadian clock and for what we've been studying cell cycle. And they share a whole bunch of features right down to the actual network. The actual network. And so I've been toying with writing an opinion piece about this because I really think that two groups should be talking more than they do. The pictures which you always show, and they look very similar to what we have seen from Jay and Jennifer today, right? Yeah, we talked about this years ago, Steve. Yeah. Yeah, no, we did. This is, so that's data from 2008. So that's data from 2008, and on the left is the wild-type transcriptional program, on the right is the one where the cell cycle is completely halted. And mostly the cells think that they're still cycling. In that there's a movie above here where the cells are actually making bud after bud, and they come out on schedule. Yeah, unfortunately, you don't have that movie. Oh, yeah. And that was an observation I made. And that was an observation I made back in 99 when I was a postdoc. So we've been chewing on this for a long time. And that's actually Tina's data, right? The proteome data looks the same. Oh, there's the, there's the, can you hit 13, slide 13? So this is data from cells where Data from cells where we've gotten rid of the mitotic cyclins, but we left the S-phase cyclins. What am I doing? Sorry. Yeah, I don't know what you're doing. Yeah, you're doing some animations. So just leave it right there, Tomas. So at zero, that's the haploid yeast. They go through S-phase starting at a minute or hour or 120 minutes. They finish one S phase, they pause for a bit. At 300, they Pause for a bit at 300. They start another S phase, and that lines up. Can you go one slide previous? I'm like, okay, I'm playing it. This is PowerPoint. Go one slide previous. Just backwards arrow. How does it do it? Or that. I'm playing the. I don't know why. This is your first PowerPoint experience, Toronto. Basically. It's your first Zoom meeting and your first PowerPoint. You're doing. And your first PowerPoint, you're doing really well. Is this right? This is what you want? I don't know. You're not sharing it. You got to share it. Oh, I'm not sharing it. Sorry. Let's see. That's... Oh, I don't see the actual. Sorry. Yeah, the data is not there, but those points at which you see them enter S-base. Which you see them enter S phase, that's where you see the S phase cyclin being turned on by the transcriptional oscillation. So they actually enter S phase as soon as you turn on the S phase cyclin, which is super cool. And as well, those green dots are actually GFP tag spindle pole proteins. So those are, you're actually re-duplicating your spindle poles as well. Spindle poles as well. So they go through round after round of spindle pole duplication. If I give them their mitotic cyclin back, I have a multipolar spindle and they'll just turn that thing into Swiss cheese. That nucleus just comes apart in pieces. But it is true, and this is one of the things that we're working on with Tomas and Konstantin right now, is that there are systems where endoreduplication is part of their development. Part of their development. And we now have a mechanism based on that network for endo-reduplication, which is kind of cool. So the idea that the G1 cyclins drive, the G1 drives G2 and G2 drives S, and S goes back to drive G1. Back to drive G1. You're saying that's not an independent redundant cycle, but it's sort of an epiphenomenon. It is. That's correct. This idea that, so for a while, and it really comes out of the people studying E2FRB, the idea was that cyclins were controlling transcription. And that the cell cycle was a cyclin cycle. The cell cycle is a cyclin cycle, and that cyclin cycle is controlling the transcriptional cycle. There's no doubt that cyclins feed back and phosphorylate transcription factors and regulate their activity as well as their stability. So a lot of those transcription factors are SCF targets, which is a ubiquitin ligase that only recognizes phosphorylated targets. So But the studies where we got rid of the CDK oscillations and show that you can still get a robust transcriptional oscillation really kind of nail it down as something that's transcriptional. And by the way, you can stop them either with... Tomas, can you scroll down in that presentation? So we did these experiments by getting rid of the cyclins. Getting rid of the cyclins. We did another experiment. Can you keep scrolling down? Zero C. Ah, that's not it. Keep going. I don't want to talk about checkpoints. There it is. That one right there. So on the right, we deleted six different cyclins. We still see transcriptional cycles. Not completely wild type, but very much so. In the middle is a CDC20 deletion. That's the Deletion. That's the deletion of the E3 ubiquitin ligase at the end of that network, so at the end of the cycle. Those cells arrest with elevated CDK levels, not reduced. So they can't get out of mitosis because they can't get rid of their CDK, their mitotic CDK. We still see transcription oscillating even in this completely opposing condition. So it really suggests that... So, it really suggests that the dominant oscillation here is probably transcriptional. There are SCF finding sites on Neurospira Creek. Ah, yeah. And Jay has also shown that if you eliminate degradation of frequency, the clock still runs. Yep. Interesting. So, yeah, they're similar. So, yeah, they're similar. It just seems like an extra piece got added into the circadian cycle to elongate it. Yeah, it could be. It could be. But the fact that you've got, you know, kinases and ubiquitin ligases at the end removing repression, right? I mean, what we've proposed is that this whole network serves as a pulse generator that transcripts, that generates a transcriptional pulse at the beginning of the cycle. That starts turning on all the genes you need to do. Turning on all the genes you need to do the cell cycle, so all the effectors, including cyclins, and that pulse gets all the way to the end and triggers that ubiquitin ligase that then goes and removes the repression, basically recocks the gun. So when you get back into G1, you're ready to fire again. So that's, would that be considered a positive feedback loop? I mean, how would you define it? how would you how would you define that in that yeah i i i always think of it you know so so there's a very uh interesting mechanism that came up for how cells limit dna replication to once per cell cycle it's called the licensing model and it's really a two-step model you have to build a complex at a replication origin but that's not sufficient for firing then you need a kinase to come in phosphorylate that Phosphorylate that complex, and that triggers replication. I think it's the same way, right? You have to kind of, as soon as you fire this gun, repression goes on and keeps it from firing again. So you only get one transcriptional wave. And then you remove that repression at the very end, and that's done by APC. That's exactly what happens for DNA replication licensing. So it's almost licensing for transmission. So, it's almost licensing for transcription, which is really interesting. This is one of the reasons, Jay, I've wanted to put in another pulse of transcription after the first one goes by. Can I add another one and make them do some wacky stuff if I have two pulses going through the cycle at the same time? So, that's why we've been thinking about which phase to put that pulse in. Are you not considering that? This is more complicated than the complicated than the circadian because the phase in the circadian clock is all about really it's about the phosphorylation status of of the negative elements whereas phase in this model it's been passed off It's been passed off from SBF to the different modules, to the HCM module, to the SFF module, and so on. So it's more complicated than you have things that you can't retrieve. You can't retrieve control once you passed it off. Yeah, no, I think that's right. And I think, you know, there's more, this is one of the other reasons I was interested. This is one of the other reasons I was interested in how much feedback you were seeing from these kind of tier one transcription factors. Because one way to think about it is that HTM1 and SFF are really tier one, tier two transcription factors, but they're still feeding back, right? Because they're involved in removing that repression in some way. So this is one of the things I got out of this discussion that one That one central part of the module that Tina's put together were white collar drives CSP1, which is a repressor, white collar drives sub one, which in turn drives white collar back again, and white collar activates itself. That entire module really just makes a pulse. And the feedback amplifies the size of the pulse, but it just creates a pulse that then. Creates a pulse that then is passed on to other things. Yep, as I was looking at that, that's what kind of got me thinking about this all again as I was looking at that diagram and thinking how much that looked like. So what we end up doing in a lot of our models is taking that SBF box and that MBF box and putting them together because they really do similar things. they they they really do similar things and they're they're they're they they have similar binding sites they're they're they can actually i can get rid of sbf and mbf will take over and do everything sbf does i'm just going to say you could delete looks like you could delete mbf altogether and you'd still get a you can you can and uh and the cells are are a little slow on some things it appears that mbf seems to control dna replication genes and sbf controls And SBF controls budding genes and spindle genes, some of the spindle pole genes. So, yeah, so we usually end up boxing those and then the networks look a lot more similar. And then APC is that ubiquitin ligase, and CLIB2 is the major mitotic kinase. Steve, is the SBF and MBF, are they related from this duplication or are they completely? No, they're very related. In fact, those are both complexes and they share one component. So they're very, very, very similar. So one of them is a complex of SWI4 and SWY6, and the other is a complex of MBP1 and SWI6. So they're very similar, and NRM1 only inhibits MBF because it binds directly to it, whereas YOX1 and YHP1 have their own binding sites in SBF-regulated genes. So at any rate, I just wanted, while I had you guys in a small room, I wanted to throw out my wacky idea for you and idea for you and see if you laughed me off the off of the Zoom call. So I guess your question is whether the tier three transcription factors in circadian rhythm are like affecting the oscillation state of the negative arm. If I got my yeah, I mean that would be one interesting way to think about it. That would be your APC, right? Or are there ways that we Or are there ways that we can box nodes and make these? So, one of the things that we talk a lot about with our crew is, you know, how similar do these networks look if we start boxing nodes and forgetting about the alphabet soup in the boxes? And, you know, to where we can make simplifications. How similar do they look? And we're actually doing that for a number of different fungal species. Fungal species for this particular network, and so far they're pretty similar. And then, what we do if we come up with a similar topology, we can use proxies for all the genes in DSGRN and ask whether we get the right ordering and we get the right dynamics and use data that are specific for all these different organisms. We could think about doing something similar if we could get these networks to look. Get these networks to look similar in some way with a circadian network. That might be an interesting exercise. Well, thanks for listening. And maybe we can chew on this and really I think it would be super cool if we could actually connect these two groups. Could actually connect these two groups and start thinking about this. We're trying to build similar networks now in human cells. There are already topological similarities that are very clear. So E2F is SBF and RB is We5. And if you go back in the fungal lineages to the chytrids, you can actually find organisms that have. Organisms that have both SBF and E2F, they'll actually bind each other's binding sites. You can put E2F in budding yeast and it'll bind to the SBF sites, which is super cool. And the thought is that what happened during evolution is that E2F RB was probably the ancestral and that SBF. That SBF and WE5 probably came in on a virus, and that in the chytrids you had both, and then they got kicked out in the Saccharomyces lineages. The E2F and RBs got kicked out. So it was kind of like what you were saying today, Jennifer, that the topologies look similar. They're just replacing the genes, right? So all you're doing is replacing the identities of the genes. Identities of the genes. So, I guess the question is: can you distill all timing mechanisms in biology to a basic setup? It's a great question. And maybe you can't, but maybe there are set that are more related than we've thought. That's what I can say with a little more of a straight face is that, yeah, I think these two things might actually be related in. Actually, be related in ways that we haven't appreciated. And I think that thinking of them as an oscillator and not worrying about whether they're temperature compensated and all these other features of circadian clocks, I think those are specific for the circadian clock job, but not required of the basic oscillator part. Joe, I saw you came off mute. Jim, I saw you came off mute. Yeah, I totally agree with your thoughts on this because, you know, as I'm developing these metabolic models for circadian system, I keep coming back to cell cycle phenomena. And so it definitely makes me believe that, you know, circadian system really is to help tune the cell cycle to the changes, periodic changes in the environment. Changes in the environment and such. And so, if that's the case, then what you're saying makes total sense. So, there is at least one parameter regime under which I make sense. Oh, I also have a question for you on energy generation in the cell cycle. Does oxidative phosphorylation or the clockwise operation of the TCA cycle, does that tend to occur more in one? Occur more in one stage of the cell cycle than another? Yeah, no, it's a great question. And I think the real problem, Bill, is that we tend to grow our cells in 2% glucose in rich media, which is not a normal place for yeast to actually live. What's really interesting, so if you grow budding yeast cells in a chemostat under limiting carbon, they start doing They start doing a they synchronize in their cell cycle and they start doing what's called what's been called yeast metabolic cycles. And if you measure dissolved oxygen, it's going up and down and up and down. And what you see is a gigantic transcriptional program that goes with that, but it looks like a frustration oscillator. So there's only two phases: it goes like this. Two phases. It goes like this, and then it goes like this. And they're completely tuned to the cell division cycle period. Yeast metabolic cycle? Oh, yeah. Yeah, if you could send me some links on that. Oh, yeah. No, there's a bunch of it for me. Yeah, there's a bunch of them. And we're studying this in my lab right now, trying to figure out the connection because we were able, we did the kind of opposite experiment. So we took batch-grown cells. Batch-grown cells, and we synchronize them in the cell cycle, and then we release them under conditions in which the cells were metabolically perturbed, and we could see metabolic cycles. Right? And we could also change the linkage number. So sometimes it's one to one in terms of period for metabolic cycle and cell cycle. Sometimes it's two to one. So that's something we haven't really wrapped our head around, but um. Our head around, but at any rate, yeah, no, I will hit you with some literature. There's a lot of it. We work very closely. David Botstein was working on this and Neg Wingreen for a while. Also, Steve McKnight, right? Steve McKnight did it originally at UT Southwestern, and there was a guy named Bob Clevitz at University of Good Hope in Duarket. Good Hope and Duarte, who passed away. Oh, yeah, right. Yeah, and he also saw the same phenomenon. So, at any rate, yeah, no, we're actually looking at that. And it turns out these genes that are oscillating are also involved in the generalized stress response. So, it's almost that growing slowly is stress-responsive. As well, the bunning yeast is storing a very particular type. Storing a very particular type of carbohydrate during these cycles. It's called trailose. And they don't store a lot of glycogen. They store things as trailose. So one idea is they're sitting there building up their trailose as much as they can, and then they burn it all at once to make enough ATP to get all the way through the cell cycle. Because you don't want to stop in S-phase or somewhere. An S phase or somewhere. So there's some kind of trigger, they burn up all their trailose, they make a bunch of ATP, they get through the cell cycle, and then they sit in G1 again for a little while. So that's one of the ideas for how you get these metabolic cycles linked to cell cycles. That's fascinating. We could run a whole meeting on that. Folks, I got to get going, which means I can stop the recording, I think.