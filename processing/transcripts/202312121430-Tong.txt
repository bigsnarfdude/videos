So, why did we do this research? Well, broadly speaking, I think the conflict between economic efficiency and social equality is the fundamental problem of social science and economics. But as a statistician, we don't directly tackle such large problems. We find a small enough place. A small enough place to look at a big problem. And this split small place is algorithmic variance, which gained some attention in recent years because now the algorithmic dictated decisions make people realize its consequences are large, like long applications, screening of CVs, etc. So our lives are indeed impacted by those algorithmic decisions. Impacted by those algorithmic decisions. So the fairness issue naturally was brought up. Okay. Then I looked at the literature and thought, maybe there is something that we still want to emphasize on, the efficiency part. We cannot let go of the efficiency and just talk about fairness. It's not sustainable in actually every context. Okay. Context. Okay, so today I'm going to talk about Neumann Pearson, which I will be more specific in a moment, and then equal opportunity. Equal opportunity is one of the popular concepts in algorithmic fairness. So in this talk, we combine them and we claim that if we consider both things simultaneously, it's a way to address efficiency and fairness together. Together, okay, but of course, Nehemian Pearson is just one way to look at efficiency and equal opportunity, it's one of many, many, many agreement fairness criteria. Okay, so we just throw one thing there and maybe some of you can come up with something even better. This is a work jointly with my PhD advisor, Jian Chin Fan. It's the first project I collaborated with Professor Fang after I graduated. And then the third author. And then the third author like this. Oh fancy. Okay, so the bigger machine is. All right. No, no, no. Okay, the third author was our colleague at Marshall School of Business in the Econom Department, and now he's in the University of Columbia. He's in the University of Hong Kong. The last author was my former student. And well, this was his job market paper last year. Now he's in Hong Kong Baptist University with the mathematics. The whole set of slides were written by the last author, Shunan Yao. What I did last night was to delete the background of the first page. Okay, so all credits are Shunan's, but Are Shunans, but all errors are also Shunans. Okay. All right. Oh, I also edited my department because Shuna was in the department of mathematics. Okay, so well, these things are in the news, algorithmic bias. And then we want to use a specific example to motivate our Our choice of Neiman Pearson and equal opportunity. So, specifically, let's talk about loan applications. The traditional way to get along was you go to a bank, you talk to someone, and by looking at you in the eyes, they judge your authenticity. They sort of gouge you whether you have any willingness and capacity to repay your loans. But that day passed. Okay, it was considered as inefficient. Okay, it was considered as inefficient and objective. Oh no, sorry, subjective. Okay, and then the algorithm makes decisions overall, but it makes some headline news actually by making bad decisions. Okay, so the decisions have some problems. For example, now if you want to apply for a credit card. You want to apply for a credit card or a loan? The applicants from every race and gender should have an equal probability of receiving credit cards. Okay. Sort of a reasonable thing, right? Given that somebody is qualified, the probability of rejecting him or her shouldn't depend on the gender or the race or something like that. Okay. And then on the other hand, is this the Is this the banks really care? Actually, no, the banks care about profit and survival. Okay, so first of all, they want to control the financial risk. That has to do with every survival of the institution. It's the number one goal of them. And given that, they want to make as much money as possible. Both reasonable. So then the institutional interest and the Institutional interests and the societal concerns are at odds. Okay, the societal concerns can be regulated by certain governments or agencies. But if the regulation is so harsh that the institutes actually cannot afford it, then they will find a way to escape from these regulatory measures. So the problem is, how do people balance these codes? In this paper, In this paper, we propose one solution. Although we haven't convinced the bank to accept it yet, but somewhere I think maybe we can do it. Okay, so continue with the example. This is also quite normal in the so-called fairness literature. Usually, when you use X to pre When you use x to predict the y, y is the outcome variable, x, the covariance, attributes, predictors, or whatever you call it. Okay. In the fairness literature, you're going to decouple x into two parts. One part is still called x, the other part is called s. Okay, the s part is a so-called sensitive attribute. Okay, this is not to say that everything here is not sensitive. Is not sensitive. It's just that in the particular problem you look at, you don't focus on that variable. Okay, so for example, here gender might be the sensitive attribute, or here it might be the race. Of course, you can consider multiple sensitive attributes at the same time, or a sensitive attribute with multiple levels beyond two. But for most literature and for most Literature and from most actual social applications. So far, people talk about this binary sensitive attribute. Okay, so for our notations, we let this sensitive attribute S to take two values, either little A or little B. Okay, you can think of them as men, women. Okay, and this Y outcome variable is binary. In particular, in this talk, I want to. In this talk, I want you to think of zero as default. Okay, somebody gets along and he defaults. That's the zero status. And here you have y equals one. This means non-default. All right. And then when a new applicant walks into the bank, based on these so-called mutual attributes and whatever you want to predict. Okay. Okay, so if somebody is predicted to be non-defaulting style, then you want to run him or her along, and otherwise you don't. Okay, and then you certainly have some past records to build your algorithm on. Okay, I hope the story is clear. Yes, it's sure. Yes, well, I don't Yes, yes. Okay, so yeah, actually, the first version of the slide has a different sensitive attribute, but in the US it was so sensitive that we will switch to this janky. Yeah, but good comment. Okay, well, fairness in machine learning, well, there are a lot of the so-called fairness criterion, and maybe. So-called fairness criterion, and many of them are at odds with each other. It's really your choice for a particular application. The so-called equal opportunity arises from the last one, the equalize odds. So basically, it means conditioning on the labels and the sensitive attribute, the missed classification probabilities should be equal. All right, and then what's the equalize opportunity? Sorry, what's the equal opportunity? Equal opportunity is just taking one of these two equations. Actually, it means in our specific example, it means given that somebody is qualified for a loan, then the probability of denying this person shouldn't depend on the sensitive attribute. On the sensitive attribute. That is, the probabilities of denial should be close, or in this case, equal for men and women. And also, there are many algorithmic ways to achieve it. In our particular talk, we do the post-processing kind of ways. Okay, we just first train a scoring function, whatever it is, and then we Function, whatever it is, and then we try to fix the threshold so as to achieve the inequalities we wish to achieve. Okay, and then we need a bit of more notation. Yeah, next time I guess we need a training to carry this heavy machinery. Now, pipeline error is the probability of making mistakes. Probability of making mistakes given that the label is zero. Okay, so for that example, it means the probability to issue credit card to unqualified applicants. Okay, because zero means default. Type 2 error, R1, means this. It's the probability of not issuing credit card to qualified applicants. And then, And then everything is conditioned on this classifier. Okay. So even though you have a probability here, the probability is only taken care of these guys. Okay, so therefore this thing is still random. R0 of V hat and R1 of V hat, they are both random because inherit the randomness from the training data. Randomness from the training data. All right, so there is what we call the hyper-narrow disparity. So you look at this, okay? Then when you replace y by the zero, it's the groupwise type 1 error for men or groupwise type 2 error for women. Two narrow for women. And then the L0 is the discrepancy between these two types of narrows. Then for the type 2, you can define it similarly. So for the societal concern, we think the type 2 error disparity is more important because it means. It means the difference of probabilities that qualified applicants from group A and B are not even credit cards. Okay, so it's the probability of rejecting qualified applicants. Okay, and the difference of that, it should be small given that they are both qualified. Okay, and then The equalize odds actually demands both type 1 error disparity and type 2 error disparity to be zero. Okay, yeah, this is one criterion, but it's not the one that we use for this project. We drop the L0 card. Okay, given that you are not qualified, then whether you are given the credit card or not is at least not of a societal concern, it's the institutions. It's the institution's concern, perhaps, but society doesn't concern about unqualified people that much. Okay, and moreover, it's stringent. If you have all these constraints, your algorithm has less freedom. Now, this relaxed equal opportunity comes from. Equal opportunity constraint is actually what we use for this project. It says the type final disparity should be small. It doesn't demand to be zero exactly because we are training algorithms based on finite sample. And with finite sample, it's hard to impose zero equal to zero exactly. So we relax a little bit. And well, this was not a And well, this was not our invention. Somebody else worked on this before. And then for the bank side, okay, so suppose the bank is not concerning the so-called fairness. What does it want? First, as we said, it wants to control the financial risk. Okay? Okay, which translates to do not classify too many defaults as non-default. Okay, and then this also means that you want to control this R of zero in our notation carpentrier. And then meanwhile, you want to identify as many as possible the dump defaulters so that you expand your business. You want to do more business, of course. You want to do more business, of course, otherwise, you don't make money. And that translates to maximize one minus the type two error, or in other words, you want to minimize the type two error. Okay. So, but then there is a subtlety here for the financial institute survival. Survival is the most important thing. Okay, but of course, in practice, some financial institutions failed because they were too greedy. If it's run by a sensible CEO, then financial risk is the number one thing one should consider about. And after you do that, you'd make more profit. Okay, so this is what we think. What we think on the financial institution side should adopt. Okay, so basically, control the more important type of an error under some user-specified level alpha. Alpha is a small number chosen by the financial institute. And under that, minimize the type 2 error. Okay, meaning controlling the financial risk and then. Controlling the financial risk and then expanding business as much as possible. Well, and then when you add the so-called fairness, in particular the equal opportunity constraint from the societal constraint, we just add one more constraint. Okay, so this constraint L1 smaller or equal to epsilon, where epsilon is demanded by the society. Epsom is demanded by the society or some regulatory agencies or governmental whatever. Okay. And then the efficiency part is handled by the R0 smaller equal to alpha as well as the objective function. Okay, so overall, we call this new constraint optimization program the MPEO paradigm. MPEO paradigm, which stands for Neyman Pearson and Equal Opportunity. And the word Neyman Pearson borrows from the Neyman-Pearson paradigm for hypothesis testing. But this is classification. It's something that I have been working on for the last 15 years, little by little. And then, okay, if you endorse something like that, at least as one solution to the conflict between efficiency and fairness, then the first question we need to ask. The generalized Neyman Pearson Nemo Okay. Okay. So assuming that the density exists, basically the MPO oracle has two parts. Uh can we have the slides? Thank you. Okay, cool. Yeah, so basically, this is the density ratio of the two classes for group A thresholding at some point. At some point. And this is the density ratio for the two classes conditioning on s equals b. The form is as simple as that. But of course, usually there's no closed form formula for the two thresholds. Okay, this is unlike the Bayes classifier for what I call the classical paradigm, where the goal is to minimize the overall classification. Always to minimize the overall classification error. Okay, sure. So, how much time do I have? No time? Okay, sure. Seven minutes Okay, sure. Sure. Thanks for the reminder. And yeah, I guess the story is maybe a little too long, but anyway, so this is the oracle, which is population level. If you want to build a classifier, it's many different ways. One way is to say put up some smoothest assumptions. Smoothness assumptions, do non-parametric estimates on this and then try to figure out the thresholds. Okay, so for that, I think one paper can be made. Another way to proceed is to put on some parametric assumption and then try to work out these thresholds. Okay, something that another student is doing and it's almost up. But today, we do a different kind of A different kind of approach. We'll do a distribution-free approach. Now, this is just a scoring function. The threshold, sorry, the density ratios are in general threshold, sorry, are can be thought of as scoring functions. So, I just need to come up with some scoring functions and then make some thresholds. Some thresholds. And the scoring functions could come from logistic regression, neural network, support vector functions, or whatever. And then, whatever that algorithm is, I want a unified way via order statistics to come up with this threshold. Okay. And to save time, let me go directly to the version one of our algorithm. I won't have time to cover the I won't have time to cover the second version, but this is the easiest way one can think of. Okay, so now you have the data set. We separate it by the labels and the sensitive attribute. For each part, I divide the data into two subparts. Then I'm going to call all these parts to train a scoring function. For example, you can think of logistic regression. That signaling function serves function serves as the scoring function and then for each part I have some left out data okay and that will help us determine the two thresholds okay so exactly how do we do it now you have that scoring function think about the sequence function applied to the left out data in each part and then for each part And then for each part, it's going to give you a few numbers. Okay, we rank them from the smallest to the largest. Okay, like this. For this part, I can choose a pivot by invoking an old algorithm we developed in 2018. We call it the name character algorithm. So basically, Some value algorithm. So basically, it guarantees that if you were to choose that threshold, then with probability one minus delta, the hypernarrow is bounded from above by alpha with probability one minus delta. And alpha and delta are chosen by you. Okay. And then you do that for this and that. Means you separate the sensitive attributes and do the same thing on. And do the same thing on the class zero data as before, so that both parts satisfy the type of narrow constraint that is the Neumann-Pearson part. And then, because the overall type of narrow is just a weighted sum of these two type of nerves, if both of them satisfy the high probability function above by alpha, then they come together, the combination will also satisfy that. Will also satisfy that. And then the next step is to do the adjustment. Okay, if you focus just on the right-hand side of here, that is now we need to work on the class one data, these points. You only choose your thresholds among here, that is to the right of these pivots. Okay. If you were choose things among here, You were choose things among here, you make sure that the type one error constraint is respected. You only become more aggressive in bounding your type one error. And meanwhile, you adjust them so that the type two error disparity is small. Okay, so this is the first version of the algorithm. And it works somehow, but it's very aggressive in terms of controlling the type of an error. Of controlling the type of narrative. There's no case that you actually violate the MP constraint. So, this is at some cost because if you are that aggressive, the type 2 errors seem to be a little higher. So, we think, okay, maybe it's not necessary to just constrain ourselves into these parts. Because after all, for the MP constraint, all we need is to have the overall type. is to have the overall type and error bounded. We don't need the groupwise, both groupwise type and error bounded at the same time. So that's it. The inside is if for like one side you choose this part, the other side could actually go a little bit here. Okay, but that well empirically it works much better but Empirically, it works much better, but technical-wise, it brings up a lot of well, I wouldn't say very difficult mathematics, but it's a little messy to keep track of things. Okay. And with the new algorithm, the violation rate is better. Okay. And the type 2 area is smaller. Okay. In terms of business, that means the bank will have more business opportunity. Okay. All right, so yeah, that's the end of it. Thank you very much.