Okay, so hello everyone. I'm very happy to give this lecture and I would like to thank very much Denise for the invitation. I'm very sorry that I'm not able to join you in person, but I have been in Canada just one month ago for another workshop and it was not possible to come back now. So I have uploaded the lecture notes so So, the slide for the lecture and exercise session on the folder for today's session, and I will upload tomorrow for tomorrow's session. So, indeed, in this lecture, what I plan to do is to present some theory. Of course, that the time is very short, so I'd like to give you some intuition why we need this minfield game theory. game theory and then why this mean field game theory how this minfield game theory can be used for applications to electricity markets in particular applications related to energy transition and demand site management um oh yeah okay so the outline of the talk is the following i will start to give a Will start to give a brief overview of the context which motivated the applications that I will present. So as I said, this is related to climate change and energy transition. Then I will give you an introduction to mean field games. And tomorrow in the lecture, I will continue to talk about the theory. So I will start with a review. With a review of the main stochastic control tools. I'm not sure that all of you have these tools, so I'll just give you some important ideas that we need later. Then I will continue with the end player stochastic games framework. And then I will explain how to go from the end player stochastic games to the mean field limit. And finally, I will explain which Finally, I will explain which are some of the methodologies to solve these mean field games. And so, this is for the lecture, and then the third item will be for the exercise session, which will see an application of the mean field games that I will discuss in the first part, in the lecture part, to demand site management for electricity markets. And this part will be based on some research papers. Okay, so I don't know if you all of you know about the Paris Agreement, which is an international treaty on climate change. And it was adopted by 196 parties in Paris in 2015 and entered into force in 2016. The main goal of this agreement is to limit the temperature increase to 1.5%. Increase to 1.5 degrees by the end of this century. And in order to achieve this target, greenhouse gas emissions have to be reduced by 45% by 2030 and reach net zero by 2050. If you look, for example, to the US Energy Administration website, you'll see that in 2022, about 2022, about 31% of emissions of CO2 come from the electricity sector. So, in order to decarbonize the electricity sectors, an important role is played by the renewables, okay, who are at the center of this transition to less carbon-intensive and more sustainable energy systems. So, the main idea is that increasing the supply of renewable energy. Increasing the supply of renewable energy, for example, solar, wind, hydropower, and others, would allow to replace carbon-intensive energy sources and significantly reduce global warming emissions. So in the net zero emissions scenario, so there are several scenarios, but the one that we'd like to achieve, which is the net zero emissions scenarios, renewables allow electricity generation to be Electricity generation to be almost completely decarbonized. So, for example, if you look at the evolution of the global renewable generation capacity since 2014, you'll see that it clearly increased. You see that it increased by 84.36%. And this picture shows the proportion of electricity generated from renewable sources. Renewable sources in the UK, so increased from 3% in 2000 to 42% in 2022. And the proportion generated by fossil fuels has decreased from 73% in 2000 to 41% in 2022. So according to the International Energy Agency, Agency. And as you have seen, the recent progress has been promising. 2022 was the record year for renewable electricity capacity additions with annual capacity additions which amount to about 340 gigawatts. And solar PV is today the only one renewable energy technology which is on track with net zero and the other types of renewable energy. Other types of renewable energy need to expand significantly faster in order to get on track with the net zero scenario. Okay, now it is also known that renewable sources of energy are very intermittent. Okay, so a higher level of penetration of renewable sources of energy is Is synonymous with greater variability. So it is crucial to increase also the flexibility on the demand side in order to be able to balance in real time supply to demand, to take into account this variability of renewables. So the main purpose of the applications that we see, and of course, of the main of the theory that we'll develop in order to be able to understand the application. In order to be able to understand the applications, are related to two main aspects. One of them is the long-term energy transition. So we try to understand at which speed, at which pace the renewables will replace the fossil fuel type of energy. And on the other hand, we try also to understand how which type of Type of demand site management solutions one should take in order to adapt to this variability of renewables. And for these two applications, we'll use for both of them various different variants of the mean field game theory. Okay, so I will start with now to week. Uh, now to with the introduction to minfield games. So, I will start with the stochastic control framework, the standard one. So, I didn't introduce the complete definition, let's say that one should introduce the rigorous one because we don't really have the time, but which is the idea. The idea is that you have a state process, which is given by this stochastic, the solution of the stochastic differential equations. As far as I understood, you have seen last week's stochastic differential. Understood, you have seen last week stochastic differential equations. The main difference between stochastic differential equations and this one is that this equation is controlled. And I do apologize because I should have put in B and sigma, the control alpha. I'm sorry. I will correct this. So I need the alpha in B and sigma, a third parameter. And the value of the associated stochastic differential equation is denoted. Stochastic differential equation is denoted by x alpha. So for each control, I get, let's say, a value of x alpha, which is a stochastic process. Okay, it depends on two parameters, the time and the omega, which is the randomness. Wt represents the Brownian motion, and the solution of this stochastic differential equation is adapted to the filtration of the Brownian motion. Of the Brownian motion, okay, and this control process, so alpha is also a control, is a process, okay, which depends on t and omega, and takes values in a set A, which usually is taken as a subset of Rd, but here I will simplify and I will work in one dimension. Then we have the controller, which controls a certain, so the controller has a certain objective function that he wants to either to match. He wants to either maximize or to minimize. Let's say that here he wants to maximize this criteria. You see my functional j that I write at time zero. Little x is the starting point of my stochastic differential equation, a time zero. And you see that I have this function f, which is called the running reward functional, which depends on time, x t and control. And then I have also the function. And then I have also the final payoff or reward function, capital G. So this is the objective function that the controller wants to maximize over all possible control processes alpha. Now I will make the problem dynamic, what it means. It means that you see here I started at time zero. Everything is written at time zero. Now I will write everything starting. I will write everything starting with an arbitrary time little t. Okay, so I assume that my process, my control state process, starts at time little t from the position little x and then has the same evolution as before. And again, I need the control alpha in B and sigma. I define the value function. So you remember, I said here that I want to max. Said here that I want to maximize this quantity over all possible alpha control alpha. So, here I do at each time t, I maximize over all possible control process alpha this criteria. Okay, so I have a value which depends on t, on little t and little x. And here you see this set represents the set of control process alpha, which take values. Alpha which take values in a subset of R. To do, why do I introduce the dynamic version? I mean, at any time t I define my value function is because I want to obtain a PD characterization of this function. Okay. And if you have a PDE characterization and you are able to say that this very function is the unique solution of a PDE in a Is the unique solution of a PDE in a certain sense? Then, for example, when you don't have explicit solutions, which happen in many cases, then you can use, for example, numerical schemes in order to compute an approximation of this object, okay, of this object and also of the optimal control. So, for example, finite difference methods. So, a key tool in order to derive the equation satisfied by the value function. Satisfied by the legal function is to this dynamic programming principle. What does this dynamic programming principle say? It says that you can split the time interval from time little t to time capital T into two time intervals from little t to s and from s and capital T. You can optimize first from s to capital T and then optimize from T to little T. Optimize from t to little s. And what you get is that the value function v, that's so this was the definition that I have here, but it is also equal to the supremum over all admissible controls of the expected value. And you observe here I take the integral from t to s instead of t to capital T as I had before. And the terminal reward is a time s, which is the value function. Is the value function a time s and xs t x alpha. Okay, and here, of course, we can also extend to have stopping times. But this is the main idea. You can split the optimization and to optimize backward in time. Okay. And using this dynamic programming principle, we can derive the HGB. This is the so-called HGB. This is the so-called HGB equation, Hamilton-Jacobi-Blaman equation, which is satisfied by the value function. I assume here that V is of class C1, 2. Why do I assume this? Because if I assume this, it means that I can take the derivative in time and the first and second derivative in space. Okay. And then for all t between zero and t and x, we have that the value of. We have that the value function satisfies this equation. And the terminal condition is v of time capital T of x is g of x. This can be easily seen. For example, if you take this equation, if you fix capital T here, you observe that this term, the integral will disappear, will be zero, and you will have only g of x capital T. But I know that the time capital T starts from little x, so it will just be g of x. G of x. So this is my terminal condition. And this operator L A of psi, where psi is any function of class, represents the so-called infinitesimal generator, which is associated with the diffusion process X. Okay, you see that it's associated to the diffusion process X because I have the drift B and the sigma volatility, which have been. Which have been present here in the definition of my process. Okay, now, in many cases, the solution is not C1, 2, but in this case, is a full theory of viscosity solution which have been developed. And you can show that the value function is a solution, but in a weaker sense of this PD. Okay, so one thing is the dynamic programming equation. thing is the dynamic programming equation, the dynamic programming principle and the derivation of the HTB equation. Another important thing that I would actually use is the so-called verification theory. So I will start with a reminder of the uncontrolled case. So it means that I don't have any control. You see, I don't have any control. I have X which does not depend on alpha. This is basically the so-called Basically, the so-called Feynman Fax formula. And what it says, it says that let's say B and sigma are nice coefficients, which means that I have a strong solution of the SDE and so on. And I define this solution of this SDE. Okay. And I assume that this PDE, you see that if I go back to my previous slide, this is simplified. Back to my previous slide, is a simplified version. I don't have the supremum over A because I don't have any control, and I have this infinitesimal generator and the running payoff. Okay, so you can see that this PDE represents a simple case of the HGP equation. So assume that I have a function which is a solution of this PDE and it satisfies the And it satisfies this terminal condition, then what we can show is that the value function admits this probabilistic representation. Okay? So what is nice is that basically this gives also, if you have a solution, you also know that the solution is unique and furthermore coincides with this probabilistic representation expectation of. representation expectation of the integral from t to capital t of f plus g of x at the terminal time capital t. Now I will give the version in the controlled case. Okay, so suppose now that I have a function C12 again, which satisfies this terminal condition and the HGB equation that I have introduced on this slide here is exactly the same. It's exactly the same. But I assume here that I have a solution. Furthermore, I assume that for each t and x, I have a maximizer of this, this is called Hamiltonian, okay, but of this part of the LGB, you see here. And I assume that the function is measurable, okay? And furthermore, the SD, which is obtained when Which is obtained when I replace the control with this optimal control, this plays the role of the optimal control, has a solution. Then in this case, we can show that Vtx coincides with the value of the control problem. Okay? So what we'll do is we'll use this verification. Is we'll use this verification theorem to solve a very simple problem. So, usually, the problems where we can expect to get an explicit solution are the linear quadratic problem. There are also others, but these are an important class. What it means that we have linear quadratic problems, it means that the state and control variables appear in a linear Variables appear in a linear fashion in the coefficients b and sigma. So b and sigma depend are linear with respect to x and alpha, and in a quadratic fashion in the objective function f and g. So for example, you see that this is an example that I will solve. You see that my drift is linear in alpha, it does not depend on x, and my objective function is quadratic in Quadratic in alpha, okay, and the terminal pair of is quadratic in X. Okay, so this is an example of a linear quadratic problem. So I will try now to use my tablet. So I will switch my screen. Do you see my white screen? Yes. Yes, okay, good. So, I will try to solve this problem. So, if I want to write the coefficients, I have, this is the b, the drift. Then I have the f of xa, which is minus one over two a squared. And then I have the terminal payoff. The terminal payoff at time capital T is g of x is minus alpha over 2 x minus z of square. So now I already told you that this, I will introduce the Hamiltonian of the problem, which is nothing else than the supreme. Which is nothing else than the supremum over is the part that you have seen in the PDE plus one over two. Okay, so what you have seen in the PD, the PD that I have written before, you remember that I have written this PD. PD is the same as writing plus the Hamiltonian, where I put little x and y and z are called adjoint variables, where here y is replaced by the gradient. So the gradient with respect to it. So, the gradient with respect to little x is here, okay? And here is the Hessian. Of course, that here I'm in dimension one, so it's just the second derivative with respect to x of V T X. Okay, so this is a reminder. Okay, so now what I want to do is to, so which is my the Hamiltonian in my case? In my case, h of x. H of X Y Z is equal to the supremum. So I replace the B by A. So I have Ay plus, and here I will have minus, sorry, minus one over two because my volatility is one, okay. Is one, okay? And is not controlled, so I don't need it. So this is my Hamiltonian, okay? And this is a concave function with respect to A. So it is admitted maximizer. And if you do the computations, you can observe that this is equal to 1 over 2 y square, okay, because I optimize for each x, y, z, and the maximizer. is alpha hat of xy is equal to y okay uh so which is my hgb equation in this case so this here it was written in the general case now i i would like to write it in the uh this particular case it will be case it will be dtv the derivative with respect to time of v plus uh one one over two of the gradient square plus over two the laplacian of v tx equal to zero the terminal condition Is V of capital T X is equal to G of X. And which is the optimal control? The optimal control will be, if you want, alpha star of T of X is nothing else than this alpha hat here that I have applied to X and the derivative, the great. The derivative, the gradient. Okay, so always y is a placeholder for the gradient v and z for the Haitian. Okay, so this is nothing else than the derivative, the gradient. This is the optimal control. So what we do for this type of problems, Problems. We look for a solution of the form ETX is equal to F T, so a separable solution psi of X plus G of T and And which is F T, what I know about F T I know that F T, in order to know something about F, Psi, and G, I look at the terminal condition. So my terminal condition was this one. So psi of t will be equal to minus alpha over minus alpha over lambda over two okay psi of x is x minus z squared and g of capital T is equal to zero so which is the the recipe the recipe is that and here psi of x is as I said is x x minus z square okay this is my p side so you look at the terminal condition and looking at the terminal condition you can identify the form of the value function and now what you do is you compute derivatives is of this value function v with respect to time and then with respect to x twice and you replace the derivatives in The derivatives in this HGB equation. I don't have the time to do all the calculation, but you can try to do this as an exercise. Yes? For the H third turn on the left-hand side of the HJB algorithm, is there supposed to be a sigma square multiplication? For the left-hand side, you mean this one? This one? I cannot see you. No, no, no. It's HAB equation, left-hand side, third term. I don't hear you very well. Can you repeat, please? What? It's a Laplacian turn. Yes. Uh yes. Should there be a sigma squared multiplication? Yes, but in my case the sigma is one. Oh, okay. So here I'm writing the HGB equation in my case for the diffusion process that I have and here probably, yeah, I should write sigma of xA is equal to one. Okay, so sigma is one, so I don't have the term. Okay, so I have this. So I have this. Let me see. Okay, I need a new just a moment. I will delete this part. Okay, so I will just keep this the maximizer this one. Okay, so you replace, you take the derivatives, you replace the NHGB equation, and then what you get, you get two equations for T. equations for t f prime f prime t plus f square t equals to zero with the terminal condition is f of capital t is minus lambda over two and then you get g prime of t plus one over two f t equals to zero okay with g of capital t equals to zero so doing the calculation So, doing the calculations, you obtain that from this, you obtain that f of t is equal to minus 1 over 1 over alpha plus t minus capital t minus t and here is g of t is equal to 1 over 2 log of 1 plus lambda capital T minus T. Capital T minus T. Okay, so you have for all T. So the HGB equation that meets the solution VFTX, which is this FT, I will not rewrite. I mean, let's be to be sure I will write it. Admits this solution. Okay. And also. And also, you remember that the condition was also so I compute also. You remember that my optimal control is the gradient of V with respect to X. So this will be Z minus X divided by 1 over lambda plus capital T minus little T. And then the optimal control. Controlled state process is dxt where lambda is this one is gradient. Okay, so we have just z minus the little x will be capital X T okay one over lambda plus t minus little t dt. And a condition you know that was that this SD to admit a strong solution and it admits a strong solution is a linear actory on X and is a mean reverting process to Z. Okay, so this is how we solve this problem. Do you have any question at this stage before that I continue with the end? Before that, I continue with the end player game? Would it be possible to give an interpretation of the solution? So, like, why does it make sense? Is there a way to interpret intuitively? Yeah, no, actually, it's related to the flocking model. If you look at my, so I will, um, I will, uh, sorry, I will go. Uh, sorry, I will go back to my slides and all my slides this one. Okay. Do you see the slide? Yes. Okay, so um As you see here in my problem, sorry, here. So I want to maximize this objective function. So maximizing this objective function, it means two things. It means that I want a control such that the value of X capital T driven by this control to be as close as possible to Z. This is my This is my target, let's say, little z. And at the same time, I want to minimize the effort that I'm doing such that the final value of the process to be closest to S, because I want to minimize alpha square. So this is what we also observe to the solution: is the fact that the process X will be mean reverting to little Z. Okay, so this is what This is what we are trying to do with this objective function. Okay. Okay. I see. So, and the solution, you said it's a mean reverting process to a little Z, yes. Okay. Yeah, I don't see the solution here, and I forgot what it was. So, yeah, yeah, sorry, yeah, it is. It would be side by side, right? But that's So maybe I uh I go back to the do you see now the other one? Yes you see this is the process oops X the optimal process oh so okay so Z so that's Z right yes. That's Z, right? Yes. Z minus X T, and the denominator is 1 over 1 over lambda plus capital T minus little T. And Lambda was a parameter or? Yes, it's the parameter which appears in the terminal condition. It's minus lambda over 2 multiplied with x minus z square. So is the lambda is in the terminal condition. Okay. And how does the instantaneous reward affect the solution? Because that was the Solution because that was not parametrized, was it? The function you have. So it appears, I mean, in the FT and when you do the computation of the Hamiltonian, there is where the running cost appears, when you compute the optimal control. Okay. Because you optimize the Hamiltonian where the running cost, the drift, and the volatility appear in the Hamiltonian. And there is one. And there is where the running cost enters into play. Can we go back to the again to the objective function? Yes, sir. Okay. Do you see the objective? Okay, and I'm looking at the instantaneous. And I'm looking at the instantaneous reward, the part that goes from zero to t. So that alpha t. Yes. Okay, so that's a that's a function of the state, right? It's not of the state if it's of the control process. Alpha t is my control. It does not depend on the state. Oh, it doesn't? No, only the terminal condition depends on the state. So my state process is capital X and the control is alpha. Control is alpha. And the running cost only depends on alpha. But I mean, the optimal alpha will depend on X, right? Yes, yes, yes. Okay. But this comes so when we write the Hamiltonian, the running cost enters into the Hamiltonian. And then when we look at the solution, we use the terminal condition. When we look for the optimal solution, we use the form of the terminal condition. We use the form of the terminal condition in order to identify our functions. Okay. Okay. Okay. Thank you. But this is the idea it's important to retain that what we are trying to do is the objective function is that x to be as closest as possible at the terminal time to little z. And at the same time, we want to minimize the effort. Alpha can be an effort that we make so that the terminal We make so that the terminal time x to be as close as possible to z. Okay? Okay, thank you. You're welcome. Okay, so maybe I should do a full screen. Okay, so now I will go to the stochastic games. So, introduce some games with a specific type of interaction. So, I have many players. So, in the part, the first part that I present, I have just one player who wants to achieve a certain objective. Here, when we talk about stochastic games, we have many players, and what makes things a bit more difficult is the fact that the players are interacting. So, here I will focus on a very specific. I will focus on a very specific class of games, the games which are called in mid-field interaction. And I will explain what it means. So, first of all, the particles have dynamic states as the one that I presented. So, the X here was the state, what we call the state, and alpha represents the control. So, each controller will be. Each controller will have his own state process X. Okay? So dynamic states, because they are driven by stochastic differential equations. Now, each player interacts with the others in a symmetric way. What does it mean? It means that, for example, I'm a player, I interact with Interact with Paul and Francis to friends. But if I exchange the position in the game of Francis and Paul, this will not change my value. Okay. So this gives a sort of anonymity of the players. Okay. So then I interact with the whole population and there is no privileged interaction with others. Okay, each player solves a stochastic control problem as the one that I presented before. Okay, where you see that some terms will depend on the others because you have this interaction. I will formalize the things mathematically just in one minute. So which is the purpose? The purpose is to find a consensus inside the population, which means that no particle has Means that no particle has the incentive to leave the consensus. And this is what we call in game theory the mesh equilibrium. So let's formulate the problem. I will work with an example which is very close to the one that we have for the one player. So I have n players, and which is the dynamics of each particle is given by this stochastic differential equation. Okay, so each controller has So each controller has his control alpha i. Now, here I take a model where the stochastic differential equation, the state of each particle, is driven by a Brownian motion, which is independent on the other Brownian motion. So the Brownian motions are independent. But you can, of course, add also a common source of noise. And we'll see in the application that we have the presence of a common source of noise. So a process which is present in design. A process which is present in the dynamics of all particles. Then, alphai are processes which have the right measurability. I will not enter into details. And the initial conditions of these state processes are identically and independently distributed. Each player has an objective function. So, this is very similar to what we have. So, this is very similar to what we have seen, but it has some new features. So, first of all, you see that the objective function of the ICE controller depends not only on his control, but also on the controls of others. And how? So, here we have the terminal payoff of the ITE player, which depends on the value of its state process at time. The value of its state process at time capital T and on the empirical distribution of all other players. So here is the interaction. This mu bar. Okay, so Ji depends on the others through mu bar. So mu bar, and here should be capital T, I'm sorry. So is the empirical distribution of the others at the time capital T. And then I have the running payoff here, where you see I have a function f which depends. function f which depends as before on xi i don't take that it depends on alpha here but it depends also on uh the empirical distribution of the others at time little t and here i have the control as before the control of the i player okay so what i have to precise it being a symmetric game so you have the symmetric interaction with the others in the sense that if you exchange two That if you exchange two particles in the population, it doesn't change your interaction with the whole population. But also, the agents are symmetric in the sense that they have the same drift, they have the same volatility, they have the same terminal payoff function G. You see, all of them, they have the function G, and all of them have the same running payoff. Payoff. Okay, so what we are trying to do is so each player will maximize this objective function j i over the control alpha i because alpha i is his control. And when do we say that n couple of strategies alpha one star up to alpha n star is a Nash equilibria is Equilibria is if there is no interest for any player to leave the consensus. So it means that if I have alpha one star, alpha n star, and I fix alpha one star up to alpha n star for all players except the ice player, then the alpha star i is the one which minimizes the cost J I along all possible control strategies of the player I. Control strategies of the player I. So you fix the others. So you are the first player. So you fix the strategies to alpha, to star, up to alpha, and star for all players starting with number two. And then if you try to optimize your objective function J1 here, you observe that alpha star one is the one which minimizes. So this, if this happens for all. So, if this happens for all players, then we say that this vector of strategies represents a nash equilibrium. So, no player has the incentive to leave this strategy. So, now I will consider a very similar model to the one that I presented in the case of the one of the stochastic control is the so-called a very simplified version of the so-called flocking model. Version of the so-called flocking model, which is usually given in mid-flight theory. So you have each player I follows these dynamics. You is very similar to the one that we have seen here. You see, it's the same. The only difference is that each particle is driven, sorry, is driven by his own Browning option. Then the objective of the I-th prayer is to maximize the objective function. Player is to maximize the objective function, which is again very similar to the one that I have presented here, you see, where I optimize. So what it tries to do, the player, the ITE player. The ITE player wants to get a control, the optimal control, which such that his final position x of capital Ti to be close to the average. Close to the average position as terminal time of all other particles. So, this plays the role of the target that I had for the one player stochastic control. You remember that I had a little z. So, in that case, I wanted that XI capital T to be as close as possible to little Z. Here, what I want is that X I T to be as close as possible to the average of the population, a time capital T. And the running reward is exactly. And the running reward is exactly the same because what he wants is to minimize the effort he's making in order to achieve the final target position. Okay, so this is very similar to the one, the player stochastic control problem. And what we try to do here is to look for the Nash equilibrium. Why I'm giving you this example, because this example is one. Is one example where you can solve the problem and you can obtain an explicit representation of the Nash equilibria, which usually is rare. So I will not give the methodology because I don't have the time, but is by writing, you will have a system of PDs, you have each player have his Hamiltonian, and by using a version of the very A version of the verification theorem adapted to the end player setting, you can derive this representation of the Nash equilibria and of the value functions at the equilibrium. So what you obtain is that the Nash equilibria has this representation that I have here, alpha star of IN, which is this one, where x is the mean, x bar is the mean of. mean, x bar is the mean of the average of all xi. Then at the equilibrium, the state process evolves according to this equation, the X T I N. And then you also have explicit representations of the value functions of each player. Okay. Okay. Okay, so this is an example where we can solve. What happens in general? In general, what happens is that finding Nash equilibria is very challenging. Usually, end player games are untractable. And the idea of the multiple game theory is to simplify the analysis by using an asymptotic. Analysis by using an asymptotic theory. So, what this means is it means that we want to reduce the analysis to one typical player because you see that all players that I consider were symmetric. All of them had the same drift, the same volatility functions, the same running reward, and the same terminal reward. Okay, so we want to reduce the analysis to one typical player, which Typical player who is in interaction with the theoretical distribution of the population. And here I have a very nice citation from the known book of von Neumann and Morgenstein, which is called Theory of Gains and Economic Behavior, which says that it's usually much easier to deal with very great numbers. With very great numbers, then to handle those of medium size. Why? Because when you have very great numbers, you can apply the laws of statistic and probability. So I'm referring here mainly to the law of large numbers, which allows to take the limit and to consider in the limit just one player which is in interaction with the theoretical distribution of the population. So let's try to go to the myth. Try to go to the mid-field game formulation. So, since the game is symmetric, I insist on this property because this is essential, we can assume that each control of all players at equilibria can be expressed as a function, which does not depend on the players, all of them have the same function, which is then applied to XIT to the individual state process. Individual state process and the empirical distribution. So the particles at the equilibria follow this SD that you see here. And now it's a very nice property which says that when, and there are many papers and of course the studies are very rigorous, but the main idea is the following is that you see the particles depend each one on the other. Each one on the others, okay? Because you see, my the control of the ith player depends on μ bar n, which is the population. But what happens is that when the number of particles becomes larger and larger, then the particles they correlate and this so they become more and more independent. So when n grows, we're very So when n grows very much, the particles become more and more independent. And these particles xi, the law will, the law of xi coincides with the law of x that I have number two, equation number two, which is the law of this, the so-called McKin-Vlassov equation, where mu is the law of Xt. Okay, so Xi converge to in law. To in law to x, which is the solution of 2, and μ bar n, the empirical distribution, converges to mu t, which is of the law of xt. So this is a very, so when n once again becomes larger and larger, the particles are independent copies, if you want, of this Martin-Blassov equation. Okay. So So now, what I want to go back to my flocking model. So, if I go back to my flocking model, what we can observe is that if I take the value function of a particle i, I go back to my particles here. You see that the value function here depends, has the same. Depends, has the same G, has the same F, and depends on the others only through the average. So we can see that when n grows very large, all value functions are very similar, are very close to a function v, which is the same for all i, which depends on tx and a variable y. And a variable y, where v tax of y or z is given by this. And this representation is exactly the one that I had for the one player stochastic control problem for a given target z. So it's exactly the same. Sorry? Is there any question? No. No. No, okay. And then the particles at the equilibria follow the dynamics, this dynamics. So you see, it's exactly the same dynamic as the one that I gave in the case of the one player problem, where the Z which was here is replaced by the expectation of capital X. Of capital X. So this gives the idea that we can try to define a fixed point problem using the target Z as a replacement for the population average. And in doing so, if we solve this fixed point problem, we can basically obtain. Basically, obtain the same limit as the one that we get in the n-player equilibria. So, when we take, we solve the n-player problem, and when we take the n, which goes to infinity, we'd obtain the same thing as in the case when we solve the fixed problem that I present here. So, I fix the target z. Okay. Remember that in the case of the N-player games, the target was. The case of the N-player games, the target was that each player wants to be close at the terminal time to the average of the others, which can represent positions, for example. Okay, so wants to be as close as possible to the average position of the other players. So I fix here that then I solve the one-player control problem is what I have shown you on the tablet, the calculations that I did. So you get. So, you get this, the dynamics of the state process along the optimal control, is this one. Okay. And then you want to find the fixed point Z of the map phi of Z, which is the expectation of X capital T associated to little Z. While I'm writing here on the right-hand side, Z is Hand side Z is because when I solve the second item, the stochastic control problem, I solve it for a fixed Z. And then I get the state process along the optimal control. And what I want is that the expectation of capital X at time capital T of Z to be the Z that I gave as input. So by solving this, This I will just switch very quickly to tablets by solving this you'll obtain You'll obtain that Z is a fixed point F if and only if that is equal to the expected value of X zero. X0. And this is exactly what you get by using the n-player games, the limit, you'll obtain this as equilibrium. So how you derive this? You remember my equation which was obtained in the one-player case. What you do is you take the expectation and you observe, so writing the expectation, you get expectation of Xt is equal to expectation of X0 plus integral from zero to T lambda of Z minus expectation of X T one plus Lambda T minus T. Lambda T minus T and here DS Yes Okay, and you observed if you take the function f of t which is equal to expected value of X T minus Z okay, so take minus Z, you observe You take minus z, you observe that f t solves the ODE. F prime of t is minus lambda f t divided by 1 plus lambda capital T minus T. Okay? And you can solve it. And you have that. you have that phi of z is equal to expected value of xt is equal to z plus f of t, which is the same as z plus expected value of x zero minus z divided by one plus lambda t so So pi z zeta is equal to zeta if this term is equal to zero. So if and only if zeta equal to expectation of x0. Okay, so doing this, you see you compute the equilibrium distribution of x and you obtain the same thing if you take what we got in the n-player game and you take the limit. And you take the limit. Okay, so you get the convergence. I will switch to my slides. Okay. Okay. So, okay. So, which is the general minimum game formulation? The general minimum game formulation is the following. I'm sorry, Roxana. Could we give an interpretation of that result again? That in terms of the large number limit of the. Yes, yes. So the idea is that here, yeah, if you look to this slide here, if you take X bar T, since they are, okay, they are not. Since they are, okay, they are not independent, so it's not really the law of large numbers, but it's the propagation of cows, they would converge to the expectation. You see? Yeah. And this one over n will converge to zero. So we get the lambda and the expectation of a particle x minus x divided by. So, and if you solve the expectation of xt, Expectation of Xt then is actually equal to expectation of X0 in this case for all T. You take the expectation and you obtain this. So this is the interpretation. So when N goes to infinity, then this X bar of T will converge to expectation of X. Okay. I'm a little bit confused. Sorry, it's just what. Confused sorry. It's just what happened to the what happened to the Z? No, I don't have that. Here I have a zero. Sorry. So, okay, so these agents were trying to average. Stay close to the average as much as possible. Yes. So, what I don't understand is that, so does the optimal energy equilibria is that. Equilibria is that so they're going to stay where they are. Is that the interpretation? Because we found. No, the average, in average, the expectation of XT will be equal to expectation of X0 in average. Okay. In average. You can actually check that if you put all these players starting from exit. All these players starting from x0i, then when you take the limit, the expectation, and so you have this equation: x bar t converges to the expectation, and then you take the expectation, and you get the expectation of x t is equal to the expectation of x0. So in average, I see on average. Okay, so that means and so the idea was that here I have the end player game, but what I wanted to show is that using what we did. What I wanted to show is that using what we did in the first part for the one player, I can obtain a limit of this Nash equilibria by using the Z. So I fix the Z. I'm solving the one control problem. So Z plays the role of the replacement, let's say, of the empirical average. So I fix the Z, and this is the Z that I want to find. And this is the z that I want to find, then I solve my one-player control problem where instead of the empirical distribution, I have the z, and then what I want is to find the fixed point of the map, phi z of this map. I think maybe something that's missing is that the time zero distribution of the agents. Regions, is that given? Or is that like essentially the Europe? They start from all these games, as I said at the beginning, start from identical and independent distributions. All right, and that's essentially why. Okay, I see. Yes, thank you. So, for the symmetry, you need this identically distributed, and then they are also independent, the initial positions. Positions. Okay, so the main message is that in this case, if you solve the n-player game, you get the convergence to the mean field where you just do this program that I did here, the fixed point problem. And on the other hand, if you solve this fixed point problem, you obtain an approximation of the end player equilibrium. Okay? So, because in practice, usually you cannot really solve the end player's game. Cannot really solve the end player's game as it was possible in the flocking model. So, in the case when you can't do it, you use the mid-field game formulation, which gives a proxy of the equilibria for the end player game. So, how it works, the general mid-field game formulation, is the following. So, you fix of probability flow of probability measures mu. So, this represents the population. So, you fix the environment. Then, you solve Solve a one-player standard stochastic optimal control problem for the environment μt, where let's say here X has a very simple form where only the drift is controlled, but in practice you can take much more general forms, also the volatility. And then you have the cost functional of the player. Okay? And And you say that you have an equilibrium when, in the case when you have an optimal control here, if you take the law of X along this optimal control, which of course also depends on mu, which was fixed, then the law of x star coincides with the mu that I gave an input. Okay, so it's exactly the same program, but written in a more general form as a As the one that I try to do here. So the target Z plays the role of the expected value, the average of the population, okay? So it plays the role of mu. And then here I had the individual problem that we solve is the one that I solved in the very first part of the lecture. So here you solve in a more general case, and then you try to. Case, and then you try to find the fixed point of a map. Is it clear? It's not clear. So the mean field game formulation is consists in solving a Solving a fixed point problem, okay, where you reduce the analysis to one player, which is in interaction with the population. So you fix the distribution of the population, which is mu t. You solve the problem of the representative individual player. Okay, we have just one player. You solve the problem in the environment μt, which was fixed. Which was fixed, and then you have to find the fixed point, which is that the law of this process X of the representative player along the optimal control has to be coincide with the mu that I gave as an input. Can I add something? Yes. So we are looking for Looking for a fixed point because, in time, the process is likely to somehow get to that point, or a fixed point is a point where we put where the function's value is equal to the where we the parameter of the function, right? So, why are we looking for a fixed point? I couldn't quite understand is like. Is like so what is fixed as a par yes, it's fixed. You fixed first the mu t, then you solve the optimization problem, and then you want to retrieve. This is equivalent of the Nash equilibria at the limit, the fixed point, where you use the symmetry. Remember that here for the example, basically what we want is that the z, I gave a z as an input, okay? As an input, okay, I solved the problem, and I want that to be equal to the expectation of x of t. So I'm solving also a fixed point. I want to get the final value of x along the optimal control to be in expectation equal to z. So I'm solving. So I'm solving a fixed point where Z here represents the distribution, the average of the population. Okay. Okay. I have another question. Because also, sorry, sorry, just maybe is it even clear when you look at this example? Oops. Where I have the end player games here, when you take a particular on this example, you take a particular player. By using this law of large numbers, when n goes to infinity, here the average goes to the expectation of x, which is the x here. So here is the expectation of x. So you look for the fixed point. Okay? Because you get the expectation of x here. Yes. What if you... So, hi, Arsana. So, sorry for interrupt. So, you have around five minutes. So you have around five minutes left. So you can maybe decide whether you want to offer a few more slides or maybe open up a question. So maybe what I will say is that this theory has been introduced by La Silence. So it is used to characterize large population gaps with symmetric interactions, the ones that I presented. And, okay, this you can look on the slide. This is a standard example that Pierre Villion usually gives. This is an example from a paper of Gian Last Trillions. But what I want to maybe to say is that this maybe I will talk in the exercise session, is that there are several approaches to solve these meaningful games. One of them is the PDE approach, which will consist of a system of PDE. PDE equations, one which is a Hamilton-Jacobi-Berman equation, which characterizes the value function of the representative agent, which is a standard stochastic control problem. And then you have a Fokker-Planck-Kormogorov equation for the measures, which give the evolution of the distribution of the population. Then you have a forward-backward approach, which consists in a coupled system of forward-backward stochastic differential with coefficients which With coefficients which depend on the law of solution, and there's another strand of methods which are the so-called compactification methods. So, maybe I will not continue now with other things because I will not have the time to finish. I will do in the exercise session probably. So, there is something else that so let's give a big plot for. Rosanna for the first part of the mean field gain theory. And we do have some questions. I think Jan can have a question, so please go ahead. What if your natural equilibrium is not unique? What if you have multiple Nash equilibriums? And in that case, how would you deal with optimization? But also, I mean, yeah, it liberates, but also for mean-field game theory, you don't have even very rare case, you get the uniqueness of an equilibrium. So then, of course, an important question, which is actually, there are many current works on this, is how to select the equilibrium, okay, when you don't have uniqueness, and there are some partial answers in the case of potential gains. Potential gains. But you don't, for example, you'll see that for the applications that I present, for the one that I present today, I will have the uniqueness, but for the one that I will present tomorrow, I will not have the uniqueness of the equilibria, but this is not an issue. And I will explain why. Okay? So there are applications, problems for which the lack of uniqueness is not an issue. But of course, for other problems, the question For other problems, the question is how to select the equilibria. And as far as I know, in the case of the very general case of non-potential games, there is no open question, but for some problems where the games are potential, then you are able to show how to select the equilibrium. Thank you. So I also have a question. I have a question in other places. You so, what we did was we have taken the solution of a one-player problem or an n-player problem, then let n tend to infinity, and we found the solution of the problem with infinite amount of players. But what I wonder is, may it not be the case in another setting. So in a So, in a problem of an in a mid-field problem, could it be the case that we solve the problem for one player or n players? Then, as n tends to infinity, it does not produce the same result as an infinite amount of player setting. No, so in the case when you have, yes, of course, but in order to be able to use this mean field theorem as. Minfield theorem as theory has said, it's very important to have symmetry gains. So, if you don't have the symmetry, then you can't do this. What do you mean by symmetry? By the symmetry, it means that all players have the same objective function, the same terminal reward, the same. Reward the same I have written here. The same terminal reward. You see, G is the same for all I, F is the same for all I, B is the same for all I, sigma is the same for all I. And then also the interaction with the population is in a specific way, in the sense that each player interacts with the whole population and With the whole population, and it doesn't have a privilege in interaction with a specific player. And also, if you exchange two particles in the population, the result does not change. So, there are some important assumptions. So, these assumptions are to ensure the existence of a national equilibrium, right? Not for the end-player game, but for the approximation by the mean-fledged theory. By the mean field theory. So you can use this mean field theorem, so the passage to the limit, and the fact that you can use the representative one player who is interacting with the theoretical distribution of the population only for this type of games. So what I want to say is that by using the mean field theorem, only for this type of games, I can get a proxy equilibria using the limit problem. Okay, thank you. Okay, thank you. Thank you. And by the way, another good question. So, what if we had the same ratio of different kinds of players and then would it also like a homogeneity in terms of players? Would it also by taking the limit yield a good proxy? I guess maybe she displayed okay, coffee break. I think Rokadina's battery may have died. Oh, it makes sense, definitely. Yeah, because the battery error was screened. Yeah, yeah. She didn't batch. Yeah. So there's no need. She doesn't ignore. You can go a little bit. Sorry, it was a problem with my internet connection. Okay, so I had a question. Okay, so I had a question, one last question. So what I wonder is that, so if we let not if we let different kinds of players and close functions in the game, but say that we keep a homogeneity, so we keep a constant ratio of that kind of players. So if in that setting, would taking the limit and letting the process extend. And letting the process extend to infinity produce some kind of nice proxy. Yeah, so what you can do indeed, and this I will do in the applications, you can introduce, let's say, several classes agents. So each class has symmetric agents which are symmetric, but the players from the different classes, from the different classes. Classes from the different classes are different. Okay, so you can introduce more heterogeneity using several classes of agents, and you can do this. Having a proportion of the population which are of one type and another proportion, which is another type. And I have this in the applications, actually. Okay. So the main idea is to assume some kind of homogeneity and some kind of behavior that Behavior that also shows up when the numbers get very large, right? Yes, yes. So the symmetry is extremely crucial in order to be able to go to the limit and to take one player, because the one player has the same reward functions as that's why we assume that all of them have the same reward functions and so on, to be able to, because they're one representative player. One representative player. And when you work with several classes, you will have for each class a representative player. Okay. Okay. Okay. Thank you. Thank you. You're welcome. So we meet at 1.30 now. Yeah. So, Roxana, so thank you so much. I just realized it's pretty late in your vocal time. Yeah, it's true. No problem. Thank you so much. Yeah, so I now grab some very high-level idea about the meme view can see. Thank you very much. We are in the coffee break, but we will see you. Okay, thank you. Thank you very much. Bye-bye.