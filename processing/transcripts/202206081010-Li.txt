All right, so I'll get started then. Yeah, first of all, I really thank for inviting me to this wonderful workshop. I learned a lot from the past talks. Today I'm going to talk about a guided neural copy model for automatic phenotyping from EHR and phenotype association study analysis. Let's get started. So GWAS, as we know, is actually focusing on one phenotype at a time. On one phenotype at a time, right? So it has a limitation that is missed many other traits that potentially share common mutations. It also, as the previous speakers mentioned, there are environmental factors that are often unknown from GWAS design. So also, as Jessica mentioned, there is this phenyl-wide association study. So in here, this is just a very simple workflow for two types of phenylwy association study design. Association study design. So, the first one is called prospective or observational study, where you have this prospective cohort, and then you have access to their electronic health record data. And then these are the sort of the EHR coding data. And the other side is this ascertained, which is similar to GWAS, where you have access to the patient questionnaire information, right? So these are often self-report questionnaire. And then you collect all that information. And then in the second step, you're going to genotype a large number of individuals. A large number of individuals. And then the goal here is to be able to identify genetic association with many phenotypes. So, one of the well-known resources is called the UK Bell Bank. So, this consists of genotype and phenotype information for about half a million individuals. So, the outcome out of that, also, as Jessica mentioned, is this type of a Manhattan plot where the x-axis are the phenotypes, where the Are the phenotypes, where the y-axis is the negative log time p-value for a specific SNP? So in here, you can see that one single SNP has a very significant association with many different heart-related diseases. And here, one single SNP is associated with many different related to type 1 diabetes. So these results are derived from the eMERGE network. So on the EHR side, the EHR data has becoming very rich. So here you have Uh, rich. So, here you have not only clinical notes but also billing code, for example, ICD 9, ICD 10, and then there's laboratory information, there's pharmaceutical information, etc. So by the year of 2015, you can see that majority of the states in the US has already adopted the EHR technology, right? So it has become very pervasive. So one of the well-known EHR type is this ICD code, right? Also, as previously. ICD code, right? Also, as previous speaker mentioned, so this we started with the ICD9 code, which is start with the first three digits. These are the categories of a disease, followed by a decimal. These are the sort of more granular information. And then recently it has been converted to ICD-10, which using alpha numerical information, which have more granularity to characterize disease in a systematic way. So, our goal, focus of this talk is to be Uh, focus of this talk is to be uh to learn EHR information using an unsupervised learning approach, which is in contrast to the previous talk, which focused on supervised learning. Right, so in here, this cartoon demonstration showing is that we have patient, we have phenotype, and then we want to factorize the matrix into two small sub-matrices, right? So, one of the unsupervised basic unsupervised techniques called matrix factorization, right? So, in here, we factorize patient by clusters, and then here. By clusters. And then here we're also learning cluster by phenotype. One application that you can do is when you multiply the two, you could infer whether this individual has type 2 diabetes, right? Given the fact that this individual belongs to cluster J and cluster J has a type 2 diabetes as one of the top features. But in our case, we're trying to focus on this side of the matrix. That is, we want to represent patient by these K clusters. And then sometimes we interpret them as the K phenotype. Interpret them as the CAPE phenotypes. So, one way to learn sort of the latent phenotypes is through this bag of words type of technique, right? So, first of all, we represent the SED code. So, for example, for patient D, we treat it as a document, right? So, patient EHR record is a document. And then the individual code are the tokens in that document, right? And then we can just count how many times a specific word. So, here are the Specific words. So here the ICD are specific words occur in that document. So in here, ICD one occurs three times, two occur one, three occur one time, four occurs seven times, and so on. So then you can flatten that to represent this actual document. Just keep in mind that this document doesn't have a specific order. So that's why we call this a bag of words. So one fundamental technique that we use very often in our lab is copy model. So in here, we are assuming that this is a sort of a Assuming that this is a sort of a simple article, imagine this is a patient sort of record information. And then we have these four different latent topics, right? For each of these tokens, we assume there's an underlying generative process that they come from one of these four topics. For example, William coming from maybe education, foundation come from budgets, and so on and so forth. So the goal here is to be able to figure out these latent topics. So these are the words that have a high priority. The words that are have a high probability under each of these topics, and also figure out the mixture membership for this top for a given document. In our case, it's the patient EHR record. So in one of the paper we published, which I just came out today, is that we actually use the patient questionnaire information from major depressive disorder patients. So this is in collaboration with Dr. Hmong from In collaboration with Dr. Meng from Douglas Hospital. So, in here, we take the patient questionnaire information and then we're trying to learn the latent topics. In here, these are the six different latent topics. Each of the topics characterized by the top questionnaire Q ‚Åá A as a feature. So you can see that this is a sort of ladder pattern where we're showing each topic capture very distinctive information, right, about the underlying patient questionnaire information. And then we train a logistic regression model. And then we train a logistic regression model. Instead of using the raw data, we're using the topics, right? We're representing patients as this set of topics. And then we're trying to figure out which topics are highly associated with the MDD diagnosis. Here we figure out these are the top six topics that has very high loss of regression coefficients. And then we conduct a phenol-wide association study by associating each of the topic with the actual patient genotype information, right, using a plink. Information right using a plink, and then we can identify these topics: M6, M22, etc. So, these are the top genes that are associated with each topic. And then, here are the arcs showing that the genes are actually shared among these topics, right? Indicating there's a potential pleiotropic effects that is one gene can be associated with several MDB-related topics. So, if you're interested, you can look into this paper in more detail. So, as we know, that EHR. So as we know that EHRs are multi-modal, so that is not only it consists of questionnaire, it also have a laboratory test, ICD code, etc. So in one of our recent paper published in Nature Communications, we treat them as a multi-modal information, right? And then we're trying to factorize these into small sub-matrices. So each of these matrix corresponds to a specific modality. Each of these matrices are actually a specific characterizing a set of topics. A set of topics, right? So, here, if you take one vertical slide and then you take a look at the top terms under each of the six modalities, right? So, these are the DRG through the disease-related group code, ICD code, and here's the laboratory test, clinical notes, and prescription, et cetera. And then you can focus in on the top terms, right? As you can see that this is just one topic that's showing that these topics are primarily for psychiatric disorder. I can see the top ones are like. I can see the top ones are bipolar disorder, schizophrenia, et cetera. Whereas here, you see the prescription are also mainly focused towards a psychiatric patient. So that's for the topic. That's what we can learn in terms of the comorbidities condition from just one single topic. We learn like hundreds of these topics. And here we can also represent patient, as I alluded earlier, by these topics, right? Because this is a bi-factorization approach, right? And then we can actually characterize patients based on their topic practice. Patient based on their topic probabilities. Here we have the rows are the patients and then the columns are the topics. And here, the one that with the high intensity showing that this set of patients have a very high probability under that topic. Now, if you have a way to interpret that topic, we can actually represent this patient belonging topic as a phenotypic risk. So that is a, you know, we have a quantitative way to evaluate the risk or severity score for a given phenotype. Score for a given phenotype, right? So, here we're using topic and phenotype interchangeably very often. So, this is a sort of a probabilistic copy model. If you're interested, you can look into this paper. All right, so we haven't resolved the challenge. Actually, there are two challenges remain, right? So one challenge, also, as a previous speaker mentioned, is that EHR is very noisy, it's very sparse, right? There's only very often 1% of the codes are observed. Than 1% of the codes are observed. And also, these topics are not really identifiable. So, in other words, for people who are doing a lot of unsupervised learning, they will understand that each time you run the model, you got different set of topics, right? Topic one for run one might be different from topic one when you run it again. So, the topics are not identifiable, right? And also, often the topics are not that highly interpretable. So, sometimes you got some. Interpretable, right? So sometimes you got some surprising copy, we don't know how to interpret it. So in here, we're proposing three strategies to solve this, tackle these challenges. So the first one, we're trying to model this special decision-making process. So this is published in last year, ACM BCB proceeding. And then the second challenge, the second strategy is that we're trying to use this taxonomical knowledge graph, trying to learn more interpretable topics. Learn more interpretable topics, right? And then the third solution that we propose is that we're trying to use this expert-curated phenotype. In other words, we're trying to use this C code, right? So these are phenotype definitions that consist of a set of ICD codes. So here in this talk, I'm going to focus on the second and the third strategy. So let's get into the graph-inform topic. So, in here, we propose this graph. We propose this graph embedded topic model. So, this is published in iScience this year. So, the framework is following this variational auto-encoder. So, that is here we have this encoder network where the encoder network take as input the patient medication and condition as a features. And then it will generate the mean and the log of the variance for the Gaussian latent variable. So, here we interpret that as a patient topic mixture. Patient topic mixture. And then this theta is going to multiply with the alpha and rho for specifically for medication. And then the same theta is going to multiply with a matrix inner product with the alpha and rho for the condition. So here we have a two type of EHR information. So in other words, this side here is the linear decoder. So that is we have a network encoder, but we have a linear decoder. The reason we want to have a linear decoder is that it helps. A linear decoder is that it helps us to interpret a topic, right? As we know, that linear model is excellent in terms of interpretability, but at the same time, we want to have a network encoder to have that flexibility to capture the patient underlying nature membership. So in here, so this is just following a matrix multiplication. Here you have patient by topic, and here you have a topic by embedding. So this is the embedding for each of the topics. So in other words, this is a trifactorization. In other words, this is a trifactorization, right? So we have a theta multiply alpha multiplied by rho, where the rho is the embedding by medications on this side. So this embedding by medications is a learn from this no-to-vac, which is unsupervised learning that operate on the graph. In this case, this is the graph, the taxonomical graph for the medications. So the learning notebook essentially tries to maximize the multinomial likelihood with respect to the node embedding. And then similarly, in here, we're learning the And similarly, in here, we're learning the embedding for the condition by running another notable back over the condition graph. So in here, we're learning the rho in there as well. And in the end, we multiply theta with alpha and rho, and then we got this x, which is the multinomial rate. And then we can maximize the evidence lower bound with respect to the network parameters. In this case, this is a sort of amortized inference, right? So that is, you amortize. You amortize your inference over the network base. That is a very standard for VAE. So that's a sort of learning process. So we apply this model to UK biobank data. So we mainly focus on, in this study, we mainly focus on the European descendants, which is about 457,000 individuals. And then we focus on 802 active ingredient medication, as well as 443 phenotype conditions. Phenotype conditions. So these are derived from patient self-report. So this is by the way in collaboration with Dr. Audrey Grant from the Department of Anesthesia at McGill. So first of all, we want to visualize our topic, right? So there's a two way of visualizing it. So the first way is to project the topic embedding and the condition embedding onto the same space using PCNI. So in here on the left side, we have the On the left side, we have the condition projected to the two-dimensional. On the right side, we have the medication projected to the same two-dimensional space. And here, the numbers are different topics. And then here in panel C and D, we're showing are the five topics. And then the top terms are the one that have a high probability. Right here, you can see that topic eight actually is mapped over here. The color indicate the specific category of the top conditions that belong. Conditions that belong to, right? For example, in here, we can see. So, these colors indicate that each of the top, sort of for each of the topic, the conditions all belong to the same category. So indicating there's certain coherence that we capture by running the model, by leveraging the graph embedding information. So, I wouldn't get into detail of the specific medical terms for the interest of time. And we also apply a more specific apply a more specific task in this case we're trying to use the topic to predict this chronic muscletal pain right the cmk right so that's one of the sort of application of interest from our collaborative uh so in here we assess the robustness of our method so in other words we're starting with the the overall 802 condition and 443 uh medications and then we progressively remove the uh sorry this is medication and this is condition we progressively remove And this is condition, we progressively remove the feature that have a linear correlation with the condition that we're trying to predict, with the phenotype we're trying to predict. Otherwise, because there are some medication conditions that are too obvious to the model, right? As you can see that as we remove more and more features, so the model performance decrease. But at the same time, we see that using a topic mixture as an input all perform using the raw data, right? So the raw data is more sensitive to the feature removal, right? So this is evaluating. Removal. So, this is evaluating based on the area under the ROC curve, and this is evaluating based on the area under the precision recall curve. So, the same message remains. And then we take a look at the top topics that are predictive of CMK pain. So these are the top three topics that are positively correlated with the CMK phenotype. And here are the bottom three topics that are negatively correlated with the CMK. Negatively correlated with the CMK phenotype. So, this is characterized by the Lasso regression coefficient. As you can see, that the first two topics, topic 56 and 34, the top condition all belong to this masculine skeletal system, despite the fact that the model doesn't know the general categories in this case, right? So, and then here, when we look at the actual individual terms, rheumatoid arthritis, some of these conditions are all indeed related to the ECMK. Indeed, related to the CMK condition, right? So, which actually sort of reassured that the model that the topic that we learned are clinically meaningful. And then we expanded our analysis over a diverse set of pain. So in here, we're evaluating the prediction accuracy compared with the baseline method. So our model, proposed model is the blue bars. In general, we see that we outperform the baseline methods across. The baseline methods across all the paint type that we're trying to predict. So, one limitation in that model is that we're learning the graph using no-to-vac in a pipeline approach. In our second contribution, we're trying to train an end-to-end model where we're learning the graph and the patient information simultaneously. So, in here, we're leveraging this graph attention network, right? So, abbreviated as a GAT. I think for this crowd, I don't need to go. I think for this crowd, I don't need to go into too much detail about the graph attention network, but the basic idea is that you're learning the attention between any pair of nodes. So in this case, would be node I and node J, where you're learning the attention where A is a learnable weights and W's are the learnable weights. And then you're using this attention to represent a specific node I, right? So in here, you see that HI is a representation of node I, which is basically a parameterization by Basically, a parameterization by the alpha, which is the attention over all of the neighbors of the null i, right, in this case. So I think there's a typo here should be I instead of word here, here should be I. There's an I missing. This is basically the neighbors of the null I, right? So J belong to the neighbors, where you represent that I using the self-attention mechanism. And then here, you can have a multiple attention weights, right? So these are called the multi-head attention, right? So in here, for example, hat attention right so in here for example kind of a three hats right and then and then in the end you're going to uh do an average cooling over those k hats to represent each node with the final representation so very often how do you learn a network when you learn it you you impose some labels over the nodes and then you can minimize the quasi entropy for the node label but in our case we're doing unsupervised learning as i mentioned so what we do is that we take this embedding We take this embedding and then we're going to use it in our medical code embedding, right? So, in the similar framework as I mentioned in the previous study, where we're using the GETM, in this GATT GETM, we replace that notable back with the GAT framework, where the beauty of that is that you can learn everything end-to-end, right? In other words, you first start with your encoder, right? So, this is the same encoder as the GETM, where you start with a bag of words. Where you start with a bag of words, and then the encoder is going to output the theta, which is the logistic Gaussian theta. And then theta is going to multiply alpha and the rho. So in here, alpha and rho are sort of multiplied together to give rise to beta, where the rows are the output from the GAT network, right? And then where we're learning by maximizing the evidence lower bound, we are actually learning the GAT networks simultaneously. Networks simultaneously with the encoder networks. So both are amortized learning. In this case, it's more coherent learning. So if you're interested, this is also uploaded in archive for details. So we apply this model to Montreal EHR data. So this consists of 1.2 million patients, right? So it's almost like 25% of the Montreal population. So this is just a basic So, this is just a basic quantitative analysis that's showing in terms of reconstruction loss, right? So, we've got better reconstruction loss when we're comparing with the two baseline: the one ETM, the embedding copy model without using graph information, and the GETM learning the graph information through a notable back in a pipeline approach. And here, overall topic quality, we also obtained a higher topic quality. So this is a data set in collaboration with Dr. David Buckridge from School of Public Health. Roverage from School of Public Health, MOGIL. And then this is the embedding generated by the GAP network. So, in here, you can see that the embedding generated for hundreds of different ICB codes and ATC code, they are actually clustered by the categories, indicating that the GAT is able to learn specific categories of each of these EHR codes, which can then further benefit the patient representation. The patient representation learning. The third strategy, as I mentioned, is to be able to use phenotype definition as a guide. So in here, we are using this fecal. So as some of you already know, so the fecals are sort of derived from this genome-wide association study. So it's motivated by that study. So in other words, here you have this table where the fecal Where you the fecal are this specific code, right? So, for example, this is an intestinal infection. So, this is a 00H. And then this consists of a set of ICD9 codes, right? So, in here, we're using this fecal as a definition to guide our topic model inference. So, the key idea is that we want to infer 1500 topics, each topic specifically mapped to one ICD code, exactly one ICD code, sorry, FICO. So, in that, in other words, your topic becomes 100%. words your topic become 100% identifiable. So we propose two solutions or two algorithms to achieve that. So the first algorithm is that we're kind of modifying our MITHR modeling. So this is a factorization for topic modeling approach. If I for modeling this in a complete unsupervised way, we first we take the ICD code for each of the patients. So this is a feature by patient matrices. So we take the ICD code. So we take the ICD code and then we apply this fecal mapping. So here you have this 1500 fecals, each fecal characterized by a set of ICD-9 codes. And then you can count. So here you have this, this multiply this to give you the fecal by patients. And then you can train for each of the fecal a two-component Poisson mixture model. So this two component Poisson mixture model for each of the fecal is going to give you the probabilities for each patient. Probabilities for each patient belong to that specific component that have a higher Poisson mean, right? And then that provides us a hyperparameter for our Dirchlet theta variable. So theta is following this 1500 dimensional Dirchlet distribution, right? Where the Dirchlet, we use this asymmetrical hyperparameter for Dirchlag, where the alphas are generated from these two component Poisson-Mission model. And then that provides a guide, that provides a guide for. A guide that provides a guide for patient-specific hyperparameter alpha, which then allows us to learn this five, which are topic distribution that are fecal-specific. So we're learning this 1500 phenotype topics by using this, leveraging this fecal information. So this is kind of also under review. You can find this paper by archive as well. So how much time do I have? Pretty much the end, you is okay. So, I can speed up a little bit. All right, so and now we apply this to two data sets. So, we're learning consistent information. So, I'm going to skip this. So, the last part is that we propose another strategy. So, that is the previous solution I mentioned that we're using a Poisson Measure model. And then we apply the latent direction allocation. Apply the latent diversion allocation model, which is a sort of pipeline approach. And then, same time, we don't actually model the age as information, as we know that age are important for patient characterizing patient phenotype. So, in this model, we're using this sort of age-dependent Markovian process. So, this is the model general process. So, in here, we're proposing each topic, we have the C topic that characterized by specific FED codes under that fee code. We also have a regular topic that characters. We also have a regular topic that characterizes the entire vocabulary. So, in other words, given the condition, we're trying to infer whether that condition coming from the actual disease fecal or it coming from the comorbidity condition of another different disease, right? As we know, that different diseases, they share different conditions. So, that's sort of the key idea of this general process. And then we're learning this H-dependent Markovian through this another VAE, which using VAE, which are using LFPM, long short-term rendering machine, to capture the MRCall dependency. So here we're capturing the top three phenotype that are negatively correlated with the age. So this is inferred by this model. And then the top three phenotype that increase as patient age. So you can see that the top in here, these are sort of anxiety sort of conditions that as patient age, it kind of decreases. Age, it tends to decrease. And then here, this heart condition as a patient age, it tends to increase, right? Which also makes sense. So, lastly, we applied our model to UK Belbach data. So, I won't get into too much detail about the data processing. So, we identify, so these are just a select set of sort of phenotype guided topics, right? So, you can see that these are sort of consistent in terms of the ICD code and ATC code that characterizing different well-known diseases. Known diseases. So, the main key idea as I started my talk is that we want to actually use this phenotype to do phenol-wide association study. So, in here, so this is one example of a few opts. So, this is focusing on one of the SNPs that are actually causal SNP for LPA protein. So, this is using non-UK biobact data. So, you can see that for just that single SNP, there are many different heart-related conditions that are significant. And then we apply And then we apply to UK burban data using the same SNP. And then we associate this SNP with the 1500 phenotype topic probabilities. And you can see that we actually identify many sort of heart-related conditions. So if I actually put this side by side, so you can see that, oops, oh, this is not a good side by side. Sorry about that. So I guess that doesn't work. I don't say. Yeah. So you can see that in here, the significance. That in here, the significance is up to 15, right? And then in our case, we go up to 40. Obviously, one of the reasons we believe, so this is a preliminary result, but one reason we believe is that by transforming the patient binary fecal to phenotype probabilities, we're able to increase statistical power to identify the FIWAS associations. So this is for the heart-related condition. And then from another separate paper, we have a fine map. We have a fine map, some SNP that are associated with metabolic measurements. So, this is from another key student in our group. We propose this sparse pro approach, which is a fine-mapping approach. If you're interested, you can also take a look at that paper. And then we take that SNP, and then we associate it with our 1500 phenotype from UK Belben, because that SNP is also fine-mapped using UKB data. And then we identify, because that SNP is associated with a metabolic traits, we see that these traits actually belong to the integral nutrition. On to the integrand nutritional metabolic, right? So, including type 2 diabetes, which is pretty interesting. So, in summary, we propose, so it's kind of we propose this idea about modeling multimodal EHR, which allows us to better quantify phenotypic risk as a probabilistic topic scores. And then harnessing knowledge graph, as well as anchoring 1500 fecal-guided phenotype, will enable a better way to identify inferred. To identify, infer identifiable and interpretable topics. So, with that, I would like to acknowledge my group. So, our group is a fairly new group. So, we started in 2019. So, these are the student trainees in our group who produced the paper that I presented today. So, we are recruiting. So, for some of the audience, if you're kind of looking for projects, feel free to reach out to me. Also, take a look at the website, our websites. Website, our website. So, here I today I focus on phenotype and genetics. We also work on a single cell model. So, I didn't get a chance to talk about that, but I'll be happy to present that some other day. Thank you for your attention.