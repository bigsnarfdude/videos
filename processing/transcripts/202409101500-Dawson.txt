So, this work is done in collaboration with my advisor, Takahito Nakajima, as well as my honorary supervisor, Luji, and CA, as well as these two guys, Jacob and Dennis, at Tulane University. Their work is going to be featured at the end of this talk. Before, though, I go into the research, I wanted to give you an introduction to our center. To our center, because this motivates the kind of thing that we are working on. So, our team is located at the RICAN Center for Computational Science, and it's in Kobe, Japan. And you can see a picture of our skyline right here. So, our center, what makes it special is that we are home to a supercomputer, which is called Fugaku, which right now is the number four fastest supercomputer in the world. At the time, it came out, it was number one, but it's now down to. Was number one, but it's now down to number four. Now, this is based, of course, on public data. And we saw from the previous talk that there might be something in China that competes with this. But let's say for the public data, we're number four. This machine is different than most of the other top supercomputers in the world. Most of the ones you have now are based on GPUs, whereas our Fugaku is based only on CPUs using an ARM architecture. An ARM architecture. Each one of the nodes that we have has 48 cores and 32 gigabytes of memory, and it competes with these GPUs by having rather large vectors up to 512 bits, so that each node can be quite powerful on its own, despite being a pure CPU node. And then what we do is we connect a lot of these nodes together, more than 100,000 of these nodes, with a very fast network. And then we tell the users that if they need more power, Tell the users that if they need more power or if they need more memory, take advantage of this network by using more cores, using more nodes for whatever calculation they want to do. And this is different than the GPU machines again, where each node might have something like six to eight GPUs and be quite powerful on its own. Okay, so we have a supercomputer at our center. So it's of course our job to develop the codes that can run and take advantage of these supercomputers. Of these supercomputers for doing new kinds of science. And our work is manifest in these two different codes I've listed here that we work on. They're both Konchan DFT codes. The first one is called Big DFT and the second is called NTChem. These two codes distinguish themselves from each other by the choice of basis set. In the Big DFT code, we use something called a wavelet basis set, which you might be less familiar with, whereas NTChem uses Gaussian or Whereas NTChem uses Gaussian orbitals, which is as standard as it gets in molecular calculations. So we work on these two different codes and we want to take advantage of their different numerical properties to decide what kind of features we're going to implement. So Big DFT, because it uses this unusual wavelet basis set, we're trying to think how can we use the numerical properties of wavelets to develop new kinds of methods and theories. So one of the things that really makes Big DFT a strong code is that VFTS strong code is that it uses the numerical properties of wavelets so that we have a very flexible Poisson solver that is able to treat all kinds of boundary conditions. So it can do the periodic calculations you need for materials that I know most of you are interested in, but it can also do surfaces, wire, and free boundary condition, as well as with implicit solvent. NTCAM, on the other hand, as I said, it's based on Gaussians. So instead, we take advantage of the nice properties of Gaussians so that we can implement. Properties of Gaussian so that we can implement not just standard con-chan DFT, but also hybrid DFT and double hybrid DFT. So, most of the focus on that project is about trying to couple different levels of accuracy with different methods. And both of these codes are hybrid MPI OpenMP codes that run on this Fugaku supercomputer, and they're aimed at treating very large systems. Or at least we think they're aimed at very large systems. You guys might, you know. Guys, might not be so impressed by this. In this case of Big DFP, the largest system we've computed is about 50,000 atoms, which is a simulation of the entire spike protein. Whereas in NTChem, we've done 8,000 atoms, in this case, using hybrid DFT calculations. So it's more expensive than plain DFT. And we've done more than 8,000 atoms with hybrid DFT using our code. And again, these now compared to the orbital-free guys and previous talks. The orbital-free guys and previous talks we had might not seem so big by comparison, but I think these are quite large for most of the people who do Cornishat VFT. So again, this is a workshop that's primarily focused on orbital-free DFT, but I think that the reason why Sergei wanted me to give a talk here is so that I could talk about our work on Khonchan DFT for large systems, and then we could try to find different parts, different Hearts, different ways that these two methodologies can meet and ways we can learn from each other. So, to do that, I wanted to go over real quickly some of the basic numerical methods that are being used in a Khan-Chan DFT code and talk about how we manipulate them and modify them for treating large systems. So, many of you, I'm sure, are familiar with doing QuanChan DFT for materials. You know that you use things like plane waves or real space as the discriminatory. Or real space as the discretion of your Kohn-Cham orbitals. And those Kohn-Cham orbitals are the things that you're primarily interested in in materials code. You have this matrix, which is n by m, for n con-cham orbitals and m basis functions that you need to store in memory somewhere. And then you have to do a number of computational steps on this matrix, which have costs, various costs, but most severely, there are costs like n squared m. Like n squared m, when you do things like keep making these orbitals orthogonal to each other or subspace diagonalization, et cetera. You can see such an algorithm here. So these costs are quite high compared to what we've seen from people doing orbital free DFT. These are costs that grow with a third power of the system size and the second power in memory. And that makes very large systems quickly intractable. Because as soon as you want to do something that's 10 times bigger, you're now doing something that's a thousand times. You're now doing something that's a thousand times more expensive or 100 times more memory. There is another formulation of Quan-Cham VFT, which is not used in the materials codes so often, but is more popular for molecular systems. And this is because it maps well to basis sets like Gaussians or numerical atomic orbitals, which are these atom-centered basis functions. In this formulation, which we call In this formulation, which we call the density matrix formulation, our primary interest is no longer the Cohen-Cham orbitals, but instead it's the full density matrix, which again now has a memory cost of m squared, because it's basis functions by basis functions, and the now computational cost like m cubed, because you need to diagonalize the entire Hamiltonian and form this whole density matrix. So, this formulation, which might sound worse at first, Worse at first is used a lot with basis sets like Gaussians. So we use it in our NTCam code. And it's also used in our Big DFT code in one of its special modes because while Big DFT is primarily based on wavelets, it also has a way of constructing this type of basis set. The way that it works is we start with the solution of atoms where we have these local basis functions that come from solving each atom. Functions that come from solving each atom alone. And then we do this process of optimization as we are solving for the density matrix, as we're solving for the ground state. We are also optimizing the basis functions with the constraining potentials so that they stay nice and local and they're quasi-orthogonal and very well-behaved. So these local orbitals are represented by wavelets, but end up looking more like the Gaussians we have in our NT-Chem code. And so because of And so, because of this process of making a very optimized basis set for big DFT, or because the Gaussians have a nice basis to them, in this density matrix formulation, the ratio of n to n is low. It's not so many basis functions that you need per each orbital. And that means that this density matrix is not too bad. It's not too much worse than the previous formulation we showed. But still, we're talking. But still, we're talking about n cubed, and this is not tractable for very large systems. So, how do we get out of this? We are going to take advantage of Kahn's nearsightedness principle to try to reduce the cost in memory and calculation time associated with this density matrix formulation. I hope that you have some familiarity with this principle, but essentially it describes a localization that exists. Localization that exists in the electronic structure, which is exponential for insulating systems and metals at finite temperature, and the polynomial at zero temperature. If you've done calculations with a materials code, you might have already taken advantage of this localization that exists, this principle, when you do something like compute maximally localized Winier functions so that you can help understand what's going on in a system. I wanted to show you this. I wanted to show you this locality with some pictures as well. So, if you look at this, it is some larger system I've computed, and you look at the blue bars right here, this is the number of elements in the Khon-Cham orbital matrix, which are below a certain threshold. So you will see that the blue is quite low down here, which shows how the Quantam orbitals are well delocalized throughout the system. Whereas as soon as you construct something like these green bars, You construct something like these green bars here, which are Cholesky orbitals, it's one type of localized orbital similar to one year functions, you'll see that you can get a very compact representation depending on how much error you can tolerate. And you will also see this locality when you construct that full m by n density matrix. It naturally expresses this locality if you have a localized basis set. And you will see as well that these orange bars are quite local as well. Quite local as well. I also plot here a density matrix which comes from a peptide. And in this case, the things that are around here represent basis functions that are far away from each other. And you see there's a sparsity that is beginning to emerge. And in fact, what you can do is you can say this density matrix has a number of very small elements in it that is becoming more prominent as we go to bigger and bigger systems, so that you can approximate this matrix as a sparse matrix, and you're able. Has a sparse matrix, and you're able to reduce the memory footprint and also calculation costs without too much loss of accuracy. I think that you could probably figure out on your own how to reduce the memory cost using this approach. You would just, of course, store only the values that are above this threshold. But how do you take advantage of this sparsity so that you can reduce the cost, a computational cost? Well, again, as I mentioned in the Again, as I mentioned, in the, I have to remember which direction we're going. As I mentioned, one of the main computational costs we worry about is this diagonalization. And this is where we construct the density matrix from the Hamiltonian. So we'd like to come up with a method that removes this diagonalization step. It's called the diagonalization-free method and takes advantage of the sparsity to get to the density matrix from the Hamiltonian and take advantage of the sparsity to reduce the cost. To reduce the cost. So, what we do in our code is to switch to a new method, which is based on a theory, a mathematical theory called matrix functions. You might have not heard of matrix functions before, but for our purposes, and mathematicians will get mad about this, but for our purposes, we can just compare them by analogy to standard functions. So you can look at this standard function here, f of x is x squared. Well, the matrix function equivalent of that. matrix function equivalent of that is just taking the matrix, this symmetric Hermitian matrix, and multiplying it by itself. Whereas the standard function 1 over x would have the matrix function analog of the inverse of a matrix. And for people who have done, as we saw with this wave operator propagation before, people who have done differential equations will see this e to the x that you have to worry about. There's also a matrix version of this, which is a matrix differential. Version of this, which is a matrix differential equation. So, what matrix function do we need to use? This function should be one that maps x right here and our chemical potential mu to be one if x, this is the eigenvalue of our Hamiltonian, if x is below the chemical potential. Otherwise, it will be zero. This is setting the occupancy based on these values. And so the equivalent for the matrix function. For the matrix function, is that we take in the Hamiltonian H and the chemical potential, and we get out the density matrix. So, how do you calculate this matrix function? What are we going to do if we want to go from the Hamiltonian to the density matrix using this kind of mathematical framework? This is done the same way that you might compute a standard function that you thought was very hard to calculate. You can do it by approximating that function by a polynomial. That function by a polynomial. So, one such polynomial approximation you can make to this special function comes from the McQueenie purification method, which you can see right here, where we just take our matrix and we do this scaling on it, this Hamiltonian scaling and shifting. And then we do this very compact iteration where we're squaring the matrix and cubing the matrix. And as you continue to do this and you iterate over and over, you eventually get a spectra that looks like this. Spectra that looks like this, and that's matching what you would do if you computed this by diagonalization. So, we were interested in this matrix function framework, and so in using it to replace the diagonalization steps. So, we decided to make a library, which was mentioned by the last speaker, which is called NTPoly, which implements these matrix functions and computes a wide variety of them, including the McGuinie purification I showed you before, but also other methods that don't require optional. Other methods that don't require a priori knowledge of the chemical potential or methods that are used outside of electronic structure calculations. So, the main idea of this library was that if I want to compute these polynomials, all I have to do is matrix addition and matrix multiplication. And furthermore, because I know, oops, I keep going backwards. Because I know that this matrix is sparse, if I implement instead, Implement instead a sparse matrix, sparse matrix multiplication very well based on filtering these small values to maintain sparsity. I should be able to compute this matrix function, and I'm always using the sparsity of the matrix to reduce the computational cost. And this allows us to very efficiently exploit these emergent properties of our system for a computational method. And so we implemented this sparse matrix multiplication. Sparse matrix multiplication and addition in a way that scales well on a supercomputer like our Fugaku supercomputer. And it can, these are K-computer results, but you will see that even on the old computer, we can outperform a tuned eigenvalue solver and solve for the density matrix very quickly. And this is because we're taking advantage of this sparsity and this new framework. Now, one thing you might notice by looking at this picture right here is that. Picture right here is that the matrix function I showed you before would only work for insulating systems. Because what you would need to do if you want to compute a metallic system at a finite temperature is that you would want to approximate, you need a matrix function that approximates this function or that computes this function right here, which is the Fermi-Dirac distribution. So, how do we compute this one? What method should we use? Well, one very general way of approximating some function is to use. Approximating some function is to use, as we saw from two speakers ago, is to use Chevyshev polynomials, where you construct a set of polynomials that, when you add them together with the right coefficients, gives you the function that you want. So let's look at the Chevyshev polynomials of, oops, something has fallen off the edge right here, but let's look at the Chevyshev polynomials of matrices. You can see, once again, we're just doing these spary multiplication and addition operations. And in fact, you can very compactly. In fact, you can very compactly describe how to generate these Chebyshev polynomials. And you'll see that I'm always just multiplying on the left side by the matrix that goes in, the Hamiltonian, to compute this. And once you have enough polynomials used, you can get a very good approximation of the Fermi-Dirac distribution. So, with that in mind, we developed a library that could implement That could implement these polynomials. And we implemented it in a way that tries to take advantage of the fact that we're always multiplying on the left by this Hamiltonian by instead casting it in terms of a sparse matrix, sparse vector multiplication. And so we can get a very efficient method for the metallic case too. And in fact, our collaborators have applied this to things like both tunsten to show it in practice. The last part of this talk, I wanted to talk about our newest method, which is called wave operator minimization, which is aimed again at computing metallic systems, particularly at things at finite temperature. So the origin of this method right here comes from the block method, which is used for approximating this function right here. Instead, it's sort of this classical Maxwell-Boltzmann distribution where the density matrix, and I hope you can see this. Where the density matrix, and I hope you can see this in the back, is just e to the beta h. So, many, many years ago, I guess the name block is there, so maybe 100 years ago, already was known an efficient way you could calculate this. You just start with the density matrix at infinite temperature. And that's just the identity matrix because everything is equally probable. And then you derive the gradient with respect to the inverse temperature right here. Inverse temperature right here, which is very easy to do. And then you just use this gradient to go from infinite temperature to the temperature you want to simulate. And the only thing you have to worry about is that you should try, as you're doing this evolution towards the temperature you want, you should make sure that you maintain the hermeticity of it. And this can be done by just symmetrizing the gradient like this. So, this is how you calculate with the block method. You calculate with the block method this distribution, this matrix function, but again, we need to compute a different one, which is this Fermi-Dirac distribution. Well, that's okay. We just need to now derive the same process, where we can derive a gradient of the Fermi-Dirac distribution of our density matrix with respect to the inverse temperature, and also a starting point. Point. But in this case, we have to worry about maintaining not just the hermeticity, but also the positivity of the density matrix. You don't want to have any negative eigenvalues. And this is done by just taking the square root of the density matrix, which is what is called the wave operator. So that's why it's called wave operator minutization. So what we do is we do these derivations in terms of the wave operator. And here is what we have at infinite temperature. We start again at infinite temperature. Again, at infinite temperature. And then we get the gradient with respect to the inverse temperature. And then we can use any propagation method to get to the temperature you want. It's like a time propagation. But instead, we have this weird variable of inverse temperature. And so we chose in our implementation to just use second order runch cutter with an adaptive step. And that is how we get this to work. So again, we're starting at infinite temperature and we're going to the temperature we want. So this is kind of a The temperature we want. So, this is kind of a cooling process, you can imagine, or a quenching that's going on. And the important thing about this method is that the density matrix will actually start extremely sparse. Here's a percentage of non-zeros at very high temperature, and then slowly get filled in as we get to that target temperature. And our adaptive step gets us there pretty quickly. And this means that we can always take advantage of the sparsity, and there's no unphysical. And there's no unphysical intermediates that would make the matrix dense again. And you'll see that it does take several, a lot of multiplications to get where we're going. And it depends on the gap. You could do the zero temperature case as well, though. But we will get there and we can use the sparsity to make an efficient method. So now I've shown you two different methods for computing at finite temperature, but I would just like to compare the two so that you can walk. To compare the two so that you can walk away and understand the differences. The main motivation we had for this new approach, as opposed to the Chevyshev polynomial approach, is that the Chevyshev approach, the efficiency depends very heavily on the spectral width of the Hamiltonian, which is your largest eigenvalue and your smallest eigenvalue, how far apart they are. What you will see is that if you compare the wave operator minimization with FOE, if you do use matrices coming from a code like C. Use matrices coming from a code like Siesta, which has a very wide spectral width, you can get a more efficient method. Whereas BigDFT has a very compact spectral width, so FOE is better there. But for something like NTChem, this would be the way to go. The other problem with FOE is that you need to store all your intermediate polynomials because you're going to use them for root finding to figure out what the chemical potential is. And this means there's a huge memory footprint from FO. A huge memory footprint from FOE method. And this does not exist with wave operator minimization because, gone the wrong way again, we only have a few intermediate matrices and a very compact expression of how to go. So now I'd just like to finish by connecting what we do to orbital-free DFT, because I have some points of contact that I thought of. The first one is that as I worked on this wave operation, As I worked on this wave operator minimization idea, and as these guys in Tulane came to me with it, I started to think that our equations now actually resemble orbital 3 DFT a little bit, right? That's because in this case, we're working again with one object, which is the density matrix, instead of this Kohn-Sham orbitals, which is sort of many objects. The second thing is that when sparsity is exploited, now the density matrix and the density both have order and data. Of course, the density matrix has more. Of course, the density matrix has more data in it because it's a matrix, but it's still order n. So there's the same amount of data in a sense between the two approaches. And also, interestingly enough, we're using the square root of the density matrix in our numerical methods, which is I understand is what you do in orbital-free DFT. Another point of contact that I want to draw attention to, though, is that now we know if you have an orbital-free DFT that is a temperature-dependent. Temperature-dependent exchange correlation functional and kinetic energy operator, you could maybe use the same minimization approach that we're doing here of quenching to try to get to a solution that you're looking for, though there's more of a risk of local minima in orbital-free DFT than in our case. But then lastly, and maybe most importantly, is that now I want to highlight the fact that we have codes that can, using the order n approach, calculate very large systems, again, like 50,000 atoms. Systems, again, like 50,000 atoms. And ES includes now at high, at temperature, finite temperature, and high temperature. And this might be a good opportunity to try to compare the two methods, particularly when we have features that have an intrinsic length scale to them. And so with that, I have a slightly more time, but I'll just leave this with a conclusion. And just with one other point, which is to say that CohenSham DFT people don't use order end code. Don't use order end codes a lot because they don't know what they're supposed to calculate. You know, Kwancha and BFT people, they're used to calculating the smallest possible system that they can to get the physics they do. But you orbital-free BFT people, you know, can do 100,000 atoms on your laptop. So you're very happy to calculate big things. So please use your experience to teach us what we should calculate with LRAN methods. Okay, so with that, thank you for your time. 