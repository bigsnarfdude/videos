Thanks, Professor Ting, for the introduction, and also thank the organizers for the kind invitation. It's really my great pleasure to attend this workshop and introduce our recent progress on the stochastic inverse problems. And this talk is based on several joint work with Jia Liang Li from Hunan Normal University and also with Pei Jingli from who just moved back from the Who just moved back from Purdue University to our institute in the Chinese Academy of Sciences? So, in this talk, I'm going to talk about both the inverse problems and also the stochastic problems, as well as their interactions. And I'm going to give a very brief explanation that what's the difference between direct and inverse problems, and also between stochastic inverse problems and the deterministic ones. So, um uh in my talk, I'm going to firstly give the formulation of the problem. And the randomness involved in the stochastic inverse problem is characterized by a class of generalized Gaussian random fields. And if we take this kind of random fields as the random source or random potential, then in the following parts, I will introduce the uniqueness for these two kinds of problems. And actually, for these two problems, And actually, for these two kinds of problems, they are corresponding to stochastic partial differential equations with an additive noise or a multiplicative noise, respectively. So, that's somehow the relationship between stochastic inverse problems and stochastic partial differential equations. So, firstly, for the inverse problem, it is well known that it has many applications in the fields of, for example, the remote. The fields of, for example, the remote sensing, the medical imaging, or the geophysical exploration, and so on. And actually, in these applications, sometimes it will be more reasonable to consider the random perturbation into account. So the randomness may come from the ancestry of the environment, or from the randomness of the media, or from the fine-scale fluctuation in the measurement, or in the simulation. So, for example, Simulation. So, for example, for the active noise cancelling earphone, which is commonly used now, we can regard it as a random source problem, actually. So, for this kind of active noise canceling problem, we are going to produce an opposite acoustic wave to cancel the environmental noise. So, the environmental noise can be regarded as a random source. So, that's why we consider it as a random source problem. Source problem. And also for some imaging problems, if the media is perturbed by some heat, water, or some other uncertainties, then the media should be considered as a random media. So that's the purpose to consider stochastic inverse problems and mainly based on wave equations. And for this kind of randomness, we can say that they have some typical features. For example, they usually have very low regularity. Usually have very low regularity. And for the randomness here, they may have different forms. For example, the commonly used Gaussian white noise or color noise or even non-Gaussian noises. And also, for stochastic problems, especially for the inverse problem, it is meaningless to recover only a single realization of the random field. So instead, we need to use a lot of realizations. We need to use a lot of realizations to recover the statistics of this kind of random fields. So that's the features for random fields. So if we want to characterize all these features, it will be reasonable to use random field. And we take the random field into the consideration when constructing the mathematical model. Then the model turns to be a stochastic partial differential equation. And in this talk here, I mainly focus. And in this talk here, I mainly focus on the time harmonic case. And I give a linear stochastic partial differential equation here. L denotes the linear differential operator. K denotes the wave number, that is the frequency. And U is the wave field. And for the functions rho and f here, they are random fields. And for f, for the right-hand side function f, it is the source function. Source function and for the function rho, it is the potential function. So, based on this kind of linear stochastic partial differential equation, the direct problem is, of course, to get the well-posedness and the regularity of the solution u if the potential or the source F are given. So, that's a direct problem. But for the inverse problem, we need to determine the potential rule or the source F based on some limit. Source F based on some limited measurement of the solution U. By seeing the limited measurement, we mean that actually in the real applications, we can only get the observation outside of the schedule. And we cannot get the measurement in the whole domain, but instead we usually get the measurement in unbounded domain, which is far away from the scatterer, or from the boundary measurement, or even from the partial. Environment or even from the partial boundary data. So, based on this kind of incomplete data, we want to recover the potential function rule or the source function f. This is almost an ill-posed problem. And especially if the potential or the source f are random fields, then it is not reasonable to recover a single realization. That means we cannot recover. We cannot recover the function œÅ and f itself. We need to recover their statistics. For example, their expectation or their variance. So that's the difference between the stochastic inverse problems and the deterministic ones. Okay, so for example, I just give a simple example for the random field. And we consider a complete probability space with a sample space omega with a sigma algebra. With a sigma algebra F and with a probability measure P here. And a random field F is defined over the sample space times the Euclidean space. That means it depends on the sample path omega, the realization, and also the spatial variable x. And we assume that for simplicity, it is in a Gaussian-white noise case, that is, it's a small function, the square root of mu times. Function, the square root of mu times b dot. B dot here is the formal derivative of the Brownian motion. So that is the so-called white noise. And the smallest function mu is given in this exponential function case. So for this kind of random field, if we plot a single realization of it, that is the first figure here, in the blue line here, and we can find out that the single path here is extremely rough. And if we plot a And if we plot another realization, that is the red one, and we can find out that this is different from the first one. For the blue line here and for the red line here, they are different. So that is why I say that we cannot recover a single realization of a random field, because it cannot characterize the whole behavior of a random field. Okay? So instead, if we plot 100 realizations, 100 realizations in this in the last figure here, and we calculate their variation. We will get a small function, that is the yellow line here, the yellow curve here. And we can get that the yellow curve is exactly the function mu. Okay? So for the function mu here, we also call it the strings function. It actually characterizes the overall perturbation of the random field. So for stochastic inverse. So for stochastic inverse problems, we aim to recover this kind of strength function. Okay? Recovered function mu. So that's our purpose. And for this kind of stochastic inverse problems, there are several challenges. And the first one is the non-uniqueness. And the non-uniqueness actually is a problem for both the deterministic inverse problems and also the stochastic ones. For example, in the deterministic setting, In the deterministic setting, it is easy to construct a problem which is not unique. So, for example, a typical example is the non-radiating source. So, that is to say, if we consider a small function way, which is compactly supported in the domain D, and we construct the source function f as Laplacian plus k squared performed on the function we. Driven by this kind of source, if we Even by this kind of source, if we consider the acoustic wave, we can easily get that the solution u is exactly equal to wave. So that means outside of the support, the solution u is always zero. So that is to say, if we can only get the exterior measurement of the solution, we cannot get any information of the source function f, which is supported in the domain D. So for this case, the source function function function function function function So for this case, the source f is called a non-relating source. We cannot uniquely determine the source function f for this case. So this is a deterministic setting. But for the random setting, actually, if the source is assumed to be a random field, in general, it can never be a non-relating source. It seems to be a good news, because in that case, we hope to find some uniqueness results. We hope to find some uniqueness result. But unfortunately, if we consider the one-dimensional acoustic wave and the random source is assumed to be the Gaussian-white noise, for the one-dimensional case, based on the fundamental solution, we can get the expression of the solution. Since this is a linear equation, it's easy to get that. And the solution is actually an A-tool integral. Okay? And for this integral, if And for this integral, if we take its average, that's zero. So we cannot get any information from its expectation. So instead, we consider its variation. That's the second moment. So if we consider the second moment, since for the Brandy motion here, we can use the Ito isometry, such that the second moment is equal to this kind of deterministic integral. And for this integral here, we can find out that x here is the observation. X here is the observation point, but for the integral on the right-hand side, it is independent of the point X. So that is to say, no matter where you observe the wheel field U, you will always get this constant for this kind of second moment data. So that is to say, we cannot use this kind of variation of the solution to recover the strings function mu for this kind of stochastic inverse problem. Inverse problem. So even though the random field is not a non-relating source, we cannot get the uniqueness either based on this kind of data. So that is to say, for stochastic inverse problems, it depends heavily on what kind of data you use. And actually, it also depends on what's the dimension of the problem. So for this example here, I only give a one-dimensional example. And we can also consider the three-dimensional case. Consider the three-dimensional case again for the acoustic wave with a Gaussian-white noise. And they have the same form actually, but in the three-dimensional case. And similarly, based on the fundamental solution, of course, which is different from the one-dimensional case. And we can also get the expression of the solution, which is still a stochastic integral. And for this stochastic integral, if we consider its second moment, we can get that it is also equal to a determinist. It is also equal to a deterministic integral here, but in this case, it's a convolution. So the result here depends on the observation point x. So you can imagine that if we can get the measurement in a bounded domain based on the deconvolution, we can get the strengths function Œº actually. So for the three-dimensional case, the problem is unique. The inverse problem is unique, actually. But the problem Actually, but the problem turns to be that for this kind of inverse problem to get the measurement on the left-hand side, we need to approximate the expectation. So for a stochastic problem, for example, if we use the Monte Carlo method to approximate this expectation, we need to use a lot of realizations, for example, p-realizations, and take average to approximate this expectation. Expectation. It implies that we need a lot of realizations in reality to get this kind of measurement data. So this is really time consuming. So for the stochastic inverse problem, another challenge problem is that how to reduce the amount of the merriment. So for example, can we reduce the expectation here and use just a single realization to get this kind of recovery formula? So that's another challenging problem for So that's another challenging problem for stochastic inverse problems. And all these examples here are actually about the inverse source problem. And if we consider the inverse potential problem, that is to say the potential function rho here is n-known and to be reconstructed. And in this case, for this linear equation here, we can also get its equivalent integral form, which is known as the Liebmann-Schr√∂ner equation. Is known as the Liebman-Schr√∂nger equation. And for this problem, we can find out that the anonym potential is coupled with the solution U here. And for the solution U inside of the support D, in general, we cannot observe it. We can only use the exterior observation for the inverse problem. So in this case, because they are coupled together for this kind of inverse problem, it is called a nonlinear problem. Even though the equation itself Even though the equation itself is a linear equation, but the inverse problem here is a nonlinear one. So, this is the difference for the inverse potential problem compared with the inverse source problem. And again, if the potential is assumed to be a random field, in this case, if we take expectation on both sides of this equation, we can get that in general, the expectation of rho times u is not. Of rho times u is not equal to the expectation of rho times the expectation of u. So that is to say we cannot simply reduce this stochastic inverse potential problem to the deterministic one. Okay, so they are different. So for the inverse random potential problem, it is highly nonlinear. So that's another challenging. And for the inverse random potential problem here, we usually assume that. Here, we usually assume that the potential, the random potential, is extremely rough. It's usually not a classical function, but instead, we should interpret it as a distribution, which belongs to this kind of Sobblev space with a negative power. So, in this case, we need to check what's the unique continuation principle for this kind of rough potential, such that we can get the well-posedness of this stochastic partial differential equation. And also, And also, since rho here is a distribution, so if we consider its statistics, for example, its correlation or some other statistics, they are all EO posed and should be interpreted as distributions as well. So another problem is that we need to check what kind of random fields we can deal with. We can choose them as the random potential such that we can deal with this difficult. Such that we can deal with these difficulties. So that's another challenge of problem for the inverse random potential problem. Okay, so in this talk, in the following part, I'm going to answer these three questions. The first one is that what kind of random fields we can choose as our random source or random potential. And of course, we wish that the class of random fields is large enough such that it could include. Such that it could include most of the random phenomena in reality. Okay? And the next question is that driven by this kind of random fields, how can we get the uniqueness for the inverse random source problem and the inverse random potential problem? Okay. And uh the third question is that can we further improve the uniqueness result such that we can use as few realizations as we can. Realizations as we can. So that is how to reduce the amount of the merriment for this kind of inverse problem. Okay? So firstly, in the second part, I'm going to introduce what kind of random fields we can deal with. So that is the generalized Gaussian random fields. Okay, so before I introduce the generalization, let's first recall the Gaussian white noise case. So as the example I gave you before, The example I gave you before, f is in the form the square root of mu times b dot. B dot here is Gaussian white noise, which is actually a distribution since the Brown motion is not derived, is not differentiable. So for this kind of Gaussian fields, if we take the expectation, it's zero. So this random field is uniquely determined by its covariance operator. And its covariance operator here is defined. This covariance operative here is defined as the covariance of f performed on test functions phi and psi. And for the Brownie motion case, we can use the ITO formula, the Ito isometry. So we can simply reduce this expectation as this deterministic integral. And in this case, we can simply get the relationship between the covariance operator Q and the strength function mu here. And the strength function Œº here. And based on this kind of relationship, we can get a recovery formula for the white noise. But apparently, this kind of definition is not easy to generalize, because for some other noises, we cannot use the eto isometry anymore. For example, for the fractional brand motion or for some other non-Gaussian noises, we cannot use this kind of formula. Okay, so to get a generalization, Generalization. Instead, we consider its equivalent definition. And we found out that for the covariance operator of the Gaussian-white noise, it's actually a classical pseudo-differential operator. And we know that for a pseudo-differential operator, it is uniquely determined by its symbol. Okay, so that's the relationship between the covariance operator, which is a pseudo-differential operator, and its symbol sigma here. And based compared these two definitions. And based compare these two definitions, by a simple calculation, we can get that for the white noise case, the symbol sigma is exactly mu of x. So that is to say, the symbol here has only the principal symbol without any residual term, and it depends only on the spatial variable x, independent of the second variable, could say here. Okay? And this definition will be easy to generalize. We can assume that for We can assume that for a Gaussian random field, which we call the isotropic Gaussian random field. And we assume that its order is negative m, which means that the covariance operator of f is a pseudo-differential operator of order negative m. So that is to say, the operator has a general symbol sigma. Sigma here has not only the principal symbol term, but also a general residual term here. Okay? And Okay, and again, here is the strength function. It also characterizes the perturbation of this kind of random field. So, this is also the quantity we want to recover in the stochastic inverse problem. But the problem is that to recover the function mu, we have to find some ways to reduce the effect of the residual term for this kind of generalization. And also, for this kind of random field, we can get the kind of random field we can get the relationship between its symbol and its covariance kernel if it exists. And actually the equal here holds in the distribution since actually because f here is a rough field. Okay so this is the generalization and for example we can construct a frictional noise F which is the square root of mu times a fractional level. Times a fractional Laplacian performed on the white noise. Okay, for this kind of random field, we can check that it is actually an isotropic Gaussian random field, as we defined before. Because if we calculate the covariance operator and to get its symbol, we can get that the symbol is exactly mu x times casi to the power of negative m without any residual term. Okay? And for this And for this kind of fractional noise, we can also check that for the order M belongs to the domain D to D plus 2. That is, D is the dimension. So in that case, we can define a generalized Hearst parameter H, which is m minus D divided by 2 here. And this term belongs to the interval 0, 1. interval 0, 1. And in this case, we can check that after a translation, this is exactly the classical fractional Brown motion. Okay, so for this kind of fractional Brownie motion, we plotted the figure here for a single realization of it. So if we choose H as 0.25, we will get the first figure here. And it's relatively smooth. And if we consider the case that H. We consider the case that h equals one half, which is actually the white noise. Because if h equals one half, that means the m equals d plus one. And in that case, the f here turns to be some kind of small modification of the white noise, which is actually the brown motion. And also, if we consider h equals zero. If we consider h equals 0 to 5, we will get a rougher case, a rougher pass for the random field, which means that if h is smaller, then the random field will be rougher. And it contains different cases actually, both the fractional Brand motions and the classical Brand motion. So this is a typical example. And based on this kind of relationship, we can get that if m belongs to the interval d to d plus 2, it is To D plus 2, it is equivalent to the fractional binding motion. So we can get that in this case, F is holder continuous because the classical fractional binding motion is holder continuous with the parameter less than the Husserl parameter H here. And if H is even smaller, that is, it is smaller than or equal to zero, then the random field F turns to be a Field F turns to be a distribution, belongs to this Soblef space, okay, with a negative power here. So, in the following of our work, we'll mainly focus on this rough case, because for the former case here, it's relatively smooth as a random field. So, for this case, the inverse problem somehow can be dealt with through a way that similar to the deterministic settings. Deterministic settings. So that's why we mainly focus on the rougher case. H is smaller than or equal to zero. Okay? So for this kind of random field, if we choose it as the random source or random potentials, there are also several works related to this kind of research. For the inverse random source problem, if the random source is a Gaussian white noise, then there are several works related to its Works related to its uniqueness, stability, and also numerics. And in this case, we can use the IDO formula to get the recovery formula, actually, which helps to establish some numerical methods to numerically calculate the strengths. And for the generalized Gaussian case, there are also several works related to different kinds of wave equations. For example, for the acoustic wave, the elastic wave, and the mathematical. And the mass to mass Maxwell equations and so on. Okay? And for the inverse random potential problem, there are only several works related to the generalized Gaussian random fields. So for the case that h is larger than 0, then there are several works related to the Schrodinger equation. And in this work, I'm going to introduce our results related to the case that h is smaller than or equal to zero. Smaller than or equal to zero. Okay, so in the following two parts, I'm going to take the acoustic wave equation as an example to introduce the inverse random source problem and take the acoustic wave equation as an example to introduce the random potential problem. Okay, so firstly, for the random source problem, we consider this acoustic wave equation with attenuation, which is also known as the Which is also known as the Hamhaus equation. So that is Laplacian u plus k squared plus i sigma k times u, which is equal to this random source f. And sigma here denotes the attenuation parameter. It is assumed to be larger than or equal to zero. So if sigma is equal to zero, then the equation will degenerate to the classical Hamhol's equation. Okay? And in this case, to get an In this case, to get an explicit form of the fundamental solution for simplicity, we denote a complex with number kappa, such that kappa squared is equal to the number inside of the bracket. And based on this kind of definition, we can check that the real part of kappa has the same order as the real wave number k. And the imaginary part of kappa will converge to a constant, which is a A constant, which is which depends on the attenuation parameter sigma here. And this kind of property will help to get the recovery formula for the inverse source problem. And based on the complex wave number kappa, we can get the expression of the solution u, which is the convolution actually between the fundamental solution j kappa and the random source f here. And for the fundamental solution g, it has Solution G, it has different forms for different dimensions, actually. And for different dimensions, here we can get that the fundamental solution has different kinds of singularities here. And also, they have different decay rate with respect to the width number k. And this kind of property will show that for the inverse problems, we can get different results for different dimensions as well. Okay, that will be introduced in That will be introduced in a minute. Okay, so for this kind of inverse random source problem, let's recall that for the case that F is a Gaussian white noise, again, we can use the Ito formula such that when we calculate the variance of the solution u, we can get the deterministic integral here depends on the fundamental solution jk and the strength function mu. So that's the basic formulation. So that's the basic formula we can get and use this kind of formula to recover the strings function mu here. But for the generalized case, since the ido formula is not available anymore, so in this case, if we still consider the second moment of the solution, we will get a double integral here, which involves the covariance kernel of F. Okay? So in this case, the covariance kernel of F depends on the symbol sigma. And for sigma, The symbol sigma. And for sigma here, it involves not only the strength function Œº that we want, but also the residual term R here. So we need to use some kind of limit approach to reduce the residual term. So that's the idea to deal with this kind of problem. So we take the one-dimensional case as an example. So we define another kernel function with a truncation. kernel function with a truncation function theta here. And theta here is nothing but to make sure that the observation data, the observation point x belongs to the observation domain. Okay? And with this kind of kernel function, we can rewrite the second moment of u as this double integral. Okay? And for the exponential function here, if we can replace the quantity in the first bracket by the variable j and By the variable j, and we replace the second term in the second bracket by another variable h, then we can simplify the exponential function such that it's almost in a Fourier transform form. Okay? But the problem is that after we use this kind of coordinate transform, we need to estimate what's the behavior of the kernel function under this kind of coordinate transformation. Okay, so this is the main part to solve this kind of function. So, this is the main part to solve this kind of inverse source problem. And actually, for this case, that's why we use the pseudo-differential operator to define this kind of random field. Because in this case, we can use the micro-local analysis to deal with this part. And we can finally get this kind of expansion of this variation of the solution. So, it contains a mean part, which contains the integral involved in the integral. integral involved involved the strings function mu here. And this term actually comes from the principal symbol of the pseudo-differential operator. And we also get a high order term here which comes from the residual in the pseudo-differential operator. And apparently since the real part of kappa has the same order as k, and the imaginary part of kappa will converge to a constant, so we can multiply both sides by k to the power By k to the power of m plus 2, and then let k goes to infinity, then the residual term will be omitted. Okay, so that's the procedure to get the final result. So I'm not going into any details. I just show the final result here for all the dimensions, d equals 1, 2, or 3, we can get this kind of recovery formula. In the deterministic integral here, it involves the unknown strength function mean. The n-known strength function Œº and it contains a kernel function actually here, depends on the attenuation parameter sigma here. So, based on this result, we can find out that if sigma equals 0 and the dimension d equals 1, then the kernel involved in this deterministic integral turns to be 1. And in that case, we cannot get any uniqueness result as well. This is actually the example I gave at the beginning. Example: I give at the beginning. For the one-dimensional case, we cannot get the uniqueness. And for the other dimensional cases, for both sigma equals zero or sigma is larger than zero, we can get the uniqueness. And this kind of theoretical result can also be verified through numerical experiments. Okay, so for example, for the one-dimensional case, if we consider the case sigma equals zero, that is the left. That is the left top figure here. The blue line is the exact string solution, and the red line is the numerical one. We can find out that theoretically we cannot gather result. So in this figure here, the numerical result is far from the exact strength function. But if sigma is larger than zero, for example, for the second figure here in the top line, we can almost get the exact solution. Get the exact solution which is near to the exact one. And also for sigma equals 2, we can get the numerical solution, which approaches to the exact solution mu. Okay, so this is based on the recovery with only one with number. And numerically, we also try that if we improve the number of the with numbers, what can we get? For example, in What can we get? For example, in the middle line here, we consider two with numbers. And we can find out that in this case, we can get a more stable result. And if we use more with numbers, we can get more stable result. Okay, so based on this kind of result, it motivated us to use multi-frequency data to get the recovery. So that will be introduced in the inverse random potential problem. Okay? Problem. Okay? And also we check another kind of strength function. We will get the same result here. So for the case d equals 1 and sigma equals 0, that's the left top figure here, we cannot get the uniqueness. Okay? But for the two-dimensional case here, even sigma is chosen as 0, we can get the unique result. Okay, so uh numerically we can also get the recovery based on only a single uh with number measurement. With number measurement. And also, if we consider a much more complicated strength function, which is more oscillated, then we can also get the result. So the left top figure here is the exact solution. And the right top figure here is the numerical solution at a single wave number here. So even though the result is not stable, but we can almost get the shape of the strings function. Of the strength function. So, this is due to the uniqueness theoretically. And if we use more frequencies, so for example, we use three with numbers or even five with numbers, the result turns to be more and more stable. So, that's the numerical experiment for the inverse source problem. Okay, so for the inverse potential problem, we consider the electric wave equation case, and similarly, we consider its equivalent integral. We consider its equivalent integral equation case. And in this case, for the inverse problem, it's a nonlinear one. So the K problem is that how can we linearize this kind of nonlinear inverse problem? Okay, so firstly, we would like to mention that we denote this stochastic integral here as a random potential operator kappa. Okay, for this random operator, For this random operator kappa here, we prove that it is compact almost surely to ensure the well-posedness of this integral equation. And also, the operator shows some decay property with respect to the width number k if we choose proper sub-left space here. And these properties will help us to get the linearization of this kind of non-linear inverse problem. So that is to say, we choose We choose the incident wave as u0 and through the potential operator kappa, we can define u1. So that is, u1 is chosen as negative kappa performed on u0. And then u2 is defined as negative kappa performed on u1, and so on. And in this case, we can get the bone sequence. For the bone sequence here, based on the decay property I introduced just now. On the decay property I introduced just now, we can show that the series will converge to the exact solution. And in this case, we can simply divide the scattered wave into three parts. That is U1, U2, and the residual term B here. Because the scattered wave here is actually the total wave U minus U0. Okay, so we have only these three terms. And for these three terms here, we can check in detail that for the first Can check in detail that for the first term u1, it is actually the fundamental solution times the random potential row and then times the instant with u0 here. Since u0 here is also in the fundamental solution form, so we can get finally it is rho times g square. So for this kind of formula, it is really similar to the inverse random source case. And this is somehow a linear And this is somehow a linear inverse problem if we can use u1 to recover the strengths of rho, of the random potential. So this is u1. And still, we still need to consider the estimate of u2 and the residual term b here. So for the term u1 here, similar to the random source case, we can get actually the limit of the second moment will also converge to a deterministic integral. Converge to a deterministic integral here with the strength mu involved. So based on the deconvolution, we can also get the strengths mu. If we can get u1 actually, but this is only a partial result. And we still have two problems for this kind of partial result. The first one is that, as I mentioned at the beginning, to get this kind of measurement, we need to approximate the expectation here. And in general, we need to use a lot of realizations. Use a lot of realizations. So, a first way to improve the result is to use, for example, a single realization. We need to delete the expectation here. And another problem is that we can see from the numerical experiments, if we can use multi-frequency measurement, the problem will be more stable. So can we replace this kind of high frequency limit by some kind of multi-frequency measurement? Multi-frequency measurement, that's another problem. So it will be the best way if we can just replace the expectation by the average over the frequency band. So that would be best. And for this kind of problem, we show that actually, even though we cannot show that U1 at different frequencies are independent of each other, but we can show that they are almost independent. That is the asymptotic. That is the asymptotic independence. And based on this kind of independency, we can show that the law of large numbers will hold asymptotically. So that is to say, when the wave number k is large enough, then we can use the average with respect to the frequency to replace the expectation. And based on this kind of property, we can just replace the merriment, the second mode. Measurement, the second moment measurement by this kind of measurement averaged over the frequency band. And in this case, we only need to use a single realization in probability one to recover this kind of strength function mu. So this is the estimate for U1. And again, we cannot observe U1 actually. We still need to know what's the contribution for U2 and the residual term. And the residual term. So for U2 here, this is actually the main part for the inverse random potential problem. Because in this case, especially for the two-dimensional case, we need to consider the truncation of the fundamental solution. And since the potential row here is a rough field, we need to use its small modification and to estimate the covariance of the random potentials. Of the random potentials. Okay, so that's the main part. But actually, the contribution of the second term is shown to be zero in the almost surely sense. And for the residual term here, we can simply use the decay property of the potential operator K to show that the contribution of B is also zero in the almost surely sense. So finally, for the inverse random potential problem, we can get that inverse. We can get that in probability one. We only need to use a single realization of this kind of multi-frequency data to get this deterministic integral. So if we know the measurement over a bounded domain whose measure is larger than zero, then we can extend the measurement into the whole domain and based on the deconvolution to get the strings mu. So that's how we can get the theoretical uniqueness for this kind of Uniqueness for this kind of nonlinear inverse problem. And this problem is only holds for the two-dimensional case here, actually. This is based on the property of the fundamental solution. So what if we consider the three-dimensional case? And we can find out that for the three-dimensional case, the fundamental solution G here has different kinds of behavior compared with the two-dimensional case. So that is, for the two-dimensional case, For the two-dimensional case, the fundamental solution decays with respect to the width number k. However, for the three-dimensional case, it does not. So that means that for the three-dimensional case, if we still define the Bohr approximation, the Bohr sequence, we can show that the sequence will not converge to the exact solution. So for the three-dimensional case, we cannot use the framework for the two-dimensional case to guess the result. To get the result. But instead, for the three-dimensional case, we use the backscattering far-field data to get the uniqueness. And in that case, we cannot get a deterministic integral of the strings function mu, but instead, we can reconstruct the four remotes of the strings mu. And in that case, we can also get to the function mu itself. So this is another framework for the three-dimensional case, which implies that for For inverse problems, it depends heavily on the dimension, actually. Okay? So, in this talk, I mainly focus on the uniqueness for the random source problem and the random potential problem. But actually, the most important and difficult problem for the stochastic case is the random media problem. So, that is the function n here. function n here involved in the high frequency part is non, or the function m involved in the differential operator is n-known and random. In that case, what's the uniqueness for the inverse problem? It's still open. So I'm not sure how to deal with this kind of problem. So for the random media problem, it's also a non-linear one, but we cannot deal with it through the framework that we deal with the random potential problem. Random potential problem. This is also because that the media parameter n here is involved in the high frequency term. So the fundamental solution cannot give us enough decay property to get the linearization of this kind of inverse problem. Okay, so this is still open. And also for the stability estimate for stochastic inverse problems, it is also open. So now we are focusing on how to get We are focusing on how to get the stability estimates for this kind of inverse problems. And another interesting problem is the numerics for stochastic inverse problems, especially for the case that the random sources or the scatterer are complex. So, for example, if they have some interactions, we cannot I'm not sure if there are any ways to deal with this kind of scattering. This kind of scatters theoretically, but at least we can use the neural network to simulate the position of the random source, actually. But for some other stochastic inverse problems, I'm not sure if we can also use neural networks to get a better recovery numerically. So, this is also another interesting problem in my point of view. Okay, so I think that's all of my. I think that's all of my talk. I'll stop there. Thank you.