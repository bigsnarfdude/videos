So this is based on two papers, joint work with Chi Chiou Kot, Alex and Robert. So Chi Cho is now in Shanghai University of Finance and Economics and Alex and Robert they are students in Watergate. Okay, so I would start from the classical triggers inequality and mention some applications and generalizations. And then I will talk about the first paper which extends this. The first paper which extends this to undirected graph vertex expansion. And then I will tell you what is the concept of related eigenvalues, which start from this problem of faster spacing time. And then I would tell you how to extend it further to direct graphs as well. And also hypergraphs. I won't be able to talk about hypergraphs. So Chiga's inequality-related edge contact. Related edge conductance to the eigenvalue of a matrix. So, edge conductance, the definition is given the graph, given any subset S, the edge conductance of a subset S is defined as the number of edges crossing S divided by the volume of S, which is defined as the total degree of the verses inside S. So, in this picture, there are four edges going out, so the numerator is four, and then there are six verses inside, each has degree four, so total degree is. Each has degree 4, so total degree is 24, and then the contents of this set is 1 over 6. And then the contents of a graph is defined as the minimum over all subset S, which has at most half the volume of the graph. We always look at the smaller sign of the partition. So this is the number between 0 and 1, which measure the fraction of edges leaving the set. And it is a useful part. It is a useful problem. Finding a subset of small edge conductance is useful in device and concrete algorithm and it has applications in image segmentations and so on. Also, on the other hand, certifying a graph has large conductance is also useful because you can use it to prove that a graph is an expanded graph and then it has many applications in theoretical computer science. Okay, so cheaters inequality, I think the best. Inequality, I think the best formulation is to look at this normalized La Partian matrix, which is defined as the identity matrix minus the normalized adjacency matrix. So what exactly is the normalization is not so important for this talk, where the graph is deregular, it's just the adjacency matrix divided by D. Okay, so the normalization is done so that the eigenvalue is between 0 and 2. It's a real symmetric matrix, it has all real eigenvalues. matrix it has all real eigenvalues and then can show that the eigenvalue is now between 0 and 2 and then we look at the second eigenvalue the first eigenvalue would always be 0 and then we look at the second eigenvalue Chicago's inequality shows that the edge conductance of a graph is big if and only if lambda 2 is big okay so conductance is lower bound by lambda 2 and then lambda 2 is lower bounded by conductance squared so Conductance squared. So there is a loss between the upper bound and lower bound, but qualitatively, one is big, if and only if the other is big. So it is an interesting result because it relates a computerial property to an algebraic quality. And also from a computational point of view, it is interesting because conductance is empty hard, but eigenvalue there is a neon linear time output. Linear time alpha. Okay, so there's a connection between these two, and then I think the main use of Chigas inequality is to, so for example, graph has constant edge conductance if and only if lambda 2 is a constant. So you can use lambda 2 to certify that a graph is an expanded graph. So this is one main application. The other application is when lambda 2 is small, the proof of Chigger's inequality would Would give you a subset of conductance at most square root under Ju. And this algorithm is useful as a heuristic to find a sparse calculator. Okay? And another application of, yeah, so you normalize it in all the eigenvalues are between 0 and 2. So you're saying that the conductance will always be less than 4, I guess? Uh four I guess is is that what you're saying? So another application of uh Chica's inequality is to analyzing mixing time of random walks. So mixing time, so let's say we consider lazy random walks on connected undirected graphs, and then we know that the stationary distribution would be proportional to the degrees, and then the mixing time is defined as the first. Time is defined as the first time such that the probability distribution is close to the stationary distribution by the T V distance. And then we know that lambda 2, 1 over lambda 2 is closely related to the maximum time. So mixing time is at least 1 over lambda 2. It's at most 1 over lambda 2 times log 1 over priming. Pi mean is the smallest entry in the stationary distribution. So then through Chigas inequality, then they are all connected. Inequality, then they are all connected. We can use eigenvalue to study expansion on the graph and then to analyze the mixing time. And then now we know that we can also use edge conductors to analyze the mixing time of the graph. So these are two main applications of Chicago's inequality. And then more recently, there are some generalizations of Chiggers' inequality. So not just look at lambda 2, but look at lambda k of the graph. And it turns out that one can prove. One can prove it's called high-order chicken equality that lambda k is small if and only if the k vertex disjoint sparse cut in the graph, roughly speaking. Okay, and then there is also this improved Chigas inequality that would tell you sometimes we can avoid the square root loss of Chigas inequality. So for example, if lambda k is a constant for a small k, then lambda 2 is a small k. Then number two is a constant factor approximation of conductance. So it helps explain why spectral partitioning works well in some practical instances. For example, the image segmentation, then we expect to see that lambda k is big for a small k. Okay, so there are some generalization, not only looking at the second eigenvalue, but also look at the kth eigenvalue. So that's a quick, quick summary of some Summary of some existing results about classical triggers inequality. So now I want to tell you about some results. How exactly was phi k defined? The phi sub k. How is it exactly defined? K disjoint subset, the maximum of the start from this fastest mixing time problem, which is defined by Boyd, Connie, and Sau. The problem is the The problem is the following. We are given an undirected graph G, and then we are given a target distribution pi. I think for this talk, just think of pi as the uniform distribution. So we want to sample a vertex from this graph uniform distribution. But now, what we can do is we can play with this transition probabilities. So we want to find the best transition probabilities on the edges such that Such that the mixing time is minimized. So I want to find a transition matrix to minimize the mixing time so that I have a fast sampling output. Okay, so maybe it's best explained by this picture. So in this graph, I have two complete graphs and they are connected by a perfect matching. So this perfect matching is actually a sparse cut. And if we do a random walk where every edge has the same weight, Every edge has the same weight, then the maximum time would be at least n, n is the number of versus graph. But if we weight the perfect matching by a factor of n, then it becomes an expanded graph, and then the mixing time would just be log n. Okay, so this is the fastest mixing time problem. Find those re-waiting to minimize the mixing time. And it's not It's not clear how to directly formulate the mixing time. So, what they did is to use inverse of spectral gap 1 over lambda 2 as a proxy as an objective to minimize the mixing time. So, in their work, they come up with this SDB formulation, try to find a weighting of the edges to maximize the, like that to maximize the spectral gap subjected to the constraint that the stationary. Subjected to the constraint that the stationary distribution is pi. So, this is their formulation. So, non-edges, they should have probability zero. Should be a random walk matrix, so every row sums to one. And then it should satisfy this time reversible constraint. So, it has two purposes. One is to make sure that the stationary distribution is time, the other is to make sure that the random walk is on an undirect graph. It corresponds to a random walk on a graph. Okay, it corresponds to a random walk on an undirected graph. And then now the objective instead of minimizing mixing time is to maximize the spectral gap. So this is what we call the V-weighted eigenvalue. So finally, we weight and to maximize the long-term. Okay, so this is the most important concept in this talk. And then they show that this problem can be formulated as Formulate as an SDP. So, lambda 2 star can be computed in polynomial time. Okay? But they didn't give any characterization of when lambda 2 star is small, when it is big. And then after that work, then Rock proved that having a subset of small vertex expansion is an obstruction of fast mixing time. So if you have a subset with small vertex expansion, If you have a subset with small vertex expansion, then the fastest mixing time cannot be small. You cannot re-weight it. Okay? And so let me just quickly define what is vertex expansion. This is what you would expect. If I am given a subset S, the vertex expansion of this subset is look at the total pie weight of the boundary of S divided by the total pie weight of the subset S. Subset S. Okay, if this, if the pi weight is the uniform distribution, so then there are three verses in the boundary, and then there are six verses inside S, so vertex expansion of this subset is one half of that. And then vertex expansion, you take the minimum vertex expansion of a subset of pi rate at most one half. Okay, and it is also a useful problem to find a subset. To find a subset of small vertexes to solve any equations or use it to analyze error correction codes and so on. So the problem of characterizing the fastest mixing time was open for a long time until recently that Oliska Taylor and Sanetti showed that vertex expansion in some sense is the only obstruction of transcess mixing time. So they prove the So they prove this Chicago inequality for vertex expansion. So lambda 2 star is small only if the graph has a subset of small vertex expansion. Okay, so if it is upper bound and the conductance is also upper bound. Vertex. Vertex expansion is upper bound. My square would lambda, square wood login. Okay, so it is a very Okay, so it is a very nice result. And it also implies that the fastest mixing time of an undirected graph is between one over vertex expansion, is at most one over vertex expansion squared. Probably I was confused about the normalization. So sine can be like larger than one here. Oh, actually, what is the definition? Yeah, but if you take every subset and it would be at most one. Yeah, it would be at most one. So they left two open questions. So they only prove it when pi is the uniform distribution. And then they ask whether it is true also for any stationary distribution, pi. And then they also ask this log n, can it be improved to log d? Log D is the maximum degree of the direct graph. And then they said if it can be improved to log D, it would If it can be improved logically, it would be optimal assuming the small set expansion hypothesis. Okay, so our first result in this work is to answer both of their questions positively. So we can prove this result, which holds for any probability distributions. We also improve log n to log E. And independently, another group proved the same improvement from log n to log E in the case of Ruby. In the case of uniform distribution. Okay? And then we also show examples that the inequality is tight. So now, without assuming the small set expansion hypothesis, we construct an integrality example. So this inequality is tight. We cannot hope to remove the log D and get an exact analog of Chicago's inequality for vertex expansion. Okay? So. So yeah, and then once we have this result, we are more interested in the concept of re-weighted eigenvalues. I think previous work, they focused on fastest mixing time. And then we ask, oh, if this is true that you can replace eigenvalue by re-weighted eigenvalues and then get a GCSI inequality when edge content becomes vertex expansion, then can we also define the k re-weighted eigenvalue of a graph? We weighted eigenvalue of a graph and then generalize the other results of our chickens quality for vertex expression. So it turns out that it is the case, that we can also define the weighted k eigenvalue. So this is the higher order Chicago's inequality. And then we prove an analog of higher order Chicago's inequality for vertex expansion. Roughly speaking, number k star is small, then I want to find that we Small, that I want to find a way weighting to maximize the lambda k of the resulting graph. Lambda k star is small if and only if the graph has k vertex disjoint subset, each has small vertex expansion. Dependency on K is a big part of K should be improved. Or shouldn't it be more? I mean, even this is not well understood. I think it was conjectured that it can be improved to polylog K, but we don't know yet. We don't bother to think too much about it. And then also improved cheekers' inequality. There is an analog for vertex expansion as well. So we can replace edge conductance by vertex expansion and then lambda 2 by lambda 2 star. Lambda 2 by lambda 2 star. So recently we also know that this k to 1.5 can be improved to k. So it's almost an exact analog of the improved GP inequality. There's an extra log e term, but perhaps it is unavoidable in this related eigenvalues. Okay, so yeah, so it turns out that this connection between eigenvalues, edge content, and matching time, there is an analog in the vertex expansion world where Expansion world where eigenvalues replaced by regulated eigenvalues, edge conductance replaced by vertex expansion, and then mixing time replaced by fastest mixing time. It would be nice if you have your favorite result in spectral graph theory and then try to replace eigenvalues by related eigenvalues and see whether you would also get a new result. Question So if you so algorithmically in terms of So, algorithmically, in terms of say approximating vertex expansion, does this new thing give better algorithms? No, I think the best is square word log n. Here, we can do a bit better in some small range, I guess. Not in general. So, now I want to tell you about how to extend it to direct graphs. So, I think people wanted to develop a special. Wanted to develop a special theory for direct graphs, but the main problem is it's not even clear how to define eigenvalues for direct graphs because the natural definition would the matrix would not be symmetric and then the eigenvalues are not real. But now we use this idea to define related eigenvalues of the parallel graph. So to explain the formulation, so let me tell you our proof of this result by Rock. Result by Rock that the vertex expansion is at least lambda 2 psi. So our proof would be slightly different. We think of it as certifying a graph is a good vertex expander by finding a re-weighting with large edge conductors. Okay? So the original proof is just showing that I have a set of small vertex expansion and then you. Small vertex expansion, and then it would give weakness of lambda 2 star is small. So now, suppose I want to convince you that the graph G is a vertex expander, what I could do, so one way to do it is I found a weighting of the edges of the graph G, satisfying the following properties. First is to make sure that every vertex is of degree 1, weighted degree 1. We can always make sure that by We can always make sure that by allowing self-loops in the graph. And then the second condition is to make sure that the edge conductance of H is large. So now I want to say if I can find such a re-weighting, then I prove that the original graph G is a good vertex expander. Why? Because if I look at every subset H in H S is defined as the total weight leaving the sub Defining the sub uh subset S divided by the total volume. So by the first constraint, each vertex is of degree one, the volume would be exactly S. Okay? Also by the first constraint, now if I know that the subset has k edges weighted, but I know that every vertex outside would have degree at most one. So if I have k edges leaving, there must also be k. Thing, there must also be k versus on the boundary of s to take those edges. So then I know that the vertex expansion of the original graph must also be large. Okay? So this gives a lower bound. And then now if I want to give the best lower bound using this approach, then I would find the best weighting to maximize the edge conductance of the graph, right? And then I don't exactly know how to. And then I don't exactly know how to maximize the edge conductance, but just using Chicago's inequality, I know a lower bound of uh edge conductance is lambda 2. So I and then I try to find the best view rating to maximize number 2 in order to prove a lower bound of vertex expansion. Okay, so now with this idea, I want to use it to lower bound the vertex expansion of a direct graph. So the vertex expansion of a direct graph is defined as follows. is defined as follow. I have a subset S. There are two types of boundary now. So there are some outer boundary that the verses with edges from S going to it. And then there are some inner boundary that they're inside S, but then they're just coming up out of S entering into it. And then the vertex expansion of a subset S is defined as the minimum of the two boundaries in the numerator. Boundary in the numerator, and the denominator is take the smaller side of either pi s or pi s bar. Okay? It looks slightly different from vertex expansion in the undirected graph, but we can show that they are within constant factor of each other. This formulation would be more convenient. And then vertex expansion of a graph is just to take the minimum subset S with the with smallest vertex expansion. With smallest vertex expansion. And then we can also define edge conductance in a similar way. They are outgoing edges, they are incoming edges. The numerator take the minimum of the two, and the denominator take the minimum of the volume of S, or the volume of S bar. Volume is defined as the sum of the total degree of each vertex, in degree plus out degree of each vertex, and then take the sum of all vertexes in S. Of all verses in S. Okay, so now let me tell you how to formulate related eigenvalues in a director graph. We use the same idea. I have a director graph G. I want to show that it has large vertex expansion. So what we could do is, if we could find a weighting H, now the new ingredient is if H is Eulerian, it's an Eulerian director graph, and then similar as before. And then, similar as before, each vertex has weighted degree 1, in degree equal to out degree equal to 1. And then the edge conductance of H is that, the edge conductance of the resulting director graph is that, then we can prove that the vertex expansion of the directograph is that. Why? It's by the same argument that the vertex expansion of G is reduced to the edge conductance of the direct graph H using the same argument in the previous slide. Slide. And then, but now, because H is Eulerian, so the edge contents of this directed graph is just basically the same as the underlying undirected graph, right? So then this is just equal to the underlying undirected graph. And now it is reduced to undirected graph. Then we can use Chicago's inequality, and then we know that how to lower quantity using lambda 2. So this is how we define. So, this is how we define the re-weighted eigenvalues. We want to find the best re-weighting to maximize this lambda 2. So, the formulation is not edge, we should have zero. And then it should be an Eulerian re-weighting, in degree is equal to out-degree. And then for general distribution of pi, the degree should be equal to pi of v. And then the objective is to maximize the lambda 2. What is inside lambda 2 is the normal. This inside lambda 2 is the normalized Laplacian matrix of the underlying anti-related graph. Okay? So it is clear that lambda 2 star is a lower bound from what I explained. And then it's less clear whether it is a good lower bound or would it be loose. And it turns out that it is not a bad lower bound. If it's not a bad lower bound, there is an analog of Chicago's inequality that the direct vertex expansion is at least lambda two star is at most square root of lambda two star times some log term log of per maximum total degree divided. We don't know whether this term is tight. So if this term we can remove, then we got we generalize the result for undirected burden suspension. Okay? Okay, so this is one result. And then an application of this result is now we can study the fastest mixing time of a direct graph. You can also ask the problem of I'm given a direct graph. I have a stationary distribution pi. And then what is the fastest mixing time to pi? So then you can write this non-edge should be 0, both sum should be equal to 1. And then this condition is just exactly the stationary distribution. Is just exactly a stationary distribution. It should be equal to high. And then we want to minimize the mixing time. So it wasn't clear how what should be the objective now because it is a direct graph. But it turns out that using some existing result by field and chong, that the mixing time could be upper bounded by one over the reweighted eigenvalue. The rewait an item value. I mean, of course, they don't phrase it like that, but we can use their results directly to conclude this. And then, so this is also a good objective function for the fastest mixing time. And then with this, applying the Chicago inequality, which shows that the fastest mixing time of a directed graph is, roughly speaking, between one over the directed vertex expansion. The directed vertex expansion is at normal one over the directed vertex expansion squared and some log terms, more log terms, but here. So this is one application. And then so we can also define related eigenvalues for directed edge contactants using similar idea. Using similar idea. And what I want to tell you is for directed edge conductance, we can actually prove a tighter bound. So there is no dependency on the maximum degree here. It looks pretty close to the original Chiggers inequality for directed edge conductance. So one consequence is that we weighted eigenvalues would provide a certificate for constant A certificate for constant edge conductance of a diagnostic graph. Just like the original Chicago inequality, lambda 2 is constant if and only if conductance is constant. Now we can also say that lambda 2 star is constant if and only if directed edge conductance is constant. So no, we don't know whether it is tight. If we it can be removed then you would be really fantastic and you would be an exact analog of Chicago. Be an exact analog of Chicago import. We think that that would not be the case, but we don't have a lower bound example. And then it also gives a new type of spectral algorithm for direct graph partitioning. I think people want to find some good algorithm for direct graph partitioning, and then we think this related And we think this related eigenvalue approach may give some new idea and I hope that it would be useful in practice in graph partitioning as well. Okay? So the main difference is the re-weighting cannot exceed the weight in the graph. And then also the denominator is a little funny. Let's not get into those details. Yeah, so to summarize, so in directed graph we also have this triangle. So we weighted eigenvalues related to vertex expansion to faster spacing time. There's also an analog of improved Giga's inequality. Can also be extended to direct graph. Yeah, but maybe surprisingly, the Surprisingly, the most natural analog of higher order Chicago inequality is not true. So, lambda k star is small if and only if the k disjoint subset each has small edge conductance. This is not true. So, maybe there's another way to formulate it to prove an analog of high-or-cheeking equality. Okay, so let me just uh summarize. Let me just summarize and mention some open problems. So there was also a special theory for hypergraph by Lewis, and the definition was more complicated. It's not so easy to make rigorous. So what we can show is that hypergraph, the same approach would also work. Just replace a hyper edge by a click. And then if I want to show that the hypergraph is a good expander. The hypergraph is a good expander, I just need to find a re-weighting such that the graph has good batch expansion. And then you would also work, and then you would provide a simple way to define eigenvalues that would recover and improve all the existing results and also obtain some new results, such as improved Giga's inequality for hypergraphs. Yeah, so we just want to say this we weighted eigenvalue turns. this we weighted eigenvalue turns out to be a simple and a general way to study expansion properties of more general objects. Vertex expansion, divertographs, and hypergraphs, they can all be reduced to the original setting of edge conductance of undirected graphs through this re-rating. And then how good is this approximation depending on how close the original object is to edge conductance? So vertex expansion is So vertex expansion is close to edge conductance if the maximum degree is small. Director graph there is a parameter, hidden parameter I didn't talk about, which is the asymmetric ratio of the director graph. Hypergraph is closer to a diverted graph if the maximum hyper edge is of small size. And then we can again recover and improve all the existing results about Improve all the existing results about special theory for these more general settings and then obtain new results. In particular, when the graph is directed, not much was known before. It's an easy way to lift the spectral theory. What we know about undirected edge content has to put this more general setting. So the proofs of this result are quite similar to the original setting once you use this definition. Use this definition. Okay, so some open questions. One open question is: I mean, if we want to make the weighted eigenvalue useful, we should make the computation fast. So we have some follow-up work in this direction. And then you may also hope to extend more results from undirected graphs to direct graphs. So, one nice question that is also asked by Oliska, Taylor, and Sanati is for And sanitary is for undirected graph. We have this local graph partitioning algorithm that doesn't need to read the whole graph and find a set of small edge conductants. So we want to find such algorithms for undirected vertex expansion for directed graphs. Could it be done? And then a very interesting but maybe very difficult question is the following. This is a conjecture. Conjecture that if I have a vertex expander of constant vertex expansion, through our results, we can prove that the fastest mixing time is at most log n squared, but the conjecture is that we can indeed make it to be log n. If this conjecture is true, then you would have a consequence in approximating Sparse's cut. There would be some exponential time constant factor approximation algorithm for Sparset's cut. But this could be a hard problem. And then more generally, we hope to develop special theory for directed graphs and hope that this idea of we rated eigenvalues will be useful. We can prove more results than an analog for undirected graphs in directed graphs. Thank you. Questions? So if your definition for the expansion in a direct graph is the minimum of A and L, would it make sense at all of you as to have a one-sided notion? It wouldn't work. We need both unstable. But when you say it wouldn't work, would you? Inequality. Yeah, yeah. I don't know anything. If you have a small set, you just have few incoming edges. We want to say this set also has small content. When you look at the outgoing edge from the other side, the high weight is very big. It's a different definition. So I assume your definition of hyperdraft expansion is just that the, you know, it goes across the cut, the hyper edge. Yeah. But can you define some more exotic, you know, is there hope for theory for more exotic motions like there should be odd number or both? Yeah, I know people study this submodular hypergraph that depending on how you cut Depending on how you cut the hyper edge, then something even simple, like odd on both sides. It could have used for like some XORs, solving exonic systems. And I am not sure that we would be able to solve those problems. So, how do you prove these things? You like take the same proof for H on the lambda K and Lambda 2, but then reweighting is really. But then rebating is via this SDP. Which which? Very? Like in general the vertex. Yeah. Like how just like, could you say something about how we look at the STP and project into one dimension from this? Right, let's take the speaker again. A clap for the organizers. Thanks, everyone. I don't know what else to say. Well, lots of people have asked us if we're doing this again, and we're all like, yes, but it is a very free. So, Three years. Thank you very much. So if you so you have the re the best free rate of edge expansion of the lower bound. So the it's another bound, but like the cap is that like log D or where of log D. So that's worked. So that's okay. So if you have the So if the graph is a constant vertex expander, we can always find a weighting such that the edge quadrupense is 1 over log B. General. Okay, so the gap there is just log d, but it's also this square. So when... Yeah, well that's, I think, sort of my question. So I mean, the square is sort of from like going from edge expansion to the spe yeah, right. Right? Chigo loses the square. You say you do lose the square going from affected edge expansion to the vertical expansion as well. Or do we not know? Make sense? Yeah, so then we don't go through this tool, we can directly do something like a cut matching game only in those pieces of block. No. Without without the square. That would be like the type checks a lot the yeah but we can't help to always be weighted constant vertex attention to constant edge on buttons. That would be not true you just mean you must lose the logic. Yeah yeah yeah okay. It isn't There is this UGC. I've heard this question, this sub-exponential time. Yeah, I've lost it. Yeah, what was that again? So this spoiler was contracted. Can you re-wait it to make