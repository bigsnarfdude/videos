Thanks a lot for the introduction. It's my pleasure to present our work at this event. So I will report on a joint work with Jonathan Leake and Timo DeWolf, which is still in progress. And so it's about capacity that I think already appeared in at least one of the talks. And so I will start giving And so I will start giving some motivation that comes from combinatorics. And I will introduce what capacity is and how it actually helps to solve problems in combinatorics. Then I will talk about entropy and relative entropy, and then explain how non-negative polynomials enter the game. And then finally, I will talk about so-called a discriminants. Okay, so I will first define First, define what is a permanent of a n times n real matrix, or not necessarily real. So, let me take W matrix and it's permanent as defined by this formula. So people say it's like determinant, but only simple, because it doesn't have minus one to the power of the permutation signal. So we take all sums over all permutations, and then we take this product. This products, okay, and we can look at the permanent from the following combinatorial perspective. So, if you pick a complete bipartite graph on n, n vertices, okay, something like this as depicted here, then I consider graphs with weights, so it's weighted graph, and h i j carries weight double. Carries weight Wij. Then the permanent can be seen as the sum over all matchings in the graph G, which I denote by M, and then sum over all products of weights of the edges in the matching. So matching, I will not give like completely rigorous definition of a matching, but Definition of a matching, but at least it is something that you can see on this lower picture. So, for example, this is a graph which is not complete. Okay, so I will then switch to graphs that are bipartite, but not necessarily complete. And this corresponds to choosing this weight matrix to be zero, one matrix. So I put zero whenever there is no edge between i and j, and I put one otherwise. One otherwise, okay. And then this matrix defines me incomplete bipartite graph, something like here. And then by red and blue sets of edges, I denote the two possible perfect matchings in this graph. So I can pair matching from one side of the set of vertices of the graph with other side and Other side, and so that no two edges in the same color share a vertex. Okay, and then they have to somewhat stop all vertices altogether. And you can easily see that there are only two such perfect matchings, the blue and the red one. And you can actually then compute it via just executing permanent. Okay, so permanent computes perfect matchings in. Computes perfect matchings in the bipartite graph where this matrix W is taken to be 01 matrix. And this matrix here represents this like 3x3 matrix already shows you which entries correspond to which edges. Okay. Oh, I'm sorry. Yes. Oh, I'm sorry. Yes. So if row and column sums of 0, 1 matrix W are all equal to K, where K is some number, then this graph GW is K-regular bipartite graph. K-regular means that every vertex has degree K, is connected with K other vertices. If I divide this matrix W by K, I obtain what is called W. K, I obtain what is called doubly stochastic matrix. So it means that its row and column sum is equal to one. Okay. And then there is so-called van der Wanderbondbond. Before it was a conjecture and it attracted a lot of attention. And in 1981, Igorachev and independently Falikman provided the proof of this. So the conjecture, and now a theorem, asserts. And now a theorem asserts that if you pick a doubly stochastic matrix W, then its permanent is bounded from below by size factorial over size to the size. And so you can already guess how it can help you to estimate the number of matchings in a certain graph, right? Because we can do this reduction to like. To like k i regular bipartite graph, and then it should it helps you to understand how many perfect matchings are there. In general, computing the number of perfect matchings is very hard, okay, and uh it's uh hence people hope to get some good balance, okay? So it's not just a Just out of nowhere. So it's motivated question by hardness of this combinatorial count. Okay, and there is a brilliant idea, I think, which is attributed to Burwitz, who introduced a tool that is known as capacity today. I think capacity also appears in different contexts, and sometimes this. Sometimes these different capacities are related, and sometimes maybe not. And it is defined as follows. So let me pick a polynomial QW. This polynomial is just product of linear forms, and each linear form is just inner product with a row of the matrix W that we saw on the previous slide. Then this polynomial actually is related to the permanent in the following way. It to the permanent in the following way. I can take derivatives of this polynomial qw with respect to z1, etc. Zn. And then I get a constant because this polynomial is homogeneous of degree n. And this constant turns out to be equal to the permanent of the original matrix W, which is also the coefficient of the monomial z1, etc., zn. Etc. the n. Okay, as I said, so computing a perfect number of perfect matchings and in general computing permanent of a matrix is very hard. So as I said, Gurbitz came up with an idea to approximate this quantity via the capacity. And the capacity is defined as well. So I pick a polynomial whose all coefficients are non-negative. All coefficients are non-negative, okay. I denoted by Q, and I pick a point in the non-negative workset. It does not need to be an integer, just any point. And then the capacity of Q with respect to alpha is the infinum of the fraction Q of Z over Z to the alpha. Okay, so it's some function, you minimize it over the positive orthon, and you obtain a number. So you can see. And you obtain a number, so you can see that it will be a finite number, and this provides, as we will see, some good bounds on the combinatorial quantity from the previous slide. So easily, because of positivity or non-negativity of the coefficients, and because z comes from the positive orcent, you can lower bound capacity from below by just the alpha. By just the alpha coefficient of q, right? The coefficient of the monomial z to the alpha. Okay? And this actually is a good new because then we don't know how to compute coefficient of a polynomial exactly because it's very hard, but capacity can be computed quite efficiently via a convex optimization program. I will come back to this later. And Gurwitz proved the following theorem. Proved the following theorem. So, there is a notion of stable polynomials that appears that was introduced, I think, by Petter, by Cinci, and by Torsten, at least. Maybe Masa also talked about them. And so we pick a stable polynomial. So if you don't know what stable polynomial is, don't worry. It's not important for the rest. Just some polynomial is some nice property. It needs to be homogeneous of degree, the same degree as a number of. The same degree as a number of variables. Then, if you pick the derivative of this polynomial with respect to one of the variables, for example, the last variable zn, and then evaluate the result at zn equals zero and compute its capacity with respect to the vector of all ones, then it cannot be smaller than the capacity of the original polynomial with respect to the vector of all ones times this. times this factor n minus one over n to the power n minus one okay so what is good uh about this uh theorem because we can immediately apply to uh our product of linear forms qw because it is a one of the most simple stable polynomials in the world. So if we apply it n times actually, so once we get derivative, it's not anymore product of linear forms, but it's still a stable polynomial. It's still a stable polynomial because taking derivatives of this kind preserve stability. And if we take, if I apply this statement n times to this qw, then as we saw before, we recover permanent on one hand. And on the other hand, we have this telescopic sequence of lower bands. And when we collect all the And when we collect all the coefficients together, we'll see that the coefficient will be n factorial over n to the n times the capacity of qw with respect to the vector of all ones. And another result of Gurwitz is that for a doubly stochastic matrix W, this capacity on the right is just one. And this delivers another very ingenious proof. Ingenious proof of Van der Warden bought. And later after this, I think, application, people used capacity and still they're using capacity and its generalization to provide lower bounds on various combinatorial quantities. And I will not talk about other applications, but you could, I think, read papers posted by people in this community. Okay. Okay, so capacity has another manifestation which is somewhat almost classical or false learn at least. So it's about actually probability distribution. So let me pick Q, discrete measure, okay, that is supported on a bunch of n points in R to the D. And let's assume that the marginals of this probability of this measure are Measure are denoted by q1 up to qn. And I pick another measure, which is now probability measure on the same set A. And then the Kullback-Leibler divergence, or also known as relative entropy of P with respect to Q, is defined by the following formula. So it's sum over I from one to N, P I times log of the ratio of these two quantities. Ratio of these two quantities, two marginals, Pi and Qi. And so it's known that this quantity for any two measures is non-negative. This is known as a Gibbs inequality. And if Q is either uniform or counting measure, so in case it's uniform, we talk about probability measure, it's counting, we just talk about discrete measure, then this Kullback-Leibler divergence satisfies, is related to Is related to another concept called entropy just by a very simple way. So it's just minus entropy of the distribution p plus possibly a constant. Okay. So this is something that some of you might have already seen before. And if you were information theorist, you would probably like that this. Uh, like that this talk be called Nani Phases of Entropy or something because, from information theory, people this DKL, this Kool-Bach-Liber divergence measures how much a given model distribution Q is away from the data distribution P. Okay, so if you like data science-motivated questions, you might like this slide, otherwise, just forget about it. So, note that when some of the marginals of P tend to zero, this Kullback-Label divergence is well defined because this product Pi times log Pi over Pi tends to zero in this case. And what is good about this function is that it's so called jointly convex in both arguments. So, again somewhat, I will not define it rigorously, but at least if you fix Q. Rigorously, but at least if you fix q and you vary p, you will see that it's a if you take convex combination of p, then this is convex function with respect to the first argument p. Okay. Okay, good. And this KL divergence can be actually quite efficiently or like usually like you have much more, let's say, the support. Let's say the support, like the support of this measure has usually quite large size, okay? But the space where this support lives is usually relative, has relatively low dimension. And that can be exploited and approached to approach this problem via dual problem. So, you suppose you want to somewhat minimize the KL divergence, or minimize this somewhat. So it minimizes somewhat loss of the from an information theoretic point of view, loss, fixing also additional constraints. You want your distribution P to have a fixed expectation alpha zero. So you then consider the following optimization problem. So you minimize KL divergence over all probability distributions supported in the support of Q. In the support of Q with a fixed expectation alpha zero. Okay, and as I said, this can be approached via the dual problem. Okay, consider the dual problem, the convex dual. So you can see that this problem is formulated as follows. You need to maximize now the domain of this optimization problem. It leaves in R to the D. And the target function is. The target function is written here. So it's scalar product of the variable with alpha zero minus log of this sum. Okay. And the good is that now the domain here is d-dimensional and the yellow problem P has the met has domain that has roughly the same dimension as the cardinality of the support. So in general, the D is much less than N, which is a Than n, which is the cardinality of the circle. And the good fact is that strong duality holds under a very mild assumption. So namely, you can solve the old problem, and this will deliver you optimal value of the primal problem under the assumption that alpha zero lies in the relative interior of the convex hull of the alpha one up to alpha n. Okay, then we can see that there is. See that there is an optimal solution x star in Rd of the dual problem so that the following probability distribution is an optimal solution of the primal problem and the optimal values of the primal and dual agree. Okay, and so by this tilde, I do know that these two distributions of the star is proportional to the distribution written here. So where delta here so where delta alpha of i is direct is direct measure okay so it's supported on the same set as q so you see that all marginals of p star are positive so these are these all marginals are just exponential scalings of the marginals of q and this is something that you need to maybe keep in mind for the moment okay and now as i promised And now, as I promised, this K-divergence or relative entropy is related to capacity. So you can now assume that your points, alpha1, alpha n, are integer. And let's pick a polynomial with coefficients q1 up to qn and that has support in this alpha1 alpha n. Okay, and assume that its coefficients are positive. And then the capacity is again defined here. This is just a repetition. Here, so this is just a repetition of the previous definition, and now we can parametrize the positive force and by this element-wise exponential function. So, these e to the x, where this is e to the x is e to the x1, etc., e to the xd. Okay, and then I can take minus log of the capacity of q with respect to this alpha zero. Okay, and it turns out to be equal, so you just So you just do this simple computation, plug it in z equal to e to the x into the yellow expression. And then you can see that this is just minus infimum of log of q of ex minus dot product of x with alpha of zero. And then if you just rewrite it a little bit, you obtain exactly the dual problem to the Dual problem to the minimization of KL divergence. Okay, so in other words, the consequence is that capacity of Q with respect to alpha zero is e to the minus KL divergence of B star relative to Q, where Q is a measure whose marginals are given by the coefficients of the polynomial small Q. Okay, and this happens. Okay, and this happens actually when alpha of zero lies in the relative interior of the Newton polytope of the polynomial Q. And moreover, so if you can now take the minimizer of the capacity optimization problem, which is given as e to the x star, so it's a positive point in the positive force, and then the optimal solution or the optimal distribution. Distribution for the KL divergence is proportional to this somewhat measure corresponding to the polynomial, okay, evaluated at this point, specific point Z. This is, in this case, the maximum entropy measure. Okay. So, and so this, what I told you, actually delivers a way how somewhat to find somewhat preferred. I would prefer a convex combination of points inside the Newton polytope of a polynomial with respect to the vertices of this polynomial, or not necessarily vertices with respect to other respect to points in the support of this polynomial. So as I said, so this distribution, since it minimizes the KL divergence, in particular, it has to satisfy the constraint that we added, right? So it has to have expectation alpha zero, okay? Alpha zero. Okay, so this expectation we know what are the marginals of this p star. These are just, when you normalize it to get probability distribution, these are just this qi times to the alpha i over the sum of this quantities, which are then coefficients of the convex linear combination of alpha zero with respect. alpha zero with respect to alpha one up to alpha n okay and these coefficients are called entropic coordinates for alpha zero with respect to the polynomial with respect to the measure q so this with this we can uh write down uh explicitly the optimal value of the kl divergence so you just plug in instead of your marginal spi this entropic coordinates and then you can also And then you can also write down capacity in a similar way. Okay. And so if you attended a talk by Maraike, this quantity could remind you something that appeared there. So this is not a coincidence. Let me explain this in the most simple case. And this is the case of a circuit support. So when alpha 1, alpha d plus 1, a vertices. alpha d plus one are vertices over this dimensional simplex in Rd, then there is actually a unique way to write alpha zero as a convex linear combination of alpha one alpha d plus one okay because then there is no freedom right so any point in the simplex can be written as a convex in a combination of the vertices of the simplex in a unique way and the weights are denoted by p1 The weights are denoted by p1 up to pd plus 1, and they are all positive in case when alpha 0 lies in the interior of this syntax. And then the capacity of Q with respect to alpha 0 is this expression. So in this case, the entropy coordinates are exactly this P i's because there is no any other choice for convex linear combinations. And this expression And this expression is exactly what you saw in Mareiti's talk in the case of circuits. And this will follow next. So it has a link to non-negative polynomials and specifically to this to circuit and age polynomials. So there are two independent approaches to this question. So one involves simplicity and other involves more. Simplices and other involves more general polytopes, but as we know now, they are somewhat intimately related. And so, let me consider a polynomial q-hat that is obtained from the former polynomial q just by adding an extra term. So, it's q0 times z to the alpha zero. So, the coefficients uh q one, q n, they are uh positive and alpha zero is in the relative interior of this uh of the Newton polytope of q hat or of q. Top of q hat or of q which is equivalent. So you can see here's a cartoon. So you can imagine that alpha zero is somewhat fierce in this relative interior. And there could be also other points in the relative interior of this Newton polytope, which we somewhat do not distinguish. So we distinguish on the one which we call alpha zero. There is an easy way to see that q hat is non-negative q hat is non-negative on the positive orson. If and only if when I divide q by z to the alpha zero, this has to be greater equal than minus q0. So I just put on the left the term q0 times z to the alpha zero and then divide by z to the alpha zero. And then I obtain this inequality. And this is exactly if you take the infinum in this case. If you take the infinimum in this case, you get In this case, you get that capacity of q with respect to alpha zero is at most minus q zero. Okay, and there is a well-known property of non-negative polynomial. So, if you if q hat, if you want q hat to be non-negative on the whole space r to the d, then all alpha one up to alpha n, which are actually here we have somewhat Here we have somewhat, it's not necessarily true. What I wrote here is this holds only if alpha one, alpha n are vertices of the Newton polytope of Q. But so this is like a slight mistake. So you pretend that alpha one, alpha n are all vertices, okay? And in order for Q hat to be non-negative, all alpha one, alpha n have to be even points, okay? They have to have even. Even points, okay. They have to have even coordinates. Therefore, non-negativity of q hat on the whole space r to the d is equivalent to the following condition. So capacity has to be at least minus q0 in case when the distinguished point of a zero is also even vector. And in case it's not an even vector, the capacity has to be at least absolute value of q zero. Absolute value of kazira. Okay. So I think you saw this characterization, but in a bit different notations in Maraika's talk. Okay. So namely, she showed in the circuit case where alpha d plus one a vertices of this simplex, this capacity can be easily equal to this quantity product of qi over pi. Of qi over ti to the power ti. And this quantity is known as circuit number of a circuit polynomial q hat. And so on. And this condition above for the non-negativity, this equivalent condition for the non-negativity of q hat is actually exactly what I think first appeared in a work of Bruce Resnick and then in the work by Timo Devolf and Sadiki. By Timo Devolf and Sadi Kilima, and then in the work of Chandrazikar and Shah. And this was somewhat different generalization and different point of views and somewhat similar objects. And so you can thus call capacity a generalization of a circuit number for more general polynomial than just circuit polynomials. Normals than just circuit polynomials. Okay. And now we come to a discriminants. So this is something that you might hate because there are a lot of horrible formulas. But if you like the book GKZ, Gerfan Kaparanov Zibinski, then you might actually like it. So let me fix now A, which will be now including also alpha of zero. Okay, so this is a collection of n. Okay, so this is a collection of n plus one integer points. Then the a discriminant of this a is a Zariski closure of the following set. So it's a set of polynomials supported on this set A, okay, that have a singular point in the algebraic torus. So this condition f of x tilde, etc., derivatives of respect to different variables of x tilde, zero, and this is exactly. X tilde zero, and this is the condition of X tilde being a single point of the variety defined by F. And then you take the risky closure and consider it as a subset of n plus one dimensional affine complex space. So like if you haven't seen discriminants before, you definitely saw this for example. So if A is just a collection of 3.01 and 2 in Z, then this And two in z, then this discriminant consists of all polynomials that have a double root, all quadratic polynomials that have double root. And from school, you know that this is equivalent to f1 squared minus 4, f0, f2 to be 0, because that's exactly this criminal from this pool. Okay, let's consider a toric action. So we consider the torus of dimension d plus one that acts on the space of Of polynomials on support A that have all coefficients non-negative in the following way. So we pick an element of the torus, small t, a polynomial f, and then we just scale variables by t1, et cetera, td, and then we'll scale the result of the polynomial by t0. Of course, you see that this action does not change actually property to have a singular point inside the torus. So if you pick a point that was singular somewhere. You pick a point that was singular somewhere in the algebraic torus. After applying this transformation, it will be still singular, but probably at another point. And so the goal here is to describe the quotient of the piece of the A discriminant that leaves in the algebraic tropics. So this is, I just somewhat summarize something that is written in GKZ book. Okay, I pick a matrix. Okay, I pick a matrix that has d plus one many rows and n plus one many columns. So I just add additional row of O ones. And I fix a basis A1 up to am of the kernel, of the integer kernel of this matrix A. And then you can identify the quotient torus, this started n plus 1 over t with the m-dimensional torus in the following way. In the following way. So it's you send a class of the polynomial T of the polynomial F to the vector F to the A to 1, etc. F to the AM. So it is an isomorphism of tori, and we will need this isomorphism to describe this portion of the A discriminant. So you get this A discriminant, because of the invariance, when you intersect, when you see this piece of the A discriminant inside the torus. Egg discriminant inside the torus, it projects to a sub-variety inside this m-dimensional torus. Okay, and this sub-variety would you know by nabla a tilde, and it's called the reduced a discriminant. And let me make the following assumption, which will somewhat be valid in a generic case, in the case of generic support. If code requires that codimension of the a discriminant is equal to one. Is equal to one. Okay, so it's a hypersurface inside the space of all polynomial complex hypersurface. And there is a nice way to somewhat parametrize the reduced A discriminant, namely this quotient R they discriminant. That is also, I think, due to GKZ, or maybe it was known before. I was not able to locate the first appearance of this. So we picked the same. We pick the same A1AN as before. This are just generators of the Z of an integer basis, the integer basis of the matrix A. Then we consider the following rational map. So it's mapped from Pm minus 1, projective space of dimension M minus 1, to the M-dimensional torus. And we map lambdas in the projective space to this topple. To this tuples psi one, etc., psi n. Okay, that are given by this formula. So you can don't pay too much attention to this formula. So it will be clear later what this corresponds to. And there is a theorem from this book. So there are first assertion is that the image of this rational map is actually the reduced state discriminant. Is actually the reduced A discriminant, and under this assumption that we made that if A discriminant is a hypersurface, then this psi, this map psi, is a birational isomorphism. Okay, and moreover, the inverse of this map is what is called the logarithmic Gauss map. Okay, so this map is known as Foreign uniformization of A or of the Of A or of the A discriminant A. And this is somewhat will appear in the relation to capacity and entropy in a couple of minutes. So also GKZ, they introduce a function they denote it by S of number. Okay, this is a function that somewhat involves this components of this map psi in a certain way. And this function satisfies the following conditions. Satisfies the following condition. So if you pick the derivative with respect to lambda l, then you recover log of psi l. And hence, if you now consider this gradient of this function s, this will parametrize the logarithmic image or the image of this reduced state discrete under the logarithm map. There's a multi-valued logarithm map. And so this function actually turns out to be related to n. Turns out to be related to entropy. And this was already somewhat anticipated by Gelfand, Kapraniko, and Zilovinsky in their book. So on page 294, you find that this function s, if you rewrite it in this way, is written here. It's easy to see how that can be written in this way. It looks like the entropy of a probability distribution, whereas the linear functions, functional spj, play the role of the probabilities of elementary events. Of elementary events, and somewhat they anticipate that it might have something to do with capacity or with entropy. So if we look at the circuit case, just to see how it works, we see that the kernel of this matrix A is just one-dimensional and it is spanned by just by one vector, A, where the first entry, A0, is. First entry, A0, is negative of this vector. All other entries are positive. And minus Ak over A0 are actually coefficients of the convex linear combination of alpha 0 with respect to the vertices of the simplex. Okay, it's actually easy to see that the function s is one homogeneous, so it corresponds to for this map. For this map psi being well defined. So, this I didn't mention this, but this psi is actually components of this psi have homogeneous expressions of degree zero. So, and this function s will be homogenous of degree one. And so, its value in particular, it's a function of one single variable, its value at the point minus one over a0, then completely determines the function. And if we compute s of this. function and if we compute s of this value we already see that it's related to capacity just log of minus one over capacity of the polynomial q with respect to the point alpha zero and the polynomial q i will explain later what this polynomial is but for the moment let's do not just believe me it will be somewhat given so in the case when alpha zero lies in the relative anterior or in the interior In the relative anterior or in the interior of this convex cell of A, we have a relation between which holds in general, not necessarily only for circuit supports in general. So capacity and entropy are related to this function in the ways written here. So where p of lambda is a probability distribution given by this marginals pk of lambda, and q is just coefficient. Q is just a polynomial whose all coefficients are one. Okay? And this is something that we believe is new and somewhat addresses this remark by Gelfand, Kafar, and Zelvinsky. And we have certain generalization of this, which we somewhat call AQ discriminants and Q uniformizations. But I don't have time for this, I think. But if you're interested, Think, but if you're interested, you can talk to me in the break. Thanks a lot. Thank you very much, and thank you for being in time, even if we would like to listen more to what you are saying. Are there any questions, comments for Kashkali? I'll ask: what's the definition of? I'll ask, what's the definition of an AQ discriminant? Okay, so if you ask, I'm happy to talk about this. So know that here we connect this function S to entropy, which is also KL divergence, where this measure, reference measure Q is counting measure. Okay, so we want someone to ask ourselves what is an analog that brings us to the more general. More general KL divergence, where this measure reference measure Q can be arbitrary. This corresponds to the polynomial Q on the last line to be also arbitrary polynomial with positive coefficients. And that's somewhat what pushed us to dig into this. And turns out to be quite elementary. So we define this what we call AQ discriminant. So just very similar definition to the AQ. Very similar definition to the A discriminant. The only difference is that here we plug in coefficients qk. So we somewhat write the polynomial in a bit different basis. We still identify the space of polynomials with vectors f0, f1 up to fn, but we prefer to write the polynomial also having qk inside the coefficient. Okay, and then you go through and apply the same definition. What's like what is a what is a A uh, what is a reduced a discrete? And so you just uh quotient out by the action of torus. So know that the two Aq and A are defined are related just by toric action. So you just multiply coefficients by Q0 to Kn. And this somewhat motivates the following definition. So this psi Q, this core Horn Q uniformization, somewhat also involves this QK. So this QK appears everywhere. So, SQK appears everywhere. And this foreign Q uniformization is related to the standard uniformization again via toric action, but now inside the n-dimensional torus, in this quotient torus. And a corollary of this GKZ result implies again that this psi q is a pirational isomorphism. And if you now look at the related function sq, you see that You see that, and you rewrite it as appears here. You see that it looks really like KL divergence. Okay, and that's exactly what we prove. So we prove that in general for any Q1 up to the n. So if you restrict this function on a proper polytope that I haven't talked about, then this turns out to be exactly KLW. KL divergence, and then you can, or like almost related to KL divergence, and then you can again link it to capacity via the link I explained before. So that's this story short. What is this Q context? Thanks. Oh, there is a question. Can you hear me? Yes. Well, just a quick comment. You mentioned Horn the uniformization. Actually, Kapranov's name should be in there because I think Horn had the uniformization only for a special case. 1990, Kapranov had a paper on this and he generalized it quite far. So the version that we know now, The version that we know now, you could attribute it more to Pranov, I think. I see. Okay, yes. I was just looking in the book and I didn't see like a reference point. Yeah, yeah. But I think they're just being modest. But Kopanov's paper in 1990 was very important. I see. Okay, great. Thanks. Sure. This is not important, but in a paper with Angelica Queto many years ago, we proved that in fact you don't need to have that this is that the A's affinely generate Z to the N for the by rationality of the Gauss map, etc. Oh, I see, that's interesting. I don't think that this has any importance here, but okay, but what about Gaussman? But what about Gaussman? Because in this case, I think it will be higher, it will have higher co-dimension, right? Or no, no, no, I mean, they are Q linear, they define the they generate Z to the N over Q, but not over Z. I see, okay. I see, I see. Sorry, sorry. It's like another lattice, right? They generate another lattice. Yes, but all the rest is. But all the rest is so this is okay. If I rationalize some of his blah, blah, blah, nothing changes. It's not important.