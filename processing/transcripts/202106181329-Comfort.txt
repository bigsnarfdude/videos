Which is on the archive down here, and we'll also be presenting this at ACT in a month. Okay, so a graphical calculus for Lagrangian relations. First, before I talk about Lagrangian relations and its graphical calculus, I want to kind of build up to it and motivate it using similar minoidal theories presenting. Minoidal theories presenting more like simple things. So, first, I want to talk about graphical linear algebra and graphical affine algebra, which has been developed by Pavel and his group recently, which will kind of be very important for this paper. And then I'll talk about how the kind of motivation for writing this paper came about, namely that graphical linear algebra and graphical affine algebra are intimately connected. Graphical affine algebra are intimately connected to the ZX calculus, which is a syntax for quantum circuits. And then I'll give a really short review of linear symplectic geometry and actually define what Lagrange chain relations are. And ultimately, I'll construct something analogous to graphical linear and affine algebra, which I'll call graphical Lagrangian-chain algebra. And finally, I'll talk about the applications. Talk about the applications of this in stabilizer circuits and in electrical circuits. So given a ring K, the monodal theory for matrices over K can be presented in terms of a monodal theory. So you have this white copy and delete, and then this gray monoid is like addition and zero. And then for every element of the ring, you have And then, for every element of the ring, you have this generator from one to one. And the first two lines are like saying you have Le Vir theory for cumulative monoid, and then the, or I guess also this second last line as well. And then the final line kind of encodes the structure of the ring. And I think this is. And I think this is like a folklore result, but it's proven explicitly in Zanazi's thesis that this is the case. This presents matrices over a ring under the direct sum as a tensor product. So here's an example. So if you have like a string diagram generated by these things, composed of these things, then the way to interpret this as a matrix is just kind of by annotating each wire with like an indeterminate variable. With, like, an indeterminate variable, and then copying it through the white commonoids, deleting it here, multiplying it when it reaches one of these elements of the ring. And then we can see when you evaluate this, this is the same as multiplying the matrix on the right by this vector. So hopefully this convinces people that this is actually true. So we can, since matrices over can since a matrices over so matrices over a ring isn't finitely complete but over a field it is so uh if you add uh both the equations of matrices in terms of this modal theory plus all of the equations for the opposite category by flipping everything around and then you quotient by the special forbenius laws for both the white and black monoid and commonoid and then you enforce that And then you enforce that every inverse law for the elements of the field. And then you make sure that these cups and caps can be transmitted to each other. Then this gives you a presentation for relations over matrices over K. So this is called linear relations, but it's literally just rel of matrices over K because matrices over K is a regular category. Regular category and actually, it turns out that the antipode and its transpose have to be equal, so we'll use this kind of asymmetric or this symmetric notation, this black box to denote the antipode and its transpose because it's the same. Okay, and there's an observation made in Zenassi's thesis that linear relations over F2 is the phase-free ZX calculus. Calculus. So that's to say that you have these two Frobenius algebras interacting to form a Hof algebra with antipode being the identity. But actually this generalizes to all to Fp, not just F2. So if you fix a prime number and you look at linear relations over F2, then essentially this is a presentation for the This is a presentation for the subcategory of FHIL, where the objects are powers of p, and the morphisms are generated by these two spiders induced by the Fourier algebras. And essentially, this is just like for each basis, you have a generalized Kronecker delta that just compares all the elements of the basis and checks if they're the same. And in kind of categorical quantum mechanics, the Mechanics. The structure generated by this is called the phase-free ZX calculus. And this is kind of a very, very basic fragment of quantum circuits, but it's axiomatized in terms of linear relations here, these equations. Okay, so you can go a bit further and you can talk about affine relations over a field. Over a field. So if you add this new generator, like from 0 to 1, which I annotate as like a gray unit with a 1 inside of it, then you can get the prop of affine relations. So this minor technicality here because although relations over a field is a regular category, Regular category. This is only the case when you allow the empty set to be an affine space. So actually, to get a prop, what we do is you take the category of relations over finite dimensional affine vector spaces, and then you look at the full subcategory of this non-empty finite-dimensional affine vector spaces to get a prop. Get a prop. And these equations characterize this. The first one is like saying if you multiply by the zero scalar, this like destroys the connectivity of everything. So zero collapses everything together. And then the second two equations is just saying that this affine shift can be copied and deleted. So we're just adding these equations to linear relations, and this gives you affine relations. Relations. Another observation is that affine relations over Fp is equivalent to the so-called p-dimensional q-dit fragment of the zx calculus with the shift operator. So the shift operator is like, in this case, corresponds to the affine shift, but actually this can be interpreted kind of like. But actually, this can be interpreted kind of like as the NOT gate in quantum circuits, for those of you who are aware of this. So, in particular, there's this kind of nice notation we can use so that this kind of motivates the reason why I used one in this box as the notation for the affine shift. Because if we follow this by an element of the field, and then we attach it to the Field and then we attach it to the spider, then we get this kind of addition law corresponding to the additive group of the field. So we can, in these gray spiders, they can kind of contain an element of the field. And then when you connect them together, you add them together with the additive group. And this is called phase spiders, and this is phase spider fusion. And again, the phases correspond to the additive group. Correspond to the additive group. So, for those of you who know about the ZX calculus, this is like a typical thing. Okay, so now that I've kind of established the monoidal theories that I'm going to be working with, I have to actually get back to this symplectic Lagrangian stuff. So I have to stop using pictures for a little bit and then go into some. For a little bit and then go into some algebra. So, given a field, a vector space has a symplectic form when there's a bilinear map omega from v squared to f. Or sorry, they shouldn't say f, they should say k. That's alternating and non-degenerate. So, these two equations. And we say that a vector space with this structure is. This structure is called the symplectic vector space. And in this case, we're only caring about the linear case. We're not talking about manifolds or anything. So for the rest of this talk, just assume everything is linear. A symplectomorphism is a linear isomorphism that preserves this form. So we can ask when symplectic vector spaces are isomorphic in the right sense. In the right sense. And the idea is that symplectic vector spaces are like the phase space of some mechanical system. And the symplectomorphisms are like the phase space evolving, like under some reversible operations. So this is just some physical intuition. But in the finite-dimensional linear case, every vector space of the form form k to the power of 2n is equipped with a bilinear form, a symplectic form of this, a symplectic form of as follows. So that to take the form you just left multiply and right multiply by vectors like this. And every finite dimensional symplectic vector space is isomorphic to something with this form. So we'll just like consider symplectic vector spaces. Consider symplectic vector spaces only with forms like this, just for the sake of simplicity, just kind of strictifying everything. Okay. So there's a notion of a symplectic dual. So instead of taking the dual with respect to the orthogonal complement, we can take the dual with respect to this, or instead of taking the dual with respect to the inner product, we can take the dual with respect to the Inner product, we can take the dual with respect to the symplectic form. So we can, the dual of a symplectic vector space is seen as the space containing all of the vectors which are orthogonal to the vectors in the original space. And when the vector space, when the symplectic vector space is contained within its dual, it's called isotropic. When the dual is contained within itself, it's called co- Dual is contained within itself, it's called coisotropic. And when they're equal, it's called Lagrangian. The third case in this talk is what we'll be focusing on. And Lagrangian subspaces always have dimension n. Lagrangian subspace k to the 2n always has dimension n. And another way to look at them is they're maximal subspaces of a symplectic vector space with vanishing symplectic form. And these generalize. And these generalize symplectomorphisms because every symplectomorphism induces a Lagrangian subspace by taking the graph. So, okay, now back going back to pictures, what do these look like in terms of pictures? Because Lagrangian subspaces are linear subspaces, at least in the setting that we're setting them. So in principle, you can use these diagrams of linear relations. Diagrams of linear relations which describe linear subspaces. So there's a prop of Lagrangian relations. So the morphisms are symplectic vector spaces equipped with this symplectic form as we just discussed. And the composition is given by relational composition. So it's kind of the same idea as in the linear relations case, but in this case, Case, but in this case, now there's a restriction on the objects being symplectic vector spaces, and the morphisms have to be Lagrangian relations, not just arbitrarily linear relations. So, because Lagrangian relations are kind of graded into two parts, the tensor product is only like there's just twists in it. So, we have to associate the. So, if you look, look at this, there's like two parts here. So, we have to, when we tensor, we have to pull them into both sides. Tensor, we have to pull them into both sides. And this condition of Lagrangian subspace being a subspace of an isotropic vector space, which is equal to its symplectic complement, can be translated into pictures as follows. So for the case of a state, you have this equation on the left, and then just for any morphism of any type, then you can kind of unbend it, and you get this thing on the right. Get this thing on the right. And this perp is the orthogonal complement. Because you can get, and these boxes again are like minus one. These are the antipode. So you can, the idea is pretty much you can define the symplectic complement in terms of the orthogonal complement and minus one, and then this is just that in pictures. And then this is just that in pictures, yeah. So, so uh, in terms of pictures, this color swapping functor, so this is on uh Pavel's blog, Graphical Linear Algebra, but I don't, it's not in the literature really like published anywhere, but it's on his blog. The color swapping functor, so that this which takes linear subspaces to their orthogonal complement with respect to the normal inner product, not the symplectic inner product. Yeah, so the function that just switches. Yeah, so the functor that just swaps the spiders' colors and then takes the scalars to their inverse is the same as the orthogonal complement functor. So this is kind of cool observation. And moreover, there's a strong symmetric model functor, which is also faithful from linear relations to Lagrangian relations given by doubling, but then on the left-hand side, you swap all the colors. And note, this is only a strong monodal because of. This is only a strong monodal because of this tensor product is twisted. Okay. And actually, so this is like the main theorem of the paper is that, so we have this graphical condition for Lagrange relations here, but we don't actually, it doesn't say much about the structure of this in terms of like amino theory. But actually, we can give generators for Lagrangian relations. For Lagrangian relations. So if you look at the image of this doubling functor, and then you add these two generators on the left here, then this gets you all of Lagrangian relations. So on the left, this is like the Fourier transform. And this one on the right here is like the generalized phase shift gate. And then this extra one, which is derived, which is very useful, is like the kind of generalization of the controlled NOC gate. Generalization of the controlled NOC gate. So I only put this here to kind of insinuate the relationship to the stabilizer group, which I'll talk about in a little bit. But yeah, this is the kind of main theorem that by adding these things to these double linear relations, you get all of Lagrangian relations. And the proof is that essentially these three generators act like kind of like elementary raw. Elementary row operations for on the matrices which represent the linear subspace of a Lagrangian relation. So you reduce the linear subspace to the ideal matrix, and then this is how you prove this theorem. And moreover, to get affine Lagrangian relations, so this generalization of Lagrangian relations where you can also compose an affine shift, you just have to add one more generator, which I call X, which on the Which on the left wire does nothing, the identity, and on the right wire, you like add one with the affine shift. And this, you can get all affine shifts this way. So, this gives you the category of affine Lagrangian relations. Okay, so now I have to talk a bit about the connection to quantum computing, which I kind of have hidden up until this point, maybe hand-waved over a bit. So there's this way of characterizing the n-dimensional q-dit Clifford group. So that's to say the Clifford group where the base is n in terms of this operator X, which is called the boost operator, operator C, the control boost operator, F, the Fourier transform, and S, the phase shift operator. And these are kind of the QDIT generalizations of the NOT gate, controlled NOT gate, HadMart gate, and the phase shift gate. But yeah, and then if you add the so this this notation with like the kets is like a basis for uh for for uh is is a basis indexed by fp um so if you add uh like the basis elements and their transposes then uh you get stabilizer circuits which are very important in quantum computing but uh this theorem here says that when p is an odd prime Then, affine Lagrangian relations over Fp is a presentation for these p-dimensional Q-dit stabilizer circuits up to scalars where we have this translation here. So, these circuits described in this lemma above are equivalent to these affine Lagrangian relations over Fp. So, the naming is kind of suggestive. So, F here, the Fourier transform, if you fix A is equal to 1, this is this phase shift gate. You fix A is equal to 1. One, this is the stage shift gate. You fix a is equal to one, this is control NOT gate, and then this is like the NOT gate. But P has to be prime for an odd prime for technical reasons, which is kind of annoying. And for people that are familiar with the stabilizer formalism, the idea here is that these affine Lagrangian subspaces are like the stabilizer tableaus of stabilizer states. Half a minute or so. Okay. Yeah, so. Yeah, so the idea here is now you have these phase spiders in both white and in gray, but now the phase group is not the additive group of FP, but rather the additive group of Fp squared. So you can capture a richer fragment of quantum circuits. And in the case of F2, because 2 is not an odd prime, then this is equivalent to what's An odd prime, then this is equivalent to what's called Speckin's toy model, not stabilizer circuits because these groups Z mod 4Z and Z mod 2Z squared are not isomorphic to each other. So there's this divergence in the even case. And then here's just another connection to electrical circuits, which was observed by Bonshi and Bayes and all these people, that when you work over the field of the real rational functions, then you get a semantics for Then you get a semantics for electrical circuits. So you have the wires kind of force Kirchhoff's law to hold. And then you have resistors, inductors, capacitors, voltage, and current sources. And these all obey the equations you would expect them to in an idealized model. So that's my talk. Here's some references which I used. And if you want to read more about it, I suggest you. Read more about it. I suggest you go to this archive link. Thank you. Thank you, Cole. All right.