Work that was started something like 10 years ago by these four people who are all in this room, so they can all shout if I mischaracterize their work. And this work was sort of continued by me starting in 2020 and thanks to Blunday. Ben in particular helped me. Ben in particular helped me get started with the software side of things and David tried his best to get me to stop thinking about the software side of things because I spend too much time doing that. And so, in particular, special ones. So, the basic problem I think that this work tries to address is encapsulated by this question, which is: what can we learn from epidemic time series when we have only limited or sparse information? Limited or sparse information about the mechanisms like the lying spread and about the disease itself. This question is relevant whether we're talking about historical time series or time series from emerging epidemics. In both cases, we contend with uncertainty about the disease, its natural history, modes of transmission, and so on. And this question actually has statistical and epidemiological dimensions. And epidemiological dimensions, the implicit statistical question is what parameters can we reliably estimate using information contained in the time series? And also, how can we robustly estimate those parameters for some meaning of robust? And the epidemiological question is: how can we use that estimated model then to infer things that we care about the disease or the population? About the disease or the population in which it's circulating. And in an outbreak context, what can we say about the effect of public health interventions that are ongoing? So our work approaches this, or has approached this broad problem by focusing on a salient feature of epidemics, which is the initial but transient period of exponential growth. And so for the rest of the talk, I'll And so, for the rest of the talk, I'll be talking about initial exponential growth rates, how we estimate them, and how we can compare them across scales. So this is just an outline. What is little r? I haven't defined that notation yet, but little r is the initial exponential growth rate. Why do we estimate it? Why do we care? And how can we do that more robustly, assuming that we do care? Assuming that we do care. And then, sort of, my contribution to this work is trying to elaborate sort of the initial methodology to try to understand heterogeneity in little R, both within time series across waves, but also between time series across jurisdictions and so on. And finally, I'll just briefly plug in our package that sort of In our package, that sort of encapsulates the software side of what we've done. So, we learned from very simple compartmental models that in the earliest stage of an epidemic when the population is nearly fully susceptible, that the cumulative number of cases will grow, will tend to grow exponentially. And so, we call that initial rate of exponential growth the lower. And we can formulate this thing mathematically or with mathematical notation. So c is cumulative incidence, then we say that c is an exponential function with x1 r times t. But much more powerfully, we can write that as by saying that the log of the cumulative incidence is linear with slope little r. And that's tremendously useful because it means. Useful because it means that we can estimate little R by fitting a line to a log-transformed cumulative time series. Now when translating our work for consumption by a broader audience, we tend to not report little r directly, but rather the doubling time. It's much more intuitive, understood as a time scale. And this relationship with the doubling. And this relationship with the doubling time emphasizes that the literal R is really telling us something about the time scale over which disease is spreading in a population. Finally, we can understand little R as the complement of a different epidemic parameter, one that dominates popular discourse for some meaning of the word popular. And that's R not, the basic reproduction number. Basic reproduction number, or not, for people who may have not defined it themselves, is the expected number of individuals infected by a typical infectious person in an otherwise fully susceptible population. And RA tells us something about how many people will eventually be infected over the course of the epidemic. Of the epidemic. But being a unit less quantity, it tells us nothing about the time scale over which those infections will occur. And so little R and R not are complementary in the sense that they are both tied temporally to the start of an epidemic when most people are susceptible, but they convey different information, distinct information about the epidemic conveying the speed and are not conveying the strength or expense. Command the strength or expense. And I added this thought really quickly actually last night, just to show that for some fixed value of R0, the time scale over which spread happens can vary drastically from days to weeks to months to years. And then for some fixed value of little r, we can see epidemics with quite With quite varied final sizes. But now looking at this bottom right panel, I feel like maybe a little R is not fixed after all. Anyway, let's handle that for now. No apologies. But you can imagine that all those lines are parallel. So, why do we estimate little R in practice? And I'm noticing that all my footnotes render. And I'm noticing that all my footnotes render before any of my points render, so I don't know if that's any concept. So, little R in an outbreak context informs how fast public health interventions need to be devised and carried out in order to meaningfully curtail, spread, and reduce burden on healthcare systems. If the doubling time is on the scale of days and public health decision making happens on the scale of Decision making happens on the scale of time scale of weeks or months. Well, hopefully, that's not the case, but I hear it happens. Yeah, so estimates are obtained by fitting clinological models. They speak for themselves in a way, because we can communicate them without sort of attaching or needing to attach. Explanations or justifications for some underlying mechanistic model that we've proposed to sort of capture the current situation. And a final reason is that we can use little RTAP to infer R non conditional on generational interval distribution. And even if that generation interval distribution is uncertain, we can still Uncertain, we can still use little R to understand how that uncertainty is propagated to R0. And the link between R and R naught is described by this very nice paper by Evan Karl Hopsich. But I won't get into the details. So, if we do care about little R, then in principle, Then, in principle, we would want to estimate it accurately or more robustly. And one of the first things we do is, in practice, is maybe to fit a line to log cumulative incidents or log cases. But estimates of little are obtained this way from an exponential model, they are inherently biased because due to susceptible depletion. Due to susceptible depletion, the actual initial exponential growth phase is transient. And so estimates of little R tang this way tend to be underestimates because they fail to account. The model fails to account for susceptible depletion. And then for the same reason, estimates of Lil R obtained this way are very sensitive to the data that we provide to the regression. To the regression, which is the set of observations included in the regression, also known as the fitting window, in our work at least. So it stands to reason that we can, oh, and I also mention that another sort of corollary of this problem with the exponential model is that the confidence intervals associated with those estimates have low statistical coverage. And I'll get back to what that means. That means here later. Stands for reason that we can do much better if we can devise or propose a phenomenological model that not only is initially exponentially growing, but also accounting for susceptible depletion, presumably with some kind of saturating behavior. And two of the candidates for such models are the logistic and the red joints. And these are given here. The Richards is just a generalization of a logistic. You set A equal to the line, A being the shape parameter associated with the Richards model. So I'm just going to show two figures from a simulation study that the group MinusMe did 10 years ago. 10 years ago, I was high school. High school. I think the key thing, what's plotted here is estimated value of little r as a function of the right endpoint of the fitting window. So as a function of how much data we're giving towards the peak in the incidence. Towards the peak in incidence and away from the actual initial exponential growth phase. This is just showing that the exponential model becomes biased really fast as we provide it with more data towards the peak, whereas the logistic and Richards do an okay job and are unbiased all the way up to the peak, more or less. All the way up to the mean, more or less. The second figure is describing the coverage, probability of the confidence intervals associated with those estimates of little R. I think I would just focus on, I would just focus on the second panel in the top row, which shows that my coverage probability is defined as. Average probability is defined as the proportion of the confidence intervals obtained from some number of simulations that contain the true value of little r used to generate those simulations. So this is showing that as we provide the regression with more data, the coverage probability for the exponential model goes essentially to zero, whereas it is preserved more or less by logistic measures. So I hope this, I found this argument or evidence quite convincing that in practice we really should be estimating logistic or Richards or similar models and steer away from exponential models. And certainly the computational cost of estimating that additional parameter is close enough. So our work with So, our work with non-modical models has precipitated an R package. I think the initial version was written by Jinling, and then I rewrote it more or less from the bottom up in 2020. The current iteration has been used extensively by David to produce plots like these, which very succinctly display the evolution of the doubling time. Of the doubling time within a time series over the course of a pandemic in some jurisdiction, in this case, Ontario. So in the top row, the fitted value of the doubling time from a logistic regression, or sorry, a logistic fit to the cumulative curve within the highlighted segment. So these highlighted segments are the fitting windows that I define. But we'd like to aim, and these plots are obviously very informative and useful and easy to sort of absorb at a glance. But we'd like to aim actually a bit higher, specifically with a graphic like this one, we can sort of point at differences within the time series between doubling times, and we can speculate about the causes for those differences. And we can speculate about the causes for those differences. Maybe an intervention took place or vaccines were distributed at a certain point in time before its second rate of event had an effect. But we'd like to do a bit better than pointing and guessing. In particular, we'd like to sort of formally quantify the importance of covariates of interest. Covariates of interest in explaining observed heterogeneity in the area we want to explain variation within a between time series by incorporating covariates and interest and those are sort of primary candidates, sort of obvious candidates are for interesting covariates are vaccine coverage, but that's sort of complicated. It's hard to summarize vaccine coverage in one number. In one number, when there are different variants and different doses and different brands. We can think about NPI status of the jurisdiction that we're looking at at the time of the given wave, NPI compliance with public health advice in the population at that time. If we have that data, we can certainly also look at demography. And they also look at demography, population density, age distribution, and we can think of a lot of other interesting things. A lot of data was made publicly available during the pandemic. There are a lot of these kinds of analyses. And sort of to support this kind of investigation, we need to move away from estimating a simple nonlinear model, such as logistic, to estimating a non-linear model. To estimating a nonlinear mix of X model. And in this sort of mix of X modeling framework, we're not estimating log R, well, log is the length, so we're not estimating log R directly or build R directly, but rather we're estimating the coefficients of a linear model. So log R is a linear function of the covariates of interest. And so the software side or the machinery that implements this sort of undertaking is done, actually. But there have been some hurdles that prevented at least me from sort of applying it straightforwardly two years ago when I was looking at growth rates globally. Growth rates globally, heterogeneity growth rates globally across countries. One of the hurdles was that covariates of interest may not be measured uniformly across the jurisdictions that we're interested in. So we either have to impute those covariates or settle for the ones that are measured uniformly. For instance, many. Um for instance, many countries did not have published information about vaccine coverage in the population at the times uh that they had case reports for. And finally, if we were just estimating the nonlinear model, then maybe all we were interested in is the number of time points or the sampling within the fitting window, how many time points are there. The fitting window, how many time points are there. But when we use this more sophisticated model, we're also interested in the number of fitting windows that we have for each jurisdiction because conceptually the sample size for a mixed effects model in this setting is the number of fitting windows. And so if that's too small, then we may not be able to robustly estimate the parameters of the So make the miss the parameters of the mix of XML, but we're interested in so all of this is encapsulated in our package called EpigrowthFit, which should be released to Cren, sort of soon. I became busy with preparing for this. The interface will be very familiar to people who have done mixedFX modeling before, and it's inspired by And it's inspired by LME4 and me. And it's okay if that's meaningless to you. The important thing is that it's there and it's relatively well documented. So if you want to, you could, and you could certainly talk to me if you wanted to. A very important development of this version of the package over the last one is that the backend is implemented on C. So, and all of the fitting happens jointly rather than sequentially if we're thinking about different waves. And so that makes using much easier. And certainly we have saved a lot of time by using the new pipe end. That's it for me. I hope I've motivated sort of use of these. So use of these saturating phenomenological models, maybe you have access to those sites, if I hope you will. Thanks.