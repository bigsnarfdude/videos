Very much for the invite and the introduction. So, I hope you are having a great time in Oaxaca. Unfortunately, I'm not able to join you in person, but I'm glad to share a joint work with Alex, who I believe is in the audience on detection and recovery gap for plant detection cycles. So, in particular, I think this work is related to many of your research. So, yeah, so please stop me. Yeah, so please stop me if you have any comments or questions at any time. I'm happy to have a discussion. Okay, so the outline of the talk is as follows. I will first discuss the motivation and then discuss the model and the results. I will give a very brief outline of analysis. But there's one thing that I'd like to emphasize, which is a low-degree lower bounds for recovery. Basically, a technology invented by Alex and Celio. Presented by Alex and Celio in a previous paper, and I want to talk about some of my present view of this method. Okay, and then I will end with some extensions and summary. Okay, so first, what is this model about? So our model is very similar to the Waltz Strawgas small world model proposing in 1998. So the idea is as follows. You have n nodes, say, on a circle. So look at the On a circle, so look at the graph on the left-hand side, and suppose there are more connections between k-nearest neighbors. So, k is something much smaller than n. And let's say you don't observe this perfect k-nearest neighbor graph, but instead you delete some edges in this graph and add in some edges for between nodes that are far apart. And then you sort of have an And then you sort of have an interpolation between the clean, cleanerist graph and the noisiest Erdogini graph. So, some interpolation between the two, and this is called the small world graph. Okay, so in terms of the adjacency matrix, when you order the nodes according to their natural order on the cycle, then you observe the expected adjacency matrix with a band whose bandwidth is 2K. Whose bandwidth is 2k, and on the band you get connection probability p, and off the band, you get connection probability q. And the adjacency matrix is just some noisy version of the expected matrix. Okay, and since this proposal, this model has attracted a lot of attention because it has wide applications in, say, collaboration network, friendship network, and even some biological networks. Although the Although the practical side of this model is very famous, the theoretical side has not been studied well until recently. So this model by Kai Liao and Rackling started analysis of the small world model, but it leaves many rooms in multiple regimes. And Yiho and Jia Ming actually had. I actually had a couple of very nice papers which proved sharp both information theoretic bounds and showed some efficient algorithms that achieves clean conditions when the bandwidth is small. So by small, I mean if k is n to the little o of one. So we are looking at a different regime as I'll be discussing later. We are looking at the regime where k is n to some constant power. Constant power. And this problem is also related to one-dimensional embedding. There's a question. Yes. Sorry, I can't hear you. Hello, can you hear me? Yes. What is the problem? You say more analysis. Analysis of what? Were you recovering the order of the vertices around the ring, or what is the problem? Yeah, so that is roughly the problem. Is roughly the problem. So, recovering the order of the vertices around the ring, or say you observe this adjacency matrix and you want to recover the expected adjacency matrix, but the actual observation is a shuffled version of the adjacency matrix. So, I will show the picture later and I will formulate the problem a bit later. Okay, so yeah, so it's also related to one-dimensional embedding. So it's also related to one-dimensional embedding of vertices and graphon estimation, and it's studied by a few groups of authors. Okay. All right. So second motivation of the problem is the stochastic block model. So as you can see, if you have, say, n over k blocks planted in a with size k, each of them has size k and planted in a graph. And this block. A block model has a similar signal-to-noise ratio to the band matrix model that I just showed you. But rather, the planted cycle model is more like the stochastic block model with overlapping communities. As you can see, this is like the signal-to-noise ratio in this model, it is very similar to the planting cycle model. To the planting cycle model, and once you have the same pq and the bandwidth k. And I just want to mention one more connection to random geometric graphs. So in some sense, we are just looking at the one-dimensional version of random geometric graphs. But in reality, say if each node has a latent position in dimension D, then you can connect nearby nodes with higher probability and connect the Higher probability and connect the nodes that are far away with lower probability. And so, this very nice paper by Bubak, Ding, Elder, and Ross studied the random geometric graphs. And after that, there were a lot of developments in this area. And in particular, I think in recent years, Liu Rass and Liu Mohanty, Schram and Young made some progresses for random geometric graphs. But we are looking. But we are looking at really the one-dimensional case, and I will talk about very briefly that some of the results can be generalized to finite-dimensional random geometric graphs, while these papers mostly focus on the high-dimensional case. All right, so let me now formulate the model and the results. So, to be more precise, we are considering n latent locations called Z1. Latent locations called z1 through zn uniformly distributed on the cycle. Okay, and let me use d zi zj to denote the distance between i and j on the cycle. So for example, d 0.2.9 is 0.3, not 0.7. All right, and we observe a graph with adjacent matrix A, where Aij is Bernoulli P if the distance between Zi and Zj is smaller than KON. Smaller than k over n. And the distribution is finally q if the distance is larger than k over n. And that's why k is still the bandwidth parameter in this model. Okay, so this is the actual expected adjacency matrix and the adjacency matrix. So by expected, I mean conditional on the latent locations zi to zn. And as you can see, it is some less strict version. Yeah, less strict version than the small work model. And at least visually, it's very similar to the overlapping case of the stochastic plot model. So this is the model we study. And so in fact, the left picture is not we observe because we assume that the nodes of this graph are Of this graph are unlabeled. In other words, we don't know the latent locations. So, what we observe might be some shuffled version of the matrix on the left. And that's why it's not a triple task to recover where the band is or whether two latent Zi and Zj are close or not. So that is. So that is part of the task to recover which nodes are neighbors and which are not. But first, there's a more basic question, which is a detection question. So by detection, I just want to detect between H0. H0 is the early training graph with some density R and H1, the alternative, is this planted dense cycle model. Plumpted dense cycle model parametrized by n, p, q, and k. And in particular, this r for the null model is chosen to be this specific value so that the degrees of these two models are matched. So, this is only going to be going to make the problem a bit harder. Okay, and for recovery, yeah, so certainly we can say we want to recover the permutation. To recover the permutation that order these nodes correctly. But here, let's do something more modest. Let's just focus on estimating whether i and j are neighbors or not. By neighbors, I mean the distance between their latent locations is smaller than k over n or not. Okay, so that is our recovery task. At least in expectation, recovering one pair of nodes is basically. pair of nodes is basically as hard as recovering all the pairs of nodes if you only care about the um the the correlation okay which i will discuss later all right so uh first let me say again that the information theoretical result is not our focus here because if for example for either detection or recovery if we focus on a constant p and q the dense case then in fact even when k uh even when k is n to some little o of one then we can detect and recovery uh we can solve the detection and recovery problems and i suggest you to look at a couple of papers by by yi ho and jiami um so here instead of information information theoretical result we focus on the computational framework of low degree polynomials because this is a powerful set of algorithms that are actually Algorithms that are actually efficient. Okay, so by low-degree polynomials, I mean I consider a polynomial FA in the entries of A, in the entries of the adjacency matrix. And then here, D is n to some little O of one. This is what I mean by low degree. But for the rigorous results, I will state a more precise D. So this is a good computational framework. Good framework, computational framework. For one reason, it covers many canonical algorithms. Certainly, it covers all polynomials. But in fact, it also covers a spectral method, at least some spectral method. The reason is that, for example, if you want to look at the leading eigenvalue of the adjacency matrix, you can write it approximately as a trace of A raised to some low degree power. That's why this framework also covers some spectral method. Also, cover some spectral methods. And also, it has achieved success in predicting computational thresholds in many models, such as the planted click and planted community detection and sparse PCA. So this survey by Dimitri, Alex, and Afonso, and also Sam Hoffman's systems are some good sources to look at. And this framework is also very related to some of squares. Very related to sum of squares. Okay, so let me state the problem for defection a bit more precisely. And here, we are taking d to be log n over log log n square in the regime where pq are constant. In some other regime, d can be relaxed, but let's focus on this particular d in this talk. And we say that And we say that the polynomial strongly separates H0 and H1 if the variance of the polynomial under either H0 or H1 with the square root, so we're considering standard deviation of this polynomial. If it is much smaller than the gap between the expectation of the polynomial on the two models, then yeah, you can simply threshold this polynomial. Polynomial to achieve consistent detection. Basically, you can do the hypothesis testing from between H0 and H1 with high probability. And this is the goal. And we call it a strong separation. In fact, this appeared in one of Alex's recent papers. And we say that the problem is easy if some polynomial strongly separates H0 from H1. And we say it is hard if no polynomial. If no polynomial strongly separates H0 and H1. Okay, and for the recovery problem, we are looking at a polynomial of the same degree, at least the same bound on the degrees, and we consider estimating the indicator whether vertex one and vertex two are close. Okay. And we consider the expectation under the alternative model, which is a planted density. Alternative model, which is a plantive dense cycle model, and we consider the mean squared error. And so it's not hard to see that if you take f to be constant, which is the expectation of this indicator, then you achieve a triple error delta. And we say that, okay, so the problem is easy if there's some polynomial achieving error smaller than half of delta, and hard if no polynomial achieves this error. This error, yeah, of course, this delta over two is arbitrary, it's not precise here. It can be made precise, but for simplicity, let me just state hard and easy to mean this particular statement. Okay. All right, so this is our main result. So let's assume p and q are constants. And then the problem becomes how large does the bandwidth. Does the bandwidth have to be so that we can do detection or recovery? And for degree D polynomials, where D is this log n over log n squared, then the detection problem is hard if k is much smaller than n to the quarter and easy if the other if it is the other way around. And recovery is hard if k is smaller than root n, okay, and easy if k is larger than root n. So as you can see, these two. Is larger than root n. So, as you can see, these two conditions are different, and therefore, there exists a gap between detection and recovery. So, in particular, one case between n to the quarter to n to the half, then detection is easy, but the recovery is hard. And that is the main message of this work. All right, so in fact, the intuition of this gap is very simple, I'd say. Let me just explain it to you, and maybe now you could. Explain it to you, and maybe now you quickly realize why there's a gap. So, if you consider the planted click model or planted dense subgraph model to be more precise, so this is number three, okay. And it's well known that root n is the threshold for either detection and recovery. And if you look at the recovery problem, so by recovery, I mean I want to recover some local structure, I want to. Some local structure. I want to tell whether two neighbors, two vertices are close locally, or two neighbors are in the same community in this block model, or say if some vertex is in the clique, right? So these are some local information in some sense. So then the recovery hardness across all these problems remains similar. These problems remain similar to each other. But if you think about the detection problem, they are actually very different because simply you have more signals for model one and model two, right? You have way more signals than number three. And that's why it turns out the detection threshold becomes k equal to n to the quarter for model one and two. So that is the main intuition of why there's a gap. Of why there's a gap. All right, so let me now briefly go over the analysis. So I will certainly talk about four bounds: right: the recovery upper lower bound and detection upper lower bounds. So let me first talk about the upper bounds. So the upper bound for detection is almost already there. I mean, certainly you have to work through all the details, but this statistic appeared already in this paper by Bubeck. In this paper by Bubeck, Ding, Elder, and Ross, because they propose this signed count of triangles in the adjacency matrix. So, namely, if here, say, if you don't have this minus R, then certainly this is a count of the number of triangles. But if you center the entries of the adjacent matrix, then this becomes a sign count. Becomes a sign count, and turns out this statistics is good enough for detection in random geometric graphs. And we are studying a one-dimensional case, so it's not hard to expect that this will go through with optimal condition. Okay, so indeed, it turns out if k is much larger than n to the quarter, then this signed count of triangle strongly separates h0 from h1. Okay, so that's the star result after bound. Okay, so that's a star for afterbound for detection, and for recovery, at least in the regime where P, okay, if PQ are constant, at least in this regime, it's pretty easy to find a statistic. Namely, you just consider the number of common neighbors between vertex one and two. And if the number of common neighbors is large, then you can. Is large, then you can tell that they are more likely neighbors. And here we also need to consider sine count. And this upper bound will become more interesting in the regime where PQ are smaller. So in the sparser case, which I will briefly mention at the end. And because we need to approximate chi, where chi is the indicator that Is the indicator that one and two are close, right? So we need to naturally approximate the thresholding function. And this is not hard to do in the low degree framework because you can approximate a step function by a low degree polynomial. And by combining these two pieces, it's not hard to show that if k is much larger than root n, then you can have a polynomial that is close to the indicator chi in the. The indicator chi in the mean squared error. Okay, so this is an upper bound for recovery. Okay, now let me move on to the lower bounds. So, first, lower bound for detection. So, to prove that there is no degree d polynomial that strongly separates H0 from H1, it turns out you have to control this ratio. So, this ratio is essentially the expectation. Essentially, the expectation of the polynomial under the planted dense cycle model over the square root of second moment of the polynomial under the null model. It's something like the expectation over the square root of the variance. And we take the supremum over low-degree polynomials. And so, as many of you might be familiar with, a trick in A trick in the low-degree polynomial framework is to expand the function in a polynomial basis. And this basis is a Fourier basis, which means that it is orthonormal. So it consists of basis phi s that are orthonormal to each other across indices s, which are subgraphs of the complete graph. And here we are looking at subgraphs. looking at subgraphs with at most d edges. And therefore, this phi s will be a low degree polynomial. And essentially, this quantity is the norm once you expand your function in this polynomial basis. And it turns out this is what controls the hardness for detection. So eventually we just need to Eventually, we just need to bound the sum more explicitly and show that it is a constant. This suffices, I mean, it suffices to show this. So let me now give you the explicit definition of this Fourier basis function phi. So again, S is the subgraph of the config graph. And this phi is simply the product over ij in subgraph S. Okay. And you take the standardized version of the agency's matrix entry AIJ. And therefore, it's a product of some random variables. So the expectation can be viewed as moments. And note that here, the standardization is with respect to the Erdogan graph for G and R, while the expectation is taken with respect to the Planting-Dense cycle model. The planted dense cycle model. And this gives sort of the moments with respect to the planted model. And this determines the difficulty of detection problem. All right, so I'll show you now the recovery lower bound. And it turns out the recovery is determined by some other quantities, which are not the moments. And that's the key difference between detection. That's the key difference between detection and recovery. So let's move on to recovery. So, to prove that the no-degree polynomial achieves a non-trivial error in estimating this indicator that one and two are close, we still consider a ratio. But this time, because we want to find some polynomial that is close to this indicator chi, right? So it is natural to consider sort of a correlation between f and chi. That is our numeral. That is our numerator. And here, the denominator is still some second moments, just for normalization purpose. But the difference is that now we don't have the normal model. We just have the recovery problem in the planted dense cycle model, right? So here the expectation is respect to E1 here, not E0. And yeah, in this previous paper by Slio and Alex. By Slio and Alex, they had a Jensen inequality trick to lower bound this denominator by that of E0 in some sense. And that will lead to this bound. We have to do a modified version of the proof, but the procedure is very similar. So namely, this bound is still a sum of some quantities squared. Quantities squared. Okay, so let me say that this Cs squared is the important part, while that constant you can ignore. It's just some constant depending on P and Q. That's the minor part. All right, so what is this quantity Cs? So I'll try to convince you that it is in some sense like cumulant for the random edges. Okay. So recall that in the So recall that in the detection case, we have sum of squares of some moment-like quantities, right? And here, I claim the key quantity here is the sum of some cumulant-like quantities. And the technical difficulty will be, okay, once you have this general bound, I mean, how do you control this sum? And eventually, we just want to show that the sum is little O of KO, and that turns out to be sufficient. out to be sufficient for proving recovery lower bound. All right, so let me now spend, I guess, a major portion of this talk talking about this cumulative quantity. So first, let me give you a definition. I don't expect you to read this page because the notation is kind of involved, but let me roughly say that, roughly say how this quantity is defined. So first, let's consider First, let's consider a set W of edges. So these are paths ij such that i and j are close. They can put locations close. So basically, we want to recover w from the graph. And now we define some sort of moment-like quantities called mst. It is a probability that t is contained in w, but s minus t is. in w but s minus t is not contained in w here w complement is uh yeah here w to c is the complement okay and once you have this uh well moment like quantities you can iteratively define the quantity c as follows so i will parse this relation later okay but namely this is how you define the cumulant quantities via iterative relation okay Relation. All right, so now let me discuss low-degree lower bounds for recovery and why it is connected to cumulants. And why, in the first place, I call this quantity CS cumulants, right? Okay, so this might be a reminder for you about what are cumulants, or if you're not an expert on this, then let me give you a super crash. A crash cause on cumulence. So it's usually in for probabilists, they usually view moments as the coefficients as a moment generating function and cumulants as a coefficient of the cumulant generating function, which is just log of the moment generating function. All right, so let's differentiate the equation phi equal to e to the Phi equals e to the psi. Psi is the CGF while phi is the MGF. And I differentiate this n times. And using some elementary calculus, if you do this n-times, it's not hard to see that the nth derivative of the MGF is equal to a sum with some binomial coefficient and the product between some derivative n minus i's derivative of the cgf times the ice derivative of mdf okay and Okay. And if you set t to zero, I mean, it's an elementary fact that if you set t to zero for the nth derivative of a generating function, you get this coefficient, right? And therefore, the mth moment is sum of some binomial coefficient times the cumulance times the moment. Okay. And this is a recursive relation between moment. Recursive relation between moments and cumulants, which I find more important than the definition from the generating functions, because this really tells you what's the relation between moments and cumulants. Now let's take this recursive definition. Now I throw away the view of moments and cumulants. I define m and c as some different quantities. So in fact, this relation is well known for probabilistic. Is well known for probabilistics, I think. This is certainly not what I imagine. I'm just showing you some basic facts. But here, the important thing is that let me view Mn as the number of subgraphs of the complete graph, and I view Cn as the number of connected subgraphs of the complete graphs. And why does this relation still hold? It's pretty simple. So if you consider any subgraph of the complete graph, okay. Graph. Let me just draw some graph. And let's consider a vertex called vertex one, maybe here. And let's consider this subgraph maybe disconnected. It has several connected components. All right, if I focus on the connected components containing one, and suppose there are n minus i vertices, and I consider the other part, which contains i vertices. Which contains i vertices, okay, and I count the total number of total number of subgraphs in the following way. I first choose the i vertices which form an arbitrary subgraph, that is this mi. And then I consider the number of connected graphs which contains n minus i vertices that contains vertex one, that is this C n minus i, and there are n minus i. And there are n minus one choose i number ways to choose those i vertices, right? And if I take the sum over i, that gives me the total number of sub graphs of the complete graph. Therefore, if I define mn and c n in this way, this recursive relation still holds. Okay, so now we know that moments and cumulans are sort of related to subgraphs and connected subgraphs. Let me write the recursive relation. Let me write the recursive relation in a slightly different way. Let me call alpha the set of indices one to n and n beta is just number of graphs on beta and the c alpha minus beta is number of connected subgraphs on alpha minus beta. As you can see, I'm just rewriting the above equation in some different way. In particular, the difference is here, the binomial coefficient is naturally incorporated in this. Incorporated in this summation. All right, now let's take this blue equation, this recursive relation. Now let me relate that to the previous cumulance quantity that I showed you. So recall that for a low degree recovery, a difficulty of low degree recovery is controlled by more precisely some S C S squares times some quantity. Some quantity. And this Cs is defined as follows. I have this W which I want to recover. W consists of neighboring pairs. And I define some moment-like quantities, MSD. And rewriting the recursive relation from earlier, I can define this Cs using this equation below here. Below here. So, if you compare this equation to the moment-cumulant recursive relation above, and you view basically this alpha as S and beta as T. So they are similar, right? Well, there are some minor details that are different. For example, there are some extra edge here or extra vertex there, but in principle, these are very similar. These are very similar recursive relations. And it turns out that if we use this recursive relation in our case, we can also prove some nice properties about this Cs that have nice similar behavior to cumulants, I should say. So I might not be able to call this exactly cumulants, but at least this is this C is some cumulant-like quantities. See some cumulant-like quantities. So, hopefully, through this recursive relation argument, I showed you that these key quantities are indeed cumulant-like. Now, let's get back to the recovery problem. So, because this relation is sort of some relation between connected components of subgraphs and just subgraphs, right? So, if I view this. So, if I view this M as the information that T and S contains about the neighboring pairs, W that I want to recover. Then this Cs is kind of the information of connected subgraphs S contains about the set of neighboring pairs. So, this is a bottom line here. It's not a rigorous statement. Here. It's not a rigorous statement, but through this recursive relation, I tend to believe that at a high level, this is the case. But why recovery problem is about connected sub-RAS? Then I can also give you some intuitive argument. So if you consider, say, recovering whether one and two are neighbors. Our neighbors. Then, say if I consider some subgraph like that, okay, this subgraph might contain some information about whether one and two are neighbors. Let's say I draw some subgraph that containing one and two. Okay, now let me consider another subgraph, which might be say the same, except it possibly has another component. Okay, so this. So, this whole thing on the right-hand side is still a subgraph, but it is a disconnected one. So, does the graph on the right-hand side tell me anything more than the graph on the left-hand side about whether one and two are neighbors? Well, probably not, because this triangle is a component disconnected from the main part containing one and two. And therefore, I feel that And therefore, I feel that, yeah, disconnected subgraphs do not offer any actual information about the recovery problem if you only care about recovering whether one and two are neighbors. That's why I think this is a high-level reason why these cumulants quantities show up in the recovery lower bound because, yeah, as I reviewed, cumulants are about connected subrefs. Okay. All right, so let me mention that very recently I realized this might be very related to cluster expansion in statistical mechanics. I think Will is probably also in the audience. So Will is an expert on this and he has some nice lectures on cluster expansions, which I have not really studied yet, but I appreciate that. I appreciate that. Yeah, I'll have a chance to talk to you later about this. Okay, so this is a story about connected subgraphs and cumulants. Now, let me, by the way, any questions or comments on this part? Sorry, is there a question? I cannot hear anything. Oh, hey, can you hear me now? Yes, yes. Yeah, yeah, oh, yeah. All right. For the C sub S, was it the case that those are equal to zero whenever S is disconnected? Yes, yes. Okay, so that's remote. Okay, yes, yes, yes, indeed, indeed, yes. Yeah, that's so we have another question. question yeah hey chen this is om i'm just wondering that this cumulin thing is really cool i'm wondering is is there some is there some generating function thing that can then be defined from these cumulants or is there yeah can you say a little bit more about uh yeah that's a very good question um so unfortunately i have not been able to construct a cumulative generating function that exactly gives this okay yeah i think i i spent the past week trying to construct one Past week trying to construct one, but failed to do so by today. So, yeah, I'm not going to say anything about that. Also, about is there, is there, is there a notion, is there some notion of independence maybe related to what Alex was just asking about? What is the notion of independence that these cumulants capture? Yes, so very, very important question and remark by you too. Okay, so yes, so. So, yes, so as I said, I feel that this subgraph does not contain any extra subgraph about that, right? And the difficulty of recovery is determined by, again, let me write it down, sum over Cs squared. And if this subgraph does not contain actual information, I'd better have this Cs to be zero. So that if I add that subgraph, this sum still remains the same. Okay, and this is related to the Okay, and this is related to the question about independence you asked. Here, you can split the graph so that here you have some the i's. Here on the left-hand side, you have some zi's, and on the right-hand side, you have some zj's. And as long as this zi's and the zj's are independent, then the corresponding Then the corresponding cumulant will be zero. So, does that answer your question? Oh, thanks. Yeah. So if I have a generating function formulation that gives this recursive relation, this will immediately follow from that. The reason is that the log CGF is like log expectation e to tx. Okay, this is a CGF for. A CGF for the random variable X, right? And if, say, X and Y are independent, then you immediately say that you can have cumulants of X plus Y that are sums of cumulants of X Y and cumulants of Y. And that will immediately give you cumulants is zero. I mean, that's not immediate. That's probably not immediate, but with some simple argument, you will see that once you can split the two parts into two independent sets, then your community will be zero. But here we can still prove this fact via the recursive relation, but I don't have the correct nice proof that directly shows that cumulants are zero for independent sets of latent variables through the generating function. The generating function, and that's a slightly unsatisfying part that I'm still thinking about. Okay, again, very nice comments and questions. Okay, so if there are no more questions, let me quickly talk about the extension and summarize our results. So, first extension. We not only consider the case where PQ are constant, but we can also consider the case where PQ Where PQ scales as n to some negative power. So in this case, the detection upper and lower bounds basically carry through. And now the threshold is not k is equal to n to a quarter, but with some factor involving the density here, right? This is not hard to imagine. If the graph is sparser, then you need more signal. Then you need more signals to detect between H0 and H1. And therefore, as P gets smaller, you need a wider band. And for recovery lower bound, there's still no problem. Basically, you can show K is roughly equal to root N, but now you need to divide by some power in P. So the difficulty lies in relation. So, the difficulty lies in recovery upper bound. And here we cannot count common neighbors because, as a graph gets sparser, there are less neighbors. So, presumably, neighbors contain less information, right? So, here we need to consider longer paths. Say, if we want to tell if two nodes are neighbors, we count paths of length L between them. And this, if you want to achieve this if you want to achieve the matching upper bound with some slack epsilon you need to consider longer and longer paths where the length depends on this epsilon and this kind of algorithm and phenomena already appeared in the literature in this paper by hopkins and storage on community detection okay so another extension Another extension possible is that we don't necessarily focus on the case where the latent locations are distributed in a cycle, right? So this is S1. You can assume, say, the latent locations are uniformly distributed in S D. And the most arguments I use here carry through to this case. But if we assume that this D is finite, okay, so. Okay, so at least in the definite case, much of the theory carries over. So, in fact, these extensions are the primary reason why we have not really posted the paper yet. So, one of my students is working on some extensions and we hope to finish this as soon as possible. But yeah, if you want to read the lower bounds, yeah, please send me an email. I'm happy to share. Send me an email, and I'm happy to share a preliminary draft of the paper. Okay, so let me summarize. So, I talked about this planted dense cycle problem, which is kind of a one-dimensional random geometric graph. And I focus on the low-degree polynomial algorithms. And there's a detection-recovery gap for some intuitive reason, and detection is related to moments of the graph, and recovery is related to cumulus. And the recovery is related to cumulants on the graph. And that is all I want to say. I thank you. So I'm happy to answer any further questions. Any questions? So, I guess there's no more questions from the audience. So, let's thank again for the nice talk. Thank you.