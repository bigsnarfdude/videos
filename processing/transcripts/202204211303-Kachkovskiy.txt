And this talk also. Okay, so I'll be talking about quasi-parametric potentials. Let me write some general form. So it's going to be operators on L2 of Zni, which multiply this piece of Lausanne plus a multiplication by potential who is obtained from By potentials, we obtained from some function using the trajectory of rational rotations. So, that's possible. Okay, so I'll put epsilon here, because I will consider the regime of Large Disorder, and for some calculation it's more natural to consider small complicated Laplace, which is, of course, the same thing. Okay, so F, so what is F? So, I mentioned. So, I mentioned monotone, and we'll be talking about the special applies of potentials, which is called, which we call sometimes Maryland type. So, they obtained from an L-bounded function originally defined on 0, 1. Okay, so then you extend it periodically into the real line, except for integer points. And the function looks like, so for most of the talk, I'll assume two things. First, the function has to be strictly monotone, k is f continuous, zero. And it's strictly monotone, so I'll write it in the sense that the derivative is bounded from below by some position. If these bounded from the lower suppositive transport. So it doesn't really assume some places that f is differentiable, but the bug has to be satisfied for the lower derivative. So it's like a lower Lipschitz property. And it goes sort of from minus int to plus. So the limit 0 plus plus negative infinity negative 1 minus plus infinity. So if you look at the graph of the function 0, 1 and then just repeats itself. Okay, so what is Amiga? Amiga is a here. Is a frequency vector with rationally independent components, and for most of the talk, we'll assume that omega is a function. So distance n omega to integers is bounded by some constant n to some power, or n is not zero. Okay, so and whenever, sorry, is there any problem? Oh, that's window. Only you guys see that with the microphone. You can't control it. But the zoom people don't see it. Okay, I'll just keep talking. Alright, so omega is definitely. Okay, so let me make some convention for simplicity: that whenever I say that some constant depends on omega, it will depend actually on the definition constants here. And tau is. Constants here. And to is some fixed number, let's say, equal to d plus 1, something that is. Alright. So potentials of this type behave differently from regular analytic potentials. So let me review a little bit what's known about spectra of these operators. Let me start, okay, so from the actual Meridian model. So in Marial model, f equals to, okay, so in this statement is tangent pi x plus 1 half. If you want to simulate this at 0 and 1. Okay, so and this is a classical model described in Saxon Fraser Krishna Sandbook. So there are several Second Frisik-She-Sandbook. So, there are several classes of results. So, the most interesting property of this model is that it has purely point spectrum for all capitals. So, usually if you're talking about analytic potentials, you have large capital and small capital regime, where you can have very different behavior. But here, middle model, you have to replace that. So, if it's established by a series of people, I'd say for a huge market. Branch Simon Figurin There are some more names, but I'm just naming the authors of the most well-known mathematical papers. So point spectrum for all epsilon greater than zero. Epsilon greater than zero, all deference and frequencies in almost every phase. Okay, so if dimension is equal to one, oh, yeah, by the way, this is basically the only known non-perturbative localization result which holds in higher dimensions. I don't do anything else. And whether something like this holds for Merrillian type potentials in higher dimensions, in my opinion, is actually a very interesting open question. So d equal to 1. There are more precise results, so I think I'll mention just to understand more. So I think I'll mention the Mistean Lier. Lero. So basically who gave a complete spectral description of the Merrillin model for t equal to 1 for all values of couplings, frequencies and phases. There are many types of spectrum you can have. Okay, so there are some more papers, but I'll don't want to give like comment. Don't want to give like a 100% complete review. So now, what's known about Merrill and Tech? Let's see, I guess I can do it here. Merriman type. And again, so I'll mention two results. So the first one is the paper by Bellessard Immens Cornhole. So they considered the case of functions of this type, which admit a meromorphic extension neighborhood of real light. So basically they're meromorphic. So there is some additional. And they showed that they also have point spectrum. So let's say F Meromorphic. Pure constructor for epsilon sufficiently small, depending on F and on the frequency. And by dependent on frequency, I mean depends on these different constants. So in the proof is done, but when you're saying the neural nine with regards to decorative. And five with regards to their area, so it's under the same assumptions. Yes, so basically the same assumptions plus f is Meromorphic. Yes, so yeah. So Meromorphic functions, for example, if you know that the derivative is strictly greater than zero, you can write a lower bound because you cannot approach zero here. Here it's just it's kind of yeah, a regular assumption. They made it, they did it using a sequence of KM-type iterations. A sequence of KM-type iterations, which is actually very similar to the start from Jean's talk. Basically, a series of procedures described by Jean. They show that it converges and gives you a demonization of the group. So, in this equal to one, I'll mention our result, which is obtained by completely different methods, but basically, But basically, if F satisfies this condition, okay, so there is a result by me, which is based on earlier method by our joint with Gustav Scan. Yeah, so basically d equal to one, f satisfies this property. F satisfies these properties and integral of log absolute value of f from 0 to 1 is bounded. Then you can actually have localization for almost every phase for all epsilon greater than zero. So this is another example. So this is another example of small coupling coefficients, but nothing anywhere near this is available in higher dimensions. And this talk will not make it any better, so it's still open question. So but in one dimension, there is a kind of a complete pitch. There are several papers about monomorphic one-dimensional cases where you don't necessarily require the monotonicity. So I can if you need, I can give you some more references. Okay. Okay, so if you are sorry, a quick question that is how the limit scopola is, the coupling constant has to be large, right? Because in the... Well, index notation has to be small. Well, yeah, okay. Epsilon has to be small, because in the Neiland model, the the the Apple exponent is positive for all coupling constants. Yes. Generalization is not necessary. Is not necessary? Okay, so in one dimension, it's actually positive for almost all energies under these assumptions. In the regular Merlot model, I think it's positive for all energies. Well, definitely it's positive for almost every energy. Can you did it answer the question or something? I also asked a question. Yeah. So can you? So the paddies and their their work also repair monotonicity? Yes. Yes, they need mountainicity. Basically if the moticity fails, they have resonances and everything fails. So basically they're method based on the fact that there are no resonances. So can you give us an example where the function is a mirror motley and one is what it's like? Monas, minus what is that? Well, basically, f is a restriction on the real line of some function that's meromorphic in the neighborhood of real line and one periodic, and its graph has this structure. I'm not sure. Can we rephrase the question? So, is there any explicit function that? Yeah, templates. This one, for example. Yeah, except except. This one, for example. Yeah, take steps. Take steps for depending on. Well, you can add, for example, some, I don't know, plus epsilon sine y x for epsilon small. So the derivative doesn't yeah, okay. So the goal of this talk is to discuss mostly Discuss mostly a different method of treating apparatus of this type, and I will focus more on the method. So it's a joint work with Parnovsky-Stremberg, the first paper, and the second paper joint with Parnovsky-Stremen and Krimski. Yes, and so I'll mostly focus on the method, but let me explain the differences of what we can obtain. Of what we can obtain. So, our method will also work in the perturbative regime. So, epsilon will be less than something dependent on the Huntington constant. Okay, so there are some regularity conditions on F, so which I will describe in the end of the talk. Describe in the end of the talk with effect time, because it's going to be more initial, but they depend only on the third derivatives of f. So conditions of f only use first derivatives. So, second thing, we are able to analyze individual eigenfunctions. Eigen functions. In particular, we can consider functions which are locally Maryland type in the following set. So, like, for example, if you have a feature like this, if you try to use this method, it fails to provide complete legalization because you have to treat all the graph at once. So, our method is applicable to energy window, and for functions of this type, we can obtain localization. So now we are in dimensional hierarchy, one? Well, it's also applicable in dimension one. Well, it's also a tickable dimension one, but yeah, yes, it also both. So f is still a function of one variable, so it's like one frequency setting, one dimensional torus, but the lattice is d-dimensional. Okay, and the second, probably the most interesting case that we can cover is when F has a flat segment. So, and there are some restrictions on the geometry of the location of the segment, relation between the free. A relation between the frequency vector of a m. So flat segments, so additional geometric restrictions. Okay, so and the method actually, for those who haven't seen it before, looks at For those who haven't seen it before, it looks very naive. We just try to write down the eigenfunction in one of the most naive ways, and it turns out that we actually get an eigenfunction. Okay, so any questions? So basically right now, I'm going to start talking about this method. And the first part will work for any dispute in the required. So I will forget about quasi-bird structure for probably 10 to 15 minutes, hopefully. So if you have any questions about this, before I Have any questions about this before I go to the next section? Okay, so I'll erase all this. So So this part of the talk is to some extent it's expository because relation perturbation theories is something that's known probably for at least 100 years already. But the structure of the terms that we need is actually not very easy to find. It's actually not very easy to find. So, there are some physics papers who discuss it, and I think there is also a paper by Arnott, who studied gaps in the spectrum of the continuous Maitrey operator. So, he used different representations for the same terms. Okay, so what's related to the series? So, suppose you have a Schrodinger patron on a tunnel Z. So H equals epsilon delta plus V. V is just multiplication by sequence V. Okay, so suppose that V0 is an isolated value of the potential. Then it's well known that there is this. It's well known that there is the resistance analytic branch. I mean, okay, so if you take epsilon equal to zero, then there is an obvious eigenvector, just delta function supported at the origin, right? And if you make epsilon small, this eigenvalue will analytically, will depend analytically on epsilon. So in this case, V0 is isolated. So obviously, in our case, it will not happen. The sequence of all this is going to be dense. But let's assume a weaker condition. Let's assume that vn is not equal to v0 for n not equal to 0. So then turns out, then we can basically do the same procedure as what we had here formally. So all the terms of the coefficients of the analytic expansion of the eigenvector will make sense. Because, I mean, you'll see from the calculation. I mean, you'll see from the calculation. So, you can write a formal expansion for eigenvalue and eigenvector, which starting from the zero-based vector, at epsilon go to zero, which is going to be some series on epsilon. And you can write down explicitly and uniquely every coefficient of the series. And then the big question is whether it's going to converge or not. Most likely for non-asolate activas, it's not going to converge. So, it's not going to converge, but turns out that in this setting, it actually will converge. Okay, so let me explain this series in more detail. So, let me do it here. Yeah, so this way you can construct the one-eigen vector, which is kind of comes from the zero-lattice entry. Lattice entry, but if we have a more general condition, like all the values of V are different, we can do it with any lattice point and produce a formal series for a countable family of eigenvectors. And then the claim basically is that this series under some regulatory conditions on F will converge and produce the accommodation of that vector. So let's do some calculations. Okay, so we look for an eigenvector, solution of the eigenvalue equation. Solution of the eigenvalue equation, epsi plus epsilon, where and let's say equal to equal e0 plus epsilon e1, so on, psi psi 0 plus epsilon psi 1, and so on. Alright, and let's write down this equation basically in this series. So we've got V cos epsilon delta. X of Psi 1 Psi2 plus equal P0 plus epsilon P1 psi 0 epsilon psi 1 psi psi 1. Okay, so this is our equation. And what we're going to do is we can just formally expand it in the Can just formally expand it in the powers of epsilon. You'll get finitely many terms at each power of epsilon. So you can write down an infinite system of equations. You just solve these equations one by one, starting from the first point. So the solution, if you don't impose anything, additional solution will not be unique. Because of basically of some normalization choices of what executive consider what executive considerable. So let's assume the following choices. So let's say E0. So let's say E0 equal to V0. And actually, let's assume that V0 is equal to 0. Just let's shift everything into the origin. This will be true. So later, so you'll see there will be a lot of factors of the form negative Vn inverse. After you go to the final result, don't forget to change it into. Don't forget to change it into V0 minus VM. Okay, so and then psi 0 will be the unperturbed eigenfunction of psi 0 is just the 0 of the basis element of L2 of dt, so easier. And this is not enough because we need some kind of normalization. So let's assume also that all the corrections are orthogonal to size zero. Orthogonal to sine zero. It basically shows how to normalize the whole vector. So you can normalize the whole thing in L2, which complicates some calculations. Or you can just normalize it so that the first entry is equal to one, always. That's more simple. From sine g, I'm talking about sine zero for every neighborhood. Okay, so now you can write down system equations and solve it. So I'm going to solve the first equation. The first three. And let me give you some general features. So, yeah, I'm gonna start here probably. I'm gonna denounce this stuff. Okay. So epsilon to the power zero, we don't get anything interesting, we just get our initial conditions. So Get our initial condition. So basically, E0 equals 0. H1 to the power 1. So what happens in the first order of perturbation? So let's write this as, okay, so what do we get? We get V psi 1 plus plus delta of psi 0 equals, so we have E 0 of psi 1 times have E0 of psi one times E1 psi 0. It's the equation of the first term. So this term disappears because E0 is equal to 0. So now let's when we solve this equation we do two things. We first project the system onto the zeroth phase vector To get something scalar, and then we solve the remaining equation. Let's first project it into E0. So, what's going to happen? Remember, psi 1 is orthogonal to E0. P Psi 1 is also orthogonal to E0, so this disappears. What happens with delta psi 0? Delta ps0 supports the origin. If apply delta, it spreads out of the origin, nothing remains in the origin. So left-hand side is zero. The right-hand side is E1. So we just get in this part that E1 is equal to 0, which kind of makes sense if you do some perturbation theory, that usually the first correction of the energy is in the second quarter. So now let's project it to our E0 alternative. And in this case, okay, so what's going to happen? Then, this is going to disappear. We get V Psi 1. We get V Psi 1 equal negative delta psi 0. So let's make it. Okay, so and then I want to divide by V. I want to invert the parater v. The parator v is not invertible because of this condition. But we assume that zero does not appear anywhere else. So it's invertible if you're sticking to the If you restrict it to the orthogonal complement of V0. So let's assume that V inverse is equal to Em equals Dm inverse Em, or M is not zero, and zero otherwise. So it's basically restricted inverse, which just kills the part on the On this space. So, in this way, you can solve for psi 2 by psi 1. It's negative v inverse delta psi 0. You can actually calculate it. So, let's, for simplicity, for calculations, let's assume that dimensions equal to 1. So, and you can just believe me that most of these things extend. So, I give equal 1. So, what's delta psi 0? Delta psi 0 is just a sum of e1. is just the sum of e1 plus e minus 1. So e1 plus e minus 1. And then you have to divide by elements of v so negative v1 inverse v1 negative v negative 1 inverse e negative 1. So this is the solution for psi 1. Okay, so let's do one more order. Epsilon square. If you have questions please stop. Okay, so we have to do this and this here, and here we have to do this, this, and this. Okay, so V Psi 2 plus delta epsi 1 equals V0 of Psi 2 plus E1 Psi 1 plus E2 psi 0. Okay, so here we have many terms that vanish too from the previous calculations. And let's again do the same drill, let's project to E0 first. So E0, what do we get? Well, here we get 0, right? Because psi 2 is our top to psi 0. Here we get something. So, and here we get it too. So if we get project 2 0, we get. get project to E0, we get E2 is basically delta psi 1 E0. I assume that everything is real, so I don't particularly careful about it in the product or here. Okay, so basically and then you can do this calculation and see that it's basically negative 1 over v1 minus 1 over v minus 1. Minus one over. So if you remember this convention, this formula may become more familiar if you saw perturbation theory before. Okay, so basically the conclusion P2 is obtained from psi on the previous step by applying delta and kind of collecting everything that comes back to the portion. So let's now do psi 2. Yeah. Yeah, here. Yeah, here. It's a multiplication operator. N is as an integer, as it did, originally. So yeah, V sub minus 1 is the matrix element of V. Yeah. Oh, bottom, yeah. Matrix element of V, yes. Okay, so let's do a cycle case. I'm gonna probably continue here. Here side two equals okay, so we move delta to the right-hand side. Okay, so we at least in this case we're gonna get negative errors delta of psi one. Delta of psi 1, which you can also write as by using the formula sub psi 1, another negative v inverse delta of psi 0 delta negative inverse delta psi 0. Okay, so now you can see some structure, which is a little bit inaccurate, but I think it's actually important to see it. So basically So basically, what happens is that, at least on this level, we start from the origin, we apply delta, which means to spread out from the origin, and we apply negative inverse, so each component gets like a factor. Then we apply delta again, so we travel again, apply factor, and so on. So basically, we travel over the lattice. Each time we make a step, we gain a factor. And then, to get to the eigenvalue, we have to come back to the lattice. In value F, we have to come back to the origin. So basically, on this level, you might think that the sum of the eigenvalue terms will be just some kind of sum over all paths that start and end at the origin, over some factors. Okay, so this is not completely accurate, but you cannot see it in this order. But I'll tell you in detail what's going to happen next. Okay, and if you look at some guy in a convective, you just look at all paths, but you don't consider paths at good origin. Can't consider path that go to origin because origin is always normalized to be one. Okay, so what happens in general? Let's write down the equation for kind of order. Okay, so beeps I am. P psi n plus delta of psi minus 1 equals the sum. So we get E0 psi n E1 psi plus E1 n minus 1 plus E2 psi n minus 2 plus E n of psi 0. So this is our equation. So, this is our equation. Well, you can see that these two terms do not exist. And in fact, you can change it in the setting all even terms, all odd terms are going to disappear, kind of because the path to get the origin has to make even number of steps. Okay, so let's solve it. Let's solve first like we did with E0 projection. If we project it onto E0, then actually most of the things are not. Actually, most of the things do not survive. This does not survive here. Nothing survives except this. So basically, we just get E n equal to delta psi n minus 1 projection to origin. Which kind of makes sense. So, like we considered before. So, to get nth contribution to the energy, you need to take the contribution to the eigenvector, make one more step, and More step and collect terms that get back into origin. So now let's look at the interesting part. Yes, so the reason why it's not so interesting, it basically means that the interesting part was calculated the previous. So let's do it now. So psi n is okay, so I'm gonna put like because I want to keep minuses at v to the structure, I'm gonna move this to the right. I'm going to move this to the right and everything else to the left. So we get negative v inverse. Okay, so now what do we get in the parentheses? So we get delta of sine minus 1 minus, let's say, E2 psi n minus 2 minus E4 psi n minus 4 minus e so on. Basically those possible terms until we run out of. Possible terms until we run out of indexes. Yeah, I just made a. Yes, unfortunately, at some point I also use m to denote lattice vector. And here m is just the order of perturbation theory. This is a bit of an oversight. Please check me because it's not obvious from the construction. Okay, so basically, then we see that there is actually more structures. So to get the formula for EN, we need to go to ψ n of the previous. We need to go to psi m in the previous step, and then we need to expand this by recursion. And if we always use this term when you expand over recursion, then you get what was the original guess, like the sum of all paths that start from the origin, come back to the origin, self-avoiding. And for each path, we have to multiply like these kind of vertex factors as we travel over that path. So, but I'm pushing there are also these tricks, which is actually going to be included. These turks, which is actually going to be a good thing. And the way to, yes, let me draw some pictures so that I won't completely explain what it means, I think. So let's consider the situation. Okay, so when we expand this, basically, every time we do recursion, we have to make a chips. We have to decide if. We have to make a choice. We have to decide if we are using this term or these terms. And then, once you use induction assumptions of these terms, again, you have a choice. You have like a tree of choices. Suppose that we consider in the second simplest case, we're always making this choice, except one time when we make, let's say, this choice. But then after expanding further, you always do this. So what does it mean? It means basically we were following the original procedure, like travel around. Original procedure like travel along the path, multiply these factors. But at some point, what happened is that we did not apply delta, but instead we made another factor of the eigenvalue form. So graphically, you can represent it like this. So we were usually we represent paths by loops, by kind of graphs, with loops, and on the verses we indicate which We indicate which lattice points we need. So for example, 0, 1, 2, 3, 4, 3, 2, 1, 0. So, for example, this is a representation of 1, 2, 3, 4, 5, 6, 7, 8 of the 8th order perturbation term where we only used this. Where we only used these things. So to this picture, we associate a product of V inverse 1 per file. So here is going to be, for example, negative V1 squared, negative V2 squared, negative V3 squared, negative V4. Everything in vertices should be. So there are seven vertices. Seven times we have to do this. So and three of them So, and three of them were done on the way there and way back. Here we're looking at one-dimensional case for instance. Okay, so if we do this, so the way to represent it is that at some point we actually kind of stopped and started a new path. Sample 0, 1, 2, 1, 0. So this path corresponds to this factor and And basically, it means that we have independent contribution of this loop, plus some extra factor which comes from the fact that we stopped. And that extra factor is because we have a minus here and we do not have a delta. So basically, we have negative of the vertical factor twice. So basically, contribution of so this picture corresponds to the term So, this picture corresponds to a term of order 12, and the formula is like this. So, we get this times the contribution of this thing, so which is negative phi1 squared, negative squared, negative v2, plus additional factor from the time we apply the inverse to this thing. So it's gonna be, in this case, three inverse. Okay, so and then you can get some because the reparsion can be done in more complicated ways, you actually kind of get you can attach more loops here, you get kind of a tree of loops, and you can basically write some rules how you allow to draw pictures and which term corresponds to which picture. Okay, so there is a way to do it completely, it just takes more time. More tech. Any questions? Now I'm going to talk closer about convergence. So this whole sequence is actually already containing this second loop. Yes, so this part... The beginning. Yes, this part is from this loop. This part. So there are three factors. From this loop. And this factor is because of the attachment, the procedure. Yeah, the bridge, yeah. Yeah, the bridge, yeah. And you say that the beach next to have many bridges with yeah, basically we took to consider perturbation all terms of order and basically consider all the configurations of all possible loops with attachments with total number of factors equal to the order of perturbation. So you can, for this term, for example, you can do like the whole loop like 12, or you do a loop of length 8 and attach a loop of length 4. Or you can do like, I don't know, something like 4. Or you can do like, I don't know, something like four, four. Like any sum, other old possibilities. Yes, yes. There are finitely many of them, like at most four to the end in this case. But there is something depending on here. Okay, so let's talk about convergence. I have about 15 minutes, right? Yes. Okay. Convergence. So now let's remember that all these vertex factors remember that all these vertex factors actually v0 minus v r so it's going to be easier to understand we shift everything to the origin from the beginning so and if you look at the quasi-theoretic setting it's actually what it's actually f of x minus f of x plus n omega inverse okay so I'm done right okay so let's get back Right, okay, so let's get back. So if V0 was isolated taking well f, then it's very easy to show that this thing converges because all the denominators can be bounded from below. The number of terms is bounded by something in the power n, so you take small epsilon, everything is good. So if this here is not isolate, then the situation is more difficult. So there are two possible, well, we basically get small denominators. We get the m. Basically, we get small denominators. We get the n approach zero, arbitrarily close. Right? So, and in the quasi-project case, there are two situations when this can happen. So, small denominators. So, we cannot distinguish them, although the second part is a particular piece of first part, really. So, if n omega is small, Then we will have a small denominator, regardless of f. There is no way to get away from it. So, but the second part is possible to get away with some classes. So, resonance. So, n omega is close to some point y, where f of y is equal to f of x. Like what happens to in the cosine. So, basically, we came. So basically, we came back to this thing being small, but not because an omega got close to zero, but because x plus an omega got close to some other branch of the function. And the reason why I consider momentum potentials is that at this moment we know how to deal with this, but don't know how to deal with this. So now, how to deal with small denims? Let me explain basically how small dominators appear again and why they actually do not make our life really better. First, if you have this type of problems, it's easy to see that if you just consider individual terms for each loop, they will diverge in absolute value. Just because there is only hope. There is only a hope to get convergence if you group all terms at fixed power of epsilon together. So, and then the goal is to kind of find some cancellations between different configurations of groups. Let me show you how these cancellations appear. And to understand this, let's try to imagine the kind of the worst possible situation. What will happen that we don't like? Suppose we start from the origin. Suppose we start from the origin and travel, I know I'm gonna write it in a hand-grading way, some point n omega close to zero. Right? Well, fractional point, or these two integers. So if you just go there and immediately go back, this is actually not a deal not a big deal, because uh if an amicus close to zero, then the previous steps are not going to be close to zero if amicus has some defensive property. Zero if Amigos has some defensive properties. So, and because Amiga is defensive, if you have just one time you visit that and Omiga, it's not a problem because it's going to be dominated by the part of the past that's kind of regular. Same thing applies if we have two different places for go to say m omega. Then it's also okay, so because between this and this, we also have to make many steps where we don't have. Many steps where we don't have small parameters. What is the problem? The problem is that we can get to an omega, then make just one step, and then immediately come back. So for example, if n omega is over, is like epsilon to the 100, so what's going to happen? We come to an omega, I'm sorry about this placement. We come to an omega between. Placement. We come to an omega, we begin x of the hundred in the denominator. Then we make one step and come back. So we begin epsilon squared. But then we get another epsilon of the hundred determinant. This thing doesn't behave well. So this is basically the main type of situation where the problems happen. So now how to solve it? Well basically we just need to find some cancellations. So how this thing cancels with something different. Uh different. And uh the idea is the following. Whenever we have a segment of a path which has this bad situation, basically replace it with kind of an attachment loop which kind of mirrors the original one. So let me give a more concrete example. So this is probably I don't need So suppose we have some clusters of origin of the point n, n plus 1, n, n plus 1, n, and then goes back. So this part is kind of safe, harmless. So then, but then we go here and come back to n, and then go here and come back to n. So basically, if n is kind of safe. To n. So basically, if n is kind of bad, it might happen that we cannot survive, like we can survive one time and a period, but we might not be able to survive three times. And if we can survive three times, we might just survive 100 times. So, because each time we do this thing, the situation gets worse. So, and then the idea is that you can observe that you can this thing, for example, you can combine it with a path. Combine it with a path that goes to M. Okay, so here we repeat on plus one m and then goes to zero. Okay, so basically, and this part we replaced by a dashed path. Zero, one, zero. So you can check that this path and this path are in the same order of perturbation. Yeah. Bath and the same order of perturbation. Yeah? Question? Five minutes? Or ten minutes? You say five minutes or ten minutes? Five five minutes. Okay, yeah, so basically, and we can also do it with this piece and also with their combination. There are four different configurations in the equivalence class that you want to consider. Let's just look at this thing. Yeah, and all that is just saying that it's determined by conditional fashion to determine the. Uh say again? How you come uh I mean why these situations appear? Because the in the perturbation series we have to consider all paths, including this one, right? So this path definitely will appear as a legitimate configuration. Yeah, definitely property has to do with the fact that this part is will also be harmless. But it doesn't save us from from from this this loss. But if you consider this thing and this together, But if you consider this thing and this together, let's look at the actual difference in the contribution. So here, okay, so we get vertex factor f of x minus f of x plus. Okay, so let's just look at the part that's different here. So we have f x minus x plus x plus n plus 1 omega Let me do it actually through the end. It's basically a bit more simple. So we get V0 minus Vn V0 minus Vn plus 1. So I'm looking f for these two things and here they become Okay, so what are we going to be covering? So we're going to get V0 minus V1 times again V0 minus Vm. So what happens? Because of this thing, instead of M plus one, we have one and we still have V0 minus Vm. Here we have it because we visit this vertex. Here we have it because of the attachment. Here we have because of the attachment. And the attachment makes it the opposite side. So we can factor it out. We can factor out V0 minus V. And what do we get? So let me finish this calculation and then make some points. So we get, okay, so V0 minus Vn in the denominator. And let's subtract these two fractions. I'm going to do it in my head. V0 minus Vn plus 4 minus V0 minus VR. V0 minus Vn plus 1 minus V0 minus V1. So can I get what? Vn plus 1 minus V1. Yes, and we're going to get also this product of these two things, right? V0 minus mu and plus 1. V0 I'm sorry about the synonym, but these terms are not small. This one we don't care. This is small. But what is this? It's basically we are looking at the region. Okay, let's transfer it back into f. So we get f of x plus omega minus f of x plus omega plus l omega. Plus n omega. n omega is small, right? And at the point x plus omega, f has a derivative. Like, we assume some kind of regularity from above, which means we can kind of combine it by derivative of f times the same denominator, and then it's going to cancel. And like an advanced version of the procedure allows to treat monotone potentials with some regulatory conditions, basically a requirement. Was basically a requirement that the durative doesn't behave very properly. Okay, so I'm done probably exactly the time, right? Okay. Are there any questions? Can we imagine somehow to change at the very beginning the perturbation point? Yeah, so I also have to expand and cancel. To expand and cancel, so in some ways they're automatically like sometimes called like self-energy or something like that. Yeah, so I think there is some possibility to do it, but I think if I understand what you mean, so basically in the end the coefficients gonna become epsilon dependent. So you have to. What do you have to with your preservation point? Rotivation point, you maybe change cleverly change perturbation point. Yeah, so this is possible to do if you do it like in multiple steps. Yeah, so you can do kind of one step when you do it on the first order, like John did, to destroy some singularities, and then maybe do it more steps, or maybe just apply this. So, this, the nice thing about this is that you do the whole series in one step. Yeah. I have a quick thing. I have a good question. So, what is at the end that we can prove in the middle of the yes? I can prove that. Okay, so basically we can get a new proof of the result by Le Sartre. So, with less regularity conditions and with dimension one. No, the thing in dimension. Yeah, so basically, the results that it's kind of It's kind of most of the stuff I explained here is kind of how to is about the method. So it's basically a new approach to almost the same thing setting in the delay set. But then realize it works, for example, for locally magnal type potentials like this. And there is some additional steps you can do to treat potentials with flatnesses. So you generalize a bit the desire and also change. The beat that is already and also change the red load. Yes. And the method is, in some cases, the method is more flexible because instead of diagnosing the whole operator, you can deal with individual eigenvectors, individual ranges of energies, and you can kind of fine-tune it depending on how f looks like around the point where we're working right now. Is there any other setting in which you can apply your rhetoric? Well, the raw, but it's kind of working programming. The raw, but it's kind of work in progress. So, this is currently the main thing that we know. I'm talking about completely different. So, if outside of this tangent, so is there any other type of potential that we can treat with? Yes, so with some additional modifications, so it works, for example, for bounded quantum cases, like this. But there are some problems. So, for example, here if you do it just like this, Just like this, you have some eigenvectors, but you don't get a complete system. There are some initial points where this thing can diverge, basically, because of the numerators. But then you can do some additional things to make it convergent, but then it becomes kind of more close to the moderator each app. Are there any other questions? People at Zoom? Okay, if not, let's thank Kyrie again. We have two three minutes break and then the next figure will be handed everything.