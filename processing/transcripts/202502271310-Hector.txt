Tell you more about kind of what we can do and a little bit less about how it works. And so I'm hoping that there's going to be not too much math, but of course, there are a lot more details and a lot of stuff I'm going to sweep under the rug. I have some preprints that I'm happy to share. I'm also happy to answer questions at the end or to answer questions via email, especially if you're not there. All right. So I'm going to be talking about how to model massive physical activity monitor data. And the particular lens that I'm going to be using is through distributed. Is through distributed inference. I'm actually going to skip my first few slides here because there were just some fantastic introductions to the NHANES data yesterday. So thank you, Hotang and John, for the really great introductions. And I'm just going to say here that I'm looking at the early waves of the NHAINS data. And the primary reason for this is just that at the time, there was this really nice paper by LaRue et al. that my student and I found really helpful to kind of get us started with these data. To get us started with these data. Now that I have learned yesterday during the great tutorial, I might actually start with the more recent waves of NFAN's data. So most of you are probably really familiar with how these data might look. So this is just a line of study participant. I've just plotted all of the activity counts over the course of an entire week. So the NHANES protocol asked participants to wear this hip-borne device for an entire week and they were told For an entire week, and they were told to only remove the device when they were sleeping or when they were going to be exposed to water, so if they were going to go swimming, if they were going to take a shower. And so, there's a couple features here that will hopefully immediately jump out to you. The first is that there's a lot of data. So we have minute-by-minute activity counts for the course of an entire week. So that's on the order of 10,000 outcomes counts. So the primary challenge here that we're going to be dealing with is just kind of the size. Dealing with is just kind of the size of the data, and in particular, how it relates to my second point, which is the fact that these data are correlated over time. So, how physically active somebody is one minute is pretty highly correlated with how physically active they are the very next minute. And so if we want to model these data as outcome data, we have to think about the correlation over time, and in particularly because of the size of the data. Particularly because of the size of the data, we're going to have to think about a large correlation matrix on the order of about 10,000 by 10,000. And so that's going to be both computationally challenging to have to deal with, in particular, if we have to use this correlation matrix, we typically have to invert it multiple times throughout any kind of iterative optimization. And then we also have to specify some kind of reasonable model for this really large correlation matrix, and that can be pretty tough. Matrix and that can be pretty challenging. And then the third thing that I'll point out, which has kind of been talked about yesterday already, is that there are some nice patterns of diurnal and nocturnal physical activity. So at night, people take the device off. They're presumably sleeping. There's no physical activity counts. But then throughout the day, we kind of see an increase in physical activity followed by a decrease in physical activity. And the activity is not the same across all days, although it does. Cross always, although it does tend to follow some pretty similar. So, the first approach that I'm going to talk about, I'm going to call this take one, and it will become clear why as I move to take two. So, in take one, I'm going to tell you about our first approach, and then at the end, I'm going to point out some of the things that we've learned, in particular, some of the limitations. And then we'll talk about take two, which is hopefully an improvement. So, in take one, we looked at a subset of about 3,000. Set of about 3,000 participants, and these were almost fully observed participants. And I will come back to this first point on my next slide. So, all I'll say for now is that these are almost fully observed. The goal was to carry out inference on the association between BMI and how participants move throughout the day through what is essentially a regression model with this time-varying association here, beta. So, you'll notice here that I'm really thinking of the activity counts as the outcome and the BMI. As the outcome and the BMI as the predictor of interest, and that I'm interested in modeling how the physical activity counts change over the course of the week as a function of BMI through this beta of t, and that my other covariates here, I'm assuming, have these fixed effects, these eta frames. And so this is a pretty strong assumption. We will return to this model and try to improve it in take two. But for now, this is kind of what we're looking at and the regression coefficient. And the regression coefficient here beta of t, this is where we're really going to be spending most of our time to try to estimate, and then more importantly, to try to carry out inference for it. So what we would like to do is to be able to say what kind of construct some maybe point-wise confidence intervals around the, you know, some estimate of beta t that, in particular, are appropriately calibrated for the particular, for example, nominal level that we want to achieve. So if I want to construct a 95% confidence interval, I want to make Confidence interval, I want to make sure that my coverage rates are approximately 95%. And so, this is where it's going to become really important to account for the correlation in the activities across. So, I'm going to propose a fairly simple polynomial regression, a polynomial basis expansion here. And so, this is for time in some intervals. I'm going to model my beta of t as a linear combination of these polynomials of order k. Polynomials of order k. And then this gamma j is really the parameter that I'm primarily interested in estimating and carrying out inference for. So this is all pretty typical. I'm hopefully not saying anything that's really surprising or shocking for anybody. The main challenge here, as I've mentioned, is really the fact that we have so much data and in particular this really large correlation matrix across the outcomes across time. Really briefly, I will return this to this in take two. We'll return this to this in take two, right? So, right now, we are assuming that our participants are almost fully observed. And so, what that means is that I'm actually throwing out a lot of incomplete data, and we're going to try to improve on that in our take. So, the primary challenges in accounting for the dependence are going to be that large correlation matrix. And so, some common tools that you may be familiar with, such as GEEs or quadratic inference functions, QIS, these are really computationally. These are really computationally prohibited. And so, a couple of alternative strategies. The first, we could just aggregate over time, but I think that we might lose some important signal. We might obscure some important variation in the physical activity across time. We could also ignore the dependence across time and basically just treat the time points as independent, but we could lose a lot of statistical efficiency and we would also no longer be kind of appropriately calibrating our measures. Appropriately calibrating our measures of error or standard errors, and so we would also lose kind of the validity of our inference. There's also the additional challenge here dealing with the mean model. In particular, in order to be able to specify this polynomial basis expansion for my beta, I have to do two things. I have to choose my nots, and I have to choose some kind of tuning parameter that's going to make my function nice and smooth. And so, over very short time intervals, that might be functional. Very short time intervals that might not be so difficult. So, for example, if I just look at this first interval, this is a very short interval, there's maybe not that many time points, maybe it's not that difficult to specify some knots or to choose a changing parameter. But as the number of time points really grows, the number of choices grows really quickly. And so, this can become really difficult both from a modeling perspective and then also from a computational perspective. And so, the key idea here is to think about these knots, these knot locations. These knots, these knot locations, that we have to choose as natural breakpoints that are going to take this really high-dimensional set of functional outcomes and actually partition them into smaller functional outcomes. And so instead of considering the whole data in its entirety, we're going to instead think of the small sets of outcomes that are formed by each of these different time intervals and only look at these tiny sets of data one at a time. And this is going to make things. Ahead of time, and this is going to make things much easier from a modeling perspective and also much easier from a computational perspective. So, again, the key idea is to take essentially just to view these not locations as these natural breakpoints. And what they do is they partition our big high-dimensional functional outcome into smaller sets of functional outcomes that are just much easier to analyze. And so, we start with our big full data set, and we use this kind of partitioning approach. And so, now we're dealing with And so now we're dealing with these much smaller sets of data. And we then simply estimate our coefficients, our gamma coefficients. You'll recall that these are the coefficients for that polynomial basis expansion. We estimate these and we can do this in parallel so that we have even more computational gains. And at the end of the day, we have to find a way to put it all back together to create one estimate over the entire course of the time series. Of the time series. So there's a couple really important things here that I'm sweeping under the rug for the sake of brevity. The first is how we actually obtain these gamma hats within each of these small data sets. We're going to do this using what are called quadratic inference functions. You can think of these as they're very similar to GEEs. Essentially, what they do is they allow us to estimate the gammas by foregoing estimation of the covariance structure, but still leverage. Structure, but still leveraging the dependence here and still accounting for the dependence within each of these blocks. So these estimates here are semi-parametrically efficient, and they're also really computationally inexpensive. So we can get them really quickly. And then we can do all of these in parallel so that all of these are essentially being run at the same time. And therefore, the procedure at this step is really, really the primary challenge is how we put it all back together into this gamma hat C. And so here there's two things that I'm scooping under the rug. Things that I'm scooping under the rug. The first is the formula. We're taking a weighted average of all of these estimates here to get this final combined estimator. And then the second thing are the weights of that weighted average. We actually have to be pretty careful in how we choose those weights because all of these estimates at this level are dependent because of the dependence between the blocks of data. So there's a lot of math, and we show that you can choose those weights in such a way that actually accounts for that dependence across the different blocks. The different blocks, and that the weights that we construct are actually statistically efficient, meaning that they're leveraging all of that dependence across those blocks to give us an estimator at the end, which is not only computationally efficient, but also statistically. So we get those two things. We get computational efficiency and statistical efficiency through some kind of weighted averaging of these point incidents. And so it looks like I've won the game. Hurrah! I've managed to model. I've managed to model my data in a really computationally and statistically efficient way. And so at this point in our project, my student and I were pretty pleased with ourselves. And unfortunately, we plotted our results the first time and we got something that looked like this. And so this was pretty disappointing. What happened was when we actually put our estimates back together to create this weighted average, we didn't account in any way for the fact that the function should be continuous at the boundary point. That's the boundary point. And so, this actually makes a lot of sense. You partition your data and you estimate your blocks of your models separately and you put things back together. There's nothing in your model structure that enforces some kind of continuity at this edge point here. And so this was pretty problematic because there was nothing in our interpretation of our model for the NHANES data that led us to believe that there should be a discontinuity at this edge point. And so we kind of went back to the drawing board and had a couple of Went back to the drawing board and had a couple ideas. So, the first is we could simply take our estimates from the individual blocks and redistribute them back to those blocks and do some kind of ADMM type algorithm. So basically iterate back and forth until we reach some kind of convergence where there's agreement between the block models and the combined models. This would be really computation expensive. We'd have to reload the data, we'd have to redo some iterative optimization, and so this would be really kind of a last resort. Another option would be. Another option would be to do some kind of smoothing and close processing, but we would really lose a lot of our ability of doing uncertainty quantification because it'd be really difficult to account for that smoothing in estimating our scatter. And so our hope was to do something better. And so our idea that we landed on in the end was to constrain our function at our partition edges, to essentially take the limit from the left and the right and to say that the function. And the right, and to say that the function had to be equal whether you were coming from the left or the right. And because we were working with this expansion of basis functions, what that actually ended up being is a constraint on the gammas. So the gammas from one block were constrained in some way to be a function of gammas in another block. And so we could actually write our constraint as a linear constraint on our gammas in some form here, h gamma is equal to zero. Gamma is equal to zero. VH has some really complicated forms called a van dermann matrix, but that's not particularly important here. This can also be extended if we need continuous derivatives using very similar conditions. So now when we put things back together, we actually have this weighted average that enforces some kind of continuity of our function at that edge point. There's some technical details here about this constraint and how it. Here, about this constraint and how it actually reduces the parameter space, and that actually makes some of the theory pretty challenging. But we can still show that our combined estimate at the end has all of these nice asymptotic properties, such as being consistent and asymptotically normal, as well as statistically efficient. So just to kind of show you what you might expect out of this method. So on the left, I have our method applied to this document that looks like this. And then on the right, I have what happened. And then on the right, I have what happens if you just analyze all of the data all at once without a divide and conquer or without a distributed step. We tried to make these methods comparable in the sense that we tried to allow them to have similar degrees of freedom. So on the left, that's corresponding to the partition edge points. On the right, that corresponds to the knot selection. We tried to make these kind of similar so that the two methods were kind of be comparable. And so on the left, you'll see that our method takes about one and a half. Method takes about one and a half seconds, and on the right, it takes 350 seconds. So, I don't need to tell you which number is larger. So, this is a really massive computational gain. And so, it's really computationally efficient. And then on top of that, it also is giving us appropriate quantification of the uncertainty in that our 95% point-wise confidence intervals are reaching approximately the nominal level here with a coverage rate of about 94%. The coverage The coverage rate for this method that looks at the entire data is low, and that's really because the method is struggling to model the function appropriately within all of these time intervals because it's looking kind of it's only using these knots to try to model that function. So, there's kind of some disadvantages here to actually using all of the data all at once because it limits your ability to specify a pretty good model within. A pretty good model within each of these. So, to come back to kind of the thing that probably is more interesting is back to our NHANES data and back to the model that we wanted. So, what we did is we took our 10,000 or so outcomes and we partitioned them into 168 blocks, about an hour, and then we constrained for continuity of our BMI effect, our beta of t, using that constraint that I showed you earlier. So, these are the estimated fixed effects. So, these are the estimated fixed effects. I'm not going to tell you about them because I've got some pretty plots in just the next few slides. And then, this takes about 50 minutes. So, this is when we parallelize pretty massively and using this pretty sophisticated divide and conquer strategy. And you might ask why it's still taking 50 minutes. And the answer is really that it's actually leveraging dependence and correlation across 10,000 outcomes. And so it's actually fitting this really kind of massive correlation structure model. Massive correlation structure model, but in a relatively fast amount of time because of the dividing pump. So this is a plot of the BMI effect. And so I've put in shaded regions here on the bottom, very bottom, you'll see the 95 percentile. And on the very top, it's the 5 percentile. And so we'll see here that people who have a larger BMI are less physically active during the day. Active during the day, but that that difference really collapses down at night, as one might expect when people are presumably sleeping. We're also able to look at how this changes according to whether or not somebody has mobility problems. And so again, as you might expect, somebody who has mobility problems is less physically active. I want to point out here that our mobility problem effect is just a scalar effect. So what we're doing here is we're actually just taking that curve from the previous slide and we're shifting it upwards. Slide, and we're shifting it up or down by some eight. Is there a question in the audience? Yeah, I'm sorry, I'm sorry. I just don't need because I see that you're in the middle of your slide. So, just to make sure we have enough time for questions. So, maybe like, do you think like how much more time do you need? Maybe like three more minutes. Perfect. Thank you. Yeah. All right. Okay. And so then we're also able to look at sex effects. And again, it's a pretty similar story. It's just a shift of those previous curves. So something that we learned is that this is. So, something that we learned is that this is super fast. It's really statistically efficient in some optimal sense. But maybe this time-varying BMI effect isn't so fast. We'd like to have something that's a bit more flexible. And then, kind of coming back to something I mentioned all the way at the beginning, we actually didn't use a lot of our data. So we actually have a lot of incomplete data that we didn't use. So, take two. We're going to fit a model that's more sophisticated, or at least we think so, where we have kind of this average physical activity that's now. Average physical activity that's now adjusted for all of these fixed effects. We have several more of them that we're trying to adjust for. And then we want to be able to account for the fact that here, as you can see, we're plotting the proportion of observed responses. People are not wearing the device very much at night, again, following the protocol. But even during the day, the average proportion of observed responses is never reaching a particularly high level of 100%. So we actually want to be able to use. So, we actually want to be able to use a lot of that incomplete. And so, just kind of looking at wear time across different characteristics, we noticed that, for example, males are more likely to wear the device longer as are people over the age of 60. And so the data is certainly not missing completely at random. And so, this kind of gave us the idea that we could actually have a propensity score model for the probability of somebody wearing the device using, again, this time-varying intercept here and adjust. Varying intercept here and adjusting for age, sex, mobility problems, and here. And so we did all of this in the same way as I described earlier using our just hide and conquer procedure, where I'm now also estimating the effects and propensity score model. And so here at the top, you'll see the estimate of the average physical activity patterns across our entire group of people. So this looks very similar to what we had before, so I won't talk about it quite as much. So, I won't talk about it quite as much. On the bottom here, what's pretty interesting is this is essentially the log odds, the adjusted log odds of wearing the device. And what you'll see is that there's this increasing probability of wearing the device throughout the morning, and there's a peak in the afternoon, and then a decrease in the probability of wearing the device throughout the late afternoon and the evening and that night. And then there's a lower probability of the beginning of the study and the end of the study. So, it turns out that, oh, so this takes about. Turns out that, oh, so this takes about what about an hour when we parallelize across 56 points. So, here the probability of having a recorded observation is higher for people who are older, higher for males. It's also higher for people who don't have mobility problems and for people who have a lower BF. We'll skip that slide. I just want to kind of end here. One caveat is that the wear status was determined by there being no activity count. By there being no activity counts for more than 30 minutes. So, if we detected no activity counts for more than 30 minutes, we assumed that somebody was not wearing the ties. But individuals that have mobility problems or high BMI may be more likely to be sedentary for a lot longer. And so this could result in the association that we observed of having a higher probability of missingness for people for those particular characteristics. And then on the methodology side, here, there is some underestimation of standard errors. Is some underestimation of standard errors if your sample sizes are too small. So, just a word of caution if you're thinking about using this in your own problem. And then, of course, we have to be careful in how we choose the blocks. I always get this question. I'm happy to answer those questions in more detail afterwards. I'm looking for collaborators who are interested in extending this to multi-level function of data. There's some connections here to urge us. And that's it. So, thanks very much for your attention. Happy to take questions if we have to. Samri? Hi, Emily. Great talk. I had, I think you comment on the covariance matrix or the correlation matrix. So as you know, estimating that in high dimensions is a problematic range. So, how do we really give us an intuition on how to handle that? So, if I understand correctly, you're asking. Correctly, you're asking how we handle the high-dimensional correlation matrix at 10,000 by 10,000. Is that about right? Yes. Yeah. So that's a really good question. So the main way that we handle it is actually by not estimating it at all. So we only estimate the correlation matrix within those very small intervals, so about an hour. So it's a 60 by 60 correlation matrix. And so that's the only correlation matrix that we ever handle. And so That we ever handle. And so once we get those point estimates from those blocks, we then have to estimate the correlation between the estimating functions themselves. And so the dimension there is actually just in the number of blocks. So in my enhanced data analysis, that was 168, which is again much smaller than the full 10,000 correlation matrix. So it's handled through two steps. First, low dimension within the blocks, and then at the combination step, the correlation is not. The correlation is not of the correlation of the data, but actually the correlation of the estimating functions of the estimating equations that are used to generate that combination. Any other questions? Yeah, I was interested in the BMI functional cohesion plot by success, but I didn't have time to really interpret what the if there was a difference, like what that means. I was wondering if you could talk about that a little bit. Yeah, this one I think. One of them. Yeah, so thank you. I know I went by really quickly here. So the in sort of two slides before here, we have just kind of the functional, the estimate of the functional coefficient multiplied by the DMI effect. And so in order to kind of show this plot here, what we've done is we've just added that fixed effect multiplied by effect. So this is really more a feature of the model that we've fit, which again, I think. Model that we've fit, which again, I think has some limitations, but it's the BMI curves, and then they're essentially just shifted by that fixed effect, multiplied by the sex. So zero for female and one for male. And so what we find is that males have higher physical activity patterns than females. You know, there's kind of maybe some reasons that we could come up with for why that might be, right? For example, due to the particular jobs that somebody might have. But I guess I would caution. But I guess I would caution against over-interpreting it, and you know, because of the particular model that we fit initially. Does that answer your question? Yeah, I do. Hi, hi. Could you say a little bit more about how you incorporated the sorry, the end of that question cut out? Could you repeat that one more time, please? Talk a little bit more about how you incorporated the electronic rescue. How we incorporated the propensity score, yeah. So when we create the, so when we go about estimating, let me go to this slide. When we go about estimating the gamma effects like we did in the previous model, we are essentially appending those estimating functions for the zeta effects here, which are from the propensity score model. So you can think of this as kind of, you know, you have a set of estimating functions for one set of. Of estimating functions for one set of parameters and a set of estimating functions for the other parameter. And then we're estimating all of those within that same QIF framework that I described very, very briefly earlier on. So we are accounting for dependence across both the outcomes and also across the probability, sorry, across the missingness indicator. So the mechanism is the quadratic inference function. Mechanism is the quadratic inference function, and then the particular details are just kind of stacking those estimating functions one on top of the other and accounting for the dependence both across the outcome and then also across that missingness indicator. The plot that we see of fetal functions. So, in Enhanced 2011, we see that the females have higher physical activity, right? So, I would, yeah, I So I would also recommend not over-interpreting because going to 2011 we kind of see the reverse pattern. The people kind of have more respectivity in the later half of the monitor. Yeah, I mean that's something I'm very perplexed by. Yeah, but in 2003 I did see this better. It's really the best. It could be the device of the placement. Basement. Almost just become more extra threat. All right, any other questions? There's one from Eric. Okay, cool. Please go ahead, Eric. Can you answer yourself? I'm in. I'm mute. Yep, got you right here. Wonderful talk, Emily. Thank you. So, yes, I will bite and ask you the question about blocks. Are these, I'm sorry if I missed it, were these a priori pre-sponsor? I'm sorry if I missed it. Were these a priori pre-specified partitions or data-driven? So the partitions are a priori specified and not selected in a data-driven way. This is always pretty tricky. I think you have to choose them in a way such that the blocks are small enough to permit something that's computationally efficient, but not too small that you can't estimate your effect well. And then you also have to choose them in such a way that the model that The model that you can specify the model reasonably well within each flock. So you're not dealing with too much data that it's really difficult to specify. And so there's kind of this trade-off, right, between model, you know, model complexity as well as kind of computational speed and statistical efficiency. And so what we've found is that having, you know, 100 time points is pretty reasonable. One of the main challenges that you actually run into for truly high-dimensional data is that if you Truly high-dimensional data is that if you want your blocks relatively small, you need a huge number of blocks. And actually, you start to run into issues not at the distributed stage, but at the combination stage. So I have some follow-up work on that, which I'm happy to talk about offline if you're interested. There's some kind of recursive integration stuff that we can do to kind of bipolar computational challenge. Really good question, though. Super interesting. Thank you. Thank you. So, for the sake of the time, we have to be on time. We have to be on time. If you have any questions, please use the Slack channel to follow up with any interesting talks. Let's thank Emily again. And Julia, there's next slide. I probably should stop this one.