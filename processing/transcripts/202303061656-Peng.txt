Yeah, yeah, yeah. We should take on first test techniques and your follow-up time. Okay. Okay, thanks. So I'll be doing a quick kind of survey of what I consider to be some of the kind of the stranger things that happen when we look at the complexity of solving systems of linear The complexity of solving systems of linear equations. I should preface this by saying that some of the things I'm about to say, for example, that you can multiply matrices by n to the 2.5, are definitely not numerical linear algebra in the sense that they're not things you want to code. The other thing that I want to say, which involves representing floating point numbers and rounding, they're also not TCS either. So TCS usually don't really look at a floating point number. Don't really look at 40 point numbers very carefully, either. So, I really don't know what some of these things really are. It's gonna almost certainly cause some, I mean, I don't promise that some of the bounds I'm about to say or some of the questions I will ask will make everyone happy. I do want to say that in case you are unhappy about some of the things I'm about to say, that there seems to be daily games of pick-up going on in the local ring from the end of the Incrupt with the antics of the company. So if you really want to start a fight, we could take it out to the ice. So the problem I want to do is I want to look at Ax equal to B, right? That's the problem that, as the title says, I want to look at the problem of just given my matrix A, vector X, compute, well, vector B computer. Compute, well, vector B, compute x vector Ax equals B quickly. And the kind of the thing that's going on here is, I want to look at this from the point of view of worst case complexity. So I want to do worst case. What does this mean? This means that I want to, once you tell me the algorithm, my algorithm has no idea what the input is. And what's going on is once there is the algorithm, I analyze the algorithm. Is the algorithm? I analyze the algorithm by looking at the worst possible input that one could give to the algorithm, subject to certain requirements of the input. I'm going to have a lot of constraints about the input function. But I want to pick the worst possible input. This is very much not how we design algorithms in practice. Usually, when we design algorithm practices, there is some assumptions on what the algorithms are coming from. We do make a lot of structural assumptions. But I want to claim that there are several reasons we want. To claim that there are several reasons we want to do worst-case analysis. So, I first want to say why we want to do this. So, the first reason is precondition. So, numerical linear algebra, right, we really like to take up. There's this kind of unreasonably effective phenomenon of iterative methods, which I think Yusuf mentioned a little bit this morning, is that once I have an algorithm that reliably produces a true approximation, I can amplify it to produce an epsilon approximation by running it something like log 1 over epsilon pi. something like one plus log one over epsilon pi. So the first reason is precondition. Preconditioning, what it means is that if you have, so the kind of the on the algorithmic side, what preconditioning means is that I can refine away error. From the kind of worst case analysis point of view, if you flip down inside, what it says that if you have an algorithm that doesn't that often will break, if you have an algorithm that does not work for all possible inputs, That does not work for all possible inputs. There's actually a fairly easy way to test whether your algorithm, like to find the worst case of your algorithm. Which is, like, say you have an album that you claim to me produces a true approximation. All I have to do is I take a linear system, a linear equation, I see whether your album can produce me a high accuracy answer, right? I just take your algorithm and run it over and over. I just say I just run my iterative refinement on your algorithm. And in a lot of cases, actually, for some of these pre-conditioning algorithms that That you are able to generate a bad input for it quickly by just running an inside method. Sorry, Richard, when you said bad input here, do you mean the right-hand side? Yeah, like you're able to generate a from an A, you're able to generate a bad B fairly quickly. Like if your album is not producing a preconditioner for A. Yes, but yes, actually, yeah, no, you do have a valid point, which is that it's actually fairly tricky to generate a matrix, generate a Generate a matrix, generate a a bad left-hand side for algorithm. That is harder. But there are also this type of work. I think Kevin DeWisi and John Gilbert had some sort of comments about evolving a bad left-hand side, using similar kinds of ideas. There's preconditioning, then there is the inner loop. This is getting to what you were saying about generating a bad. So the other things that linear systems are. So, the other things that linear systems are they get used in the inner loop of a combinatorial optimization algorithm. So, they get used within another loop, so interior point method, and so on. Interior point method, so on, these things also treat the left-hand side. They actually generate your entire sequence of A's. And chances are, those things are solving a much wider class of problems. They're solving something combinatorial optimization, they're solving something from like fluid dynamics. So, chances are, whatever the difficulty of those problems, they also go into the linear systems. So, that kind of iteration also generates your left-hand side, and it generates your harder. Generates your left-hand side, and it generates your harder, harder problems. And the third one, this is kind of really where the TCS view is kind of interesting, is that it lets you do reductions. So I'm sure we've all heard about something like NP hardness reduction. NT hardness says that I can show that one problem is hard, I can show that if this problem is hard, the other problem is also hard. That kind of reduction strongly relies on reliance. Relies on the algorithm is being worst-case, in that it relies on that if I'm able to solve all inputs of this problem, I'm able to solve all inputs of the other problem, it's kind of giving a mapping from inputs to the other. So reduction is also strongly relies on having worst-case complexity. And so here's a case of why I would want to do worst-case complexity. But of course, because I want to solve all problems, it's fair for me to put some restrictions on what kind of A could show up. So there's actually a lot of assumptions. So, there's actually a lot of assumptions that you can make as well. Okay, so then there, oh, so I should also say what the first thing, so the first kind of assumptions I add bound complexity. So, bounding complexity. So, the first thing is that there's also the model of computation. So, there's first there's the model of computation, which is I could have is I could have the exact arithmetic, which is really just everything happening over a field. There is bit complexity. There is low memory and then there's also distributed. So I can have different models of computation. I could also have different output goals. So the output goals is, I mean I could either have say x minus x star, two norm, small. Sometimes this norm can change. I guess I should put a star here. I get my actual answer, but approximately in some small. But approximated in some small norm. There's entry-wise. I claim that one that's actually often overlooked in TCS, but I claim that this is actually where NLA gets a lot of things correct, is that what they want is they want the eigenvector or the determinant. There's also eigenvector. And then there's determinant. So the output goals, this is kind of, I mean, this is This is kind of, I mean, this is kind of, this is a lot of it is actually borrowed from the numerical literature. But I kind of, what the TCS has to offer for this is actually the representation of numbers. So the representation of numbers actually gets very interesting in a second. So the representation of numbers, right? So usually the standard in NLA is floating, is with rounding, right? So rounding. So this is a float. Of floats, of floats and floats and doubles, rounding into floats. But I claim that there is also two representations that we actually often overlook that TCS looks at a lot more, which is that they would actually look at either ZP and also uh just straight up P. This is the fractions. So T when TCS says bit complexity, they really say bit complexity. Complexity, they really say great complexity. Let's say that my numbers are just integers, and my input matrix is just a bunch of integer entries, and I want to output actually the exact fraction, which is the determinant of one thing divided by the determinant of the other thing. And here is, once you go into this kind of representation, it's actually kind of, the thing that's surprising is actually how many of our current best complexities for solving linear systems go through one of these representations. And so here's where I'm just. And so here's where I'm just going to, in the interest of limited time, I'll just start listing what I consider to be some of the biggest surprises in the space. So there's what I call strange complexities. I'm not sure if it's strange. I mean, it's partly strange because I mean, they don't make as much sense. They don't make a lot of sense to me. But if you have ways of making them sense of them for me, I'll be very interested in going up. So what I what I consider to be strange so the first is that C G. So the first is that Cg. Right conjured gradient Ax equal to V over Z P in order NM time. And here where, so I'm going to use some, so because I'm talking about worst case complexities, right, I'll be using, or sort of parameterizing them. So I'll use n as a dimension. M as number of non-zeros. I'll be using R for rank. And what other ones I will have? I'll also be using epsilon for error. This is not so crazy yet. The next one has to do with the matrix multiplication. Is there a definition of positive definite in this ZP case? Oh, ZP case, there isn't. So basically the So basically, what I'm about to say once I list these stacks is I say if you start taking combinations of them, and then you get the thing that really don't make sense. Which is basically once you try to take like this and you try to take it into floats, what happens? You try to take this into floats. But before that, let me list the other two things that are kind of crazy once you're in the finite field case. One is that solving Ax equal to B for F. For dense A over Z P is as expensive as solving A X equal to B, where B is where, so that's like N systems. So this is N systems. So, the ability, so theoretically speaking, at least in some settings, there's actually no difference in our current view of solving one system versus solving ton of these systems. And then the third one that is kind of crazy is that solving. You're not telling there's reduction, is that the best upper balance? Oh, yeah, this is just like strange, like current, best known upper balance. Yeah, current, best known. Yes. No. To build on the same question, there's nothing ruling out an entity alpha sparse over finite field algorithm for alpha less than omega. I mean, there's nothing even ruling out, say, like for sparse, like alpha less than two. Like, I think even 1.5 over, like, something. I don't know what an algorithm is ruled out. Yeah. So something more basic. I'm not so sure. So now you're counting Counting going to the extension field, and that's why you're using like why are okay, why are and n to the omega algorithms. No, this is all n to the omega. This is both of these are n to the omega. I'm saying where A is dense, solving one system and solving n systems has no difference when you're restricting students over Z2. Let's just go to Z2. But that is also true for uh the numerical algebra sense. In numerical neural algebra sense, uh if you look solving one system uh is NQ, solving n system you put the factorization, then you solve and so you get n cube. So let me get to the next crazier one. Okay. The next crazier one is solving Ax equal to b over q. Solving exact fractional solution is is the same cost. This is bit complexity? N3 omega. Do you also consider bit complexity? Bit complexity completely. And the craziest thing with this one is that it doesn't even use floating point numbers. It's using what's called p-adic representation to do this. So the citation for this is actually Sorihong 05. So this is the one which when So this is the one which once you get a numerical, you think from a numerical point of view, it gets a little nice. Okay, and then once you start bringing these back into floats, this is where things get weird. Things get even crazier. So now let's bring back floats. So once you bring back flows, floats and doubles. I say doubles. I just call flows. That is called floating point numbers. Break back rounding. Once you break back rounding, what you get is you get that, I mean the first one is that low factor width matrices. So these are things with factor width 2, so Laplacians. Laplacians, factor width 2 are solvable in O tilde. In O tilde m log 1 over epsilon. And then the similarly for general matrix input, what you can get is you can get input sparsity time. So you get things like o tilde of f plus poly r log 1 per epsilon. So you can get to, so instead of having to deal with the, there's a method called sketch and solve, which says that instead of having to have runtime that depends on the dimension of your problem, you can just work up with runtime that parameterized by the intrinsic dimensions, so the actual rank of your problems. And then once you're back, once you're in this setting, then you also have the next thing, which is two commodity flows. Two commodity and also two Laplacians of triangle matches are what's called lin uh LEA plus linear system complete. Uh this is uh Erasmus and uh And uh draw so this is a whole bunch of things like that. Yes? What's factor with two? A factor with two is uh if I my system can be so this is this is the kind of a like a general way of characterizing structure. So Laplacians is like some of two by two. Like you kind of put so you have what factor width is is uh What factor with is my matrix A is written as the sum of AI, and each of the AIs is a 2 by 2 PSD matrix. So it's just like you can write it as a sum of simple i. I believe the next talk we'll talk about this kind of structure. But so what I want to point out is like these are kind of very strange facts to start with. And once you start combining them, is when like all, like you can generate a lot of very like. Like, a lot of I consider questions, I think, are very interesting this way. So if you start combining these, for example, if you take, you put together 4 and 3, how hard is LX equal to B over Q? Is this easy or hard? You can also try to put together 3 and 5. So you can say that, hey, I can solve exact fractional solutions, but what about exact fractional solutions? But what about exact fractional solutions over-constrained matrices? So once you take 3 plus 5, you get over-constrained matrices. Getting things like over-constrained matrices. Matrices over Q. So this is like: if I have a matrix that's tall and thin, I want to. If I have a matrix that's tall and thin, I want to solve a linear system and I want to produce the exact fractional solution. Could I still use some kind of dimensionality reduction there? And there, the whole issue there is actually it's not even clear that the representation here works well with reals. This type of stuff put together with CG. I mean, so then the question with C1 plus anything is like, could I have some other class of system that can be solving subquadratic time? Class of systems that can be solved in subquadratic time. So I claim that to be one of the kind of bigger questions in the complexity solving linear systems is what are other class of systems that can be solved, I mean ideally independent of matrix multiplication time. So what are the other classes of systems that can be solved in subquadratic time? So more subquadratic time. So you can either take one, you can either take one and four as your starting point, I guess. More sub-quadron. More subquadratic. Wait, can you explain what you mean subquadratic? Subquadratic, I just mean better than n squared. So CG is like nm. But how would that mean less than n squared? Yeah, so I guess I should also say the quadratic. So like, can you get quadratic? So the question is, for example, like, can you do like any AX equal to B for sparse in an M type, but over Q? That's also not rule. Q. That's also not ruled out. M is always at least n. Yes. So if you start with one as your starting point, you're going to get to the question, yes, you will be going for n-square type algorithms. NM type algorithms is interesting for this as well. But if you use the low factor with stuff, then you can start asking what can I go subquadratic for some of these? And the space there is actually extremely nuanced. For example, for some of these problems. For example, for some of these problems, so once that they know that's hard, once you make some numerical assumptions, for example, if you have two commodities, but all the edge weights are unit weight, you can actually say some things there as well. So these things are actually extremely nuanced about what kind of systems are coming out of it. For example, we know that say two commodity flow is complete for a linear program, but we actually don't know that it's unit capacity to commodity flow. What's the hardness of that? So the thing is, what's not done here is actually people haven't made assumptions on the numerical. Made assumptions on the numerical values other things. I think I'm out of time. So CG over ZP, I mean, how do you avoid dividing by zero? Since you don't have positive definitions, oh, CG solving X is going to be over ZP? Yeah, there's denominators in CG, right? And they're not. CG, right? And they're known to be positive because of positive definitions, right? But are you assuming there exists a solution X? Yeah. So what you do is you do crylog space. And once that space becomes, like you can check whether it's low rank. So what you do is you form the crylog space, you form the like the what you call the gram matrix of that. And that thing has low displacement rank. And just directly work with that instead of Not quite CG, but that's what. Oh, yeah, yeah, it's not quite CG, yes. Okay, so my dad's traders are quite long. Any other question? Sorry. 