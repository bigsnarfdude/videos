Some kind of non-linear inverse problem. I'm interested in shape reconstruction. So you give me the, you know, maybe Cauchy data from some PDE and I want to recover some shape. That's highly nonlinear. So instead of linearizing, what I'm going to do is I'm going to use what's called the qualitative method. So I'm going to keep all the nonlinear structure. I'm not going to throw anything away by linearizing, but I'm going to pay a price in the sense that I'm not going to be able to recover everything. I'm only interested in recovering the shape. And recovering the shape. So, if there are any parameters in the PD, like coefficients, that's not what you're going to get here. What you will get is some kind of interior shapes that you might be interested in. And I normally do that like so. We're going to see, we're going to construct some function w from the data. So, from the Cauchy data from your PDE, and not want that W to be positive on the interior region of interest. So, let's say I'm trying to recover. Let's say I'm trying to recover D. W should be positive on the interior of D, and let's say zero outside. We want to construct this function once again from the data given by the PDE. And I want to do this nicely. These methods are computationally very simple. If you can type the letters SVD and MATLAB, then you can make this work. And they're analytically rigorous. And you say, well, that's awesome. Why don't we just use that for everything? Why don't we just use that for everything? Well, you know, there's the no-free lunch theorem, right? One thing that's nice about these methods is that I'm not going to need an initial guess, right? Since I'm not iterating, I don't need to start somewhere to try to figure out where I need to go. So I don't need any initial guess. And that's actually good for a lot of shape reconstruction problems. No initial guess means I'm trying to do shape reconstruction problems, so I don't need to tell you that there are one or two or three inclusions to recover. Inclusions to recover. That could be something that you would have to add into your iterative solver. I'm not going to need to tell you anything about maybe if it's a boundary value problem, what's the boundary condition on that interior region. That's something you're going to need to add into your solver, right? When you compute the adjoint, you have to solve the adjoint problem. So you need the boundary conditions or the physical parameters when you do an iterative method, right, to get your adjoint PDE to update at each step. So you would need to. Step. So you would need to have estimates for all of these or these, some of these, if you're looking at an iterative method. But the problem is, no lack of information can be remedied by any mathematical trickery. Lots of times for a lot of these inverse shape problems, you can prove uniqueness for only maybe a couple pieces of COSHI data. One, two pieces, you know, four, let's say. Here, the theorems are going to tell you I'm going to need the full Dirichlet in LMF. I'm going to need the full Dirich-lated Neumann map. So theoretically, I'm going to have to say, hey, give me the full Dirich-lated Neumann map, and then this will work out very nicely, as we'll see in some examples. So you have to trade something. I like to think of it as I'm losing a lot of a priori information instead of knowing these things, so I have to basically take in more information on the back end. So the engineers have to run a couple more experiments, let's say. And that sounds like a their problem, not a me problem. A their problem, not a me problem. And earlier today, and a nice thing about coming to a conference where everybody talks about inverse problems, I think we all know what EIT is at the moment. Jennifer Mueller talked about it this morning in some other talks throughout the couple of days. But the idea, so this kind of cartoon I just kind of stole from this website, you put some, let's say, electrodes here, and you take voltage and current measurements for EIT, that's the Cauchy. For EIT, that's the Cauchy data, the normal derivative and the value of the function. And what you want to do is you want to get an image something like this. And so, for me, I'm not going to get like this, so you can tell this different color. So, you know, we're going to say that that's the heart nodes of the two lungs. I'm not going to get that. They're all going to be maybe one color. I won't be able to tell, you know, what the heart is and what the lung is. But, I mean, I know what a body looks like. So, I know that these are probably the lungs. And you can tell here that, okay, this lung is smaller than. Here, that okay, this lung is smaller than the other, so that's that's probably the problem. Right? So, that's kind of the idea for these qualitative methods. In some cases, you don't need to have all the information. You might not need the conductivity everywhere in the body. You may just need something that says, okay, which one is the problem? Or is there a problem to begin with? And that's what we're looking for. And so I'm going to do this for kind of a model problem. I got this from a book. I got this from a book, and this kind of PDE here, Kai's, this is the indicator function. So D is our kind of original larger region. D0 is the subregion we want to recover. And this is not, yeah, I t just be more of a diffuse optical tomography, but we're going to say that U solves this problem here for a given F. So that's the data that we put on the exterior boundary. We know F. Boundary, we know f. And what we can measure is the Dirichlet-Neumann map. So we know F, and we can measure the normal derivative for that system, system 1 here, for any F. And since I know the exterior boundary, I know the region D itself. I don't know D0, that's what I'm trying to recover, but I know the region D. So since I know the region D, I can always just take the harmonic lifting of F. I know F1 to boundary, I can just find the harmonic function. Is find the harmonic function that has the same trace. So, this you can argue is known from the measurements, and this is known from calculations, let's say. And if I know both of these, the inverse problem is to recover this kind of compactly embedded sub-region just from these boundary measurements. Like I said, I'm gonna need the full diversity anointed to do this. And this is, you know, the top is called a regularization of. The talk is called regularization of the factorization method. So I think nobody here is going to be surprised when I factorize my data operator. So let's try to do that. And the one way you do this, and so I like this model problem because there's going to be a lot of technicalities, but a lot of these kind of how you factorize these operators works in a very similar way. And so I look at this problem here for W, and why I look at this problem here for W? And why I look at this problem here for W, because I'm looking at, you know, lambda minus lambda 0 here. And if you just subtract the two PDEs for u and u 0, they satisfy something almost exactly like this. They both have the same Dirichlet data, so you get a zero Dirichlet condition. They both have the negative Laplacian in it. The only difference is here now for u, you have this indicator function in it that u0 doesn't. So I'm going to write this as a source problem. So, I'm going to write this as a source problem for some h, let's just say in L2. And I can define this operator g. It's just going to take this problem is well posed. So, I can take the source and map it to the corresponding normal derivative. And we can see if that this w, its normal derivative, will match up with our operator if I take h to be negative u on the interior. Just think of it as moving this guy over here and subtracting. guy over here and subtracting u minus u0. And I say, okay, that's good. So what if I, oh there we go, what if I define an s now, another operator I'm factorizing, so I'm probably going to need at least two. So I define the operator s. It's going to take the Dirichlet data and map it to the solution u on the interior boundary. Once again, these problems are all well posed, so these operators are nice bounded linear operators. And just so Operators. And just so backtracking, we see: okay, if I take S, that takes in F and spits out U on the interior, and then take that and plug negative that into G, well that's exactly the difference of my dirich-lated non-moments. So I have a factorization for my operator, like I want. But actually, we can go even a little bit further. We can compute the adjoint operator to S, right? And so this. To S. Right? And so this, you do the, you know, I like to call the kind of fundamental theorem of PDEs. You multiply and integrate by parts to figure out what the adjoint operator is. And it turns out that the adjoint operator, right, this is the correct spaces for the adjoint, and it will map it to some source G to the negative normal derivative of the solution V here. And this is just from integration by. And this is just from integration by parts. And the operator S we can show is it's software to be compact and injective. The compactness, our solution is an H1, so it's L2 image compactness. The injectivity comes from unique continuation. If it's zero on a subset of a positive measure, then you can just uniquely continue it. So those are no big deals. But an important thing here is this operator S star is actually quite nice. Because the operator S star knows. The operator S star knows exactly where the region of interest is. Because the normal derivative of the Green's function is in the range of S star, if and only if, right, this Green's function has a singularity. It's only in the range of S star if I put the singularity inside the region of interest. If I move the singularity outside the region of interest, it can't be in the range of S star anymore. And the idea kind of is, if you think about it here, okay, if I'm outside the region of interest, Outside the region of interest, then V is harmonic. These are kind of indicator functions here. So V is harmonic outside the region of interest. Well, the Green's function has a singularity. How could something with a singularity equal something that is harmonic? This would basically say the Green's function would have to be exactly harmonic outside the region of interest. And that's just not true if the singularities out there. Right? To prove that it is in the range, when you... To prove that it is in the range when you tuck the singularity inside the region of interest, that's a little bit more involved. And that's for another day. But yeah, so you can think of it as I'm basically playing around where I put the singularity to get this result. And this is the standard Dirichlet-Greens function from your PDE course. Okay, that's great. You say S uniquely determined, or S star uniquely determines the region of interest. And I know what you're saying. And I know what you're saying. We don't know S star. We weren't given that. We were given the Dirichlet Deneum mass. And in particular, we kind of can't get S star because that's kind of cheating. The range space where S star is L2 on the interior region. So if we knew what S star was, we'd have to a priori know what the region of interest is. So why do we care? Well remember, we factorize our duration of the Deneuman operator using S. The Neumann operator using S. Can we go a little bit further to get S star into the equation? Right? And so if we look here, I just took the equation for W and I just added the characteristic function times W on both sides. And if you look at this, this looks very similar to the equation we use to define S star. Actually, it's the exact same equation we use to define S star if this is the source term G. So putting everything together. So, putting everything together, if I take this to be the source term g, then these two have to be equal. And you say, okay, well, how do I get this? Well, I can define the operator now t. So now I have these two operators, t and s, that I use to factorize this difference of the derivation Neumann maps. So this has a nice symmetric factorization, s star t s. And this is exactly, very nice, exactly. Exactly, very nice, exactly what I want. I have a star here that knows, remember, that knows exactly where the region of interest is. And it's in my factorization with, guess what, something that I know. This is measurements. These you can compute a priori. So that's all well and good. The question, ah, well, we need one more thing before we can answer the question. One other property we need is that S, this middle operator T, is coercive. So velocities, we need to. So velocities, when you do these things, you one, want to have a symmetric factorization like so. You want S to be compact and injective, which we have. But we also want that middle operator to be coercive. And you can do that. That's again, you know, fundamental theorem of like PDEs. This is what the inner product of TH and H would be. You just replace this with the negative Laplacian of W, and then you integrate by parts here because it. And then you integrate by part T because it has a zero derivative condition. So, T is coercive. Nice. What does that mean? Well, there's a nice, and it would take a long time to go through how you get this result here, but what you can get is basically the range of S can be identified with the range of this operator to the basically one-half power. You showed that this operator is, it has. Show that this operator is, it has a square root, and then you say, okay, the range of s star can be identified with the range of its square root, and then you use a singular value decomposition to say what that means. So there's a lot of stuff that we're sweeping under the rug here, but you know, I got like a few minutes left, so we got to do it. So that's, once you do all of that, you have that, oh, this means that the sampling point Z is in the region of interest if and only if. Of interest if and only if this sum is finite. The lambdas here are the singular values of my operator that I know. The xn's here are the right singular vectors that I once again know. If I know the operator, I can compute a singular value decomposition. So I just have to compute this and I'm done. I can compute the reciprocal of that, because remember, I want w to be positive inside the region of interest and zero outside. So here, w, this w, So here, W, this W would be zero outside the region of interest and positive inside. Now, the problem that we see here is that my operator is compact. So the singular values decay very rapidly. And so theoretically, this is very nice and beautiful. Computationally, it's problematic. Anyone who's ever taught numerical analysis class would be yelling at a student who divides by the singular values of. By the singular values of a compact or L-conditioned matrix, right? So, what do we do? Well, you regularize, okay? And so, the result here comes from this recent paper. It's also on the archive as well. But basically, what I'm able to show is if I have this nice symmetric factorization that satisfies the assumptions just like we have in our problem, here you either need it to be a complex Hilbert space at Hilbert space X, or this kind of dual product, where this is an X and this is in its dual space. You either need to be in a complex Hilbert space, or if you want to remove the complexity assumption on your Hilbert space, this needs to be symmetric. I call this, I think this is like a thing of, I like to call it conservation of assumptions. In science and engineering, you have conservation of mass, energy, momentum. I think in mathematics, you have conservation of assumptions. You move, you move one assumption one place, you have to. You remove one assumption one place, you have to add something somewhere else. So, if you want to remove the complexity assumption on your Hilbert space, you have to add symmetry of this bilinear form. And what the main result says is if your operator S is compact and injective, hey, we have that. If T is a bounded operator and strictly coercive, you don't even need it strictly coercive everywhere, you just need it on the range of S. But we also have that. Then, Then some element L is in the range of S star if and only if this limit is finite. And this x alpha, that's just you solve AX equals L using some regularization scale. And you can show where the, again, the singular values and singular vectors of A show up here. The x alpha you can write out, and it looks like this, where phi is given by your regularization. It's the filter function for your regularization. Is the filter function for your regularization scheme? And we'll see a couple in a little bit. So, how the result goes, you write out, okay, A has a singular value decomposition. That's not new. The new thing is the left and right singular vectors actually form a dual basis. And the key here is that you're mapping a Hilbert space into its dual space. So you can show that the left and right singular vectors actually, they're not just an orthogonal basis for x and x star. For x and x star, they're actually a dual basis when you look at their least representation operator. So that's one of the key pieces. The regular factorization method tells us this is true. This is exactly the standard factorization method. Then you say, hey, x alpha dual product with Ax alpha looks like this. And then you just have to show that this, basically show that this limit actually equals this quantity. Equals this quantity. So this limit is finite if and only if this is finite as well. So you go through that, and what our theorem now becomes for our problem, because we have our division normal maps satisfy all the assumptions that we need. So what's nice about this, the factorization method, it also gives us uniqueness, because if you get the same Dirich-Lady-Neumann map here, then that would say any two regions would give you the same. Two regions would give you the same limit here, so they would have to be the same. So you get uniqueness, and you get a way to compute the region we'll see in a second. Because once again, we know this, and we just have to compute the regularized solutions to this equation. And so the assumptions we need in our filter functions, you know, kind of standard assumptions, pointwise they should go to one and they should be uniformly bounded with respect to alpha. With respect to alpha. And two common ones: this is from Technology Regularization. And this is the spectral cutoff, right? Which just says stop dividing by the singular values when they get too small. Okay? And so I just wanted to plug this in here for us to see. This is a regularization here. When you plug it in, you plug and chug, move some stuff around. This is what you get here when you use Tittenhall's regularization. And here, once again, the alpha regularization. And here, once again, the alpha, right, it saves us just like you would for standard regularization. It saves us that, right? What's the power of the lambda? It's basically lambda cubed over fourth, so yes, it's like one over lambda, but you're saving yourself by the alpha that you added into the denominator there. So you would expect that this should add some stability to the problem. And so now instead of putting a one here, you should, whatever your filter function. One here, you should, whatever your filter function is, square it and plug in the eigenvalue singular values, and that's what you should use. That's my claim: is that's what you should be using for your numerical reconstructions. Okay? And so this was, I didn't just come up with this, this was very much motivated by something called the generalized linear sampling method. And for our problem, it will look like this. And what turns out is that what I noticed is that when I was going through this, that the generalized linear When I was going through this, that the generalized linear sampling method is just a specific regularization technique for this problem. This is the misfit, right? You want to minimize this to get a problem so that this equals this. And because of our operator, this actually acts as a penalty term. And so what you can, once again, roll up your sleeves, go through the calculations. For the generalized linear sampling method, this is what you would get here. You would get exactly this kind of term here. This kind of term here looks almost identical to what you get for Tichnoholme regularization. There's just an extra lambda here in the denominator. So this would be the filter function for the generalized linear sampling method, and it satisfies the assumptions we need for this to all work out properly. And I'd be remiss to say the generalized linear sampling method is actually quite general. So for these kind of operators that work, you don't have to put, so here I put the same operator here in independent. So, here I put the same operator here and in a penalty term, you don't have to in a generalizing your sampling method. So, it is a more generalized idea. But for this problem, if I put the same operator here and here, then it's actually the same kind of problem, right? It's just a kind of, it's covered by this theory. All right, okay, well, I've been talking a lot, and you gotta get some pictures, right? I can't leave you here without any nice pictures, or I think they're nice pictures. Or, I think they're nice pictures. So, we're gonna, the domain D is going to be the unit disk, and that's just because I don't have to do anything to compute the normal derivative of the Green's function. It's just given by the Poisson kernel. Now, okay, if it's not a disk, you could compute the Green's function using the boundary interval equations or using the spectral decomposition for the Laplacian with zero diversity conditions. So, there are other ways to do it. Other ways to do it, but let's just say it's a circle so we don't have to go through all of that. I'm going to compute the Dirich-Leden Neumann mapping, where f is just the complex x-ventures, since I'm on the boundary of the circle of radius one. Then this, if I have this for enough, then I say enough of these basis functions, and I've approximated the derivative of my map. So I do this for, if I'm remembering correctly, n goes from negative 20 to 20. n goes from negative 20 to 20. I need to use a Galurkin method to solve for u minus u0. And for simplicity, I'm just going to assume that the region is star-like with respect to the origin for some row. And what you get here, so this is what you get, the dotted line on both slides is the actual region of interest. So it's kind of, I call it pear-shaped region here. Region here is the region of interest, and this is the basically level curves of the function w. So I do exactly like I did. I just do a discrete version of that. I take my Dirac Lein Norman map. I discretize it to be a matrix. I compute its eigenvalue, singular values and singular vectors, and I just plug it in exactly like it is in the formula, but in a discrete setting. This is the row, and I take the. So here I don't, you know, how did I come up with this? Now, how did I come up with this? I just picked alpha at hoc. I think that's something I need to study. An interesting question is: how to pick, you know, what would a discrepancy principle look like for something like this? I don't know, so I'm just going to pick alpha to be fairly small in these examples. So here, this is the level curves, and this is W. The solid line is the level curve, I believe, at 0.1. And here I add 5% random noise to the data. The data. You can do this for a more complicated region. So, this one is kind of nice because these sampling methods usually sometimes fail when you have these non-convex fits. And you see here, this kind of getting this non-convex shape fairly well. Here again, level curves, I guess, contour plot, I should say, not level curve contour plot. And this is the level curve again at 0.1. Still 5% noise added. And here I And here I would say for these two examples, I just used the spectral cutoff. So I just say stop dividing by the singular vectors when they're 10 to the minus 5. That's too much. And here I just now kind of a rounded square. I just wanted to see if basically the filter function, how much does that change the reconstruction? So here this is with the generalized linear sampling methods filter function, and this is with still the spectral cutoff. Spectral cutoff, same regularization parameter in both. And I like to say, you know, in the eyeball norm, they look fairly similar. Again, the dotted line is the actual region of interest. And we have the contour plots here. And one thing I like about these methods is that they are fairly generic in the fact that you just have to factorize your operator properly and it works. So before the examples, those numerics were from diffuse optical tomography. Diffuse optical tomography, the problem that we were talking about earlier. But recently, with one of my graduate students, we showed that this also works for this kind of problem that comes from EIT, where here you basically there's a jump in the normal derivative that is proportional to the solution on some interior boundary. And the code works almost exactly the same, except for the code to solve the forward problem is different, but the code for the inverse solver, you have to change nothing. Solver, you have to change nothing. Once you give me a discretized direction map, I find a singular values, a singular vectors, plug it in, plug and chug exactly the same way. So this is one thing that's really nice about this is that a lot of the hard part of the coding is the getting the data. But this in-verse solver, you don't even have to change from problem to problem. And lastly, for my PhD, I looked a lot at a lot of I looked a lot at a lot of inverse scattering problems. And so, one thing that I'm currently writing up now is: I said that this should be stable with respect to noise in the data, but I haven't proven that to you. You would expect stability because I'm not dividing by, I wouldn't be amplifying any noise randomly. And so, here, with the theorem that you have, I was been working on, and I think I finally have it, you get more or less the same results. So, if you have some perturbation of your operator. Perturbation of your operator, right, where the perturbation converges to A, your perturbed operator converges to A in norm, then you have to ignore this double limit, but now this double limit will be finite if and only if that was in the region of S star. And this is exactly what you think it is. It's the regularized solution to this here. And what this picture is, this is from an inverse scattering problem. So, you know, I like this. These first two problems here are from tomography, and this is from an. From tomography, and this is from an inverse scattering problem. And once again, I discretize, I use what's called the far-field operator, I discretize that as a matrix, find its singular values and singular vectors, nothing about the inverse solver changes. But here, this is what I get when I don't regularize. So alpha here is 0, and I forgot to add it to the slide. I think the alpha here is like 10 to the minus 6 or 7. You can ask me later on, I can look it up. I can look it up. But here I add 5% noise to the data, and see here the dotted line is what I want to recover, and I'm not getting it because I've added noise to the data and I'm dividing by the singular values of a compact operator. Not smart. But once I regularize, we do see, yes, I am now filling out the whole region of interest as one would expect. And here are just some references. These are just some books if you are interested in inverse. Inverse problems in general, that's a good book. This is where I got the problem from: diffuse optical tomography problem. These are two really good books if you want to learn about the factorization method and inverse scattering in general, and some papers that kind of motivated me to look at these types of things. And that's all for this. Questions? So I was wondering if you're a little bit more. Uh so I was wondering if you have multiple, let's say smaller s scatterers, how well would you be able to resolve them? Uh in case the column is very high, you have multiple scattering between them? Yes, so when there are multiple scatterers, when there are multiple, even just objects, what will happen is if they're well separated, then it's then it's okay. But if they're fairly close, then the image kind of blurs. So actually in this paper here, my graduate student, we looked at This paper here at my graduate student, we looked at the case where you had multiple small volume regions. And for that, we came up with something called the music type algorithm to handle that case, where we can show that, yes, you can come up with an indicator function that works fairly well and works very well if the regions are well separated. But yeah, if they're fairly close together, then basically, this isn't really high resolution, so they're really small and close together, then the function w kind of believes. Then the function W kind of bleeds in between, and you can't really tell if there's one big one or two small ones. So if you have that a priori information that is small, you can kind of use some kind of asymptotic expansion to get a music algorithm that'll work better. It's only in summary, you like derive a factorization for the difference of TTM mappings by looking at the adjoint of a solution operator? For the. Or is that what S? Yes. So that is correct. Yes, so that is correct. So, yeah, S is kind of like the S is a solution operator, right? It maps. We go back, that's too far. S maps the data to the solution on the interior boundary. And you need that adjoint operator for S, as usually how this kind of works. Okay, and is that just like you were looking at the range of the adjoint pass, and you're like, I mean, it's like the other one region you're trying to get shape right. Yes. Oh, okay. Yeah. Oh, okay. And and and and it would it would show um that the difference between TM packings it's like I think it was like it's compact, right? It's one-to-one. It'll be, yeah, I don't think I said it, but it's it's it'll be compact, injected, and have a dense range. Oh, and it's actually used for like regulation or use. So that's that's one of the, you know, that's if it's compact operator, right, inverse is unbounded, so the next best thing is injective with the dense range. Depends on how much data is needed for like, for example, the medical imaging example. Mm. Yes. That's a very good question. I do not. So that's a question for Jen. She's way better with the kind of specificities of how an application just works. I like just more just seeing how analytically you can get things like this. Those are very interesting questions. A very introvertful questions. I just needed answers. Oh, for the toy problem, you don't need to seem to have a. So I use like, I need what? So 20 by 12 step, 41 pieces of Cochi data. I think you can knock it down a bit. So for small, so I know if you need small volume regions, the answer is how much data you need is if you, if there are, what is it, if there are J regions, then you need the data. Regions, then you need the data you need is m plus 1 has to be greater than J. So J, if they're small volume. So would this translate to something we could do for larger volume regions? I'm not sure. But for small volume regions, we can show that. So you have J regions. So here, you have three small regions, right? So you need at least four or five pieces of Cauchy data for the music algorithm to work here for trying to recover those. It's independent of the size of the? It's independent of the size of them? So this one is not. This one, you need these to be small volume. Oh. Right? So this I know specifically for small volume regions. For larger ones, is that, do you have some kind of analog? Do I have some analog for that? I don't. But I do know for small ones, this is kind of true. So I kind of like to think of, I try to take enough, but not too much. I don't want to take, I don't want to tell engineers that you need to take a hundred, you know. You take 100, you know, 200, 300 pieces of Cauchy data. I just say 41. To be honest, I just say, let's try it for 41 and it works. So I'll say, okay, cool. But yeah, that's a good question of could I maybe go minus 10 to 10 to see if it works. Not sure, but that's, yeah. Yes, that's the US because that's one of the nice things about this for a lot of companies.  I also think, I also think this could be also very difficult if you have four internet needed good enough. So this could be a website. That's fucking childhood. Oh, yeah, I can tell you. This is for the voice server and then. 