Okay, I'll just use the keyboard. Okay, this is the definition of the problem. So we have given a complete of a V and the type of attributes are given. Edges are divided into class and minus edges. So if you have a class edge UV, that means UV should be clustered in the same cluster. If you have a minus edge UV, they should be in different clusters. Then our goal is to find a cluster we see that minimizes the number of unsatisfied edges. And so if an edge UV is a class edge and it's like If UV are not clustered in the same cluster, then the edge is in disagreement. If UV is a minus edge and they are in the same cluster in the out of the clustering, then UV is. For example, this is the instance, we only put nut edges here, and suppose this is the not queen, then we have four edges in disagreement. Edges in disagreement. So those three class edges, the two vertices should be put in the same cluster. We put them in different clusters. So those are in disagreement and this minus edge is in disagreement. So we need to minimize the number of edges in disagreement. So is the problem clear? Okay. So there is a long list of papers that is problem and each one keep improving the approximation ratio of the previous one. Approximation ratio of the previous one. So it's known that the problem is APX hard. And the first few results are combinatorial algorithms. And the best combinatorial algorithm gives an approximation of ratio of three due to eigen, Chaoica, and NUMA. And in the same paper, they developed an LP-based algorithm that gives a 2.5 approximation. The LP is called a metric LP, and it's known that. And it's known that it has an integral energy gap of two. And then later, Chawa et al. pushed the limit of this approach to close to two. And they gave a 2.06 approximation for the problem using the metric LP. And that also proves that the integrality gap of the LP is at most 2.06. Then recently, a token added nearly. If a Cochrane Adam Lee and Newman break this value of Q by considering the Chivani items hierarchy of the metric LP, they can get the approximation ratio from likely minus 2. So that's 1.994. And in this talk, I'm going to talk about our new result that gets the approximation ratio of 1.73. It's based on shawan items hierarchy and the combination of two items. And the combination of two algorithms. And also, we have ongoing work that further improves the ratio to 1.5 minus delta for some more constant delta. Okay, so I'm first going to talk about some common components used in many previous algorithms. I will talk about my our 1.7 scene approximation. Okay, so the first component. So the first component is the metric LP relaxation. So we know that if you need out the clustering, so we define XUV to be zero if U and V are in the same cluster. X U V is one if they are in different clusters. Now X is a matrix. Then in the L P, we just require X U V to be between 0 and 1, and X forms a matrix. That's the matrix L P. That's the metric LP. The objective function is this one. So if UV is a plus H, we pay a cost if U and V are separated. So that's indicated by X U V. And if U V is a minus H, we pay one minus X U V. That's the metric L P. And this L P has an integrality gap of Q. So the instance is a star instance. The last edges form a star. So the size is N, then Latin is optimum. Then roughly the optimum will have cost a minus q, but magic LQ, you can just have half ninth for each class edge, and minus one for all the minus edges. That's the integrated gap of half. And then many previous algorithms are using this pivot idea. So in every iteration, we just choose a pivot and then basically Pivot and then based on the pivot, we choose a set C of clusters. So C will contain P. Then we name C as a cluster, remove it from the set of vertices, and then repeat. So that's a P-WAT-based algorithm. Some of the algorithms use this metric LP, some do not, some use these most stronger LPs. Okay, so here are some. Here are some known algorithms based on this pivot idea. So, if we add, so suppose P is the P-bot and we consider V, if we add Pv is a plus H, we add a V to the class C. So that's a combinatorial algorithm. So if Pv is a minus H, we don't add a B. And this algorithm gives us real approximation. And then a natural idea using this. idea using this metric LP by independent around. Suppose P is the pivot and we consider V, then we put V into the cluster C with probability 1 minus X T V. So that's what's suggested by the metric LT. So that's the independent rounding algorithm. It gives the 2.5 volt oxidation. Then the work of Chawai it L, so improves this 2.5 to 2.06. They kind of They consider a distorted probability function. So they define two functions: f plus and f minus. So if you are if you have a p what is p and then consider v. So if p v is a plus h, then they add v to the set with probability one minus f plus x equal T V. F class X V or T V. So they distort the probabilities and that will give a 2.6, a 0.6 approximation. And all those algorithms need to use this so-called triangle-based analysis. So what's a triangle-based analysis? So we define LPUV to be the LP cost of the plus edge. If it's a minus edge, it's one minus edge UV. So during the algorithm, So, during the algorithm, if at any iteration UV is decided, that means after this iteration, we know whether UV is in agreement or in disagreement. That means one of the UV is clustered. At that time, if UV is decided, then UV will release its LP budget of LPUB. Okay, but if at the same time, UV is in disagreement. UV is in disagreement, then we pay a cost of one. So then to prove an alpha approximation, you might wonder: can we prove that for every HUB, the expected cost is at most alpha times its expected budget we missed? But that's a too strong property that will solve the weighted version of the problem and it captures multi-card. So that's a hard problem. So indeed, So, indeed, it suffices to do the following thing. So, we can focus on three vertices, U, V, W, and then we consider the expected cost given by UV conditioned on that W is a P1. And that's the expected cost of UW conditioned on that V is a P1 and the other term. So, that's the cost we need to consider. Then we come to. Consider. Then we compare this with the expected budget released by UV when W is a pivot, plus the expected budget released by UW when V is the P1. And the third term. But this is the probability that UV is decided in that iteration conditioned on uh W is the P watt times the uh L P value of U V. LP value of UV, right? So that's the triangle-based analysis. We just need to focus on three vertices, EU, VW, and compare the cost and the budget. So here we collisionally use two facts. First, we choose each pivot uniformly at, choose the pivot uniformly at random from the remaining vertices. And also, all the edges are unweighted. We use those two facts. Okay, let's see why. Let's see why the independent rounding algorithm gives a 2.5 approximation using this triangle-based analysis. So recall that in the independent rounding, so if we have p-var p, then we add v to the cluster c with probability 1 minus x2. That's an independent event. So the worst case is with a triangle. So we have a UVW triangle. We have a UVW triangle, the distance between U and the V, between U and the W are 0.5, and the distance between V and the W is one. And this is the plus edge, plus H minus H. Okay, so this is the worst of the case. So let's see what triangle-based analysis gives us. So if W is a P-watt, then V will not be in the cluster because the distance is one. Because the distance is one. And U will be in the cluster with probability half. So the cost will be half. Because if U is in the cluster, V is not in the cluster, and UB is in disagreement, right? They are separated. And the same for cost of UW. Then if U is the pivot, then V will be added with probability half. W will be added with probability half. W will be added with probability half. They will be added together with probability one quarter, but that's a mistake. If we put them together, it's a minus 8, we incur a cost of 1. So it's 5 over 4, the total cost. But then what's the budget will be in this? So if the W is the pivot, then the probability that UV is decided is half. So if U is It is half. So if U is in the set, then UV is decided, but the LP value of UV is only half. So it's half times half. And this curve is also half times half. And the LP, VW, that's zero. VW is a minus edge. It's a length is one, so it means it's correctly zero. So this gives us a budget of alpha. Then this shows we need to set alpha to be 2.0. We need to set alpha to be 2.5, and this is indeed the worst case to give the 2.5 approximation. And then you can see that how can we improve this 2.5 approximation? So the issue is where U is the pivot, we put V and W together in the set with probability 1 over 4. This should not be the case because we know the distance between V and W is 1. They should never be. They should never be put together. We should not do an independent rounding in this case. So that's the idea of Cohen Addit and D and NUMA. So they do a co-related rounding. So in order to run this coordinated rounding, so they need to introduce some LP variables like this. So the LP variables will indicate the clustering status with respect to With respect to VUW, the three vertices. For example, this means whether the three vertices are in three different clusters. This means if UB are in the same cluster, W is in a different cluster. So we can have those variables. And then if U is the pivot, then we add V, but not W to the clinic with probability YUB smashed up. So that was given by the LP value. And similarly, we put W in the cluster, but not V with probability YUWV. And we put both V and W in the cluster with probability Y U V W. Okay, so if we can do this, then we consider the triangle again, then this cost of Vw becomes zero because we put a Vw. Because we put a VW in the same cluster with probability YBW, but that will be zero. Okay, so then that suffices. It suffices to set alpha equals two in this case, and they can get slightly bit no two by showing that so this is the only bad triangle that gives the approximation of two, but not all the triangles are of this type. Okay, but there is an issue here. An issue here. So we cannot guarantee this property for every pair VW because that will require the pairwise relationship. You can solve the quadratic problem. Yeah, if you can do that. And so the idea they gave is using this like Habanti Pan technique to make sure that this property is satisfied on average. Satisfied on average with error epsilon. So, what we actually have a time technique says is that the following: Suppose we have a pseudo-distribution Z over the subset of N and the pseudo-distribution is a LP solution and in the polytope of a R-level lift of Chairman eigenfier. So, I'm going to skip the details, but we know that. But we know that it's a polytope of size and with order R, and this is a LP solution in the polytope. That given Z, we can randomly output S, such that the probability of both V and W is close to the desired probability V V W. So VW is the desired probability, and this is the actual probability. Is the actual probability, and we cannot guarantee this is small for every pair, but on average, this is small. Okay, on average, over all pairs, we double. Okay, and so on average, it's not an issue because in the collimulation class-to-me problem, all the edges have the same weight, but this edge epsilon is an issue. So, if the cost of the edge is small, then we cannot afford to pay. Then we cannot afford to pay for additive error field signal. So, what Colton Ed and Ital did was they distinguish between short, median, and long edges and only do correlated rounding for the median, non-last edges. So that under this editive error. Okay, questions so far? And our algorithm is also going to use this technique. Okay, so here comes our 1.73 approximation for the problem. And our algorithm is based on two algorithms, combination of the two algorithms. One is the coordinated rounding, which I just described. And then the second one is the set-based rounding algorithm. So it uses set-based LP. So this LP has a variable y for every subset s, which indicates Subset S, which indicates if S is a classical or not in the classical V. And then we just require some of Y over all A's containing V is one for every V. This is carving. Then from the Y vector, you can give the X U V vector. X U V means u whether you and V are separated or not. So this is the definition of X U V. Then we can have the objective function the same. The objective function is the same. And so for now, let's ignore the running time issue. We cannot solve this LP because we have exponential number of variables. And suppose we can solve it, then there is a set-based rounding algorithm using this LP. So what's the rounding algorithm? It's very simple. Start from all the vertices and empty clustering. Each iteration, we just choose a iteration we just choose a random set ace with probabilities proportional to the y's values. Okay, we just choose a random ace. So if a is non-trivial, so if it intersects v prime, then we just let ace intersecting v prime be a cluster and remove a and repeat. So that's the whole algorithm. Each time we choose a cluster, let's choose a set ace. I choose a set A and then let A be a cluster. Okay, so what does this algorithm give us? So it gives us approximation ratio of 2. Indeed, it gives better than 2. For most of edges, the approximation ratio will be better than Q. So we have a per edge ratio. Here is a per edge ratio. Suppose UV is a plus edge. So what's the probability that? So, what's the probability that U and V will incur a course? So, we only need to focus on the set space that intersects UV. It contains at least one of UV. If you don't choose such a set, then UV will be delayed to the next iteration. So, if you choose such a set, then UV will be decided. But the probability that they incur cost is the sum overall set space such that. sets ACE such that ACE intersects contains exactly one of UV. That's 2xUV and this is 1 plus XUV. Okay, therefore minus edges, the probability that UV incurs the cost is 1 minus XUV over 1 plus XUV. It's just a simple calculation. So XUV is the cost of UV in the LP if UV is a plus LG. If you be the Knut H. So that means for Knut H, the approximation ratio is 2 over 1 plus X, which is like most 2. For minus H, the ratio is 2 over 1 minus X. No, 2 over 1 minus X. So yeah, this should be 1. 1 over 1 minus X. 1 plus X. 1 over 1 plus X, which is at most 1, it's even better than 2. It's at most Y. Q inside most Y. So that means the only bite edges are class edges of length roughly zero. X is very close to zero, you get approximation ratio of two. The key observation is that those edges are very good in the other rounding algorithm, in the correlated rounding algorithm. So in the correlated rounding algorithm, Cohen Eden showed an approximation ratio of two. Approximation ratio of two, but we can do better, we can give a per-edge approximation ratio. So, this is the modified triangle-based analysis. So, we will define two ratio functions, alpha plus, alpha minus. So, if you have a plus h uv and its length is x uv, then we set the approximation ratio to be upper plus x. For the minus h, uv. For the minus edge, we said it alpha minus xup. Then everything will be the same, except now the alpha for the three edges are different. It's not a uniform alpha. It's the alpha, the coefficient for the edge. So this is alpha, the sine of uv x uv, alpha the sine of uw x u w. So that's the modified triangle-based analysis. Triangle-based diagnosis, we use different ratios for different edges. And what we can show is we can set the approximation ratios for plus edges to be minimum of 1.515 plus x2. And we can set the minus edges ratio to be 2. And this is not a bottleneck because the other algorithm, minus edges are good. Okay, so then we can combine the two algorithms to get the Algorithms to get the final ratio. So we only need to focus on plus edges, minus edges, they are okay. So set-based algorithm, this is the ratio, 2 over 1 plus x. And the correlated rounding algorithm, this is the ratio. So the worst case is decided by the four points here. And you connect in the two points here, two points here. This point is. Two points here, this point gives the approximation ratio of 1.73. Okay, so let me talk about a little bit about how to solve the set-based LP. And we will use a polynomial-sized LP instead of the exponential size LP. In this LP, we don't have a y eighth variable, but we have a z eighth variable. So this eighth is the sign. This athle is the size of a cluster. This capital H indicates if capital S is a subset of cluster or not. So this means whether S is a subset of a cluster of size S or not. So this small S uh is uh can be uh as large as N, but we uh restrict the capital S to have size at most R. Ace to have size at most r, where r is pointing one over epsilon. And then z will indicate whether a is the subset of a cluster or not. But there is only one exception where a c is empty set, this will indicate the number of clusters of size s. So you can think of z is the number of clusters of size s containing capital S as a subset. But if S. Set. But if S is not empty, that's 0 or 1. So it will indicate this event. But if S is an empty set, it could be more than 1. Okay. So the constraints are some natural constraints. And I'm not going to talk about them, but I will talk about the rounding algorithm if we have the Z vector. So recall that in the set. So we require that in the set-based rounding algorithm, we need to choose one ace with probability proportional to y a's. But now we don't have y ace variables. What we do is we use this two-step process. First, we choose the cardinality s proportional to the weights of those cardinalities. So Z empty set divided by Z empty set. So this is the number of clusters of sizes. Number of clusters of size S. This is the total number of clusters. We choose a cardinality. Once we fix the cardinality, we choose a P water P proportional to the total weight of the P water P appearing in the clusters of size A. So once we choose the P water P, we apply the time technique to choose a site A's. Okay. And then we have the same issue. The same issue, this technique will lead to an editable error of epsilon for every edge. We will end up painting an epsilon and square editor error at the end. Then we handle this error using tweak nasty. So that's the title of the talk. So we consider tweak nastered instance. What's a pre-clustered instance? So first, every edge is either a plus edge or minus edge. edge or minus edge and on top of that we have many disjoint subsets of vertices so each subset is called an item so it means that those sets we cannot break them so this subset all the vertices should be in the same cluster in the output then every edge between two vertices in an item is called In an item is called atomic edge. Other edges may be an admissible edge or non-admissible edge. So every edge is one of the three types. So they form a complete graph and then an edge can be either a plus edge or a minus edge. So each edge has a sign and has a type. So a non-admissible edge means if we have a non-admissible edge UV, we should At UV, we should not put U and V in the same cluster. That's a requirement. So that's a pre-cluster instance. Then, given the pre-cluster instance, we define a good clustery to be a clustery that does not break the item and always breaks the non-admissible edges. If UV is a non-admissible edge, you and the V should be separate. That's just the requirement. And we show that. And we show that given a correlation clustering instance, we can efficiently reduce a pre-cluster instance such that the number of admissible edges is small compared to the opt. Also, we don't lose too much by requiring the current screen to be good. So, there is a good colour screen of cost 1 plus 3% opt. So, why does this help? The crucial property is this. The crucial property is this property. So indeed, like Havanja time technique, we can make it lose the additive error for UV only when UV is strictly between 0 and 1. So if UV is XUV is either 0 or 1, basically we don't need to round it. We can handle them trivially, so we don't lose the LDB epsilon error. But for atomic HMOV, we can set HMOV to be zero. set X U V to be zero for non-admissible edge UV we can set X U V to be one so the only edges we pay the additive epsilon of cost are the admissible edges but we have a few of them okay so that's the 1.73 approximation in the OE work with many people we managed to reduce the ratio to one point five The ratio to 1.5 below 1.5 is still based on two algorithms: a set-based algorithm and a poor claiming algorithm. So we don't use the coordinated learning anymore. Okay, to sum up, we give a 1.73 approximation for the coordination class 3 problem, and it's based on a combination of two algorithms. And we use the pre-class screen technique to avoid adding. Technique to avoid an additive lip time error for every edge. And here are some open problems. So the set-based LP is the strongest LP we have so far. Although we cannot solve it, but it's good to understand it. So what is the integrated gap of that LP? So in our ongoing work, we can show it's at least four-thirds, but it's good to give an exact number. And also, Number. And also, we can consider the problem with the LT norm of the disagreement vector. So EV is the number of disagreement edges incident to V. We need to minimize the LP norm of this vector. And there is a more general clinic problem called hierarchical clinic, which can be built as a chain of coordination clinostry instances. Instances. And for that problem, we have a constant approximation, and the ratio is huge, and the algorithm is complicated. It is good to improve the ratio. And also, we can consider the LP norm of the hierarchical tomato. Without a problem, we don't have a continuum. Okay, thank you.  