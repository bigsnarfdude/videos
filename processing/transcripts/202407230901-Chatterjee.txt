who is graduating tomorrow and she has to graduate because uh she's going to join uh postdoc in scanford uh the week after so that's the really but i'm really enjoying the workshop i've been here for a few days earlier actually i was in jasper this is my highlight for my animal site and but i heard like actually the park has been closed now and they are evacuating so i'll be lucky to escape that or that just in time and thank you Same time. And thank you. Thank you to organizers, Anvil, Lauren, and all the other organizers for inviting me and still be part of it, although I'm not going to be able to talk to you this day for the whole time. So yeah, so this is the title of my talk. I'm going to talk about some work which we have been doing over time and over developing this transfer learning framework for data integration. But at this end, we'll make things much more scalable for PowerPoint scale data analysis. And I hope to show you some data. Analysis, and I hope to show you some good applications of it. I want to start by acknowledging the people who are driving the research. Proshanji Kundu, the priorities student who did some of the original work in terms of generalizing the connection theory, and then Luji and Yao, who is scaling things up computationally for bank-scale data analysis. So, yeah, so this is the introduction. Most of the statistical methods, machine learning methods, are designed for analysis of this kind of data. For analysis of this kind of rectangular data, where it can come from one study, or we assume that there are multiple studies, we have done everything possible to harmonize the data so that it looks like this if we could analyze the way. But increasingly, across power banks, across over studies, across consortium, we have this kind of data problem where different studies, of course, did not collect big information in the same design. They may collect some overlapping feature, they may collect some distinct features. Even within a study, it may have that people might. Study may have that people might work by design on some success of individual, we may collect more detailed information than the food cohort would give out. And it's a great example, for example, profiling data is available on the one-tenth of the sample of the food cohort application, what I will show. And this is, for example, if you can within UK bowbank, if you look at the different modalities of data that have been collected, you can, so there's a whole set of individuals, you get a basic set of stacked information. A basic set of discount information collected on everybody, and then on success of people, different modalities of data has been collected. And suppose now we are trying to develop a very comprehensive risk prediction model for any incident disease outcome like heart disease, and inputting all the modality of data, sometimes you may just analyze using those people who have everything. And but that is a leads to a lot of loss of information because as you can imagine that you know there are different parts of the data, but you do not have everything, but you have success of available that can be Substance of a high level that can be used for prediction. Another application which I'll show is like basically we have bows and circuit grade, like UK bows and because we have, you know, in this case a covert study, you can think about the cross-spective incidence data, we have genetic data, but we also have lots of basic covariate data, biomarker data. And if you have that kind of information, like a covert study, it gives a lot of flexibility to do various kind of analysis just going beyond your standard GWAS analysis or you know type chemotype analysis. Like for example, Like chemotypal disease. Like, for example, an example I've shown, like, I want to understand how the genetic association of high diabetes might be mediated by various types of risk factors like EMI, other standard risk factors like cholesterol level, or even baseline in D1C level, you know, that is the power marker for diabetes diagnosis. And so, you could do that analysis only within UK bowel then, and you know, that can get done with that, but that analysis is a lot less powerful because even lot less powerful because even for common disease in a cohort like UK Bowen, we probably have 10, 15,000 type 2 diabetes incident outcome data. But in the external US consortium, which are mainly driven by case control studies, we have much larger sample size. And we have very powerful summary statistics that are emerging from those studies. But those statistics are basic summary statistics. They are not adjusted for over limited things you can do. So I'll show up. Can do. So, I'll show an application of how these retired medication frameworks can be used to analyze K-bow and data to develop more complex models by interpreting some existing information from the outside studies. So, here is the framework. I'm going to describe the method in a very basic, simple framework, keeping the UK Bowerband application in mind. So, suppose we have a main study like UK Bowland, where I may have access to individual level data, and I'm trying to develop a model to predict an outcome. This is a Y, this is an incident outcome in UK Bowland. And I am considering three steps. And I am considering three sets of variable. One, I consider basic adjustment factors, that means the variables I want to adjust the model for, like you know, my principal components, center, age, gender, these are factors we adjust for. And I keep them separately because there may be similar adjustment variables from the external study. For example, again, genetic principle components or centers, and it is not meaningful to talk about the variable could be different at all, the different studies, like center definitions of this different. And even if they are the similar. And even if they are similar, their effect size could be very different. So, you do not want to make any assumption about these effects or the nature of these adjustment factors that are actually in the population. And then when I talk about a set of variables, which are key variables, which are called overlapping features, that means that are measured in both study, the main study and the external study from which I am trying to borrow information. And again, for example, in the summary statistics example, the word I think feature might be the sleep guide. Feature might be the SNP value, that which from which I want to borrow the effect of the SNP and the values, that is the overlapping variable. And the main assumption we make is that we take a reduced model, if you do not include an additional covariance, then after adjustment for the respective design variables, the effect of the Z variables is fixed. That is my transcript of the assumption. So I do have to make some assumption, otherwise, of course, I cannot make information borrowed from one study to another study. So that's the excuse. Study. So that's the we explicitly state that assumption in terms of this in any reduced model where we ignore the other covariates like that which is not measured in the external study. After adjusting for the basic design variables, these features, has the same effect in the two studies. So and this is a testable assumption because I can actually feed the model, reduced model in the, that is available from the external study and I can feed a similar model in the internal study and I can actually compare parameters etc. So it's not an testable assumption. I'll give you an example. Not antestodal. I'll give you an example like how to test that. And then what model am I trying to develop? I'm trying to develop a model actually for the main study. Like if I, you know, if I could observe A, C, and W, I'll try to develop a model. And it could be, for example, I'll focus on the GLM, like, you know, trying to focus, I don't know, like linear integration, realized linear model. And my goal is to include all of the valuables. And goal could be prediction, or we can also do hypothesis testing. I know, so the way we develop the theory so that we can do all these high tasks. Theories whether we can do all these post-selection infrared and high-dimensional high-dimensional setting. So, that's my, and so here is the population. Is it important to have a population concept? So, the what population I'm trying to develop the model for? I'm trying to develop the model for the population underlying the main study. That external study only provides information on some parameters from which I'm trying to follow. So, that's the concept of the population is important. So, this is what I call the axis of transfer learning. So, that this happens in two steps. Learning, so that it happens in two steps. One is like, okay, the first you have to assume that you have information on, oh, one more thing: that the framework, the nice thing we call transfer learning, I do not need access to the individual level data from the outside. People have created a model of certain, of certain, you know, again, it will be including the design variables and the z variables, and they can just give me the parameters and maybe answer any testing as we have. Or if I had individual level study, I can do my own analysis and consider the reduced model and do that. So that's why it's called. Model and so that's why it's called transfer learning in the sense that you do not actually need the individual level data, you transfer the knowledge to the parameters of the data which are. And then, so this is so you have to first connect the reduced model from the external study to the internal study and this is my assumption, parameter exchangeability that will be explained. And then what happens is that, so this is where we have been developing some framework that we have shown that for any model, you know, generalized linear model or even Coch's model or other more complex models. Model or other more complex model, there is a very general way of writing relationship between parameters of extended model and parameters of smaller model. And this specification of this, it is not explicit in the sense that it is not a closed-form solution. For linear regression, we know that there is a closed-form solution, right? Because from somebody's statistic data, we can try to fit a joint model, right? That is the closed-form solution. But we have shown that these equations hold for any kind of non-linear model, any kind of complex model, and these are some of the papers. Model, and these are some of the papers which we think can apply to the equations. And so, we are taking advantage of that. So, that connects the parameters of the reduced model to the target model and infrastructure. And that is how we are basically borrowing the information in the big picture over here. So, here is how the actual methodology works. So, we use this framework called generalized methodology. It's actually rooted in econometrics theory. It's very popular in econometrics and not as popular in citizens. And not as popular in statistics, but it's a very powerful tool. And actually, one of the inventor Peter Hansen got Nobel Prize for inventing this. And so the idea is that, so if you have only analyzed the data from the main study and if you're trying to fit a generalized linear model, you have estimating functions like your standard estimating functions that we try to find. And then we use that external input information and the connection of the parameters between the parameters of the reduced model and the extended model. Model and the extended model, we have this set of equations which I am not going to detail to set up another set of equations on to estimate the parameters of data by based on information that is available from technical studies. And these are very general equations. I am writing these equations in a GLM form, but you can write these in a non-GLM model as well. And so, what happens if we have another set of over-identified set of equations? Because remember, the standard score equation is the number of equations is the same as the number of productivities. But I have put down more constraint in the. I have put down more constraints in the parameter. So, what we have? We have a what we call over-identified set of equations, and that's where generalized method of moment comes. That we cannot solve this equation to be exactly 0. So, what you do is this objective function where you say, okay, we cannot solve this to be exactly 0. So, let's define a distance function and that how far it can be from 0. So, this is a coordinated distance function you can see, and where C can be a proper weighting matrix. I am going to come back to that how to choose the C in the section. So, that becomes like you know objective function. So, that becomes like your objective function to minimize, and it's kind of think of it as like a log-likelihood type of function that you work with. And then, if you want to fit a high-dimensional model, you can pretend as a pseudo-log likelihood, you can add penalties and all that stuff. Because you can actually work with Bayesian, you can actually have shown that you can work with Bayesian stuff. You can predict that as a likelihood. It's an approximation. Just one uh one last slide about Matt, and then I'm gonna go to the applications. That uh the one key parameter is to be able to choose uh what is that C matrix you can do. To choose what is that C matrix you can choose. You can choose the identity matrix. You know, but that would be a fine. It would be a valid estimator, but it would lose efficience. So you can choose any C and you still get a valid estimator, but you will lose efficience unless you choose that C properly. And the GMN theory says that the optimal C is basically should be the inverse of the variance covariance, asymptotic variance covariance of the asymptotic functions, which is kind of intuitive. You know, that we have seen this many times in standard asymptotics. But in the high-dimensional But in the high dimension, one thing there we realized, and it requires a lot of work for us to understand that these parts are, you cannot just take the inverse of the value of scoring metric, you have to farther lingularization of the gradient score metrics. And there are nice little things you have to think about how to choose that kernel, and that will depend on either the kind of regression task or a classification task, it will depend on that. What is that object of math? Okay, so that's about the kind of the method. So that's about the kind of the method. So the more recent development, what we are excited about is the fast implementation of these things so that we can actually do scalable analysis. So what we have done is that that objective function can be messy actually. That is a non-concave objective, a non-convex objective function. So, but what we took advantage of is that here, that here we already have a pretty good estimate. Suppose I had just done a standard analysis of the main study, I already have a pretty good estimate. So, what you can do when you have this kind of problem and you are trying to do a more complex optimization. We are trying to do a more complex optimization. The way to do it is that you can initiate with that good estimator, and then you can only have to get a one-step update of your optimization, your conduction type of update. And there is a theory exists that says, you know, from MLE theory, that one-step update starting with a consistent estimator gives to as efficient estimator as the full alternative update. So we are taking advantage of that kind of theory, and you show that that theory works here. And so, basically, we have a what you're proposing is that to start what you do with the standard analysis. That you start what you do with the standard analysis, and all you have to do is a one-step update, non-iterative update. So it's very fast. And then, even for high-dimensional data, like if you're trying to do a few mass models like LASSO or adaptive LASSO, we have shown that because the objective function can be now rewritten in terms of a least square problem, we can use standard GLMNIT function, which is a very powerful packet for creating high-dimensional models. You have all the options, like you know, glass floor, glass flow, glassignate. All you have to do is that you can choose your options, you just create this. Can choose your options, you just create this pseudo-outcomes X variable and then put it to the TLM, and that you can get it with. So, these are some of the recent development which has allowed us to make it very scalable. For example, if you want to do a GOAS using this kind of data integration approach, the analysis in the logistic equation is as fast as the link analysis or standard GWAS. One comment: Because you can think about it, especially if you have this kind of subset of data within a This kind of set of data within a study like the Baobank, where you have the proteomic data and the rest of the data we did not have proteomic data. The first thing keeps to mind how to handle the data is an imputation approach. You can think about how can I build the imputation model to impute the proteomic data for the rest of the rest of the code. You can do that if you have the individual level data, but it requires another assumption to build a complex imputation model. We also have not assumption, we are also using the data, the main study, to kind of think for the kind of a reference. Kind of infer that kind of the reference data, what is the relationship between your X, W, and Z variables. But we are doing a completely model free. We are not developing any imputation model. It's just the reference data being used to kind of understand the relationship between variables and that are getting very equations. That is the one advantage compared to the imputation: that we do not need the, even if we had the level data from the external study, we will not need to make any moderate assumption about the implementation solution. The cost is that if The cost is that if you are confident that you can develop a good imputation model, unless implication model, I believe that you can do the efficiency compared to this approach. Because you are proposing basically semi-parametric efficiency versus efficiency and by parametric model, right? So both of those two will be different. But it is an interesting question to explore how much efficiency lose if you can build a good implementation model compared to a completely semi-parametric approach. Okay, so let's come back to the application. So what I promised, like one of So, what I promised, like, one of the applications I was interested in, like, you know, so I have EKAN data, I want to ask certain questions about mediation in genetics, like whether how much of the genetic association for type 2 diabetes is explained by various measured risk factors, clinical risk factors. Some of them are also eatable, starting with BMI, but there are cholesterol level, smoky alcohol, all of them have been shown to be, you know, has some hitable component. And again, we are using some coherent information from the UK balance, but we UK bow bank, but to increase the power of the study, we wanted to use somebody's statistic data from the external GWAS. This is a GWAS which will not include UK bow banks. We use that data from that summary statistic from that slide, which is 60,000 case, 60,000 control whereas UK Bow bank has about 10,000 case and 10,000 controls. So that's why we wanted to take advantage of that external slide. So remember the first assumption is that you have to think about what is the assumption we are making. So the first assumption we are going to be making that. Making. So, the first assumption here we're making is that the effect of individual states, after an adjustment for the basic variables, and with meetings like the whatever genetic principle components that were, you know, an outside study and whatever genetic principle component I can construct in UK all that, conditional on that effect of the sleep size and adjustment for maybe age and sex, that's what the external study did. And we could test that. So, this is the effect of the sleep attacks in a simple logistic regression model, only an extremely. Model only based on ink about them and if you based on the summary statistics, and if you said, are they equal? And this is the QQ plot for that S. And you can see that the effects are to be homogeneous, which is not surprising that we have seen like a fortune disease, the effects size within the singular ancestry population, you know, may not be that difficult. So we move forward with the combined analysis. The first analysis, the meta-analysis of UK Biobank and that external summary statistics, the standard meta-analysis, you can see like there are Meta analysis, you can see like there are loss and loss of signal, as you know, like for type of gravity or there are lots of sensations. So, the next series of plot what I'm showing in the top is the standard meta analysis. In the bottom, I'm showing conditional GMAs. And the first conditioning has been done with respect to BMI. And the first thing you notice is that if you compare the first and seeing which association signal goes away, and the obvious one goes away, actually, it doesn't fully go away, but the largest one goes away. And that's kind of a good positive control check. And that's kind of a good positive control check. If I could not make that happen, then that obviously my method is failed. And so you can see that the F2 association is going away, which is known because F2E is an LA obesity related gene and the association with diabetes as we previously shown is mostly driven by dialysis. But nothing, you know, not much, other ones did not drop that much. And then we threw in a bunch of other variables like you know, BMI, waste circumference, blood pressure, smoking, alcohol. Circumference, blood pressure, smoking, alcohol, LDL, and then I start seeing some few more associations is going away. What I'm listing in that, what are the associations, you know, the highlighting those places where association is becoming dramatically less significant, the corresponding genes, and what are the covariates which cause that association in the model. And so, few of them started going away, but most of them still stays. And actually, one association appears that was not there because of the conditioning after trial service. That could also happen from a plus. So, that could also happen from our plastic detection model, that things can go media here. That was not the end. So, the next what I did was: so, in UK Biobank, we have baseline HBA1C, which is a biomarker for actually diagnosis of diabetes. So, because we are looking at incidence, so we are currently looking at the baseline, we are excluding people who have diabetes, but we are using their HBA1 sealer. And let's ask the question: the conditional HB11. The question that conditional HbA1C, which genetic variants are associated with the progression through type 256. So, this is a kind of a now you can use the kind of progression GWAS by using this technique using the baseline HPA1C. That if you condition on IPA1C, there will be lot of the association, and this is for incident diabetes, is going away, but there is a couple of them, especially one of them, does not work. TCF7L2. And that is interesting because actually this was, of course, TCF7L2 is actually one of the first major hits for type 2 diabetes. And this was early in a paper, they did an association between this variant and type 2 diabetes incidence among pre-diabetic patients who are participating in this diabetes prevention trial. Did not have diabetes yet. They are pre-diabetic because they have kind of lower glucose tolerance. And then they showed TLD. And then they showed clearly this is the variant which is actually associated with the type 2 diabetes incident, even condition on HBA1C. And the conclusion was that this variant is actually affect type 2 diabetes risk through not your usual pathway, which is the incident resistance, it is the incident secretion pathway through the beta cell mechanism. So you can see that condition, no, so this now we are being able to do it in a such analysis in a large scale, like what is the progression, so I can do a progression GWAS using this approach. Progression G was using this approach by combining the balance. And we found that there are other variants which are, you know, after conditioning on HDPA1C, this will predict the progression in type to diabetes, suggesting that there are limitability in the diabetes progression and beyond questions. Quick, just a next application, very quick. So, the UK Malgan propheomics example, which I read, like you know, that was a paper that just came out actually in HR aging that they found they were trying to develop the model. That they found they tried to develop the model using the UK Bald and Protealomics data to predict complex diseases in general. And they showed that generally proteinomics will have some power to predict disease, like a complex disease, including multiple cancers and heart disease and diabetes. And when they develop the model, they use the data on 55,000 tables. They asked the data, these are the data where they had these factors and the program data. So we said, why don't we, you know, because in the BAOS, actually, the other risk factor data. Like actually, the other risk factor data, which are these risk factors for the common diseases, they are available for everyone. So, why are we analyzing all the data of 50,000 people? Let us try to develop a model combining the potential data from 50,000 people and everything else of the path using the transfer architect. I'm going to go show, we have done it for many diseases. I'm going to just show two of them for simplicity. One is for breast cancer, another is for protorectal cancer. So, this shows the number of incident cases in the cohort. Cases in the cohort of 550,000 people. So you can see much smaller. And this is the number of incident cases in the much larger, it's not actually 50,000, 300,000, and 30,000 because of the exclusion criteria and all that stuff. But you can see that obviously much larger sample. And this is the number of risk factors, which are non-croteomic risk factors that when we put into the model. So these are some of the risk factors we considered. We carefully selected and went through the literature, identified the known risk factors into the quality of the score. And this is just a quick application showing like. And this is just a quick application showing, like, you know, if you do the standard analysis, if you say, for example, if you take a model vessel LASO and give you the proteomics data on the 1500 protein, and this extra analyticization really held out data set from the main study, and then if you do the combined analysis using the two types of data, we saw a consistent improvement of model performance across multiple diseases. And how much improvement will depend on how, for example, how sparse. If you got main study as modern. So, if you main study a smaller number of cases, but obviously the power is lacking, then you get bigger improvement, and then the larger number of cases are there, you see smaller improvement, but you will see consistent improvement across across different diseases. So, that brings my conclusion that, you know, so we are very excited about this kind of general procedure. It opens up various many applications. Like, you know, like, for example, we have a lot of discussion about we can analyze at mixed population, we should adjust for. Population which adjusts for a local admixture, right? And that is perfect individual level data. But then, what do we do with the summary system? You can set up a model which if I have some individual level data, I can set up the administrative model, allow for interaction of sleep interact with that local administrative component. That's going to be my target model, but then I can use my summary statistic data to integrate information from summary statistic data. I do have an application I'm interested in developing an optimal polygenic score, which could use conditional. Polygenic risk score, which could be used conditional on other risk factors because currently, as if we are developing polygenic risk scores, as if they were going to be used by themselves, right? But if we are going to be used by other risk factors, they are going to cancel some of those attacks that we are seeing genetic. So, what would be the optimal polygenic risk score for those kind of things? It can help to develop more mechanistic model, like we have a collaboration on Alzheimer's disease, where they are interested in modeling the risk of Alzheimer disease, not only in terms of who has or who does not have, their biomarkers to process observe the longitudinal process of the. Observe the longitudinal process of the disease process. And it's a small study, but I want to develop a model for a mechanistic model for that. But I do not want to ignore the information that I have from the algorithm as I get from mind that mechanistic model and what is descriptive model using this framework. It can be also extended to try and future outcomes. So with that, I think that's what I do to show that this, hopefully I could show that this is a promising framework. It's a principle framework, we've got a good theory behind it, but it's also practical, like the computational. The packet of the computational is quite scalable now. Therefore, I'm letting you all pick up the webinar. Thank you. We have time for a few questions. Lovely work. Really appreciate everything that you're doing here. A couple of questions on the program, because we share a lot of strong interests, I think. We're also very excited on. You talked about computing for the full set of UK Biobank, but just out of curiosity, Biobank, but just out of curiosity, so about 7,000 of them are like ascertained by the UK Biobank Pharma Partners. So they're ascertained for like specific, and I think like 45,000 or so are like a random set that's like representative of the rest. Have you thought about like whether that ascertainment impacts like what is going to show up in implementation? Yeah, I haven't thought a lot about how to do the incubation because we didn't want to do the incubation, but I just wanted to, you know, kind of. Integration, but I just wanted to, you know, kind of our framework does not require the implementation, that is advantage, so it does a model-free way. But yeah, it will affect. Like, you know, if you're, you know, you have to think about if it is ascertained by disease outcome, you know, then we may have to wait by the enrichment to make sure that we are developing a model for the population, not for a certain sample. So there are ways of waiting. If you do the design, if you are waiting, then we will do it to come up with the implementation model for the population. Okay, I would think that's here. Okay, we could think about that too. And then I guess a second question, on the Alzheimer's front, diagnosis varies, you know, based on a series of symptoms that seem to be showing up and we kind of have like confirmation of what's going on in the brain until a certain postmortem. So what are you training on? And are you concerned that some people are going to get diagnosed? Like, are you training on like cognitive symptoms? Like, how have you been? I'm not gotten into the project yet, but this is something I'm starting. So I I believe there is a bow part. I believe there is a bow marker people use based on spinal, you know, like measured from spine. And it's very expensive, you know, it's not easy to collect. And they have repeated observation, you know, and that helps them to understand the more latent process of the when the, because it's not just yes or no, right, it's about the continuum development process. And they have shown that, you know, the, you know, in terms of modeling other dissectors and, you know, and how the process works. And so I was kind of inquiring: like, okay, can I explain how the genetic association works? Can I explain how the genetic association works in terms of the process? But the sample is very limited. So that's the basic idea, but I've not gotten started the project yet. And we have a postdoc arriving in September. We're supposed to work on it. But that's the basic idea. They have a very tight longitudinal measurement on some bowel monitor that they believe that the you know that there it's a good measurement of the underlying process instead of just saying who has the alignment market you know things like that. So there are a lot of details we'll figure out. Efficiency you mentioned like how Green Root can compare analytical efficiency between different approaches? Yeah, so but that's the includation versus non-implutation pre-approach, you know. So that would be, we can do empirically, of course, you know, like Peter Pete will talk about a study where we'll be able to do this empirically of think of like implementation versus Think of like an imputation versus but, but you can also do theoretically, right? Because so the our method is called semi-parametric because we do not make any distribution assumptions. So we believe that we are achieving that semi-parametric efficiency bound within that setting. But I know, and then the question is that if you make a parametric assumption, then of course if the parametric assumptions are correct, we'll win it.