Participants that not all of us are quantum chemists. So I decided to make some brief introduction into quantum chemistry first to explain what kind of things we are trying to solve and what kind of things we are doing. And then probably try to say something how I see what kind of problems we have in adopting quantum chemistry to some new hardware, no matter what. Where, no matter what, but of course, the goal is to use the quantum computer. Right, so what is the problem which we are solving in our science? We're trying to figure out, we have some molecule or material, and where are the electrodes? So, what we are doing in this system. So, this is the main question. But there are some subsets. Question, but there are some subset of these questions which can be addressed because we can ask ourselves. So, if we are assuming that nucleus are fixed, so where are the electrons? Or we can forget about excitations and just look at the problem for ground state. So, this is what you are doing most of the time when I do DFT. So, these are smaller problems. And a good question: is it more complex? A good question: Is it more complicated than classical thing? Because if you use any computer, you can solve different things and you can solve problems from a classical world. Well, in a sense, they are quite similar because the interactions which exist in a quantum world and in a classical world, they are the same. You have some motion, so kinetic energy, and you have some interaction which is coulombic. You have some interaction which is Coulomb interaction, so it looks the same. But the problem is that in a classical world, you can consider all particles being independent. So you're solving your problem for each particle. And in quantum world, you have all of them all together. And that brings a lot of complexity because then we have multi-dimensional function. And another problem with electrons are so light that we can't consider We can't consider our equations as a continuum. So there are some discrete levels, and so then we have to get this quantization. Summarizing this, you can say that our main equation, Schrodinger equation, we can solve for the cases when we have two or three particles, if you're doing this correctly. Well, it gives us just a few systems like hydrogen, helium, H2, and that's it. H to plus, and that's it. And we would like to do this, we would like to apply this to everything. So, how do we do this? Whether quantization applies to atoms, then we get some solution, but most stable situation for electrons to deal on some orbitals. And we can compute these orbitals, and the lowest by energy rate looks very simple, the highest. Simple, the highest they look really fantastically artistic fades. And we need to remember one important thing: that this is an approximation. Because electrons can be everywhere. It's simply the most probable place for them to be there. And I will not have any slides about density functional theory because you can just solve equation for Just to solve the equation for the density, I will be mostly talking about solving of these equations for wave functions. But if you would like to speak about this functional theory with me, we can do this during coffee batteries. Very important thing which we use when we are trying to solve our equations is to use basis set. The concept of a basis set is very simple. The concept of a basis set is very simple. If you take a picture like this, actually, I would like to thank Sergei because he gave me an opportunity a year ago to make this picture, and you would like to digitalize this. So then, a simple thing which you are doing, you're trying to present these peaks, these mountains, by some simple functions. And you can see that I have two examples here. Examples here. I can present this by rectangles, but then I need to have done many. Or I can use some smart function and almost perfect represent this mount. So what is the difference? At the end, it's the same. I make representation of a real function by some linear combination of some basis. Linear combination of some basis functions. But there is some catch here because the number of these functions will determine the size of my matrix. So if I have quite many of them, when I have to operate with huge matrices, and this is a problem, I would like to make the system as small as possible. So, let's briefly describe Let's uh briefly describe uh Harti-Fock method, because this is one of the basic methods which is used in quantum chemistry. Uh when you instead of presenting our uh instead of looking at your uh wave function as dependent on all particles, you make simplifications. You present this function as some determinant and a nice feature of this determinant that it has some nice permutation properties. So this is simplification. So, this is simplification. And this simplification actually means that you are making your solution for some configuration. So, you are making some assumption where the electrons are sitting down and not excited. And of course, this is very bad approximation. And then you have to apply this algorithm, which is iterative. You started from density, you construct Fog matrix. You construct Fokmatrix, get coefficients, get density, and you just continue doing this. So, this is one of the problems which I will mention a couple of times in my talk, because many of algorithms in quantum chemistry are iterative. We can solve this problem. So, instead of having one single configuration, we can consider many different Many different configurations, many different possible ways of placing electrons on our orbitals. And then we can create a wave function which has a sum with some coefficients, unknown of course, we can optimize them. And then we get a solution which is very nice. Nice in the sense that if this sum is infinite and the basis set is infinite, then we have a solution. Infinite, then we have a solution which is identical to Schrodinger equation. So then it is really exact. So it's great. The only problem is that the size is infinite, so it will take some time to get the proper numbers. The biggest limitation of multi-configurational theory, that if we will try to say that we are only considering some excitations, then if we have Then, if you have a system of two orbitals and two electrons, when we have only three combinations, so it looks like we're making three Hartifock-like calculations with some coefficients, optimizing them, and then that's it, it's done. If we go to high numbers, soon we get numbers which are completely astronomical. And the problem is that even for light atoms, For light atoms, we can't use only one orbital here. It should be several. So then we are stuck again. So we need to do some approximations, and there are plenty of them. There are many methods which try to use different strategies to use a small set of these active orbitals and consider the rest in some. The rest in some way. So, many, many methods here. I just flushed this to you because you might have seen this in other talks. But my favorite would be to use this CAS-CF method together with perturbation theory, CAS-PT2 or RASP2. And there are two reasons for that. One reason, it has the same complexity. Have the same complexity and the same time when you spend in your calculations with the main problem to solve this forecast complete active space problem. And another thing, well, I simply like this method because it works most of the time. So it's really simple and practical. If we look at the typical flow of electronic structure calculation, it's the same for quite many programs. First of all, Many programs. You first of all have to take some basis set, then you compute integrals. Sometimes you make in a pack of these integrals or you make in some condensed form of them. You construct Hamiltonian. Then you solve a problem which is a linear algebra problem. You need to get some eigenvalues and eigenfunct. And then you make an iteration in order to optimize this thing. Fair. And if you use perturbation theory, it's again linear algebra. So, even from a theoretical point of view, quantum chemistry is about linear algebra. So, where the time is spent if you're running the calculation? Definitely not in integrals, because computing of integrals is a very small part. It also can take hours and sometimes days, but it's the smallest part. But it's the smallest part comparing to the rest. So I will not talk about this, I will talk mostly about these big elephants in the room. It's a computing of cost function and perturbation theory. And also it's important to mention that it is not only about IO because amount of, sorry, not only about CPUs, there is an IO which is quite big and you also need to have a lot of And you also need to have a lot of memory to run these corporations. And before I go to some practical slides, I just would like to mention this, that quantum chemistry, unfortunately, has different ways to define what is big and what is small. Because your system can have a lot of atoms, but not a small basis set, or it can have a lot of orbitals which are important. Lot of orbitals which are important. So you can make it big in different dimensions. And this is a problematic for computer science guys because they would like to take one example and profile this and get some nice improvements in that. You can't do this here. You need to have several examples. And I have my favorite benchmark set. It's eight different particles. Started from small one, it's chromium. Started from a small one, it's chromium 2, one of my favorite. Fuller ends and some bigger things. And by color, well, you hardly can see the difference between yellow and green here, sorry for that. But you can see that this system sometimes were big in terms of the number of atoms or basis functions, sometimes we are big in terms of the active space. And I like these examples because we are reasoning. And I like these examples because they are reasonably lengthy. It's a couple of hours to run them. So I can do some benchmarking. And I will do this benchmarking in a second. A tool which I like to use all the time is called Calgreen. Really fantastic thing, but it's painfully slow. I started to run these calculations a month ago when I know that I'm coming, and still there are some empty spaces. Still, there are some empty spaces here. But what we can see here, if we especially look at some large calculations, that almost all of the time, 90% of the time, is spent in linear algebra. And this is DGM functions. So I wrote here, if you forgot what is DGM here, you have multiplication of matrices and summation of matrices. Also, you have some DGM. Also, you have some DGMV, which is matrix to vector, and DAXV, just vector to vector operations. So, with time spent in mostly in matrix-to-matrix methods, if you do cast PT2, perturbation theory, it's the same thing. Well, you see, it's almost empty here. I have to wait another month to have a stable clue. But the same story for large The same story. For large systems, it's again the militation of linear algebra. But there is one problem here. If you look at what kind of sizes of these matrices you are trenching, then you will find out this is a size of one dimension, another dimension, and the dot size and the color just telling you how many of these poles you have. Then you can see that the domination. But you can see that the domination here is by matrices which almost look like a vector of the matrix. And the optimal thing would be to multiply big matrix to big matrix. So it should be here in this corner. But there are very few calls for that. So we need to have some efficient implementation to matrix to vector or toll Tall metrics. We try to use our code multi-swift GPUs, and then there is a big problem here because you have to transfer the data between CPU and GPU. And there are two approaches. One is of course optimal. You rewrite your code completely and then you process everything from GPU. But there is a lazy approach, which I would prefer. Lazy approach, which I would prefer. You need to make a matrix multiplication, you transfer this to a GPU processor, and then you check it back. Well, basically what you have to be careful here, you don't want to do this with very small matrices because you are wasting time, and you don't want to do this with very huge matrices because GPUs usually have much smaller memory per code. So we can do this, but then the gain, because mostly of the sizes which are used in the calculations, is pretty small, something like 10%. You don't want to do that. A good news that as I heard that NVIDIA soon will release some access from from memory from between GPU and CPU, because then it will be a really big change. Big change. A few words about parallelization because this is another way how you can speed up your code. Our code is mostly written in Fortune, but it has some access to memory and parallel UTLs through C. And what is usually good for parallelization, if you have a loop and then you're computing something. It's really a nice thing. But what is really bad, if you here have a call to a function. Here, have a call to a function, especially to a function to a linear algebra. Because in this case, you are killing parallelization inside this routine. Some numbers which you can see about benchmarking. If we are using cluster serve code, sometimes we have very nice polarization, very nice scaling. So each row here is increasing. Here it's increasing the number of cores by two, and you can see that it's follows until it reaches some moment when there is no any gain, because then you spend more time on interaction between different cores. But for CASPT2, it's another story because sometimes you can see that there is no any difference between running on one core and on two cores. Core and on two cores. And this is because of code which sometimes doesn't have efficient polarization, so it's keeping one core as a master. But there are some cases where you don't get any gain by using polarization. So this is a complexity of quantum chemical codes. If you use polarization together with linear algebra, for instance with m ten, For instance, with MKL. Then you need to know that as soon as you run code in a parallel, then MKL is setting OMP num threads to one because there is no any other resources left. Or it's assumed there is no other resources left. And I was trying to make some checks what is better to do polarization in the code, in the loops, or to do pluralization in linear algebra. Linear algebra level and the result is the same. It's about the same. So you can't get bored by it. I'm trying to have some conclusion, leaving some time to discussion. So why I'm skeptical about the idea to use quantum chemical codes and move it to any new hardware? Well, this old big These old big codes were a little bit under source code. For instance, if you take MOCs, it's 66 megabytes of source code. You can't rewrite this easily. And if you look at another challenge, if there are some new codes which are popping up quite frequently, they are nice, they are new, sometimes written in Python. But the problem is that they are supporting only trivial cases. And then it's difficult to use them. And then it's difficult to use them in real scientific application. Well, if we look at some new algorithms which are used nowadays, like stochastic CI or divide and contour like DMRG kind of techniques, they are still a little bit new and not so popular and not very efficient actually. And if I'm looking at the algorithms, If I'm looking at the algorithms, the biggest challenge is that our algorithms are numerical, that they depend on these basic sets and so on on these matrices which were constructed. And another thing, it's done in iterative procedure. And there is not many of obvious bottlenecks because this code our codes are old, so a lot of bugs and a lot of problems were fixed already. Problems for fixed solar. And finally, since we are talking about pretty large matrices, they all require a large amount of nerve. Some optimism which I have is based on the fact that most of the time we are spending our resources in doing some linear algebra problems. So it's not our specific problem. So someone in some good days Someone in some good days will solve it. It's a general mathematical thing. Probably one thing which can be done is to reduce the dependence on iterative procedure. So if we can find some coefficients instantly, probably it would be a good thing to do. And for instance, we can have a better starting point, for instance, node variational. And also, And also, what was mentioned by James that some use of fragments and some use of embedding also can be helpful because when you're trying to reduce the size of your real problem. Or something completely new, I don't know what. So finally, thanks for Maltus and OpenMoltus. If you don't know the difference, then of course we can chat during the coffee. We can chat during the coffee. And I think that I have five minutes switching off. It's fine.