He's a research staff member and manager at the IPM Research AI working for the Watson Research Center in New York State. He co-directs the IPM Science for Social Good initiative and is also a data ambassador with DataCline that is also in New York. He received the PhD degree in 2010, also in Electrical Engineering and Computer Science at the Massachusetts Institute of Technology, and his research focuses on domains of statistical signal processing, machine learning, data mining and imaging processing. Data mining and imaging processing. And I think today he'll be talking about interpretable machine learning for safety and teaming. Where I guess he'll be introducing some of the panelists also about his book on transport TV question. Kush, the floor is yours. Great, thank you. So let me start sharing my screen. All right. Okay. Yeah, thanks for. Yeah, thanks for inviting me to the workshop. I'm apologetic that I'm not there in person. Some family reasons prevented me from being there. I would have really liked to be there. It would have been great for at least my mental health. And I'm sure interacting with folks in person would have been great. So yeah, so the book that I just published two months ago, it's titled Trustworthy Machines. It's titled Trustworthy Machine Learning. The PDF is available for free at trustworthymachinelearning.com. And then, if anyone wants a paperback, it's available at the lowest cost possible just to cover the printing charges on Amazon. All right. So, the title of the talk is Interpretable Machine Learning for Safety and Teaming. So, let's just dive in. So, it's going to be somewhat of a high-level talk. So, not a lot of technical details, but A lot of technical details, but hopefully at least present some of my vision of where things are and could be. So, when we use this term responsible AI, there's different aspects to it. There's kind of AI ethics, there's trustworthy AI, and then AI governance. So, the way we've been kind of thinking about it and using the terminology is where AI ethics is more about what should be done. Is more about what should be done. So, the principles and values, then trustworthy AI being the techniques and algorithms and software to actually instrument that. And then governance being kind of the broader operationalization, so the processes to do this in actual organizations. So, going further on this, so on the trustworthy AI, we're seeing this as big. We're seeing this as becoming very important because of a lot of critical workflows in various industries and sectors now using AI. And in particular, with the aspects of trustworthiness, there's specific reasons as well. So there's aspects of brand reputation, there's increased regulation. So the folks Regulations. The folks who are in ban from Europe are, I'm sure, well aware that there's a high-risk AI set of regulations that will soon be enacted and so forth. And when we talk to customers and clients, we see organizations that have hundreds or thousands of models in deployment that they want to make sure are well tested and well governed and so forth. And then over the last couple of years, there's been a renewed focus. Couple of years, there's been a renewed focus on social justice in many societies around the world. So, when I say trustworthiness, there's probably this question of what does that even mean? So, one source of a definition of trustworthiness comes from the organizational management literature. So, they've identified kind of four attributes of what it takes for a person or Of what it takes for a person or by extension an AI to be worthy of trust, right? And it's important that I'm saying trustworthy, not trusted, because there is a distinction. So people who aren't worthy of trust could be trusted, people who are not worthy of trust or who are worthy of trust could not be trusted, and so forth. But in terms of what does it take for a person or an AI to be worthy of trust, so the first Of trust. So the first attribute is something like competence that the person is able to do what they say. Second is some sort of reliability so that that competence sticks around in different conditions, different settings, and so forth. Third attribute is some level of openness or intimacy so you can communicate back and forth with the other person. And the fourth is some level of selflessness so that they're working for goals beyond just their own, right? And all of these map All of these map to different sorts of desiderata for machine learning systems. So, the first attribute mapping to accuracy, second attribute mapping to distributional robustness, to fairness, to adversarial robustness, third attribute mapping to explainability, to communicating of uncertainty, to broader transparency of overall development life cycles, and also. And also to value alignment, which is humans instructing machines on what they want. The first three being the machine communicating to the human, the last one, the humans communicating to the machine. And then the fourth attribute being the use of AI for social good, but more importantly, empowering people, no matter what situation that they're in, to be able to use AI for their own goals. So, if we think about these four attributes, one way to split them up is looking at the first two as means for safety, and then the second two being related to teaming or collaboration between humans and machines. And the way I'm structuring the talk is to kind of describe interpretable machine learning in the two ways. Uh, in the two different ways. So, what is what are the important aspects from a safety perspective, and what are the important aspects from a teaming perspective? And I don't think they're the same. All right. So let's start with safety. Okay. So safety is a commonly used term in lots of engineering disciplines, and it usually means some sort of absence of failures or the absence of conditions that render a system dangerous, right? So we often So we often hear safe or safety in front of other things like safe food, safe water, safe roads, safe toys, safe neighborhoods, all sorts of things. And each of these domains has specific design principles and regulations that are only applicable to that. So a safe toy might be one that doesn't have lead paint and it doesn't have small parts that a child would swallow. A safe road might have a maximum. Might have a maximum curvature and a smooth surface and things like that. A safe neighborhood might be one that has low crime, etc. So these aren't definitions that could be broadly applicable. But there is one such definition, and it's by Moeller from 2012. So what Moeller says is that we can define We can define safety more broadly, more abstractly, based on the concepts of harm, risk, and epistemic uncertainty. So, what is a harm? So, any system that we're talking about is going to yield an outcome based on its internal state and the inputs it receives. And that outcome could be either desired or undesired, right? And there's some costs associated with those outcomes, and presumably there can be many. And presumably, they can be measured and quantified maybe with difficulty, but in some sense, right? So, an undesired outcome is a harm if its cost exceeds some threshold. And unwanted events, so any event that's of small severity is not counted as a safety issue. So, if you got the wrong movie recommendation, even if it's undesired, would not be considered a harm. Not be considered a harm good. All right, so then moving to risk. So we don't know what that outcome is going to be, but if we know its probability distribution and we can calculate the expected value of its cost, then the risk is the expected value of the cost of a harm. And in contrast with epistemic uncertainty, we still don't know what the outcome is going to be, but we To be, but we also don't know its probability distribution. So, epistemic uncertainty, which you can contrast to a different type of uncertainty known as aletoric uncertainty, results from the lack of knowledge that could be obtained in principle, but might be practically intractable to gather. So, we're not going to debate whether all uncertainty can be captured probabilistically or not, but we'll make Or not, but we'll maintain a distinction between risk and uncertainty following what Mueller has said. Okay, so what is safety then in terms of these three things? So safety is the reduction or minimization of both risk and epistemic uncertainty of harmful events. So first of all, costs have to be sufficiently high. And more importantly, safety involves reducing both the probability of expected harms and the Probability of expected harms and the possibility of unexpected ones. Okay. All right. So risk minimization is the way we've all learned statistical machine learning. This is just a busy slide, not to actually go over it, but just to say that's exactly what we do typically when we're doing machine learning. But it's epistemic uncertainty that we haven't had the most. Had the most focus on, right? So, risk minimization doesn't capture the fact that there could be epistemic uncertainty. And it's not always the case that the training samples are drawn from the true underlying probability distribution that we would be dealing with in operation. And more than that, the distribution that those samples came from cannot always be known. And if we're training on a data set from a different We're training on a data set from a different distribution that can cause a lot of harm, right? And even if the training data is drawn from the true underlying distribution that will be in effect in operation, training samples might be absent from large parts of the space due to small probability density there. So those are all reasons why we can have epistemic uncertainty. So coming back to what Mueller and friends talk about in Talk about in general engineering systems, they mentioned four ways of achieving safety. So there's inherently safe design, which is excluding a potential hazard from a system. So an example of that is filling blimps or georgal airships with helium instead of hydrogen, because then there's nothing to ignite and explode. And there's a picture of the Hindenburg on the right to illustrate that fact. Illustrate that fact. Then there's another strategy which is called safety margins. So, a system that is stronger than it needs to be for an intended load. So, hurricane resistant windows are an example. So, stronger than the wind that you could expect. Then there's safe fail. So systems that remain safe when they fail in their intended operation. So, an example of this is a dead man switch on a train. So, if the train engineer lets go, the train stops. Train engineer lets go, the train stops in its tracks. Presumably, they're hopefully in a safe situation. And then there's procedural safeguards. So these are things that have more of a human factors orientation. So certifications, warning notices, and so forth. So how do we take these and bring them to AI? So on the inherently safe design, we can bring in directly interpretable models and causal modeling. Interpretable models and causal modeling. In terms of safety margins and safe fail, things like uncertainty quantification and selective classification have a role to play. And on procedural safeguards, transparency has a role to play. So very quickly on these last three, the safety, margin, safe, and procedural safeguards, before I jump in in greater detail for the first one. So if we're computing So, if we're computing uncertainties on, let's say, an AI system that's counting how many people are in a crowd, if we can have a good estimate of the uncertainty, then we can limit attendance below the venue capacity and stuff. So that's an example of a safety margin. If we're talking about, say, fail, again, using uncertainty to help us. So, this is a dermatology example. Dermatology example. So, if the machine is right, so this is an eighth-class problem trying to figure out what disease, what skin disease is shown. So, even if the machine is right, which means that it has the tallest or the highest confidence for the correct class, if it doesn't have good confidence, that means it thinks all of the classes are somewhat likely, then it's less confident and we would. It's less confident, and we would want to pass this off to a human expert rather than in a more confident situation where maybe the machine can act more autonomously. And then there's transparency as a procedural safeguard. So we've been working on this concept of AI fact sheets. So throughout the development lifecycle, if you collect both qualitative and quantitative facts about what's going on and then render those out for appropriate Those out for appropriate users in the ways that they need to receive these facts, then they can notice issues and so forth, right? Put in governance frameworks. So those are all, I mean, safety mechanisms in terms of the second, third, and fourth ways of approaching safety from a perspective of minimizing both risk and epistemic uncertainty. But the one that I want to say more about is inherently safe design. Inherently safe design and directly interpretable models. So here we see a DNF. So it's for the task of predicting or deciding who should receive a home equity line of credit. So this is a fairly simple rule. So the number of satisfactory trades being greater than 23 and an external risk estimate being greater than 33. And an external risk estimate being greater than or equal to 70, and the net fraction revolving burden, which relates to the fraction of the debt that you have being less than a particular number, or another clause which involves the same things, right? So this is something where we as people can look at it and know that there's no spurious thing that's getting pulled into the model. Into the model, and that it is kind of doing something that makes sense, right? So it's not doing something that could be harmful in a way that we would recognize. So this was actually a model learned from training data using an algorithm called Boolean decision rules via column generation. And there was an open challenge, the FICO explainable machine. Challenge, the FICO Explainable Machine Learning Challenge a couple of years ago, for which this was the first place winner. It was developed by my colleagues Sanjeep Ash, Octa Gunluk, and Dennis Way. So this rule set sort of work based on column generation is part of a larger agenda that we've been pursuing at IBM Research on these sort of rule learning approaches. So it started back in 2013 with an ICM. Back in 2013 with an ICML paper on learning single Boolean rules. And then over time, there's been a bunch of other things that we've done, whether it's on scoring systems or kind of these full rule sets with extensions to generalized linear rule models, application to causality with overlap regions or adding in fairness and so forth. Right, um, and I believe, I mean, this is very important exactly for the safety aspects of it, okay? Um, so these are the references that go along with this, and the slides are available, so if you want to check out the references later on, um, so just very briefly, I think most of you already know this, but um, just to repeat it in case that you don't. So, finding compact decision. So, finding compact decision rules involving few Boolean terms is an NP-hard combinatorial optimization problem, and the older methods that are out there are heuristic in some way. So, over the last eight or 10 years, there's been a renewed interest in rule learning driven by more principled optimization objectives. So, the way we started working on it was Working on it was kind of relating it to what's called the group testing problem. So, the idea being to discover a sparse subset of faulty items in a large set of mostly good items using what are called pooled tests. So, the first time this came up was around World War II, and the US Army was testing recruits for whether they had syphilis, and they couldn't test each recruit separately. Couldn't test each recruit separately because it would have been prohibitive. So they just mixed together the blood of multiple recruits together. And then if the test came back positive, the logical or of all of those army recruits had syphilis. And then if you do this smartly, you can get away with the logarithmic number of tests rather than a linear number if you tested each recruit separately. So, how does this relate to learning Boolean classifiers from training data? So, if we start with a standard supervised binary classification problem where we have features X and labels Y, right? The way to do this is we construct individual Boolean clauses from features. From features. So, if you already have a categorical feature, you can, I mean, just create a bunch of Boolean terms from that, like something like a compensation plan equals quota-based. So that would be an example. If you have a continuous dimension, then you threshold it into various sort of thresholds. So if the number of satisfactory traits is greater than or equal to 23, then this feature is true or it's false otherwise. True, or it's false otherwise, right? So, by doing this, you can create an entire truth table matrix A with individual entries. So, the rows are your training samples and your columns are these Boolean features. So then the positive training samples are now equivalent to a diseased pool of Army recruits. And the goal is to determine a coefficient vector. And a coefficient vector w that specifies which Boolean terms to OR together in a decision rule to recover those positive samples. So it turns out to be a Boolean algebra setup. So instead of a normal linear algebra thing where you have y equals a matrix vector product of A times vector W, here it's Y and you want it to be equal as close as possible. equal as close as possible to this Boolean matrix A. And then this notation which we have here with this vector, also Boolean vector W. So it's like a normal matrix vector product, except wherever you have a sum, you have an AND, and wherever you have sorry, wherever some, it's a logical or and where you have a multiplication, that's a logical and, right? So that was the starting point of how So, that was the starting point of how we got into this. And then, with the scoring rules, there's something called semi-quantitative group testing. That's what we used. And then over time, we kind of got more and more into sort of advanced integer programming techniques that led us to the place that we're at now. All right. So, that's on safety and kind of using kind of rule sets and Kind of rule sets and other simple models in that fashion. So, the newest work that we've done, it's currently under review, is on actually taking these ideas a little bit further. So, it's a mathematical formulation for assessing the safety of a supervised learning model. And the way we do it is by computing the maximum deviation over a certification set, so a set of possible input features. Input features and seeing how a given model deviates from some other model that we already know some other way is safe. And it turns out that when you try to compute these maximum deviations, so an interpretable model like a decision tree or a rule list or a generalized linear model or a generalized fatal model and so forth, the maximum deviation can be computed. Maximum deviation can be computed exactly and efficiently. And moreover, these interpretable models produce tighter bounds on the maximum deviation compared with black box functions. So what we are able to show in this work is that we can certify a very nice kind of safety because of the fact that we have interpretability. So I think this is headed in a in a really nice direction. A in a really nice direction. So now moving on to teaming, right? So this is a completely, in my opinion, different sort of thing that we want than safety, even though both together combine to yield trustworthiness. So there's this interesting quote: when you create a human plus AI team, the hard part isn't the AI, it isn't even the human, it's the plus, right? The plus, right? And I was actually, so my son, he's in kindergarten. He brought home this book from the library a few weeks ago. It's titled Jeremy Thatcher Dragon Hatcher. And as I was reading it to him, it was really interesting for me to actually think about, I mean, teaming and stuff, right? So there's this dragon that's newborn, and this kid has. This kid has to take care of this dragon, and the only way that they can communicate is through telepathy. And it's a completely different way of thinking for the boy, where he has to kind of think in images and actions of images. And that's the only way the dragon can go back and forth and stuff. So it's kind of, but they had to figure out this new like plus and so forth, right? All right. So. So, when we think about the bigger picture of trustworthy machine learning, this is a diagram that I put together while I was writing the book. And the most important part will come at the end for what we're talking about here. But I just want to kind of take you through this just for general interest, right? So, there's these different spaces across the top and different biases that can come in. That can come in that threaten different types of validities as we go along. So there's a construct space. This is kind of an ideal world. And because of social biases and features, as we define them, it can lead to certain types of threats to what's called construct validity. So the thing that we're measuring isn't what we actually care about. About right, so that's that gets us into an observed space where we have the features that we want to measure, and then we actually have to measure them through sampling of individual people or whatever. And that can have representation biases, temporal biases, and so forth. So that threatens what's called external validity. So that means that whatever models that we do develop might not work for a different Develop might not work for a different population or different setting. And then, once we've collected the data, we have the raw data on which we have to do some data preparation steps, some feature engineering and things like that. So that can introduce its own biases from the data preparation itself. Also, there's scope for some sort of data poisoning attacks to happen at that point. And that threatens what's called internal validity. What's called internal validity. So, the fact that you kind of did something wrong, right? Then, once you have your prepared data, that's when you do the modeling. So, train the model and stuff. So, there's the possibility of underfitting and overfitting or having a poor inductive bias, which threatens generalization, which I think all of us are well aware of. And then, this, I think, is the interesting aspect. So, once you have the predictions, Once you have the predictions, they have to be communicated to a human who's perceiving those predictions, right? So there's the opportunity there for human cognitive biases to threaten the teaming, the human-machine collaboration, where the humans misunderstand what the predictions actually are. So, in my view, the human-machine collaboration really requires Collaboration really requires communication. So, that interaction is mainly a communication problem, and we have to deal with the so-called last-mile problem. So, once you have a really good machine learning model, still and it's fair and it's robust and all of that stuff, you still have to get it to the human. So, overcome that last mile to reach the human. And that end consumer of model predictions is a person and. Is a person and they have their own local observations, they have their own cognitive biases. And so, in my view, model explainability from the teaming perspective is a problem of communicating some sort of quantized random variable that the human consumer might fuse with their own information to make a final decision. All right, so if we're thinking in that way, so we have a machine. So, we have a machine and we have a human. So, the machine has some features X that are conditioned on the true label Y. The human has some other features X2 that are also conditioned on the true label Y. And the machine is communicating something to the human, and that's this U. So, this could be just a binary label. So, that would be the prediction. So that would be the prediction itself, or it could be more than that. And then the human combines it with whatever they know to produce the final prediction. All right, so there's this picture that a lot of you have probably seen. It comes from DARPA's XAI program. And it nominally says that there's some trade-off between explainability and some. Explainability and some sort of learning performance. And what I'm going to analyze right now is that trade-off true or false, in particular in the teaming scenario. And I'm going to use information theory to do that, given that we're looking at this kind of as a communication problem. So specifically, we want to treat We want to treat this as what's called a distributed detection problem. So we'll model the output of the machine learning model as a multi-level quantizer. And when it gives us just two levels of information, so one bit of information, that's a black box model. So just got the loan or didn't get the loan sort of thing. And then if it gives us more information, so more than Information, so more than two levels, more than one bit of information, but not too many, then it's an interpretable model of some sort, right? So we're kind of abstracting things away. So what we're going to do is analyze the overall accuracy of the human and machine collaboration, not just the machine in isolation. And what we're going to try to prove is that the system with more than two levels, so the interpretable model has a higher, what's called churn-off information. What's called turn-off information and thus higher accuracy. So, when we do this through what's called distributed detection theory, we start off with Bayes' optimal decision rules. And in classical detection theory, without the distributed aspect of it, the assumption is that there's complete observations available at a central processor for the decision making. But here, in distributed detection, But here in distributed detection, we have one node that's processing its inputs and then passing on some compressed thing to a fusion center that makes the global decision. So we could call this kind of like a tandem architecture where there's a machine doing something and then passing off something that's not its own full observation, but something else to a human, right? Something else to a human. So again, same setup, binary classification with labels Y, features X1 that are observed by the machine, X2 observed by the human, and they're independent conditioned on Y. And this conditional independence is to make the math simpler, but it's not actually required, right? And the U here is an optimally quantized version of an optimal classification. Optimal classification based on the features x to some number of levels k, right? And y hat is the final classification based on u and local observations x2. All right, so the theorems that we've proven are kind of proving what I wanted to prove. So if you have more levels k prime than some other number of levels k then you K, then you have more turnoff information in the version with more quantization levels. And because the best achievable exponent in what's called a Bayesian probability of error is described exactly by this Chernoff information, what it implies is that the probability of error in the two-node network, as we've described it with the machine and the human, with k equals 2, so the k equals 2, so the black box model is larger than the network with k prime greater than 2 quantization levels, so the more interpretable sort of thing. So when we are doing the teaming, this trade-off is completely false, right? And I do want to mention a couple of limitations, but I do also want to make sure that this is very clear as well. Is very clear as well, right? So, a couple of limitations. So, we don't intend to imply that more quantization levels lead to more interpretability because after a certain point, more information is just too much. As I mentioned, this conditional independence of conditional independence of the observations was an assumption for the analysis, but it doesn't change qualitatively the message and it does. And it does assume that we're dealing with probability distributions, not samples. Again, it's to allow the analysis to happen, but there's nothing fundamental about us being in that population setting other than the fact that it yields an optimal accuracy. But the more important point is that the abstraction that we did to get to this conclusion. We did to get to this conclusion doesn't differentiate between a truly interpretable model that people can relate to and some other type of quantization, some other type of bits that are getting transmitted. So what that really calls for then is to think more about what is it that we mean when we say human-centered explainability. So this is a chart that Vera Liao put together. Vera, Liao put together. And I think it's really nice to think about what is human-centered explainability and what are the types of things, what are the questions that we're trying to answer. So there's questions of how, of why, of why not, of how to be that, so how to be a different prediction, how to still be this, so maintain the current prediction, and what if. And what if? So, what needs to change or what changes according to a different sort of change of input, right? So, again, what Vera has put together are kind of like going beyond just, I mean, the high-level question into what are the ways to explain that are related to it, and then what are example methods that. Example methods that go along with those questions. So, I'm not going to go read through this, but you can check it out yourself later. But I will give a few examples of these. So, I'll do a how, I'll do a why, and I'll do a how to be that. So, let's start with how. So, this is a brand new method that we just published. Method that we just published at NERIPS 2021. It's called CofferNets, which stands for continued fraction networks. And this is based on this idea of this construction called a continued fraction, which is where you have a number and then you add to it another number which has in its denominator another fraction of the same form. Fraction of the same form continuing down as far as you want to go. And these continued fractions have been around for a long time. So, in ancient India, ancient Greece, I mean, lots of people have studied these for a long time and they have lots of favorable properties, right? But no one had thought of using them in this particular way as an architecture for a machine learning sort of thing, right? So that's what we did. And it turns out that Turns out that because of the specific form of these continued fractions, without approximation, you can directly recover sort of partial dependence sort of relationships. So like how does a feature affect the output, right? So when we normally talk about partial dependence plots and stuff like that, there is some kind of approximation. Some kind of approximation. When we are looking at generalized additive models, that goes away. There's no approximation anymore, but the form is where the different features can't interact, right? Other than through addition. And so what these coffer nuts allow us to do is actually have more interactions through this continued fraction sort of representation, but due to But due to this very special set of properties, things that are known as continuance, we're actually able to get this in closed form without approximation. So that's one way to do how. Then there's why. So this is an example of illustration of this approach called Protodash that's implemented in the AI Explainability 360. Explainability 360 open source toolkit and its demo that's online. So here we have different prototypical examples, some that are close to a given input and some that are far from a given input. And then if you go through different features, you can see kind of what is similar and what's different. And by that, gain some understanding of what's going on, right? And this is the same data set, by the way, that. The same data set, by the way, that HELOC data set from the faculty explainability channels that I'd shown the decision rule for before. And similarly, there's another demo that we have on the AI Explainability 360 website for the contrastive explanations method. So this is a how-to-be that sort of question. So if Julia was denied her home equity line of credit, what does she need to do in terms of In terms of changing these features, so increasing consolidated risk markers from 77 to 82, number of satisfactory accounts from 10 to 13, and weighting and so that the worst deliquency score goes from six to seven. So this is a guarantee. So these changes will, they're the minimally sufficient changes that will change the outcome in the model. And so again, The model, and so again, there's no approximation involved here, which I think is a good thing because when you're looking at things like Lyme or SHAP and so forth, there's some level of approximation. So the model might be doing something different than what the explanation is doing. But here they're completely in tune with each other. And one more thing I'll say before wrapping up. So when we are talking about teaming. So, when we are talking about teaming, there are some other considerations that we've been thinking about recently as well. So, one of those is why not just ask, right? This seems like such an obvious thing to do, right? So, why not ask the user population for what sort of explanation would make sense to them, right? So, we've figured out some ways to actually have training explanations. Training explanations to go along with training features and training labels so that you can then learn to predict from just a set of features what the predicted label and the predicted explanation would be. And if you do this with the user population that you're going to be serving later on, then those explanations would be in the same language as what you need, right? Another thing, given that what we're trying to do is overcome cognition. Given that, what we're trying to do is overcome cognitive biases, such as anchoring, is we've shown a way to overcome some of those biases by playing with how much time a user has to incorporate the information that they're receiving from the machine. And this is not from us, but by the same summer intern that we had who did this time and debiasing sort of work. Devicing sort of work. So her name is Char Virastogi. So just last week, she and collaborators at Carnegie Mellon came out with this really nice paper that I am reading again and again this week. So it's kind of analyzing what are the complementary strengths of humans and machines as decision makers in terms of what are the task definitions, how do they Task definitions: How do they represent inputs? What sort of internal processing are they able to do? And how do they produce outputs? And they've gone beyond just a qualitative description. They've put down really nice mathematical formulations for a bunch of these things. And so it sets the stage to actually have optimization approaches that would have good complementarity. Good complementarity. All right, so to conclude, again, what I have tried to impart is that trustworthiness involves four attributes, of which two are very intimately related to safety. So risk minimization and minimizing epistemic uncertainty. And then there's the aspects of teaming, which are, in my view, separate concerns. So communicating. So, communicating back and forth is a very important aspect of that. And that's where explainability that comes up in other ways is relevant. And there's more things related to value alignment and values themselves that are also relevant. So that's kind of what I wanted to cover, and happy to discuss more. And just some links to our open source toolkit. Just some links to our open source toolkits that are relevant for these topics. So there's AI explainability, 360 uncertainty quantification, causal inference, and AI fact sheets. So let me stop there. I have a question, so thank you so much for the presentation. Thank you so much for the presentation, it was great. I would like to hear your opinion about the challenges for trustworthy AI in settings like, for instance, Federated Learning. So where basically you don't have access to the data, you are only sharing the parameters and some access to serving. So yes, what's your take about that? Yeah, I think federated learning is actually Federated learning is actually helpful for a lot of things, right? So, I mean, by its very nature, it's helpful from the privacy perspective, which is another aspect of trustworthiness, which I didn't bring up very much. But yeah, I mean, when you're talking about these other sort of things like fairness and robustness and so forth, there's, I mean, definitely people who are figuring out ways to still do bias mitigation when you're, let's say, Do bias mitigation when you're, let's say, in a federated setting. So, some of our colleagues at the Omeden Research Center of IBM just had a paper, I think it was probably last year now, that kind of talks about bias mitigation in a federated learning sense. So, yeah, I mean, I think if you're smart about it, you can do a lot of these things in that setting as well. And people are doing it and being People are doing it, and being federated already gives you advantages. So, I think that from a trustworthiness perspective, so I think that's great. If not, I have a question. So, like, I was thinking about this theorem that you introduced before about the trade-off that doesn't exist, this trade-off, right? When you pair This trade-off, right? When you pair humans and AI. This graphical model with the machine and the human. And I was thinking that the human might actually have more variables influencing his decision making. Yeah, if you have a comment about that, like what could impact and what kind of biases might actually, if they might actually affect the community around this. Yeah, no, that's a great question. So I'll say a few different things. So one is that Few different things. So, one is that actually in the picture, it doesn't matter which one's the machine and which one's the human, right? You could swap them and the result still holds, right? We just label them because that's typically how it's done, where the machine is providing input to the human who makes the final decision. But you could imagine the opposite, where the human is providing input to the machine who makes the final decision, also. But more to the point, yeah, so if there's sort of Uh, sort of, so the X1 and the X2, right? So those features, um, uh, so yeah, I mean, if there's something that where the X2 is not particularly informative, right? So if it's completely non-informative, then that inequality just goes to the equality part. So both, I mean, all the settings are just the same if there's no additional information that the Same if there's no additional information that the in the x2. But if there's even a tiny bit of additional information that's in the x2 that's not in the x1, then you do get the inequality part of it to happen. And so the last paper that I mentioned from Charvi and collaborators, I mean, on the input side, I mean, they actually go through and talk a lot about why they're. Lot about why there will be complementary information between a machine and a human. And so I don't think it's unreasonable to think that there's going to be some difference, there's some positive thing in the X2. So do you think that that might affect the validity of the theorem, or do you think that the theorem could still hold a validation? Relies on their using the optimal decision rule. So, yeah, I mean, the theorem should hold. I mean, what happens if you're in a finite data setting and the human is biased? Then, yeah, there might be a little bit of tweaking that we would need to do. So it's a good question. I haven't thought about exactly what the tweaks would need to be, but I don't expect it to completely fall apart. Completely fall apart. Thank you very much. Yeah, I'm going to come over here so that you can't hear me. Can you hear me? Yeah. You talked about risk and harm and epistemic uncertainty. Uncertainty. And of course, it's different from our empirical uncertainty, but there's also something, at least in the climate risk modeling, which is normative uncertainty. So that what is considered harmful or undesired would maybe differ and take healthcare from what the patient thinks is undesired or the undesired or the hospital manager thinks it's undesired. Yeah. Yeah. Yeah. No, I fully agree with you. Yeah. So I didn't have time to bring that up, but yeah, I mean, Up, but um, yeah, I mean, there's a whole process of value alignment, right? So, um, uh, who decides what is a harm, what is the loss function, what is the concern, right? And you can think of that with a single agent and people very obviously, including people who have had lived experiences of those harms themselves. But then there's also the question of Also, the question of what if there's a committee of people who are together figuring out for and at the end of the day, there can only be a single AI system that's deployed, right? So that's a very unresolved problem. So people have worked on, I mean, kind of preference elicitation in various ways. And one of our colleagues at IBM Research, Francesca Rossi, she does a lot on this value line problem using some. This value line problem using something called CP nets, conditional preference networks, and so forth. But the question becomes: if you have more people, then all the existing techniques that are out there eventually boil down to a voting scheme. And so the majority rules, right? And I don't think that's always the best thing to do because, again, if you're in the minority. Again, if you're in the minority, but you have some unique experience that tells you, oh, this is really harmful or whatever, you shouldn't get drowned out by just a voting scheme. So how to do preference aggregation across multiple individuals is something that I think is a very open and interesting problem. You can take a more kind of participatory design approach, just have everyone sitting in a room together and then together try to do the solicitation. Together, try to do the solicitation, but that's not always possible, right? So, is there any other way to do it? And I think I want to work on, I mean, problems like that going forward. So, yeah, I think there's a lot to be done. You're on mute. I'll have to speak up. Yeah, sorry, I was thinking about the very unique Speaking, but I was talking very, I think we have time for one questions. Okay, do you hear me? Yeah, I can hear you. Yeah, thanks for the talk. Do you have any specific advice for the mines like healthcare when we have very low sample size? New data is hard to come. So most of the time we are working with autopsy problems. So it's a specific context. It's a specific context uh I am working out and it's very hard to speak with Green Instagram so at the end we fall short and use simple linear models because it's really hard to explain. Yeah, I mean I think one thing that's happening is I mean when you have very small sample sizes there has to be some other source of number There has to be some other source of knowledge from somewhere, I think. You can't be fully data-driven in terms of the models. And so bringing together kind of symbolic or knowledge based reasoning and stuff together with more machine learning stuff, I think is one path forward. So some people use the term neuro-symbolic AI for that. So where you have, I mean, the statistical data. I mean, the statistical data-driven sort of stuff, along with some expert knowledge that you've obtained some other way, and then trying to combine those together. So I think that's a promising sort of way to go forward. But yeah, it's a big problem. I mean, I don't have any unique insights. I mean, I think a lot of people should be focusing on those sort of problems because this other direction where there's like billions. Where there's like billions and billions of data points. And these foundation models that's getting more attention than maybe it needs to, whereas the small data side isn't getting enough attention. So yeah, I mean, I don't have any new insights for you, but yeah. Another question from Beck Kush again was very instructive. Thank you, everyone. And yeah, again, apologies for not being there.