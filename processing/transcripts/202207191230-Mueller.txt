Thank you very much, Francesco. Everybody can hear me okay with this mic is working. Okay, cool. Well, I'll just jump right in. This is some work that we're finishing up right now. It was funded by an LDRD program that we have at the laboratory. It's like a two-year little research program. We started out this project with, together with Mariam, who is, I guess, a network science person as a whole project. Um, as a little project, we um added Vincent. Is this thing work? Vincent, yeah, we added um Vincent Vincent to our team. He's a really friendly person. If you ever feel down, you go talk to Vincent and you feel suddenly really happy. I don't know why. He's a very positive person. Along the way, we met Shang Yang, who is a high energy physics person who works with us on some applications of our work. And then last year, we had a bunch of really fun folks who were also contributing to this project with applications. This project with applications and added a little bit to the code together. So, I'll talk about hyperparameter optimization and how we are in deep learning and how we are doing that with our surrogate modeling techniques. So, if you look around a little bit, especially in the Department of Energy, a lot of people ask you now to use machine learning for your work, if it's useful or not. So, I'm being a little bit sarcastic about that. But you can, for example, But you can, for example, you can use machine learning to predict traffic flow, let it be traffic flow on the roads, or it could be flow over a network where you send data from A to B. A lot of people are thinking about maybe you can use deep learning models to accelerate computationally expensive simulations. So let's say you have a very coarse resolution simulation that runs really quick, but it's not very accurate. Or you have the very same simulation, but at very high resolution. So it's very accurate. So it's very accurate, but it takes forever and a day to run. So now, if you want to run this thing for longer, ideally, you don't want to wait 10 years for your simulation to finish. So, one of the ideas is why don't I use them sort of a coarse resolution model as well as the high resolution model? And I train a machine learning model on the difference between coarse and fine resolution. And then going forward, I only run the coarse resolution thing and then sort of like correct it with my machine learning model. Seems to be doing pretty well for certain. Seems to be doing pretty well for certain applications. There are applications in high-energy physics down here on the left side where maybe you want to use a GAN. I'm not going to say the long word because I was mess it up. Where you want to have a machine learning model that gives you as an output, like an approximation of distribution that you're interested in. Idea is again, parametrizations and simulations take forever and a day to run. A machine learning model can maybe help you save some time there. And then we have also. There. And then we have also very simple applications where maybe you live in drought-stricken California and you're running out of water. Can I predict groundwater levels into the future using a machine learning model? You like to do that, especially in areas where you don't actually have a mechanistic model that you can run. There's no simulation that can run to make you these predictions. So, but maybe machine learning can step in there. But these are all like just some application examples. But then, let's say you're in the situation, you have a bunch of data, you know what you want to do. There are some questions that you have to answer before you can use machine learning. So, and one of the questions is around what architecture are you using. So, hyperparameters basically define your architecture, and they also, in some sense, define how well your model can actually perform. But then, identifying the best architecture is also not straightforward. So, architecture, hyperparameters are, for example, Parameters are, for example, how many layers do you have on your network? How many nodes do you have in each layer? What's the batch size? What's the learning rate? What's the number of deeps to use? And if you look around when people do application and use machine learning, they kind of like, maybe they report what architecture they used. They never tell you how they found that. And if you go ahead and you try to reproduce what they did, it kind of doesn't really work that well. But usually, what happens is somehow you conceive of some hyperparameters, some number. Some hyperparameters, some number. You train your deep learning model. It takes, depending on how much data you have and how complex your model is, it takes a long time. And then you can actually figure out, okay, did this set of hyperparameters perform well or not? If you didn't like it, then, well, you sit down and think, oh, maybe I'll add a layer here and a different dropout rate over there. So it's a little bit cumbersome and slow for the most part. And here's just an example. So in practice, hand tuning is used. So, in practice, hand tuning is used very much, random sampling is used a whole lot. But if you think about, here's just an example of four different sets, four different types of deep learning models like convolutional neural networks, MLPs, LSDMs. And if you give for each of these different types of networks a very limited number of options for hyperparameters, you have quickly already like 500,000 to 30 million different combinations of architectures that you could. Of architectures that you could try out, which is a little bit too much even for a nice supercomputer to handle. So you need something else a little bit more efficient than just randomly poking around in the room or doing a parameter sweep for that. So, well, people thought about that, right? So you can write this down as a bi-level optimization problem where your theta is my hyperparameters. Parameters and they are representing sort of like the performance of this model that I have. But now, in order to evaluate the performance, I need to train that model. And training is basically solving something like a lower level, a nested optimization problem. So these W stars here in machine learning speaker basically your weights and your biases. And they come out of solving this lower level optimization problem. So what happens is at the upper level, you set theta, it's now fixed, you solve this lower level. Fixed, you solve this lower-level problem, you get these optimal weights and biases, and now you can actually evaluate what this upper-level performance for those hyperparameters is. So that's why it takes a lot of time to do. And a lot of the time we like to use, where's Matt, you're over there, stochastic gradient descent to do these kind of things, to do this training, because there is a lot of these W's running around here. And that can actually cause some problems or it goes. Actually, it causes some problems, and it goes into this reproducibility problem. That maybe I have this time series that's here in black is my true time series that's further in the past, longer still. But let's say I picked an architecture theta, and I go ahead and I train that thing, and I make a prediction, and I get this blue curve as a prediction out. Now, this is pretty far away from this black curve. I can be like, yeah, this model is not very good. But then I give the same architecture to my buddy somewhere else, and he trains it. Else and he trains it and he gets this orange curve out of it for the prediction. And he's like, It's not too bad, you know. So now we have like already two different opinions of how well this architecture actually performs. And one of the problems is that it's usually not very well addressed. This whole like variability that comes in from training with stochastic gradient descent methods. And now you can end up like reporting your results in a paper and nobody can reproduce it because it's all like a draw from a random number. Number. So then the question is: can you actually find architectures that are less impacted by that, I don't know, variability from the stochastic gradient descent method? So one of the goals is here of the project basically is that we wanted to optimize, automate, to search for the optimal hyperparameters that we have. So instead of having here our little human, it works automatically. We wanted to figure out if there's a way to quantify. To figure out if there's a way to quantify sort of like the prediction variability and so like bake that into the architecture search. And then you also find out pretty quickly that that would take forever in a day. So we were looking around if we can implement it somewhat efficiently so we can make use of a huge number of GPUs as well as CPUs depending on what you have available. So the first thing that we did is we tried to automate the hyperparameter optimization. The hyperparameter optimization using surrogate models. That's what we like to do, what I like to do. Basically, here's your file-level optimization problem. And I just say I interpret the lower-level problem as a black box. I know I stick my hyperparameters in, something happens in the back, and I get back a performance u of theta here. And then I can simply say, okay, I fit a surrogate model that maps theta to the performance of theta. And then the surrogate model can help me in an iterative optimization process to figure out what hyperboral. Process to figure out what hyperparameters I should be trying. I did make some simplification assumptions here. I say all the parameters are constrained to be integers. I can discuss that more in the end if that's interesting to anyone. Okay, and then, yeah, we know pretty much how surrogate model guided optimization works. So, before you can fit a surrogate model, you need some input out prepares. You go ahead, sample your space a little bit, get some objective function values back, and then you. Function values back, and then you can just generate a surrogate model. This could be right now, this could be anything, really. So, here's a two-dimensional example. I sampled at four points, I know what the function value is, I fit a surrogate model, and that's my best guess of what the underlying function looks like everywhere where I didn't sample. How correct is this? Well, I don't know. But then the idea is you use the surrogate model, so like solve an auxiliary optimization problem on it, and then figure out where in the space you want to do the next expensive function evaluation. Expensive function evaluation. So, let's say this guy over here, and now you have a fifth piece of information. So, why don't you use that and you update your surrogate model? And then you keep sampling like that through a space. Surrogate model tells you where to sample. You do your expensive function evaluation, update the surrogate model until you've reached your budget of function evaluations you wanted to get to. So, that's the overall idea. There is, we played around with different types of surgery. Around with different types of surrogate models here, both radial basis functions and Gaussian process models. I'm not going to talk about that very much because I don't trust this a whole lot. I think there's something broken. But basically, radio basis functions are quite nice in the sense that all you need to do is to build them. You solve a linear system of equations. Problem of the Gaussian process model is when you fit that is sort of like you have to solve an optimization problem, which can take a lot more time than what you made. A lot more time than what you may want in your problem. And then this auxiliary problem here in step four, I think the problem that you solve there sort of like depends on the type of surrogate model you want to use, because different surrogate models give you different kinds of information that you can then exploit to figure out where in the space you want to sample. So I'm just talking about the regular basis functions, and this is this is old from, I don't know, five years ago or something, but this was. Know five years ago or something, but this was an old mixed energy optimization algorithm that we developed that uses surrogate models. And the way it works, just to describe this picture a little bit. So here I have a two-dimensional space, these black dots, and this red dot is where I have already evaluated my objective function. The red guy over here is my best function value I've found so far. And the contours underneath is my radial basis function approximation of the underlying function. So this is like my best guess of what. So, this is like my best guess of what's going on. And now, in order to use the information from the radial basis function, I say I want to create a bunch of candidate points by random perturbations. So, what you would do here, I assume that parameter one has to be an integer, parameter two can be continuous. So in order to get these perturbations, these perturbed candidate points, you perturb parameter two anyhow that you want, any real number, but parameter one, you only perturb. Parameter one, you only perturb in integer steps. That's why these guys look a little bit like they come up here in columns. And then you look at these candidate points, these green squares, and then you look at what's their predicted objective function value, how far are they already from already evaluated points, and then sort of like make up a score between these two and select the best candidate point as your next sample point. So, this is a high-level idea of what's going on there. Yeah, so compared to random search, or we talked about evolutionary algorithms earlier, it generally requires fewer queries to the expense of function evaluation, but conversions guarantees you only have one probability. Like if you keep sampling long enough, eventually you'll hit the optimum. Because we say we'll never sample one point more than once. Okay, cool. But then what about this performance variability? So this was just surrogate models in general. So, this was just surrogate models in general. The performance variability comes in when you evaluate your performance metric of your hyperparameters. So then it kind of like percolates into the surrogate model. And then it also then impacts how you solve on the surrogate model for a new point. So there's different steps where you would have to take that somehow into account. And if you wanted to do, I don't know, a fully Bayesian approach. I don't know, a fully Bengayan approach to quantifying the uncertainty that comes with that variability. You quickly realize the compute time is way too much, so it's not going to be very effective. So we kind of like settled on a sampling-based approach where, on the one hand, this is what I had before, you just take the same architecture, train this thing five times, you get five different predictions out of it, derive some statistics from it, and use that as your information of the performance variability. And then the other thing you can do, people have played around. Can do people have played around with Monte Carlo dropout where you take your deep learning model, you train this thing, but then as you make predictions going forward, you kind of like randomly say, you say that random points will not contribute to the predictions or random nodes in a network will not contribute to the prediction. And you can, it's what we call a dropout mask. You can do that five times. Five times, randomly chosen nodes do not contribute to the prediction. Do not contribute to the prediction, you get five predictions out of it. So, those are two different types of sampling methods you can use, and you can combine them too if you wanted to. But they help you basically to get at least a sense of like the expected performance of your architecture as well as the variability of that architecture. And then you can use that information to bake that into your performance metric of the hyperparameters. Yes, and this is kind of where Winset comes in. Yes, and this is kind of where Windset comes in. So if you go back a step here, I lost the keyboard somewhere. Yeah. If you go back, if you train a model multiple times, it takes a long time. If you do a lot of dropout maths, it takes a lot of time. But there is some obvious parallelism that you can exploit there. And since I don't know how to do it, you hire somebody who knows how to do it. So Vincent baked that into a software. He called it HIPO. He pronounces it hypo. Where you can basically use that. It's very easy to install. It comes with pip. It has a really easy config file that you manipulate and tell it, like, here's the path to my data and here's the model I want to tune into. There's also a really cool website with it. And yeah, Vincent is very responsive if stuff doesn't work. So it's pretty easy to use. There are really pretty simple example scripts there as well. So it's cool. It accelerates things a lot. Accelerates things a lot. And what he does here, I skipped some slides here, we are exploiting his unnested parallelism. So basically, your Slurm job is your hyperparameter optimization task. And now you have, it gives you different options where you want to do parallelization. So on the one hand, you can say that I have three different sets of hyperparameters. Each hyperparameter set Parameters. Each hyperparameter set I want to train five times. So now you can train the three different hyperparameter sets in parallel, and then you do sequentially the repeated trial of the training. So that basically what can happen in the SLURM task down here. The other option that you can do is you have one hyperparameter set. You want to train each hyperparameter set in this example three times. So you do three trials for the same hyperparameter set. And then in this remote. Parameter set. And then in the SRM task, underneath in the nested parallelization, you do the training in parallel. So it depends a little bit on what part you parallelize in terms of like what's the overall compute time for your hyperparameter optimization. In the end, all of these SLURM tasks, they collect what they have figured out in terms of what are the losses from different trainings, from different repeated trials. You figure out, can I build a surrogate model yet? You figure out can I build a surrogate model yet? Do I have enough input-output data pairs to fit the surrogate model? If not, you go back up there, you select a bunch of new hyperparameters and do your training. But if you have a surrogate model, you basically collect all of your data across all the different SLURM steps that you have, you bake it into the surrogate model. The surrogate model gives you like, here, this is the next point you have to evaluate. Okay, so this all works. It works really well in CPU as well as GPU. Whatever you Whatever you have available, and apparently, the batch file is no more than these couple of lines, so that's pretty cool. All of this is stuff that you can find on the website that Vincent made. And then the other cool thing that we're doing is this asynchronous parallelism, because when you have a deep learning model, the amount of time it takes to train depends so like on the size of the deep learning model, because the larger it is, the more data you have. The larger it is, the more data you have, the longer it takes to train that thing. Now, it would be like a little bit of a waste of time to say, okay, I have these five different hyperparameter sets. One is a very simple model, one is a very huge model. To have the simple model that's trained in a minute, make it wait for 20 minutes until the last guy is done. So why don't you like make it asynchronously in parallel? So as soon as one of these hyperparameter sets, like set 18, has been evaluated, it throws its data back to this LURB job, job. To the slurp job, job says, like, oh, use the updated surrogate model here of the next hyperparameter set, and that way you kind of like don't waste time, compute time by just having these jobs wait for each other. And it works. It works super well, actually. Okay, and then I'll look at the clock. I have 10 more minutes. Cool. Last year, we applied this method to 3D image reconstruction. And now, a disclaimer: I'm not an image reconstruction person. Reconstruction person. So, this is what video explains to me. Basically, you have a sample, you shoot some x-rays at it from all the way around from 360 degrees. It gives you 2D slices of pictures. And these 2D slices, you can put together into a 3D reconstruction. Now, one of the problems is that when you have like a sample that you basically want to image, the more x-rays you shoot through, it kind of like the sample kind of like breaks along the way. It always gets a little bit destroyed. It always gets a little bit destroyed. So, ideally, you want to do, I mean, think about, you know, medical applications. So, ideally, you want to do as few as possible of these 2D images as you go around. But then when you go to the 3D reconstruction, you still want to have something that's very accurate. So, kind of like minimize the number of pictures you take, but still get something really informative. So, we tried that out on a, this is just a test data set here. And on the left, Data set here. And on the left side, you can see the complete sinogram that you get as your reference reconstruction with a reconstruction algorithm. And in the middle, you can see what we can get out with machine learning, basically. We used the UNET last year. And you can see on the right side is the difference between basically the machine learning reconstruction versus the real reconstruction. And the darker it is, the better it is. So you can see it is pretty well. There's a bunch of like light spots in there, but But according to VIDIA, this is an extremely good result for them. And this year, VIDIA is back, and this year we're working with variational auto-encoders to try to achieve something very similar there. Okay, and then I'm totally changing gears. Let's talk about high energy physics then. So, this is a collaboration with Sheng Young. He's a, I want to say, a computing science engineer at the laboratory. He's a particle physicist. He's a particle physicist, and he has these really cool simulations of how particles collide, and then other little particles come out. You can run it on a computer, and apparently, there's the thing called the hadronization process, which is basically a parametrization, which takes forever and a day to run. And basically, his idea is that maybe we can replace this hadronization module at some point with a machine learning model. We're using a GAN for that. We have about 500. We have about 500,000, a little bit more. Um, two-particle we call events. So, you have a particle sits there, incoming particle, and out go two different particles. So, the assumption is that only two particles go out. It's a very simplifying assumption because you cannot really predict how many particles come out of a collision. But we need to start somewhere. So, we have a particle incoming, and two guys go out. And basically, an event is characterized by a four vector. Is characterized by a four vector for each of these particles. For the one that comes in, it has three momenta and an energy, and the two guys that come out have three momenta and an energy, even though like one of the outgoing is like you can compute it from the other one. But in the end, you use all these momenta and you can basically compute what the spherical coordinates of the outgoing particles are. And then when you have 500,000 events, you can stick them into this kind of a distribution. So, this is what we want to see. So, this is what we want to see. This is our target distribution. And now, the idea is: this is what comes out of the simulation. And now, the idea is: can we train again to get distributions that are very similar to these two? So, we went ahead and did that. We had a bunch of different hyperparameters that we could train, that we could tune, I guess. And the optimization metric that we're using here is a Wasserstein distance. So basically, there's all like a difference between two distributions. Like a difference between two distributions in simple terms. And what you can see here in this figure on the left side is the hyperparameters sampled with random sampling, and on the right side is the radio basis function samples. So each of these little dots in here is one set of hyperparameters. The Wasserstein distance is our overall metric we want to minimize. So you want to be as far left, I guess, as possible. And then on the y-axis, you can see the variability. Axis, you can see the variability of the performance of these hyperparameter sets. So, ideally, you want to be in this lower left corner, it means good Wasserstein distance and very low variability. And now on the top, you can see in histograms basically where random sampling concentrated at sample points in terms of the Wasserstein distance. So, you can see like random sampling concentrated, found a bunch of points that are kind of like here in this high Wasserstein distance regime, where we don't really care about versus radial basis function. About Merz's radial basis function. It sampled there a lot too, but you can see it concentrated its samples to this good Vusserchein distance regime where you want it to go to. So, at least for this example, it seemed to be doing what we wanted it to do. Here's a little bit of a convergence graph. We only have one function, so it's a number of hyperparameter evaluations on the x-axis versus the best Rosserstein distance. And ideally, you want to minimize that thing, you want to go down as quickly as possible. Quickly as possible. So the green line is what we get out with the radial basis function as a surrogate model. Blue and red is what you get out from the random sampling and from the Gaussian process model. I would not trust this red line here. So we only talk about green and blue for now. So you can see it does better than random sampling. And I don't know how much longer you would have to sample until maybe random sampling eventually gets to a quality of solution as the zurroguard modeling does for you. Modeling does for you. Okay, and then we looked a little bit more into these hyperparameters, and that's another thing that you can do with the software, which is really fun. You can look at slices in a parameter space and can do all sorts of cool illustrations. Eventually, you'll be able to like hoover over a little point and it tells you exactly what were the hyperparameters, what was the loss, and so on and so forth. So, it's, I don't know, I think it's really cute. But, um, five out of the six hyperparameters that we had looked sort of like on this right side. So, across the whole. Side. So across the whole range where we tuned the hyperparameter, you could see that you can get all sorts of losses there, which seems to be like, okay, do we care about these hyperparameters? And there was only a single one where we saw like had some sort of a, I don't know, dependency of the loss on the hyperparameter, and that was the learning rate of the generator. And we've actually found that for two high-energy physics applications. So we looked a little bit more into that, mostly because Zhang Yang was. Bit more into that mostly because Shang Yang was really interested in it as well. So, what he did then is it's sort of like an analysis step in some sort. So, we fixed the architecture of the model, except for the generator learning rate. So, we did a little bit of a parameter sweep over the generator learning rate because we wanted to understand what happens to the model performance as you change that hyperparameter. And now you can see that for huge learning rates, the VASA. Learning rates, the Wasserstein distance goes up the lot. So basically, the model performance goes worse. The variability also gets larger. So these are kind of like solutions that you don't really want if you want a reliable prediction. And the things we were interested in are here in this lower left corner. So then we did what we call like a focus search, which you can also do in the software, where you basically zoom in and you say, okay, I'm only interested in learning what happens, for example, for the generator learning rate. For example, for the generator learning rate for this very small area that I'm interested in. And then we did a little bit of a study here of using 20 epochs for training versus 100, 200, and 1000. So the epochs, the more epochs you use, the longer this thing takes to train. But what comes out is kind of like expected, right? If you have a low number of epochs, don't train for very long, basically the best Wasserstein distance you can achieve is not quite as good as if you train for a thousand epochs. For a thousand epochs. So, if you train longer, you get better results. But the other thing that we also try to understand is: can I make my generator learning rate as small as I want to and like get increasingly better performance? And it seems to be that there is, at least for this problem, there's always some sort of a bowl shape. I mean, for some, if you don't train very long, the bowl is very steep and then it just goes open. And then the other thing, which I find interesting, because I'm unpatient, I don't know if I would want to wait for a thousand. Know if I would want to wait for a thousand epochs to get maybe an improvement of, I don't know, 0.01% or something. But then Zhang Yang says, Well, in high-energy physics, we're happy to wait that long. So there's a little bit of a trade-off there to be done. But it was pretty cool to look into that. And we found very similar results for a different data set, also from high energy physics. So, and I'm not sure if this is generally for GANs or if this is just for the If this is just for these two applications that we looked at. And here's maybe the final result there. Those are those two distributions of the momentum I showed of the spherical coordinates I showed in the beginning. The black thing is the one you want to match. Red is what comes out of your GAN. And these little like pink, I guess, bars are sort of like your variability. So you can see that for both of them, you can get pretty close to. Them, we can get pretty close to the truth. One of the issues that we still have is like the extrema of the distributions that we don't seem to be able to match very well. Not sure yet why that is, but we get pretty close to the truth. And if the physicist tells you this looks fine, then you accept that too. Okay, pretty much out of time. So the last application that we're currently working on is graph neural net. We're using graph neural nets to predict network. Using graph neural nets to predict network traffic is very difficult because we have so basically imagine a road, but instead of cars, you send data from A to B. So you have 102 of these traffic links that are illustrated here. And I want to predict what is the traffic on a certain link at a certain time of day. So I know where to route my data. So I minimize packet loss and latency. It's very difficult. So we applied the algorithm here. And this is funny that this doesn't show up. Okay. This is funny that this doesn't show up. Okay. I don't know where these dots went here. But hyperparameter optimization basically can get you some improvements or just some random guesses that you used to have. The problem is, though, that you have 102 of these links that you're trying to predict. So here's a time series and blue is the truth and red is what we predict. I can't really tell you if this is good or bad. And it gets extremely difficult because the way that Mariam formulated the problem, she liked to just like lump all. The problem she liked to just like lump all of these 102 arrows from the time series into one sum. So it's, I find it really difficult to optimize, but maybe, uh, maybe Matt has solutions for that, judging from what I said saw in the morning. But anyway, in summary, current hyperparameter optimization approaches as used in practice, they have a few limitations. So like I said, grid random sampling is out there, hand tuning is a lot in the applications literature. Maybe some surrogate modeling would be very helpful to do it efficiently. Modeling would be very helpful to do it efficiently and effectively. That whole thing with the variability that comes out from stochastic gradient descent is not really addressed in the literature. So I can't really reproduce anyone's results unless I have their random number seed. So these uncertainty quantification methods come in pretty handy. And then if you really want to do hundreds, thousands of hyperparameter set evaluations, then this tool allows you to do this asynchronously and nested in parallel. And with that, And with that, I will leave Berkeley Lab. This is, I think, my last conference or something. So I'll go to N-ROL and I work on something that's not high-energy physics in the future.