On ISMI, of course, on large deviations on random graphs. All right, so yeah, thanks. Let me share my screen. So Okay. All right. So, yeah, we'll carry over from where we left off. I'll briefly sort of recall what we did yesterday. And then I also sort of, there will be lecture notes available online about everything that I'm going to talk about and actually more. And also the things that I'm writing up during the lecture will also be posted after every lecture. So I think you should expect the material from yesterday to be online very soon. Okay. Okay, so just to recap. So, yesterday we were for P not too small the property of this tail events. Where th is the number of copies of H inside G. You can also think of it as homorphism counts because for P, not too small, those things are pretty much the same. This is roughly exponential of something, which I was calling because H and P delta. P delta plus maybe some correction. And this was an optimization problem. This was a solution to an optimization problem, which was you look at the best possible re-weighting. So Q is a weighted graph. You look at the relative entropy of The increase of Qn with respect to P, and Q is such that is actually typically pretty large. Okay, so this is where we stopped yesterday. So, roughly for peanut to small, the optimal strategy. For p na to small, the optimal strategy is to have an inhomogeneous random graph which makes which makes the atypical event that you want to attain typical. And the cost is the relative entropy. Just to be concrete, this is Q is a weighted graph. And just to be concrete, this is some, this is a symmetric. Some, this is a symmetric matrix, but let's just do I not equal to j relative entropy of i p of q i j. So this is something that we defined yesterday, which was the relative entropy of a Bonnelly random variable with mean qij with respect to Bonnie random variable of mean p. And then you sum over all things, but the degree of freedom is actually not the full matrix, but only half of it because it's symmetric. And so basically the average So basically, the every term actually gets counted twice. So you essentially actually this is this is the cost. Now, at this point, there are a couple of things of interest is how what is it? So, what is the solution? Now, for again, let's for the moment just stick with P. By the way, I should actually sort of, yeah, I forgot to sort of open my chat box, actually. Let's okay. Sorry. Okay. So, for the moment, let's just briefly spend some time in peace, let's say, fixed. Sometime when P is, let's say, fixed. Although, yesterday we did talk about P going to zero with N, very slowly, maybe, but still P was allowed to be going to zero. But let's for the moment assume that P is fixed. And then actually, this quantity, which is an N in it, is actually related to a continuum problem. So there is a graph on Where let's say I just and so this is this is going to be exactly the same as this but instead of a weighted graph of size n and look at a graph on so again it's the same thing now this is going to be a graph on and I essentially minimize Minimize our IP of Q, a graphon, such that something pretty similar to this holds. Right, so now I want to sort of take a continuous function on 0, 1 square. So yeah, I guess I don't have to recall everything from Isa's lecture, but we sort of briefly talked about graphons, which were the continuous version of graphs, where everything, graphs actually. graphs where everything graphs actually naturally embed as graphons and these are just zero one valued functions which are symmetric on zero one square and so ip of q is now just this sum is actually just going to be replaced by an integral so it's literally half integral i p of qxy okay so this is a natural way that the discrete problem transforms into a continuum Discrete problem transforms into a continuous problem. It's actually sometimes simpler to work with the continuous problem, and it's actually not very hard to go back and forth between the two. Okay, so instead of looking, instead of asking this question, I'll sort of ask the same question about phi H V delta. Okay, so so a natural question is when And also, let's say the argmax, so this is an infimum, everything is actually compact, but you can ask about what is the graphon that minimizes this. When is phi h p delta attained at? Delta attained at a constant function, which is the same as roughly saying, same as roughly saying the large deviation event Makes GNP, which was the original measure that I started with, behave like GNR for some R. In this case, we are looking at upper tail events, but you can also look at lower til events. Upper till events, but you can also look at lower till events. But for the moment, just let's fix, let's just sort of focus on upper tail events. But so we had sort of asked this question in the past: like, when do you expect an Edashini graph with density P under this conditioning to behave like an Edashini graph homogeneous where everything sort of gets simultaneously raised to density R. Now, this is so you have to sort of solve this. So, this is a convex. This, so this is a convex function in Q relative entropy is a convex function in Q, but the set on which you're minimizing it is not convex. So it's actually not super clear how to actually solve this optimization problem. Now it turns out, okay, so Lubetsky is out. Lubert's Kizar using a generalized form of Helga's inequality found the above region Exactly when age is regular. So age is a graph that we're embedding. So age is a graph whose number of copies we are interested in. We want it to be larger than its typical value by a multiplicative factor. And we want to understand the graphon, which has that property typically and has the smallest smallest. And has the smallest entropy cost with respect to the original measure, which is constant P. Now, like I said, so this is sort of a non-trivial problem, but it turns out there's a sort of nice application of Helders inequality, which only works for regular graphs. And I'll tell you roughly what the solution is for, let's say, triangles. So let's say H is A3. So the typical density is So, the typical density is density is p cube in the Grafon sense. So, there is no N anymore. And let's say I want to achieve density R cube, where R is bigger than P. So it turns out. So, it turns out roughly this is the description. So, look at the function. So, I described that the relative entropy ip of x is a convex function. But now, let's say look at the function x going to The function x going to ip of root x. Now, this is not a convex function anymore. In fact, the graph of this function looks something like this. So this is this is p squared because i p of p is zero. Of p is zero. But it's not convex. And but now I can, so now the solution, what the solution of Lipitzka's is, now you look at its convex minor ends. It's look at the largest convex function, which is below. Consider convex minor end which is a small Which is the largest convex function which is below the original function. And the claim is if Regina So, R cube is the density that I want to achieve. So, the claim is if R square, IPR is actually, so this is a point on the function actually, because IP of R is literally IP of square root of R square. So, this is a point on the function, but the point is, if this function, if this point also lies in the convex minor end, then the answer to this question is. This question is R. So if you look at the graphon which has the smallest entropy with respect to P, but has density at least R cube, then the graphon that minimizes that entropy problem is actually the constant function R, provided this condition is true. Then the constant function r The constant function R is the optimal solution. And it turns out that it was also shown that if this is violated, if violated, then there is Is a block graphon construction which does better. So, this basically classifies the region of space where the constant function is the optimal solution versus where it's not. And the graph. And the graph of this region, if you look at the phase diagram. So let's say this is a P R space. So I'm looking at the upper tail problem. So I'm looking at only when R is bigger than P. So it turns out that there is a curve like this. It turns out that there is a curve like this, such that this region is the replica symmetric region, which is sort of a physics jargon for saying that the R is the constant, R is the solution versus here. Where there are better candidates. Is this color actually visible? Maybe not too much. Now, okay, but this is all, I mean, like it don't. I mean like it turns out that this r squared to ipr or x to ip root x for a general graph for a general regular graph did this becomes x to ip of x to the 1 over d where d is the degree so for any regular graph a similar story is true where you instead consider the function True, where you instead consider the function x going to ip of x to the 1 over d, but d is the degree of the graph. But the point is, this is only this sort of argument only works for regular graphs because, like I said, so entropy is a relatively nice function, but the set on which it's maximized or minimized is actually a non-trivial, it's a non-trivial non-convex set. But turns out that the homomorphism density. The homomorphism density is something which is also sort of hard to work with. And it turns out that if there is a version of holders' inequality that actually relates, so the sort of key point is holder's inequality, sorry allows you to pass from more personal densities. Monophysome densities to various norms of the graphon. And that actually turns out to be a useful thing. But this sort of argument currently is only sharp for. Only sharp for regular graphs, and it's an open problem to find the exact phase diagram for any connected. Connected non-regular graph. So the bound that the holders in equity will give will only depend on the max degree. And it turns out that that's not sharp for non-regular graphs. So just want to comment with Bhasha Bhattacharya. We have a an unpublished actually, it's been around for some time, unpublished result which improves the knife holder bound for K one two. For K12. So even for a path of length 2, this problem is getting the exact region is open. Like you look at all graph, so you look at all graphons. So the density is p square here. You look at all graphons which has Which has density which is bigger than p squared, let's say r square, and you want to look at the one which has the smallest entropy cost. I want to understand the exact region in which it's the constant function is a solution versus where it's not. And if you apply the standard holder bound from this Lebetzki Zau, then the bound that you get for the two path, a region that it's not different from what you get for the triangle because the holder bound only depends on the maximum. only depends on the max degree and the max degree of the path of plan two and the triangle both of them are two and so you don't get any improvement but you can actually work a little bit more come up with some better inequalities and actually get something slightly better but but it's still not sharp and i yeah so this is an and any progress for general graphs would be fantastic so by slightly better you mean a slightly larger region yeah yeah yeah where things are where you have this Things are where you have this, yeah. So, yeah, so where you have symmetry, yeah. Yeah, so so the region for the two-star looks something like, I mean, like the bound that we have looks some, I'm not sure what the figure looks like, but in principle, something slightly different beyond what is for the pandemic. Is it clear that you always have some non-trivial region where you do not have the C? Where you do not have the symmetry? So for P, yeah, for P, very small. I mean, like, as you will see, so far we've been talking about P being fixed. And so now we will soon sort of transition to P sparse. So the first comment is for P sparse, where P is going to zero with N, which actually also kicks in when P is super small, you can really sort of do better by having some block structure and instead of having a constant. So there are some. So, there are some questions here. Matthas is asking for which graphs. So, okay, so one point here. So, for no graph, the non-constant maximizer is known. Like, it's known that there is something which is better than constant, but for p fixed, there is no graph for which it's known what the optimal solution is in the region where it's not a constant. Does it answer? Okay. Yeah, so for P very small, you will see that actually having tiny blocks will actually be more optimal. Okay, good. Any further questions? Okay, so there are actually simulations. So, simulations predict not literally for this model, but it's actually generally believed that it should be the same for this model as well. But there are related models where simulations are actually done that optimizers. In the symmetric breaking region should be stochastic block models with two blocks. I think that's what that's what I'm showing. So the solution should be actually Actually, not a constant function when it's not a constant function, it should be like a roughly approximated by a four-block graph on the different densities across the blocks and inside the blocks. So that's what there's been extensive simulations done by Charles Redin and his group. I don't know. So, okay, so a lot of things are actually known for sparse graphs where you actually can handle entropy. Sparse graphs where you actually can handle entropy. So, for dense graphs, this is roughly the story. Yeah, so this is somehow, somehow, yeah, it's not super clear analytically why it should be, but maybe you can think of doing some portability argument to say that, oh, actually, if you have multiple blocks, maybe you can actually improve things by some sort of convexity argument to sort of just go to two blocks. Not four blocks, two blocks, like four blocks in the four blocks, literally, but actually two partitions. But actually, two partitions. That's what is expected to. Can you scroll the tablet? But your letting is not visible. Oh, actually, oh, I see. Sorry, did it freeze? I say, sorry. Just a sec. Let me try it. Yeah, okay. Sorry. Oh, how long has it been not visible? Sorry. When you started writing the simulations predicted, that was not visible. Okay, yeah. So, yeah, simulations predict that optimization. Optimizer should look like this. I say, sorry, yeah, so I think it should be fine now. Sorry, thanks, Melindia. Okay. So we'll soon sort of transition into sparse cases where actually you can do a lot because you have a lot more control on the entropy. But one more thing that I wanted to, in the dense case, want to talk about. Yeah. So recall. So, recall so for which uh for which age are these simulations? So, like triangles, paths of length, you like, and also like it's to be completely precise. I think the simulations are not for exactly the setting that I'm talking about. It's about related settings of exponential random graphs, which I'll sort of soon go into, or constrained models where you sort of look at all graphs uniformly picked from a subset of graphs, which has a given. Uniformly picked from a subset of graphs which has a given number of edges, given number of triangles, and things like that. Okay, so relative to what Omar said, so now we will sort of talk about a particular class of measures. So recall a standard way to prove large deviation bounds. Deviation bounds is by computing exponential moments. So, which actually relates to, so this naturally relates to, so this naturally. Leads to study of a class of Gibbs measures on graphs of the following form. So probability of G is proportional to Is proportional to exponential of some Hamiltonian. And the Hamiltonian roughly looks like the following. So let's say everything is size n. So n squared times, let's say, a summation i equal to 1 to s. To one to s maybe beta i t h i g. So let's say you fix graphs fix graphs h1 h2 hs. So h1 could be an edge, h2 could be a triangle, h3 could be a four cycle or whatever. Like so fix some finite collection of graphs. And now you want to jump. And now you want to charge a particular graph, the measure should be charging it exponential of roughly something related to the densities of this subgraphs into G. Okay, so which means basically if the beta is a positive and large maybe then more mass More mass on G with high subcrab densities. Okay, so this is known as the exponential random graph model. Or your gm. On the other hand, if beta is a negative, then you actually put more math, then you actually put more mass on graphs that actually, let's say there was only one, which is, let's say, a triangle. So let's say there was just H1 and H1 was a triangle, and beta i was a large negative number. Then you actually put much more mass on. On graphs that actually have very few triangles. And then you would expect things like, oh, actually, then the mass is more or less concentrated on graphs that are mostly like bipartite. So if beta i's are negative, then subgraphs are avoided. Okay, now so this is not a property measure. So, this is not a property measure. You have to normalize by something to make it a property measure. And let's call that normalizing constant as Z. So, Z is the normalizing constant, which means that this divided by Z is now a property measure. And let's call Psi to be, let's say, the log. So, this is also sometimes called the partition function. And so, let's call psi to be. And so let's call psi to be the log of the partition function. So it turns out, it's not surprising that large deviation theory can be used to study your GM and in particular. The free energy or the log partition function in okay, so what is what is z? So z is sum over all graphs exponential of beta exponential of h so which is n square summation beta r. Summation beta i, the densities. The densities. Now, okay, so so I don't remember exactly what I did in the last lecture. So, THIG should be thought of as a density of HIG. This is an order one quantity. So, you basically normalize everything. So, you basically normalize everything by all the factors of n. So, this is supposed to be. So, for example, if hi was triangle and g was an edoshuni graph, this would typically be like p cubed, not n cube p cubed. Another point is you multiply by n square because that's roughly the entropy of the full system because there are roughly n square edges. So, you want to understand that and it turns out that so you can actually use large deviations to actually use large deviations to answer this. So psi roughly looks like n squared, psi is a log of this. n squared times a variational formula. So so over all graphons w actually let me let me just make it normalized Let me just make it normalized. Let me just normalize it. So h is this, and let's normalize. So h is just again an order one quantity such that it makes sense for graphons. So then h of w minus well I can also normalize this like this just to make sure that things are normalized. So this is then So, essentially, what it says is: I want to sum over all graphs exponential of this Hamiltonian. Now, the Hamiltonian is liberally subgraphed densities. So, what it does is it's so if you actually use the Lativesian framework of Chatterjee and Varathan for graphons, so this sum you can sort of divide your space of graphons into various corners depending on what the value of the Hamiltonian is. So, this is a continuous function, so this is a space of graphons. So, maybe you put. Function, so this is the space of graphon. So maybe you partition it. Okay, I'll not continue, but such that in each part, h does not oscillate much. H is a continuous function on the space of graphons, it has sub-graph densities. Okay, so now you want to contribute. So, for Z, you have to sum up the contributions of each of these parts. Now, Now, if this part is close to a particular graphon, then the probability that the 1 over 2 to the n measure, so z is sort of a tilting of this uniform measure, 1 over 2 to the n measure, maybe 2 to the n squared actually, because the system size is n square. So for each part, the contribution from the exponential h of w and the probability that this one about The probability that this 1 over 2 to the n square mass, the uniform measure force here, is roughly comparable to the entropy of W with respect to 1/2. And so, of course, there is a sum involved, but the number of terms in the sum is so small that actually the part that has the biggest contribution dominates. So, this is not a very uncommon thing. And this is what sort of comes out of this large division theory. This is actually basically. This is actually basically similar to an application of Aradhan's lemma given this large deviation. So I won't say much about it. The whole point is: if I have large deviation information, I can use that to understand exponential random graphs. But one thing that happens, though, is that if beta i's are all positive, so this is proven by Chadichian diaponics in 2013, relying on this previous work of Chadij. Relying on this previous work of Charijin Varadhan. And in the same paper, it was proven that if betas are all positive, then actually nothing very interesting happens. So then the optimizers in the above problem are constants. So it turns out that exponential random graph, if all the betas are positive. Random graph, if all the betas are positive, actually looks like maybe a mixture of Edoshini graphs. Again, the relabeling has entropy, which is exponential in n or maybe n log n because there are n vortices. The total possibilities of relabeling is exponential in n log n because it's at most n factorial. Whereas all the exponential scales at which we are sort of dealing with here are exponential in n squares. Of dealing with here are exponential in n square. So, really, the relabeling actually does not really affect this sort of consideration. But, of course, everything is exchangeable. So, any graph, its relabeling will also have the same probability under this measure. Does that answer the question roughly? Yeah. Yeah, so everything is happening at a much sort of larger entropy scale. Okay, so this means basically that ER. Basically, that ERGM looks maybe a mixture of Hidashwini graphs. So, not super interesting, but there are interesting questions to be asked nonetheless here. And so, this was actually made even more quantitative in a recent work of Eldon and Eldon Grause in 1915. In I think maybe 2018, 19. I don't remember exactly the dates. But and so there is a okay, so if beta is all positive, then things are things are roughly like a destruction curse, but there could be multiple solutions. And actually, you can ask, like, so then there was a sort of There was a sort of paper actually earlier than Jarajan Daknis by Shankar Vamedi, Guy Bresler, and Alan Sly. Sorry, which looked at high and low temperature. So I won't actually make precise what these regimes are, but roughly what it means is in the high temperature, there's a unique solution. This is not literally true, but roughly multiple solutions. Right, so I claim that the optimizers are all constant, but there could be just unique, one unique constant solution, or there could be multiple constant solutions. And they proved that in this case, in the high-temperature case, many nice things happen. So, fast mixing of globodynamics here, slow mixing. Fast mixing, meaning order n squared log n, where n is the size of the graph, here exponentially. Here, exponential in n. But let's look at this regime for the moment. So, here it says roughly that there is one Edoshroni graph. So, there is one P star such that ERGM looks like, okay, so this has to be made, okay, one has to sort of talk about what this means, looks like an Edoshroni graph with density P star, but in what sense? So the strategy diagonal. So, the strategy diagonal result, or even the Vamiti Bressler slide result, tells you that this is close in cut distance. So, close in cut distance. But you can actually ask more. Like, if you actually expect things to be like an Edition graph, you can actually ask questions about, let's say, concentration of measure, central limit theorems. Theorems of observables, like let's say, central limit theorem for number of edges in the graph. So, of course, in an Adashoni graph, everything is an independent edge. So, of course, you have the CLT. But if you expect ERGM to be close to an Education graph, you can actually ask things like this. And these are actually mostly open. So, in recent work with a student at Berkeley who's now starting a postdoc at UCLA. Starting a postdoc at UCLA. So with Kyon Sik Nam. So we showed concentration of measure, actually Gaussian concentration of measure. I'll not make precise statements because I don't have the time. For the sheets functions. And this is roughly using Stein's method of exchangeable pairs. And for the second question, we show a partial CLT. So, what we show is if you take Edges E1, E2, EM, well, let's say M is little of N. So maybe one of the sort of the big question is, let's say to prove central limit for the total number of edges, but let's say you take some smaller number of edges, which is little of n, n is a graph size, and also let's say they are vertex disjoint. Giant. So, no two edges in this class share a vortex, then indeed satisfy a CLT. So, whether this edge is present or not satisfies a CLT as m goes to infinity. So, but it's open view. It's an open problem of, I think, much interest. Okay, I think it was the statement clear. I mean, it was not super precise, but the point is you expect things to be close to 820, so things should roughly have some decay of correlation. So you should be able to sort of expect some Gaussian fluctuations that we already proved, but to prove actually a central limit. We already prove, but to prove actually a central limit theorem is much more delicate. But we can only prove it for a smaller number of edges, and if things are sort of not super because the point is the correlation estimate that one has actually changes when two edges share a vortex versus when they don't. And so that's why we really need the edges to be vortex disjoint. Okay, so I'm actually. By CLT, you just mean for the number of edges in the graph. Yeah, I mean, so I mean, you can look at other observables, but the first step is to just sort of just look at the number. Step is to just set up, just look at the number of edges and prove a central limit theorem for that. Yeah, prove a full case. Yeah, that I should make that clear. Yeah, thanks. Okay, so that's what I think I wanted to say about sort of the dense settings. I mean, there are versions of this, even when p goes to zero. This, even when p goes to zero, and there are structure theorems that actually appear in this work of Eldon and later Eldon Gross. But let's sort of revert back to our original question of large deviations. So, I mean, like, as we have already seen, large deviations and exponential random numbers are not actually that different. Like, you can actually, if you have a very good understanding of exponential moments, you can actually use that to get some large deviation bounds. But let's sort of But let's sort of just go back to our original setting. So let's go back to this phi h and p delta quantity, which we saw when p was fixed was constant in some regime and then maybe non-constant in some regime. So now we will consider So now we will consider the case when p goes to zero with n. So Frank is asking if a solution is known close to the crossover line in the LZ problem. For what P fixed. No, I don't think for P fix anything is known. I think there are some related problems in external graph theory. So these are actually not the same problems, but they are of similar flavor. And I think Raspberry does have some understanding around some phase boundaries using his theory. But in this concrete setting, I don't think anything is known. But there are related things in external graph theory where some progress. Things in XML graph 3 where some progress has been made. Okay, so P will go to zero with end for the rest of today's lecture and tomorrow's lecture. So first observation, constant function is never the solution. Okay, so what do you mean by that? So I have So, what do you mean by that? So, I have this optimization problem. I want to look at a weighted graph so that the subgraph density is bigger than typical, and I want to understand what the best possible re-weighting is. And the point is, I claim that when p goes to zero, the constant function is actually never the solution. Why is that true? So, let's see. So,  Right, so if actually, if actually the constant function was a solution, then the constant function actually means that the total not only does the number of triangles or whatever subgraph you're interested in. Okay, so when I say constant function is never the solution, I mean when the graph, subgraph that you care about is not trivial. I'm like, if the subgraph is literally an edge, then it's nothing different from a kind testing problem. So let's pretend that it's never like you're always talking about non-trivial graphs. Are you always talking about non-trivial graphs? So, whenever the graph is non-trivial, meaning not an edge, the constant function is never the solution because the cost of making the total number of edges. So, if you actually the constant function was a solution, then it would actually imply that the total number of edges increase. So, in particular, the total number of edges was typically, let's say, roughly n squared by 2 and choose 2 times p, but maybe now that gets boosted up to some 1 plus delta factor. And it turns out this is roughly the cost. out this is roughly the cost is roughly e power minus n square p with some constant depending on delta it's sort of an easy check that this is what it will turn out to be now it turns out that there are better strategies let's say for a triangle you can actually so let's say this is my full graph and let's say I can pick a small part of the space of size np maybe with some constant so that I put in all So that I put in all the edges here. So I take an N P cross N P box and I put in all the edges there. So how many edges do I put in? That's roughly n p choose 2. But okay, so let's ignore the 2 for the moment. So it's roughly n square p square. But very in a very compact fashion. Very compact fashion. And so the cost of this is actually what? I'm introducing n square p square many edges. Each of them, the probability of a particular edge appearing is p. So the cost is p raised to the n square p square. Which is the same as e power minus n square p square log 1 by p. Now you see that this, so you see that n square p is actually much, much larger than n square p square log 1 by p if p is small. So this is a much more optimal strategy than increasing everything. Than increasing everything. Okay, so I want to actually, so I'm running out of time, so I want to actually sort of tell you what phi h n p delta is. So one has to actually slightly bear with me. So let's say h is a graph, h is connected. And so, this turns out to be a strategy to get this boost. Another comparable strategy is actually the following: take some small part and make everything here one. So, you take some small set of vortices. I mean, like, of course, this is so small, you also don't care. Maybe you can make this one as well. So, the point is: here, you took a So, the point is here: you took a small set of vertices and put in every possible edge within that small set of vertices, which means you planted a clake. So, this is a clake. What you are doing here is you take a small set of vertices and you are actually connecting them to everything else and also to themselves, maybe. And so, this is like planting what we call an anti-click in our paper, but An anti-click in our paper, but subsequently has been called the hub, or it's just basically a very unbalanced, complete bipartite graph. So it turns out that these are the two dominating mechanisms when p goes to zero. So I'll give you the asymptotics for this, but let's say h is connected, and so then I'll define what are of Of max degree, let's say delta. Okay, and let's say h star is the induced subgraph on degree delta vortices. So if age is regular, then h star is h, but if h H is regular, then H star is H. But if H is not regular, if you look at the max degree vortices, you can look at H star, H star is sort of the subgraph that you induce on those vortices. And I define IH star X to be the independence polynomial. I'll define what this is of H star, which roughly means the following: this is summation i H star. I8 star k x to the k summed over k. So I h star k is the number of independent sets of eight star of size k. So I have some graph. I have the high degree ones here, the max degree ones here. The max dp ones here. I look at the subgraph only restricted to that that I call h star. In h star, I look at the independent set. So, independent set, for people who are not familiar, so independent set is a set of vertices with no edges between them. Okay, Okay, so I take a graph H, I look at H star, I look at its independence polynomial, and then I look at the solution to this equation. So P H star, oh sorry, I h star X. So let's call theta to be the solution. So, delta was the boost that I wanted. So, I want to solve this problem. I want to find the best possible variating which makes the H density in my graph to be 1 plus delta times its original density. So, I look at this number theta, and it turns out that this is asymptotic. So, phi h H N P delta actually roughly looks like n square p to the delta log 1 by p. In the triangle case, delta is 2. So n square p square log 1 by p. And this is going to be two things. So one is this is exactly theta if it's regular, if h is irregular. And it's a minimum of two things. Minimum of theta. Of two things, minimum of theta and one half delta to the two over the size of h. Okay, it's a sub complicated looking thing, but it's not super important. But so the sub so basically what it means is the following. So so there are two things here, and the solution is a minimum of them. So the two things actually correspond to these two constructions. So the byte. So, the bipartite construction corresponds to theta. If you do it optimally, you will get theta. The click constructions will give you this. It turns out that if h is not regular, then the click construction does not make sense. And you only have the bipartite construction, in which case this is what you get. So, this is jointly with Patacharya Lubetsky Zao. Zhao in 2016. Okay, so the regime of P must be mentioned. So this is, let's say, for P much, much bigger than n to the minus 1 over delta. Delta is the max degree of H. Okay, so this basically says there are these two competing constructions, and depending on which one does better, that is exactly what dominates this mechanism. Any questions? Any questions? So I'm out of time, so I'll continue with this in the next lecture, but I just want to finish with another related setting about, so subgraphs are polynomials of independent bits. Another related setting is arithmetic progression. In random subsets of let's say 1 to n or maybe even Z mod n. So essentially what you have is you have some interval 1 to up to n. You pick each entry independently with probability p. So you get some random subset S, which is a subset of 1 to n. And let's say you care about And let's say you care about the number of arithmetic progressions. Tk is, let's say, the number of arithmetic progressions of length k in S. And so you can actually develop a similar theory. Instead of the subgraph is nothing very special, these are also polynomials, but it has instead of sort of tools from graph theory being handy. Here you'll have some arithmetic commentaries coming to play and. And with Bhattacharya, Fernando Xiao and Ifei Xiao. I forget when, maybe, I don't know. I would like to say maybe, I don't know. 17, maybe we proved precise. Symptotics for the corresponding cultivation problem. And it turns out that here actually things like this come out. So, related to the following question. Given M with subset A of Z, let's say even of size M maximizes number of K APs in A. I mean, like, so actually, it's also not unrelated. So, the sub in the subgroup problems, actually, So in the subgraph problems, actually, things related to this is also relevant. Like suppose I give you some number and I want to look at all graphs with this given number of edges, which is the one actually which maximizes the number of triangles. Here, the corresponding question is, I give you some number, I look at all possible subsets of size M on Z, which is the one that actually maximizes the number of gate arithmetic progressions in A. So for K equal to 3, this was done earlier by Green. Done earlier by green and CISAC, and we extended to all K. And the answer is actually not very surprising. So the interval is an extremal Z. So, unlike the graph case, where there were two competing candidates. Candidates, sorry, the hub, authentic click, and the click, in the arithmetic progression case, planting an interval is asymptotically optimal. It's not to say that there are not other, you can see that if an interval is optimal, any dilation of the interval will also be optimal, meaning that you can replace an interval which is an electronic progression of actually common difference one by any electronic progression and of the same size. So that will also work. But those are. Over, but those are, of course, of smaller order terms, the number of such choices. But the leading order in the graph case is dominated by clicks and anti-clicks, in the arithmetic progression case is dominated by intervals. And so I think I'm out of time. I did not sort of talk about how to actually approve large divisions. In this case, I only talked about the variational approach of things, variational sort of side of things. And in tomorrow's lecture, I'll sort of start with that, hopefully finish it quickly. Start with that, hopefully finish it quickly, and then move to how some of these ideas and maybe new arguments are needed to understand other sorts of nonlinear functions of graphs like the edge of the spectrum, the largest and the second largest and the first few largest eigenvalues. So I think I'll stop here. Yeah. So let's give everybody the opportunity to unmute their mics and we'll give And we'll give a round of applause to Joshua. So, John is asking about the express solution of