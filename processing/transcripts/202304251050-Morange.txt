Thank you. Yes, so it's a pleasure to be here to present you a kind of overview of background and signal shapes with like an experimentalist take on that, showcasing what we do in the LHC experiments, what Atlas and CFS in that regard. And to start, just to emphasize why we are so much into background and them out shapes and the associated And signal shapes and the associated uncertainties, I would like to start with the statement that every analysis that we have nowadays is basically a multi-dimensional shape analysis. Traditionally, there is a split between what you call a cut and count analysis, where you just evaluate the number of background and signal events in some regions after selections. So for example, here, basically you have a table, okay. For example, here, basically, you have a table and a shape fit. So, like these two gamma gamma, as we have seen, where you are looking for a bump on top of a spoofly fully baggy distribution. Those kind of fits, you being usually more than 60. But more than that, there are actually hidden shapes in all analysis, because there is no analysis with just one signal region. You have now multiple signal regions, control regions, and they are not completely. Regions, and they are not completely independent. You really want to correlate what you learn on your background from a control region to a signal region. Things are also correlated between different signal regions in your overall fleet model. So, you need to extrapolate from one region to another. And this is a shape effect. And that's why we really need to be accurate on signal and background shape in all cases for all analysis records. All analysis that we are doing at the elevation. And with the need for shapes comes the need for modeling uncertainty. And they are more and more important nowadays. First, because we have very large data sets, okay, 114 inverse center of data in both Atlas and CMS using HC run two. Now already over 14 versus run three data. So the statistical uncertainties become smaller. So statistical uncertainties become smaller and smaller, even in searches for high-mass particle now origins of lots of statistics. And with large data sets, we have now extremely precise detector calibration. Experimental effects are known at Bermuda level or even below for electrons, muons, and photons. And even more difficult objects like jets have a efficiency. have a efficiency of tagging digits, these are known to better than one percent. So there have been large reductions in experimental uncertainties in the past 10 to 5 years. And therefore, many analysis start to become dominated by signal and background shape modeling uncertainties. So that's something we really have to be careful about and we have to deal with it. So modeling is really a big So modeling is really the leading concern in many analysis. So the first thing we want is to have good modelling of our signal in background out of the box. And so a huge success from the past year has been the move to next-to-leading order generator for most of the processes that we are simulating in the Atlas and CMS analysis. And this is all the more important that the use of advanced machine learning techniques requires. Technique requires excellent modeling of the correlation between the multi-dimensional variables that you are putting all together. So, we need to have good modeling and we need to have small modeling uncertainties, which are obviously easier to achieve when you have good modeling out of the box. And there is really at the heart of the analysis design to choose the right experimental techniques, the right selection to keep the modeling uncertainties. Keep the modeling uncertainties under control with lots of techniques involved that I'm going to review. And just to show you one example of Higgs to gamma gamma, like the leading uncertainties, and you see that many of them are from background modeling. So the leading uncertainties are on the top, and when you go to bottom, they are less important. We have lots of uncertainties from background modelling, and some from signal modelling, which are quite important in this analysis. And so, let's say one of the main ideas to reduce the uncertainties is to use the data as much as we can, because our best Monte Carlo is the actual data. And so, you have a full spectrum of experimental techniques going from more theory or Monte Carlo driven to more data driven that the analysis are using depending on on their particular cases. Are particular cases. So, very heavily theory-based uncertainties or signal uncertainties, also when you have backgrounds without good control region on them. So, that's what you tend to use. When you start to have very good control region, you have something in between. Basically, you use you make use of the profile likelihood to profile your uncertainties, upload your uncertainties, and And constrain them to reduce them and have, in the end, actually extrapolation uncertainties from control region to signal region. And in some cases, you can have like almost truly data-driven uncertainties. So smooth background description, like H2Gamaga, I don't have seen previously, and also embedding techniques. So there are really a full range and A full range, and we are picking in all these aspects depending on the cases. And so, in the next 20 minutes or so, I'm going to show some particular examples of these different cases to highlight what happens. Okay, so starting with background shapes and the textbook examples of Monte Carlo-based uncertainties for a background. For a background. So it's the search for peaks produced in association with a top quark pair and decaying to a pair of B quarks. Where you have a very masky background, which is a simultaneous production of two top quarks and two B quarks, which is your dominant background, so like the reddish thing here, which is a very complex process to model by any Monte Carlo. So you have this nasty background and a small C. Nasty background and a small signal-to-background ratio. And so control regions are not enough to perfectly control this background. Because if you look at what the analysis model looks like, you see that the overall cross-section of this background, because it's dominant in your signal and control regions, you can constrain the total cross-section very well. Using profiling, okay, you you constrain uh um the cross-section don't too. The cross-section down to 10% or even less, so that's very good. The problem is that, despite the fact that you have a control region, you have huge extrapolation systematics going from the control regions to the signal region. And so you end up with like five to ten different uncertainties, which are two-point systematics that try to cover your uncertainties. To cover your uncertainty on the knowledge of the description of this background by Her Monte Carlo. And all these uncertainties are actually very large and have a huge impact on the total results. So the total uncertainty in your signal strength in Atlas is like 0.25 due to this modeling alternative. In CMS it's a little bit smaller, but it's still on the same ballpark. Um so this is really a very difficult case and it's Very difficult case, and it's a huge amount of work to achieve that, and you are still dominated by it. So, that's really what you want to avoid whenever possible. Another point is that even when you have very good modeling of a process like TT bar, which is a process which is everywhere in all analysis of the LHC because of its extremely large cross-section, it's actually Large cross-section, it's actually difficult to have perfect modeling everywhere. Because it's so ubiquitous, TT bar is a sizable background even when you don't expect it. And that's because of limited experimental efficiencies. So even if you veto BJET, because TC bar now only you have BJET, you can end up with large TT bar backgrounds, so here in the heat W analysis, or you can end up with fairly sizable TC bar backgrounds. Up with fairly sizable T-bar background here in the VHDB analysis because you are in a very specific weird corner of the phase space because of your signal acceptance and your analysis selections. So even if we have in general excellent modeling of the T-bar background in the bulk of the distribution, there are clearly difficulties in having good modeling in plates of distributions, in corners of the faces, where you have Where you have first issues to get enough Monte Carlo statistics to describe your background well. So we end up with filtering and slicing strategies to improve on that. And then you end up with similar issues of trying to extrapolate from the bulk of your distribution to the tails of your distribution, which are the similar region. And you end up with not so small systematic uncertainties related to that. Symmetric uncertainty is related to that. Okay. Now moving to process where we have fairly good modeling and we have good handle on the uncertainties. So in the case of search for the Higgs decaying to peak works produced in association with the vector boson, okay, where your backgrounds are W plus B Z and Z plus G Z. And Z plus B jets, OC2. So those backgrounds are the largest one in this search. And the difficulty, one of the difficulties, is to generate enough Monte Carlo events in the relevant phase space, which is fairly high momentum. And you need digits on top of that, so you need to have filters to enhance the number of events you have. It's possible to have next-to-linear order samples, but they are extremely costly to produce because of filter efficiency, the spread of Monte Carlo weights, and just the difficulty to produce those. But they are definitely worth it. The previous CMS analysis had to continue using leading order samples because at that time they did not manage to get the next linear order runs. And they had to pay a price in that they had to basically have hadocy weighting of the linear sample in the momentum of the vector boson with a fairly sizable uncertainty associated. So it's costly and difficult to move to next higher order generators, but it's really worth it for the goal of reducing the Uh the goal of reducing the money uncertainties. Continuing a little bit uh on this analysis, which is a good example of using profile XiO2 to really have a good control of your modeling uncertainties. Uh because this analysis has the advantage of having pretty good control regions, uh basically inverting some selections so you can get very good sidebands or using as in CMS a multi class B D T for that. CLS and multi-class BDT for that. You end up with control regions which are fairly pure in each of the backgrounds that you want to control because there are several important backgrounds that you have to control them all together at the same time. But because you have good purity in your different control regions, then you can really make use of profiling and having those control regions allow you to constrain your background cross-section and some of the background shapes. Cross-section and some of the background shapes, and you end up with quite a bit smaller extrapolation uncertainties from the control region to the signal region, because the control regions are fairly close to the signal region. Still, it's not an easy work in the sense that you have to choose correctly what are your modeling uncertainties. Okay, so if you Okay, so if you compare different generators, if you change the tuning inside a given generator to evaluate your overall uncertainty in this pattern, this is still a very difficult question. Plus, you also have Monte Carlo stats uncertainties when you compare several generators. When you compare several generators, ideally we will have to, you would need to have as many Monte Carlo events in both of them, but since they are very costly, Of them, but since they are very costly, we can't afford. So, we have to resort to some smoothing techniques to try to reduce that. Going to smooth background, where we can be very much more data-driven, with the textbook Higgs2gamma-gamma example that has been discussed in previous talk this morning. So, here we use semi-parametric model, okay, fit of analytical functions, which are definitely. function which are definitely more accurate than resorting to Monte Carlo simulation and that applies to quite a few Higgs analysis but other analysis like digest searches and things like that. Procedures to evaluate the uncertainties have been established for a long time with a long time disagreement between ATLAS and CMS. With CMS using this disk profiling, that has been mentioned this morning. And ATLAS basically selects And uh at last basically selecting one function and estimating uh what is like the maximum bias that could be with that function, what is called the spurious signal, whose estimation requires vast amount of Monte Carlo events, which is clearly a limitation to go for higher immunosity. But on this new techniques have emerged in the past years to try to To try to improve on that or just test new ways to evaluate smooth pattern distributions. So, just showing few examples here. So, in the Higgs-WMU analysis, they managed to have very simplified, super-fast simulation of leading order samples, which are clearly accurate enough to Are clearly accurate enough to describe well the background. So, when you can do that, things are quite doable. Otherwise, resorting to a new way to estimate smooth background, such as function decomposition here, or using machine learning-inspired techniques, such as Gaussian processes. Both are showing very good prospects in either reducing the amount of spurious. reducing the amount of spurious signal that we have in our analysis or just replacing the analytical fits with uh those methods. Okay. Sorry, maybe it's a stupid question, but can you say more about serious serious expansion in the last type of one burden, right? I don't remember that one, but it's well, yes. As far as I remember, it's just a more general expansion than polynomials over higher classes of functions. All I can remember. Okay. Another important technique which has been used in, which can be used in few In which can be used in view analysis is what is called embedding, which is really almost fully data-driven. So textbook example is a Higgs tutautau, where you have that tutautau which is close to the signal and which is hard to model with the correct level of detail. And yeah, so you have three deficiencies in near Monte Carlo. Near Monte Carlo simulation. And on top of that, you also have this issue to get enough Monte Carlo statistics to describe things correctly. And so the idea of the embedding is to take a well-understood process in the data, which is close to the one that you want. So basically, we want to show Z to tau tau. Okay, so we start from Z to mu mu, like actual z to mu. Like actual Z to mu mu events. We replace the muons in the data by simulated taus event by event. So that works extremely well. So it's not without uncertainties, but it it's still resolving a lot of the modelling issues by the methods. The modeling issues lighter experience. So that's some other things which are not fully correctly described. That's under the cartet. But it can be tested using simulation if that matters for your use case or not. And clearly, that's a huge benefit in reducing the bias and reducing the uncertainty compared to using the C. The uncertainty compared to using the simulation of making use of the technique. Clearly, this has been extremely powerful for Higgs to target our class in VBF Higgs to BBR analysis, embedding of Z to lepton by replacing the lepton with B quart to to time that background which was very difficult to model. To model. So whenever it's possible to use it, it's a very powerful feature. Yeah, thanks. Okay. Yeah, just one word on dealing with hybrid cases, because in analysis we have always several backgrounds that you want to deal with and we tend to use more and more machine learning techniques. This comes with additional complications. So Complication. So, showing an example of a non-blitz particle search, okay, so they wanted to train a multi-place ability, separate between their signal jets, QCD, and a beam-induced background. So, their jets and QCD, okay, they had Monte Carlo simulations, but the beam-induced background was coming from a control region in data, okay, but that sample was known to have significant fraction of Q C D Fraction of QCD contamination in data. And some of the input variables that they use in the neural net were really badly modeled by the simulation. And so if you don't pay attention, your neural net doesn't learn to really separate your signal from your background, but it learns to separate the data from the Monte Carlo, which is really not what you want. Okay, and so they have to resort with more advanced machine learning techniques. So in that case, adversarial training. Case adversarial training, basically telling their neural net, no, no, please don't learn the data Monte Carlo discrepancy by putting data and simulation in the control region so that the neural net knows how to differentiate data from Monte Carlo. And basically you go from this kind of data Monte Carlo disagreement to something which is working extremely well while still keeping the power of your neural net to separate the The power of your neural net to separate the signal from the background. So, you really need to use very advanced techniques to deal with such cases. Okay, so in the last few minutes, now I'm moving towards signal shapes. And so, okay, so this is something that we have seen yesterday, right? So the signal shapes that you basically see where you want to compare your data points to your signal, okay? Data points to your signal, okay? It's a result of the convolution of going from theory parameters to theory modeling, detector interactions, and then you end up with your observables. So it's a big convolution, and what are signal uncertainties are uncertainties that affect all the terms in this convolution. For background shapes, all the use of control region and data remote techniques are actually way. Techniques are actually ways to shortcut this huge convolution to reduce the impact of the modeling uncertainties. For single chains, you have no other way to, you need to have them everywhere. So, just showing quickly an example in Higgs case, where we start to be very sensitive to theory uncertainty, so-called underlying event pattern shower, typically. Where we have to resort to either comparison of different predictions or variation tune of the prediction inside a given Monte Carlo generator. And that leads to sometimes fairly sizable uncertainties, just because that's hard to get them small. Fairly sizable uncertainties in the Uncertainties in the typical ethics to gamma gamma analysis in some part of the phase space, in particular. So you might say, okay, if I just evaluate my signal in one way, like one signal strength, I have huge uncertainties. But maybe what I can do is to go for differential measurements, more precise, and that way I can avoid some of these uncertainties. These uncertainties. Okay. So instead of measuring one signal cross-section, you just want to measure simultaneously cross-sections in well-defined parts of your phase space. So for the Higgs, they are based on the Higgs production kinematics, that's what we call a Higgs simplified template cross-section. Where instead of measuring one single strength for Higgs, you really want to measure it differentially in many different bins at the same time. This requires a much more refined set of theory uncertainties, but at the same time it allows you to reduce them a lot because if you measure all of these beams individually, then you don't need uncertainties to account for how the signal is changing in this bin with respect to the other one. And what remains are remaining uncertainties inside each bin. So overall it's net reduction. So, overall, it's a natural reduction of signal uncertainties when you do your differential measurement. But maybe we'll finish with that. The uncertainties they don't magically disappear. It's just that you have factorized your problem into a measurement of differential cross-section. Then, if you want to go back to your underlying theory parameter, if you do an interpretation of your differential measurement, then the uncertainties appear again. And just to exemplify that, so I fast. Examplify that. So, AFAS has recently showed a measurement of the Z-boson differential measurement using run1 data. It's an extremely precise measurement. It's a joint measurement of 1600 parameters, where modelling uncertainties are below the permit level. They are maybe completely negligible compared to other uncertainties. But once you take those extremely precise data Take those extremely precise data and you interpret them to measure the strength of the strong coupling constant, alpha s, then suddenly all your underlying theory parameters appear again, and the final result is really dominated by PDF uncertainties and scale variations. So you have not logically made your uncertainties disappear, you have just factorized uh your problem, which in a way is a net game because you can already uh Separate things better. Okay, and basically I'm done. Thanks for your attention. Thanks, Alec. Pretty much perfect time. So shall we just take any quick questions on the talk, then move on to the discussion? So, can you say a bit more about this first signal approach that Advis is using? Like, it's not the first time some of us have heard about that. And I was like, do you think it's related to what? And I was like, do you think it's related to what was discussed in previous talks about kind of biases and kind of overfitting the scene? Also, if you can say a bit more about that. Yes, that's that's it, yeah. So I'm not an expert in that domain, but yeah. So the idea was you do some kind of model selection. So you test uh several, let's say, different degrees of pe polynomial or different shapes, okay. Or different shapes, okay? And then you select one, but you need to take some uncertainty in that selection. And the way you do that is using high statistics Monte Carlo, basically. Okay, you just take your Monte Carlo, a background Monte Carlo, you fit it with the function that you have chosen, and then And then you evaluate with toys how large could be a bias by using that function. And that's basically what you call your spiral signal. It's how you take your background Monte Carlo, you fit with an analytical shape, and then use. and you inject a possible well and you add in your fit and a possible additional signal and you see how large that possible signal could be. It's an estimation of the for your background model how much you underestimate the amount of background events in your signal region. And you control that by looking at those sort of signal free regions? You control that by looking at You control that by looking at mono-crown samples and different classes of background models that all fit the background functions in the control regions up to a certain degree to see how much additional data you could have in your signal region, which actually is coming from a mismodeling or an underestimation of your background model. About underestimation, not overestimation. Because the problem seems to be that those background models kind of sort of fit the similar, in which case you overestimate. Sort of defeat the single, in which case you overestimate the background. So it's not that it's somehow the underestimate. It's it's the underestimation because it's basically trying to say how what is the probability that you're claiming a signal when there isn't one. That's the only part that you're trying to cover. Right, okay, so you're more worried about claiming a signal than there is. Yeah, we don't really care about claiming no signal, but worries about nothing. So, I wonder, unless there are any other questions on the talk, it seems like we're floating into the discussion session already. But are there any remaining questions? Because otherwise, I'll press stop and start and then we'll move on. So, stop.