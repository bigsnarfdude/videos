What I'm going to talk about today lecture is TWAS. And instead of doing the linear to us, I'm going to do non-linear TWAS. And because I'm going to do non-linear to us, it's more challenging. In sense, that is saying I need individual level data. But in many situations, we don't have individual level data. So what we propose to do is that we do imputation. So we impute the tree. And then we can combine steward summary data with individual level data. So that's what I propose here. So that's what I propose here. And I think my motivating example actually is ADSP data. I think I will mention it briefly. And then I'll end up with a short discussion. So now we already talked about the TWAS or predict scan or Mendelian randomization. So one reason I like actually the approaches because actually they have causal interpretation. So here's my causal model. Here you can say X maybe change expression, say actually a genes expression, say actually are sniffs. And then why is there the outcome of interest? And then my question of interest is the beta. So whether actually X is causal to Y. So when I say true model here, I mean actually assume it is the true causal model. And the impractice is because we have a lot of technical functions. So u here could be, for example, environmental effects on zero. And because we don't observe there, so if we just do a simple regression, regress y on x, it's well known that the Y on X. It's well known that then what you need to get is biased estimate of the beta or causal practical beta. You know, of course, bias inference, everything, right? So, so then in causal inference, we have this so-called IV regression. So, we treat Z or SNUX as instrumental variables. And then we do the first model, station Y, we impute X, we feed X, we impute the gene expression, and then the magic actually is that instead of using X, if we use the impute. Use the x, if we use imputed x, then under some suitable conditions, and it's guaranteed that your inference will be correct. So, even you have hidden confounders. So, this is why people talk about this two-stand display as a causal inference, because my beta here is a causal effect. So, I want to emphasize here is that my goal is: I want to identify actually causal genes for my treat. Of course, I have to impose a lot of strong assumptions, but at least that's my goal. That's my goal. And a possible limitation of, oh, this is a standard kind of TWAS. A possible limitation is that here I assume that the gene question level outcome is linear. And the impregnation is that maybe it may not be linear. And because we assume it's linear, we were not infinitely typewriting, but we are going to lose power. So then we thought that maybe we can finish some non-linear models. But before we took the effort to do it, so we Took the life work to do it. So, we first try to use some parametric models. So, what we did first is instead of just having this linear term here, we also have x squared. So, we're going to impute the largest x. We also need to impute x squared. So, expected value of x squared, of course, is different from square of expected value of x. But anyway, we can still impute that. And then we applied the G-Text data and the UK Biobank data. We did find some additional causal genes or pupil genes. Causal genes or putative causal genes. So then we say, okay, let's move on. So we want to create a more kind of powerful or faster model, a linear model, so it's a neural network model. At the beginning, we want to just use this deep ID model. So this was proposed in machine learning literature. And then when we applied our data, actually, we found a lot of problems. So at the end, we had to actually develop our own methods because the delivery is less negative. Let's make deep uh no yeah deep learning by Ivy Refreshment. So uh and and and there's actually one downside of doing this kind of fancy models is that because neural network models, there's no simple state theory. So, if you want to throw the inference, we have to do this sample speed. So, we use one part of data to train the model and then use the remaining part to do testing. So, that's actually our downside. If I'm interested, you can read this paper. You can read this paper. So it's already published. All right. Oh, no, I'm going to skip it. So, if some of you have interest, I can't talk about it. Yeah. So, not just the quickest review is that the good thing about the linear TWAS is actually, you can use the TWAS summary data to fit the model. But a lot because in our stage 2, or stage 1, I'm speaking to use linear model. Stage 2, I want to use non-linear model. And you can say that. And then you have you can say that I I cannot use GWAS summary data. So here just those of you who are not familiar with with with this is that here is the typical data, right? You have each row is an individual and then you have individual level data. Then of course you can fit whatever model you like, right? But with the two of the summary data is that you only have actual issues. The data changes here, giving you the marginal association, and then you also have standard error and so on. And then of course the trick there is that, for example, if you want to create three Is that for example, if you want to fit a model, in your model, if you use ordinary square, then you can see that the z-parameter is marginal association parameter. And then you want to figure out this correlation matrix or LP matrix, you just use some reference sample. So that's why it works. But for non-linear models, then this will not work. I'm not aware how you can fit a non-linear model using GWAS summary. Okay, so this actually was a problem. Actually, it was a problem. We had been thinking for actually quite a few years. And at the end, at least for me, I realized that it almost impossible to fit a linear model using to our summary data. So then we thought about how about we step it back just a little bit. So suppose I have some actual z, but I don't have y, and then I also have another q summary data. Is it possible actually I can impute the y, well I can estimate y? Why can't I estimate y? So that's our question. If it turns out, actually, it's possible. My turn actually is very simple, just based on some L. And this is the square. By the way, this is also probably not sphere. So here, again, let me kind of give you the problem here. Problem is that I have a TOAS summary data, right? And based on two summary data has those beta stars. So those are marginal association parameters. And now I have another data set. I have another data set. And then I have the genotype, Z, but I don't have Y. But of course, for G1 summer data, I can imagine I have corresponding Z star, I have corresponding Y star, but I don't have Z. I'm just using Z for the purpose of notation. So the idea of how to impute Y is this. If I have Y, well, I can calculate marginal statistics. Very simple, right? Just listen. And the same because I assume that my TOR summary data. GWAS summary data and my current data are from the same population. So I assume that they should be close to each other. And then that's the idea. I can write them as loss function. I just want to minimize their differences. And then you can see this is just a linear model. So I can get the closed form solution. And here I'm using general universe here because I have standardized my C such that I can eliminate the intercepted term. And after I do that, this is not a full rank. This is not a full rank, so I have used general inverse. So there are some complications there. But basically, this is like a piece of square. Okay. One interesting thing is that I don't need to assume actually our current data are independent of the data, chiwa summary data. In particular, is that if my z is the same as the z star, so if you give me chiwa summary data, you also give me the genotype data. I can't exactly recover the white star. It's guaranteed. It's easy to. It's it's guaranteed. It's it's easy to prove this. Okay. Uh so here I need to make some comments about the other methods, right? So some of you may say, well, why don't you just use you know one of many many existing PRS methods? And you know if you want to do risk of prediction, yeah, it's fine. But for purpose theory is that because actually I'm going to do is downstream genetic analysis. It's not actually a good idea to do this for why. Because in those models, they already specify which SNPs in which way influence the outcome. Which will influence your outcome? It already tells you the whole story about genotype and phototype association, so it would not work. And then, of course, there are some other methods. I think Mr. Kemp is here. So the proposal method, I mentioned this water complete. So they use some other variables to predict the Y. I'm not sure yet. We just started looking at this more carefully. So right now, my concern is that maybe we're going to lose some. Maybe we're going to lose some specificity, right? Think about if you use a cardioascular risk factor to predict AD, you're probably going to end up with some SNPs associated with cardiovascular risk, but not AD. But if you use imputed AD, then it's probably going to be associated. So that's what I mean. You may lose some specific association with AD. So, all right. I'm going to skip this. So, here actually is why we were interested in this problem. Because a few years ago, we were funded by NIA. And then we want, so what we propose to do is we want to identify pederty posal genes or proteins for AD. And then at the last time lecture, ADSP sample size is only above 15,000. And currently, the new release of the data, most recent release of the Release of the data, most recent release of data sample says only 30,000, 36,000. So it's very, very small. And we were ambitious. We want to create a complex neural network models. And we have to do samples and so on. So we knew that then we probably going to have actually very limited sample size. But on the other hand, there are many actually published QAS summary data, especially in the last few years. And these are two actually big ones. So I think this is by Conco at Hawaii. So we call it IGAM 2000. Top. So we call it I-GAM2. So that's the second version of I-GAM2. So you can see even just the number of cases is like the sample size of the ADSP. And in the second one, EADB is even bigger. But one possible downside of EADB is that it includes not only clinically diverse cases, it also includes those so-called proxy AD cases. So, proxy-AD cases is defined as someone whose parents or siblings have dementia. They are going to say, okay, I'm going to. Have dementia, then you're going to say, okay, I'm going to impute it. This person is going to have AD. You can see it's a very rough method. But in the terms of that, actually, at the beginning association testing, it can increase power dramatically. So this has become very, very popular. So I'm going to show you some of these. I'm going to compare with this method. Parks and AD. So in some situations, you get all back data, because you have errant information, you can do this, you can impute your treat using this sort of proxy cases. Using this so-called prostate cases. But in some other studies, for example, ADS-PDA, we don't have parameter information, we cannot reach it. So here is actually what we're going to do is we do have access to uniquely independent level data. So we have the Z, we have all the genotypes of all those almost 400,000 individuals. Here we just look at the wet-weighted sample. So the sample says ambulance lower than 400,000. We have all of their individual level channel types. And then we also have. And then we also have summary data of IGAM2 or EADB. So we can use IGAM2 or EADB, also the genotype data of the YouTube data. Then we can impute the white hand. And then we can treat this as we have individual level data. Then we build a neural network model, whatever model you like. And then we after we build the model, we apply to the EDSP data to test the association between imputed Y and imputed our fitting model of the gene. You can put it the gene or imputed the protein with observed or diagnosed ADK6. So this is training, this is text. So I'll show you some of the results. So the first application is TOAS. So stage one is we use G-TEX data, we impute the gene expression, and then is that for UK BioBank data, we have Is that for UK BioBank data? We have our method. We build AD cases, and then we build on machine learning model using our finger method, and then we use ADSP data to do association testing. So, here actually are the three genes that can be detected to be detected by any of the methods. I will explain this. So, what is the ADS-P here? ADS-V here is that we fit on a linear model using only ADS-P data for both training and testing. Data for both training and test. So, this is actually the standard approach what you're able to get. And then I GAMP here is our imputation method. So, we use IGAM data to impute ADHS for UK biobank individuals, and then we build them up, then we apply to the ADSP to do association testing. So, for example, Perisin, we can detect one of the well-known AD genes. And the EADB is we also do imputation, but we use EADB summary statistics. Summary statistics. And the proxy this way is, as I mentioned, is that because you could output your data, you do have parameter information, so you can use this so-called proxy PD cases. So in this situation here, it's really significant. So TWAS L is just the standard TWAS. Linear L means it's linear. And as I mentioned with this parametric approach, so we're testing not just a linear term, we can also test a quadratic term. So here actually they are almost the same. So, here actually they are almost the same. Possess three genes. So, anyway, this is that is it. Apply our new method. Maybe you can detect some additional genes. So, again, as I mentioned, because there are not many actual signals in any G1, so it's really difficult to detect. And the G-TAC sample size is small. So, then we applied it to the UK Ball Bank protein-mechanical sample size much bigger. And those are actually proteins we can detect. And I can similar organization. And I can cellular and organization is the same. So, any of proteins that can be detected by at least one method. So, you can say here: well, yeah, if you just apply our standard neural network model method, you can detect this one, this protein, and other methods cannot, but other methods also can detect some association that would be missed by non-linear ADSP or linear ADSP method. For example, those few proteins. So, I think this is actually quite a general conclusion we have. Actually, quite a general contribution we have is if you apply the linear vector, you may be able to detect some additional genes, not many, at least some of that. And it's probably complementary to linear method. So if you're interested in the Miami plot, here I just want to show you is maybe my Miami plot. So the top one is in UK Bible, indeed, actually, there are a few thousand diagnosed AD. Few thousand diagnosed AD cases, but because of relatively young individuals, younger individuals in the UP data of course due to the nature of late onset of AD. So you don't have many cases here. But if you use that, you can detect some signals here. The bottom here is based on the IGAC, much, much bigger ADG was data. So you can detect those few big signals. I think this FOMO 19 corresponds to up to E4. To make up forward, so you can detect it. But I probably should put them together. And then this one here, I compare with them. This is using our LS implementation. This is using AD proxy. So based on parent information, you can see GRAs are similar. But if you compare this one with the previous AD GAMP, I GAMP, I think it's closer to this one because we queued it from Chrome I GAMP. So usually what we From ICANN. So, usually, what we get is very similar to ICANN. I just want to show another thing. If you use SHA-S, I mentioned it to you, PRS would not work. If you use SHA-SCS, which is based on a shootage method, you use many, many SNPs in PRS. You can see it will give you a lot of false parsing. And if you use LDPRDIC tool, which is actually more sparse, yeah, it will give you something, maybe also some both false possibility of false magnetic. Those false positives or false negatives. So I'll end with a short discussion here. I think this idea of using GWAS summary data and then some known genotypes to do treating putation, I think it's a general method. I personally feel it's obvious, but I I don't know whether you agree with me and uh I'll be happy to discuss with you. I feel that it's complementary to many existing treatment imputation methods, like True imputation methods like autocomplete and so on. And this method can be also applied in many other applications. But I hear I skip a lot of the technical details. There are still a lot of open questions. For example, theory. I suspect, I hypothesize that the LSI computation will give you consistent estimate of genetic components of a treat. So what is my right-hand? My right-hand is generally the expectation of the Essentially, it's an expectation of the white, given all the genetic information. So it's why I call it genetic control. I think it's consistent, but we're dealing with both large and large P problems. So proof is not easy. I don't have a proof yet. I hope we can have some results. And even for computation, actually, even as I told you, it's very simple. It's just ordinary square actual problem, but I have this big matrix. And so when we do this in here, for you to go back, we have the bench by bench. Have batch by batch. And there are some downsides of doing that. And of course, what we care most about is the inference, how it will influence statistical inference. How much we're going to lose power. Sometimes we might actually have inflated type of error. Here I just want to mention just very briefly. So I talk about the non-linear inference for stage two of TWAS, and then some of you probably ask, how about the stage one? Personally, I think stage one will be probably less productive if you consider non-linear models. Consider nonlinear models, okay? But maybe I'm wrong. So that's where we also are doing something. So the stage one, we also try nonlinear models. And we, I mean, we get a similar conclusion. Depending on the genes, for some genes, the nonlinear model actually can give you higher R-squared. But for some genes, for many genes, probably more than half, you're going to give a lower ones. And also, there is this also issue. I mean, whether you can improve the gene expression, if gene expression, your computation. If gene expression imputation using functional annotations, I know it's a different people say it will be very helpful. What do we found actually? It still makes the message. For some genes, it will help, but not everyone. So that's why. The second one, of course, everyone is talking about large language models. So the natural question is if you choose the DNA sequence just as a text or a book, right? And then you can input actually the whole DNA sequence. Input actually the whole DNA sequence, you can train actually a model, a large language model, and then you input the DNA sequence to predict the genes question. How it will work, okay? I think it's not a zero yet, so probably need to do more. Yeah, all right, yeah, so those are actually my students. Yeah, put in possible, yeah, thank you. So I have a question. So I don't know if Jose is still on scope, but is there a difference between Boxy cases and a family history that she was talking about? I think in her method there is much more direct work. She's doing something like a conditional modeling. What we did is we're saying, well, we say if one occurrence has AD, then we say, okay, this person is going to have AD. Yeah, but she's segregated a case that had AD and we can have a Cases that had AD and the cases that had a parent had AD. Right. Right. Considering them as K, which I think is pretty much the same thing you did. I just wonder, I'm just wondering if we would end up with different results. Oh, they're going to end up with different results, I think. Yeah, because this is much the same product. Parks AB cases is much simpler. You're just modifying what is it is more complex. I think it's more complicated. Okay. It could still be more complex if there are cases that have two affected pairs, but I think probably it's almost exclusively one affected pair. Almost exclusively one affected parent. So you've got a bunch of cases of one affected parable. Okay, where you're doing a GWAS on that subset, you're doing a GWAS on one subset, they actually have a case themselves. So I'm just wondering, actually, is it really fundamentally, I could choose one to answer a different spirit. I want to answer that question. I think it's very different. This is Wolpakman or this will be right on the Pixamachi user. I don't know whether I mean, for example, applied here. I'm not sure. I'm not sure how to make it.