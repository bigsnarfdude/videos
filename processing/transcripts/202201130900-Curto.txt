The starting point conceptually is really Hopfield Networks, which hopefully are familiar to basically everybody here. And I just want to stress this idea of trying to encode memory patterns as attractors, fixed point attractors of a network. And that's kind of an idea that's captured in a lovely way. In a lovely way in the classical Hopfield model. And so, but here I want to, I guess, sort of the goal of my talk is to really shift the perspective from away from stable fixed points as our kind of traditional model of attractor towards looking at dynamic attractors as well. And so I have this question, like in terms of attractors, our focus is usually on stable fixed points. Is usually unstable fixed points. Why is that? I think it's because they're mathematically easier to study and identify. But unstable fixed points are just as important, even though you never converge to them in the dynamics. And so it's counterintuitive to sort of make a case that unstable fixed points are important. But the reason I sort of firmly believe this is because, at least in models that I study, they give Models that I study, they give us really valuable clues about dynamic attractors. And I think dynamic attractors are critical for understanding dynamics in the brain. And by dynamic attractors, I just mean things like limit cycles or quasi-periodic attractors or anything that's sort of an attracting set that is not just a stable fixed point where the activity still continues to move in some kind of rhythm. And then here's a little bit of an outline of some of the topics. Of an outline of some of the topics I will go, I will talk about in the talk. All right, so the setting that I focus on is not the Hopfield model, but it's a kind of modern version of it, I guess, which is threshold linear networks. And these have been mentioned a few times already in this workshop. They're firing rate models, and I'm choosing the threshold nonlinearity, which is right here, which is just taking the maximum between the argument. Taking the maximum between the argument of the function and zero. So you just cut off negative values. And the threshold in your network is really characterized by a connectivity matrix W, which is telling you the strength of interaction between neurons and some external input vector, which may or may not change in time. So a lot of the results I'll tell you about are assuming static input vector, but then some of the modeling and engineering that we do with And engineering that we do with these networks actually allow that to change in time. And one of the striking things about these networks, as much as they've been studied, but some people still don't know this, is that they really can capture kind of the full repertoire of nonlinear dynamics. So even though they almost look linear, right, they actually are kind of fully nonlinear in the sense that they can produce multi-stability, limit cycles, chaos, quasi-periodic attractors, everything. Attractors, everything. And in fact, you can get all of this behavior for effectively inhibitory W and a constant external input. So it's kind of a miracle because the equations are so simple. But maybe I would call it sort of following Sarah's comment from the other day, I would call this the unreasonable expressiveness of almost linear models. Okay, so here are some older results in the Are some older results in the symmetric case? So, I would say the mathematical study really started about 20 years ago with Han La Sur Sung, Slotine, and she collaborated. They proved results that analogous to the fields running under unconditional metric metric, deconvergence stable. And I also worked some more. I also worked some more on proving results about what collections of, we call them binary neural codes, what collections of neural stable fixed points you can get. But I really want to shift perspective, and I've shifted, you know, in the last five, six years to caring about both stable and unstable fixed points. And this is just a paper dump. But the motivating question is, what is the relationship between the attractors of a network and the underlying network? The attractors of a network and the underlying architecture. And here I really mean again, dynamic attractors as much as stable fixed points. So I really want to kind of look at the broader repertoire of nonlinear dynamics. And I just want to quickly go through an example that shows that these networks, even small ones, can have coexistence of many different types of attractors. And so I don't really think about it as. Think about it. I don't really think about it as there's a fixed point regime and then a chaotic regime or a limit cycle regime, but rather all of these things can coexist. So, this is the same W matrix, exact same connectivity for all of these solutions. The only difference is initial conditions. And so, in essence, these networks can support a variety of different types of attractors at the same time in the sense that just picking different initial conditions, you may fall into this limit cycle. You may fall into this limit cycle, this sort of strange or chaotic attractor, or a stable fixed point. So you have a lot of expressiveness here. And the special case that I focus on the most, although this is not, you know, we do venture quite a bit beyond this, but in the interest of time, we're just going to talk about combinatorial threshold linear networks. And these are ones where, so the trade-off, right, is like now we're allowing completely asymmetric. Allowing completely asymmetric W matrices, but we're going to restrict ourselves to just two different types, two different values for the synaptic weights. And they're both going to be inhibitory values. It's going to be like a weak inhibition, the negative one plus epsilon, and that's what you're going to get when you have an edge from J to I in the graph, and a strong inhibition when you don't have an edge. And we think of this as sort of an effective interaction from having kind of a C of inhibition or some kind of global inhibition in the background. Of global inhibition in the background, as illustrated here, together with excitatory connections. And so, when there's an excitatory connection, it sums with this sort of a strong global inhibition and gets you a weak inhibition as opposed to a strong inhibition. And so we think of this as kind of like a generalized winner-take-all network where all the neurons are competing with each other, right? But some compete more strongly than others, and the competition is directional. Directional. And so you definitely have activity that is interesting and shaped by the arrows of this graph. So just to give some fun examples, here's a very cyclically symmetric graph. It produces a very regular limit cycle, but it also produces for a different initial condition, exactly the same W matrix, exactly the same theta, right? I'm not changing the equations at all, just initial conditions. All just initial conditions, I can also get a completely different attractor, which I like to call the Gaudi attractor. And we can also get, you know, again, here I get a very simple sequential limit cycle, but also for the same network, a quasi-periodic attractor. So this is a projection of the Taurus trajectory in the seven-dimensional space. Okay, so how do we analyze the reason, you know, the main reason I'm interested in these networks is because they are. I'm interested in these networks is because they are dynamically very flexible and expressive. So you get a lot of nonlinear dynamics, but also they're surprisingly tractable mathematically. And the reason is because they are sort of locally linear. So you can think of the threshold linear networks as a patchwork of linear systems. So basically the arguments inside these nonlinearities in each equation are defining for you a hyperplane in the n-dimensional. In the n-dimensional space. And so you can think of these equations as chopping up the space into these different chambers. And when you've picked a side on each of these hyperplanes, so when you've picked Y1 through Yn, each one of these, you can pick them to be positive or negative, in which case you'll be on either the plus side or the minus side of the nonlinearity. Once you've selected, once you've gone to that point in your state space, now your equation. Space now your equations locally are linear because you either have a zero in the appropriate coordinate or you can drop the brackets, right? And you really just have the argument of the function. And so there's a one-to-one correspondence then between the fixed points and the allowed supports. So what I want to stress here is that because locally these systems are linear, your fixed points of the entire network have to be. Points of the entire network have to be fixed points inside one of the local linear systems, but not all of those fixed points will survive to be fixed points of the whole network. And the key feature here is that the fixed point has to be a fixed point of the local linear system that also sits inside the appropriate chamber where that linear system is governing the dynamics. Okay. And so the game then of figuring out the fixed points reduces to figuring out. It reduces to figuring out what chambers contain their own, what linear chambers contain their own fixed point. And so here's an example to show how, you know, how generically most fixed points are not realized. Most of the local linear systems do not actually get their fixed point in their chamber. So in this case, I've made a very simple n equal three example where you have a high percent because I can draw. Hypersonic because I can draw it right. So there's a hyperplane arrangement in three-dimensional space. It is partitioning the state space into eight chambers. Within each of these chambers, the system is completely linear, but then there are these boundaries. And only one, there's only one fixed point of the full network. Okay, so the eight fixed points corresponding to the different chambers, only one of them actually lies inside its appropriate chamber and is right here. And this fixed point you can think of as corresponding to a limit cycle. Corresponding to a limit cycle that emerges when you perturb around it. So you get this kind of spiral-out behavior. The different colors here on the right are showing the transitions between different chambers. And so obviously a Linux cycle is a non-linear dynamics phenomenon. And so you have to be transitioning between different chambers to get it. But it's kind of interesting how it emerges from this passing through. You know, passing through these walls and then having the dynamics change and then pushing it back into the central chamber. And it's kind of always spiraling out when it's in the central chamber, but then it gets pushed back in. So this is, you know, kind of a clue as to, you know, the unstable fixed point is kind of at the heart of this limit cycle, as you would see in a hop bifurcation, for example. And here are some, you know, some other examples of these are actually strange or chaotic attractors, and you can see these. And you can see these little holes here. There's actually an unstable fixed point kind of at the eye of these activity patterns as well. So we care about the unstable fixed points because we, you know, we believe and we have shown that they have a really tight correspondence to dynamic attractors. And so one of the lovely things about these combinatorial threshold linear networks is that we can actually prove a series of graph rules, which allow us to relate the fixed points. To relate the fixed points to properties of the graph. And so I don't have time to go through these in detail, but this is sort of my favorite part of this game: is that you can do this thing where you directly study the graph and infer the fixed point structure. So I'm going to give you one example of a graph rule. And that is if you have a uniform in-degree graph, that means every vertex is getting the same number of incoming edges. Of incoming edges, then you're guaranteed to have, if that's a subgraph of your larger network, then that subgraph will support a fixed point, meaning there will be a fixed point with only those nodes firing and everyone else silent, if and only if no node outside the graph receives D plus one or more edges from inside. So let me give you an example. So this uniform indicates the D is from the uniform indegree. So here is an example. I have a couple of three, I have a I have a network here with two three cycles, right? And this, the three, the cycles are uniform in degree, uniform in degree one. And so what the theorem tells us in this case is that you will have a fixed point corresponding to the cycle if and only if no node outside the graph receives two or more edges from the cycle, where G is now the subgraph, the cycle. And so you see that in the case of the 2, 3, 5 cycle, that is true. No no doubt. Cycle. That is true. No node outside is receiving two or more edges from it. So that fixed point survives and there's a corresponding limit cycle. It's an unstable fixed point and it has a corresponding limit cycle, which is very similar to the three cycle one we saw before. In contrast, the 1,453 cycle does have an outside node that is receiving two edges from it. And so this fixed point corresponding to this three cycle does not survive. And correspondingly, we do not see an attractor. Not see an attractor for that cycle. Okay, so the game then is like somehow these unstable fixed points corresponding to these uniform in degree sub they survive, you can get a an attractor for it. If they don't, you do not. And then, you know, going back to stable fixed points, those are actually corresponding to clicks to all the all bidirectionally sub connected subgraphs. And here you also have this game where the fixed point survives if and only if no outside. Survives if and only if no outside node receives K or more edges, where K is the size of the clip. Okay, so where are we in the outline? So I want to give a little more evidence for why fixed points, including unstable fixed points, are key to understanding the dynamics. So one interesting class of graphs that we've also studied are the directional graphs. So in this case, you have a graph that has, you know, some set of nodes, but you can partition a node. set of nodes, but you can partition the nodes into two sets. We call them omega and tau. And all the fixed points, all the fixed point supports are contained in tau. So what do I mean here? I mean that, you know, we have all these stable and unstable fixed points for the network, but they're localized. They're all supported on just a sub network. So here, maybe this is a nice example, A3. I have four nodes. There's, and then I have this partition into omega and tau. And it turns out that even though this looks very symmetric, But even though this looks very symmetric, okay, using graph rules, we can prove that there's only going to be one fixed point and it's supported on three, four. Okay, so one, two is also a click, right? But it has like this external node that's receiving too many edges from it. And so it doesn't actually survive that fixed point. So here we have a situation where, you know, the fixed points are localized onto a subnetwork. And it turns out that the activity converges to that subnetwork. Let me give it. To that subnetwork. Let me give a more complicated example down here: A6. So, here is another example of a network with a subnetwork that contains all the fixed points. And it's not that the activity is going to the fixed point, because in this case, it's an unstable fixed point, but the activity is getting sucked into an attractor that is localized on that subnetwork. Okay. And then here are some examples of graphs that are not directional, even though they look like they might be. So, look at B2, for example. It has only So look at B2, for example, it has only edges going forward, but it's not a directional graph because it's really defined in terms of the fixed point structure. So somehow the fixed point structure is giving us more meaningful information than just kind of looking at the feed forward sort of structure of the graph. This is not directional, even though it looks feed forward. And we can actually chain together directional graphs and see that the activity flows through them in the forward direction. Okay, even though we have sort of Direction, okay, even though we have sort of forward and backwards edges. So, this is sort of, you know, thinking of like a Sinfire chain or some structure like that. Here we're building a chain where the key is that for each directional graph, the fixed points are sort of in some subset, which then becomes the bad set for the next directional graph. So you're kind of funneling in. I'm saying I'm sucking the fixed point, the fixed points of this G1 are in tau1, but the fixed points. Are in tau one, but the fixed points of g two are in tau two. And so what happens is the activity wants to on this subgraph wants to move to three, four, but on this subgraph wants to move to five and so on. And you can chain it all together and running simulations, you see that the intuition holds very precisely. You actually get the activity moving forward. So once again, even though the activity is not going to any of these fixed points, what you see here is that the structure of the fixed point. Here is that the structure of the fixed points is really determining the flow of the network. Okay, let me see. This is just advertising some paper I wrote with my women in computational topology group where we actually pushed this idea and were able to prove some nice theorems about fixed point structure and larger networks. Okay, so now let me. So, now let me talk about core motifs. Okay, so hopefully, I've convinced you at this point that both the stable and the unstable fixed points are important. And what we find is that it's actually not all the unstable fixed points that correspond to attractors. It's like the minimal ones, the ones that have minimal support. So, in this small network, for example, you see that we have five different fixed point supports. So, there are fixed points supported on, you know, Fixed point supported on, you know, one, two, five, two, three, five, et cetera. But these supports are minimal in the sense that the other supports all contain one of these, like one, two, three, five contains actually both of them, and one, two, four, five contains one, two, five. Okay, so minimal in sort of a combinatorial sense. And we find that it's only the minimal fixed points that give rise to attractors. So let me give a Rise to attractors. So, let me give another example here. This here, we have three fixed points for this network. Again, we can work these out using graph rules. And the minimal fixed point, the only minimal fixed point is 2, 3, 6. We do have a nice limit cycle corresponding to that. And look what happens when we try to initialize activity near 1, 2, 4, 5. You know, that's another perfectly legitimate fixed point. But, you know, it's unstable, but so is 236. But when I initialize activity, Three six, but when I initialize activity near one two four five, um it actually gets um sucked into two three six, okay. Um, and so this is a well, okay, sorry, this is an example where the minimality is not enough. And I will, I will show you the full definition of core motif on the next slide. Um, and then here, this is the network we were looking at before, and you can see that the four attractors that we saw. Four attractors that we saw precisely correspond to the minimal fixed points. Okay, so the key concept is the core motif, and this is going to be a subgraph that has a unique fixed point that has full support. And what that means is that these fixed points will be minimal both in the larger network and in the restricted subnetwork. Okay. And so these are the ones. So we've identified what are the, you know, among the stable and unstable fixed points, we've identified. Fixed points, we've identified which ones are the ones that are really giving rise to the attractors. So, you know, we've classified all the core motifs actually through size five and actually size six also. But for size four, we have clicks, which give you stable fixed points. And then we have four additional core motifs, which give us dynamic attractors. You can see them here. I like this one on the bottom left the best. On the bottom left, the best. We call it the fusion attractor because it's like a three-cycle together with a fixed point, like a stable fixed point for node four. But together, they create, you know, this is one trajectory, and it looks like a fusion of the two smaller attractors. Okay, so the rule of thumb of how we connect this is that the core motifs, which again, we can work out with graph rules, we can work out purely. out with graph rules. We can work out purely from the combinatorics of the graph. These guys will have an associated attractor in the network if and only if the fixed point survives, okay, to the full network. And if the fixed points, meaning, you know, that it actually lives in its appropriate chamber when you look at the network as a whole. And, you know, to going back to this example again, we can see that we have many core motifs, but these guys But these guys that I crossed out do not have surviving fixed points, which is again something we can work out using our graph rules. And so we can identify that there are only four with corresponding surviving fixed points and those correspond to our four attractors. So we've tested this obviously more broadly than this one example. And let me see, we've, you know, so we looked at core motifs in larger graphs. So these are the size four ones, but we can, we look at them embedded in larger. But we can look at them embedded in larger graphs. There are additional n equal five core motifs. They're actually more than these five that I'm showing. There are like 37 total. And you can see again that these are all graphs that have a single full support fixed point. And so it's minimal and minimal in the subgraph. So it's minimal and sort of both in the global network and in the local network. And they have these corresponding dynamic. Corresponding dynamic attractors. And so we went ahead and we looked at all 9,608 non-isomorphic directed graphs on five neurons. There are a lot of them. If you go to, we stopped at n equal five, because if you go to n equals six, this number jumps to 1.5 million. So this is sort of the largest family we could reasonably do. And of these, you know, and we sort, you know, we search for attractive. And we searched for attractors in all of these. We did really sort of extensive searches with lots of initial conditions and so on. And we found about a thousand of them have surviving core motifs that are not clicks and hence we expected dynamic attractors. This is pre-searching, no simulations for this. Just using graph rules, we were able to sort of identify who we expected to have dynamic attractors and which ones we expected to only have stable fixed points. To only have stable fixed points. And then we tested it. Then we actually did extensive simulations. And our rule of thumb correctly predicted the set of attractors in all but 20 of these graphs, 22 of these graphs. And actually, the mistakes are interesting as well because they are salvageable in some sense. But oh man, I'm really short on time. On time. Okay, I'll just say, you know, it worked really well. Okay, so we do believe that we have a pretty good handle on this correspondence, at least in small networks, between the unstable fixed points corresponding to core motifs and the different attractors. So I want to finish up by talking about modularity of dynamic attractors in small networks. And this is, I think, an unexpected, but really. An unexpected but really lovely outcome of our big classification analysis. And because one of the questions that we've always asked ourselves is: which edges matter, right? What, you know, when I have an attractor encoded in a network, not just a stable fixed point, but a dynamic attractor, a limit cycle, something interesting. Can I change edges? Can I change, you know, I'm changing the vector field, right? When I change edges in my graph, can I change edges and still preserve that attractor while encoding a While encoding additional attractors, like adding fixed points or other stuff. And the remarkable thing that we found is that, yes, there are actually, for a given dynamic attractor, there's a modularity to the way that it's stored in the network. And there are actually edges you can play with. And so these pictures are like these very compact representations. The black edges are the ones that mattered, that really like were essential for shaping the attractor. And these blue edges were edges you could play with. You could remove. You could play with. You could remove them, you could weaken them, strengthen them, and it wouldn't affect the attractor, even as it might affect other attractors in the network. Okay, so this is, you know, it's sort of getting at the concept of how you could store these things in a modular way, like we can do with stable fixed points, but now for fully dynamic attractors. So let me give you an example, because I think it's quite striking the extent. When I say the attractor is the same, I really mean it's the same. I really mean it's the same, like as a trajectory in the state space, it looks identical. And so, let me give you: here's an example where there are actually 64 graphs in this attractor class, and this is like a three cycle, but it has this little red bump at the bottom from this node. And I'm going to, and this is a projection of the trajectory, and the red is kind of the tail end of the trajectory where you can see you falling into the attractor. And I want you to pay attention here to all these. Now, I want you to pay attention here to all these edges that are changing in these graphs. So, when these edges change, that is changing the W matrix, right? That is changing the vector field. So, these are not equivalent dynamical systems. And sometimes I'm adding additional attractors via these changed edges, and sometimes I'm removing additional attractors. But this particular attractor for the three cycle remains the same, and I can find it by initializing near the fixed point. So, that pinch point here. So, that pinch point here, I'm perturbing around that fixed point, that unstable fixed point that corresponds to it, and I fall into that limit cycle every time, even as I change other aspects of the network. Okay, so the modularity, I think, you know, for us was very striking. Okay, so I've shown you the three cycle a lot. Now, it's a little boring. Let me show you a more exciting. This is for a different core motif. And you can see again, I'm changing edges in and out of it actually. And the Actually, and the attractor itself as a trajectory stays the same. Okay, so every one of these frames is a different simulation in these movies. I'm just like starting over with a new network that I've aligned them, right? Because we understand the core motifs and because we understand, you know, the fixed point structure that leads to the attractor, we're able to align these graphs and really exhibit that the attractor is the same. Is the same. And then this one is: this is my favorite: the fusion attractor. You can see again, edges in and out can change. You see, that's an outgoing edge, that's an incoming edge. You know, if you pay attention to the graph here, lots of stuff is changing, right? And these changes alter the attractor structure of the network as a whole. So I may be adding fixed points or adding another limit cycle, but the particular limit cycle corresponding to the sub- To the subnetwork that's creating the fusion attractor is staying the same, right? So, this is really about all the equivalent embeddings for this particular attractor. And so, to us, this makes us really hopeful that not just fixed points, but also dynamic attractors can be encoded in a network in a modular fashion, right? So, in the same way, I can learn a song, you can learn the same song, but the rest of our network. Right, but the rest of our networks are totally different, right? So, somehow there has to be a way of like encoding dynamic attractors with enough modularity that it doesn't screw up the rest of the network. Okay, so I think I am over time now, so I will skip that. This is now going, well, this is sort of an interesting movie too. I'll just play a little bit. Now we're interpolating through TLN space, moving off of the combinatorial thread. Moving off of the combinatorial threshold linear networks and allowing in-between weights. And there's this interesting thing when if I have the same attractor for two different graphs in my combinatorial threat, in my CTLNs, then I can sort of the weights of the edges that differ between them can be smoothly interpolated and preserve the attractor in between. So you have kind of a convexity to the space where the attractor emerges within the TLN parameter space. The TLN parameter space. It's a little hard to explain. Okay, I'm going to skip this part. This is now, and just to say that we can use these ideas to like, to make networks that do interesting things very easily. And, but I am out of time. So anyway, I'll just show you the slides and skip to my conclusions. Yeah, so main story is all the fixed points matter. Main story is all the fixed points matter, the unstable ones are really important. And we've identified which unstable ones are the most important. Those are the ones corresponding to core motifs, these sort of minimal unstable fixed points. We can use graph rules to engineer networks. I didn't get to say much about that. But I also want to emphasize the next talk, Juliana, she's going to talk about actually using these ideas to engineer cool. To engineer cool central pattern generator circuits and actually make sort of circuits that do meaningful neural computations using the theory. So I would like to finish by thanking my group. So Katie Morrison has been working with me on this from the beginning, five or six years. So she's my main collaborator on all of this work. Caitlin is a former grad student who has also worked a lot. Student who has also worked a lot on the core motif stuff in particular. Chris and Jesse are former postdocs in the lab, helped work out graph rules and a lot of the mathematical theory that we have. Nikki is a new postdoc in the lab, also working on related stuff. Josh, Juliana will speak next, so please stick around and see her cool talk. And Josh and Caitlin are also, these are all grad students currently. And then on the Yeah, currently, and then Anna and Vladimir are our previous collaborators. And then the nerve theorem stuff I showed also has additional young collaborators, mostly postdocs or starting faculty from my WinComp group. So I'd like to acknowledge them as well.