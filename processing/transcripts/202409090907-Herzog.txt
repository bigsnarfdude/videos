The invitation to speak here. And you know, it's a very beautiful atmosphere to be doing math in. And thanks also to the organizers for putting this together. I know that it's a lot of work to do something like this, so thank you. So the plan for today is to talk at a very high level about the convergence methods available to show basically that you have geometric periodicity for second-order launch of N dynamics. Of end dynamics under a general class of potentials. So, to give kind of a rough overview of those methods, so that's my plan. So, do that. And so, there's kind of two different paths you might think available to do this. So, you can think maybe more of a probabilistic approach, which makes use of the total variation distance. And there's also, you know, you have an explicit stationary distribution, which is known. Its density is known up to a normalization constant. So you can also think about measuring convergence in some Hilbert space with respect to the stationary distribution nu. So there's two kind of ideas. This probabilistic approach makes use of kind of the Apanov structure while this more maybe functional analytic approach is used uses Pomkaradi. Approaches use is called correct inequalities. You are essentially doing the same thing. You're showing convergence equilibrium, you're just doing it in different distance, so there's natural overlaps between the methods. And so you'll see some of that today. And the main maybe mathematical technical issues, the one that I think about most is that the system is degenerately damped. So there's damping naturally acting on certain directions in space. Certain directions in space, at least explicitly, and you got to figure out how you get damping in the other directions in space. Because the system satisfies the fluctuation-dissipation relation, that degenerate damping is basically the same thing as degenerate noise. So the system also has degenerative noise, which leads to some complications as well. And the types of potential functions, so the types of nonlinearities in the equation, can make the arguments harder. Can make the arguments harder. So, if we're talking about potential functions, you can think of maybe the classical ones or your polynomial type of potentials with maybe even polynomial interaction potentials versus if you think about singular interactions in the context of molecular dynamics. So, those jumping from certain classes to other classes can make things a little bit more complicated. So, there's a lot of people who work on launch event dynamics. In fact, if you do a literature search, In fact, if you do a literature search like maybe me and the past few weeks, it can be quite overwhelming because there's a lot of people. I mean, the equation comes from statistical mechanics. There's a lot of people from, who work on Forkov Chain Monte Carlo, folks who work on, say, the gamma calculus, and folks from Boltzmann. So there's quite a few people who have interest in the system, so there's a lot of people working on it. So when I think about When I think about convergence equilibrium, at least in my understanding, the first thing that I learned about was the Dublin's condition. So for the Markov chain on a finite state space with some transition P, and you want to show, say, that initial conditions, or two different initial conditions get closer and total variation after one snap, right? So if you want to show that, you need sufficient overlap between the two measures. Between the two measures. And kind of a natural condition would be to kind of lower bound the measure by some minorizing measure eta on the state space. And it's immediate from this condition that you have nice convergence equilibrium boundary with this constant 1 minus epsilon to the k. And it's a nice and simple proof. So from this Doblin condition or Dublin condition, the minorizing bound, you can define and do define a new transition Q using that measure and then you rewrite everything in terms of you rewrite P in terms of Q and you immediately see the contraction right so immediately that started from two different initial distributions you see that contraction happen okay so it's a very nice and simple condition because the argument is very nice and nice and simple okay but this is a Markov chain on a finite state space and And moreover, you're asking something that's quite difficult to verify, right? You want something in one step to satisfy that Dublin lower bound. So of course, there's nothing special about being on that finite state space. You can kind of write the same condition on a general state space. But if you, you know, in practice, if you were trying to check the duplicate condition on a finite state space to be an irregucible and aperiodic Markov chain, you would need to take a sufficient You would need to take a sufficient number of steps for that condition to be satisfied, and then you would have the condition granted. Now, I'm not saying that this is what you would want to check if you were studying a finite state Markov chain. You might want to think about doing some couplings depending on the particular problem that you're studying. But this is to illustrate things that are coming later. So, if the duplicate condition, so if you're not on a, you know, this is also going to be hard to satisfy in a non-compact domain, right? You know, domain. If the Dooblin condition is not satisfied globally, so typically what you show is that you need sort of return times to some small set where the Dublin condition is satisfied to be sufficiently fast. So you need to come back on average in log time. And this sort of makes use of the Apanov structure. So the convergence picture and say maybe total variation is as follows. Total variation is as follows. The idea would be to, you know, you have to exhibit a Lyapunov function v that has certain properties. So, namely, it should, you know, go to infinity when you go out to infinity in space, whatever that actually means here. And that you have some kind of contractivity along the Markov semigroup, right? And that would tell you you come back or that you have exponential moments when you come back to this egg-shaped region. And the egg-shaped region. Region. And the egg shape region is typically defined in terms of the sublevel sets of the function that you find, v, okay, and then from which you verify the Dueling condition on this smaller set. So this is a nice simple picture, and you can establish geometric ergodicity from that. And you can see some of these references here where you can find out more of the details. But of course, the picture is sometimes simply the one that the reality, right? The reality, right? Because actually, in many contexts, maybe outside here, it's quite challenging to find the right Lykona function. And maybe there's some parameters in the system that are interesting to scale right. So that's a problem. And then you come in here, and you're like, well, how do I find eta and epsilon? Or how do I find maybe something a little bit better that does better quantitatively? So the picture, at least for convergence equilibrium for these systems, kind of splits into two pictures no matter which approach you have. And you know, you have to do different things to make them better. And so that's what we'll talk a little bit about today. But to illustrate this picture, we'll look at stochastic gradient dynamics on Euclidean space. So here, I'm just looking at a random perturbation of gradient dynamics, also known as overdamped Langevin. And Bt here is a standard Brownian motion, one dimensional Euclidean space. Dimensional Euclidean space, and we'll assume some basic structural conditions on the potential function. So, kind of maybe a natural condition. So, if it's a potential function, you should think about q squared or q to the fourth or q to the sixth, and what kind of properties does that exhibit. So, as you go out to infinity in space, that should blow up, it should look couple up. Locally, you have a lot of freedom, but in certain growth conditions. And certain growth conditions. So if you think about maybe polynomials for a minute, a polynomial is going to satisfy this condition pretty much right away. So the term that should be dominant, at least if the polynomial is even degree or higher, that this term would dominate the other two terms. Make it about taking the gradient d squared that's quite large for large values in space. And so this is going to be this, and this is also going to be that. And so that's why you would have. And so that's why you would have that kind of natural condition satisfied. And also it should be integrable here, which is natural because the stationary distribution for this problem is e to the minus u up to a normalization constant, whereas density is e to the minus u up to a normalization constant. Okay, so where does the Lyapunov structure come from? Well, I've cooked it up, right? And the Lyapunov structure comes from this condition, too. So if I apply the generator to kind of the natural energy of this system. Natural energy of this system, right? I get exactly that, and I get this bound. And then, if you look at that at the level of the Markov semi-group, you get exactly what you would hope to get, right? PTU is less than or equal to this plus a constant. So, for large values that we do in the state space, right, we have our kind of contractive type estimate. So, the other piece, right, so that's taking care of what's happening on the outside of the egg-shaped region. Happening on the outside of the egg-shaped region, right? We found our Lyapunov function. Okay, so for large values of u, that's going to work. So, Dublin condition is a little bit, you know, maybe more theoretical, at least in this context. So, you look for two things. You look for sort of, you kind of irreducibility, right? And you look for smoothing. Okay, so those are the two things that you're after. So, where do you get the smoothing from? So, the smoothing in this context, because I'm assuming. This context, because I'm assuming u is a nice smooth function. This is its generator here, L, is a uniformly elliptic operator with smooth coefficients, so it naturally has a nice smoothing property. So in particular, if I look at fundamental solutions of the Kommogoroff equations, they have to be sort of smooth and strictly positive on these domains. And what that means is that we have a nice smooth transition density, smooth in all of its arguments, including time. Its arguments, including time and space, both in the forward and the backward variable. So it's smooth and strictly positive. And from that, basically, you can use this property to show that you have the minorization condition satisfied for the vague measure on a bounded set. So you play games with smoothing and lower balance here, and then you can get the delumate condition. You need to unlock your iPhone first. Oh, yeah, of course. I need to unlock my iPhone first too. So, seriously. So, Siri is also listening in addition to apologies for that. And so, a lot of times, you know, many arguments, so the epsilon, the one that you would kind of like to have a better quantitative understanding of what the epsilon should be, is often typically existential. It's just some quantity that exists. And so, maybe a question would be: how do you do maybe more quantitative minorization? And there's this nice Of linearization, and there's this nice, you know, fairly recent paper by Josephine Evans in 2018. It's aiming to do that using value event calculus. Okay, not for this system, but for launch event dynamics, like this one. So I said that there's kind of two overarching approaches that we have: this kind of more of the Apanoff structure probabilistic argument using kind of a Harris or Dolby type argument. So, how do So, how do you do it using this more functional analytic approach? So, we make use of the fact that the system has a unique stationary distribution, where we know what it looks like, at least up to its normalization constant. And so we want to try to measure convergence in L2 with respect to this stationary distribution. So, in particular, I want to show that PTph is going to approach its mean. Is going to approach its mean with respect to u as t goes to infinity exponentially fast. So here I'll just subtract the mean. So suppose that the mean of mu is zero with respect to mu, and I want to show that this norm converges to zero exponentially fast. And so you just kind of, in my opinion, you kind of follow your nose, you do kind of a standard energy type estimate. So you differentiate the norm squared. So, you differentiate the norm squared of PTP. And what you can try to do here is use the fact that is a stationary measure and kind of get rid of terms where mu doesn't charge them. So, if I differentiate, get L P T phi of P T phi, and then I want to try to write that as L of P T phi squared divided by 2, right? And when I do that, I'm left with this term that should kind of go away. That should kind of go away because it's stationary, so that goes away. And I'm left with the Curie de Champ corresponding to this operator, the generator L, which is in this case the full gradient. So we get this, and so we're in great shape, right? Because we know we feel like the gradient is usually stronger than the L2 norm, so we should be able to maybe compare that back to the original L2 norm and we don't. So under our hypothesis, so this is where the Poincaré. Hypothesis, so this is where the Poincare inequality enters into the picture. Under our hypothesis, the measure μ is going to satisfy a Poincare inequality. So I can do this comparison. So that is, I can find a constant called the Poincare constant rho, such that for all phi which are in H1 with respect to mu, we have this bound. Now the thing that makes it a little bit non-trivial, at least in this context, or actually, I think it's kind of non-trivial, is that you're Kind of non-trivial, is that you're asking for a functor and creating quality of the full space. So if you were to try to do this in, you know, say a bounded domain, the measure is positive everywhere, you can kind of play the same game with Lebesgue measure, you know, to do that. But to show this in the full space, you need a little bit of additional structure. But combining the above with this formal calculation, you now obtain the bound that you want, which is this contraction. And in terms of overlaps between kind of Poincare and Lyapunov, you often use these types of bounds right here to verify or validate that Poincare inequality is satisfied. So I've talked about these two approaches in this concrete example where everything is sort of nice. We have basically what we want right out of the gate to be done. Right out of the gate. I mean, maybe we have to do a little bit of extra work to verify things, but now let's look at something a little bit harder, which is called underdamped Langevin, or second-order Launcher Band, or just Launchevin dynamics. So it's a second-order version of the stochastic gradient dynamics. And so here, this is a second-order equation. So this is Newton's law for the position and momentum of, you can think about a certain number of particles. About a certain number of particles in Euclidean space. And the forces that are acting on the particles are: you know, you have a friction force, you have balancing thermal fluctuations, and then you have potential functions right here. And depending on what you assume about the potential function, it can be like you have your wells and your walls, you can all have all sorts of things that are contained in this nonlinear area. Are contained in this nonlinearity, just as in the case of the stochastic gradient dynamics. So, for the remainder of the talk, I'll actually deal with singular potentials. And when I deal with singular potentials, the domain changes a little bit. So, the domain is actually a space where the potential is actually finite across the Euclidean space, so the position momentum. And that's because where u is actually, it will allow u to take the value infinity where you know the potential is singular. Okay, so we allow that to happen, and that just kind of restricts the domain a little bit. And u will satisfy, we'll assume it's had it's a smooth function that has a couple of conditions which are more or less reminiscent of what was in the case of our overdamped launch band. We'll assume that the gradient goes out to infinity at whenever u gets large. u gets large. So whenever u gets large, its gradient has to grow. You can think about that as u kind of grows faster than the linear function. And then we'll have a growth condition. We'll impose a growth condition on the Hessian of U. So it can't be too large. And I put the power 2 in red purposely. I think basically if you get, so epsilon, I can make this as small as I want at the expense of, say, a larger constant. But if you Of, say, a larger constant. But if you get too close here, if this is too large, then in fact, you can show that e to the minus u is no longer integrable depending on what you plug in. So you can think about logarithmic interactions or Coulomb potentials. You're kind of floating with the boundary right there. So it had to be a little careful. So this system has this sort of natural Hamiltonian structure. If I deleted this Ornstein-Illenbeck piece, you could just have your Hamiltonian. Right, you could just say you have your Hamiltonian. You could think about it as having the Hamiltonian orbit, even if it's not really going to be anything in orbit, it's not periodic or anything like that. So you have a Hamiltonian, which is the kinetic plus the potential and energy. And it also has a stationary distribution mu here, which is explicit up to a normalization constant. And it's e to the minus, it's a Boltzmann type measure. Okay, so that's the setting. So, if we decide to apply the known methods for establishing convergence equilibrium, so we decide to say, okay, let's try the Lyapunov approach. What would happen if we just did the natural thing? If I take the Hamiltonian, which is your natural energy functional for this system, and I apply the generator to it, I want to see some kind of contractivity at large values in the space. Get large values in the space. And you would immediately see that that's not happening because I get kind of negative p squared. I want to have negative u. Where is negative u? It's not there. Okay, so I'm kind of stopped right here. So I don't see that this is kind of contracting at large values like it was in the case with overdamped Lajabin. And if I apply the Poincare approach, take observables in L2 with respect to the stationary distribution. With respect to the stationary distribution with mean zero, and I differentiate kind of the natural kind of energy estimate, then what you would find is that you're only going to get, instead of a full gradient, you only get gradient in P. So if you'd hope to apply a Poincaré inequality, that's not going to give you kind of your global convergence count. You're only going to be restricted to sort of values in P. And so that's problematic. So you're kind of stopped right there. So you have to figure out what to do. You have to figure out what to do to kind of get around these issues here. All right. And the reason is sort of simple. It's clearly not the same dynamics, right? We obviously see that. But it's not a dynamics which is sort of, I think, a point-wise contractive. So when I say point-wise contractive, I usually mean by, you know, this like contracting at lower values. So it's not actually happening point-wise. Okay, so what is happening, right? And so in this case, And so, in this case, time averaging is extremely important. So, let's do some movies to illustrate how contraction is happening. So, here, this is a potential in dimension one. I just fix a friction coefficient. And I have a, this is kind of a potential that restricts me to kind of smaller values in space. And this is going to ensure that I actually don't cross a hard wall at q equals zero. Okay, so if I run the system, Okay, so if I run the system, what happens? I start from some large value. I didn't put scales in, it's not that important. I'm just doing qualitative things. So I run the system, and you kind of see almost like a Hamiltonian orbit, but because I spend enough time on this Hamiltonian orbit, it kind of kicks me in, like on a smaller Hamiltonian orbit, and then I do the same thing again. It kicks me in on a Hamiltonian orbit, and then so on and so forth. So I start large. Maybe if I didn't have those additional forces, I would still. Have those additional forces, I would stay out here, but then I would keep coming in and in and so on and so forth until noise kind of takes over the system and dominates. So really what's happening is that time averaging, you know, is, you know, so the value of p squared, which we get on the right-hand side of the generator, is averaging out to something that looks like the full Hamiltonian if you spend enough time along the trajectories. Okay, and then let's do something maybe a little bit more complicated. Let's do something maybe a little bit more complicated. So, and this would be in dimension three, but it's like three particles in R1. And I'm going to plot the relative positions of the particles. They can't actually pass one another with this potential. So, this is like a singular potential, which is going to tell me that they actually can't cross one another, and R1 is not a space. And this is kind of a confining potential. And this is kind of a confining potential. Okay, so what does this look like? So here, the relative positions are plotted in blue, red, and cyan. And then this, you know, this light green thing is the energy of the system. So the noise in the system, so of course the positions are differentiable in the system. So the noise actually gets these things to kind of move around a little bit. And then they start talking to one another. And when they get close, you have these like, you know, energy spikes. You have these, like, you know, energy spikes. But what you can see is that, you know, even though we have these large energy spikes, the system tends to come back to something more reasonable, right, in terms of the energy space. And so that's what's happening. It's a little bit different. And so to explain the averaging, just to understand what I was saying before, which is if you look at sort of the behavior of the Hamiltonian over a fixed time window and compare that with the behavior window and compare that with the behavior of say p squared over 2 and r1 okay for simplicity and you can see right if i look at the hamiltonian maybe i start from a large energy value and that over time the energy kind of comes down and then what you see with the momentum is that you know it's it could be small but then it spends sort of enough time being large that you know if i look at the average value over time that those are these are not negligible with respect to one another right if you put it on a tahoo Another, right? If you put it on the top, it would maybe give you some kind of constant. So that's sort of what's happening. So the P doesn't always, it's not always going to be large, but it'll spend enough time being large. Okay, so if you have this understanding that what's really happening, what's really important is time averaging, actually, how do you use this to get after an argument which would prove you have exponential. Which would prove you have exponential convergence equilibrium. Okay, so here again, I'm assuming way more than I have, but it's again in the name of understanding. So if I let av f be the average value of f along a Hamiltonian orbit containing qp, and again I'm assuming like you have periodicity and all sorts of nice things about my orbit. And h is this Louisville operator corresponding to the Hamiltonian. So I might try to. Hamiltonian. So I might try to replace this time average of p with this average along the Hamiltonian orbit. And that's all I'm doing here. So I take time average, I replace it by the average value of this. And of course, these are the same initial conditions. And then your hope would be like maybe that's lower order. So is that lower order? And how maybe to show something like that would be lower order. So instead of using age as in your layout panel function, which You need your layout on a function, which is clearly not what you need. You need something else here. You need to add a perturbation that encapsulates these kinds of averaging effects. So, this lower-order perturbation, we hope it's lower-order, it would satisfy this kind of corrector equation for large values. So, something that's also important, which is, you know, I've neglected the Ornstein-Mullen back part of the generator, right? And so, for large values in state space, that's certainly negligible. That's certainly negligible. I mean, proving it, you don't need to prove it. But if you look at the picture, you don't see much noise when you're sufficiently far out, right? So that would be the idea. So you would like to try to solve something, you know, solve this equation, then maybe that would be a good guess of the eponym function. Now, the thing that's hard is there's a couple things that you have to make sense of, which is, like, you know, do you have to try to make sense of this average value? Probably not, because. Average value? Probably not because periodic orbits for Hamiltonian, I mean, that's a big open question in general, right? That goes back to Camp theory, and you're not going to get anywhere that. So you have to do something, right? So you have to find some kind of lower-order perturbation that satisfies what you need, at least in terms of constructing the Lyapunov function. So one example where we know we can compute everything is maybe in R. Is maybe an R1, where you have a very specific polynomial potential q to the 2n, or 2n, say. And what you notice is that if I take pq, right, and I apply the Louisville operator to pq, I pick up some positive contribution in p squared, but that's not a problem because I already have the Hamiltonian itself, which will help balance. Hamiltonian itself, which will help balance that if I put a constant in front of it, I don't care if I cancel that out. But I also get the dissipation that I need in Q, you get minus 2nh of q. And in this context, this is actually the solution of the corrector equation. You see that this is actually the average value of the Hamiltonian along one of those fixed orbits, rescaled appropriately. And so for this, it works, right? And so for the simple example, you try to make it work. Work in general. So, if you have potentials that are polynomial-like, have structures similar to a polynomial, even if they have lower returns, then this is naturally going to work. So, some of the early papers in the early 2000s would employ not this condition, but something that allows you to validate that the PQ trick works where you just take p dot q. And they kind of, the reason why I put one here is that polynomials sort of fit. One here is that polynomials sort of fit within this class of naval squared u is less than naval to the first power. So, but I highlight this paper, I highlight this paper in particular, not because someone is in the audience, but this is a very nice work. It's not really focused on, I would say, large growth at infinity. It's really focused on more local structure and using a probabilistic coupling argument to get more quantitative estimates locally in space. Estimates local space. It's a very nice work where they have a nice combination of couplings and they show contraction and they construct a Bascherstein distance. So if you want to talk about singular potentials, which is, you know, you shift this one essentially to two as long as you change the constants a little bit. We looked at this problem from the Lyapunov perspective in 2017 in a very low In a very lower-dimensional example, so just looking at like things in R1, so position and momentum are in R2, and very specific potentials. Okay, so we did it in that case. We solved that equation, figured out what the perturbation is. It's not PQ. You have to do something different than use the PQ trick in this context to figure out what the corrector looks like. So, this was sort of generalized in a paper in 2019 to generalize. In 2019, to general dimensions, we didn't actually solve the corrector equation in which you found a sufficient correction that works in a general context. And this includes sort of the singular potentials like Leonard-Jones. And if you know that, what that is, so it's like 1 over r to the some power, p, which is, yeah, so 1 over some power. And then Louis and Maddie sort of extended those ideas to deal with cool potential. To deal with cooler potentials, so like logarithmic regularities. Okay, so that's the Lyapunov idea and what kind of goes into that. But I didn't say anything at least here about how you deal with local structure. If you want to think, this is kind of global structure coming back to the egg shake region, how do you kind of get better quantitative balance there? That's not done here except here. So, you know, how do you do that? And so let's go to the And so let's go to the more Pincare-type approach and say, like, a lot of the Pincare-type approaches seem to be quite good at getting maybe better quantitative bounds for convergence to equilibrium. So let's discuss how you would handle it here. So it's slightly different. So again, you start with your standard energy estimate, just like we had before. We realize we don't have the full gradient in PQ, we just have it in P. You know, in PQ, you just have it in P, and your kind of hope is maybe that time averaging takes care of that. You know, you would hope that this would be the thing that you have. And so that would be great if that was true. Okay, so idea one. Okay, so there's three ideas here that we're going to talk about within the same kind of puncare approach, or within the same kind of puncture type atmosphere. So, what you would try to do, maybe, is look at what happens. To try to do maybe is look at what happens, you just kind of do like a Taylor expansion, right? Saying, Well, I'm missing derivatives in Q. Maybe I can find them if I kind of look at what happens iteratively in time to this term. So you just take the energy estimate, apply it to roots and t along the trajectories of the system. You do the same kind of calculation, except what you want to do is, again, write this as like L of a function squared. And so to do that, And so to do that, you commute L with the gradient in P. And so there you have the L of a function squared roughly up to some correction. And what you'll find is that this is like what we had before. And so you get something like that, which is negative. And then you find this kind of interesting looking gradient. It's like a twisted gradient, so a combination of P and Q. And you're like, okay, what is that? So maybe if I could do the same thing. Is that. So maybe if I could do the same thing with that, right, then maybe I'll find this kind of missing contraction of Q. So let's do that. So let's introduce another gradient. So we'll just call that, we'll give this a name, gamma tilde. So there's the twisted gradient. And if I differentiate and do the standard energy estimate again, the same computation I did previously, what you realize is, wow, there it is, right? There's your kind of missing contracts. You get a nice. Kind of missing contracts, so you get a nice kind of contractive type estimate here. You get some negative term here, which doesn't really affect things. There's something here, but hopefully, we can actually deal with that. And it looks like there's terms here that we can maybe estimate, and that's kind of the beast term that you have to figure out how to do this. But it tells you what we should be thinking about using, which is some kind of modified H1 norm. So you have the L2, you have the derivatives in P. You have the derivatives in p, and then you have this kind of twisted gradient form. Okay, so these papers really motivated sort of the work of Villani on hyperporcivity, which outlines this in much greater generality, but very similar to what I've just sort of presented here. And so, you know, the devil is always in the details, of course, and so estimating this particular term, right, is. This particular term is a beast. And so, as you see, it contains the Hessian of you. And we were talking about putting conditions on the Hessian. And the issue here is, well, we have these derivatives, which you can probably maybe do a Rob Peter, PayPal type estimate. But this thing, if it grows too fast, it's presenting a little bit of an additional challenge. So the kind of condition that's employed in this is, you know, David squared is like, you know. Nabla squared is like the gradient of the first power. And then you can do that, and there's some clever ways to kind of subsume terms and the things that you have available to you that are negative. So that's what's done in Volani. In additional to that is a nice quantitative regularization estimate that takes you from this H1 space back down to L2. So he has like a quantitative estimate that allows you to get from this H1 convergence back down to L2. Back down to L2. So, in terms of singular potentials, you have these papers by Florian Conrad, Martin Grothaus, and Stilgenbauer, which do this. I think they establish basically convergence to equilibrium at a polynomial rate. Fabrice Baloin and Pierre-Montmarché do this kind of more from the gamma calculus point of view. Gamma calculus point of view, and then we have some this work of Tai Guin, Marmarché, and Zhang, which you know proved this for a family of singular potentials and a slight variation of this norm where you put a weight on one of the terms, which allows you to kind of deal with this thing, basically. And the same thing is also true in a paper with Fabrice Masha and myself in 2021, where we put it a little bit differently. We put it a little bit differently. We put a kind of weight right here, W U. And that W has some kind of Lyapunov structure which allows you to deal with this term precisely. So when it's large, that term allows you to kind of get rid of that large behavior. Question? For that, Amber, you guys got exponential rates and configurations. Is there some sense that that then is the Is there some sense that that then is the right way to use? So, for example, the one where they got polynomial rates, is that expected to be improvable? No, I think it's just an artifact of, so this was for singular potentials. I think it's just an artifact of methods and the art. So it's expected to be exponential for sure. Yeah. So, and, but these, I mean, some of this, I think our paper doesn't, I mean, we didn't do scaling, optimal scaling, at least with respect to the friction parameter. Respect to the friction parameter in this paper. So it doesn't behave that well. I think I don't know if we were really focusing on that at that stage. So that's kind of how you do this in this H and weighted H1 topology. Okay, so idea two, keep me out of time. I think in one minute there'll be three dings or something like that. There'll be three games or something like that. So hopefully, you give me a few extra minutes. But idea too is: okay, so forget about trying to go jump up to H1 and do some regularization estimate. Try to do it directly in L2 by a perturbative method. And so the idea is you have this L2 norm, and then here's your perturbation. Delta will be some small parameter, and A is some operator on L2, a bounded operator on L2. Operator on L2, a bounded operator on L2, which has to be constructed. And this is often referred to as the DMS approach or the Moholen-Smeiser approach. And the idea is, okay, we've got to figure out how to construct A. So how do you figure out how to construct A? Well, you just, again, you take d dt of this quantity and you figure out what's a good choice of the operator. And when you do the calculation, you pick up a bunch of terms. Up a bunch of terms, okay, but we really want to focus on getting dissipation q. So you get L perp or L dagger A plus A L just from standard calculus here. And then this is the adjoint with respect to the L2 new inner product. And so what I'm going to do is I'm going to basically average out things in P. So take the average with respect to the stationary distribution and see if I can kind of find my dissipation. I can kind of find my dissipation in Q. And one term is of particular importance. It's AHπ V, and then the rest is like a remainder. Okay, so what's so special about this term? So again, you have the Hamiltonian structure here. That's a little operator. So you might be able to get something there. Okay, so the idea would be: okay, if I want to pick A, maybe just pick it so that it has the right sign, right? That it has the right sign, right? So, in particular, you would pick it to be minus h pi dagger, so that it's exactly something that's negative, right? I get some contractivity. I don't know if it's going to be in a certain direction, the right direction, but it turns out that it is the right direction, because when I integrate out this and p, I'm left with a function and q. So, I'm going to act the Hamiltonian on this. You're only going to get derivatives in Q. This is just a function of Q. Is just a function of q, so when you integrate this out with respect to the stationary distribution, right, it's independence at time infinity, so this actually is just a nice constant, right? And so I'm basically done, except that's not a bounded operator in L2, right? So I've cooked up a nice guess, but that's not a huge issue, I would say, at least from this. It's not a bounded operator in L2, so you just kind of It's not a bounded operator, I'll tell you. So you've just kind of renormalized the operator. You've applied the right resolvent, which is this term right here, which allows you to have about an operator in L2. But usually, in this context, at least when I've done this, is that we work on a nice dense domain, so everything sort of makes sense. And so you can actually see what this, if you like not thinking operators, you can see what this actually is using the language of probability. Using the language of probability, it's this, where Q, this dynamics, is actually the overdamp one. All right, so that's the idea behind the Duvo Mohoten-Smeiser approach. And I think maybe it was first done by Hera in 2006, and later abstracted in a couple of papers by Dubo Mohoten-Smeiser. There's a number of papers by Mark Grothaus and others. Martin Growth House and others. And I think this one in particular does the DMS approach using wheat pump array, but he also does some things with manifolds, general manifolds. This paper by Benedict Leinkuhler, Matthias Sachs, and Gabriel Stoltz used the DMS approach in other settings. So this particular setting is adapted launcher band, which is launcher band dynamics, but you make the friction a dynamic variable. The friction dynamic variable, so it's a little bit more dynamically challenged, but it still sort of works as setting. And then we did this sort of in the context. So these papers again employ this kind of Volani-type condition, which ensures you kind of have like polynomial-type potentials. But for singular potentials in 2021, using the DMS approach, using both the weighted and unweighted topologies, where you get kind of optimal estimates in gamma, except for one regime and the weighted topology. Except for one regime and the way the topology where gamma is large. So it doesn't quite work right because the way that the Lyapunov structure is con you know, the way that the Lyapunov structure is induced doesn't really make sense for a large gamma limit. So it doesn't do well for that particular regime. Okay, the last idea, maybe the most natural idea, and that's kind of maybe the flavor of current reading, which is don't change the number. Change the norm. Just, you know, it's time averaging is what's happened. We shouldn't actually try to do some funky thing where you change the norm, jump up topologies, jump back down. Just use time averaging somehow. And it's quite subtle how to actually make this work. You might say, well, does there exist some kind of time average pump array? Can you do a time average pump array where you're only using the derivatives in P and you don't need to try to find Q, right? Is that actually satisfied? Right, is that actually satisfied? Okay, so that's the question. And to get a motivation for the answer, you have to dig deep in Hermander's original paper. Okay, so you have to go back, you don't have to go back to 1967, but the paper was written in 1967. And Hermander's condition had a very, very amazing paper, which gives you regularity of solutions of certain second-order PDEs under the Hermander's. order PDEs under the Hermander condition. So the Hermander condition, right, I'm an open set, an open domain, a Euclidean space, just says if I have a family of smooth vector fields, it satisfies Hermander condition if the list of vector fields, where this is the commutator or leap bracket, contains a basis of Rd. So when I evaluate points, I have a basis of Rd. And so this works. So just in our context, the kind of vector fields you might be looking at is very simple kind of from Android. Looking at is very simple, kind of commander-type conditions. You don't have to compute a lot of brackets. So, for example, in our context, x0 would be something like that in one dimension, and x1 would be this. And we note that it's not, we don't get a full kind of spanning set between the two when p is equal to zero, right? And if you want to see what happens at p equals zero, I have to take the bracket of x1 and x0. And then you have this kind of twisted-looking gradient, and you have a basis, okay? So remember condition check. Condition. Check. Now, what does Hermander condition tell us? A lot of people know this in the audience, but Hermander's condition tells us that if it's satisfied, if I look at this operator, it's a second-order operator of this kind of sum of squares form. It's very Xen-like, actually. But it satisfies Fermander's condition on you, then in fact, you get a nice. Then, in fact, you get a nice smoothing type estimate, right? And so, the smoothing type estimate, so if you had like a standard elliptic operator, then this H, this S would be 1. But here you have some kind of sub-elliptic smoothing estimate where S is somewhere between 0 and 1, and it depends on how many brackets you take and whatnot. And the point is that if you have an equation where mu is actually a smooth thing in the sense of distribution, then that's going to, by some bootstrapping arguments, show that. By some bootstrapping arguments, show that it was also going to be a nice function. Okay, and so that's often how the standard kind of regularization estimate is stated. And if you look at Cohn's proof of Hermander's theorem, it's like a six-page, really quick and dirty proof using pseudo-differential operators, and it'll show something like this. But in fact, Hermander shows something a little bit stronger, and he shows something like this. He shows something like this, which actually in turn implies what this smoothing estimate looks like here, but it shows something like this. So it shows that you have control of the HS norm. You can control the HS norm if you control this kind of hypolliptic norm. So these are only the kind of the noise vector fields here acting on the equation. And then you have some control over the norm for the drift, which is in the dual space of this kind of norm. Of this kind of norm with respect to this hyperliptic norm. So you can show that this implies this estimate, but this one is particularly important. And the reason it's important is that if you just ignore this for a minute, this almost looks like a Poincaré inequality. It almost looks like that. And the point is, how does this help? Well, let's look at kind of space and time as Euclidean space, not just space. You know, space. So if we look at, say, t of PT, and I look at the equation it satisfies with gamma equals 1, let's formally plug it in to that estimate. So again, I'm waving my hands big time here. But if I formally apply it into the estimate, it turns out that you can estimate this dual norm of this thing. Again, this is L2 in space and time, right? That this thing actually becomes just this right here. Just this right here. Okay, and so the idea is now this looks like a Punk array, so maybe you should be thinking about a Punk array, a Punk array inequality that looks something like this. And that is kind of what's underpinning Andres take a picture underpinning some of these recent works where you try to kind of some early works using some time averaging methods, but this. Some time averaging methods, but this was, I think, really written down by Albert and Armstrong, Barat Novak in 2021, where they did this for longitudinal in a bounded domain. So some things were a little simpler, but they used this idea to show that you can get exponential convergence this way. This was, you can do this in the full domain in some of these recent papers. You can look at Bergatti in 2022 and Bergatti and Saltz in 2023, where this act, you can actually make all this work. You can actually make all this work. I highlight this particular paper because it's important. It's not related to Long Chevin, because they use this method and it's an entirely different setting. This is for projected Navier-Stokes with hype elliptic points. And they can make all these methods to get quantitative rates of convergence as you take the viscosity down to zero. So it hasn't had a lot of love yet, but it needs a lot more love because you can make that work using this type of idea with Hermander. Of idea with Hermander. Even though you don't even know what the stationary distribution looks like, you can do uniform estimates and whatnot. So I'll stop there. So thank you. Maybe we'll try to get a short break in here, but I use it on my questions. Yeah. Is there any point-away inequality when you use the twisted gradient instead of the gradient? That's right, yeah. So it still satisfies the Poincare inequality. But it doesn't help you much. What's that? But it doesn't improve things. Well, it does because you take, it's just the twisted gradient. So it has Q and P combination, and then you have P. So you can play little algebra games to get the gradient, the full gradient. So you can still do the punk grade equality of that. For those two terms. So it still gives you meaningful aspects. Yeah, absolutely. Yeah, yeah. Absolutely. It's just a functional linear comment. Oh, sorry. Question? Oh, sorry, Dave. I didn't mean to interrupt you, Jonathan. Oh, hi, Jonathan. How are you? Nice to see you. Can I ask you a question? Of course, you can ask me a question. Finish with your answer. Finish your answer. Sorry. I think my answer is finished. I think so. So I have a question. So, in a way, when you, the first part you emphasize, the averaging, kind of when you're taking a limit to infinity in the Hamilton, you know, you get the Hamiltonian dynamics, right? That's right. And you know, I love that perspective. But in some ways, when you and I read that paper in 2019, we were picking kind of a different route in Timmy, where you're having. infinity, where you're having a close to first order Brownian dynamics, right? That's right. And that's kind of related with the L2, the L2, the L2 hyperpersidity point of view, right? Sure. So my question is to follow. My question is to follow. Can you understand kind of the DMS, the norm and normal DMS case for this averaging in any way as being kind of close to that? To that, that scaling limit. And that explains how the apada function works. And I also just wanted to maybe say that if you say, like, I noticed when you listed that high-averaging the apada function, you didn't middle mention Lupre-Ballet stuff from O6. Yeah, I should have seen that. No, I just like that. I think that that's a good idea. I did cite Luré Ballet earlier. I did. I don't know. I don't want to say that. I learned more about that in depth. But go ahead. Yeah, I don't know. Yeah, I don't know. I mean, in terms of getting, yeah, I don't know if I have a good understanding between like that large, you know, large Q limit to construct the Lyapunov perturbation and these other perspectives. Because I was thinking more, do you understand the flow of the DNS, those perspectives, the more quite correct perspectives, the last two, the four manager and the, do you understand that in terms of that little bit? I'm not thinking about the Liaka. I can't under no, I I still don't. I I can't under no, I I I still don't, actually. I still don't. So well, I think that if we stop now and get dated the rest of the questions over coffee, we'll get the 15-minute break here. So let's thank David again. So the next clerk is 1030. Nice to see you, Jonathan. Yeah, sorry I couldn't make it. It was just too complicated. It's okay. We've got plenty of time for questions. Everybody sit back down. All right, yeah, sorry. I got confused about what the schedule was. Come back at 10.30. It's not like I had a chair at the session. I tried to keep that. All I had to do is look at the first person.