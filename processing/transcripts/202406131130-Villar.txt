He is in Uruguay right now. So, thank you for joining us from Uruguay for that. Yeah, thank you for having me. I really, really like Oaxaca, so I'm very sad that I'm missing it. But I hope you have a great time, and I hope that you had a great trip to Monte Alban yesterday. Okay, so my talk is going to be. My talk is going to be about invariant machine learning on point clouds and graphs. This is joint work with my PhD student, Teresa Huang, my postdoc Ben Bloom Smith, and our collaborator at Apple, Marco Kuturi. So this talk is in the context of symmetries in machine learning, and there's many There are many directions that one can explore regarding the use of symmetries in machine learning. So, one is the fact that many machine learning, deep learning architectures are defined so that they satisfy symmetries as a form of inductive bias. So, convolutional neural networks, recurrent neural networks, graph neural networks, transformers, etc., have some form of symmetries encoded in their design. And this is. And this is related with the general area of geometric deep learning, where one not only wants to impose symmetries in the design of the machine learning models, but also use the geometric structures that may occur naturally on the data, like the fact that we can have data expressed as graphs, as manifolds, and they have implicit symmetries, etc. So there's a lot of work. Et cetera. So there's a lot of work that goes into, and there's a lot of mathematical theory that goes into the analysis of these machine learning models in these non-Euclidean settings. And in particular, the main focus of this talk has to do with point clouds that come up in the physical sciences. And in the physical sciences, the notions of symmetries that exist. Notions of symmetries that exist are very explicit. So, for instance, there's in the physical world, people study two types of symmetries. The active symmetries, there are symmetries that occur in physical models, and they have to do with conservations of quantities like conservation of energy, conservation of momentum, angular momentum, and these are typically continuous symmetries that. Continuous symmetries that they have to do with like classical Lie groups, like the orthogonal group, and also other groups as well. And also, there are other types of symmetries that are more general and much larger groups that have to do with the fact that if I have a physical entity and I write Entity and I write a mathematical equation or mathematical model for it. The mathematical representation of a physical object is not unique, and I can choose different ways to represent the same object. And these different ways to do it come with like different coordinate transformations, and those can be expressed like the symmetry, the behavior under coordinate transformations can be expressed as forms of symmetry. Be expressed as forms of symmetries with respect to very large group actions like reparametrizations. And this is something that shows up in units, for instance, as a simple example of how different ways to express the same system in different units can be expressed with a symmetry, coordination transformations, etc. And then there is the claim that if one imposes this symmetry, One imposes these symmetries in the machine learning models that one wants to use in the physical sciences, then we want to obtain a model that has the correct properties. So that's something that people care a lot about, like the fact that you can express things that are probably correct. And also, you can prove that mathematically you generalize better if you satisfy the constraints of this. The constraints of these models, of these systems. So, my talk today is going to be about machine learning and point clouds. And point clouds appear in the physical sciences quite often. So there's applications to chemistry, to astronomy, to computer vision. And this slide shows you a lot of recent papers that implement machine learning. Papers that implement machine learning models on point clouds using different techniques, representation theory, graph neural networks, and different tricks that people can use. Today, we're going to focus on the use of invariant theory to implement machine learning on point clouds. Learning on point clouds. And in particular, we have one application in mind, which is something that we discuss with our collaborators in cosmology. And this idea came up from our work with the group of David Hogg at NYU and his PhD student, Kay Story Fisher, who basically was working on a cosmology problem where they have like two types. They have like two types of simulations. So they have this dark matter-only simulation, which is a simulation of the evolution of the universe that only takes into consideration gravity. And so it's cheaper to run. And there's the full set of simulations that are called hydrodynamic simulations that not only take into consideration gravity, but also take into consideration electromagnetism and different other Different other forces that can occur, and therefore is much more expensive to compute. And so, the idea is that if she can run a cheap simulation, can she learn what are the properties of the resulting galaxies with that she would have obtained if she was able to run the more expensive simulation. So, it's basically. So, it's basically a regression problem on point clouds because these simulations are just based on n-body simulations. And so you have a point cloud and you want to predict properties of point clouds that are invariant with respect to permutations of the points. All the points are exchangeable. And then or ton and transformations, so rotations and reflections. Rotations and reflections, and also translations. So that's basically the goal. And so the setting that we consider is we have our point cloud. So we can think of it as D, sorry, end points in dimension D. So in the case of K, D could be equals to three, the Euclidean space. The Euclidean space and n is the number of points. And in cosmology simulations, the number of points can be huge. They were telling me that for the next data releases, they expect that they have like in the order of 10 to the 10 points. So n is large and d is quite small. And basically, her regression problem is given one object is pointless. object is a point cloud and you want to learn uh you want to learn a property of that point cloud so your your inputs are point clouds so uh expressed as um in r d times n and your outputs say that it's a scalar maybe you have several outputs but just you can think of one just for simplicity and this is ed invariance so ed includes translations or tonal transformations Orthonial transformations and also permutation invalid. And so we're going to show you how we can deal with all these symmetries. And the permutation is the one that is going to be more interesting in the end. So the first symmetry that we want to deal with is translations. So our function needs to be whatever function we learn here, f. Whatever function we learn here, f needs to be translation invariant. So the simplest way to work with this is something called canonicalization, which is basically just you can think of your space and you can think of just the set of orbits by translation of your space. And so if your function is translation invariant, then if you have one way of choosing one point perfect. One way of choosing one point per orbit, one representative per orbit, and define the function there, then you can extend it to the entire space by just saying that it's invariant in the set of orbits. That is like a natural way to do that. And it's called canonicalization. And it's just choosing one representative per orbit and extend the function everywhere with that way. So, here, the natural way to choose. Here, the natural way to choose one representative per orbit is you take your point cloud and you center it. So you consider the center of mass of your orbit, which is just the average of all the points. And so to each point, you subtract the center of mass. And then you get one representative, which is the centered version of the point cloud. And whatever function you want to learn is a function of the centered version of your point cloud. Cloud. I mean, this idea of canonicalization appears very in many other settings. So I just wanted to show you how you can do it, but this is very simple in this case. So once we dealt with translations, we're going to deal with rotations and reflections. So in order to deal with rotations and reflections, we're going to bring to from our classical invariant. Our classical invariant theory toolset. We're going to bring the classical, the first fundamental theorem of invariant theory by Weil, and it's in his 1946 book. And basically, the idea is the following. So if we have a function that takes n vectors in Rd and outputs the scalar, so that this function is invariant with respect to the action of the Orthogonal group. With respect to the action of the Ortogona group. So, if you apply an Ortogona transformation to each of your input vectors, the function doesn't change. So, Ortogona transformations correspond to rotations and reflections in Rd. Then the first fundamental theorem of invariant theory tells you that a function is OD invariant if and only if you can write it as a function of the inner products of the input vectors. So, you take your input vectors, you So you take your input vectors, you construct a set of inner products, and whatever function you write in terms of the inner products is going to be invariant because you know that if you apply an orthogonal transformation to your input vectors, the inner products don't change. And then the fundamental theorem tells you that the inner products are a sufficient set of invariants that allow you to write any invariant function. So any invariant function. So, any invariant function can be written in terms of the inner products. And you can write it in like this commutative diagram, like this. So, if you have a function that goes from n vectors in Rd to R, and that is invariant, then it factors through the matrix of inner products. So, it can be written as the composition of this like phi map that just takes you to the inner products and then this F. takes you to the inner products and then this f tilde map, which is the function that makes this diagram commute. This is something very classical and we used it to construct equivalent functions in a paper in UNEPS in 2021. And there's very similar characterizations for Lorentz groups, rotations, symplectic groups, and unitary groups. But it's just basically just the invariant theory for the classical. The classical Lie groups, which is known very classically. Okay. So, what this tells you is that now we know how to deal with translations, rotations, and reflections. So, now we're going to deal with permutations. So, we have our point cloud, this B in R D times N. And so, we have the action of OD, just like multiplying your matrix of points on. Your matrix of points on the left. And then we have the permutations, which permutes the points on the right. So you can write your action in this form. So we know how to deal with like the action of OD on the left, which is basically just take the matrix of inner products. That's what the fundamental theorem for OD told us. So now we're going to deal with permutations. So what we do is we Do is we observe that when we construct, if we apply a permutation to our points and we construct the matrix of inner products, then the permutation acts on the matrix of inner products by conjugation. So multiplying by permutation on the right and then the transpose of the permutation on the left. So being Being a function that is simultaneously OD invariant and SN invariant is the same as being a function on the matrix of inner products that is invariant with respect to the action of permutations by conjugation. So, multiplying on both sides on the matrix of inner products. This is the, this is like if you have the matrix of inner products. This is like if you have the matrix of inner products here, the action of permutations acts by permuting rows and columns. This is what this means. And well, I guess that in this drawing, it should be pi transpose impossible, but it's the same. And so this symmetry is the symmetry of graph neural networks. And that Luana talked earlier today, though, she didn't focus on the symmetries. So, she didn't focus on the symmetries. So, this is the symmetry that shows up in graph neural networks because when one has a graph, then when you express it as an adjacency matrix, you can use a different adjacency matrix that represents the same graph by just relabeling the nodes or putting them in the different order in the adjacency matrix. So, we're not going to use graph neural networks, but one could say, Networks, but one could say that one possible approach to work with this problem would be to construct the matrix of inner products and then use graph neural networks on that matrix. The issue why we're not going to use graph neural networks is because I told you earlier that B is very small and N is very large. So if I construct the matrix of inner products, then I have like O of N squared complexity, but we'll see that actually using some tricks, I'm going to be able to Using some tricks, I'm going to be able to learn invariant functions with O of n d complexity, which is much smaller than O of n square. Also, the techniques that we're going to use are going to have improved expressive power over the message-passed neural networks, though I'm not going to comment much on the expressive power of graph neural networks. I can discuss that offline if you want. And the most interesting part to me is that. The most interesting part to me is that these ideas are based on interesting mathematical tools that could be used more broadly and are in a mathematics of deep learning conference. So I think it's cool to discuss how can one use CALOI theory to define ideas that could be used for machine learning models. Okay, good. So the idea is the following. So, the idea is the following. So, we have the action of permutations by conjugation in this matrix, and we can see that these permutations can be seen as a subgroup of the group that is the product of the permutations of n elements and permutations on n choose two elements. That basically, what they do is they take any diagonal element to any diagonal element. To any diagonal element, and any off-diagonal element to any of the diagonal elements. So we know that if you are a transformation that permutes rows and columns of a matrix, in particular, you do send any one diagonal element to another diagonal element, and an off-diagonal element to an off-diagonal element. But this group. Element. But this group is larger than this group because you basically don't here, you don't ask that these permutations are consistent with the action by conjugation. So you could do something that is inconsistent. So for instance, send this to this, and then, I don't know, something from here to here is inconsistent with the conjugation. With the conjugation. So, this group allows things that are changes that are not consistent necessarily with the action by conjugation. But you know that if you have a group that is small, like the inclusion of this group inside this group gives an opposite inclusion in the space of invariant functions. In the space of invariant functions. So, in particular, if a function is invariant with respect to the larger group, then the function is going to be invariant with respect to the smaller group. So, inclusion of the groups corresponds to inclusion of the invariant functions, but on the opposite direction. So, now what I'm going to do is I'm going to say I'm looking at the polynomials. The polynomials that are invariant with respect to this group action. So, here, the polynomials that are invariant with respect to changing diagonal elements to diagonal elements and changing off-diagonal elements to off-diagonal elements. You can think of like these polynomials that characterize the set of the diagonal elements and the set of the off-diagonal elements. But as polynomials, you can write them in terms of the You can write them in terms of the elementary symmetric polynomials. And so the Galois theory approach tells you that if we can construct a set of invariants that are only fixed by the desired group, namely that no other bigger group fixes all these invariants, then those invariants can be used to generate the field of invariant functions. And there's a theorem from 19. And there's a theorem from 1956 that tells you that if you can construct the field generators, so if you can generate the field of invariant functions, then these field generators can uniquely identify all the invariants except for a small set, which is the Which is the bad set, which is like a small set, which is a closed measure zero invariant set. So basically, if you have the field generators, then you can identify all the orbits, except for some orbits that you may not be able to separate. So you're not going to get separating invariants for the entire group action, but you're going to get. But you're going to get something that separates everything from the rest, except for a small batch set that you can characterize it using some algebraic techniques. And so the idea is, in particular for this case, the idea is the following. So we know that the group that we care about was the action of permutations by Of permutations by conjugation in these symmetric matrices. And so we constructed a larger group that contains this, which is something much simpler, which is just permuting off-diagonals to off-diagonals and diagonals to diagonals. And then this group has a field of invariant functions with respect to this group that is easy to characterize. It's just the field generated by the symmetric polynomials that send diagonals to diagonal. That sends diagonals to diagonals, and the symmetric polynomials that corresponding to all every off-diagonal elements is in the same equivalence class. And so this is on top of the original field, which is just all the rational functions on all the entries of your symmetric matrix, this is a Galois extension. And so if we can find an invariant An invariant that is only fixed by the small group that we care about, then and by nothing bigger, then that gives me the field of invariant functions with respect to the group that we care about. So basically, what we need to do is we need to construct an invariant with respect to SN action that is not invariant by anything larger. invariant by anything larger than the s n action that we care about. And so we can do that. And basically you can write down an explicit polynomial, which is this f star, which is the sum over all pairs of points, i and j, different from j of xi, so the diagonal entry xii times xij, the off-diagonal element xij. So this J. So, this you can see that this is like an invariant with respect to the action of permutations by conjugation. And you can prove that it's not invariant by any of the larger groups. And then we can see that if we take these elements that we had. These like elements that we had before, the diagonals, the off-diagonals, and this f star, then the field that we generate with these sets of invariants, that is the field of invariant functions with respect to the group action that we cared about. And that's a nice characterization of the space of functions using Galois theory. Functions using aloof theory. Any questions so far? Yes. Can you hear me? Yes. So I got lost about two slides ago, and the reason is the following. The construction of invariance to translation and rotation is very clear. And in this slide, I think 11 is fine. And in this slide, I think 11 is fine. 11 probably is fine. The fact that when you were motivating the fact that we can consider in a subgroup, I immediately thought, well, the set of permutation matrices is a subset of the set of orthogonal matrices, and somehow I thought you were going to be motivating with that. With that. But how you went into considering these diagonal elements versus the off-diagonals and why and why that connected to the construction that follows about permutations. That was the part that I got lost. Okay, so the idea is that if I have a permutation acting A permutation acting by permuting rows and columns, then it will always send a diagonal element to a diagonal element and an off-diagonal element to an off-diagonal element. Do you agree with that? Yes. But not every permutation that sends diagonals to diagonals and off-diagonals to off-diagonals is consistent with the conjugation because you can send You can send, yeah, so there's more transformations. So, basically, what we can do is we can construct another invariant that ties that whatever you're doing with the diagonals and the off-diagonals is consistent with the fact that you want to do the conjugation. And so, you add that invariant, and then you can use this theory to tell you that adding that invariant that ties these two. That invariant that ties these two things, these two transformations together, is sufficient to implement all the field of invariant functions with respect to the action that you care about. Right. But you're still using the fact that you're working with your tolerant group because now this matrix that you have here is the matrix of inner products. Yeah. And then my next question was: how do we know that? How do we know that the polynomial you had later, maybe slide 12, is sufficient? Yeah, so basically, what you need to prove is that this polynomial is not fixed by any other group that is larger than that one, that contains that one and is larger than that one. So basically, you want to say that. Basically, you want to say that so any other group inside here, right? So you have SN, this is inside this group, and then you say, well, what subgroups of this group fix this element? And you see that all the subgroups of these groups that fix these elements are also inside SN. If you prove that, then Galois theory tells you that. And Galois theory tells you that then that is sufficient to say that adding this suffices. Okay, great. That was very clear. That's how the proof technique goes. That's basically what the fundamental theorem of Galois theory tells you. Perfect. Thank you. Yeah. And with that, that gives you the field generated. The field generators, and then the field generators using this Rossen-Leed theorem tells you that you can use that for universal approximation. So, in practice, we're not going to use these polynomials, the symmetric polynomials. We're going to use something that a deep set architecture, which is a machine learning architecture that implements these invariances. Basically, the deep set architecture is Architecture is you can say that a permutation invariant function can be written as the composition of two functions, this phi, which is like a feature map that gives your for each of your inputs, it produces like this RL representation and then this row, which is like what you use for aggregating all this. All these features. And so the deep sets, the paper tells you: well, if you want to write the permutation invariant function, SN permutation invariant function from elements in Rk and elements in Rk to Rs, then you can write it as a sum of these features of this feature map on each of the On each of the, in this case, columns of your input matrix. And this is a machine learning architecture. These are implemented typically with MLPs, and they prove that if you choose the feature dimension to be on the order of 2kn, then you have something that is universal of. Is universal universal approximating. So, combining these deep sets with what we had earlier, we can show that if you define a machine learning model that takes a deep set of your diagonal elements, a deep set of your off-diagonal elements, and this object that we constructed using Alois theory, then if you do an MLP or something that An MLP or something that is known to be universally approximating on top of these features, then you can universally approximate invariant functions on symmetric matrices outside an invariant closed zero measure bulge set. And this is what comes from this Ross and Schlid theorem, like the BAL set. So that's for the deep sets. For the deep sets, and I know that I'm running out of time, so I'm just going to summarize very quickly that we can go from of n square to of n d invariant features by using tricks from low-rank matrix completion. So the idea is the following: so we have our original points, original points in dimension D, and so we D. And so we can take, for instance, the k-means clustering centers of these endpoints in Rd by taking k equals to d, so these centers. So we do some clustering, we take these centers, and then we construct this matrix of inner products that includes the points and the centers. And so this matrix over here is. This matrix over here is a rank D matrix. And so you can observe that if you take the last D rows of this matrix, if your centers are genetic, meaning that they're linearly independent and span the entire space, then you can use this row to reconstruct the entire. To reconstruct the entire matrix of inner products. So you can use a machine learning model based on these rows of the matrix. And basically, the reason why we can do this is because we can choose the centers. The K-mean centers are OD equivariant, and you can make them to be permutation invariant by just say that you order them by norm or something like that. By norm, or something like that, and with that, using this, you can do you can use deep sets on this object and reconstruct the entire matrix of inner products. And so your model would be something like you take your point cloud, you take the sea to be the K-mean centers, and then you take a deep set on the inner products of your points with your. Your points with your with your center. So this is a D times N matrix. And then you have this C transpose C, which you would need to use for identifiability, which is D by D matrix. And if you use this model, then you can prove that the same theorem that we had before holds. So this model over here can universally apply. Can universally approximate invariant functions on point clouds outside the measure zero closed invariant set, which is the val set. And so this machine learning model has the advantage that it is on MD features, which is computationally much more efficient. We have some numerical examples. Examples, a molecular prediction. I can talk about it if you're interested later, but in the interest of time, I'm going to skip it. We also have some numerical examples in a regression problem on these point clouds that are coming from like some furniture. And basically, what we want to do is we want to learn the Grammo-Vasser sign distance or a lower bound between the Gramo-Vasser. Distance or a lower bound between the Gramo-Baser sign distance between point clouds, and we can use this model for regression. And yeah, and so to finalize, I'm going to say, well, we define these machine learning models on point clouds via these lightweight invariant features, and this Galois theory delivers. This Galois theory delivered a technique to prove genetic universality. We can use tricks from Lowbrack matrix completion and deep sets to give something that is information theoretically optimal complexity, O of N D instead of O of N square. And as future work, I'm interested in applying these ideas to these real cosmology problems that Kate had in her PhD thesis: a formal connection between Galois theory. Connection between Galois theory and the stone-biased stress theorem, which is something that is used a lot in machine learning, at least in some parts of these machine learning models that work with invariant functions. And also, we have some roadmap to extend these invariant functions to equivariant functions using some techniques that Ben and I wrote in a paper in the Notices of the AMS a couple of years ago. Of years ago. So these are my references. In particular, the paper that I mentioned today for the longest time was this paper that appeared in an archive like a month ago. And yeah, I'm happy to take questions. Sorry, I went a little bit over time. Are there any questions from the audience? Can you hear me? Yes, now we can hear you. Great talk. Thank you. So there are some moving parts here, and I just want to understand why they're all fitting together. So the Galois theory is what allows you to go from the invariant The invariant functions on this large group SN cross SN choose 2 to the group you're interested in, which is Sn right. And then completely separately, what allows you to go from order N squared features to order N D is this is a separate trick, right? Yeah, it's a separate trick. It doesn't, unfortunately, it doesn't use Galva theory. Yeah. And the bad set is the same in both cases with order N squared features and with order N D features? No, the bile sets are different. Is one contained in the other? Not that we know of. So actually, so here in the second part, the battle sets have to do with the fact that we are using the identifiability of the centroid. Identifiability of the centroids. So, if you have that, for instance, if your centroids by K-ming's clustering or whatever thing you use to construct the centroids are not linearly independent, then that is part of your bad set. If you have centers that have the same two norm, so you cannot order them by norm, then that's another bad set. So, yeah, the bad sets are different here. Thank you. Here, thank you, yeah, yeah. I wish they were more related, but they are not, yeah. So, you have some very technical question. So, the fact that you can get the star is this primitive element theorem. So, in general, how let's say how algorithmic size is the obtention of F-star? That's a great question. I don't know, but you always have a primitive element in characteristic zero, I believe. So, but yeah, here we construct it by hand, but maybe there is a systematic way to construct this primitive element. Yeah, that's all I can say. Yeah, that's all I can say. Yeah. Thank you. Any other questions? I have one more, Soledad. Can you go back to the N squared to ND part? Yes. So this was very nice. What I was not clear about was the following. If the cluster center If the cluster centers are obtained, say, via k-means, then they are the average of a subset of points. And so if I permute the points, then that induces also a that could induce a permutation on the cluster centers. It's well known. Well, assuming that you can solve the chemist problem, like which you cannot because it's anti- Which you cannot because it's going to be hard, but say that you can solve the k-means problem, then the k-means objective is independent of the order of the points. So the k-centers are going to be the same centers regardless of the order of the points that you give them. Yes, but my question was that I thought at first that you were going to consider this the matrix here at the bottom and now consider permutations of size. Of size n plus d, but the issue I had was that the permutation of the n entries induces some permutation of the remaining d so they're not fully independent. Well, yes, but what we do is we have uh yeah, that's that's that's a important subtlety. So, the way we choose C of V is, of course, it's all the equivariant because if you Of course, it's all the equivariant because if you rotate everything, then the centers rotate, but you have to choose them in a way that are SN invariant. So, the way you choose them in a way that are SN invariant is you take these case centers and then you order them by norm. And so that, like if you permute the points in your point cloud, then it doesn't matter because the centers are going to be the same and then order by norm, they're not going to change. Order by norm, they're not going to change. I see. So, this block over here is fixed if you do if you apply the permutation on the Vs. Yeah. So, the only failure case is if you get the bad luck that they have the same norm or as absolute normal norm. Yeah. Which is, you may say, well, you could do the same for your points original for your original point cloud. Just say, well, my points I order by norm and then. Order by norm, and then you can just have them identified by permutations. But that's not going to be useful in practice because you have many, many points and then order them by norm is something that is not robust. But in the k-mean centers, you only need to take k equals b, so that would be something that is more robust, we expect. I mean, if you choose larger. If you choose larger K, then you may end up with something that is even more robust. I don't know. And is the choice of K equals D adequate? Vis-a-vis, you know, this is a large point cloud and the data naturally has some clusters. There could be more, more than D clusters. There could be something more reasonable than K equals D. And also, there may be something more reasonable than doing K-means clustering. This is just a way to get this identifiable. To get these identifiable centers that are OD equivariant and set invariant, but anything that satisfies this would do it. We said the k-means clustering with K equals D just because we could use that for easily, but we could do something else that maybe is better than that. Wonderful. That was great. Thank you. And information theoretically, K equals B suffices. And you cannot do Suffices and you cannot do fewer than K equals D. Great. There is one more question here. Hi, Teledat. So would you explain a little bit more about that regression task on the furniture point cloud? So like are you taking two point files as an input and you're trying to predict, estimate their the lower bound on their The lower bound on their grombat block or spine distance. Exactly. Yeah. So you have two. So these point clouds are subsampled. And then you take these two point clouds, you have a training set and you have a test set. And then you say, like, what is the lower bound on the Grobovaster sign distance? And that's your target. And so, one way of knowing how good this learning is done is by looking at the rank correlation so that makes it. The rank correlation so that make it basically say that whatever you learned it preserves the order with respect to the truth, and then this is the this is the results. So it's not that great, but I mean, I think that it performs decently. I think these results can be improved. This is, yeah. Thank you. All right. Okay. Thank you very much for that. Thank you very much, Solidad. Thank you very much. Enjoy Oaxaga. All right, we'll take a break for lunch and be back at two here.