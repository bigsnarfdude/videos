I'm joined by Ishtoa Guzman and he's a professor in the Department of Mathematical Engineering at the best university in South America. Yeah, I'm not going to say any claims about that. But yeah, thank you so much for the invitations. I really appreciate. Invitations. I really appreciate coming to this workshop. I learned many interesting things and also at this wonderful location. Yeah, so what I'll be talking about today, it's specifically about privacy and more specifically about differential privacy. So we heard a little bit about that yesterday. But yeah, okay, let's get through it. So first of all, I will explain what are my concerns about privacy. Concerns about privacy, you make these questions very concrete. And at least for my more classical instruction about statistics, right, when I'd like to think about statistical models about, you know, models that learn about trends in a population rather than focusing on the nuances of maybe individuals, right? And that maybe it's nowadays is a bit questioned by Questioned by deep learning, particularly because of memorization issues. So I'm just going to douche the bullet regarding whether memorization is necessary or not for learning, and just say that if, you know, by either legal or ethical reasons, you think you should not be memorizing, right? What you should, how you should deal with that. Okay, but first of all, there's a question of even, is there an issue of memorization at all, right? And I want to explain now that there actually is. Explain now that there actually is. So, when you use large neural networks for training models, right, some of the state-of-the-art models would have a number of parameters which can increase proportionally or even larger than the data set itself, right? So there's the potential risk just because of their capacity to completely memorize the data or memorize significant parts of the data set. And this is a conceptual concern because it's unclear how the training may How the training may induce such memorization, but there's experimental work supporting that hypothesis. So, for even before LLMs or not the state of the art, but even earlier work was able to show that this memorization phenomena happens when you're dealing with text data. And for this particular work, they were focusing on models that would make next word prediction, right? So, you would train your model, you would get your parameters data, and Your parameters data, and then you can evaluate what people call the log perplexity, which for me is just like negative log likelihood of your model, right? In terms of what's the likelihood of this sequence of words. Okay, so having that in mind, the experiment that they conducted was inserting so-called canaries on the data set. So they inserted sentences that would contain sensitive information, such as, you know, my social security. Such as, you know, my social security number is something, or my bank account number is something. And they would test the likelihood of that sentence compared to other possible inputs you would have put here. And what they observe is actually that the sequence that was input on the data set had substantially higher likelihood than other possibility. So that's some form of evidence that this memorization is occurring. Now, you can even take a step back and say that even for more, you know, for simpler models, the same issue may be happening. So, for instance, here, I'm just showing a simple picture of SVM, right? When you're using max margin separation, what's going to happen is that the normal direction from this hyperplane is constructed as a linear combination of a relatively small number of data points. Number of data points, and therefore, these data points correlate very effectively with these vectors. Yes, could you go back to the previous slide? Yeah, so maybe I misunderstood, but I'd like to question the statement. So if I understood correctly, what one is evaluating is the log likelihood of a particular sample on the learn mode. Yes. And supposedly, Supposedly, let's say you actually train a generative model and you train it by maximizing the log likelihood. So by construction, you training examples are giving guidance to the model to have high likelihood in the sample. So, isn't this kind of what we're trying to do in the first place? Well, it depends, right? Well, it depends, right? So like the goal is to learn a good job. Yeah, but should that model be outputting, you know, maybe private emails from people, right? No, that's right. But what I'm simply saying is that one thing is what the model actually generates, but a different thing is just measuring the log likelihood. The log likelihood of a training example has to be high because that's how we train. Because that's how we train the models. Sure, sure. I mean, I'm not debating that. But what I'm going to say is like, there's the question of whether you can do the training preventing this from happening. But I think it's a contradiction. That's what I'm trying to argue. I'm not sure about that. What I mean is that there is a different thing of can I extract the raw data model versus does one of the training examples have highlighted. I like that. In in a sense, I think it's it's a contradiction because you if you're learning, you have to give high likelihood to the training stuff. Let me put an example related to this, right? So suppose that a bunch of people in the training data set were saying exactly the same sentence with their own social security number, right? So perhaps the train model here is going to be outputting maybe uniformly at random among those actual sentences. Among those actual sentences. My claim is that if you train privately, you would learn this sentence, but you might be inputting random sequences at the end. Because the trend of just outputting this sentence is there, but it's not correlating to just one particular individual example. That's the hope, right? I mean, I'm not saying that's exactly what's going to happen, but that will be a goal if I'm trying to privately learn this model. Does that partially answer your question? So, does that partially answer your question? Okay, cool. So, yeah, so as I say, this question may arise in different settings. And if we're by any reasons, you know, putting as a constraint on our learned models that they would not memorize data, then what do you do? Okay, so now we'll present differential privacy, which is a notion that deals with these kinds of challenges and many others, in fact. So, the high-level idea is just if we were training our model with certain data sets, right, and we take one of the individuals from a data set that we replace it by another, we'd expect that the results are nearly indistinguishable. So more formally, we consider neighboring data sets. These are data sets that only differ in one of their entries, meaning that I'm substituting the information from an individual by another. And then what we're And then, what we want is that the output of the algorithm, which in this case needs to be a randomized solution, will be epsilon delta differentially private if the outcomes of any event that I can measure on this output space. Okay, any event that I'm looking in the outcome space, and for any pair of neighboring data sets, I see almost the same chance of these events happening. And when this almost the same is quantified with these parameters. Is quantified with these parameters epsilon and delta. Okay, so notice epsilon plays this multiplicative role here, delta plays this additive role. And yeah, this is so-called, I mean, this is known as approximate differential privacy. If we let delta to be equal to zero, which is actually helpful in some settings, this is called pure differential privacy. So let me give you an example of how you can attain that. Give you an example of how you can attain that. Suppose that we're interested in estimating the mean of a data set of n data points, which every data point is bounded in the unit norm ball. So the main point about computing this empirical mean, it has this nice property that if I would replace any data point here, Zi, by another Zi prime, the change in the mean is something proportional to 1 over n. So in that case, we say that this. So, in that case, we say that this statistic has a low sensitivity, right? So, there's an upper bound on how much the model or the mean can change due to the presence or the substitution of an individual. In that case, to release a private version of the mean, what you would do is you take that exact mean and you perturb it with a Gaussian distribution. And the question is, what's the amount of variance I need to introduce to satisfy this? Amount of variance I need to introduce to satisfy this definition, and it turns out there's a way to compute that explicitly. Importantly, it depends, you know, let me say the standard deviation is proportional to the sensitivity divided by epsilon times the square root of log one over delta. Okay, so it looks a bit messy, but it is just what it is. The important thing I want to point out about this is like if you're in a high-dimensional space, the error that is introduced by this perturbation is quite large, right? Because you're Quite large. Because you're adding noise on these different coordinates, things will scale up as with the square root of the factor. So, in fact, if you plug in the sensitivity that is customized for an empirical mean with this value of sigma and this kind of error, what you would see is that the in this case is a private version of the mean minus the Minus the actual mean is something which is going to be order of square root of d log one over delta divided by epsilon. And as I said, for high-dimensional models, this is not a useful rate. So unfortunately, I mean, okay, so we have that result for mean estimation with bounded data. Unfortunately, that rate. Unfortunately, that rate is unimprovable. So you can prove that any algorithm which satisfies this definition, it will be such that there exists a data set which incurs in that much error. So, yeah, and I think this is perhaps one of the main reasons why it's difficult to apply differential privacy in practice because we're interested in training or learning very high-dimensional models. And these kind of polynomial in D-dimensional factors are pervasive. Dimension factors are pervasive in this literature. Now, let me point out that these results are referring to the empirical mean. You can turn this into mean estimation with IID data just by adding a term which is proportional to 1 over square root of n, which in this case is just the mean estimation rate. So, any questions so far? Okay, great. Let's move on now into situations where you can. Now, into situations where you can get around this issue. And our starting point is looking at learning sparse models. So suppose now that you're trying to estimate the mean of IIB data, but we're now, you're given the fact that this mean is S sparse, meaning that at most S coordinates are non-zero. This is a lot of information, so you should expect to see better rates. And in fact, that's true. In this setting, you can basically replace this. You can basically replace this third root of d factor by S log D, which for people working on sparse recovery is not surprising. You're replacing the effective dimension of these same models. And in fact, you can also prove that this rate is optimal. You can also extend, I mean, it's a different kind of analysis, but you can take this intuition also for sparse linear regression, or it's complex. sparse linear regression or it's convex relaxation so in that case it's also possible to prove that the rate for in this case just empirical risk minimization so you're just trying to oh sorry things wrong with it okay never mind um yeah we're looking at empirical risk minimization based on uh observations in in regression right In regression, right? We're assuming that, I mean, we're assuming that the predictor we're looking for is bounded in the L1 norm. Okay, so here, basically, the sparsity is hidden in the radius that you will be picking over here. So you won't see sparsity explicitly, but you do see this polylogarithmic dependence on the dimension. So great improvement upon the baseline that we were discussing earlier. It is. It is. It is. You can prove for empirical risk minimization and even for the stochastic setting that I will discuss later that that's the optimal rate. These dependencies may not be optimal, but you see these are polylogs, so maybe you don't worry too much about those. Okay, so let's switch gears. What I've been working in the past four years or so, it's studying questions in. Studying questions in stochastic convex optimization and solving these problems under differential privacy. Now, the reason for that is that SEO accommodates to a large number of machine learning models in a very abstract sense. We're not just focusing on mean estimation or regression. We can take general losses. I guess for this audience, this slide is almost redundant, but I will just point out we're interested in a certain set of predictors. So my feasible solutions for the optimization problem. For the optimization problem, subset of d-dimensional space, we have data points in any sort of sample space, and we consider a certain loss function that I will only assume it to be convex and perhaps Lipschitz. And given that, I'm interested in minimizing the population risk, right? So, this is, you could say this is learning rather than solving an empirical minimization problem. An empirical minimization problem. Now, SEO is interesting in the sense that obtaining generalization bounds is quite non-trivial. It's not true that if you solve ERM, you would get analog rates for SEO. So there's something that is fundamentally different to the settings I discussed before. And if you're interested in more about that, I can give you more information of mine. Of course, in these settings, I'm not interested in solving this problem under knowledge of this distribution. I'm going to be using IIT data. Okay, so same as for mean estimation. Here is the baseline of what it can be attained for differentially private SEO when you make the following assumptions. You consider your feasible set as contained in the unit bulb. Unit ball. And we assume that the losses are convex and lip sheets for any data point. So under those two assumptions, only the optimal rates would look somewhat analogous to those of mean estimation. And as I said before, in the statistical setting, this one over root n is unavoidable. So in fact, you can prove that these rates are optimal. Let me just say that the way you attain these rates are You attain these rates are based on running something like SGD with adding Gaussian noise on the gradients, but it's more elaborate than that. You need to be very careful about how you deal with the effects of this noise addition over time. You mean the parameters here in the upper bound? I just mean, can you just go over what the left-hand side those are? Side. Oh, yeah. Yeah, yeah, yeah. So, you know, we're in the previous slide, I just showed that I'm interested in minimizing this. Like, f sub d of x is just the expected loss over distribution d, right? So a bit delayed. And yeah, what you're seeing here is just the minimum excess risk. I mean, minimum risk you cannot take. So this right-hand side. Risk you can attach. So, this right-hand side is effectively the excess risk of this algorithm. Yeah. Okay. And so what we worked on was sort of the L1 counterpart of this result. Now, we consider a feasible set to be containing an L1 ball and just, you know, up to scaling, you can assume this is the unit ball. And we consider losses which are convex, lip sheets, and we introduce an addition. And we introduce an additional smoothness assumption. Turns out to be fundamental here. In that case, you can improve this rate in its dependence on D from square root of D to log D. Okay, and I will explain in the next couple of slides how we do that. Now, let me observe: this is not the optimal rate, and it was obtained in a concurrent work by Acifel and Current Talware. The actual rate Tower. The actual rate, the optimal rate you can obtain here is something like one over root n plus what you saw in the private last result. So it's a bit better than this. And the other thing is like you can also address approximating stationary points with non-convex losses using similar techniques to what I will describe now. Okay, so let's think about how to deal with this issue. Think about how to deal with this issue of these polynomial in D-rates for excess risk. At a very high level, this lower bounds I described for, I mean, that I just mentioned for mean estimation tells us, very roughly speaking, that things like noise addition or even trying to output these high-dimensional vectors in some privatized way, it's always going to lead to these factors. So, you need to do something different. So you need to do something different. And what we do is we consider versions of, I mean, optimization algorithms that have a more efficient encoding of their iterates in a way. And this is given by something known as the conditional gradient or the Frankoff algorithm. So we're trying to optimize. So we're trying to optimize an objective over a polytope, right? What we do is at any point in time, you can compute the gradient of. Time, you can compute the gradient of your loss. And without privacy, what you would just do is minimize the gradient over this feasible set. Since this objective is linear, we do know that a solution like it is without loss of generality a vertex of this polytope. And what you do next is you will take a convex combination with very small weight to this direction. So you move a little bit towards that. And then you get to this new point, you do the same. Get to this new point, you do the same thing over and over. Okay. If you think about it, what happens is that the whole trajectory of the optimization algorithm can be encoded by the sequence of vertices that were picked at any point in time. So that gives us, in a way, a very compact representation of the trajectory of the algorithm. Now, that's non-private. How you privatize it? What we will do is something. Basically, what will privatize this vertex selection by just enumerating all vertices, taking noisy versions of these evaluations of the gradient, and for those noisy evaluations, selecting the best one. So it turns out that that's enough to satisfy differential privacy. Of course, you're doing this many times, so you need to account for how this privacy budget increases over time. And in fact, the And in fact, the techniques that were introduced for the private lasso do exactly that. I mean, why am I even talking about this because this rather old work is because Frank-Wolf is not quite stable regarding stochastic estimates of the gradient. It's very unstable if you think about it, like solution about perturbations of the gradient can lead to very different projections. Gradient can lead to very different trajectories of the algorithm. So you have to be careful about how to make this work in the stochastic setting. Yes? So, how hard is it to enumerate vertices? Yeah, yeah, yeah. Okay. So, in the L1 ball, it's just 2D. So, it takes linear time to implement this. But if you have polytopes which have, you know, there's polytopes with exponentially many vertices with polynomially many inequalities. So, what I'm going to present now is not applicable to those settings. Now is not applicable to those settings. I mean, maybe it is, you have like lots of computational power, but I would say it's unreasonable. So we're stuck with settings when you have rather a small number of vertices. Even though I would say existentially, this algorithm would work in any case. It's just that this log D in the excess risk, you will pay for it. Okay, so here's the idea. How do you make Frank Wolff work in the stochastic? How do you make Frank Wolf work in the stochastic setting? We leverage some techniques that were introduced around the time for variance reduction. Now, variance reduction in the stochastic setting works in the following way. Let me ignore this for a moment. Let's just say we want to rank Frank Wolf. We start from some point, and we're going to take a very large mini-badge. Let's say we take half of the whole data for estimating the first grade. Whole data for estimating the first gradient. And we do this Frank-Wolf step with noise based on that very large mini-patch. Okay, so you will move. And as I said, the steps of the Frank-Wolf algorithm we'll be using are quite small. So you move from x1 to x2 with a tiny step. Now, if you would be running SGD, maybe you would recompute a mini-batch gradient at that point, right? What we do instead is, since we're very close to the previous point, you can try to estimate this new gradient. Estimate this new gradient using your previous estimate of the gradient times an estimator of the gradient difference. So this is what you see over here, right? So we take mini-batch gradients and we take an estimator of the gradient variation and we update our estimator using that. It's some form of telescoping that's going on here. Turns out that for our analysis, we even have to average that estimator with the standard mini-batch one, but that's just a little detail. Little detail. We do this, we plugged it into this private Frank Wolf machinery, and we're able to get this excess risk rate. Let me say that the perhaps most technical part of this whole analysis is the fact that if you think about it, if you're at some iteration T, you will be using an addition of gradient that comes from all over the past. This is private information, so you need to privatize, like even if you use something at the first iteration, you're worried about their privacy. You're worried about their privacy all throughout the computation. So, getting recursive bounds on the sensitivity quantity that I described and how it interplays with the privacy analysis is the most challenging part of this. Everything else sort of builds upon previous works. I don't understand the first step. What does it mean to do the argument of something plus lab? Yeah, sorry. For this noisy selection of verbs. A selection of vertices, what we're doing is adding Laplace noise. That's just for technical reasons. I mean, what Laplace achieves, and I didn't discuss this earlier, it achieves pure privacy with delta equals zero. So it's just for convenience. But perhaps your question is more about what am I even solving here? This is a finite number of vertices. So this is just like an enumeration of a large vector, and I'm picking just the smallest value of it. Just the smallest value of it. So it's brute force if you think about it. It also relates to your question. Yes, we don't, I mean, I do have some ideas on how to do this more cleverly, but I think that would only help in computation rather than excess risk rates. We can discuss about that if you're interested. Okay. So that's the algorithm. Now, let me point out that so far we have seen this. That so far we have seen this optimal excess risk rate for Euclidean norm, L2, for L1 norm. And it's unclear how these two things sort of interpolate because these rates look quite different. So something that we did in our paper, and this came up as a bit of a surprise, it turns out that, you know, if you think about maybe LP settings, right? So interpolating between these two, whenever you move away from one, Whenever you move away from one, you immediately have this square root of d lower bounds. So there's some form of, there's something fundamentally useful about polytopes that allows us to do this compression, which otherwise you just simply can. And that applies both for population excess risk and an empirical version of the problem. It's essentially the same proof. I won't go too much into the details, but let me point out that, yeah, first of all, Let me point out that, yeah, first of all, there is this interesting change in behavior as soon as you move away from one. These lower bounds are actually tight. You can design algorithms that attain these rates up to this p minus one factor. And I guess the most interesting feature about this lower bound is that we leverage the fact that LP balls, whenever you move away from one, equals one, they're strongly convex. And in a way, that what we do. And in a way, that what we do is effectively show that the same reasoning that allows you to get a lower bound in the Euclidean setting still applies here, just because there is enough curvature to sort of, if you think about lower bounds based on packing, I mean, you would have enough curvature so that any kind of approximate solution to this optimization problem is sort of backed in the space. Yeah. So I have two questions. The first is just technical. Do your constants depend on P inside of your? Depend on p inside of your bounds? In the upper bound? Yeah, besides the p minus one. You mean these lower bounds? Yeah, sorry, both of the omega terms in the theorem. No, no, no. The only dependence on p here is explicit and it's of this kind. You see that the lower bound completely breaks when p is equal to one. Yeah, but that's the only dependence that you have. And my next question is more conceptual. I think we've seen I think we've seen now two talks that have this kind of LP weirdness happening, which I mean, we hadn't really seen before, all from like, I think three complementary perspectives. I was wondering if you just have any comments on how this might relate to, for example, Fanny's talk. Yeah. I mean, in her talk, P was kind of going down to one, but very slow. Right. To one, but very slowly relative to the dimension. So there's kind of, I don't know, there's something weird happening there, too. Yeah, I was very intrigued about her talk, too. But I gotta say, since I haven't read her papers yet, I wouldn't be able to say anything, you know, precise about connections. But, you know, this like, I mean, most of my work has been on these non-Euclidean settings. So I'm looking forward to learning more about that and perhaps thinking about connections. About connections. The only thing I can say is that in my experience, and I mean, whenever you move away from L2, sort of understanding this kind of convex geometry and particularly like the connections with Banach space theories is super useful. I think there's a lot to gain there. What is weird in this case? In Fanny's case, it was clear that it was something weird. Yeah, it's that the dependence is p minus one and it immediately blows up after one. Yeah, I mean, okay, let me give you a reason for this weirdness. I agree with Joshua. It's weird. I mean, I didn't expect this to be true. Non-privately, in the setting between p equals one and p equals two, the excess risk rates are always equal to one over root n. p equals one has this slightly larger factor of log. Has this a slightly larger factor of log d on top, but that's it. Like, there's there's essentially no difference between optimizing you know within L1 setting to an L2 setting as far as the excess risk rates are concerned. So the fact that privacy introduced this sudden jump on the risk whenever you're not exactly polyhedral is, for me, surprising. And okay, let me go even further and say that then again, non-privately, what you would do is just as What you would do is just, as long as you have some form of strong convexity in the space, you do have some kind of potential function to carry out, something like SG. So more precisely, it's a stochastic mirror extent, but rates and analysis work somewhat analogously. Here's not. In fact, up to this year, we had, I mentioned this Frank Wolf algorithm in the stochastic setting. Every follow-up paper. Every follow-up paper that I've seen on this topic was still using FrankWolf. We didn't have anything like mirror descent working here. And I will mention at the very last slide that we do have now results for mirror descent. But yeah, anyway, so it is, I would say, subtle. I don't know if weird, subtle. Okay, cool. So thanks for your questions, by the way. Yeah, let me move now to a different setting, which is To a different setting, which is gradient sparsity. So, I think in this first half of the talk, I didn't have to work too hard to convince you that using L1 norms is useful because many of you do actually work with these settings and you have given very interesting talks about that. But now I will switch gears to something which looks quite different, which is gradient sparsity. And let me explain why this gradient sparsity is interesting. So, for settings where you're training neural networks, where you have categorical variables. Have categorical variables with large dictionaries. Perhaps one way to use them in the training is to consider embeddings. So here in this case, you have a recommender system that is taking one of their features as location. And there's many possible different locations, right? What you will do is you take this matrix where every row corresponds to an assignment of this. I mean, an assignment of this categorical feature, and you would have a perhaps not too high-dimensional embedding representation of each of these categories. So you do your training, and whenever you will be taking, say, a user for doing your training, you compute the gradient associated to that user. What's going to happen effectively is that only the row corresponding to the category of that user is activated and everything else is zero. Zero. So you have a lot of sparsity in the gradients. That is certainly true for individual users, but it turns out that you can show experimentally that in some settings, this sparsity is preserved even if you take very large batches. Okay, so here this blood is one means perfect sparsity, so like zero vector, and zero means no sparsity at all. So you see that for very large batch sizes, like the fraction of, you know, Like the fraction of non-zero coefficients remains quite small. Now, let me say that we don't use that in the theoretical models that we study, but it's something interesting to consider from the practical perspective. And my colleagues from Google actually have implemented these things in practical settings, particularly related to ads modeling. Okay, so now let me try to come up with a theoretical counterpart of these observations. Observations. Oh, sorry. Before that, let me say: okay, so we said, you know, prosity is a nice structure arising in these models, but in a way, it's also a necessity. It turns out that, you know, computers solving this or training these models benefit dramatically from sparsity in gradients and sparsity in general. You can perform these computations much faster when you have sparsity. And that's at stakes with the standard way of doing private training because when you Of doing private training because when you're adding Gaussian noise to the gradients, you completely break this parsitis part. So it's interesting how you can get around that. Okay. So yeah, now I will be introducing a theoretical model, which is the same model as before. We're either interested in solving empirical risk minimization or SEO with the additional assumption. With the additional assumption that for any hypothesis and any data point, the gradient that you see is as sparse. And of course, I'll be making the standard assumption about lip shiftness that leads to bounded gradients, but yeah. Now, let me discuss a little bit about consequences and observations about how this additional assumption plays a role. Oh, sorry, before that, let me point out we're not. Before that, let me point out: we're not the first to look into this. There's this very interesting paper a few years ago from people at Facebook that were making similar claims about sparsity ingredients and how they are relevant for private training. In fact, the bounds that they obtain also have this polylog dependence on D, which is quite interesting. The downside I would say about this work is that they use this stronger mini-byte sparsity assumption. Mini byte sparsity assumption, which I think it obscures a little bit what's going on with the optimization analysis. Yes? The assumption: What is the role of Z? Here? Yeah, sorry, I should have, for any candidate solution and any data point you have as far C. Yeah. Yeah, so they use these slightly stronger assumptions. I think they obscure a little bit how you can go around with the optimization because, in the end, actually, the bounce. Because in the end, actually, the bounds they get, they're not optimal, and they don't have any lower bounds either. So, it's, I think, when reading this paper, it's kind of unclear what's going on. And I think our contribution is sort of clearing that story. Let me say that compared to the first part of the talk, there is a fundamental difference between model sparsity and gradient sparsity. And in my mind, it seems that these two settings are dual to each other. Because if you think about sparse recovery, you would have an objective which You would have an objective, which is this least squares loss, which is based on using random observations of this sparse vector. If that's the case, you do need density of your observations to be even able to recover sparse structures. So the gradients that you see in those settings are typically sparse, but the models you're learning are typically sorry. The gradients are typically dense, and the models you're learning are typically sparse. Here we're seeing the complete opposite. Opposite. We're considering gradient sparsity, but we expect that the models that need to be learned to effectively accommodate for any possible user, they will have to be fully dense. Yes? Besides the empirical results, any concrete examples where this might be the case? Of having density and let me say that this is a highly idealized situation. Let me say that this is a highly idealized situation, right? So, yeah, in our case, I think it all traces back from this previous work from my collaborators on this ads modeling. I don't think they have like a uniform bound on sparsity, but they kind of play with different choices of this upper bound. And then you use private mechanisms that sort of assume that bound. Let me put it that way. As I said, there's not many papers, you know, looking into. You know, looking into this, and I would say that this other paper may have other examples where this situation happens. But yeah, there's not much out there. Okay. No, I was simply curious if you consider Releo network and the activations are zero in certain regions, then the gradient would also be zero. And that's true. The assumption of sparsely in activations has been used a lot for robots. Been used a lot for robustness, for instance. Yeah, that's true. I haven't explored that direction. I mean, I'm very curious about taking more examples where this assumption holds, because as you will see what I will present, most of the algorithms we propose are very simple. So I think they have potential to being applied in different scenarios. And back to my previous question, why do you need the sparsity to hold for all X? To hold for all X and Z seems why not just for the expectation? Oh, um, the recovery guarantees it may work. Um, I would have seen much stronger to require, yeah, yeah, absolutely. I mean, this is what we're doing in our work is like a highly. What we're doing in our work is like a highly stylized setting. Now that you say so, I will show the algorithms you obtain for mean estimation, and those are the baselines for then solving stochastic optimization. Those mean estimation algorithms do not require uniform spar or like almost sure sparsity to work. But what I'm not sure is whether your accuracy analysis would go through just with, you know, in expectation guarantees of your sparsity. I would have to think further about that. Sparsity. I would have to think further about that. The other issue is that, you know, if you look at, and okay, let me go to this part. So what's behind everything that we will do next? Okay, so this is the insight is just one line, which is, I'm happy when these things happen. You can explain very simply what's going on. No matter what is your batch size or your data size, you can always have a uniform upper bound on the L1 normal mean. L1 normal mean. So, in a way, that this is saying that these vectors we're trying to estimate are simple in some way. And let me walk you through the proof, which is trivial, right? You just use triangle inequality here, and then you say, you know, each of these vectors is S sparse, so I can compare the one norm with the two norm. But instead of using a square root of d factor, I will just use the square root of the sparsity, because I also had this unit norm assumption. Unit norm assumption in L2. So, no matter what's your gradient distribution, just because of this sparsity, you have this nice upper bound. So, I guess to answer Renaissance question, as long as you have this, you're good to go. And perhaps, maybe not just in expectation, but if you have something like you can control the moments of this thing, maybe you can still go with all the analysis. And then you say again that this assumption is relevant for the accuracy of the algorithms, but Accuracy of the algorithms, but not for the privacy. Our algorithms will be privacy whether this sparsity assumption holds or not. So, in principle, you can still use it. It maybe will just not giving you the rates that I'm claiming. Okay. So, yeah, here's a summary of rates that we obtained for mean estimation. I'm sorry that they look so small. I'll try to walk you through them. We have results for pure and approximate differential privacy. Let me maybe just. Let me maybe just focus on the approximate part because it's what I've been discussing so far. So, the excess risk grade that I said mentioned for mean estimation early, it's here. You can always take the minimum of one because when you're actually your estimator is terrible, you just say everything is on the unit ball. So, that's the worst error I can get. That's the baseline for us. So, the question is: how gradient sparsity would change? I mean, not gradient sorry, data sparsity in this case. We're just doing mean estimation with data which are. Estimation with data which are S sparse. So the upper bound here has these two terms that you see over there, but there is this new term. And as you can notice, the dependence on the dimension is polylogarithmic now. So we get this gain due to the sparsity. I guess this looks worse than this one in the sense that you have this query dependence on epsilon n instead of just linear. But think about it. I mean, you can go with much, much higher dimensions that you can otherwise go, right? Dimensions that you can otherwise go. And we also have nearly matching lower bounds. So these rates are, in some sense, almost unimprovable. If you notice, the only missing factor here is this log D, which I wish we could get because, I mean, I would like to understand the dimension dependence for sort. Do you think that's a deficiency in the lower bound? Absolutely. Yeah, yeah. I will explain the lower bound for the pure case, which is simpler, and then maybe I can say what's. And then maybe I can say what's failing for the approximate case. Yeah, rates are nearly optimal. The algorithms that I will show are very simple, and I think that they can be implemented in very large scale. And in a way, they are sparsity promoting. That's not something that we elaborate much on the work, but you will see that we're basically using something like compressed sensing. So we can expect these estimators to be sparse. Now, in terms of the downsides, the Now, in terms of the downsides, the main downside of these estimators is that they will all be biased. Okay, and that will come up later, why it is actually an issue. Okay, so as a first approach, which is extremely simple, we use something called the projection mechanism. But people working on signal processing, I think they know this forever. What you would just do is you take your empirical average, you perturb it in Gaussian noise. You perturb it in Gaussian noise in the same way that we're doing over there. So, in principle, this estimator would have that error rate. But we use this a priori information of our vector lying in a small L1 ball, and therefore we can project. And it turns out that that projection just like it works as a, it cleans up the noise, right? You get things which are better than what you would otherwise get. This method to This method to make it purely private instead of approximately private, you would change this Gaussian perturbation by Laplace. It's essentially the same idea. And by the way, the proof of why this algorithm works and it gives the rates that we have over here is like a two-line proof. It's just using the projection theorem. Now, I guess if there's any difference to what Nikolov et al. did in their work, is that in their work, Is that in their work? They say if your data lies in some set, you use this same algorithm, but you would project on the convex hull of your vector set, which in this case is the set of SS sparse vectors. So that's something which is in principle hard to compute. And instead, we work with this like relaxation of it, which makes the algorithm very efficient. Okay, so that works both for pure and approximate privacy. Let me point out that for Let me point out that for approximate privacy, you can do slightly better than what this other algorithm does. I didn't even show the rate, but the one that you saw on the table is based on this algorithm instead. So we use just compress sensing. Okay, so you have your empirical mean. Let me ignore now for now what is this number of measurements parameter. Let's just call it M. You would take your signal, you'd make m measurements using random calculations. Using random Gaussian matrices, I mean vectors. And then on this M-dimensional space, you would apply the Gaussian mechanism. And then you try to recover just by using L1 minimization on those noisy observations. So very simple idea. Yeah, the privacy of this algorithm is based on the fact that these random matrices satisfy restricted isometry properties with very high probability, and therefore, And therefore, the sensitivity that you have on the mean, it will sort of carry over to the sensitivity of this projected version of the mean with maybe a small factor increase. Okay, the accuracy is a bit more nuanced. We use results from robust sparse recovery, which I don't know if they're familiar to you, but basically the idea is that if you're trying to estimate a signal Z bar, in standard compressed sensing, you will In standard compress sensing, you would make the assumption that z-bar has some sparsity. These robust recovery results are able to let you take the best approximation of your signal with certain sparsity bounds. So remember, Z-bar, if N is very, very large, may not be sparse at all. But I'm claiming that there's going to be a very good sparse approximation of it. So that's what's playing a role here. And then the other contribution in terms of the error is given by the Contribution in terms of the error is given by the amount of noise that you observe. Okay? Now, okay, let me say, I'm claiming that Z-bar can be approximated by some vector which is sufficiently sparse. Why is that so? You can use the approximate Carol Theodori theorem. Namely, what we're doing is you take this mean, you sample at random data points from this one to n. Points from this one to n. And that thing, just because of concentration, its estimation error decreases at a rate of one over the number of samples that we pick. Now, since there exists a convex combination which is approximately equal to z taking k samples, those k samples would overall have a sparsity of s times k. So that's what's going on. Moreover, Moreover, the noise contribution, because we're projecting on an M-dimensional space, scales like this, right? And for these robust sparse recovery results to work, you need to oversample your observations with a loud knee factor. Okay, so if you put all this together, we just basically trade off the increase on this noise given by the larger value of k and the decrease that you obtain in this approximation with one of root k. So if you just optimize that, it gives you. Optimize that. It gives you a bound on K. When you plug this bound on K to define M, you will get exactly what you see over here. Sorry, there. Okay, so yeah, it can be explained in a few lines. But I think it's conceptually interesting, this robust partial recovery result. Okay. Cool. Yes. This was a little too fast for me. So I forgot what K was. For me, so I forgot what k was. Oh, yeah, yeah. So my claim is the following: you have this z bar, which is one over n sum of z i, i equals one to n. What I'm saying is that take now, let's say z hat, which is one over k where Where Zk is just a random coordinate. Let me see. Yeah. It's just a uniform data point from the sample. So you do this repeatedly, right? And what we know from just, I don't know, is Chevichev or Hofding, right? The rate at which zero. Hofding, right? The rate at which Z hat will approximate Z bar in the two norm is something like one over root k. So there is a convex combination of small number of data points that will approximate this thing. That's basically it. And as I said, since these vectors are S sparse, we're taking k of them. This existential approximation will have a sparsity of s times k. And so that's what allows you. Of S times K. And so that's what allows you to go from the worst case, which is n little n times s to s k. Right, right, exactly. Cool. Okay, so let me mention a little bit about the lower bounds. I mean, Ren√© feels free to stop me whenever it's time. So, how do we prove lower bounds in DP? For pure DP, it turns out that packing your possible data set or data points. Data sets or data points in your sample space is enough to prove a lower bound. Okay, so we'll first use the fact that S-sparse vectors can be packed in R-dimensional space with a log of the cardinality of the packing to scale as S log D over S. This is actually the baseline for many lower bounds in sparse recovery. Now, you can use this existential, this existence of packing to prove lower bounds in DP, and this is like a general reduction that would result in a rate. Introduction that would result in a rate which is minimum between one and the cardinality of the log of the packing divided by epsilon n. Now, that's not enough. This is far from what we want to get. Remember that there was a square root on top of this. So what we will do now is not use a packing in R D, but using a packing in R T, a smaller dimensional space. That would allow me to construct many hard instances in this. Many hard instances in this very high-dimensional space, right? Which will be somewhat independent from each other. So I'm basically embedding hardness constructions in like lower-dimensional space, and I'm free to pick what the dimension of the smaller blocks will be. There's going to be a number of blocks, and of course, if I choose a certain value of t, that would limit the number of blocks I can get. Then you can use like a simple probabilistic consideration because these lower bounds in DPE only hold, let's say, with constant probability. Only hold the say with constant probability. You have a number of independent trials of trying to estimate these vectors. Just using basic probability, you can say, you know, only a constant number of them can succeed with high probability. And if you play with these parameters, I mean, it will mean that the error rate you would obtain overall, right, because these are means, each of these means may have certain amount of error like that. But I'm interested in the overall mean of this thing. In the overall mean of this thing. So there's some kind of scaling going on that I'm omitting here. But this, just this consideration, will result in a lower bound, which is case as one over square root of k. And proper tuning of k is going to give you the lower bound that you want. Let me just sweep under the rug many details, but it's basically just embedding like this not too strong lower bound construction in this sort of tensorized way. Sorry. So there's the question about approximate DP. Question about approximate DP. This gives us lower bounds which are tied up to like log-log factors for pure privacy. In the approximate case, we're missing these log D factors. And the reason is we don't have such an analog of these packings for approximate privacy. In approximate privacy, use a different technique called fingerprinting codes, which we weren't able to figure out how to make it work in the sparse setting. So our baseline instead is just using the S-dimensional load. The s-dimensional lower bound for mean estimation, which would just have a factor of s here instead of s log d, which what we think it is. And then, but then the same tensorization would work. So you get the lower bound, but you're missing this log factor. Yeah, I think that would just come from the fact that you're essentially ignoring all D2's S multiple versions. You're just taking one S. Let me say, I don't know how to sort of look Don't know how to sort of leverage this degree of freedom of the sparsity pattern for the fingerprinting lower bounds. Like, I tried in many different ways, and I didn't get it. So I think it's, you know, it's a small factor, but I think it's conceptually very interesting to figure out how it works in that machinery. Okay, so this is the last thing I want to discuss, right? So we discussed mean estimation. The question is now how we go from that to ERM or SEO. So let me consider a baseline algorithm which is for DP mean estimation, such as the ones I just discussed before. And let's consider using those mean estimation procedure to estimate mini-batch gradients in a private way. Then you can just use this as your surrogate for the gradient of the mini-batch loss and just use SGD. What's the issue though is these mini batches. These mean estimators of the mini-batch gradient are biased. And if you have worked on SGD long enough, you would know that bias, it's really hurtful for SGD. So we were interested in ways to mitigate this issue of the bias, and I will explain next. We use a, by now, classical technique from simulation, particularly from this paper of Blanchet Auin. What we do is, if you What we do is, if you think about these mean estimators for different possible batch sizes, they would have different rates and they decrease with the size of the batch. We have this one over root n. I'm thinking now that that batch is like 2 to the k, so you have this scaling with 1 over square root of that. What we'll do is we'll pick a batch size at random in an exponentially increasing schedule. We pick batches of size. We pick batches of size one, two, four, and so on, up to log n. I cannot go further than that because then I completely exhausted the data set, right? But the probabilities of picking these batch sizes will also decrease exponentially. Okay. What you do is you pick this batch size at random, given that now you're going to pick a batch of that size uniformly at random among every possible one. You will compute the mean estimation for that thing, and then you. Estimation for that thing, and then you would split the data set in two, and you would also compute the mean estimation on these two halves of the data. So, if you look at this, we're basically trying to get some telescopic going on. And there's some scaling that is needed for the telescoping argument to work. There's a tail term from this telescoping that is exactly this, that's one size mean estimation. So, you can prove based on these ideas that you get bias reduction. Basically, the estimation error. Basically, the estimation error of this randomized algorithm scales us the best possible mean estimation that we would have with the full batch size, but using batches which are frequently much smaller. And we do it in a way that you can control effectively the second moment of this thing, which is crucial for the SED conversions. Let me skip the details. I already explained what's going on. There's some telescoping argument that it's very easy to figure out, but the point is like, yeah. But the point is, like, yeah, you get the error estimation of the best guy you would have picked. How does this work for SGD? Well, yeah. And particularly for private SGD, right? So we have bias scaling such as the one of the full batch estimator, as I said, with batch sizes, which are typically smaller. This is great for privacy because whenever you can do sub-sampling, there is sub-sampling amplification theorems in privacy that you can benefit. Amplification theorems in privacy that you can benefit from. And as I said, that second moment it's nicely balanced. Now, the issue is that since we're picking these batch sizes at random and we're using this privacy amplification result, somehow the evolution of the privacy is driven by a stochastic process. So, in particular, the privacy parameters become random variables. And it's actually tricky to analyze how privacy evolves in that case. There's recent work who has. Case, there's recent work who has figured out this and indicates that these privacy parameters are predictable. And then you can define stopping times to say when I'm before exhausting my privacy budget. Now, the consequence of that is that the algorithm will run for a random time because we can only run it until our privacy budget is exhausted. And the other issue is that this bias-reduce estimator is heavy-tailed. So you would not expect to have high probability guarantees on the error. Guarantees on the error. Now we address all these issues in the analysis. We consider an analysis of randomly stopped and biased SDD. Due to the heavy-tailed nature of these estimators, we can only guarantee success with constant probability. You introduce a boosting procedure to amplify that success probability. And the technique I just point out, it works equally well for minimization in convex setting or stationary points in the non-convex setting. Okay. Okay, I will completely skip this. There's more results in the paper. I'm happy to take it off if anyone's interested. I think I'm almost over time. There's some open questions. I mentioned the one on the lower bound for mean estimation. For non-convex setting, we have no lower bounds whatsoever. In summary, sparsity is a crucial feature in some ML models. I presented two guys of sparsity, model sparsity and gradient sparsity, and how this gives us benefit for private. And how this gives us benefit for private learning. I mentioned these two works, right? This 21 this year. And the one I didn't get to talk about is the use of this mirror design algorithm with privacy, which even extends to this minimax optimization, which is very important in recent applications. So, with this, I will conclude. Thank you very much for your attention. Thank you very much for a great talk. Any other questions? Thank you very much for a great talk. Any other questions? Online, any questions? Maybe two very brief questions, remarks. One of them is, was I correct that in the main second part of the talk, the assumption is that the gradients are sparse with respect to the standard dictionary, but it's not that they are sparse. But it's not that they are sparse with respect to some dictionary. I would guess that in principle you can turn this into, because it would be just a change of variables in a way, right? And I think in terms of if you, you know, if you think. Think about would it be a change of variable if you know ah you mean when you don't know the dictionary? Just know it is, of course. Ah, I think then it's very difficult because. Very difficult because all the compression is based on the basis. But the reason for my question is because I think there is one particular type of deep networks for which this could be actually very applicable. And it's the case of ReLU networks. And the reason is that in that case, because the input-output map is piecewise linear, then in every region, the gradients are... In every region, the gradients are linear combinations of each other. Right. So a random gradient at any point is always a sparse combination of all of the gradients of the training if the training set is reaching out. And so you know that there is a dictionary which is the gradients of the training example, which is again a noisy sample, so it makes the problem even more interesting. Right. And so sparsity arises by the network itself. The network itself, and you know the gradients are sparse with respect to that's very interesting. Yeah, exactly. So, the extension to that it was going to be my suggestion. Yeah, okay, I would say first, like none of this is going to work in that case for sure. Like, but in principle, I mean, one can consider that. Oh, there's a major issue though. Yeah, yeah, yeah. The gradients themselves are private information. So, if you if you know. So, if you know that they're sparse, but they're sparse based on a basis which is private. Well, but because of the structure, you could just select one example per region, the value. And that doesn't need to be a point. Ah, okay. Yeah, I'm not sure. I mean, but it's very interesting. Yeah, I haven't thought about that. All right. Thank you very much. Soledad, are you there? Yes. Can you hear me and can you see me? We can hear you perfectly. I guess we need to. Could you stop sharing?