It's great to be here. It's really exciting to see that so many people are interested in this topic. And I hope you enjoy the conference. So my job is to give the introductory conference, the introductory talk, which will introduce the basic ideas that you will be seeing throughout this conference. I understand there's a wide variety of people from a wide variety of backgrounds at this talk. So what I've decided is that. So what I've decided is that in terms of prerequisites, I'm not going to assume that you know anything about linear logic or categorical proof theory. And as far as category theory goes, the only thing I'm going to assume are the basic three or four definitions, category functor, natural transformation, and a junction. And so I hope when I'm done, you'll be ready to hear and excited to hear the rest of the talks in this conference. So here's just an overview. So differential linear logic. You. So, differential linear logic was an extension of ordinary linear logic, which was first defined by Jean-Eve Girard. And this extension is due to Thomas Erhard and Laurent Renier. And what they did is they added an inference rule to the traditional rules of linear logic, which captures differentiation as an inference rule. And it was specifically inspired by some models that Tomak constructed, where the morphisms were power series. were power series, and you can formally differentiate power series. And so they asked the question, what does that look like as a logical inference? The corresponding categorical structures are called differential categories, and those are due to myself, Robin Cockett, and Robert Seeley. And then after this initial work, the question is, now that we have this new way of thinking about differentiation, how do we apply it to other fields where differentiation is a really fundamental part of the subject? Part of the subject. And so, as I said, I'm only assuming categories and functors and natural transformations. So, I do want to take a minute or two to tell you about monoidal categories because this you're going to see variations on this pretty much every talk for the rest of the conference. So, a monoidal category is a category with a binary operation, which means a binary functor, and it has to be associative, and it has to have a unit object. And we'll also assume symmetry for this talk, although that's typically not assumed. Typically not assumed. But remember, since we're talking about categories, we don't think in terms of equalities, we think in terms of isomorphisms. So, A tensor B tensor C parenthesized to the left has to be isomorphic to A tensor B tensor C parentheses to the right. A tensor I, A, and I tensor I all have to be isomorphic, and A tensor B is isomorphic to B tensor A. These isomorphisms have to be specified, and they have to satisfy some basic equations. They have to satisfy some basic equations. There's lots of kinds of luminoidal categories in the world, but there's two that will show up today and for the next few days. The first are linear examples, categories like the category of vector spaces over a field or Barnach spaces or Hilbert spaces, where the tensor product is the usual tensor product that we study in linear algebra. And in that case, the unit will be the base field. The other kinds that we'll see a lot of are Cartesian monoidal categories. Of our Cartesian monoidal categories. So that's categories like the category of sets and the category of topological spaces and continuous functions. In this case, the monoidal structure is the Cartesian product of sets, or you take the product topology, and the tensor unit is the one-point set. And so there's a crucial difference between these two types of monoidal categories, and that's the existence of some canonical maps. So in the category of sets or in the category of topological space. sets or in the category of topological spaces you have a diagonal map which goes from a to a cross a and you have projections that go from a cross b to a and a cross b to b but vector spaces and hilbert spaces don't have structure like that there are maps between a tensor b and a and but there's nothing that behaves like the projection behaves in the category of sets and the cat the best kind of monodial categories are ones that are also closed which means that the functor a which means that the functor a tensor blank has to have a right adjoint. So it has to be a left adjoint. And most of the ones that we'll see over the next few days are closed. So category of sets is closed, category of vector spaces is closed, finite dimensional Hilbert spaces is closed, Bonach spaces is closed if you pick the right tensor products. And in each case, the closed structure is the set of all functions of the appropriate type. Of the appropriate type. So for sets, it's all functions. For the vector spaces and Hilbert spaces, it's the linear functions. The larger category of all Hilbert spaces is not closed because there's no way to put an inner product on the set of all bounded linear maps between arbitrary Hilbert spaces. And the category of topological spaces is also not closed. So you might guess, well, I can give the set of continuous functions the compact open topology, but that doesn't work. Doesn't work. There's a large class of topological spaces, which it does work for, and those are the Kelly spaces, which are the compactly generated house-door spaces. And of course, the category of smooth manifolds and smooth maps is not closed, where I mean smooth in the traditional sense of differential geometry. What is the space of smooth functions between two smooth manifolds? Well, it's not a manifold in the sense that we usually study. So closed categories are decided. So closed categories are quite desirable. On one hand, they allow us to work with function spaces. So functional analysts are very interested in studying those sorts of structures. But as we'll see when we get into the logic on the next slide, they also allow us to model implication. So they have sort of a dual role. So categorical proof theory began with the work of Jim Lambeck back in the 60s in a series of papers that were published in those old Reports of the Midwest category theory seminar. Category theory seminar. So, the idea is this: We want to form a category where the objects are formulas in some specific logic that you're thinking about, and the arrows are equivalence classes of proofs. We do need to model by some equivalences to make things work out properly, so that's sociativity and so on. But the equivalences are all very obvious and ordinary. So it's perfectly reasonable to think of a proof instead of an equivalence class of proofs. And then we study that category, the resulting category, to determine its structure. What can we say about it? Is it closed? Does it have products? What sort of limits does it have, etc.? And the nice thing is that typically that category will be free in some specific sense. So as a simple example of the sort of example that was studied quite substantially, let's think about the logic where you have conjunction, disjunction, and implication. So it's called And implication. So it's called intuitionistic logic. So in this setting, conjunction takes on the form of a categorical product and disjunction takes on the form of a co-product. So in particular, in a category with products, I have, as I said, a projection from A cross B to A. We're now going to think of that as a logical entailment. So A and B logically entails A. And the diagonal map will now represent that A logically entails A and A, et cetera. A and A, etc. So, and then we add an implication by assuming that we have a closed structure. So, I've rewritten the adjunction here again, and this time using logical notation. And every adjunction is determined by a unit and a co-unit. And in this case, the co-unit would look like A and A implies C to C, a map from A and A implies C to C. And that should be familiar from. Should be familiar from symbolic logic courses. That's what's called modus ponens. So A and A imply C, logically entail C. So as a general rule, what will happen is that our logical connectives will be functors. Our inference rules will be represented by some sort of natural transformation. And the categories with the same structure can then be thought of as models of the logic. And this is a huge subject. I highly recommend you look at the book by Lambeck and Phil Scott called Introduction. And Phil Scott called Introduction to Higher Order Categorical Logic, which carries out this program for this logic, intuitionistic logic, and for topises, where you can add in quantifiers and so on. Oh, so there's one place. So you might say, okay, this is great. Every time I come across a logic, I want to study it from this point of view. But there's one logic where this doesn't work at all. And unfortunately, that's classical logic, the logic that we all use in our day-to-day. Logic, the logic that we all use in our day-to-day lives. And the problem here is what you would try to do is the following. So if I'm in a category which is Cartesian and closed, then just by manipulating the adjunctions, there's a canonical map from A to A implies B implies B. And so what we would try to do is say, let's imagine that I have an initial object, which we'll think of as false. And then I will define not A to be A. Define not A to be A implies false. And then you wind up with the above map giving us a map from A to not, not A. And so to model classical logic where you don't ever have to say not, not anything, you would just imagine that that's an isomorphism. The problem is that causes a collapse of the structure. So any such category is a Boolean algebra. So partially rated sets are categories, but they're not terribly interesting as categories. Not terribly interesting as categories. So the whole project just doesn't work with this naive attempt at defining classical logic. There are tricks to get around this, but they're not really relevant today. But the reason I mention this is in linear logic, the issue doesn't come up. So we can talk about linear logic with involuntary negation. And these are the star autonomous categories that were first defined by Mike Bark. So, categories. So, categorical proof theory, we need to talk about a specific formal system. And so, we're going to use the sequent calculus. It's the best way to present this sort of thing. So a sequent is something of the form gamma. And this turnstile here represents logical entailment. So gamma logically entails A. And gamma will be a finite list of formulas in whatever logic I'm thinking about. And A will be a single formula in that logic. That logic. So, sequence are constructed and manipulated using inference rules. And one of the difficult things about categorical proof theory is you frequently wind up with a slide with nothing but very fine print and lots and lots of inference rules. So I'm not going to do that to you. The rules are all readily found. I just want to show you a couple of examples. So here's an inference rule. This is the right conjunction rule, which says that if gamma logically entails A and delta logically entails. logically entails A and delta logically entails B then gamma and delta together logically entail A and B and the right implication rule says if gamma and A together logically entail B then gamma logically entails A implies B so there's some structural rules that we have as well and these are sort of bookkeeping rules which allow you to keep track of what premises are being used and where so the exchange rule says I can So the exchange rule says I can manipulate the order of my premises however I see fit. All of these rules, by the way, should be logically obvious because what we're trying to do is capture reasoning, right? So all these inference rules are trying to capture the ways in which we reason about things. Contraction says it's silly to make the same assumption twice. So if I know that gamma and A and A together logically entail B, well, I don't need to assume A twice. To assume A twice, gamma and a then will logically entail b. And weakening says I can always add in another premise and still get my same conclusion. So if gamma entails b, then gamma and a together entail b. And so as I said, we want to build a category where the objects are formulas and the arrows are equivalence classes of sequence proofs. So if I have a proof of gamma logically, proof of gamma logically entails A, that should be interpreted as an arrow in my category from the conjunction of the elements of gamma to A. And in traditional logics, like intuitionistic logic, this will be the Cartesian product. And the interpretation is built inductively. I'll just show you one example. So suppose I want to model the contraction rule. So suppose by induction, I have an arrow that represents the conjunction of gamma, conjunction A, conjunction. Of gamma conjunction A conjunction A to B. And I want to build an arrow from gamma conjunction A to B. Well, what I need to do that is a map from A to A and A. So I need that diagonal map I was talking about before that exists in Cartesian categories. And similarly for weakening, I would want to use a projection. So I would want to go from a conjunction of gamma. From conjunction of gamma, conjunction A down to conjunction of gamma, and that gets me the proper morphism there. I should stop periodically and ask: are there any questions? I can't see hands, so if there is, please say something. Okay. I have a question. Sure. I don't understand when you draw a horizontal line with something above and something below. There's something above and something below. What does that lie? What's the convention here? Right. So, proofs are built inductively, that you start from the top and work your way down. So, what I'm assuming is that suppose I have a proof that gamma logically entails B, and I add one more line to that proof. So, you're, let me switch to the whiteboard for a second. So, you're about to see how hilariously awful my whiteboard writing is. Hilariously awful my whiteboard writing is, but proofs will typically start with identities. So just A logically entails A. I think I get it now. It's just. Okay. Well, let me just. So at the top of your proof, you'll have a bunch of identities and then you can join them together. I told you this would be hilarious. B logically entail A and B. And B. Proofs don't have to be pink, by the way. Okay, so let's get back to. I also have a question really quickly, if you don't mind. Okay. Will there always be a single thing to the right of the turnstile? And the logics we look at today, yes. When you don't have that, that causes all sorts of technical problems. Okay, so that's how I do weakening. That's how I do weakening. And there's one more rule to mention, and that's the cut rule. So the cut rule says the cut rule allows you to string proofs together. So in much the same way you'd prove something quite complicated by proving a bunch of lemmas and then stringing those lemmas together. So the cut rule says if delta logically entails B and gamma and B together logically entail A, then I can combine those two proofs. And Gerard likes to draw plugs. And Girard likes to draw plugs, right? I'm plugging the out plug of B into the in plug of B to get a proof that gamma and delta logically entails A. So this is completely analogous. People that do linear logic and minimal categories like to think of composition of processes. That's what this is capturing. But there's a theorem that says that, and this was proved by Gensen for a specific logic, but it holds for pretty much anything that deserves. It holds for pretty much anything that deserves to be called a logic, and that is if you can prove something with cuts, you can prove it without ever having to use a cut. The proofs might be way, way bigger, but you can still do it. And the proof of this observation is an algorithm in general. And proof theorists and computer scientists like to study the algorithm itself because you can say lots of interesting things about it. So that was the sort of stage. So that was the sort of state of the art pre-linear logic. So linear logic was introduced by Jean-Ive Girard, and he wanted to drastically reinterpret the way sequence work. So we have been saying the sequent gamma entails A is saying that from the premises gamma, I can conclude A. But now let's rethink it. Let's think of gamma as representing a bunch of resources that I have at my disposal. Bunch of resources that I have at my disposal, and A representing the output of what I get when I plug those resources into some system. So imagine that I have a machine, and the things in gamma correspond to tokens, and I'm popping tokens into the machine, and the machine outputs A. So from that point of view, my previous rules of contraction and weakening, which seemed quite sensible, are now horribly wrong. Because let's go back to them. Contraction now says I have a machine and I. Contraction now says I have a machine and I put gamma, and then I put two A's into the machine and it spits out a B. Well, there's no reason to believe at all that if I just put one A into the machine, I will still be able to output a B. And weakening says if I can construct a B without putting in an A, then I can still get out a B if I also put in an A into the machine as well, which is great if you're the owner of the machine, but from a logical point of view, it's not good. And so we don't want either of those. Not good. And so we don't want either of those rules in our logic anymore. So linear logic begins with the removal of these rules. And what happens as a result is we go from categories where we have Cartesian products to categories where we have a tensor product. In other words, categories where I don't have the projection and the diagonal that I talked about before. So we're talking about maneuver. About before. So we're talking about monoidal but not Cartesian monoidal categories. And I guess this is repetitive, but I no longer have these maps from V tensor W to V and from V to V tensor V that would allow me to model contraction and weakening. So we're very much in the realm of vector spaces, possibly with some extra structure. That's why Gerard picked the name linear logic was with this in mind. And he also used the notation of a tensor to model his Notation of a tensor to model his conjunction. I'll just mention in passing, you can also get rid of the exchange rule. And when you do that, you get something called non-commutative linear logic. You can also have modified exchange rules. So there's something called cyclic non-commutative linear logic. But those aren't going to come up in my talk again. Okay, but it turns out that it's quite important to be able to use contraction and weakening, at least in some limited sense. Limited sense. So we want to be able to use those rules, but for specified formulas. And what we're going to do is have an operator on formulas called bang of x, which is denoted by an exclamation point. And for any banged formula, then I can contract or weaken with respect to it. And the intuition here is that a banged formula is a renewable resource. So I can use that X as many times as I want. Can use that X as many times as I want. And that includes, of course, not using it at all. So that's why weakening is valid. But if I have, imagine a Xerox machine which spits out as many copies of my X that I want, I don't need two Xerox machines. So therefore, contraction then makes perfect sense. So how do I model this in terms of categorical proof theory? Well, this is going to be an endofunctor on my category, my monoidal category, and the properties it's going to have to have. The properties it's going to have to have, most importantly, are it's going to have to be a co-monad. If you haven't seen these before, a co-monad is an endo functor equipped with two natural transformations from bang to bang bang and from bang to the identity. And these have to satisfy some associativity and unit constraints. Okay. And what we want to do is, remember, we want to be able to do contraction and weakening, which means I need these maps. I need these maps from Bang A to Bang A, tensor Bang A, and from Bang A to I. So I need Bang A to be a co-algebra. So co-algebras are familiar in the theory of half algebras and bi-algebras. So I need my bang A's to be co-algebras. And how do I make sure that happens? Well, the best and simplest way to make sure that happens is to assume that my category, my monoidal category C also, in addition to that tensor product, also has a plain old product. product also has a plain old product. And that furthermore, the product satisfies this isomorphism. Bang of A cross B has to be isomorphic to bang of A tensor bang of B. And people who think a lot about this like to imagine this as the bang is representing e to the blank. And so what this would say is e to the a plus b is e to the a times e to the b. That's the intuition, at least that. The intuition, at least, that most people have when thinking about it. And when you have this isomorphism, then I can re-derive those co-algebra maps. Can use contraction and weakening with respect to bang formulas as desired. Here's how you do that. So since I have products, I have a diagonal map from A to A cross A. I hit it with the exponential functor, the bang functor. That gets me from bang A to bang. That gets me from bang A to bang A cross A. I then use the isomorphism, and that gets me my desired map. And assuming that you've made the right assumptions about these various isomorphisms, then you can prove that the resulting map is co-associative, co-commutative, co-unital. And so Bang A is a co-commutative, co-associative, co-unital co-algebra. And that's what allows me to model contracts. That's what allows me to model contraction and weakening. So I can now do logic when I have all of this structure. Rick, the question has appeared in the chat. Is this a good time? Sure. It says, so for vector spaces. No, it's okay. I'll read it to you. Oh, okay. It says, for vector spaces, is bang V equal to the free vector space on the underlying set of V? I'm going to answer that exact question in a few slides. But yes. Not the free vector space, the free algebra. You'll see in a minute. Okay. Right. So whenever you have a co-monad, there's another thing you can do called the co-Kleisley category. And so the co-Kleisley category was defined by Heinrich Kleisley not long after the construction of monads. The construction of monads and co-monads, and it's a new category associated to the original category where your objects are the same as the base category, but an arrow in the co-Kleisley category is an arrow in the base category from bang X to Y. And so if you've never seen this before, it's a nice little exercise, which I highly recommend. It's not obvious how you compose these two of these, and it's not obvious how you prove that they're associative. All of that can be proved. Associative. All of that can be proved based on the equations that you require the comonad to have. And from that, you can derive everything you need to derive. So linear logic then begins with the real Gerard's realization. So Gerard isn't a category theorist, so he didn't phrase this in terms of co-Cleisley categories. But what he realized is the following. He was doing something entirely unrelated to this at first. He was looking at a category of coherent spaces in state. Of coherent spaces in stable maps, which is, for those of you who know such things, a model of simply typed Lambda calculus. It's a Cartesian closed category. But what he realized is that there's a decomposition, that his category he was looking at, actually the internal, the HOM decomposes as two separate things. So the stable maps from A to B, it doesn't matter if you know what a coherent space is, the stable maps from A to B are the same thing as the linear maps and linear in a very maps and linear in a very traditional sense of what linear means from this operator bang A to B. And so notice this is exactly what's going on in the Cloak Lisley category here. And so with this realization, and then Gerard asked, okay, so what's going on logically? Linear logic immediately followed. So differential linear logic comes from looking for specific models of linear logic. linear logic and noticing that the examples that he came up with, the arrows could be differentiated. And so therefore there's a notion of smooth map and there's also an underlying notion of linear map. And so what we would like ideally is a decomposition similar to this one, where on the left-hand side we have smooth maps and on the right-hand side we have linear maps. Okay, we have a bang and we have linear maps on the right. And we have linear maps on the right and smooth maps on the left. And let's think about what an inference rule capturing differentiation should be if that was the case. Let's not worry for the moment whether we can actually do that or not, but let's try to figure out an inference rule based on that idea. Okay. So, and let's think in particular about just plain old Euclidean spaces. So calc 3 or wherever you do this in your calculus stream. In your calculus stream. So let's imagine that I have a smooth map from Euclidean space X to Euclidean space Y. And I want to think of that as being a map from bang X to Y, a linear map from bang X to Y. Well, if I wanted to differentiate this, I would take its Jacobian matrix. And so at a point of X, the Jacobian matrix is a linear map from X to Y. So therefore, the process of taking the Jacobian should be a smooth map from X to the linear maps. From X to the linear maps from X to Y. Okay, this is the key point. And when you realize that's how differentiation works, the suggests your inference rule should be this. If bang X logically entails Y, then bang X logically entails X linearly implies Y. And that's the inference rule we add to the logic. Okay. So this is this. So this led to the differential lambda calculus and differential linear logic that Erhard and Regnier studied. So Robert and Robin and I always try to avoid assuming closed structure as much as possible. And just by properties of a junction, this inference rule is equivalent to this inference rule. If bang x logically entails y, then x tensor bang x logically entails y. The logic continues to satisfy. The logic continues to satisfy cut elimination. There's a notion of proof nets, and I wish I had more time to talk about this, but proof nets are a graph theoretic way of specifying proofs in linear logic. The first version is in the very first paper by Genives. They were then simplified by Danos and Regnier. And then, so there were all sorts of extensions and modifications to the basic idea of proofnet. proof net. They're a graph theoretic way of specifying proofs. And under this way of drawing proofs, cut elimination becomes a graph rewrite system, which theoretical computer science people love studying. And the rewrite system is really well behaved. It has remarkable properties that are well worth studying. Okay. And I think I've already made this point, but what we will have is a co-Kleisley category, which is a model of Lambda calculus. Which is a model of lambda calculus, and this leads to the differential lambda calculus. Okay, so what am I going to do categorically? Well, what I need in terms of category theory is I need an operator, which says if you give me a smooth map from bang x to y, then I need to differentiate it, which means I need an operator d of f, which goes from x tensor bang x to y. And if you do that, that suffices. And if you do that, that suffices, it suffices to differentiate the identity map. And if you can differentiate the identity, it turns out you can differentiate anything. And so what I need is a map little d, which goes from x tensor bang x to bang x. And the trick is, if you have any old smooth map from bang x to y, I can differentiate it by precomposing with little d. So d of f is little d. Is little D composed pre-composed with little F. Okay, but now I have to state some axioms for what this map little d should satisfy. And the first thing I need to realize is I have to have additive structure on my ARM sets. I have to be able to add maps because one of the rules is going to be the Leibniz rule, the product rule. And so I have to be able to say what it means to add two maps. So, by some means, there are several ways you can do this. I have an additive structure on my homesets. Okay. And And I need a bit more than that. I need basic differential identities as well. So all of these now have to be expressed co-algebraically because remember, this is a model of linear logic. So bang x is still a co-algebra. And so what you have to do is write down the rules of basic calc1 in terms of co-algebras. So sometimes in my more sadistic moments, I imagine the first lecture in my calculus for business. The first lecture in my calculus for business students class, beginning with telling them what a co-algebra is. But I don't think that dream will ever become a reality. Nonetheless, the four rules that you need are the basic Calc 1 rules. Derivative of a constant is zero, derivative of a linear function is a constant, the Leibniz rule, the product rule, and the chain rule. You can find all of these in various papers on the subject. They're all, the first papers are all on my website, my website, but they're all. My website, but they're all over the place. And I wrote down one for you. So I wrote down the product rule. If you are like me and have a hard time thinking in terms of co-algebras, it's very sensible to read these backwards. So read from right to left instead of left to right. So from right to left, what this is saying is multiply my two terms together and then differentiate. This has to be the same thing as differentiating the left-hand term and then multiplying. hand term and then multiplying right since i'm reading left to right or right to left delta is now multiply so differentiate the first guy and then multiply plus i don't know if you can see the plus there plus differentiate the second guy and then multiply that's the product rule and similarly you can write down all four of these rules uh as co-algebras okay so there are many many subtleties to this definition so i put this sentence there because i suspect several people are laughing I suspect several people are laughing about this. There are all sorts of modifications to the original definition that you can do. You can start with assuming that the bang functor is monoidal, or you cannot assume that as you wish. There's additional equations you can add, but you don't have to add to have the basic theory. One way to assume that Bang X gets that Gets that sorry, one way to assume that your category has additive structure is to assume that your category has byproducts. In other words, products and co-products that coincide, much like the category of vector spaces. What's happening here? And when you do that, you wind up with some additional rules. And I'm slightly behind on time, so I'm going to skip how these additional rules work. I'll just say: if you want to see, Say, if you want to see some of how these variations go and which ones are equivalent to which other ones, there's a paper called Differential Categories Revisited by myself, Robin, J.S. LeNay, and Robert, where a lot of these ideas are sorted out. Okay. So I'm seeing, Christine, the number of 12 appearing up in my corner. Are those 12 questions? Or is that, what is that? I'm not sure what the 12 is. There is a discussion. There is a discussion going on in the chat and it's progressing. Okay, so I should just ignore it. I think so. Okay. So now we want to look at models because this whole subject is pointless if there aren't good models that capture differential calculus. So there's a meta theorem in linear logic that says the category of sets and relations is a model. Says the category of sets and relations is a model of everything you study in linear logic in a boring way. And that's true for differential linear logic as well. So you build a category where your objects are sets and your arrows are binary relations. Composition is the composition of relations and the banko-monad is the finitary multiset functor. And in this case, the tensor is the Cartesian product of sets. And so multisets just mean multi-set is a Multisets just mean multiset is the same thing as power set, except an element can appear more than once in the set. And so you can check that that isomorphism between bang of A cross B and bang of A tensor bang of B is that that holds with this definition. And so I need a, so you have everything you need. I need a map from X tensor bang X to bang X. So I need a map that takes an element of X and a sub and a multi-set. In a multi-set, and I just toss x into the multi-set. And that works. And you can check that all the equations of differential calculus are satisfied. So this goes back to the previous question about free algebras. So here's an example. If you give me a vector space, I can associate to it the free symmetric algebra, the free commutative algebra. And what that is, is it's k plus. It's K plus V plus V tensor V symmetrized plus V tensor V tensor V symmetrized, etc. etc. etc. And the easiest way to think about this is by choosing a basis for V and then thinking of S of V as the space of polynomials in that basis. I realize that's a basis-dependent way of thinking, but it's intuitively the easiest way to think about it. And so it's standard that this is a monad on vector spaces. It's a free algebra, so therefore it's a monad on the underlying category. And so it's a co-monad on the opposite of the category of vector spaces. And so since we're working in the opposite, I need a map going in the opposite direction of what we're used to. So I need to go from S of V to V tensor S of V. And what it is, is if you give me a polynomial, I just do the partial differentiation with respect to all. Do the partial differentiation with respect to all the variables in that polynomial, and then I sum. This will be a finite sum, even if my vector space is infinite, because we're just talking about polynomials. And it's straightforward to check that all the equations hold. In fact, here, all the equations are the real calculations of first-year calculus. So you might say this is a little bit unsatisfactory because you would like to do an example on vector spaces. An example on vector spaces. So, Daniel Merfett and James Clift showed that Maas Swedler. So, Ma Sweder, the famous Hof algebra guy, wrote the sort of canonical textbook on Hof algebras, has a construction of the co-free co-commutative co-algebra. And that induces a comon out on vector spaces. And those guys proved that it's a differential category as desired. Okay, so Christina, I should stop at like 1155, right? 11:55, right? Well, actually, at 11:45, so we're over. No, because I started at 9:15. Oh, I'm right. I'm so sorry. You're exactly right. No, we have you scheduled to go right up to 10 o'clock. So you've got seven minutes left. Okay. Sorry, I'm on a 45-minute schedule and I forgot. Yep, it's good. So, topological examples. So, what we would like is topological examples of models of the differential calculus. Differential calculus. And so, because functional analysts look at spaces of smooth functions and properties of them, so we'd like to be able to say something about that. I'm just going to very briefly mention this, but I want to refer you to a talk tomorrow by Marie Kurjan, which is going to do a lot more of this sort of stuff. I just want to give you the flavor of how it works. So, there is a category of convenient vector spaces, which is studied by Frohler. Which is a study by Froelicher and Kriegel, where it turns out all of this structure works perfectly. And so, this is work that I did with Toma and Christine Tassel. So these are a special class of locally convex vector spaces. They are a symmetric monoidal closed category. So already this is really a nice structure. The tensor product is just you take the algebraic tensor and you complete it. and there is a convenient vector space structure on the set of all linear continuous maps. But even more interesting is there's a nice notion of smoothness in this category. And so how do we define it? Well, we'll say that a function between convenient vector spaces is smooth if it takes smooth curves in E to smooth curves in F. And this is inspired by something called Bowman's theorem, which says that that's true about Euclidean space. That's true about Euclidean spaces. So, a function between two Euclidean spaces is smooth if and only if it's composite with every smooth curve is also smooth. So, it takes smooth curves to smooth curves. And Froeliker and Kriegel's idea is let's take that as our definition in this more abstract setting. And that works really nicely. So, Frohlicher and Kriegel proved that the category of convenient vector spaces in smooth maps is Cartesian closed. This is a fantastic result. So, this is. Fantastic result. So, this is a category of where you can do calculus and you have function spaces at your disposal. So, already this is really nice. Even better, it's got a co-monad on it. So it has exactly the decomposition that we wanted to study. So, you have a base linear category and the co-Kleisley category is the smooth category. It also satisfies this isomorphism that we need to do linear logic. They didn't. Do linear logic. They didn't call it bang, obviously. They called it Lambda, I think. And so, therefore, every object has a bi-algebra structure. And so Christine, Tomas, and I proved that, in fact, this is a differential category. And the derivative is a derivative in the standard sense that we're used to studying. So it really is a convenient differential category. And even nicer than that, Even nicer than that, there's a well-defined theory of convenient manifolds. So there's a terrific book by Kriegel and Mikor called The Convenient Setting for Global Analysis. And they develop in great detail the theory of convenient manifolds. Okay, one more thing I want to mention as a lead into the rest of the conference. And that's the theory. Oops, I don't know why there's a blank slide. It doesn't matter, I guess. Uh, it doesn't matter, I guess. Um, of Cartesian differential categories. So, one category that doesn't fit into anything we've talked about so far is the sort of Calc3 category, right? Your objects are Euclidean spaces and your arrows are smooth maps. That's not the Koch-Klisley category with respect to any bang. And so that issue led to the idea of Cartesian differential categories. And the idea here is to ignore the base linear categories. Um, the base linear category and just work directly with the smooth category. Okay, so if you do that, then your differential takes on a different form. If you give me a smooth map from x to y, its differential is going to now be a map from x cross x to y. There's no bang to fuss with anymore. And it satisfying various properties. It does have to be linear in the first variable. So in particular, it has to make sense to be able to define what linear in a variable means. What linear in a variable means, um, but you can do all of this, and um, examples are the co-Kleisley of a differential category, co-Kleisley category of a differential category is a Cartesian differential category. But the category whose objects are Euclidean spaces and arrows are smooth maps is a Cartesian differential category, which does not arise as the co-Kleisley category of a differential category. Category of a differential category. And for details in many examples, see the next talk by JS. And most importantly, to this conference, Cartesian differential categories are very simple examples of tangent categories. The most important idea here, this is due to Robin and Jeff. And for more information on that, I will just say enjoy the rest of the conference. Thank you. Thank you, Rick. Thank you, Rick. I don't know. It's always hard to have problem Zoom, but it looks like we worked it out. So we have a little bit more than 15 minutes scheduled for questions. We don't have to fill up the 15 minutes with questions, but before I open it up to the floor, there was a discussion that was going on in the chat. So I just want to check with Alexander, Yuri, JS to see if there are any further questions, and then I'll open it up to the floor. Yuri says no, he's good. Yuri says no, he's good. Everybody's good. Okay. Any questions for Rick? Yeah, so I'm looking at this chat. It looks like I addressed it with talking about the free algebra and the co-free co-algebra. So is that all sorted in people's minds? Cool. Yep, so I have a question, if I may. If I may, yeah, let's yeah, you talk about uh this um uh bosonic um focus space construction for vector space, and so that gives you a model. Sorry, can I ask who's speaking? I can't see. Um, Nicola Blanco. Oh, hi, Nicola. Yeah, and but so when you work with vector space, you don't have a duality, it's not star autonomous, so you want you may want to go to finite-dimensional vector space, but then Finite-dimensional vector space, but then this bosonic construction is not finite-dimensional, right? So, you could go to fermionic vector space. And does this give you a model of differentiation? It doesn't give you all the isomorphisms you need to even have a full model of linear logic. There are various problems with, unless you work over fields of characteristic two. Jeff Krupp has a question. Oh, sorry, Rick. Go ahead. I was just going to add, but it's still, you know, even if it's not a model in the standard sense, it would still actually be quite interesting to think about that question. You know, you don't have to have the model. You don't have to have all of the isomorphisms. It could still be interesting. Thank you. Jeff Crutwell? Yeah, I just wanted a small credit thing. Attending category was actually your. Tangent category was actually Yuri Rositsky. I just wanted to. I mean, I know you knew that, but I just wanted to say it because you've predated us by like 30 years. I'm so sorry, Yuri. I should have added. I will change the slides before they get posted. Sorry. No problem. I had a question about the Swidler model for the co-commutative co-free algebra. What does this look like? What does this look like? The actual co-algebra? Yep. So, what you do is you take, you actually use the symmetric algebra construction. You take the sum of symmetric algebra is one for each copy of the element of the vector space. Okay, I think I see what it looks like. Okay, so it's not like the product instead of the direct sum. It's a small number. Yeah. Yeah. The paper by Daniel that I mentioned is really covers all of this nicely. And when you look at finite dimensional vector spaces in that construction, it's quite straightforward to write down what the differential operator is. And then you use the fact that for categories of co-algebras, arbitrary co-algebras are the directed co-limit of the finite dimensional sub-algebras, sub-co-algebras, and you use that fact to write down the formula generally. Use that factor, write down the formula generally. Thank you. It's possible that you addressed this during the talk and I missed it, but I wanted to ask, in what settings is differential linear logic really a natural thing to think about? So I think that question is answered by the number of applications that have come out of Come out of the subject. I think an argument can be made that working directly with Cartesian differential categories is probably a more natural thing to do if you're doing stuff like functional analysis, where you don't want to have to fuss with a co-monad. But I think that there are enough talks at this conference where significant progress is being made to suggest that I think it is a natural way to think about things. So I would say, So I would say yes. Okay, we'll take one last call for questions. I see Tom Goodwillie's hand going up. Here's a really basic or naive or something question. Way back in the beginning of the talk, before you got to linear logic, you were mentioning intuitionistic logic and classical logic. Intuitionistic logic and classical logic, and I didn't even get the point of view there. I don't know what it means to say that in classical logic, you don't ever have to say not, not A. Right. So in classical logic, not, not A is the same thing as A. Intuitionists would argue that they're not the same thing. That not, not A is a strictly weaker statement than A. So one of the points of intuitionistic logic is only formal constructive proofs are allowed. Constructive proofs are allowed. So, in particular, there's no larva excluded middle. Right. Okay. And once you get rid of larvae excluded middle, then not A and A are two very different things. Right. I figured that was basically the answer, but I got somehow mixed up by what you said. I guess A entails not not A is. Entails not, not A is sort of part of anything. So that direction, yeah, that direction is still true. In classical logic, it's an if and only if, and it's not very interesting. Exactly, yeah. In intuitionistic logic, there's still one direction. So A does entail not, not A. It's the other direction that's problematic. Just because you don't have a proof of not A doesn't mean you can prove A is the way to think about it. Okay, thanks. If I may comment on this last point. On this last point of not not A. A good example of a situation where A and not not A are not the same thing is the poset of, of course, the Eiting algebra, so-called, of open subset of a space. So this is actually a category and closed category, but well, really. Well, Rick, you know very well this example. I am discussing it because I guess topologists like Tom Goodwilly may be able to understand it because it has something to do with topology. So, in this case, not A, where A is an open subset, is the exterior of A. Of A and not A is the exterior of the exterior, which is bigger than A actually in general. And so, okay. Thanks, Alex. I had a question. You mentioned something called cyclic logic, which I hadn't heard about before. I hadn't heard about it before. And I was just wondering if there are connections between that and differential logic, because I'm interested in cyclic homology, which is all about using sort of cyclic actions of cyclic groups in conjunction to study calculus, differential calculus. Right, so cyclic logic means so you have gamma logically in. So, okay, so I need to change the setting slightly. If you have classical negation, what you can do is. What you can do is you can take all your formulas on the left-hand side and move them over to the right-hand side by negating them, right? So imagine instead of gamma logically entails A, maybe I'll go back to the whiteboard again. So if you have classical negation, so not, not A is equal to A, then what you can do is move all of these guys on the left over to the right by negating them. Them okay, and so you need that to be able to say what cyclic linear logic is. Cyclic linear logic says I don't have the arbitrary exchange rule, but I can cyclically permute the formulas in this sequent. Okay, so not arbitrary permutations, just cyclic permutations. And the logic is much more well-behaved than the purely The purely non-commutative logic. I have no idea what that has to do with cyclic homology, however. Maybe I'll try to make one last call again for questions. Okay, so hearing none, I would like to thank Rick again for a really lovely introduction to the conference. It was fantastic. I would like