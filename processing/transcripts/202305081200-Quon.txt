Or those images variable, it's still pretty hard to find the right differences. Okay, so basically what we have been trying to work on is basically some form of like network embedding, right? So the general principle here is that instead of trying to infer these really high-dimensional noisy networks, what we're going to do is essentially try to do something like principal components, which is Like principal components, which is basically like taking these networks and just embedding them into some low-dimensional space and then trying to compare. So, in these kind of low-dimensional space figures that you'll see a lot of, each point here, instead of representing a cell like in a typical single-cell talk, each point here represents a network of an entire population. Right? And so, if there's 10 individuals, there's going to be 10 points on this figure. And so, the general principle here is that if two points are close in this space, then Close in this space, then globally speaking, at least with respect to the features that we've chosen, the network structure should be generally similar. And so the trick that we use to basically generate these embeddings is that instead of actually building a network, instead of trying to actually infer a ball and stick network and embed it into this low-dimensional space, we actually just try to directly infer this space from the data without inferring the network. And so the strategy. Network. And so the strategy that we use is that we basically take data from a single cell type across individuals, and we basically build an autoencoder for every single cell population. And so the general idea here is that if you take each one of these genes in your gene expression profile and you center and scale them appropriately and transform them, then the only, then what an autoencoder should learn is essentially just the covariation patterns between the input features. Co-variation patterns between the input features. And so, the general idea is that if you have two different types of cells with different underlying co-expression networks, then the decoders of these networks should essentially capture different co-expression patterns. And so our idea is that if you look at the, if you train autoencoders for different cell populations, and then you compare the parameters of that network across cell types, then somehow those parameters must capture the differences in coexpression in some sense. And so basically in these In some sense. And so, basically, in these plots that I'll be showing you, these are principal component plots, but what they're calculated on is actually the parameters of the models. So, you basically have a giant matrix where it's kind of like a parameter by individual matrix, and that's what's basically being projected into here. And so, I won't go over too much of the details of how this is done, but there's a lot of kind of specific choices that you have to make when you do these things. So, generally speaking, if you train two So, generally speaking, if you train two different autoencoders on different data, there's no guarantee that this parameter here is comparable to this parameter here. And so, to actually make this work, we have to do essentially multi-task learning, which means that we're going to train these auto-encoders simultaneously across all the populations and only allow a sparse number of parameters to differ between these populations. And so that's basically what makes this low-dimensional projection work. Yes. Exactly, and so that's why you have to multitask across the species and allow a very sparse number of differences so that each of the So that you can't just flip one parameter and still get an equivalent covariance matrix. It's not necessarily Yeah, I guess that's potentially. Yeah, that is potentially true. Yeah, we basically hope that with enough regularization that hopefully that doesn't happen often, but you're right, there's no guarantee about this procedure that that doesn't happen. Yes. So that's kind of the, yeah, I must have, yeah, I didn't explain that properly. I think the point here is that if you try to infer an explicit network and then compare them, there's a lot of like design choices you have to make, like how do you define an edge, whether it exists or not, and so on and so on. Or not, and so on, and so on. And so, we're trying to basically compare networks in this embedding space so that we don't actually explicitly infer edges and try to compare them. We're just saying that, look, here's some parameters that distinguish the co-expression patterns of these cell types. And how do those parameters vary across individuals? Yeah, but when you explicitly infer these ball and stick networks, you generally have to. So, how is the way this blue thing is versus the red things? Because could be but you are fixing the six links there and there are different six things. So how Oh so this is yeah, this again maybe was a poor choice of visualization. In the actual model, we're not fixing any links here per se. The point of this visualization is just to say that if the co-expression patterns of two cell types are different, then the decoder has to somehow at the end of the day capture that these three are linked, these three are linked in this cell type, but not necessarily here. But we, yeah, we don't fix anything. We don't fix anything when we're actually training these models. Okay, and so basically, I think some of the broader questions that we're trying to answer with this model are essentially when you look at the network of a cell type as it's going through development, we wanted to kind of see whether we could use our model to distinguish: you know, are those changes in gene network structure moving kind of in as a kind of a continuous process or is it moving in kind? Process, or is it moving in kind of like discrete steps? And so, we basically performed a number of simulations where we tried to embed networks where the differences in the networks were kind of big between different individuals versus kind of a more continuous difference between individuals. And so, in a nutshell, again, each point here represents an entire network, and the color of the node represents the basically the structure of the underlying network. And so, in this case, Network. And so, in this case, where we try to basically simulate discrete steps in gene network movement, you basically see that at the very least, you can see there's a very clear distinction between where each points of each network type sit in this visualization. Unfortunately, we couldn't actually correlate the distance in this embedding space to the so-called distance in the covariance matrix, but at least these things separate out cleanly. And so, we did a number of other simulations. And so we did a number of other simulations where we, yeah, we did continuous simulations where the differences were smaller. And so at least you can still see that the cells of different network structures still segregate somewhat in this embedding. And so we went back to our basically initial fetal liver cell atlas, where again, we focused on a set of six cell types for which there's a lot of cells that were sequenced. And basically, these cells were sampled from Cells were sampled from basically the first and second trimester. And so, what we did again is that we took the data, which again, when you draw a typical UMAP visualization in the points or cells, you get nice clear separation of individuals by the time point. And then we basically, again, transformed the genes individually so that, at least with respect to mean gene expression, you can't really tell the difference between individuals of different ages. And so, again, we're just trying to see whether we can. So, again, we're just trying to see whether we can capture a distinction in the co-expression that were conditioned on there being no difference in mean expression. And so here are the embeddings that we generated. So again, each point here is an individual colored by which part of the, basically which time point they were sampled from. And each one of these cell types was basically trained separately. And so broadly speaking, what you can see from these visualizations is that for each one of these five For each one of these five cell types, at least, you can see like a relatively clear distinction between the red and the blue, which represent the first and second trimester. And so our take-home message from this is that at least preliminarily, there does seem to be a pretty big difference between first and second trimester, although we couldn't really see any more refined trajectory like we were looking for. But if you actually look at the autoencoder and try to figure out what parts of the uh what parts of the network are changing the most between these different uh time points you can actually pull down some relatively small gene modules that seem to vary quite a bit in terms of their co-expression and so i'm skipping over many details here but when you when we first carried out this project we did this in a very naive way and kind of didn't really put too much thought into features didn't really put too much thought into feature selection. And so it turns out that when you do this kind of, when you train this kind of model, you can run into all kinds of problems where if you have genes that change significantly in expression, then your power to detect the changes in the edges vary quite a bit. And so for most of these visualizations, I'll show you the genes that we selected specifically are not differentially expressed significantly between the different populations. And so presumably these changes in edges are not driven by. Presumably, these changes in edges are not driven by just simple genes turning on or off. But I'm actually going to stop here for liver because if you notice, for most of these visualizations, we only have about eight points on this plot. And so it's really hard to, it's actually really hard to identify consistent gene structure that varies with development because we have too few samples here. And so we're basically repeating this analysis with 130 people instead of 10. But that. But that analysis is not done yet. So basically, one of the other collaborative projects we have on the lab is essentially trying to study how to take patient-derived IPSCs and efficiently transform them into different types of neurons. And so one thing about taking, for example, one thing about working with IPSCs and differentiating them into neurons is that when you take the original fibroblasts from your individuals and you reprogram. Fibroblasts from your individuals and you reprogram them into IPSCs, what everybody knows is that not all IPSCs are created equally. So even if you take fibroblasts from a bunch of individuals and you apply the same reprogramming protocol to turn them into IPSCs, what you can readily see is that some IPSCs are really easily differentiated into terminal neurons and some are not. And so one of the kind of big questions in my mind anyways is the question of what, like, why do some IPSCs have My PSCs have essentially higher potential for differentiation versus those that have low potential. And so basically, there was a study done a few years ago where basically in this study, they took 215 different IPSC lines and they basically tried to, they applied the same differentiation protocol and basically tried to differentiate them into two types of neurons. And what they did is while they're differentiating these IPSCs into neurons, they basically sampled cells at different time points. Basically, they sampled cells at different time points along this differentiation process. And then, what they did is they did two things. For each line, basically, they looked at the cells that they sequenced at day 52 at the end of the process, presumably. And they basically, for every line, they calculated some measure of potential, where potential basically just said for a given line at the end of the process, how many of the cells that I sequenced here were basically terminal neurons or that I could recognize as. Basically, terminal neurons, or that I could recognize as terminal neurons. And so, for some cell types, the potential was zero in the sense that all of the cells that they sequence were just progenitors. And for others, the efficiency goes as high as like 90%. And so, in their study, they were trying to basically do two things, one of which is figure out whether there's genes whose expression level correlates with efficiency or potential. And the short end of the story is that they didn't really find any. And they also try to find SNPs of these IPSC lines that also. These IPSC lines that also basically potentially drove potential, and they didn't find any of those either. So, what we did is we took this data and we basically looked only at, we specifically only looked at the data of the progenitors early on from day 11. And so, what we wanted to do is try to figure out whether there's something about the network structure that is indicative of how efficiently transformed they were at day 52. We're at day 52. And so basically, in this plot, every point is a single donor. There aren't 215 points here because a bunch of most of the donors weren't sequenced very deeply. So there's only about 60 or 70 points here. But each point represents a donor and their corresponding network. And this is basically the PC visualization of those parameters. And what we found is that when we looked at the basically the genes that drove those differences in co-expression across the lines, basically there's strong enrichment in. Basically, there's strong enrichment in wind signaling and only wind signaling. And so, what we did is that we then took the potential calculations that they made from day 52, and we just put them on top of the visualization. And surprisingly, what you can see is that there seems to be relatively clear separation of low and high potential lines. And again, this is a bit surprising to us because essentially these PC. Because essentially these PC visualizations are only generated based on the parameters, these autoencoders that had no information about the potential. And so these PCs really just capture the major directions of variation in the networks. But somehow they also seem to capture differences in potential as well. And so that was pretty surprising for us. Again, when you look at, when you try to figure out what are the genes driving those differences, basically what it Those differences. Basically, what it comes down to is a set of about 20 to 40 genes, again, enriched in WINT signaling. And what we found is that WINT basically has very, or that portion of the WINT pathway has very poor connectivity in high potential lines. And it has basically there's relatively dense connections in lines with low potential. And so we did a number of other experiments where we tried to take our very small number of lines, divide them into two mutually. Divide them into two mutually exclusive set of lines, repeat the analysis on the two different pools. And what we found is that even when you divide your data in half and you kind of do the analysis separately, you can still see enrichment of wint signaling. And you still see that basically wind co-expression is negatively correlated with potential. One thing that was really surprising to us actually was that we actually then looked at the terminal neurons. Then we looked at the terminal neurons. So we went to the day 52 cells that were identified as neurons, and we basically built gene networks of only the terminal neurons. And so again, each one of these points is a donor. These now represent networks of terminal neurons instead of IPSCs. And then we basically did the visualization. And again, what you can kind of see to a lesser extent is that even the terminal neurons seem to have significantly different network structure. Network structure, and those differences seem to be correlated with efficiency, which suggests maybe that there's some essentially there's some residual memory of the original IPSC line, even in the terminal neurons. But yeah, we have yet to validate that. So finally, we then try to ask the question: is this wind finding specific? This went finding specific to this system where we're differentiating IPSCs into neurons. And so we found another data set which was significantly smaller where they basically tried to do a similar experiment, except instead of differentiating into neurons, they differentiated into these type of cells called definitive endoderms, which are kind of like progenitor cells on the way to pancreatic and hepatic cells and so on and so on. And basically they repeated the same experiment, sequencing these cells along the way. Cells along the way, along the kind of differentiation protocol. And the way that they measure efficiency is that instead of, like, unlike the first study, what they did is that they basically plotted a big UMAP visualization of all their cells from all the time points. They defined one end as being the pluripotent cells, the other end being definitive endoderms. And then for every line, they basically just calculated like an average pseudo-time. So, you know, to the extent that you believe pseudo-time, basically they compute average pseudo-time. Basically, they compute average pseudo time for each line, and then that's essentially their measure of efficiency. So, what we then did is we took the same network that we trained from neurons using the same genes but a new data set, we piped data from each individual through that neuron network, drew the visualization, and then plotted potential on top of it. And so, this separation here is much worse than. This separation here is much worse than in the neuron data set, but at least to my eyes, you can still see some level of separation of the low and the high potential lines, suggesting that maybe this is, you know, maybe it's something that's more general across different types of lineages and so on. But we're still working to figure out whether this is real or not. Okay, and finally, we tried to repeat their analysis to find genetic variants that are both that are also associated with differentiation potential. And we focused mainly on genes related to the ones that drove the PC visualizations in our plot. And basically, we found variants in like two or three different genes that actually, well, one of them actually reached genome-wide significance. The other are kind of close. But essentially, we found variants. But essentially, we found variants in a couple of genes that turn out to be negative regulators of WINT. And so we think that they represent potential targets for trying to modulate the differentiation potential of IPSCs. Yeah, and so I'll basically stop here. So in summary, basically, we have come up with some framework for trying to implicitly infer and compare networks across different cell populations. And one of the And one of the things that makes this framework interesting is that that then allows us to try to associate phenotypes of the individuals of the donors with variation in gene networks. So we try to look at developmental time and differentiation potential. So this finding that wind co-expression is potentially related to differentiation potential is not something mind-blowingly new in the sense that if you go into the literature, you can find a ton of examples of people who work with IPSCs who use, for example, wind inhibitors. Who use, for example, wind inhibitors to increase efficiency for different IPSCs when they try to differentiate into different lineages. I think one of the nice things about our work is that only about eight out of 33 of those genes that we pulled down are annotated in Go, at least as Wint. And Wint itself, at least if you believe Go, you know, is actually involves quite a few number of genes. And so we think we've potentially narrowed down some more specific. Narrow down some more specific drivers of differentiation potential. But some of our ongoing work with our collaborators basically involves trying to directly perturb some of those 33 genes, including the WINT regulators, to determine if we really can affect potential differentiation potential of IPSCs. And another thing that we're kind of interested in is actually looking at the fibroblasts before reprogramming to see whether the WINT structures maintain. The WINT structure is maintained even after reprogramming from fibroblast to IPSC, since it seems like there's differences in both the IPSCs and the terminal neurons. Okay, and so this work was largely, the computational work was largely done by a senior graduate student, Rita, in the lab. This is a collaboration with some other stem cell folks at UC Davis. And the liver analysis is basically part of a, again, larger liver. Basically, part of a larger liver human cell atlas network. And yeah, here's our funding. And I'm happy to take any questions. Yeah, Rich. In the beginning, when you In the beginning, when you have the autoencoders, if you set those to kind of really boring linear gates and you only have one layer, what is that an analog of? Like, what's the straw man where you really limit the heck out of those autoencoders? Is that just make the covariance estimate, give you a single sample of expression, and then do like a maximum likelihood estimation of the distances? Or what is that? So you're asking what happens if you, for example, make the whole decoder linear? Yeah, like it's always. Decoder linear, yeah, like it's always kind of fun to just make a one-layer version of those things to see what the kind of linear baseline would be. Yeah, so if you if you make the decoder like a linear, like a completely linear decoder or even just a one-layer linear decoder, it's pretty close to functionally, it's pretty close to PCA. I mean, are you asking if you would see the same thing if you had a linear decoder? Well, it wouldn't, no, but you still. No, but you still have the interactions between it wouldn't be PCA unless you chose. All right, I'll bug you afterwards. I think it would just be PCA if you if the if the decoder is fully linear and the encoder is I don't think the encoder has to be linear, but if the decoder is fully linear, then it's what you get is pretty close to the PCA because it's just a bunch of like matrix multiplications, it's a bunch of linear projections. Yeah, all right, or Yeah, all right. Or some strictly positive version. Yeah, right. Yes. Very, very nice. Thank you. So a couple of questions. Another issue in this IPSC differentiation is the homogeneity of the terminal population. Do all the dopamine, you know, the excitatory neurons actually look the same or not? Have you looked at not just the ability to differentiate? Not just the ability to differentiate, but ability to differentiate into a homogeneous population or not. And then the second question, and I think you showed something about the and related to my original point. I think my hypothesis is that what that space is capturing is the modularity of the network, not necessarily the connectivity within those modules. So, if so, and you showed that in progression where you see that the wind. Progression where you see that the wind kind of module is kind of more disconnected. We know a little bit about the wind signaling pathway. So, can you start hypothesizing parts of that pathway that maybe are not acting because there's separation now? There's something that happened along the signaling cascade that is turning off whatever expression of downstream genes need to be turned on when wind is active. I was just wondering, we know a little bit more about that, about the structure of that. About the structure of that pathway. There's a lot known about WINT, but specifically the eight or nine factors beyond SOC 13 that are annotated as being WINT and in the network, there's not much known about those factors. And so there's a lot of problems still that we haven't resolved about our analysis, which is that the problem with working with co-expression is that if you have genes that are almost perfectly co-expressed, then when you actually Then, when you actually use these kind of feature attribution methods to figure out what the network is doing, you might miss like the algorithm might pick one gene, even though it could have equally picked another gene that was equally co-expressed. So the part of WINT that is potentially being pulled down by this network could be bigger than what we see. But at least for the genes that we see that are annotated as WINT, there's not a whole lot known about those. There's not a whole lot. Those there's not a whole lot known about their role in IPSC differentiation or even like stem cell differentiation that we can find. Okay, we'll leave your question for the discussion so then we can continue. Thank you very much. And our next speaker is from the next one. Your next speaker is Richard.