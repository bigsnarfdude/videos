All right, thanks. So it's a great pleasure to be here and to be here in person. It's so nice to have this type of meeting again, although it's still hybrid. So today I will give the blackboard talk, which is the other great thing about being in person. I always like blackboard talks because they're more informal and more suitable for an environment like this. I don't know how it works with blackboards and hybrid, but I guess we'll figure out. So, what I'll be briefly describing, it's supposed to be a fairly short talk, is based on unfinished work, it's based on a bunch of projects. That's all kind of unfinished, almost finished, not quite finished, etc. With a bunch of collaborators, we start. We just lost video. I don't think they can hear us when we talk. I think we can only talk among ourselves. I'll deal with it. Oh, there. Okay, it's back. Back button muted. Yeah, but I can only see you, cannot see Jan. I can see Jan writing, but he's muted. Yeah, we can hear him. I'm trying to deal with it now. Just a particular statistical approach to something that you could call everything or not. And it's based on at this point that. And it's based on this point that low energy knowledge is high energy ignorance. I think what we are seeing and have been learning over the years is that everything that we obtained from our lower energy fist. Obtained from our lower energy physics and our low energy computations gives us an interesting insight into the full theory that we're looking at, but it's extremely incomplete information that we're obtaining. So we, depending on how many bits and pieces of low energy information we have available, we learn certain things about the high energy theory and the UV theory, but that knowledge is very incomplete. We would need to measure and compute everything to fully establish what the high energy theory. Everything to establish what the high energy theory is. And the question is whether one can quantify a little bit this ignorance. And quantifying ignorance is, in some sense, the thing that statistical physics is all about. The question is whether the ideas of statistical physics can be applied to this particular situation. And that's especially important if our low-energy knowledge includes gravitational knowledge. Because if we have gravitational low energy knowledge, we have some additional interesting insights into the high-energy physics of the theory. Energy physics of the theory. For example, if we have gravitational knowledge, we can compute the entropy of a black hole. We can even compute the entropy of a black hole at a very high temperature. So we know something about the density of states at very high temperature. That's UV information, but it's still accessible to us as low energy observers because we have gravity at our disposal. So somehow doing low energy effective field theories in the presence of gravity is a bit of a funny thing and it gives us Of a funny thing, and it gives us a bit more information than maybe in standard low-energy effective field theory that does not have gravity in it. Maybe it's a bit like having low-energy effective field theory plus a description of the hydrodynamics of the theory or something like that. Maybe that's some sort of analogy. Now, this ignorance has several components. One part of the ignorance is One part of the ignorance is spectral ignorance. So with just some low energy measurements, it's impossible to figure out the detailed high energy spectrum of the theory. For example, if you compute the partition function of a black hole and you interpret it as a partition function of the theory, you'd find the temperature. It's an approximate computation that you do with the black hole. It's not an infinitely precise. It's not an infinitely precise UV computation. And what you get is you do not get a very precise sum of exponents. That would be brilliant because then you would have exactly solved the theory. But rather, what you get is something that looks more like this. Where rho E is some continuous N is a density. It's the density of states. It's the density of states. So, what you get is a continuous density of states. That's clearly incomplete information. If you take the actual underlying very precise, complicated, probably chaotic microscopic spectrum, and you average a little bit, you probably end up with something like dissolving. So, this is some sort of averaged or coarse-grained, washed away, whatever you want to call it, version of the precise spectrum of the theory. That's an example of spectral ignorance. Ignorant, but there's also basis ignorant. We also don't know precisely what a high-energy basis of state is. There's basis ignorance. And very often, this is something that's even familiar from condensed matter theory, this very often shows up as an approximate unitary invariant at high energies. Some approximate unitary invariance. So, practically, it means that if you take your theory, you take a narrow band of energy eigenvalues, the very high energies, then in many cases we see that you might as well do a random unitary in that energy band, but it would have zero implications for the low energy physics. So, there is both spectral ignorance and there is basis ignorance. Now, normally in statistical physics, in standard statistical physics, what we get is spectral ignorance, or we get some, sorry, well, we get some ignorance, but we don't get all this ignorance in standard statistical physics. So we need to upgrade our standard statistical physics approach in order to capture all these features somehow. I should like, I won't say much about averaging versus non- Say much about averaging versus non-averaging and so on, and that might be a fun subject for a discussion later in the week. Although there's all this ignorance, it does not imply that the UV theory is average. It just tells us that based on our low energy information, you might as well have replaced the UV theory by an average theory. It would not have made any difference as far as the low energy observations are concerned. It, however, does not logically imply that the UV theory is average. UV theory is average. If you want to figure out that the UV theory is average, you need to do a proper UV computation to establish that fact. And that's all I want to say about averaging for now. So let's describe a new little statistical approach to try to set up some sort of general section of the standard statistical physics approach where we have all this ignorance included. So, how do we normally do statistical physics? So, if you do quantum statistical physics, then one way to formulate what you're doing is you ask for a density matrix rho, which maximizes the entropy given some constraints. So, if we add a Lagrange multiplier, and for example, we insist that the expectation We insist that the expectation value of the Hamiltonian is a given energy, which would be our low energy measurement. Then, in statistical physics, we could say, well, all that we know is the expectation value of the density matrix. I do not know what the density matrix is. I just don't. But the standard rules of statistical physics tell me that what I should be looking for is the thing with the largest amount of uncertainty that's compatible with my low energy measurement. And if you simply solve this optimization problem, this has an extremum as the, which simply looks like this. Where beta is a function of E and it's precisely such that the expectation value of the energy is in fact E. So beta here is a function of E. So that's the simplest way to get the standard Noltzmann distribution from some From some sort of statistical thinking. But this gives us a specific density matrix. And density matrices encode a certain probabilistic feature and a certain level of ignorance, but this is not nearly close enough to the type of ignorance that I'm interested in here. So the proposal is to not look for density matrices that extramize some statistical quantities. Some statistical quantity. The idea is to look for a measure on the space of density matrices. A probability distribution on the space of density matrices that extermises a suitable functional. And what's that functional? It should map. It should maximize as the proposal the following quantity So, this is very similar to what I had before. The last two terms here are the same. But now I have to measure rho. And there's another piece minus log Î¼ of rho. These two things here together, this is like the classical Shannon entropy associated to the measure mu. Because mu is not a quantum thing, it's a classical probability distribution, and if we want to associate the notion. Distribution, and if we want to associate the notion of entropy to a classical probability distribution, we should use the classical Shannon information. This here is the standard entropy row, but now it's weighted with this probability distribution mu. And similarly, here's a Lagrange multiplier, which tells us that the expectation value of the entergen should be E, but in the sense it's averaged over this space of density matrix. So that's basically the proposal. Now, I will try to argue that with this rather minimalistic set of assumptions, you get a lot of minus and you reproduce many things that we thought should be true, but they simply follow from this sort of straightforward between quotation marks and statistical physics approach to the drawing. Any questions? So, if we take this thing and we vary new of row to figure out when this is maximized, it's just a simple variational problem. We find that this probability distribution looks like some normalization factor. So, this is the solution to the problem, where the temperature is again such that you reproduce the right energy. So, the temperature will always implicitly be a function of the energy. You just have to fix it so that the expectations are comes out. Okay. And a different, simple way to write this is with a slightly way to write it is with a slightly different normalization constant as this n prime times e to the minus the relative entropy of rho and the thermal state are we able to ask questions so this is just completely straightforward right and don't even have to do it i hope you're just very new and basically what you get is addition fannies and that directly spits out as well using Using the fact that obviously this is a normalized measure. If you integrate it over all rows, you should get one because probability should add up to one, just a classical probability distribution. Now, from this, we see two things. First of all, we see that this thing is peaked precisely at the thermal state. Because relative entropy vanishes if and only if the two density matrix are equal to each other. He's asking if you can comment on how this relates to the microphone sample. Not directly. I don't have a very good answer to that question. I think it there might be maybe a way to engineer something that's more similar, but this is very much canonical thinking. So maybe there's a different approach that is more micro-canonical in nature. Nature, but some features of the microcanonical ensemble will be built into this story as well. This is very much canonical thinking at this point. And I don't have a better answer right now to the question. So this is some probability distribution. It speaks as the thermal state, but it has a width. And in order to find the width, we need to expand this thing around the thermal state. With some perturbation eta. And then the relative entropy doesn't have a linear term, it starts off quadratically. And it's a nice little computation. The distinguishing the leading quadratic piece, the distinct is something like this. It's some quadratic Gaussian distribution, and the metric that appears I use energy eigenstates as my basis. So this is the metric that appears in this exponent. If you want, this is the quantum information metric, negative thermal state. Because if you expand relative, Because if you expand relative entropy to second order, it's this thing that we call the quantum information metric. And that stakes this particular form. Now, if you look at this and this Gij, if the energies are close to each other, the energies are relatively close, and this is roughly constant. Because if it's a bit like a derivative, like if Ei is EJ plus epsilon, then this looks a little bit like the derivative of the exponent or one of the derivative. So it's approximately constant, and this approximate constant is given by C beta E to the beta average energy. So E bar, here is the average energy. And using the standard relation between the free energy and the entropy, and so on, this happens to be equal to E to B S E bar. That shows that the width or the average amplitude of the anons is e to the minus s e bar over 2. And it might be a number that's familiar because this is precisely for. That's familiar because this is precisely, for example, the same thing that appears in ETH. In ETH, it has a fluctuating piece, and its amplitude is precisely given by the same entropic factor. So that comes out automatically without putting it in or anything, just comes out automatic from the analog. What's also interesting is that Gij is approximately independent on I and J if the energies are close to each other. So Gij is approximately constant. So Gij is approximately constant in a window where the inverse temperature times the energy difference is much smaller than one. And if it's approximately constant, it's approximately it doesn't do much, and that automatically gives us approximate humanitarian value. This unitary invariance, it simply reshuffles these indices a little bit. But if g is constant, it's really unitary invariant. So the second thing that's nice that we would like to get is this approximate unitary invariance in narrow energy bands. And it also comes out automatically from the computation with this minimal set of ingredients. Minimal set of ingredients. So, those are two very nice and appealing features of this simple proposal that I gave. It's also interesting that the eta IJ, they spoil factorization because the Because of these etas, if you compute the expectation value of rho tensor rho, we can now compute the expectation, we have some measure, we can compute expectation values of any functional of rho, which we could call rho tensor row average. It will no longer be equal to rho average tensor row average. Because if you expand this row as the thermal state, which is the expectation value, plus a small correction, those small corrections will be. The small corrections will be correlated among these two rows. And again, using these equations here, we can be a bit more explicit what it looks like. And we find that rho ij cancel rho k L average. So I now use some matrix elements and again use the energy eigenstate basis. This is equal to grow value IGA. This is roughly the structure that you get just from this leading or the quadratic perturbation theory data. Again, for people who know VTH. Again, for people who know ETH, this is very reminiscent of the structure that you get in ETH. You can also use it to compute example the product of the one-point functions, but then statistically speaking in this particular theory, and one gets that this is equal to the thermal expectations value of product and again. And again, there is a dimensionally suppressed something that schematically looks like this, where this is traced in some suitable micro-canonical energy window. So, in that sense, you do see some micro-canonical energy window appear. You could say, well, this is the two disconnected one-point functions, and this is something that connects. Functions and this is something that connects the two, so this is more of speaking like a wormholter. If you want to put that label on that, you need to come up with some gravitational computation for the computation, which, and I'm just not doing that for now. But you could say, well, this has the flavor of a hormonal. I think Jamie and Franks, when they did their unitary averaging, they saw a very similar structure appear just from unitary averages, but this also includes spectral correlation. But this also includes spectral correlations and not just unitary averages. And we can write a more precise statement here using this precise metric, but it's not inciseful and they won't do it. So we cannot exactly compute what these kinds of things are. We cannot do the exact computation because we have to do a very complicated integral of density matrices. However, you can prove that if the spectrum of the theory, such as this K of these, if the spectrum of the theory is sufficiently chaotic and doesn't have Something chaotic and doesn't have all kinds of accidental relations in it that the structure of this expectation value, the exact expectation value, must have the structure of a sum of the permutation group of some function labeled by sigma of the Hamiltonians of the K subsystems times the swap operator. Times the swap operator. So this is the thing where you take a k vectors in the Hilbert space and you apply permutations to those k vectors in the Hilbert space. The simplest example is the swap operator where you just flip two states in the tensor product of two Hilbert states in the Hilbert space. So this is something you can prove. And this is, for example, somewhat reminiscent of a structure that appeared in a paper that Holman Yu wrote. Appeared in a paper that Holman Yu wrote, I think, sometime last year. With forget something else, where also there was a different argument why this kind of permutation organization of computations came out, but here it just follows from the computation without any particular effort or further assumptions, except that the spectrum needs to be suitably chaotic. And cool. So I'm even slower than I thought I would be. So the last few. So the last few minutes, it's really tough to do 30-minute blackboard time. Is what's the connection to ETH? So we call that ETH looks like this an ETH is not a theorem. It's a hypothesis that is consistent with low energy observations, but does not imply new low energy observations. It's just consistent with low energy observations. And it is something that in principle can be checked numerically in any given system. But it's not a theorem or something, it's just a hypothesis. Hypothesis that looks like this. So the statement is that you have a low energy operator and you compute a matrix element between two high energy states: there's a diagonal piece and a random piece, where the random piece can be thought of as being a Gaussian variable with this statistics. And this pre-factor e to the minus s e over 2 is the same thing that we saw before. Now the main idea is that Is that this thing here implements all the low-energy force grading and everything that you want to do? So, how can we connect this idea to ETH without introducing by hand all kinds of other statistical ingredients and features? Well, here the proposal is as follows: suppose we compute Supposed to compute a correlator, a finite temperature correlator. And this is an approximate computation. So this is low energy. That's not the exact UV, exact precise answer. It's some approximate low energy computation. This would then be given. By something that Ruffy looks like this, where the A's and the B's are chosen so that we do an actual finite temperature computation, and also, you know, you need some Euclidean time propagation to go from one to the other. So that's all in the row A and rho B and stuff. And it is that this low energy computation comes out because of this averaging here. So if you did not do this, this would just be the exact computation. Eth on the ETH, on the other hand, says that the same quantity is given by some other statistical thing, where instead of doing something with rho, we do something with the O's. Where this is now some sort of average, some statistical average, but now the operators are themselves. Efforts, but now the operators of themselves are becoming statistical objects. That's what ETA says. ETA says: take these expressions, and if you want to compute any finite temperature correlator, this correlator here, here, the O's are statistical objects, and you stick in the precise finite temperature propagator in between, not some approximate thing. Now, so this is, if you want, the relation between each. Is if you want the relation between ETH and this proposal here. So, this is in principle computable, and one can try to connect this to this. And that is the proposed connection to ETH. And if you want, it's a proposed refinement of what ETH is supposed to be. And we can morally roughly see how it's going to work. Because if here we this row here, you can write it as some u row diagonal. Diagonal U dagon and this measure of rho will contain a measure of unitaries and the measure of this spectrum lambda. But let's for now ignore this integral of the spectrum lambda and only look at the unitary components here. So this thing involves a unitary integral. And if we insert this form of row here, you get Insert this form of rho here, you get some unitary integral. But this thing here will translate into rho beta to be a u dagger OU etc. If I replace those growth by this and I assume for the time being that the spectrum is thermal, again, there's some spectral integral to be done as well. Integral to be done as well. And now, if you have this structure and you now simply do the unitary integrals, say, assuming a simple approximate unitary invariance, you automatically get all the index contractions and all the statistics required for ETH. So, this is compatible with ETH. Exactly how you do the spectral averages is not so clear. I think they are inconsequential. But if you want, this might be a more precise way to do VTH. And it now all starts from. And it now all starts from a very precise, well-defined, statistical physics-inspired starting point. So, maybe this is how we should think about ETH. And the last, can I have two minutes? Sure. Go ahead. The last thing I wanted to say is that you can do this with ETH, but you can apply the same logic also, for example, if you're interested. Same logic, also, for example, if you're interested in statistics of OG coefficients. If you compute, for example, the genus 2 partition function, it has, of a 2D CFD, it has a statistical interpretation in terms of statistics of open ecosystem. In terms of statistics of OPE coefficients, where O is some sort of operator IGAKLMN IGAKCLNN star IJAK LMN and now we can apply the same logic we can simply define the statistics of this O here. simply define the statistics of this all here by equating this to the sorry to the actual precise OPE coefficients O but now F is against the row so very similar to what I did for ETAs again how you define precisely in this language statistics OPE coefficients by equating Statistics of B coefficients by equating this expression and this expression to each other. And this computation we know how to do and how to extract the statistics. And we have now also, that's in one of these projects, also shown that if you take this point of view, that you can do this computation in different ways. But if you do this again using unitary averaging, you reproduce the statistics, at least the magnitude of the statistics. And we also did. And we also did a random matrix theory type computation for this using this Altransoner sigma model thinking, and that also would lead to a fascinating of this. So, again, here, these two things seem to agree with each other. Cool. And just maybe I'll. And similarly, we can now study how horror holes appear in this framework. And that simply has to do with, again, a lack of vectorization of the rows in this measure, and so on. Measure and so on. And to conclude, I think it's fun to explore this idea further. As you add more low energy information, you can refine new overall. I only input energy now, nothing else. But clearly, you can input other conserved charges, you can input correlation functions, flow energy operators, and so on. And the more you input, So on. And the more you input, the more narrow this new row is probably going to be. But you would need to input pretty much everything that you know in order to squeeze it onto a delta function. And that seems unlikely. And that's why no matter what you do, it will remain some probability distribution on the space of density matrices. I find a very interesting question, and I don't have good intuition for it, is that this thing might have a structure that looks like this. I'd have a structure that looks like this, but it's a bit like a matrix model. What I had now was just inputting energy gave us something that only had a single trace in the exponent. But generally speaking, if you input more and more information, you might also need double traces and so here. And the question is whether you really need those double traces, what are they good for, and what the scaling of this epsilon is supposed to be. In other words, is this a bit like a matrix model where you just have single traces, blah, blah, blah. Or you just have single traces, blah, blah, blah. Or should we also include these terms here? And whether or not these terms are there is closely related to the question of whether or not there is new information in wormholes or whether everything that we know about wormholes is contained in one-sided information. And it's just all some self-consistent math. If there's new information in wormholes, you need these terms. If everything is extractable from one-sided information, then maybe you don't. Maybe you don't. And I'll stop here because I'm way over time already. Oh, that sounds great. Thank you. So you may have seen in the chat that in order to allow questions from the in-person participants, the overhead license team committee, is there any chance I could try? Could it be nice to have virtual participants be able to ask questions in the middle of talks? You might have tried to do that. Do you mind if I tried? You guys should be able, you guys online should be able to ask questions now. If somebody could speak. But do you mind if I try to see if there's feedback if I run the audio out of my computer in the middle of the talk while the overhead mics are speaking? Yeah, you can try. It'll happen. But I've muted the ceiling mics now, so it won't happen. Well, I meant for future in-person talks, if audience members want to ask during the talk. You can try. You'll get feedback. If you wear earphones, that's. Earphones. That's if anyone has earbuds or something, you could monitor the in-person chat. You know, and during online talks, obviously, you'll be able to. Well, yeah, yeah. But yeah, we're waiting for a solution. Okay. But okay. So I guess yeah, we can open up to questions both from in-person folks and virtual participants. So raise your hands. Yeah, can I ask a question? Yeah, go ahead. Yeah, so you motivated the initial idea saying that you would average over density matrices, right? Yeah. In the expression that I think is in the right part of the blackboard, you were calculating the two-point function. If I understood correctly, those A and B's are the difference in time minus involved time. And so it looks, I mean, I would have expected if you average over only over rho, that there would be only one factor of rho. But there you have rho to some. Of rho, but there you have rho to some powers, which suggests to me that you are thinking of rho as the exponential of the actual Hamiltonian or some other Hamiltonian. Is that what you're doing? Yeah, that's what I'm doing here. So you can ask what the right way to model this computation is in this framework. The idea was that row is also modeling time evolution. Ah, okay. Okay, okay. And I think Okay. Um and I think um because if we don't do that and we stick in a fixed Hamiltonian then I don't think this works. Uh because what I very briefly sketched is if you do this unitary averaging that it seems to be required in all these intermediate channels to get the structure out to come out okay. And if you don't put the rho here but just E minus I H T or something, then I don't think this relation works anymore the way I described it. So I think it's necessary, but I don't. So, I think it's necessary, but I don't have a full understanding of the degrees of freedom that you have in this equation. Okay, thank you. Questions? Can you write again the thing? You wrote it kind of fast, your rule for picking the state row. What exactly are you maximizing? Can you explain more the motivation for why you're maximizing that particular thing? Maximizing that particular thing? Sure. The thing that I wrote down is that you're going to be able to do that. Something like this. That's the one you're talking. That's the one you're talking about, right? Yeah, that's right. Yeah, yeah. Can you remind us what is mu of rho also? Yeah, so maybe a simple way to think about the logic, especially of the first two terms, because I guess all the Grange multiplier piece is self-explanatory. For example, if you compute entanglement entropy and gauge theory, then if you then break a spatial region into two parts, you have all these legs sticking. Into two parts, you have all these legs sticking through the boundary, and then the structure of the Hilbert space one gets is not just a tensor product, but typically it's the sum of tensor products. And if you want to do the right computation of entanglement entropy and gauge theories, then what you do is you write the full state. is you write the full state as a sum of classical probabilities times reduced density or mixed states that live on each of the summons in this thing. Then I think it was shown that the right thing to do is to sum a classical piece plus a sum over I where you weigh all the entire Over I, when you weigh all the entanglement entropies of these robi's with Pi. So this is how you compute entanglement entropy in the case theory, in case where you don't have a precise factorization of the Hilbert space. And if you want, this is simply a continuum version of this type of information theoretic expression. So whenever we have this hybrid combination of classical and quantum information, I think the logic is to use the shenanigans. I think the logic is to use the Shannon entropy for the sort of classical probabilities and the quantum information, the quantum entropy for the quantum bit, weighted with the classical probabilities. And this is just a continuum version of that statement. But there's no gauge symmetry here, or is there? I mean, yeah, I understand gauge symmetry. That's a great question. I had thoughts about that. I don't have something very insightful to say. Maybe somehow splitting the thing between low and high. How splitting the thing between low and high energy is the cousin of this splitting, but I don't have a very good answer. That's a great question. I'd love to know the answer to that. Okay, we can take maybe one more very quick question. Okay, if not, let's thank y'all again for