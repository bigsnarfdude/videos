Organizers for getting us together in this beautiful place and putting together a nice program. My third time in BAF, so I'm very happy to be back. But it's also a little bit sad because last time it was here was eight years ago at a similar workshop between chemists, physicists, and mathematicians, and it was also co-organized by George Hageron. So it's really exciting because he passed away last year. He can't be here. I have done a lot of work recently on Hagen-based packets. Work recently on Hagen wave packets, and I would so much like to tell him about it. But I guess that's life. So, today I will talk about not about Hagen wave packets, but something simpler, Gaussian wave packet dynamics, but I hope from a little bit refreshing perspective, from the perspective of non-linear energy equation. And so, this is mostly described in this theoretical paper in General Chemical Physics, but I will show you. But I will show you examples of this infinite family of Gaussian wave packet methods and applications to electronic spectroscopy. And so there has been work in progress in my group and various people have contributed. So I would like to thank in particular Tomislav, Roya, Eriks, Solan, Miroslav, and Marius. So, you know, I would like to thank Graham for a nice introduction to Gaussian Wave Packet Mass. I have also done a little bit of work. I have also done a little bit of work on a Gaussian basis method with many trajectories, coupled or non-coupled. But we wanted to have something as cheap as possible so that you can combine it with, let's say, more expensive electronic structure methods. And then I went back all the way to single trajectory methods. And it turns out that they are very crude, of course, much cruder than the methods that grab this grab. But they are often better than what is a standard. Better than what is a standard in electronic structure codes for computing spectra. So, even though some of these methods are very old, there is a way to improve state-of-the-art in standard codes. And so, we applied it to electronic spectra for different molecules, and I found, let's say, two or three methods. We gradually tried to improve them a little bit to include a little bit of tunneling, non-adiabatic effects, generalized from Gaussians to more general weight packets. General wave packets. And then I realized actually all of these can be described in a more common framework. And this is what I call this generalized Gaussian wave packet dynamics that I will discuss today. So I hope, even though it sounds very simple, a single Gaussian wave packet propagated along one trajectory, it's very rich, surprisingly rich. Okay, so first let me review quickly the time-dependent strategy equation before I talk about nonlinear one. Before I talk about nonlinear one, so this is the standard uh form ih bar of psi dot is h times psi, where h is the Hamiltonian evaporator. Today I will always assume that it's separable into kinetic energy and potential energy that depend either on momentum or position. And the kinetic energy is quadratic form, and position and momenta are d-dimensional vectors. Okay, so I want to point out, I don't think I need to do it for mathematicians, but you know, I say this is. Mathematicians, but I say this is a linear Schrodinger equation, not because the potential is linear, even the kinetic energy is a quadratic form, but because the Hamiltonian does not depend on the state of the system. It turns out, and this is again something kind of obvious, but for me it was also a big revelation that a lot of very famous and useful approximations to the time-dependent reduction equation are the exact solution. Exact solution of a certain non-linear Schlencher equation, which is of this form, and the only difference is that there is an effective Hamiltonian which depends on the state of the system. So one could generalize it even more, that you could have arbitrary function of the state on the right-hand side, but it's nice to keep it slightly less general, a so-called quasi-linear form, because you preserve many structural properties of the linear strategy. So on one hand, the mapping from psi to H. The mapping from psi to H effective psi is nonlinear, but if you fix your effective Hamiltonian, then for any phi, this mapping is linear for all psi. In addition, I will assume that the expectation value of this effective Hamiltonian in arbitrary possible different state, phi, is real for all phi and phi. So this is a generalization of Hermitian property, and it also is very useful for this family. Very useful for this family. So, how would you generate interesting non-inertial energy equations? So, first of all, there's, I guess, most famous one, Gross-Pitaevsky for those Einstein condensates. But in general, what Gran was talking about in the previous talk, it's a very general this is this Dirac-Frankel time-dependent Laurier principle. When you apply it to a given Ansatz for a wave function, you will always get a non-linear Schranger equation. A non-linear Schranger equation. Sometimes it may reduce to a linear one, but in general it will be non-linear, and this is the reason for the different numerical issues in the solutions that Graham was discussing. Okay, so examples are time-dependent hard tree or hard tree font method or multi-configurational time-dependent hard tree or hard tree font method. So instead of a basis set and expansion, I will use a single Gaussian packet on that. But I also want to say I will not use always the directory. Want to say, I will not use always the Dirac line cooperation principle, it's more general than that. Okay, so what is a Gaussian wave packet? Again, we heard a lot about it from Graham. I will talk about a single one, and I will use a slightly different parametrization. It took me many years to reach this one. But anyway, so there is a pre-factor I over H bar which helps to control the semi-classic limit with this quadratic form, and the parameters are. And the parameters are position, momentum. Actually, there are expectation values in these observables in the Gaussian. There is this complex, symmetric Bitz matrix with positive definite imaginary part, and the complex scale which controls normalization and phase. And I want to point out this is, you know, to make the formulas a little bit simpler, but it's always shifted from the center of the Gaussian. So Qt is over here in the middle. QT is over here in the middle. Alright, so to give the main message, quickly at the beginning, I want to say that I wrote this proposition. I think I kind of prove it, probably it would not be completely regressed from a mathematical point of view, but I think it should be okay. So the assumption that you have function V0, V1, V2, which are D-dimensional real scalar, vector, and symmetric matrix. Vector and symmetric matrix, and they are functions of the state psi. So then the Gaussian wave packet solves the nonlinear time-dependent energy equation with an effective quadratic potential, which can be written in this form, where x is again the shifted position, if and only if the parameters of the Gaussian satisfy the following system of ordinary differential equations. And so these are these equations. And so what is interesting, it So, what is interesting, this covers, you know, let's say global harmonic approximation, variation Gaussian approximation, Heller-Star Gaussian approximation, but many others. So I will mention the example as I go on, and I will also mention several new ones. So, first of all, position and momenta, they look a lot like Hamilton's equations of motion, but they're not quite, because this is V1, it's a general function of psi. This is Riccati equation for the matrix. Again, it is quite general. And finally, It is quite general. And finally, gamma is a generalization of classical action. Okay, so now let me give the first example, this global harmonic approximation. So I didn't write it in the standard form of my effective potential because it would hide that this is actually a linear operator. So the potential is a quadratic function of position. You can write it in that uh general form, but this is trivial because it reduces to linear strange equation. Okay. Linear franchise equation. So let's look at a more interesting example, which is a local harmonic approximation. Igen Graham mentioned it. So he was using it for the matrix elements of the Hamiltonian between Gaussian basis functions, but I have just one Gaussian. So I can write my effective potential using a Taylor expansion to the second order of the potential about the center of the wave market. So you can see in this movie. In this movie, the dashed curve is the quadratic effective potential, and you see it has the exact energy gradient and even curvature of the potential. The black one is the wave packet, and you know, original potential is the blue one. It's unharmonic. So, equations of motion are very simple. These are really Hamilton's equations of motion, and this is the standard Ricardus equation. So, you need a potential energy gradient and Scm. And these are the three coefficients. And these are the three coefficients. So, what is that? Well, this is nothing else but a very old, now almost 50 years old, Heather's thought-Gaussian approximation, where he used, he wanted to find a solution of a Schradinger equation which would be quite practical for many-dimensional systems, which would behave nicely at the semi-classical limit. And so, you assume that your state is a Gaussian and it can stretch. And it can stretch and rotate in a phase plate, that's why it's called thought, because the wave packet can melt, right? In contrast to the frozen Gaussians. And what is nice, and Hell already observed that, that the only approximation is a local harmonic approximation of the potential. So of course, this is exact for global harmonic systems, but of course it doesn't capture back in splitting or tunneling because it uses classical trajectories and because it uses only single calcium. Single health thing. So we were working with different many trajectory methods and there was a issue of convergence. It's hard to, you know, you can get reasonable results with hundred trajectories, but if you want to really completely eliminate numerical errors, you might need thousands or millions. So we looked at this about 10 years ago and checked if actually it could be useful for computing spectra. Okay, because at the time that Heller wrote this, it was very expensive to compute Hessians with electronic structure. So now it's possible. So let me, especially for mathematicians, let me mention what I mean by global and local harmonic approximations for electronic spectroscopy. So Brahman was also talking about multiple states. Here I don't use the non-adiabatic couplings, but let's say you absorb a photon of visible light. Of visible light, you get to the excited state, and in the global harmonic approximation, you approximate the excited state on which the wave packet now has to move. So, two natural choices are the green one, which is vertical harmonic model, or the adequate body harmonic, which is expanded about the minimum of the excited state potential. The local harmonic, again, the potential is time-dependent, effectively. It's implicitly independent of time, but it depends on the state. Depends on the state. So it is moving with the wavepack. So let's see how it works. So this is our most recent example where we compute the isotope effects in the spectra of ammonia. So again, for mathematicians, I want to say that the potential energy surface for these isotope policies of ammonia is exactly the same. Or almost exactly the same, we all know. But what is different are the masses of the different atoms. Different are the masses of the different atoms. You replace hydrogen with deuterium, and that of course changes the dynamics. And because it changes the dynamics, it will change the spectra. So these are the experimental spectra. And you see that you change the peak spacing, you change the envelope, and other things. If you use the global harmonic, adiabatic harmonic approximation, you get completely carbon-H spectra. If you use the vertical harmonic, you get actually a reasonable envelope, but drop the Envelope, but the rock peak spacing. And if you use this local harmonic, which I call here on the fly Abinitio Fargo approximation, it actually gives pretty nice spectra. And it can capture the difference between the different isotopologs. So you might think that this is a simple molecule, only four atoms, right? But the electronic structure is expensive. And actually, this is quite floppy. So it's surprising that something which is local harmonic works. Which is local harmonic works. So, this was only what we did as our later application. The original one, 10 years ago, was on oligotiophenes. So, you know, again I show this molecule where the global harmonic is very far from experiment, but the local harmonic is very nice, you know, except for maybe these artificial peaks over here, and you can push it easily. And then you can push it easily to 105 vibrational degrees of freedom. So, this is direct dynamics, just to show you that it's possible to refer quite existence to 105 degrees of freedom. This we could do quite easily 10 years ago, now it would be trivial. By the way, because of the discussion that was after a previous talk, there was a question, is there a general procedure how to design the tree for multi- The tree for multinay and CTDH. So, in this original paper, unfortunately, we mixed everything together: the abinitial idea, the revival of sarcastic approximation, but also procedure how to disentangle the spectra. So, I will not talk about that, but I just want to say that if your system is close to the semi-classical limit, the semi-classical trajectory with the Riccati equation gives you information about the coupling between the moles. And so, And so we used it to design a tree and let's say disentangle this spectrum to spectra corresponding to a few degrees of freedom, which were almost perfect. So it's kind of automatic generator of reduced dimensional models. It's not completely rigorous, it is just like starting point into semi-classical limit. So now let me go back to the mathematical properties of the Gaussian wave packet dynamics. Of the Gaussian based packet dynamics. And first, let's look at the linear strange equation. So, I am happy that in the audience are many people who have contributed to structural preserving methods for differential equations. And in fact, for me, implementing the algorithms for other methods also made me think about how to preserve the structure even for approximations, not only for integrators, but for approximations. So, first, the linear energy equation. Linear energy equation is time reversible. So, time evolution is reversible and conserves the norm, the energy, the scalar product between two different states, and also symplectic structure of the Schradinger equation, which can be defined as the imaginary part of the scalar product. Now, let's move to the nonlinear Schr√∂dinger equation. So, first of all, you know, tough luck, but scalar product is not conserved, which is kind of obvious, right? I think it's kind of standard thing in a linear algebra. This kind of standard thing in a linear algebra that what is it? If a mapping, general mapping concerts scalar product, it must be linear. So here, if it's non-linear, it cannot concert. So I show it here. You take the derivative of the scalar product, and you end up with the difference of the two effective Hamiltonians associated with the two states. And so, of course, you cannot have both of them. If they are, in general, if they are different, it's not conserved. It's not conserved. There is a actually useful lemma, it's time dependence of the observable. So it looks very much like what you see in the textbooks, where the observable is a linear operator. You have a commutator contribution, and you have a partial derivative with respect to time of the observable. But here, the observable is not even dependent on time. It just depends on the state. But you still need the total derivative with respect to time that. That. So now you can squeeze as much as possible out of it. So, for example, if your operator is identity, this shows you that the norm squared is conserved. If your operator is Q or P, you substitute it here and you obtain a generalization of the LMPS theorem. Okay, so it works completely generally. For the nonlinear strategy equation, you can also define effective energy. It's the expectation value of the It's the expectation value of the effective Hamiltonian instead of the exact Hamiltonian. And again, so you can use the lemma and obtain time dependence of the energy, which uses only the commutator term. But if you look at the effective energy, the commutator term is obviously zero, so we only have the other term, the total time derivative. And you can further simplify for separable Hamiltonian. So for Gaussian behavior. So, for Gaussian wave packets, you can further simplify it and you can express the time dependence of the energy and the effective energy in terms of the coefficients of the effective potential. And I can advertise for experts, so in the variation in Gaussian wave packet dynamics, these parentheses terms exactly cancel out, and so you get conservation of energy, which of course. And so you get conservation of energy, which of course you must have, because the time-dependent variational principle conserves energy for arbitrary answers. Okay, so the last thing, which is maybe the last property, which is maybe the most complicated one, is the symplectic structure on the manifold of Gaussian-based packets. So this was studied quite a lot by Lubig and co-workers, and by Tomoki Oksaba and his co-workers. And so if you write down Workers, and so if you write down the symplectic structure for Gaussian wave packets in all variables, it will be quite complicated. But if you use symplectic reduction based on the norm conservation and you express it in terms of only QP and the width matrix, it becomes relatively simple and you can study conservation of this symplectic structure, which is easier. So I just say it beforehand, the whole family Gaussian. The whole family Gaussian-Defmack dynamics does not in general conserve the symplectic structure, but many interesting methods do. So, now that I discussed the projects of the Gaussian wave packing dynamics, it's interesting to discuss the integrators. So, you would like to preserve as many as possible of these properties. And so, it's very simple. We use a symmetric composition of the strength splitting of the effective potential into kinetic and effective part. An effective part, and all we need to do, as usual, is exactly solve the kinetic and potential propagation. So, what is nice about this approach, it will be automatically time-reversible and non-conserving for all the methods. It will be symplectic as long as the original approximation is symplectic, and it unfortunately will be only approximately energy-conserving if the original method conserves energy. So, how does it work? So, let's say this is the kinetic part of the Hamiltonian. These are the equations of motion where I replace the Hamiltonian with just the kinetic energy. So, this is in fact linear Schrdinger equation. It's a free particle motion. So, it has a simple analytical solution. Position and momentum are completely trivial. You have to do a little bit of work for the width matrix and the phase. And the phase, but you can do it. For the potential propagation, you cannot solve it analytically in general. You have to make an additional assumption, which surprisingly works pretty much for all the methods I have seen in the literature, and you assume that the coefficients of the effective potential, V0, V1, and 2, depend only on the position and the imaginary part of the Witz matrix. If you do that, then the position, because the position is constant, Because the position is constant and the imaginary part is constant, also the coefficients v1, v2, and v0 will be constant, and we can solve these equations even more trivially than the kinetic part. It's simple. Just integral. So these are the integrators, which work for all the methods which I described and which I will describe now. So let me go back to the original Heller's method, the local harmonic approximation. Harmonic approximation. So, as I proved, it's automatically done-reversible and non-conserving, which is very nice, but unfortunately, it doesn't conserve the effective energy and even not the symplectic structure. So that was one of the big problems of the original method. It's very useful, it's practical, but it has this issue. And in fact, it led to Moki Okseba to come up with symplectic versions, which I will discuss. With symplectic versions, which I will discuss in a moment. One other thing is that it's a little bit expensive because it requires the Hessian, which is, strictly speaking, you should evaluate it along the trajectory. One can interpolate it every few steps if the potential is smooth, which is a pragmatic thing that everybody does. But what we did was something else. Imagine you have a thousand times. First, we interpolated every two steps, then every four steps, every eight steps. Every four steps, every eight steps, and it was becoming worse and worse. And suddenly, at 16 steps, it was pretty bad. But then we decided not to interpolate it at all and just take a single hash n. And the result was quite excellent. So we call this single Hessian approximation. Originally, it was very pragmatic, but now we know much more about it. So the idea is that you keep exact potential energy and gradient, but the Hessian is kept con the curvature is kept constant. The curvature is kept constant. So you can see it in this middle picture where I have where we keep the curvature basically constant. So it's somewhere between the global harmonic and the local harmonic result. So as I said, originally we just wanted to speed up calculation, so now this becomes completely trivial. Because if you don't have to evaluate Hessians at every step, it's easy. But it turns out that. But it turns out it conserves sympathetic structure and even the effective energy. So this is surprising because it's such a simple method. The proof is very easy. I will not go through it. It follows from the general expressions which I mentioned earlier. But let me show you an example of ammonia spectra. So this was the local harmonic, the expensive one, which was pretty good. If you use the global harmonic, it's terrible. But if you use the same But if you use the same STM as you use in the global harmonic, in the single Hessian approximation, it turns out to be very good, and you could say even slightly better than the local harmonic one. So you cannot expect it in general because it's in principle less accurate, but because of the better geometric properties, perhaps it's more stable and has other nice behavior. Okay, so. Okay, so that was, that I think is a very practical method for initial calculations. Now, let me mention the most accurate method. So, this is the special case of what Grah was speaking about for a single Gaussian. So it's variation Gaussian approximation. Again, you can describe it with an effective potential, but now the coefficients are different from the Taylor expansion. You need expectation values of the gradient, Hassan, and the potential. Gradient has Cm in the potential. So you see, for example, that the trajectory doesn't move exactly along the original potential, the curvature is not exactly the same. But this is the most accurate of all the methods. But it's expensive because it requires the expectation values. So just to show you that this can capture tunneling, also Brand was taking advantage of it for its feeding and convergence and so on. But this is kind of an extreme example of double-well potential. The variation of one can go through the barrier. One can go through the barrier, whereas the local harmonic one just gets stuck in the minimum. I was very excited about this, but actually, that we could describe systems with tunneling, but it's not right. Most of the time, when you have tunneling, there is a splitting. Part of the wave packet tunnels, part doesn't. And often the part which tunnels is smaller than the one which doesn't tunnel. So in the end, actually, this might be more accurate than this one. But just the fact that you can you can actually detect tunneling. You can actually detect tunneling, it is useful. So, how to make it practical? And this was exactly the method discussed by Tomoke Osaba, who sympatheticized the local harmonic approximation. Actually, originally, these equations were also obtained by Patanerk and Skieve. And so, the idea is to apply variational principle to the local cubic approximation of the potential. So, you get the same B0. Same V0 and V2 coefficient, but here you have non-classical force. So you can tunnel. It is much cheaper than the fully variational method, but it requires the third derivative of the potential. I wanted to check. I think I started a little bit late, but like two minutes more, something happened. Okay, so let me show you a few numerical tasks. A few numerical tests. So I described completely general properties for all the Gaussian wave packet methods, but I will show it to you only on the example of this local cubic variation approximation. And this was done, the calculations were done by Raymond Adassier Dynamic Michael. So first of all, the energy. We were discussing conservation of exact energy and the effective energy. So let's maybe first look at the exact energy. So the red one is the variational method. The red one is the variational method, which of course has to conserve it. The green one is the local cubic. It looks conserved, but if you expand it and zoom in, it is not conserved exactly. It shouldn't be because it's not fully variational. But the local harmonic one doesn't conserve the exact energy even more. So it is much worse. If you look at the effective energy, again, the local harmonic doesn't conserve it at all. The local qubit doesn't conserve it. The local cubic doesn't seem to conservate, and if you zoom in, it actually does conservate, although it is different from the fully variational effective energy, which is actually the exact energy. So, this is property of the approximations. Now, let's look at the integrators. So, we can compose symmetrically the strength splitting, and we can achieve arbitrary even order of convergence as expected with respect. As expected with respect to the time step. So it's quite easy to reach machine precision, although you probably don't need it for such a crude approximation. But you can. If you look at symplecticity, the symplectic structure, so the integrators are symplectic, so local cubic variation approximation is symplectic. So they conserve the symplectic structure as a function of time or as a function of time step completely to machine precision. Machine precision. If you use, for example, Runge Kuta for its order, even though the original method is symplectic, it quickly loses the symplecticity. And it only is symplectic if you use very small time step. And then maybe the most important slide, which are the geometric properties, like norm conservation of time reversibility. So again, all the integrators are exactly to machine precision, norm-conserving or time-reversible, but the rule. But the Runge Kuta is not. And if you look at the energy, because of the splitting, unfortunately, it's only approximately energy conserving. So the higher order integrators conserve the energy better. So that's one property that you use with these integrators. Okay, so I'm almost done. So now I would like to say to you something. To say to you, I described to you single-hashed approximation. Now, what would you do to improve over the local cubic one? The most natural answer is local quartic, right? Force-order tail expansion. It would be a disaster. First of all, nobody would be able to do it because it would be very expensive. And second of, which is surprising, it breaks all the geometric properties. It's not sympathetic, it doesn't conserve the effective energy. So it's a little bit like Helder's local harmonic proximum. So it's a little bit like a Hellenist local harmonic approximation. So instead, we came up with a single qualitative approximation where we expand the potential to third order plus you keep one force derivative. And this method can be again described by this quadratic effective potential. You need one force derivative, but that means that it's pretty much the same cost as the local cubic one, because typically the number of time steps is bigger than the number of degrees of freedom that you want to describe. So the extra cost is an extra. Describe. So the extra cost is negligible and it is exactly symplectic and it exactly conserves the effective energy. So I think this is more academic method still because even the third derivative on the fly is relatively expensive, but I would say this is a kind of a balance between fully variational and something which is still practical. So with that, I would like to conclude. So I would say the main message for me, I think, is that many I think that many approximate solutions of the Schweringer equation can be thought of as an exact solution of a non-linear Schwering equation. The Gaussian wave packet dynamic in particular solves a non-linear Schwering equation with the quadratic effective potential with state-dependent coefficients. And then I highlight these two methods. The single-Hessian Gaussian wave-packing dynamics is very efficient. It's really very practical. It's like classical molecular dynamics cost. Cost. It is on top of it, it's symplectic and conserves the effective energy. And as I said, a little bit more academic. If you want to go beyond local harmonic, I think this one would be a method of choice, even better than local cubic one, because I didn't mention it, but it avoids some instabilities, because local cubic will give you a potential like that. And it's quite easy to go there. Whereas, if you have a positive force derivative, you somehow stabilize it. And the last thing. It. And the last thing, and it has all the geometric properties. And the last thing, you know, I discussed only Sword-Gaussian methods, but in the original paper, I also talk about a family of Frosten-Gaussian methods. So once more, I would like to thank Tomislav Roya, Eriks, Helen, Miroslav, and Marius, and thank you for your attention.