Very nice location, and actually, also very nice conference and atmosphere in here. So, I'm happy to present this project. It's actually a project that started when I moved to Genova. So, actually, before being in Bologna, I've been a couple of years in Genoa and working with these. Let me see if it works. Works, yes, so it should work. Yes, working mainly with this team that is based both in Genova and in the University of Helsinki, where I was previously working. Actually, the main reference for this talk is going to be the one above there. And in the last part of my talk, I'm going to present something that is the extension of this work and that is currently working progress. And that is currently work in progress, but hopefully soon pre-print. Okay. So the idea of this project is that, of course, we are doing a bit of a combination of machine learning and inverse problems. So just to set up all the ideas, it's better to be everyone on the same page and understand what we are working with. Of course, this audience is very much expert of inverse product. Audience is very much an expert of inverse problems, so I'm going to be fast on something and slower on something else. Of course, you will notice that medical imaging is there as a possible application because, of course, we are considering linear inverse problems, but it's still quite a theoretical discussion. So a lot of imaging problems fall into the description of a linear inverse problem. And here we are not focusing on a specific one. So let's start essentially. So let's start. Essentially, the goal of this project can be summarized in one sentence. So to learn the optimal regularizer for inverse problems, specifically for linear inverse problems. So let's start describing it from the end. Of course, we are all very familiar with inverse problems. Linear ones, so we just consider a model like this, so where you have measurements coming from an operator applied to an unknown quantity. Or applied to an unknown quantity, possibly corrupted by noise, and you want to invert, so to go back from y to x. And the problem is usually that you have ill-pauseness. So let's not focus about non-uniqueness. Let's consider injective operators A. But typically, what we are going to see is what happens when the inverse of the operator is unbounded. So we are formulating everything in infinite dimensional spaces. In infinite-dimensional spaces, actually, in Hilbert spaces, for simplicity. But the idea is that it's natural to formulate inverse problems in an infinite-dimensional setup, so working with functions, even though of course in all applications you have discretization. But in a sense, if you are able to prove something that holds in infinite dimensional spaces, then you have some discretization invariance for your application. For your applications. And that could be something, of course, interesting, but it's, of course, very hard to treat some of the details in this infinite dimensional setup. So most of the technicalities of this talk are going to come from the infinite dimensional setup. And we are all very familiar with applications of linear inverse problems. I just wanted to mention that, of course, there are some, at least in a simplified In a simplified formulation, some medical imaging modalities that fall into this idea of inverting a linear operator. Okay, what is a regularizer for inverse products? Of course, again, something very well known. The idea is that since we have instability, it's not a good idea to apply directly the inverse of your operator to your measurements, even though in many cases, like in the Radon transform, the inverse is well known. Is well known because, of course, even small perturbations on your data can be amplified by applying the inverse. So, the idea is that instead of using the inverse, you approximate the inverse, you provide typically a non-linear approximation of your, I mean, a non-linear operator approximating your inverse. And one very common idea is to do so by means of variational regularization. So, you substitute the operator that takes the data and gives you back a kind. That takes the data and gives you back a candidate solution with an operator that takes your data and gives you back the solution of an optimization problem. So, in this optimization problem, what you do is to minimize a functional that combines essentially two parts, one that is the data fidelity part and one that is the regularization. So, data fidelity just means that you want the measurements associated to your candidate solution to be very close to the measurements that you have. And the other part is actually where you encode the prior information that you have. Encode the prior information that you have on your true solution. So that's usually the trickiest part. Whenever you study variational regularization on books, it's quite natural to first immediately ask yourself the question, how do I choose this regularization functional, of course? Because, I mean, also the first part is actually tricky. So how do I choose the data fidelity term is also an interesting question because, of course, it depends. An interesting question because, of course, it depends on the noise model that you have. But let's just focus on the other part. Because, of course, we have some theoretical requirements, so some properties that this functional must have in order to have a solution, possibly a unique solution, and you have to some good properties of regularization functions. That's okay. But also in practice, what you usually know is that you should try to encode some information about your solution. For example, let's consider the easiest example. Let's consider the easiest examples, like the T con of regularization. So, when you add this kind of functional here, you are trying to say that you want to have an X that somehow is able to reproduce your data and it doesn't have a large norm. So you are penalizing having large norm. Of course, this is a prior information you are putting it. In most of the cases, it's not what you look for. So, if you want a natural image, I mean, it's not important that it has a small. Not important that it has a small norm in general. Maybe you may want to encode some more advanced features, like for example, sparsity, as we saw in the talks, in many talks this morning. So meaning that just few components with respect to a specific representation of your signal are different from zero, or maybe you want to impose that few components of the gradient, and so you get total variation. But the idea is that. Variation. But the idea is that still, this is something that you decide. You pick one regularizer, one regularization function. So let's now move to the learning part. So of course, this is, I mean, I'm trying to present all together a quite large family of strategies that are now quite popular. So, in a sense, since I was already saying that in Saying that in designing your j functional, you need to incorporate some prior knowledge on your solution. It makes sense that this is a good point where the classical theory about regularization meets data-driven methods and like machine learning methods or in general, statistical learning methods. So you may want to encode this information using data. So the idea is that we can try to We can try to learn the best regularizer, the best function that approximates, in a sense, the inverse of your linear operator by choosing, by minimizing, I mean, by finding the best possible regularizer within a family of regularizers that are parametrized. I mean, suitably parametrized. What does it mean? It means that, in a sense, you replace Replace this expression of your variational regularizer with an expression of a regular family of regularizers that depends in some way on some parameters. And I'm going to give you details in the next slide, but let's by now consider that we have a family of functions depending on a parameter theta, a parameter that can be arbitrarily large or even living in infinite-dimensional space. So whenever you have this family of So, whenever you have this family of possible regularizers depending on a parameter, what does it mean to learn the optimal one? Of course, this is a super basic idea. It's just maybe, I mean, I think it's interesting to hear it in a continuous framework. So, the ideal setting for it, it would be to have access to a statistical model, a complete statistical model of Statistical model of the quantities involved in your inverse problem. And by that, I mean a statistical model for x and for epsilon, so the noise. So it's something that it's, of course, typically unknown. You don't know the statistical properties, the underlying probability distribution of your prior and of your noise. So it's not like in the Bayesian framework where you impose a prior. So here the prior is something that exists, but Priority is something that exists but is unknown. So we imagine that the images that we want to the blur, or I don't know, the objects that we want to reconstruct, so our X are distributed according to a law. Of course, we don't know this law. So if we were to know this law, and in particular the joint probability distribution of X and of the noise, the noise is actually easier to get in most of applications. Maybe it's Gaussian, maybe you know some properties about that. Some properties about that. What we can do is actually super simple. So, to compute this quantity here, which is like a square loss of, I mean, when you use your regularizer for a specific, you get a specific choice of a candidate regularizer depending on a parameter theta, you apply to your data and you compare with the original solution of your inverse problem. Solution of your inverse problem, and you just take an average among all possible x and y's. Okay, that's the simplest idea to evaluate the quality of a regularizer, so how good it is in going back from y to x. And then this just depends on theta. This is the expected loss depending just on theta. And then you minimize it. I mean, it looks like super simple. Of course, it's going to be extremely complicated, but you can minimize. Extremely complicated, but you can minimize it and get the best parameter. So that means the best regularizer within the class that you specified. Of course, this is not possible, but it's not that far from being possible, especially if you have a very large training sample, meaning that, of course, you don't know the statistical properties of the underlying random variable x, but you may have a lot of some, I mean, a very large sample coming from it. Coming from it. So, whenever you have a sample of size m drawn from this joint probability distribution, then what you can do is to replace the expected loss with the empirical loss. And this is something you can compute whenever you have a sample. And this again depends on theta, and you minimize and you get theta. So, this is something that is possibly close to the optimal one. And this And this is, in a sense, the supervised learning paradigm. It means that you need to have pairs, labeled pairs, like pairs of inputs and outputs. I'm going to show you also, but now of course it's a bit obscure, that we can do something also when you have a training sample based just on the, I mean, the solutions, the possible solutions of inverse problems. So imagine like that you want to work with imaging. Work with images of the lungs or something like that. You have a large training sample just of noiseless true images of lungs. Of course, this can be quite complicated to have in applications, but luckily we saw in the talk this morning that it's possible to create some even real-world, very, very accurate samples. And this is probably something we can take advantage, even if we do. Take advantage even if we don't have a pair of input and output. We can just use a sample of input/outputs. Now it's a bit complicated because, of course, it's the input of the inverse problem, but the output of the network. So sample of x, let's say, and use this sample to get an approximation. Of course, I need to show you ad hoc techniques to do this. But just to mention, this is what we call unsupervised learning. Actually, it would be more correct to say not supervised in a To say not supervised, in a sense, it's not the supervised learning. I'm not sure if this falls in the category of unsupervised learning, how it's usually described. Yes. What do you estimate? You estimate, actually, you don't need to estimate the expected loss. You just directly compute an estimate of this parameter. I mean, you estimate the optimal, an approximation of the optimal parameter. Yeah, yeah, yeah, sure. But in a very specific example, I mean. But in a very specific example, I mean, yeah. Okay, just before moving to the next part, I just wanted to show you that this is just a part of a larger pipeline. So here I said, I tried to describe the effect of having a sample, a training sample and not the full knowledge of the statistical model. Actually, the pipeline should be much, much larger if you consider the problem in applications even. Problem in applications, even. So, the idea is that usually you don't get you cannot really get a minimizer, not even of your empirical loss, because you do maybe some inexact method, you do some optimization technique, and you don't get precisely the minimizer, maybe an epsilon minimizer. So, you're going to have an outcome of your training. And then, hopefully, this is close to what I was mentioning before. So, the empirical target. But then, remember that the empirical target is. Target is minimizing with a supervised or unsupervised technique, no matter. It's supervising, I mean, it's minimizing, sorry, the empirical loss, but you are also minimizing it, not among all the possible functions, but among a function functionals living in a parametric family. What you want is to be close to the optimal target, meaning the one you would get by minimizing the expected loss. Minimizing the expected loss in the same family, and then, of course, the rest is. I mean, there is something more. Of course, you selected a family of regularizers, but actually you can also look for the best possible functional among all the possible measurable functions between y and x. And actually, there's a formula for this, and it's the Bayesian estimator. And you may wonder how far I am in my. Far I am in my when I minimize in this family, am I far from the best one that would be possibly outside of this family that I parametrized? And then, of course, there is the true inverse map that is still something different. So, this is the full pipeline. The first part would be discussing the optimization error. So, do I trust my optimizer? The second part, that's what I'm going to address, is more the generalization error. So, am I able to generalize from So, am I able to generalize from my sample to the optimal regularizer based on the full, I mean, the integral, the expected loss and non-de-training on a sample. And then what I'm not going to show you, I mean, what I'm going to show you is just the red part. Okay, the rest would be, I mean, it's just to show how the red part is inside the whole pipeline of the errors in learning. This part would be the approximation error. So how good is my family? So, how good is my family, my hypothesis class, how it's called in statistical learning, in approximating the best Bayesian estimator? And then, probably the last part is something, I mean, it's typically called irreducible error in statistical learning because typically the noise level is fixed in statistical learning. And you don't want to reduce this too much because, of course, you don't want to be as close as possible to the inverse map that is unstable. You may want to be. Stable. You may want to be close to the inverse map just if you know that the noise on your sample is reducing. This is more familiar for people coming from inverse problems. So you have typically estimates that when the noise level is going to zero, you may want to be as close as possible to the inverse map. But typically, in statistical learning, the noise is fixed. So this is called irreducible error. Okay, a very quick review of Quick review of strategies. I mean, this is still something I haven't explained to you. I just told you, suppose that we have a family of regularizers parameterized by some parameters. Going to show you very clever ideas, and then I'm going to focus on the least clever idea. I mean, the easiest idea, let's say. So, the idea is that, I mean, the easiest, the most typical way of describing Typical way of describing this kind of family is by means of bi-level optimization, or actually, is by defining R theta itself as the solution of an optimization problem. So, in a sense, we are going to define a family of functionals depending on a parameter, and then we are calling the minimizer. I mean, we are defining R theta as the function that takes. As the function that takes y and gives you back the minimizer, supposing that it's unique, it exists and is unique, of the parameterized functional that you are minimizing. So this is just moving the problem one step forward. So how do you parameterize the functional inside that you are minimizing? Let's start with easy ideas. So the easiest idea would be, for example, you fix a discrepancy term and your favorite regularization term. Your favorite regularization term, and you just put a parameter in front of it, and you may want to find the best one. So it's a family, quite not so expressive family. So it's just everything is fixed, but just the level of the regularization can change. And you may look for the best value of the regularization parameter. Or you can have something more advanced. Maybe you can parametrize both these functions. Both these functions, for example, putting some weight or some change of variables here in front of the difference between AX and Y, and maybe taking an LP norm, not a P norm, and not a two norm. And here again, you can change the variables or like the representation. You can change, yeah. I mean, you can do. You can change. Yeah, I mean, you can do pretty much what you want. Or actually, what you can do is something even more complicated. So, instead of providing a parametric expression of the functional that you are minimizing, you can provide a parametric expression of some functionals strictly related to it, for example, the proximal of the functional. So, if you know, I mean, if you are going to approximate this minimizer, for example, using a proximal algorithm, a proximal gradient algorithm, then you don't need to know. Then you don't need to know precisely the function that you are computing, but you just need to know the proxima. So you can have pretty much a lot of idea, possible ideas. Of course, what you need to balance in this choice is, of course, you want to have a family of regularizers that is sufficiently expressive. So you want to explore a lot of possible regularization techniques. Possible regularization techniques. But also, you may want to include already a part of knowledge that you have on your solution. Easiest idea. Suppose that you know that your solution is sparse, but you don't know the basis. Okay, so it's a balance. You're not requiring that your solution is sparse in a fixed basis that you decided. You know it's sparse and you want to infer the basis from the data. And then, of course, you have some technical requirements. Just wanted to mention. Requirements. Just wanted to mention that there are a lot of possible strategies here. I'm going to focus actually on the easiest one, so this one. So there are some cases, easy cases, where you parametrize your functional and you get an explicit solution of the inner minimization problem. So the minimization problem defining r theta. Let me show you again. So for some easy choices, Some easy choices here inside, you can have that you don't have to solve this inner minimization problem, and you have an explicit expression. So, in that case, it's actually easier to derive properties on your R theta and also maybe to compute the optimal theta analytically. And I'm going to focus on this one. But just to mention that there are a lot of other possibilities more complicated, let me just mention three, four names that are not three names, I think, that are probably. Are no three names, I think, that are probably familiar to you. So, alternative ideas with respect to restricting to a case where you have an explicit expression would be: first one, to instead of solving the minimization problem defining r theta, to approximate the minimizer using an iterative scheme. And this leads to at least two very famous paradigms that are unrolling and plug-and-play. They are completely different in how they are. Different in how they learn the parameters, but the idea is similar. So you don't define Rθ as an exact minimizer of a function that you parameterize, but you define it as the outcome of a sufficiently large number of steps of an optimization routine. Okay, it's still in the same framework, it's just that you're approximating a minimizer with the output of an iterative scheme, and I think. And I think, in the interest of time, I will just mention the names that you know better than me, of course. There is also something in between that is the deep equilibrium machinery. So it's when you are able to define the minimizer, R theta, as a fixed point of a suitable map. So this is another strategy, an alternative strategy. It's not an explicit expression, but it's not an approximation. Expression, but it's not an approximation, so it's more complicated, of course, to work with. You need to use implicit automatic differentiation, but it's something alternative. But as I was mentioning, we're just focusing on the first example. And let me just mention the easiest example. That's what we are working with. So, suppose that you are looking for the best regularizer among all the possible functionals that has this. Functionals that have this form here. So you can have, it's basically a t-con of regularization, so squared norm. But you, before taking the norm, you remove this h. It can be a background, for example, and you can change, you put a linear operator in front of it. Forget about the inverse now, it's just for notation mainly for later, it's going to make it easier. It's easier, but you want to learn B and H, so these are your parameters. So, in our this is what you can call generalized economic regularizers. So, changing H and B, you have a family of possible regularizers. You may want to find the best H and the best B. Is it an expressive family? I don't know, but it describes a lot of different possible operators. So, the easiest one. Operators. So the easiest one, of course, is Tikhonov. You just use identity and zero. But if you have identity and something else, it's just promoting the fact that you have small norms with respect to a reference object. It can be useful in many applications where you have a background and you want to just find something that is different from the background. And also imposing here, for example, that this operator is a differential operator, you can, for example, promote smoothness. Example: promote smoothness of your solution pretty much as you do with the total variation, but you don't promote the fact that there are local jumps, it's just smoothness anywhere. Okay, so this is essentially the family we are working with. So I replaced theta with the H and B, that is the couple of parameters that we are going to optimize on. Notice that H and B by now are living in infinite-dimensional spaces because X is. Dimensional spaces because x is living in x, that is a Hilbert space. So, also h is an element of this Hilbert space, and b is an operator from x to x. So the parameter itself lives in an infinite-dimensional space. Okay, so we are ready to get all the results that we had for this problem. So I think Uh, I think this part I'm going to skip actually just to say, I already mentioned to you that most of the technicalities of the paper, if you read the paper, uh, most of the technicalities come from the fact that everything is in infinite dimensional spaces. And let me just mention what goes wrong and then let's just keep all the solution. So, in finite dimensional spaces, that would be super easy. So, you have a model for X and for Y that is just X and for y, that is just square integrable random variables, you suppose to know the covariance of the noise for simplicity. I mean, it's something probably in some application is not so meaningful, but in other applications, it's something that you can collect some information about, for example, by taking measurements of pure noise, let's say, and you can deduce properties of noise. So instead of solving the inverse problem like this, you solve the Problem like this, you solve this minimization problem parameterizing B and H. And let me just mention that we whitened this discrepancy term. So when you look for the solution of this problem, of course, you have an explicit formula, it's quadratic, so you just differentiate and you set the derivative equal to zero, and you get this expression. Or actually, this is a bit easier. You see that it's an affine function of y. So So, it's actually quite simple, even to get the best choice of H and B. I'm going to show you later. The idea is that everything is a bit broken when you move to infinite dimensional spaces. First point is that it's hard to get, I mean, it's hard to just confine yourself to square integrable random variables that would mean random variables with a covariance operator that is trace class. This is a bit too restrictive. For example, it doesn't include. Too restrictive, for example, it doesn't include the white noise where the covariance is identity. So, long story to incorporate also the white noise, don't care. So, every time you're going to see a Yota appearing in the formulas, just forget about it because we have a long time, but not so long to discuss on the results. So that's it. And also, let me mention that we cannot formulate the problem like this. Not formulate the problem like this, but we need to move to a more complicated formula. But the idea is that we want to solve this problem. It's just that we formulate in a different way because especially this part of the quadratic term. So when you apply sigma epsilon to the minus one half to y, this could be infinity. And so you're trying to minimize a function where there is a part that is plus infinity. So, but this part does not depend on x, so you can just Part does not depend on x, so you can just remove it, and essentially you get something that is like this. So, just to mention, if you read the paper, you look that we are minimizing this functional here, but actually, what we want to do is this one. Okay, just keep in mind the easy expression. And keep in mind that we need to have this kind of compatibility assumption between the noise and the operator A, essentially. So, what we are going to require very easily is Very easily is that our operator is sufficiently smoothing with respect to the smoothness of the noise. That's the key message. Okay, under this simple assumption, what we can do is to characterize the optimal choice of the regularizer. And actually, it's, I mean, not so surprising. So, the idea is that whenever you have functionals, as before, depending on the parameter H and B, and you look for the one that minimizes. H and B, and you look for the one that minimizes the expected loss, the best choice is just to use as a reference h the mean of the distribution of x. Of course, the distribution of x is unknown, but the best idea, if you were to know the statistical distribution of x, would be to use as an h then the mean and as b the square root of the covariance. The square root of the covariance. So, what's nice is that it's the same as in finite dimensional cases. You get an easy expression for the optimal regularizer. But also when you look, I mean, the optimal choice of mu, I mean, the optimal choice of H and D does not depend on the noise and does not depend on the operator A. So, in principle, you So, in principle, you can have two different strategies to recover the optimal regularizer that is just mu and sigma x. So, how we prove it? It's not so interesting. So, no, it's actually interesting. We don't have time. So, we have two possibilities if we want to learn the optimal parameter. One, of course, is to minimize empirical risk, and that's already what you know with the sense. What you know with the sample, but actually, if you just have a sample of x, the easiest idea would be to compute the empirical mean and empirical covariance. It's so easy. I mean, the easiest idea. But that already is going to give you an approximation, possibly a good approximation of the optimal choice of the parameter. So, without doing any kind of learning, essentially. So, now the only question that is left is: how good are Left is how good are these estimators with respect to the optimal one? So, how can we characterize the quality of the regularizer using H and B as prescribed by these choices with respect to the optimal one? And the idea is that you, I mean, what is usually done in statistical learning is to compute what is called the excess risk. So, expected loss evaluated in the empirical estimator minus the expected loss. I mean, the minimum of the expected loss. So, the one evaluated in the minimum. So, the one evaluated in the minimizer. And let me just show you the results. What we get in the unsupervised case, sorry, in the supervised case, what we get is that this quantity here decays as essentially the square root of one divided the square root of m. That's what you usually have in, it's like a concentration estimate, essentially. You're approximating, I mean, it's like evaluating the evaluating the quality of, I mean, it's sorry, it's quite natural result to have the square root of m at the denominator here. Just you have some small corruption appearing there. And especially what I didn't mention to you is that everything works in this infinite-dimensional machinery, provided that you introduce a very important assumption on the space. Assumption on the space where you are looking for your minimizers. You cannot look for every possible H and B living in an infinite dimensional space. You need to focus on a compact set. And the compactness is what gives you some results even in infinite dimensional settings. So looking for minimizer in a compact set is the solution, the easiest way to get some estimates in an infinite dimensional setup. And in a sense, Infinite dimensional setup. And in a sense, this parameter S appearing here, so this S prime is just a bit smaller than S, is a way to quantify compactness. You can imagine like that the space theta has some, I mean, to quantify the compactness, you usually do compute covering numbers or something related to the decay of the singular values of the embedding of your space. Values of the embedding of your space into the larger one. So there are many ways to quantify compactness. I'm not going to discuss them. Hs is the space of Hilbert Schmidt operators. So it's a compact, I mean it's a smaller space. Hilbert-Schmidt operators. So you don't look for any possible operators inside from x to x. But all the details here, please don't focus on them because it would be too long. Because it would be too long. No, please, I mean, meaning that there are so many details I can't cover in the presentation, but if you are interested, just ask. How we prove it's actually nice, but of course, in the interest of time, I'm going to skip. And this works also for the unsupervised case. This is actually easier. So it's just using concentration estimates for not just Gaussian random variables, it works for sub-Gaussian random variables or. Sub-Gaussian random variables or bounded random variables. So you have quite large possible models for x and epsilon. You are not just sticking on Gaussian random variables. You get some estimates that are precisely the order of one over the square root of m. And sorry, I didn't mention it probably before, or maybe I did. m is the size of the training sample, of course. So when the training sample goes to plus infinity, you have a decay of this quantity. Of course, we tried to do a bit of numerics on this, mainly just to verify these estimates, just proof of concept, let's say. We worked with the denoising problem and with the deblurring problem. So, what we did is to consider Gaussian distribution of x with unknown mean and covariance, synthetic. Synthetic data, of course. We created a data set and we used them for our purposes. And what we did was, of course, we have an analytical formula to compute the best possible empirical risk, expected loss. And what we can do is to approximate with the supervised and with the unsupervised technique the optimal parameter and compute these quantities. Compute these quantities. And of course, also, we have to repeat this experiment many times because the estimates that I shown you before hold in probability or in expectation. So we have to repeat the experiment. And not only we did that, but we repeated the experiment many times, also increasing the number of elements in the training sample. So we have for different values of m, these quantities, and we just look for the dk. If it goes like one over the square root of m, and it does the blue curve is the unsupervised, the red curve is the supervised, the results are pretty much the same. We were not focusing on comparing the two techniques. The blue one is actually the easiest one, so it's probably the most interesting. In different setups, we compared with Gaussian white noise, uniform white noise, or uniform white noise with respect to a different. Uniform white noise with respect to a different representation. This second experiment is actually going back to what I said in the beginning. So we are working in infinite dimensional setup. So we want our result to be discretization independent. So we tried the same result on signals defined on different discretizations of the interval. And just we got the same decay, maybe in this. Decay, maybe in this case, the blue curves are a bit more convincing, so they don't change that much across different discretizations. And we repeated the same also for the blurring example, and you can trust me that it works, it doesn't change that much. And in the last minute, I will just tell you that we extended this. I don't have time to discuss all the results, but just to mention that this was published a couple of years ago. What we have been doing. A couple of years ago, what we have been doing in between, that was one year and a half ago, but what we have been doing in between is trying to extend from generalized Tikhonov functionals to L1 functionals, essentially. So the idea is that we want to promote sparsity, and we are all very aware of the fact that sparsity can be promoted by minimizing one norm. Minimizing one norm with respect to some change of coordinates. And the idea here is that instead of working in the analysis formulation, we work in the synthesis formulation, but it doesn't change that much. The idea is that we want to learn B. So we want to learn the operator that makes our solution sparse, okay? The representation of our solution sparse. And this is quite expressive, actually. This is quite expressive actually. So, learning a transform that makes your signal sparse can encode very different properties of your signal. So, for example, it can encode signal with spikes if B is identity. It can encode, for example, very smooth functions, or consider, for example, you're working on imaging or imaging problems in R2. If you use B, that is a wavelet basis or a curvelet basis. is a wavelet basis or a curvelet basis sparsity promote promoting sparsity can encode different informations about your solution so we want to learn b is it that harder than learning b and h so forget about h in this case is it that harder when you have one norm essentially instead of having the squared two norm yes it's much harder the idea is that you don't have an explicit formula now for the Have an explicit formula now for the inner minimizer. And so it's very hard. I mean, it's probably impossible to characterize the optimal choice of B. And so you don't have a straightforward and supervised approach. So there is no formula to compute B. But actually, the supervised technique still works. So what I was skipping before when I said, ah, the proof of the supervised is interesting, is that actually in the proof of the supervised learning result, we didn't need to use any specific Need to use any specific, I mean, some specific properties of the Tikonov regularizers for sure, but it's something that can be generalized also to this case. So we are working on this extension. We are actually very close. And if you are interested in that, hopefully soon I'm going to, I mean, we're going to release a preprint about that. And so I probably don't have time to get into the details of what we are doing here. But let me just mention. Doing here. But let me just mention that the key idea to prove generalization estimates that can hold also in this case is a combination, as I was mentioning here. The generalization result in the supervised approach rely on the fact that you are able to, even though you are looking for an infinite-dimensional space of parameters, you can say that the regularizer depends continuously on the choice of parameters. So you have a stability result, meaning. So, you have a stability result, meaning that you change a little bit theta and you have small changes on R theta. And then the second part is that you're able to cover efficiently the compact space you're working in. And this second part is more well known. You use covering numbers of compact sets and they work fine also in infinite dimensional setup. What is difficult, especially in this case in one norm, is to show that if you change a bit, a little bit B, you have. Little bit B, you have a little bit of a change in the minimizer of your problem. And that's what took us a lot of time to prove, but we are now able to do that. And so essentially, the main result in this case is that if you change a little bit B, you have a small change in your minimizer. And that's it for this. So I hope I didn't stress you too much after lunch. I showed you some ideas essentially on how. Ideas essentially on having a broad picture of learned regularization of linear inverse problems and focusing on a very simple example, at least in my opinion. But still, what is interesting is that working in infinite dimensional setup is possible, even in this case, and it gives you discretization-independent results that are somehow interesting, probably also in the applied community. And whenever it's possible to have also an Whenever it's possible to have also unsupervised techniques, those are very interesting. But of course, there are limited cases. It's not just the case of the Tikhonov regularizer, but probably you need to wait a bit more since we finished another project about another unsupervised technique for different problem. So, thank you for our attention. Thank you for your talk. So, I would have a question for this slide where I showed the synthesis versus analysis formulation. Yes. Actually, I think it's known that the synthesis is worse compared to the analysis formulation, right? Because it's much harder to generate signals than to analyze them. Yeah. And one possible solution could be you could. Could be: you could just have a look at the convex conjugate of it. Then the analysis model turns into a synthesis model just with an anti-sparsity prior. So you have the infinity norm then. Oh, okay. I mean, the dual problem. Yes, the dual problem. Yes, that's something we haven't investigated. Actually, I mean, as in the Tikhonov case, you remember we had. Of case, you remember we had a b to the minus one at some point. So, for some purposes, it's easier to consider the synthesis, but not for the optimization part. For example, if you require that B is wave flat, some orthogonal matrix, then of course you have a closed-form solution, right? Yeah, there are some cases exactly where you can. And it's just wave flat thresholding. That's probably nice because what I didn't mention is that one main example that That one main example that we have in mind, for example, is something like this. So that B, you are looking for operators of this form. So B that come from a wavelet, and you look for the optimal mother wavelet. So in that case, probably you can have also, I mean, the supervised machinery works, but maybe you have something that is also. Yeah, we did something in this direction some years ago where we learned an optimal wavelet. Oh, yes, yes, yes, precisely. And that's probably. And you just have some simple quadratic construction. Up that you just have some simple quadratic constraints there, of course, non-comics, but you have some quadratic constraints from the equation that makes wavelet the wavelet. Yeah, yeah, yeah. This you need to have then, but it works. Thank you very much for pointing that out. Of course, I mean, now this last part is in very, I mean, it's still in its development. So I'm sorry for not pointing out the connections to the literature, but I think that is probably something. The literature, but I think that is probably something very interesting to compare with actually. A second question: so, when your lower-level problem is taken off, then basically your regression, your model, your regression model is a linear model, right? Yeah. When you compare your research, your conversions research with what is known for linear models. So, how does this compare? I mean, what what is nice is that we recover precisely that the optimal estimator is what is the linearized minimum mean square error estimator. Minimum mean square error estimator. So the idea is that looking at the family of generalized teacon of regularizer is, I mean, and looking for the optimal one within that class means to look for the linearized minimum means parallel estimator, which was actually a bit complicated to extend and to consider in the infinite dimensional setup. But in the finite dimensional setup, there is nothing new. So it's precisely the same. So it's precisely the same estimator. And in your generalization bound, did you actually assume that the optimal model is a linear model? Yes, yes, yes, yes. Otherwise, you would have an approximation error. So what I was not mentioning in the beginning, that's just. Maybe I didn't get it. Thank you. Thanks. Very nice talk, Lucas. Nice talk, Lucas. I have a comment first. So, when you actually show the discretization invariants, so in my group, we do that a lot as well. So, if it doesn't look like discretization invariant, even though the blue one is closer, yeah, the other one. So, that's actually, I have two questions here. First, why the supervised is not discretization invariant? And second, why the supervised, because the supervised one, you have more information. Because the supervised one, you have more information, right? You have the data as well. Why actually is it worse than the unsupervised one? They are both very good questions. So, I mean, we don't expect the curves to be precisely the same since we know that the decay is the same, but the constants can be different. So, we did not investigate too much if there can be a dependence in the constants. So, everything that is like, you know, parallel lines would be enough for us to say that the decay as m grows is. Is not altered by changing the discretization. Of course, in the blue case, they are also very similar, the curves. And why the blue is better than the red? Of course, in the red, you use more data, but the data that you're using are also corrupted by noise. So the X sample is perfect images and I mean, perfect solutions of your inverse problems, but in Y, you also have noise. So that's probably why it's working worse. Why it's working worse. And that's probably also why it would be more interesting to look at these unsupervised techniques and not supervised, but theoretically, they work similarly. I see. So just one more comment. So, you know, if you apply this for imaging, you know, medical imaging, you know, at least in the US, I'm not sure, because if you assume that you have X and Y, that's actually, you know, you have to deal with the privacy, you know, legal issues. You know, you can do things that he did on. You can do things like he did on, you know, a piece of chocolate, that's fine. But if you want to do with like real humans' organs, right? Even for either an unsupervised supervised case, you know, that's, can you comment on the practicality in that, you know, medical imaging, you know, context? Of course, I can't comment because it would be completely irrelevant what I say, but yes, it's something that it would be complicated, of course, to apply whenever you don't have access to very large data sets. Don't have access to very large data sets. And so, especially for medical imaging, this could be problematic. But, I mean, it's still something that could be interesting to investigate, especially this capacity of generalize from the training sample. So, I mean, the hope is that you can get something even by learning on a sample that is not so far from the true distribution. And so, maybe you can use exactly some very clever design. Some very clever design data sets also to infer something.