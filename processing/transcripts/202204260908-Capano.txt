The assignment. I'll explain in a minute what I mean by overtones versus angular modes. So, first of all, just a little background. To date, we've detected nearly 100 binary black hole mergers from the first three observing runs of LIGO and Virgo. This is a plot that LIGO put out, sort of summarizing all the observations. And each of the blue dots here represent black holes. Black holes. And so you can see the vast majority of the things that we've detected are binary black hole mergers. So that's where you have two black holes that merge to form a final larger black hole. And that's what's represented by each of these. You can see the mass range for these things ranges anywhere from around five solar masses all the way up to as much as 200 solar masses, although some of these very large ones are the statistical confidence in them is a little bit weaker. In them is a little bit weaker. And these binary black hole mergers give us an excellent opportunity to finally do black hole spectroscopy. So as I said, when you have the two initial black holes, they orbit around each other, eventually in spiral and merge to form a final black hole. And immediately after merger, that final black hole is a perturbed curve black hole. And so the gravitational wave that's emitted in that final state. wave that's emitted in that final stage, the ring down stage, we expect to be a superposition of quasi-normal modes. And so that's represented here. So, you know, this final gravitational wave, you know, we expect to be this sum over damp sinusoids, what we call quasi-normal modes. And this is just a schematic representation of that. So you have your gravitational wave, and if you just look at the very end, then you can break this down as a sum over. Break this down as a sum over these damp sinusoids. And one of the things, sort of a consequence of the No-Hera theorem, is even though each one of these, this is an infinite sum of damp sinusoids, each with their own frequency and damping time. However, as a consequence of the Noher theorem, if the real object is truly a curved black hole, then all the frequency and damping times should be described by just two parameters, the mass and the spin of the black hole. Of the black hole. And so, if you can observe more than two of these, two or more of these quasi-normal modes, if you can extract information about them from the signal, then you can do what we call black hole spectroscopy. You can test to see are the frequency and damning times truly described by just the mass and the spin of the final black hole. And if that's the case, then it's strong evidence that the final object really is described by the curve metric and is not something else. Not something else. So that's the basic idea, but there's a number of real practical challenges to actually doing this. So first of all, as I said, they're damp sinusoids that we're trying to look at here. And that means, though, that the signal that you're trying to extract is short, weak, and it's buried in noise. On the right here, I have a plot of what GW15914 looked like. This was the very first detection of a gravitational wave. Of a gravitational wave. And I marked down here in red roughly where we think the ring down is. And the top plot here is what the signal actually looked like in the noise. Even though this was a very loud event and you can actually see it with just a minimal amount of filtering, but even given this event, you can see that the ringdown phase, you get maybe one cycle sticking above the noise. Some additional challenges are also that. Are also that, as I said, the quasi-normal mode, this model of a sum of quasi-normal modes, is really only valid after merger. And when after merger is a bit of an open question. So you don't even really know where exactly this model applies. And then another challenge is that, as given in the name quasi-normal modes, the modes are not orthogonal to each other. And so this makes it somewhat difficult to figure out how many modes are actually observable. out how many modes are actually observable above the noise. And so two key questions that you have to address when you're trying to do black hole spectroscopy is first of all, when is this quasi-normal mode description of the signal valid? And secondly, how many modes are observable in a given signal? I emphasize observable because yes, in theory, there's an infinite sum of these. And so you should, you know, they should all be present in the signal. But when you're actually trying to do a black hole spectrum, Do black hole spectroscopy and learn information. What you really want to know is what modes are observable above noise. Because if you just throw everything in your signal model, you're going to get junk in, junk out. So how do we actually go about doing this? Well, we use Bayesian inference. And just a sort of a quick primer or a reminder of what Bayesian inference is. So we have some data, call it D. So this is a time series, say, that you've Is a time series, say, that you've sampled with, say, n samples. And you're assuming that in that set of data that you've extracted, that you've gathered from the detectors, that you have some signal in there, which we'll call H, and you have some signal model, in this case, a sum of quasi-normal modes. And so it has some parameters, frequency, damping times, et cetera. And what you want to know is, given that you've observed some data and you have the signal model, what is the probability distribution or the probability that that signal has some? That signal has some set of parameter values, what we call the posterior distribution. And so to actually measure this thing, you apply Bayes' theorem to decompose this into a likelihood times a prior divided by some evidence. I'm going to talk a little bit more about the evidence in a minute, but I want to focus first on the likelihood. So the likelihood. Hi, can you can you hear me? We can still hear you, yes. Okay, sorry, for some reason my headphone suddenly cut out. So I'll focus first on the likelihood, which is sort of how you actually measure the signal and the data. So, in gravitational wave analyses, it's pretty common that we assume. It's pretty common that we assume that, in the absence of a signal, the noise is described by a stochastic Gaussian process. And so, in that case, you can write down what the likelihood function is. It's given here. It's essentially just a multi-dimensional Gaussian distribution where the components in that, the thing that goes into the exponent is this noise vector, which is just the data that you observed. Is just the data that you observed minus the signal that you're considering with some given parameters. Now, a key part of this in order to evaluate this likelihood expression, you need to know what the inverse covariance matrix of the noise is. So you have this inverse matrix here. Now, in typical gravitational wave analyses, you assume that this covariance matrix is a circular matrix, and then you can apply some math and you get sort of this canonical. Sort of this canonical likelihood function that falls out. However, when you're doing a QM analysis, because the model is not a model for the entire gravitational wave and only for the post-merger analysis, for the post-merger part of the signal, this introduces some complexities to evaluating this likelihood and it's a little bit tricky to evaluate this. However, there have been three different approaches now to Approaches now to do this, and so there are pipelines out there that you can download to do this ring-down analysis. And there's three in particular out there right now. One is called Pyring, and this was developed by Gregorio Carullo et al. And this one is used by the LIGO Virgo collaboration and all the testing GR papers that they've put out where they've done black hole spectroscopy. There's another There's another package called Ring Down, and this has been put together by Max EC and Mulfar. It has a number of common features as Pyring. And this is the software base that was used to detect an overtone or do an overtone analysis in GW15 and 914. And I'll talk a bit more about that in a minute. And then there's a third package called Pi-CBC inference. This is what myself and my colleagues work on. Myself and my colleagues work on. We use a slightly different method to evaluate the likelihood, but it gives the same result as the other two packages. And we've used this for applying doing a subdominant mode analysis of another FEN DW190521, which I'll talk about in the second half of my event. I should note that these other two, PyRing and Ringdown, they're very, they're specific to just analyzing ring downs. PyCBC inference, however, is sort of a larger parameter. Parameter estimation package, and you can do like normal gravitational wave PE with it in addition to doing ring-down analyses. Colin, could you just clarify? So, I understand what my CDC does for myself. So, it's inverting that matrix, the thing cated out. Can you just tell us what the other two packages do for that problem? The other two just numerically invert the matrix. So, essentially, they numerically invert it. So, kind of almost a brute force thing. A almost a brute force thing, and then they just numerically evaluate this expression. Okay, but in any case, all of them do claim to take into account the extent of the gaming on the ungated part of the data. Yeah, they're all doing basically the same thing. It's just different approaches to the same result. Yeah. Okay. So in theory, all three of these pipelines should produce the same result given some data. Given some data. But as we'll see, that's not actually the case. If it's possible to just backtrack just one second and tell us why it is difficult to do this analysis because we only have part of the spectrum, the causinormal modes is just part of the spectrum. And you said that causes difficulties. Why can't I just look at just that part of the spectrum and do this analysis? Yeah, so the problem is that you are trying to exclude a part of the signal in the time domain. Exclude a part of the signal in the time domain, not the frequency domain. And it's a very, you know, the difference between you're trying to exclude this merger and just look at this bit. But the timing here, it's a few milliseconds that you need to, you know, a few milliseconds makes a big difference. And the problem is, if you stare at this likelihood function long enough, you realize that essentially what this is, is the convolution between the inverse. Convolution between the inverse power spectral density and the noise residual in the time domain. The inverse power spectral density has a width of several milliseconds. And so it's kind of coupling things together in time over a time span that is significant for these type of analyses. So you kind of need to really break off what happens before the rate. Before the ring down. In math terms, what this means is this covariance matrix normally is a topless matrix. When you break that off, though, okay, if you break it off and it's just ringed down, it's still a toplitz. But the standard way where you sort of apply a frequency domain analysis to this is you assume that this toplitz matrix is actually a circulate matrix. That breaks down when you're trying to exclude parts in the When you're trying to exclude parts in the time domain, and so you need some other method to invert the matrix than the sort of standard thing where you assume circulate and then get to the frequency domain. So part of the analysis involves going to the frequency domain, and that is why you cannot simply cut. Yeah, essentially. Yeah. Okay. So like I said, there's three different paths. Like I said, there's three different pipelines here, but they essentially all have kind of settled on a standard approach to detecting quasi-normal modes that I kind of sort of break down here. So the first step is you use a full gravitational wave template, meaning it's a template that models the full inspiral merger and ringdown to approximate the to identify the approximate merger time and then the sky location for the signal. Then you analyze the signal just the post-merger part of the signal. The signal, just the post-merger part of the signal, using that estimated coalescence time and also fixing the sky location for technical reasons. If you want it, because like I said, you don't know exactly when maybe when the ring down should start, essentially what you do is you just do different analyses starting the quasi-normal mode at different start times. And you essentially perform an analysis and apply Bayesian inference to it and calculate evidence. inference to it and calculate evidence to see where is the strongest evidence for a particular combination of modes in the data. So essentially, you do multiple analyses with different sets of modes to identify the most likely combination of modes. And then if you do have strong evidence for more than one mode, then you can perform black hole spectroscopy, meaning that you then Black hole spectroscopy, meaning that you then allow the frequency and damping time of the modes that you've identified to deviate from their GR value, and then you see if the data prefers the GR value or not. And that's sort of the general gist of what these analyses attempt to do. Like I said, when you're trying to identify what modes are observable, you need some statistics, some way to decide is there two. To decide: is there two modes in the data? Is there three modes or just one mode that's observable? There's a few different ways to do this, but sort of a standard way is to calculate the Bayesian evidence or the Bayes factor. And what that is, if you recall Bayesium from earlier, there was this normalization constant that it called the evidence. And this is arrived at just simply marginalizing the likelihood times the prior over the entire parameter space. And this normal. And this normalization value that you get, this evidence, will be different for different models. So, if you have two different models that describe the signal, say you have one model where you have two modes, and so you have more sets of free parameters, and then you have another model where you only have one mode. You calculate the evidence for each of these two different models, and then you take the ratio of these evidence, and this gives you what's called the Bayes factor. And this essentially, this base factor is kind of like an odds ratio of how much does the data favor one model versus another. There's different sort of you get a number out of this base factor, but what does that number mean? A sort of standard table that's used is this one produced by Cass and Rafferty to sort of quantify or give you a qualitative understanding of what these different Bayes factors mean. Understanding of what these different Bayes factors mean. And so a Bayes factor between one and three, that's not very strong evidence, as they say, not worth more than a bare mention of essentially the data isn't telling you, can't decide between if model A is stronger than model B. A Bayes factor between 3.2 and up to 10, though, you would call that sort of substantial evidence for the data favoring one model versus the other. Between 10 and 100, Between 10 and 100, you would call that strong evidence, and then above 100, you would say that's really decisive: that the data is really favoring one model versus the other. And so this is, I'll be quoting Bayes factors later, and so you have an idea of what these numbers mean. Okay, that's it for the background. So now I'll talk about what is the evidence for overtones. What I mean by overtones is if you recall. What I mean by overtones is if you recall the expression for the sum of the quasi-normal modes from earlier, you have three numbers: the L, the M, and the N. The L and the M generally refer to them as angular modes. And then for any given L and M, so for instance, the dominant mode is the 2,2 mode, there's an infinite set of overtones. So with the N equals zero being the fundamental mode, and then larger N being overtone. Larger ambient overtones of that mode. And so, what I'm going to be talking about here in particular is the overtones of the dominant mode. So, just a little bit of history first about in general black hole spectroscopy. So prior to 2019, most black hole spectroscopy work focused on trying to detect subdominant angular modes, i.e., where the L and the M, not the 22 mode, but fundamentally. Not the 2-2 mode, but fundamental, so say 330, 210, etc. And the rule of thumb was that, like you say, when you're close to merger, it was thought that the gravitational wave at that point in the space-time is highly nonlinear. And so this Q and M description of the signal isn't really valid at that point. And so you had to wait some time after merger before the nonlinearities radiated away and you're sort of in this linear perturbative regime where this Q and M. Regime where this QM description was valid. And the rule of thumb that was set up, that was arrived at from looking at some NR signals was that you had to wait around 10M after the merger before this description became valid and you could try and look for subdominant modes. But there's the problem with that is, first of all, as we said, we've got damp sinusoids. So if you have to wait 10m after, you're losing SNR quickly. The other issue. Quickly. The other issue is that to date, the majority of the detected binary black holes are approximately equal mass aligned spin binaries. And for those type of systems, subdominant angular modes are pretty weak. And so this combination of factors led us to think that we probably would have to wait until LIGO Voyager or LISA before we had any hope of doing of detecting more than a single mode and doing black hole spectroscopy. This plot on the right. Spectroscopy. This plot on the right is from this paper that myself and some colleagues did, where we sort of estimated the number of events you could expect per year for different detector future detectors. And if you just focus on the light blue thing here, this is the number of events for which we thought you would be able to constrain a single subdominant mode frequency deviations to be within 20% of the GR value. 20% of the GR value. And you can see that only until we got to LIGO Voyager, which is sort of a future detectors that might be built sometime in the early 2030s, only then might we get even just one event per year where we could detect this. So that was the thought prior to about 2019. However, in 2020, there was this paper by Geissler et al. where they showed that looking at an NR signal in a line space. At an NR signal, an aligned spin equal mass NR signal, they showed that if you include overtones of the dominant mode, then it seemed as though you could apply this quasi-normal mode description of the signal right at merger. So the plot on the left here is showing mismatch versus time, where time zero here is the peak of the signal, so essentially merger. And the blue line shows what happens if you only include the fundamental mode in your Include the fundamental mode in your signal model, but then each of these lines shows as you add more and more overtones to that, what happens. And essentially, as you add more and more overtones, the mismatch, so basically the smaller values is better, the mismatch gets better as you get close to, and even as you get closer to merger. And so even if you just include a single overtone at merger, the mismatch falls to around 10%. Whereas if you don't include the overtone, it's nearly Overtone, it's nearly 90 percent. Um, and so this seemed to indicate that you could maybe use quasi-normal remote description at merger. And if that's the case, then you can analyze right at merger where you have much more signal-to-noise ratio, and you can use these different overtones to do black hole spectroscopy. And so, taking that, they then applied this to the first detection, GW5914, and sure enough, they found that. 14. And sure enough, they found that if you only included the fundamental mode in your signal and you analyze right at merger, you get biased results. So, this plot here on the left shows the estimated final mass and spin for the signal. The blue line shows what happens if you just use the fundamental quasi-normal mode. The dashed black line here shows what you expect from the full IMR template. And you can see these two don't agree with each other. You can see these two don't agree with each other. But if you just even just add one overtone and that gives you the yellow line, suddenly you have very good agreement with the full IMR signal. And so then using this, they then looked at what happens if you allow the frequency and the damping time of this first fundamental mode to deviate from GR. And that's what the plot on the right here is. So you would expect the GR value is 0, 0. And you can see that they got... And you can see that they got a result that you could say was broadly consistent with GR. Even though the SNR was strong, you couldn't still produce very good constraints with this, but at least it included zero in there. The most likely value is a non-zero deviation, right? Yeah, so this color map here is showing where it's a this is a probability density, 2D probability density, and where it's darkest is showing where. Density and where it's darkest is showing where the bulk of the density is. And yes, it is off of zero. But if you want to, you know, say it is still within the nine. Actually, I think this is the one sigma error bars contours. So it is still within. Zero is still within there. So this was interesting, but okay, so it gives you a result that's more consistent with IMR and kind of consistent with GR. Consistent with GR, but what is the actual evidence here? How do we know that we're just simply not overfitting the data by adding this overtone in? Well, in their abstract, EC et al, they said that they tried using the measured amplitude of the overtone to as a significance estimate for what is the belief that we have this overtone in here and that you can measure it. In here, and that you can measure it. And they claimed a significance of 3.6 sigma. Basically, where that came from is they take the posterior on the amplitude of the overtone, the measured posterior, and then they simply count how many sigma away from mean, from the mean is zero. Unfortunately, they didn't plot in their paper, they didn't plot the amplitude posterior when you just include one overtone. This plot here. Overtone. This plot here shows when you include two overtones, but the yellow dashed line shows the width, the one sigma width for the one overtone analysis. And basically, the idea was zero for the amplitude on the overtone was 3.6 sigma away from the mean. And so that's how they're trying to claim this 3.6 sigma confidence. However, this kind of way of looking at amplitudes and my Kind of way of looking at amplitudes, in my opinion, is a little bit dubious for claiming significance. And sort of as a counterexample, you can do the same thing with subdominant modes with the same signal and seem to get also strong evidence for other modes that probably aren't actually in the data. So this is what I'm showing here is a result where we included the 3.3 mode, the 3.3.0 mode in GW15914 and measured, and we seem to get a non- measured, and we seem to get a non-zero amplitude for the 3,30 amplitude. So this is showing the x-axis here as the ratio of the 3,3 amplitude versus the 2,2 amplitude for 150914, at least when we appear to measure. You can see this peaks away from zero, and we get a measured value of about 0.3. And if you take this ratio, if you look at how many sigmas away from the mean is zero, you get about two sigma. About two sigma. The problem with this is the 3.3 amplitude, this is almost certainly not actually there. The real 3.3 amplitude for the signal is probably much, much weaker. The reason we believe that is we're very confident that 150914 is roughly an equal mass binary. And so, for that type of binary, you expect the 3,3 amplitude to be quite weak. Another way to look at this is if you look in the lower left. is if you look in the lower left here. I don't know if this thing is in the way. Yep. If you look in the lower left here, I have a posterior distribution. So the orange line here is the posterior distribution on the mass ratio, where one is equal mass and zero is sort of infinite, essentially point particle. And you can see 5914, like I say, we very much expect that it's an equamass signal. Expect that it's an equamass signal. But if you assume this analysis for the A3,3 amplitude is true, then you get a much different distribution on the mass ratio. And so just simply looking at the amplitude of a measured quasi-normal mode, it isn't that great. If, however, you look at the Bay factor, so you calculate what is the Bayes factor for a model where you include the 3,30 in the quasi-normal modes versus a model where you just include the 220. Include the 220. The base factor is only 0.9, meaning that the data actually, at best, you can't say one way or the other, but if anything, the data slightly favors just the 220 mode, and that's more kind of consistent with expectations. So in this sense, in this analysis that you did, you just did not include any overtones. Is that right? Just looking at that. That's correct. We're looking at 10M after the merger. We're just looking at the fundamentals here. Yeah. So what are the Bayes factors then? So, what are the base factors then? So, if Bay factor then is a better measurement of significance, what does the Bay factor say for the overtone analysis? Well, it's not that strong. This table here is a table that's been produced by the LIGO-Virgo collaborations for all the binary black hole events for which they've done an overtone analysis. There's a lot of numbers on here, but I just want to highlight the red column here. The red column here, this is their calculated Bay factor for the overtone versus no overtone at merger. And you can see that for 5914, that's this number here, the base factor isn't very strong. It's only about four, which if you remember from that table earlier, it's borderline between not worth a bare mention and substantial evidence. So it's somewhat. So it's somewhat weak evidence. So this was at the merger, you said? So at T, whatever the colours is at the T colours here, would be? Yeah, so they're using as their model the fundamental mode plus the overtone at merger versus just the fundamental mode. But at merger means the coalescence time, basically. Yeah, coalescence time, yep, coalescence time, yep. And then further complicating the picture is even Complicating the picture is even this number is somewhat controversial right now. So, back in December of last year, there was this paper that by Katesta et al. that appeared on the archive where they used the pyring code. So that's again the code that's used by the LAK and has been used in some other analyses to evaluate the base factors as if you look at different time steps. Time steps around merger of 15914. So zero here is the merger time, and they looked at some times before and after. And the red crosses are what they got for the base factor. So this is log base factor. So zero here is one. And they found that actually they actually got even less than one. So slightly favoring, disfavoring the overtone. And they also did some injection analyses where they Where they performed, added simulated signals to the data and to see kind of what range and base factors they got. And they got this large range around one. And so basically what they were arguing was the base factor, not only is it not very strong, but you get this large range in base factors and you really can't trust this analysis at all, was their essential argument. Man, um, but one thing yeah, yeah, is a different number than the other one, right? Uh, sorry, say that again. So, so the number at t equals zero should be the same as the other one if yeah, yeah, so the number at t equals zero should be the same as the other one. So, there, so already you can see there's some disagreement in what the actual base factor is. So, they're getting something like, I think this is around 0.9, the LBK originally reported 4. Originally reported four. And as you'll see in a minute, we have even different results that get something different. So, yeah, let's focus just at t equals zero. So this is what Tess et L put out. The other thing, too, is they actually produced an amplitude posterior. If you look at the red line here, this is their amplitude posterior on this overtone. They actually got that the amplitude posterior included zero, whereas EC and FAR, if you're not going to be able to do it Whereas EC and FAR, if you recall, had zero essentially excluded at 3.6 sigma. So essentially the result disagreed with the C and FAR, AC et al., I should say. Then a few months later, back in February, however, EC and FAR published a rebuttal where they recalculated the base factors themselves, doing the same sort of thing, so time stepping, and they get completely different values for the base factor. So zero. Values for the base factor. So zero here should correspond to the same zero here. So zero, contested L get less than one for a base factor. EC and far, however, get something close to 100 for the base factor for the overtone. I think this value here is around 60 or so. And then if you just go half less than half a millisecond earlier, they get something close to 10 to the 4. So they get very different values. So, they get very different values. In fact, if you plot those on the same plot, this is what it looks like. So, ignore the green line for a second. Blue is contested at L. So, this is Bayes factor as a function of time. Orange is what EC and FAR report. And you can see very different. Now, as I said, there's a third pipeline, PISADC inference. So, we decided, we're currently working on the same problem, decided to take a look. On the same problem, decided to take a look at that and see if we can play referee and see what result we get. Our results are very preliminary. We're still working on this, but so far, that's what we're getting is the green line. So again, if you focus on zero here, sorry, the times are a bit shifted, but the dashed line is essentially zero at merger. We essentially get something in between them, although a bit more favoring A bit more favoring EC et al. And we should say that in this analysis, we're trying to do exactly the same priors as Catessa et al. So we clearly get stronger evidence in Catesta L using the same priors. One of the things we've discovered in doing this is EC and FAR are actually using different priors on the amplitudes in Catesta et al. And so part of this difference may just simply be due to prior, but there is appears to be some other difference. But there is appears to be some other difference that we're still not sure about, perhaps in the data processing that's causing these large differences. Okay, that's base factor. I do want to say, though, there is some other evidence, sort of circumstantial evidence for the existence of the overtone at merger. There's, if you look at sort of the change in area or area theorem tests using the overtone. Tests using the overtone. Essentially, when you include the overtone, you do get something that very much agrees with what you expect from the IMR results. The other thing is, if you look over on the right here, the LVK has been taking, like I say, they've been allowing the frequency of the overtone to deviate from GR for a bunch of events, and then they're sort of combining these posteriors over multiple events. And they've been finding that when you combine these posteriors, you're getting something that Combine these posteriors, you're getting something that's slowly approaching zero. And so, if it was the case that the overtone was just sort of overfitting the data, you might think that you would get something that would be, you know, there's no clear reason why that should agree with GR if you allow the overtone to deviate from GR. So, the fact that you are getting something approaching zero, it does seem as though including the overt. though including the overtone, it clearly is a better model than just using the fundamental mode at merger. Whether or not though this quasi-normal description where you include the overtones is a complete description of the signal at merger, I think that's still an open question. I have a plot later to show that, although I see I'm essentially almost out of my 40 minutes. So I don't know if I'll get to that. Anyway, that was supposed to be the first half of my talk, but there are Be the first half of my talk, but are there any questions about this before I sort of move on? What was your base factor? The base factor that we're getting, yeah, from PyC inference, we're getting something for here, we're getting something around 10 when we use exactly the same priors as Catessa et al. I should say though, these priors that Catesta et al. are using, they're, I think, overly broad on the amplitude. If you use what I think is a On the amplitude, if you use what I think is a little bit more of a sensible amplitude prior, we get something more around 100, so more in agreement with EC and FAR. Okay, I only have like three minutes left, so I'll quickly breeze through the second part. You can go on for, let's say, 10 minutes. Okay, I'll try and go fast. Sorry about that. Okay, so that was overtones. What about Okay, so that was overtones. What about angular modes? Well, if we look back at this table that the LVK has been producing, they've also been looking at what happens when you include subdominant fundamental modes at 10M after merger. And this column here is their reported Bayes factors for subdominant angular modes at 10M after merger. And you can see, again, they really have no support for any modes. Support for any modes. However, one of these events, GW190521, they had marginal evidence. And I'm going to show real quick that we did a follow-up analysis of this, and we actually found very strong evidence for a subdominant mode. So very quickly, 1905-21, this was an event. It's the sort of most massive, confidently detected binary black hole that has been detected to date. That has been detected to date. And because it's so massive, you really are just seeing the post-merger signal in it. On the right here is sort of the white and time series for the three detectors that were on at the time. The gray line here shows an estimated merger time from one of the IMR models. And you can see if you look at Livingston, that's the most sensitive detector. Really, you only have one cycle prior to merger, and the rest of the observable signal is really post-merger. Observable signal is really post-merger. And so, this is an excellent candidate for doing a quasi-neural remote analysis. And the other thing about this event was the initial analysis done by the LBK indicated that it was roughly an equal mass binary, so one for which you wouldn't expect subdominant modes to be very strong. However, follow-up analyses showed that there was actually there's a possibility that this event was actually an asymmetric binary with mass ratio larger than two. It would mass ratio larger than two. In that case, you would expect to see a subdominant mode. So, myself and my colleagues, using PiCBC inference, we performed a quasi-normal mode analysis of this event. We performed two analyses, an agnostic analysis and a curric analysis. In the agnostic analysis, we just allow the frequency and damping times to vary independently of each other for the multiple modes. And in the curric analysis, we assume curricul so the frequency and damping times are fixed. Curve so the frequency and damning times are affixed by the mass and spin. I'm not going to dwell on the details too much, but I'll just jump to the sort of the results from our results. So, if we look at this agnostic result, so essentially we had two modes that we separate in frequency. One mode is around 60 hertz, and this is where you expect the dominant mode, the fundamental mode to be. And we very much, this is showing the posterior on the result, so y-axis is. On the result. So, y-axis is damping time, x-axis is frequency. This is what we call range A, where we allowed one of the modes to vary. And you can see it locks right in on the dominant mode around 63 Hz. And this is what you expected from the IMR results. However, what we also found, which was surprising, was we found a second mode at higher frequency, roughly around 100 Hz. And you can see there's this posterior distribution that seems to be locking in at something around 100 Hertz with an. At something around 100 hertz with a damping time around 20 milliseconds. Now, what was interesting about this is if you, again, this is a result where we were not assuming a Kerr solution for the relation between the quasi-normal modes. However, if you assume this thing that we found around 60 Hertz is the fundamental mode, the 220 mode as described by a curve metric. You can use that then to produce. You can use that then to produce a posterior distribution on the final mass and spin, and that gives you this. If you then take this and use this to predict where you expect the 3,3 fundamental mode to be, what its frequency and damping time to be, you get this, this blue contour. And you can see that the predicted 330 frequency and damping time lies right on top of this thing that we found in the data. This thing that we found in the data. I've also shown here where you expect would expect the 440 and the 550 modes to be. You can see that you don't really see anything there in the data, but that's actually kind of expected for this type of system. So the fact that we got this 330 here and not the others is very much what you expect from GR. We then did a current analysis to follow this up. I'll skip the details here, but I'll just say that. The details here, but I'll just say that doing a current analysis at different time steps, we ended up finding that the base factor where you have the 330 and the 220 versus just the 220, it peaked out at around 44 at one particular time step, about seven milliseconds after our reference time. And so at this point, you would say that the data, it's more than 40. It's about 40 times more likely for the 330 model to be correct than just a 220 only model. And so this is very strong evidence for the 330 being present. We've done some validation stuff on this. I'll skip this. So given though that we've identified possibly that this 330 is in the data at this point in time, we then did a black hole spectroscopy test where we allowed the frequency of this 330 to deviate from GR. Deviate from GR. And basically, we got a very good agreement with GR. Essentially, we constrained the 330 to be within 11% of the GR value at 90% confidence. And this is a very good number. Like I said, if you recall from that plot that we had earlier, before this event, we didn't expect, we thought we might get a 20% constraint in the 2030s. And so we're already getting 10% constraint. 10% constraint. So I think this bodes very well for the future. The reason why this is happening is this event is much more massive, and the asymmetry in the mass ratios appears to be larger than what we had expected from previously observed events, which I think indicates that this event probably came from a different formation channel, probably dynamical capture rather than stellar collapse. Okay, I'm really running out of time. Okay, I'm really running out of time, but one last thing I want to point out: going back to the overtones, we also did an overtone analysis on this event, and we found very strong evidence for the overtone. However, when you do a GR test with this, you get something that's very much away from GR. So this is where we're allowing the overtone to deviate from GR. And so again, 0, 0 here is what you expect from GR. But what you can see here is that the data very much was peaked away from that. Was peaked away from that 0-0 value. I mean, 0-0 is still within the 90% credible interval, but only barely. And so, really, this kind of seems to violate a GR if you assume that this overtone signal is a complete description at merger. However, we've done some follow-ups where we've used some injections, so really true GR signals, and we repeat this analysis. And sure enough, if you use this, just the 221 plus. Just the 221 plus 220 at merger, and you allow it to deviate. In general, you find that the posterior ends up peaking away from 0, 0, even with GR values. That is not the case for the 3, 3, 0, though, roughly 10m after merger, and you get something more consistent with what we found from the real analysis with 1905-21. So at least for these. One. So, at least for these types of asymmetric events, this would seem to indicate that, at least at merger, that just including the fundamental overtone, an overtone of the fundamental dominant mode, sorry, isn't really a complete description of the merger of the signal at that point. Anyway, I've gone way over my time. I apologize for that. I'll just leave my conclusions up and say thank you. Okay. Thank you, Colin. So, any questions? Yes. Can you hear me, Colin? Yep. Okay, thanks for the talk. So your analysis were showing, just to double check, the 230 mode, is it a different event from what Max and you were doing there on the 221, right? Or is it the same event? Yeah. Yeah, yeah, no. Sorry, just to clarify, they're different events. So, the event that Max CC and company were with the overtone was the first detected binary black hole, GW150914. This event that we're claiming is detection of the 330 is 1905-21, which was much more massive. The final mass, estimated final mass of this event, well, it's in excess of 157. It's in excess of 150 solar masses, whereas 5914 was around 70 solar masses. And then in your this last slide, you showed that you had some more tests for the 221. I couldn't get the partial building. So is it still outside the care value or is it getting better? I couldn't get the data. What's the outcome of this policy? Yeah, sorry. So you're talking about this plot here. You're talking about this plot here. Yeah, yeah, because the previous one is that the 230 kind of agrees with the care value. Yeah. But the 221 is a bit outside of the expected care number. But here, is it getting closer to care or is it still outside? Yeah, so yeah. So yeah, sorry, let me explain this a little bit better. So, well, short answer is it appears to still stay away from Kerr. So what we did here is Is we simulated a bunch of GR signals using GR templates, added them to some data, and then do this black hole spectroscopy test where you allow the frequency and the damping time to deviate. On the left is where we're looking at merger, and we are including just the 220 and the 221 mode. And so these are all GR signals. Each one of these contours corresponds to a different simulated signal. Corresponds to a different simulated signal and the color of the contour, the contours are colored by the Bayes factor for the overtone. So we have very strong Bay factors for these simulated signals. And this very light yellow line, the base factor for the overtone is 10 to 12. So the data, if you didn't know if this was a real signal, you would look at this and you would say, oh my God, the data, it's really screaming that yes, there's the overtone is a correct. That the overtone is a correct model. But when you allow the frequency and the damping time of the overtone to deviate from Kerr, you're getting something that's peaked well away from zero, even for these very loud signals. On the right is doing the same thing, but looking 10M after and using simulated signals that are more asymmetric mass ratio, so where you expect the 330 to be there. The 330 to be there. And there you can see that the contours are generally centered more on zero. They're not perfectly on zero, but it is much better. Honestly, I wouldn't expect them to be perfectly on zero because these signals have more than just the 3-3 mode in them, but our signal model is only including the 220 and the 330. And actually, for that, it's doing quite well, surprisingly well. The scale of the right-hand plot is a bit larger than the next. Is a bit larger than the next axis, yeah. Yeah, I do actually. Fair point, I have a plot where uh it's on the same axis, so I'll show that. Uh, so this is uh same plots, but now the limits are the same on both. Um, actually, I should also mention on this plot, the white lines here are what happens when you multiply all the posteriors together to get a combined constraint. And you can see with the 330, again, you have this double peak thing. Again, you have this double peak thing, but one of them is centered on zero. I think the double peak here is probably picking up on one of the other modes. Like, like I said, these simulated signals have more than just the 3.3, like they have the 3.2 in them, but our signal model only had the 3.3 in it. But if you look at the overtone, you know, the white line, it's way off. It's less than, well, yeah, you can see the white line's way off. So can I ask just a couple of questions? Yeah, I think there's a Questions, yeah. I think there's a question from Abai Ashtakar. Yeah, so I the one question, two quick questions. The first is that you know, in the beginning, you gave us a long table, but