No share now. It should work. Okay. Yep. Okay. So this is a talk about weak rates. My co-workers are William Saulkett, who will speak later this week. Is he here? Is he around? Okay, anyway. And Thomas Peter. Hi, William. Nice to see you. Hear you. Okay, so we are all in Berlin. That is unless recently. So, William, the Unless recently. So, William defected and is now somewhere in the US. All right, what's the problem? Weak rates. So, you have some random variable of interest and then some approximation, weak approximation, presumably, and you have a class of test functions. And so, the aim of the game is to compare this guy here with this guy here, where you use the approximation insights. So, this is an error, and that depends. This is an error, and that depends clearly on n. And you know, just to fix the language, we say that there's a weak rate gamma if this error goes to zero at this rate gamma. And so that should be true for every fixed test function in your class under consideration. So this is totally generic and just definition and language. Now, let's be more specific and look at what's called rough volatility. So you don't So, you don't need to know anything about rough volatility, but if you have some interest and fable for mathematical finance, then you probably know that this has been a success story in quantitative finance for the last eight years or so. So, okay, so you have a kernel here. It's called the Volterra kernel, and we shall exclusively work with this very particular form here. So, it is exactly what you think it is. It has a singularity when H is. Has a singularity when h is less than a half. I mean, you can, you don't have to rule out one half. It's good to have somehow a sanity check for the Browning case. Otherwise, you will end up with a Volterra process. And because of the singularity, it's worse than Browning motion. And this is why people talk about rough volatility or rough process is not fractional. I mean, fractional would be H, whatever you want, but rough is a hint that H is less than a half or potentially less than a half. So that guy called Riemann Leowelford. Called a Riemann-Lierwell fraction of Brownian motion. H is, of course, the Hearst parameter. And a shorthand notation would be that you think of K as being convolved with the white noise that's hiding here. And throughout the talk, H is fixed, so there's no need to carry it along, but we need to distinguish it from W, or we need to distinguish the fractional Brown motion from the Brownian motion that sits underneath. And so we use the notation W hat. So watch it. W hat. So watch it. W hat is a fraction of Brownian motion and it's fully correlated with the underlying Brownian motion. So this whole story, there's just a single scalar Brownian motion. There's no Levy area, no nothing. Single Brownian motion exists. And so what you're interested in is a classical Wiener integral or eto integral of this form here. So here's your usual Brownian differential. You do eto integration. Here's some adapted process that you get exactly Process that you get exactly in the way that I described above. So here it is, it's just different. We're writing it that f is a nice function. I don't want to say too much about it, but it's not linear, it's not polynomial, it's somehow generic function, and you sooner or later need to impose some growth conditions because you want to take expectations. So I also want to keep track of the running time t here, although ultimately it may be fixed, but the analysis requires you to move some time and to do something. And the approximation here is what... And the approximation here is what's called the Euler or left point approximation of the eto integral. It's one of the first theorems you prove when you teach this stuff that these left point sums converge and the limit is the eto integral, etc. That's exactly what this guy is here. And so I don't think I'll use that notation much, but you can use it as a tilde if you don't want to keep the n for each ignorance. And the interesting thing is that you can write it in the very same form as. Same form as you know the limiting guy that you're interested in. You just have to do one little trick, you have to tweak the kernel a little bit. So it's enough to define some unmodified kernel where your t here is replaced by, you know, this little step function of t. So if you think of this, this is, if you draw the graph, this is exactly a step function. It always waits, it's always lagging behind. It waits a little bit, then it jumps up, and then it's constant, et cetera, some step approach. Some step approximation to the identity function. And that's the one you use here. So I should observe that, although here I don't use the kernel, I just use it on the simple x, but it's convenient to define it everywhere and just set it to zero when s is greater than t. Okay, so it's just a fancy, perhaps unusual way of writing classical left-point approximation. It is the same thing, okay? It is the same thing, okay. So classical versus non-classical. When h equals one half, I don't know a reference, but this is precisely in the form that we have looked at, but you can squeeze it easily in some STE setting. I mean, observe, we're not dealing with any differential equations of any sort. This is a stupid integration. There's nothing to solve. There's no fixed point, no nothing. There's no fixed point, no nothing. So you can place it in a somewhat trivial SDE setting, apply standard results. And what you know, what people know for the last, I don't know, 30, 40, 50 years maybe, is that you have weak rate one. Okay. So in contrast to the strong rate, which is one half, unless you do Milstein or fancy stuff, but the weak rate is one. So you do better. That's used every day when people do Monte Carlo. Now, what is the result for H? What is the result for h less than a half? I am very lucky because my chairman here knows the answer, but maybe you don't. So it may be 2h or h plus 1 half. Pick your favorite expression that works out to 1 when you plug in h equals 1 half. Okay, so why would this be very bad? And why would this be? Well, not so bad. Because h, so are we? H, so are we told by people in quantitative finance these days? H is very close to zero. That means in this case, here you would be damned that your Monte Carlo would not converge because to humanity would be nothing, you would not see convergence. Here, well, h goes to zero, you're still left with one half, we create one. So you might be okay. So certainly you don't want, sorry, you don't want anything that vanishes with H. Well, fortunately. This age. Well, fortunately, numerical evidence, again, people are doing this. I mean, although the model looks somehow like a cooked toy model, it is not very far from things that are actually used in the industry. I'm not having time to tell you that, but if you want to know a bit more, the introduction of Paul's paper on this actually makes a good case about this Romano-Tusi formula, etc. So, okay, so what could be the answer? First sign of hope. What could be Gansa? First sign of hope. This is a computation that has been contributed or attributed to Andreas Neuinkir, although I don't think there's any publication here. In any case, it's a quadratic test function case. So this function phi is not a quadratic function. And so all we have to do is to compute this guy here. So x is a is, you know, under reasonable assumptions on f, x is a Martigal zero mean. So this is the variance of it. Okay. So you do a little bit to calculus. So, you do a little bit to calculate into isometry, and you easily find this. Is the so here's the advantage of my software here. I forgot the expectation. So, here we are fixed. Let me save it. This is just what you get by etosometry. And now the nice thing that happens here is that you take the expectation of something that depends on your fractional farm motion only at some time s. So, you're not looking at the process, you just look at the marginals. The marginals is a Gaussian law. There's a Gaussian law. So, if you want, you can rewrite in terms of the heat semigroup. So, let me say P is the heat semi-group. You just have to evaluate the different time, not S, but S to the 2H. This is the equality. I mean, what do you have, the fraction parameter scaling or just the scaling here? And then you can use facts on heat-semi groups, like, you know, when you move forward in time, then you have somehow the Slipsitz estimate, and you apply this for reasonable F, you apply this for to this time. For to this times, so the original time skills this one, s to h. And then, if you compare it with the other guy, remember this eta function. This was the step function approximation that makes s somehow lag behind and kept constant at this size, mesh size one over n. So this guy here up is one over n close to this guy here. And the function two to the power h, although you may always think it's a Holder function, it's actually most of the time it's. It's actually most of the time, it's a perfectly nice Lipschitz or C-infinity function. You just have to stay away from zero. Okay, so most of the time, this is just, you know, you can use the Lipschitzness of this function here and the difference of, okay, it should be obvious, okay? So again, this guy here is one over n close to this guy here. And unless you're lurking around close to zero, so or if you don't do that, then this is the rate here. Okay, so why doesn't it matter? Why doesn't it matter? Why is it irrelevant what happens at time zero? Because you integrate from zero to some macroscopic time t. So, what happens at zero is quickly washed out and you keep this rate here. So, this is my two-line explanation of why this works. So, what did we show here? We showed something seemingly surprising that no matter what age, age may be as small as you wish in this computation, the rate is always one, which is the sum. The rate is always one, which is the same we had in the classical setting for Planck motion. So that's weird, right? I mean, it's obviously true, but it's not 2h, it's not h plus one-half, it's one, at least in the case of quadratic test functions. Okay, so here are some names, by the way. Andreas Neuke, he's the guy who proposed this previous result, proved it, or argued it. I don't know. There's a paper by Robert Pone with Christian Payer, the first one where they use some, I try to not go into religion. I try to not go into the literature, but they use some Markovian approximation to fraction power motion with a large bunch of correlated Ernst-Ulmbeck processes. Then they use somehow facts for Markovian SDs, known facts, and surprisingly enough, they are able to pass to the limit with uniform estimates and get some rate, which is not optimal, but I don't know it by heart. There's a follow-up paper by Fukasava, also with Spire, and I'm ignoring some co-authors, my apologies. Some co-authors, my apologies that I don't know personally. So, there's another guy in this paper with Fukasava, there's another guy in this paper with Carl Temprone. So, my apologies. And there's one note by Paul, and he knows all about it. And I think this is the state of art. Certainly, for what I'm talking today, this is the most relevant. So, what he showed, amongst others, I need to emphasize, is that for cubic test functions, so now we're looking at phi of x equals x to the cube quote. x equals x to the cube or two x to the power 3 and fairly general f so again that's reasonably general let's say you have a weak rate of the form hold on 3h plus one half and of course it can't be better than one so there's a kept one so let me decipher this for you if h is greater than one over six this guy here happens to be bigger than one and you dead end at the rate one compared if h is going to zero well then this whole thing goes down To zero, well, then this whole thing goes down to rate one-half, and you know, you're still reasonably happy or hopefully so. But in any case, h equals one or six. You know, I didn't spell out the decimals, but from practical point of view, that's reasonably small. So I think this is a regime that's quite relevant in practice. And there you have a rate one, which is the best you can hope for. So this is a cool result. And moreover, he has some numerical evidence that this is really sharp. So, okay. And that works for curic test function. Okay, and that works for cubic test functions. So, Paul left this as an open problem to do anything more generic here. So, again, f is this function inside. Let me repeat the f. You'll see the next slide. So, what are we showing here today, or what we're talking about at least? This is the result with William and Thomas Wagenhofer, a very smart PhD student in Berlin. We have the following estimates: so, Romanta. Have the following estimates. So, Romana, remember, here are the test functions: phi. So, Paul took here cubics, and we say that works for all polynomials. And so, what works? Well, the same thing. So, xt is still our veto integral, and f is still the, you know, some people call it volatility function because this guy means to finance this called the volatility. And so f is very general for Paul and very general for us, and I'm not going to give you details, but it's you know, as general as we think is reasonable. As we think is reasonable, and here's the Euler approximation as before. And what is the rate? Well, it is pulse rate 3h plus one-half, and it can't be better than one. So here's the rate one. So we extend pulse precision. So how do we do that? Well, right. The good thing about the remark I made earlier, and this is really a remark. Earlier, and this is really a remark that you can also find in Paul's paper. Let me jump back here: is that up to changing the kernel here, these problems have the same forms. So the aim of the game is, or what we're planning to, is understand really well, understand really well this one guy here, and then we at the same time understand really well this other guy here, because it's the same story. Here, because it's the same story just with a different kernel. Okay, so okay. So let's do some computation here. So let's focus on this one and see what we can say. First thing we can say is we can apply Ito. Okay, so and here is a mod Martigal increment. So in the Ito formula, you get an extra term, which under reasonable conditions on F is a Martigal increment that disappears when you take the expectations. So I just wrote you out there, you know, that. Wrote you out the you know the term that doesn't appear, disappear when you take expectations. And now I need to, you know, hope you remember your Marley Bay calculus. There's something called the Clark-Occundan House formula. And I'm not sure if Bruno is here already with us, but if he is, then he may tell us that there's a functional etocalculus version of this that I have not tried out. But the computation is so simple that I don't think there's much interest. There's much interest in coming up with fancy variations. So, what does the Clark or Cohen-Hausmann formulate tell you? It tells you: if you have here, think of this as something that's defined on Venus space up to time t here, then C would be the expectation. And then what you have here is all the rest. Okay, so whatever. Okay, so what does the Clark Hausman formula say? Well, here you have to take what people call somehow the Have to take what people call somehow the diagonal malevolent derivative, or so. So, inside, you take the derivative, which is what you get by somehow poking your Brownian noise at time s. And then there's no reason for this guy to be adapted. So, you know, if you think here's here at some later time t, if you broke your Brownian path at time s, then you know, the outcome will still depend on what happens later. So, this is in general, there's no reason for this to be as adapted or fs adapted, so it won't. Or FS adapted, so it won't. So you have to take the conditional expectation. That's a famous result. This works, it has nothing to do with the particular form of that random variable that happens. It works very general for, I don't know, L2 functions on Linear space and even for Heater distributions and God knows what. So, and you can compute this. Let's do it. The Mullen derivative. Okay, there's a chain rule so that the F squared turns into 2FF prime. And then you have to compute the Mullen derivative of this W hat. But if you look at the explicit form, You had, but if you look at the explicit form here, where is it? Job, job, job, job, job. Maybe I should not have done it. So you can easily figure out what happens here if you poke the branch noise at time s here. You just pick up this factor here. So let's go back. Here we are. This is what the Malevin derivative spits out. Usually, the problem with this Clark or Kuhn-Hausdorff or Hausmann formula is that you have this projection here, and so you know, you have. Here, and so you know, you have to do something to get rid of it. But here it works out very nicely. So, let me show you. We also know that dx has some martingal form. I mean, it's by very specification, x is the integral of something against dw, and this something is this rough volatility, and here it is. So, two here is this here, and one here is this guy here. And I realize I should not have done this. I should not have done this because the conditional expectation here, the projection is here, and here I just computed the mullion derivative. So, see how nice my software allows me to fix my own mistakes. Now, by the way, let me do even better here. I think that's it. So, where are we? Well, remember, I want to take the expectation here. That means we take the expectation here. I bring the expectation inside and now I. Expectation inside, and now I can use etosometry because this one I can write as a dw integral, and here it is. And this guy, too, I can write the dw integrals. Here it is. And so by etosometry, I know what this becomes. It's just the expectation of, you know, of, I mean, this is basically to calculate etosometry. I have to multiply this guy here with this guy here, and then, you know, sum it up. And this is the etosometry. And there's still the big. Eclosometry, and there's still the big interval here from zero to t. So, in the end, if you plug it all in, you end up with this one. So, let me just emphasize here that, strictly speaking, I should have plugged in not one, but this guy here. But if you play around with the chain rule or what's called the tower rule, you convince yourself this is FS measurable, so you can bring this inside. So, it nicely disappears, and that's very useful. Let's get rid of this again. How by? Get rid of this again. How bye-bye, and here we are. That's the answer for the cubic case. And this expression here is, I think, in Paul's paper too, somewhere he can tell us better than I can. All right. Now, the good news is that this scheme can be iterated. So, if you play around with Ito formula, Ito isometry, and Kiaku-Kohn-Hausmann, you can keep going and things become bigger and bigger. So, I'm not showing you. And bigger, so I'm not showing you details about this one, and I'm certainly not showing you details about this one. So, just to tell you, it's possible, and someone needs to help you to organize this mess here. Organizing the message. Two or three minutes left. Oh, wow. Are you kidding me? Okay. All right, then, in a nutshell, in a nutshell. So, what we're interested in is this here for general. In is this here for general powers n. Okay, at least that's the aim here. And it turns out if you want to get some recursion running, you need to be a bit more general. This is often in induction proofs. You prove something a little bit stronger, then it gives you a stronger induction hypothesis that allows you to keep going. So you have to do something of this form here. And I'm not being able to explain you all that. Suffices to say that you sum here over certain words that just formalizes what Ito does. And these words you. And these words you integrate them here as operators on the f, which is exactly what ito does. You spit out derivatives on f left and right. And so, in the end, you end up with this big sum. And then you take f equals one, and that gives you sort of the answer. Sort of. Sort of means you need to be explicit about this guy, otherwise you're going nowhere. And you can, so this you can be explicit, hack it out. So you get a bunch of kernels. Why do these kernels show up? Because you know, remember when you do the exact. You know, remember when you do the Clarko command, you get out a kernel, and every time you keep iterating, the new kernels pop up. So, this is formalized here, and you can be explicit about this size here, too. So, there's one big sum here, and for each of these guys, there's another sum here. So, it's a bit messy, it's big. But in the end of the day, you have a linear combination and finite for fixed polynomial. All these things are finite linear combinations of this form here. And if since you have to plug it in, you have to plug in your fraction. plug it in you have to plug in your fraction parametric so you end up with expressions of this form here so i'm sure paul has i'm sorry i know paul has encountered them as well but you know probably for for less i mean less complexity so and then you have to start estimating so um time is running so i can't really read them all but what should be obvious is if you do it for the original k and then for the changed k for the changed kernel k so so this guy here you know is the You know, is the one where you're lagging behind using your step function approximation to the identity time here. So this depends on n here. So this guy here and this guy here as vectors component-wise, they're one over n close because of the nature of this approximation function here. And so you write it as a sum of little updates where you just update the k slot from dk to sk, this is easy. And the algebra for k is similar. Algebra for K is similar. The estimates are not. So I'll focus on what has to be done here, time permitting. So if you go back here, what we have to do still here is we have, oops, what's going on? Okay. We still have to integrate, right? I mean, if you go back here, there was this big integral over some simplex. And so, and there is also these cases here. So if you put it all in. Are here so if you put it all in, you have to integrate over some simplex here. Look at these update functions here, and then there's some kernel stuff here that I don't want to talk about here. And then it turns out that you can get the required estimate. So the desired estimate is pulse rate, provided this has nothing to do with the precise structure. You just need some good estimates on these five functions. And so it turns out. And so it turns out what you need is some estimate of this form here. So if you think of the phi function, let me remind you what the phi function is. Here it is. It's some multivariate expectation of some fraction of Brownian motion vector. You need some estimate on the derivative here in the J slot. And if you can manage here to have such an estimate, so of course this t vector here consists of these tms, t1, then These TMs, T1, then you're in business. So, this requires two pages of computation, but this is it. So, how to get there? Well, you need to say something smart about derivatives of such expressions in the variables. And this is, well, we found a slick way to do it. And so we fix everything else. So, you know, we want to understand the derivative in one slot. So let's fix some index j and look at the tj variable only. That's the variable of interest. Let's call it t. That's the variable of interest. Let's call it t. Everything else is fixed. Then the estimate we want follows from a little lemma that I want to show you. The proof is not very hard, but I've never seen it before, so I like it very much. And here it is. You can enjoy this lemma without anything else of this talk. So if you understood nothing till now, you can still join. So xt is a multivariate Gaussian that depends on a real parameter t. It's centered and there's a Parameter t. It's centered and there's a covariance function that depends on t. And you're interested for some reason at the dependence of something like some test function g, invalid at the Scaussian vector, you take the expectation, et cetera. And so the statement is that this function here, provided g is nice. So here we take it the Schwartz function later. You can approximate and make way more general stuff. So you probably need, well, you do need C2 and then some bounds. So everything exists. Some bounds, so everything exists, what you write down. But the proof, the first proof here, where you have the idea is for Schwarz functions because you use free analysis, and then you have this eto type formula, or you know, it looks like what you would get from Ito. So if you apply Ito to a Gaussian martingale, then you would get exactly this. But in general, you can't use that trick here because there's no reason why this guy here, so you have the derivative of the covariance matrix coming in here. In general, there's no reason why this would be positive semi-definite. Positive semi-definite, so you can't take a shortcut over Gaussian martingales, but there's a direct foul proof it does it. So I'm done. Thank you very much. With other questions? Quick. Yeah, so it might be expected that the weak convergence rate. Weak convergence rate should be maybe twice a stronger convergence rate. Is it something you have in your case? So that's what I tried to convey at the beginning. It's not true here. So, you know, strong is H. And that was the initial guess that I presented here. That would be twice, this guy here would be twice the strong rate. And that's simply. Simply not optimal. I mean, it's not false, but it's not good. Yes. So that's very easy to say. Okay, thank you, Sonet. Thanks, Peter. Peter, yeah? Okay, thank you. It stopped a lot, it's all very good. Stop share. 