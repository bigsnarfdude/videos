I had two talks at least that mentioned something about interpretable models and explainability techniques and I will be talking about the explainability techniques. So although some people consider interpretable machine learning and the explainable machine learning interchangeable, this might be more accurate. So I'm actually a PhD student at the University of Warsaw, and my background is in computer science, so there will be, I think, one equation during these slides. And I'm also interested in explaining machine learning, more so on evaluating explanations and robustness. So I'm looking forward to the remainder of this workshop. But for the purpose of this workshop, I thought it might be better to talk about supervisor analysis because there's, I presume, a lot of statisticians in the audience and some medical applications, which is part of my work with people from our lab. From our lab. And I also have been working on SALSCA software, so just implementing those post-war explanations for machine learning models, either for Python or for R. If you are into interactive visualization, you might check out the Model CD package. And if you are into Python and the toolbox for external motion learning, you might check out the DAI package. And Subrex, I will talk about more in this talk. So, actually, the question. So, actually, the question I want to discuss here is how to explain machine learning solar models. And over the last one and a half years, we are working on this problem with many awesome people from our lab and not only. So, I will talk about the method or more like a mindset maybe we propose. And then I will briefly mention a software we recently published in bioinformatics. Published in bioinformatics. And then I will show one cool application, I think, that illustrates maybe how these explanations can help us to debug machine learning models. I'm for sure not claiming that we solve the problem. I think it's a cool question to, research question to work on. And I think there are more open questions than working on these problems. On these problems, made us aware of more even more open questions. So maybe you get interested and can contribute to it more. Okay, so first I'll start talking about time-dependent explanations and what I mean by this time-dependence. So here I present like two probably very well-known machine learning like explanations for machine learning models for either classification or regression. Classification regression, so it's a single-value prediction. So we would be accustomed to if some of you might know the Python sharp package, the paper has over 15,000 citations by now. So what it would do is kind of decompose the machine learning prediction into attributions of each feature. So here we are trying to predict accurate kidney injury, I presume. I I presume and there are some some features and the the model predicted 0.45. For this patient the the average model predictions is 0.35 and you can see which feature was the most important, how this prediction decomposes into those components. So it's a black box function that can be kind of this prediction can be approximated with sharp values. And on the right plot there's kind of an addition to that. So for the most important features So, for the most important feature, which is protonine, you can see how the prediction will change if the value of this feature changes. So, presumably, this was a decision, three-base model, and there is some split at 90, and the probability decreases a lot. So, this here we are trying to explain one value, right? The probability, and what about time-to-event prediction, like extra analysis. So in 2020, there was this survey method proposed, as I think one of the first and most cited now to answer this question of how to explain survival models. And it's based on the Lyme method, which is very popular in explaining machine learning. It's locally interpretable model agnostic explanations. And I won't dive deeper into the market behind it, but the main idea is to approach. The main idea is to approximate the black box function, so it might be your Lando Forest model or your deep neural network. It's an interpretable model in the local neighborhood of observation X. And this is the quote from the paper, which was also in one of the previous presentations, similar one. So for this point here, we would actually sample a neighborhood and then fit this local linear model to kind of explain this black box function, which is. This black box function, which is the decision model here. And so, what people did in this survey method is kind of try to adapt this LINE framework to cycle analysis. But although it was a really cool idea to initiate work on this problem, there were some limitations. So, in short, they were kind of trying to expand again Lago Severa model with, so there were something in the neighborhood observations, they were fitting a quote. They were fitting a quantum proposal-Hazard's model and kind of taking those coefficients of the coefficients model as those important features. And they arrived at such explanation. Here are the coefficients for some problem, and this is the explanation for one prediction. And here in the right we can see scrabble uh functions, so how how the explanation approximates the Labox model Labox function. LabOx model LabOx function. And there are a few limitations of the method that we thought might be addressed. So, first of all, this explanation doesn't have time dimension in it. We thought that if we kind of go into this task that actually many people consider the type to be really important in those rockers, etc. Then, probably the explanation should also kind of show us the time dimension. Also, here we have. Also, here we are approximating a complex model with linear coefficients. Might be a problem because, in some sense, the final explanation might not even add to the final survival curve, as shown here. And then, finally, the coefficients are not important, right? So, they kind of depend on feature values. You can say that the higher the coefficient, the more important the feature is, because if you are considering like Because if you're considering like height and weight of the person, it will differ. So maybe they should have used p-values, which might be considered more appropriate, but I'm not sure enough. So we tried to address those problems. And this is how we finally arrived at the SwiftSharp methodology, which is in a sense, we'll be trying to explain not the model, the prediction, but we'll try to explain the whole. Will try to explain the whole survival function prediction for a given observation. And we'll arrive at those curves that are actually attributions of each feature to this survival curve. So they will kind of sum up to this curve. And then if you aggregate those, I'm sorry, integrate those curves or maybe aggregate with different measures, you will arrive at the local importance for a given observation, which is only the Which is only the high-level idea. So, this idea of this paper is to first adapt kind of very popular sharp values, which rely on sharp values from game theory to maybe alleviate some of those limitations. So, sharp and lymph are very popular in the literature, and we thought this might solve some problems. So, the general idea for sharp is to use some. The general idea for sharp is to use some game theory to estimate additive feature attributions, those three i's, i is for a given feature, to the modest prediction for observation x. And you arrive at this nasty equation, which I tried to only shorten, like you might be surprised wait in this equation there is no f, just a remote function. And actually, what this equation does is it goes over all permutations of your features and try to kind of evaluate. To kind of evaluate how much does adding this feature to the subset of features increase or decrease the estimated expected prediction. So there's a lot of theory on how to estimate those value functions in game theory. And by now, there are over 24 algorithms to estimate shape values which you are told. There are different algorithms. There is, because of course, computing this for Computating computing this for all subsets is like two to the power of p. So, if you have like ten features in our data set, it will already compute very long. Exactly, big values are really hard to compute. So, people came around to kind of maybe sacrifice some approximation error to make the computation more efficient. There are a few algorithms that are like Kerner Sharp, Twitchharp algorithm, which is really fast for three-page. Really fast for three-base models. And then there is also a huge discussion on should these expected values be computed with regard to marginal distribution of our features or conditional distribution of our features. So of course you would like to compute them with regard to conditional distribution because then we are on data manifold. But estimating conditional distributions is very hard in practice, especially if we increase the dimension of our features. So there are people that are fans of There are people that are advanced for this approach, there are more people that care more about this, but maybe that's not key information here. So finally, we kind of try to use this concept to create a new method. And it's not only adopting the method to new tasks, but I think it's might be trying to propose a new mindset on how you should. Mindset on how you should explain the survival function, not kind of pointwise prediction, because in this domain the time dimension is important. So, finally, we arrive at such a plot where actually in an explanation, finally, we have the time dimensions. And this is an explanation from some cancer prediction, I believe, survival of cancer. And here we can see that, for example, this red feature, this kind of Feature is kind of really decreasing the script function a lot in the prediction of our model. And somehow we can now think on what we would consider be the important feature. So our idea was maybe to integrate the area above or under, if you're here, the curve to kind of say how important is the feature. But of course, you might propose other measurements like compute variance. Was other measurements like compute variance or other aggregate statistics. So here is an exemplary aggregation of those into local feature importance and then we see, okay, this feature was the most important for the model to make the prediction. What is more, sometimes it might be that your black box model actually has, I don't know, in the first three months, it attributes negative attributions to a given feature and then it changes. Attributions to the given feature, and then it changes. So, I don't know, after three months, it actually gives positive attributions to the given feature. So, then if you use Superim method, it won't appear on the plot because it only has one coefficient. It can kind of diverge over time, but our method show in the paper, I won't be showing all the plots here, can kind of capture the time dependencies in e-features. So, we try to So we t tried to we we like chose five metrics, I believe, to compare different methods and also in the paper we show a use case, this cancer use case. But I wanted to only show that the one plot which kind of maybe explains intuitively why our method might might be better, but of course I'm biased here. So what we did is in this case first trained the the the interpretable model The interpretable model to kind of to this data set. And here we ranked the importance of features in this model. In this case, it was, I believe, p-values, and then in another case, in Subforest, it was just the gene importance. And then we were kind of looking at how many times in those local explanations over the whole data set, the features appeared as most important, like the first most important, second most important, up to eight. Most important, second most important, up to eight most important. So there were like up to 300 observations in a data set, and for 130 observations, this H was the most important feature. And for 122 observations, actually, the ejection fraction was the most important feature. And we were trying to visualize is there like a correlation between the ranking that is encoded in interpretable model, right? Encoded in the interpretable model, right? And the explanation. Of course, it doesn't need to agree one-to-one because actually this is the aggregation of local explanations, which doesn't need to correspond to global explanations at all. But intuitively, hopefully, there would be some overlap. While here, if we use sort line methods, different weird stuff happens. So, for example, the sixth most important feature in a model, an intertable model, actually appears. Minute Pressable model actually appears as the most important for 150 observations, which for many people might be disturbing. And we did the same for Rennon Survival Forest. And here also, I guess the aggregation of those surveillance explanations were correlated with this ranking of features which we can extract from variable importance measurements in forest. Forest. So, of course, evaluation of explanations is tricky, and I'm up for discussion on this topic itself. But in the paper, we try to show maybe differences and for people to choose what is more intuitive in that case. So, now we have a method. Great, we would like someone to use it? Oh. Yeah, you could go back. So, I was trying to understand what you were saying about the difference between the global and the local. So, are you saying that? So are you saying that there's something off about what Lyme is doing that it's missing, or is it just a different interpretation? So maybe not even Lyme, but if you consider this plot, so for each observation in your data set, you would create a local model, and then it might be that actually your model is overfitted enough that if you aggregate local explanations for each observation, the The output will be drastically different than if you only use some method to get global parallel importance. So, intuitively, why? I guess that's what we're doing. Because the decision boundary might not be smooth enough, and then at different points, you would have like those models would be not stable enough. I don't know if I can persuade you. Persuade you. Yeah, no, I'm not skeptical. I'm just trying to get the interview. So, actually, both of those models are not so complex. To be fair, we were trying to find a good use case where a deep neural network on survival analysis data has such a use case that can be reproduced maybe because deep neural nets actually are so unstable in training that they had a hard time and were using for those experiments. Experiments. But also, the Survelem paper didn't add an implementation of the method, so we had to implement it from scratch, which was really not so straightforward because there are some ambiguous epsilon values, I mean, epsilon hyperparameters without default values that you need to choose something, right? But yeah, the code is on GitHub. But yeah, the code is on GitHub, so everyone can play. Well, the model will predict the whole curve. Okay, so it takes the entire curve and that's your distribution for each feature, relevance of that feature in either direction at any point in time. Even if it's they had an event earlier than the whole step. Had an event earlier than the formal stuff. So it should be shown here. So, this also we had the discussion: what should be the average survival function? Because in Shapi values, as I shown the first waterfall plot, you kind of, the Shappy values add from the expected model prediction. So it's probably the mean prediction of your data set up to a prediction of a model. And those attributions are from one point to another, like relative. So then we are discussing. So then we are discussing if this mean survival function should be actually maybe a median because there is not such a clear interpretation of mean survival function, but then it doesn't add up if you take median, so there are some axioms of those shopping values to care about. But yeah, so for three features you'll get three curves, and then um if you add them with the mean, serial function, whatever it is, you'll get the serval function for this the um patient fee. Yeah, hence the T the name of the method kind of so we actually also changed notation. So first we followed the notation from sharp paper. Follow the notation from Sharp paper and we use like i for feature for feature index. But then we change it to so i is actually a parameter of the function and then we use t to the note that we kind of explain this function f t we treat it as like you would say classification model in point t. As time goes, the feature in code will change. Important will change. The transition will be more important than this passport. Yes, exactly. So it might be that here, up to day 50, like this green feature is more important than the red one. Because if you would integrate this area here, now technically you're seeing it would be more important. But then actually, after day 60, this feature is more important. At least for this patient. But but b but your f your feature importance is aggregated throughout the lifetime, no? Well, yes, so if you want to get general so we wanted to compare with the throughvel method which gives you feature import and local feature importance for one patient. So to do that we kind of thought okay let's aggregate it over time. It would make the most sense. But then you can also choose maybe you only care about up to 150. So you cut here and 150. So you cut here and integrate it up to this point. Yeah, because sometimes you're you're interested in feature that will contribute to uh survival or not survival early or later. Good point, thank you. Sure. So maybe I will fast forward through the software because also I'm running out of time. So the software package is quite simple. If you knew that If you knew direct packages before, we kind of tried to implement the same methods, not only Sharpy values, for this time dimension. It's quite interesting. Samplots are new to us, so if we want to add this time dimension to visualize variable effects, we need to use color. And it depends if you have discrete features or continuous features. It's only the proposition. We are working on that. We are working on that and also looking forward to pull that up by the software and want to maybe claim that other visualizations are better. So the code is pretty simple to use. We chose R because we think there's more people working with phones in R, but there's also a SuperShop package in Python if you want to use it. So we just create a model, pass your model and data to an explainer, and then you can, using our package, you can develop. Using our package, you can evaluate the model's performance using GNOME measures, but also you have access to all those outputs that I just shown. And then we have seen some adoption among not only our painters, but others trying to use our package in their work. So we are really happy to see this adoption and looking forward to see also more there. Okay, now coming back to this application, so we had So, we are working with medical data. We have some data from a hospital, and we are actually trying to classify diseases from X-ray images. But we had this column, how much time the person was staying in a hospital. And we have this tool, so I wanted to check how they will there be useful in such a task. So, we asked ourselves, at first I must say, a stupid question: to what extent can the patient's length of stay in the hospital be Can the patient's length of stay in a hospital be predicted using only an X-ray image? And yeah, we are looking for some correlation between what is in an X-ray image of a person and how long the person stayed in a hospital. Actually, a lot of people working on this hospital in Post Lay, but no one bothered to ask such a question. So this was a good problem to work on. And the motivation was this survey paper that several like 100 papers on hospital length of state prediction. And their conclusion was not many people try to explain their black box models, so maybe we wanted to show how to do it. And then, of course, this task, you might consider it as single-value regression, like how many days a person will be there, or actually time-span classifications, for example, will be longer than two weeks. It's a binary decision. But also, this survey says so, like we agreed that time-to-empt prediction or like server analysis in this case is. Or, like, software analysis in this case is more intuitive because also your data might be censored, right? Someone will die, someone will go to another hospital, only for some patients will know if they actually went out of hospital or not. So, we had this handcrafted data set, which is actually openly available on GitHub if you want to play with it. There were like 1,000 patients from Polish hospital, and their mean time in hospital was like 13 days, up to 300. days up to three hundred uh three hundred days and um and to get uh extracted from those extra images some features so our baseline was okay age and sex it's not connected with an image we have this two then we had some human pathology occurrences by physicians which had to kind of agree what is on an image or not and then we extract use this pyrademics tool to extract Tool to extract some mathematical statistics from those images to kind of make the problem tabular, as tabular data. And we benchmarked some data sets, some models. Didn't do much hyperparameter tuning, but we're looking for a good contender for interpretable models. And we arrived at breadth and boosting decision tweets to be a good contender. Of course, the scores are not so impressive, but at least we can see that there is some correlation between the Correlation between the images and the outcome. So, we wanted to find a good contender to explain and maybe see what happens there. And I wanted to emphasize that in this problem, it was really a problem for interpretable models to kind of increase their performance by adding more features. So, this is what I see in working with this high-dimensional data, but in the case of sustainable That in the case of such data, not really adding more features from more modalities will increase the performance of a model. While actually, if you use dot-in boosting decision phase, which is considered a low-boost machine learning model, it's okay, maybe not. It at least didn't decrease the performance, so we can just give it like 1,000 features and hopefully it will find the best function, not like we need to do some feature selections ourselves. And we are trying to explain that the Gradling Boosting Decision Tree. Explain the grabbing boosting decision tree, which is the best model here. So, here is the source shop plot, and also we try to adapt other explanations, not only this method. So, this is a simple WATIF analysis. Here we can see that, for example, this plural effusion, physicians couldn't agree if it appears on the image or not. You might have two physicians that claim it is there, and one physician says it's not enough to claim it is. Not enough to claim it is there. So then you might kind of explain the uncertainty of your model because if physicians can agree, so you can put those values to the model and say, okay, if it occurs, this will change the survival function by this much. And if it's absent, the error of the model will decrease. But the final conclusion for the analysis was the global analysis. And actually, it appeared that the most important feature in our model was whether. Feature in our model was whether medical devices appeared in X-ray images or not. And after talking with physicians and radiologists, it actually is kind of clear that this feature should be important because people that have their x-rays done and kind of have these medical devices on them, it means that they actually couldn't be kind of removed, and this might be a symptom. And this might be a symptom of the person being in a worse state. Or, for example, someone was lying down and it had all those medical devices on them and they couldn't remove them to make the image, which here is kind of a bias in our model, because the model is mainly using this medical devices feature to predict whether the person will stay longer in a hospital or not, which we thought was kind of a cool result to like how can you debug your model with these explanations. With these explanations. Finally, the takeaways from this talk maybe is you wanted to kind of try to explain these time-to-event use cases with time-dependent explanations. If you want to play around with the data sets, it's available on GitHub. I would like to discuss, maybe over this workshop, how to evaluate explanations and interpret them, because it's tricky for physicians to interpret those plots. And also, And also, at the end, I have a shameless plug. I opened a three-month research visit next year, which will be self-funded by something like the Polish National Science Foundation as part of my PhD. Thank you. We will have time to have one short question. So, for the sharp value, you compute. The shock value you computed over time. So you compute at each time point and connect. Do you also do some sort of smoothing, make it more smooth over time? No, we didn't. We just computed it for 100 time points. Of course, it's computationally demanding, and we hope future work will kind of improve this, because it for sure can be done smarter. So right now you are doing separate time. So Do you in separate time? So, I guess a point to be computed jointly somehow, then we utilize different time points together to more effectively. Yes, so maybe if your model is really have those time changes of importance in time, like drastically, then maybe it's not. But I guess we didn't see such models for your global global. Include or local institutions because normally when we do the explanation, it's more open found locally, and now your statement is different locally on the time. And how about locally on the spatial on spatial so for example in your example you have the image data and then they have some local image? There are some locally special space fit into special locally explained uh so in this case we didn't kind of explain the back to input image but we only relied on these tabular features. Uh so I don't think anyone is working on like silencing maps in time, right? I didn't see such things. Maybe if you have time I hope I can ask one more question. Okay, I will allow you to. Okay, I won't allow you. So I I'm thinking my my struggle for here why we need to use a Shapely score for these problems. Because the reason is, for many of the biological research, we want to really understand the parameters associated with the outcome why. The Shapely score was suffered because it came out with the game theory called the game suffered from the two variables have a highly correlated. So think about like five variables that are almost the same. So they will only account. The same, so they will only account one fifth of the importance by using the shapeless form. And although if you are the one of the variable, they will highly associate the one. But if you put the five together, none of them will be shown up. Yeah, so I agree. That's why you said that. At the end of the day, they cannot answer the most important side of the question. What the variables are really associated with why. And so because this correlation is over, it heightens in the biological research, all the variables become less important. All the variables become less important if they cannot discover the most important part. Yeah, so I agree this is a huge challenge. So, if you adapt subvalues to this task, the disadvantages will come with it. I think it's like we should better visualize them, maybe to also add this correlation structure to the thoughts so people are aware of it. Thank you. 