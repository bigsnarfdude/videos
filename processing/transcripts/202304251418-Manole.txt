So this is just an advertisement. Alright, so the first application I want to talk about is this problem of comparing collider events. This is a problem which is not, it doesn't seem to be related to the topic of systematics, but I will connect it to systematics as we go along in a couple of slides. So the question that I want to answer is, given two collider events, what is the notion of distance between these collider events? And this is a question that was addressed by a group at MIT in this paper, Comiske et al., in 2019. And they proposed a solution based on opinion transfer, which I'll start by telling you about. Which I'll start by telling you about. So, for me as a non-physicist, this is what a collider event looks like. I have some cylinder here, which is representing my collider. When I collide two protons in the middle, I have a bunch of jets that emanate from it, and we'll record them on the edge of the cylinder. So I'm representing the jets by these blue dots, and the size of the jets is, sorry, wrong way. The size of these jets is representing some notion of energy, or in our case, the transverse momentum of these jets. If we kind of take a longitudinal coverage, Kind of take a longitudinal couple around the cylinder, we can represent the locations of these jets in a bivariate plane where these are two angular variables. So we can always think of our events as being a weighted point cloud of the following form of points which are represented in this two-dimensional space and which additionally have a PT associated to it. Okay, at least for the purpose of defining this metric, these are the only coordinates that we're going to be using. Okay, so here's a picture where I'm now drawing two different events, one in red and one in blue, and I'm going to ask the question, how do I make it? Red and one in blue, and I'm going to ask the question: how do I measure the distance between these two weighted point clouds? Okay, you can puzzle over this, and maybe a naive thing you might want to first do is think of, you can just say that maybe we can stack all of these red points into one big vector, we can stack their locations and weights into one big vector, do the same for the blue ones, and then take the Euclidean distance between them. But you'd run into at least two issues if you did that. One issue you'd run into is that there's no canonical ordering in which to stack these things together. And so your distance will largely depend on how you ordered them, and this will be Depending on how you ordered them, and this will give you particularly ambiguous results. Another thing that you'll run into is that you may have a different number of jets in each of these events. And so the dimensionality of these big vectors you would have formed would be different. And so there would be no obvious way to compare them. And so the kind of the observation that this paper made was that it's very reasonable to think of these two events as being distributions. They're distributions with support points given by these points and this bivariate point. In this bivariate point, and the corresponding mass corresponding to each of these support points is precisely the Pt, which I'm again representing by the slice of these points. Okay, and so the proposal is simply to define the distance between collider events as some notion of lossist distance between them. One has to be a little bit careful about this definition because the total amount of Pt in each of these events is potentially not going to be equal to each other. So these are no longer probability distributions, as Philip was describing. As Philip was describing, but there is a generalization of optimal transport to these unbalanced distributions, which we can talk about in the discussion later on. But that's basically the idea. The idea is: how much work does it take for me to transport the transverse momentum from one event to the other? And the extent to which I have to transport this PT is going to tell me, is going to give me some notion of distance, roughly the weighted sum of each of the distances along these elements. Yes. I think process also does in some way because Events in some way, because from the point of view of visor events, any kind of a phi shift is a variant. That's a great question, and that's something that we ran into when we used this metric in practice. So, what we did is we ultimately defined a metric which minimizes this up to all possible symmetry transformations that you could apply to these events. But I have another question then. If you are basically in this example, you have only a specific particle that Only a specific part of the detector because the scale is small. So these are two jets basically. But if you look at them, if you rotate the red point to match the blue points, you're basically doing a transformation which is disallowed by the physics because it changes the speed properties inside of the. I'm saying you would do the transformations that would be physically allowed. So what are these? I can, if I recall correctly, I can add two pi, or I can shift one of these events up and down. One of these events up and down according to phi, and then I can reflect them according to data, if I recall correctly. So I would admit the symmetries that are present in the problem. I'm talking about the non-symmetries. So there are some operations that you cannot do. Because if I have, say, some decay or some particle that has a certain spin, you cannot move the stuff rotate it because it will be violated rules of physics. I guess, okay, so this gets to the point where I don't know enough of the physics. This gets to the point where I don't know enough of the physics, but in our case where, well, in the place where we've been applying this, these are always V jets in our situation. I don't know if that helps things, and that's a case where you would be able to do this. But yes. And what are you doing with BT? I mean, why don't, for instance, you have it as a third dimension? Because it's this work interpretation of what the optimal transporter is doing. How much mass do I want to transport from one point to the other? And the idea is to try. The idea is to treat the amount of mass of each jet as something proportional to its energy or its PT or something of that kind. That's the suggestion. And to do the optimal transport along these angular variables. So these angular variables are telling us the support points of these jets, but the total mass assigned to each jet, mass in the probability distribution sense, is some notion of energy or PT. That's the suggestion. Just to see if I can clarify what else was saying. The blue and the orange, are they the same type of physical event coming from the same process, or are they just stuff? In my application, think of them both as being events that... Well, in this case, there are many jets. In our case, we're always dealing with events that have four jets in them, all of them tagged as being B jets. Yeah, sure, but there are many ways of producing four B jets. No, so in that case, no, they don't necessarily. Ah, I know, no, so in that case, no, they don't necessarily come from the same physical process. Okay. So they do not necessarily have the same correlation. Physical correlation. Yeah. Any other questions before I move forward? Okay. Sounds good. So now let me tell you how we can use this metric between collider events for the problem of background estimation, which we've been talking about a lot today. So I'm back to this model that we've seen many times. I'm assuming that we have data arising from this mixture of a signal distribution, which I'll take to be known. Distribution, which I'll take to be known, and a background distribution V, which I'll take to be unknown. We have some epstone that represents the proportion of signal in the data. And again, I'm thinking of epsilon as being non-negative here. So I'm in a setting where I'm assuming that B is not only unknown, but that I cannot realistically simulate it with Monte Carlo. So for us, the motivating study here has been the search for the decay of dihiggs into four bottom quarks, where there's a very large QCD background that makes Monte Carlo simulation intractable, to the best of my understanding. Intractable to the best of my understanding. Again, I don't have the physics background to understand that deeply. Okay, so we're in a situation where we want to estimate this background in a completely data-driven manner without using any montage colour. We're in a similar situation as Lucas' talk this morning. However, one big difference from his talk is that we are going to make use of a secondary sample of collider events in order to do the background estimation. So we're going to assume that we also have access to some collider events coming from a signal-free distribution. Signal-free distribution, V tilde, which you should think of as being close to V. So in some applications, you could think of V tilde as being a Monte Carlo simulation, which you don't think is perfectly accurate. In our application, B tilde is itself a data distribution. It's going to be, in our application, a distribution of jets that were tagged as having two rather than four decoys or B jets. And that's a distribution which doesn't have any signal for our purposes, and it's one that we believe would have a background. And it's one that we believe would have a background which is similar to the one in our problem. So we'll use this auxiliary data set to provide a zeroth-order approximation to our background. And then we'd like to propose a method to apply corrections to B tilde to get something that's a better estimate for B. Okay, so how are we going to use optimal transport for this? Well, I'm going to form, oh, and before I say that, I should make an important point that even though I'm representing these as one-dimensional distributions, we're going to be doing this. We're actually going to estimate B in the full-dimensional space. I'm viewing B as a In the full-dimensional space. I'm viewing B as a function of the underlying collider event, and I'm going to estimate B as a distribution over collider events, not as a one-dimensional distribution. Okay, so I'm going to form controlling signal regions as follows. And again, these are controlling signal regions in the full-dimensional space. And the procedure that I'm going to propose to use is going to be the following. The first step is going to be to estimate an optimal transform out going from the control region to the signal region in the auxiliary data. And again, recall that there's no signal. And again, recall that there's no signal over here, so I'm allowed to look in the signal region here. There's no identifiability issues, and I can learn this transform map. And then I'm going to take a leap of faith, and I'm going to assume that this transform map also pushes forward V from the control region forward onto its restriction into the signal region in the data set of interest. And if that unsets is correct, then a reasonable background modeling procedure is simply to take my estimate of B in the control region of my sample of primary interest and pass it through this optimal transfer app to get an estimate of. Optimal transform app to get an estimate out of B in the signal region. Okay, so we're making a modeling assumption here that we get that the transform app between these two distributions is the same. This is in contrast to many other methods that have been used for these kinds of studies in recent searches at CMS and ATLAS, where the background modeling is done with, in some sense, an orthogonal kind of procedure. What's typically done in these studies is to estimate the ratio of the densities of the control regions among these two samples. Among these two samples. And then, once you have an estimate of those two ratios, you can extrapolate it into the signal region and use it to provide a correction of utility outgradient. This is an extrapolation that's being done in a somewhat orthogonal direction from us. It's an extrapolation of a density ratio from the control region to the signal region, whereas what we're doing is we're extrapolating a transport map from this auxiliary data set to the 4B data set. So it's an extrapolation the other way, and the hope is that having these two distinct methods. Having these two distinct methodologies with distinct modeling assumptions would allow them to serve as cross-checks for one another. So, that was our motivation for studying optimal transfer for this problem. Like I said, we're doing this over the full-dimensional space. These are distributions over collider events. And so when we're doing this optimal transfer, you have to ask what is the ground cost. But I've told you that we have a reasonable metric between collider events. It's precisely that optimal transfer that I described earlier. So we're really doing nested optimal transfer here. We have a bunch of jets at the bottom. We have a bunch of jets at the bottom, all those form events, and then the events now we have distributions of events, and at each layer here we're using optimal transport to compare these various objects. So there's a nice use of optimal transport here. Okay. I am close to being out of time. Yes, I'm not. So again, this is just an illustration to try to convey what's going on, but in practice, we're not doing this in one dimension. We're not doing this in one dimension. In practice, the X variable is the collider event, the full collider event. So, in case of for the jets, there's a what? Let's say 12-dimensional in my case. If I'm just thinking of the two angular variables in Pt, these are 12-dimensional objects, yeah? Okay, so if you're just thinking about embedded mass, if I could neglect the mass of the embedded. Yeah, we've been neglecting the mass because we don't know how to embed. We've been neglecting the masses because we don't know how to embed it in that metric that I presented earlier, the one that was presented by Comiske and Al. So, yes, we've been neglecting the masses. Any other questions? Okay, I had another brief slide over here, which I'm going to skip in the interest of time. I'm happy to come back to it, but that was just a generalization of the ideas I've mentioned already. I want to close with an application that's purely statistical. It's one that's not directly connected to particle physics, but I wanted to put it in because, in some sense, this is, in my opinion, one of the places where optimal transfer has the most. Places where optimal transfer has had the most impact in statistics so far in the literature in recent years. And I wanted to highlight it because goodness tests like goodness of fit have come up over the past couple of days. And this is one way to define a whole swath of new multivariate goodness of fit tests. So I'm going to pose the following question. How do we define a quantile for a multivariate distribution? The usual notion of a quantile or a CDF makes use of the natural ordering of the real line. And I'm going to pose the question, what is the natural way to define this? The question: What is the natural way to define this in for general dimension? And I'm going to suggest that optimal transfer is a way to do that, and here's why. So, let's start by looking at the one-dimensional case. And suppose that I have a distribution P. I'm assuming the ground class is Euclidean O at this point. And let's ask what is the transform map from P onto a uniform distribution between 0 and 1. Okay, well, so in particular, if I have a point x, I'm going to ask what point T of X does it map to? Well, Philip has already. Does it map to? Well, Philip has already told you that in this case, for a strictly convex cost, the transfer map is always going to be the gradient of some potential. And furthermore, when the cost is Euclidean, it's the squared Euclidean cost, that potential is also convex. And so what that means is that this mapping is monotone. Okay, but a transfer map must be mass-preserving. And if it's furthermore monotone, it must mean that everything that's in here must be mapping over here. And just by making note of that observation, you can convince yourself that the. You can convince yourself that the transform map, this point T of X, must be the point which is equal to all of the mass on the left-hand side over here, just because that's a uniform distribution on the right-hand side. But this we identify as simply being the CDF of the distribution P. So what is this saying? This is saying that optimal transport in one dimension, if I'm pushing forward onto a uniform, the transform amp is just the CDF. If I go the other way around, if I transport the uniform onto some other distribution in one dimension, I get the quantile function. But this suggests a very natural... But this suggests a very natural procedure for doing this in general dimension. If I want to define a general multidimensional quantile function, the idea is simply to pick a reference distribution, such as the uniform distribution on the unit cube or some other reference distribution, and define the quantile function from the multivariate distribution as just the optimal transport app from the reference onto that distribution or the multivariate CDF on the other layer map. Now, why is this useful in statistics? Why is this useful in statistics? Because if we have notions of population CDFs and quantiles, then we also have access to their sample analogs, order statistics, ranks, things of this kind. And ranks are part of the statistical folklore in non-parametrics. There's a whole area of non-parametrics based on rank, which defines tests for goodness of fit, to sample testing, and others of this kind based on the notions of ranks. And now this kind of framework suggests a way to generalize. Kind of framework suggests a way to generalize all those tests to multivariate data. Okay, so let me very briefly show you what this looks like. In the sample version, like I said, if we have a distribution P, which now we just have access to a sample from P, the idea would be to pick a reference distribution, such as the one over here. I'm not going to go into details why one would pick this reference distribution, but this is one that's very often used. We would find a discretization. We would find a discretization of this reference distribution, and we would simply define the sample CDF or the sample ranks of these data points as simply the points that they map to under an optimal transform map between these two. And like I mentioned, once you have a notion of multivariate rank, this leads to the generalizations of all sorts of possible rank-based tests. And it can be shown that the efficiency of these tests is very similar to the one-dimensional calculus. And this is something that has received a lot of attention in recent. That has received a lot of attention in recent statistical literature. So that's why I wanted to point it out today. Okay, so I'm going to briefly close. I won't go over this conclusion because I'm out of time, but I just want to briefly mention two open problems that we both wanted to mention. So one is the following. So Philip kind of mentioned that in practice when you're doing optimal transport and you just have sample access, there isn't a canonical There isn't a canonical estimator of the transform. And there are at least two different classes of estimators that Philip talked about. One which starts by computing an optimal transform between these samples and then extending it to out of sample points using smoothing or other methods of that kind. You can view this as a non-parametric regression problem to extend this discretely defined transform to the entire space. And then a different class of estimators are ones that I would call smooth n-map, which first smooth these. map which first smooth these points and provide densities for for or density estimators for P and Q and then the estimator of the transform app is simply the unique transform between those density estimators. Okay so these are two classes of estimators and then there are other heuristics that have been studied in the literature. And now the question is now we have a good understanding of these two kinds of estimators theoretically. We have a very poor understanding of these kinds of estimators theoretically. But these are the easiest to implement. But these are the easiest to implement in practice. And so there's still a gap between our statistical understanding of these estimators and what we can do in practice. So there are many open questions of finding estimators for optimal transport maps, which are both statistically and computationally efficient at the same time. And finally, the last thing I wanted to mention before I close is the question of quantifying statistical uncertainty of optimal transport maps. So this is a workshop on systematics, but I just want to mention that quantifying statistical uncertainty for optimal transfer maps is very much an open problem. It's one that Is very much an open problem. It's one that I have been working on for the last couple of years. And again, for at least one class of these estimators, we have some first results in this direction. We've been able to derive central limit theorems for these kinds of smooth end map estimators in some of my recent work. This is a procedure that would allow me to get confidence intervals for these optimal transform maps, but like I mentioned, these estimators are very difficult to compute and practice. So it's still very much an open problem to find kind of practical confidence bounds for transform maps. Kind of practical confidence bands for trust for massive practice. Okay, I think that's all I had. So thank you very much. I'm going to start and talk and start the recording for the discussion.