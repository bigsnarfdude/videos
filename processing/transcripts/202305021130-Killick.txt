Okay, so I'm going to be talking about soil. And the reason I'm talking about soil is because we have a project together that's joined across the pond as well with something I've just forgotten the name of, so bear with me on that one. It's long, long COVID brain pulp, these random things. Maybe I can. Well, these are random things. Maybe I should get into a scanner and then they can work out where the problem is going. Anyway, so what they've done is it's a simulation, but it is in the field. So it's not like a lab experiment where you're controlling everything, but it's also not just naturally occurring. Okay, so it's kind of in between. And so what they've is they've got this trough here and they've got a packed bunch of soil and they're basically going to simulate rainfall coming from above for a period of From above for a period of time, and they want to be looking at pooling and how the water drains in the soil they've got at the moment. So, that's kind of the aim, that's the start of the experiment. As we get the water coming in, you can start to see some pooling, especially in this middle part of the trough. This is the area of interest they're kind of looking at for the purposes of our experiment. And as it increases, you kind of get more. It increases, you kind of get more water coming in. Now, from purposes of kind of statistical properties, the important thing for this is to understand that when you're just looking at the burst structure, it will have a certain variance structure. And then as water kind of comes on, you're going to get a higher amount of correlation because you're no longer looking at the substructure, you're just looking water with water, and it looks very similar. So that's kind of where I'm going to come from. That's kind of where I'm going to come from. This is just three of the pixels in the image for the raw data. Kind of showing you there that you can see that there's high variability coming in. There's not a constant mean structure. What is actually happening here is because this is outside in the real world and not in a lab experiment, we've got differences in the images that you can kind of see as you. And the images that you can kind of see as you as you kind of take through that one is much darker than this one in terms of there's more people standing around and stuff, and they haven't got a control on the cloud system either, the weather. So basically, you're seeing some random variation here that is due to the lighting that we have for the images and other kinds of cloud coverages and things like that. So, what we could have did to start with, so Sean had done a bit of A bit of an internship with Davidson's group at Cornell. And Dan Cowell, along with David and us, developed this Bayesian trend filter with dynamic shrinkage. So we kind of went, let's see if that works. So basically, all it does is it's like a pre-processing technique where you're basically taking into account that you might have time varying second order structure and you're just wanting to filter out the trend. So this kind of takes us from the top. So, this kind of takes us from the top image to the bottom, where you can clearly see, you know, it's not doing a perfect job, and I'll kind of show you in the results later where that kind of is occurring. But at least from kind of my Lehman's point of view, it did a good enough job from two hours to get at the correlation structure without having much residual mean variation in it. Yep. How well is it slides? Yeah, so it's quite. I haven't played with it with epMRI data, but I have played with it with data that contains a lot of outliers. And you can set the settings of the filter to basically be sensitive to outliers and just remove them, or to be less sensitive and do more of a smooth filter. So the aim in this is: I'm not. The aim in this is: I'm not caring about the mean stretch, I just want to do the changes in covariance. So, kind of elaborate that a bit further, you can kind of think that, so I'm a change point person, and so I kind of think about this in the sense that we have an underlying null hypothesis that the covariance structure is constant across time, and then that's going to break at some point in the alternative to say that there's a change. Alternative to say that there's a change and the prior to the change and post-change will be a different covariance structure. I'm not saying any position about what those variances need to be either side or how they relate to each other. I'm kind of assuming that we've got independent estimation of before the change and after the change. You can make that more created if you want to, but there's no need for this scenario. So typically in the literature, what they do is they look at text. Is they look at statistics that are kind of weighted differences between your underlying covariance structure and the covariance structure that's estimated with the change point in. And so different people use different weights for this. The bar there is to indicate that they're estimated. And so typically, if you have a large spectral norm of this type of test statistic, then you're going to say there's a change. Type of test statistic, then you're going to say there's a change point, you're going to reject the null hypothesis, and so there's a couple of references down there. But for all of these, we need this estimate under the null. Okay, and if you scroll up the estimation under the null, won't see anything. So we kind of came to this thinking, you know, I've done a lot of work in univariate change point estimation. And when we're looking at change variance, we don't look at differences. Variance, we don't look at differences in variants. What we look at is we look at ratios of variances. And when we kind of look at this test statistic here, where we're looking at differences in covariance matrices, that to me was a bit, why are we doing that? So what I'm going to do is I'm going to take this inspiration from the univariate approach, where you're basically looking at the ratios of the variances. And under the null hypothesis, the ratio of the pre-change variance to the post-change variances. The pre-change variance to the post-change variance is going to be close to one. Okay, but the crucial thing here is you don't need that estimate under the null. So I'm arguing this is kind of a more natural way to look at this problem. It's also scale invariant. So, you know, if you have a scalar multiplied by that, it really doesn't matter. So instead of the test statistic being this difference, the spectral norm of the difference in these covariance matrices, I'm going to take a look. Variance matrices, I'm going to take a look at this test statistic here, where I'm taking the eigenvalues here of this matrix R, which is just my ratio matrix. This is arguably a ratio between the two matrices A and B. And so, all I'm doing is I'm looking at this J largest eigenvalue, summing up over a certain number of eigenvalues, and I've basically got these. And I've basically got these two here to basically look at both sides: increases and decreases. So, when I do that, I've got some really nice properties that don't necessarily hold with the previous test statistics. One is that it's symmetric, so it doesn't matter which direction I look at the data. It's also symmetric with respect to the inverses, so it doesn't matter if you're looking at covariances or precisions here. Covariances or precisions here and it doesn't depend on the underlying covariance, which for me is a huge thing because trying to estimate an underlying null hypothesis when you actually may have change points in the data and may have many change points in the data is really not a simple task. So now we've got this test statistic. I'm going to introduce a little bit of theory. I'm not I'm going to introduce a little bit of theory. I'm not going to kind of go through it in detail because I really don't want to bore you before lunch. But if you're interested, then you can kind of look at the paper, which I'll give you the reference to later. So this is where the random matrix theory side comes in. So here I am looking in the kind of lower dimensional space. I'm not looking in this high-dimensional space. And what we can basically show here is that a suitably normalized test statistic will tend to this normalized. To this normal distribution. And so the key here is the suitably normalized part. And so if you look at our raw test statistic here on the left, you'll see that it is sensitive. And so this is under the null and this is the alternative. So you can see it is sensitive. We do get kind of a little peak, but that's going to be quite difficult to look at. You can't just look at a max of that or anything to be able to identify. Of that, or anything to be able to identify that. So, it's quite common in change points to get this kind of like U-shaped distribution before standardization. So, we then do the standardization side here, and this is where we're getting kind of our normal white noise structure coming through under the null, but under the alternative, we're seeing quite a well-peaked, so this is over several realizations. We've got quite a well-peaked curve here. I just want you to notice that you know, this curve peaks around 30. So, if you're if you're thinking about So, if you're thinking about confidence levels for a standard normal distribution, you're way above this confidence level there. So, I'm now going to kind of show you some simulations of how this actually works in practice. I told you I wasn't going to bore you with the theory. So, we have a method on top by Alex Howe that is and co-authors, which is a likelihood-based approach that is. That is, as I said, looks at the difference between the covariances and looks at the spectral norm of that. So, that actually, in general, I'll give away the end show, that actually works quite well, but it just doesn't scale to larger sample sizes, larger time courses, because it's quite computationally intensive. Then there's the Galiano method, which has the same limiting distribution as our test statistic, but But just doesn't seem to be able to get the peakness coming through. So, because of that estimation of your covariance matrix under the null, and you're looking at that difference, that estimation of that covariance under the null really hampers down the value of the test statistic. So, you can see here that it's peaking around four, which, given we have the same asymptotic distribution, that we go to is much, much. That we go to is much, much smaller. Then we have the ratio, which is kind of our method. You can see in these small change scenarios, it's quite noisy. But again, you're kind of seeing that maybe we're kind of getting somewhere around six here for this one. And so then there's a Wang method at the bottom, which is here and show and co-authors. And co-authors, and that is designed for high-dimensional. Okay, so it's not truly a fair comparison. I want to be open and honest about that. It's designed for high-dimensional test statistics, but it's one of those ones where reviewers ask you to compare with stuff because they think, oh yeah, you should do that. So there we go. But as I said, all those three others are working on the differences and not the ratios. This is kind of looking at the error of the change point estimation here. So this is given that a change. Point estimation here. So, this is given that a change point is determined, where would it be? So, you can kind of see that the Galeano method is working quite well here, but often part of these, they don't actually pass the threshold for detection. So, then if we move on to a larger change point, this is where it's a lot more interesting. You know, a small change in covariances is quite challenging to. Challenging to simulate in the sense that what's small for our test statistic might not be small for other test statistics. So here you can see that the Galiano method is well peaked again. You've got a nice tight band around the histograms, but you're kind of peaking there around 20. And then you've got the ratio method here, which is which is kind of ours. It's a bit more diffuse, but the peak is much higher around 30. This is important for when I come to multiple changes later. That's why I'm stressing it. Changes later. That's why I'm stressing it. Okay. And then the Wang method here, it is working better, but it's still kind of quite noisy. And also the change point estimation can be quite far off. And I think the key thing with this one is it doesn't kind of come down fast. So if you look at ours, you are coming down quite fast after your peak. The others are less so. So then when we move on to the multiple change. When we move on to the multiple change point scenario, and here we are just using a binary segmentation approach here, when you're working with changes in covariances, it can be really challenging in terms of the sample sizes that you're looking at. Because as soon as you take a data course and then you start splitting it, you start to get into, okay, well, am I actually invoking the asymptotics of the estimation of our covariance matrices, et cetera? And you're getting into large estimation with very few data points. Estimation with very few data points. So it can be particularly challenging to find many change points in these types of series. So I've got various P and N on the left. You'll see none of them are high dimensional. And as I said, Alex's method works really well, but you can't really scale up to either larger B or larger N because it's just rotationally too intensive. So this is our false positive rate here. So you can see this gullion. Here. So you can see this Galiana method is working really, really well for your false positives because most of the time it doesn't detect anything. The Wang method has certain disadvantages in terms of it. It actually identifies a lot of points and in the wrong places. So that's why we've got quite a high false positive coming here. And then we've got quite a low false positive rate coming through. Positive rate coming through for the ratio method. Obviously, you've got small p, small n, you're going to get higher false positives rates here. That's just due to the fact that I'm trying to squeeze the same number of change points in a much smaller series. So, when we now look at the true positive rate, this is where Alex's method really comes into its fore, it gets the most. Most, especially in those small P scenarios. But then, as we kind of come higher, the ratio method is the one that's kind of winning at getting all of the change points. And you'll notice that the Wang method is very similar. So the false positive and true positive rates are actually about concurrence. So for every true positive, you also get a false positive. And then the Galiana method, this is what I was kind of alluding to. This is what I was kind of alluding to in the single change point examples I was showing you earlier. It's just when you're looking at smaller and smaller sample sizes, that test statistic just isn't getting high enough. So it is well peaked, but the actual ability for it to detect change points using the asymptotic properties is very flawed. So it's a nice, well-peaked test statistic. I'm not denying that at all. It's just the threshold for a change point is too. point is is too high. So then if you start to look at other measures like mean absolute error here, then this is on the estimated covariance matrices, then we do try to be we do have a lower error than almost all the other methods. So even Alexa's method gets the right change. Change points, it doesn't necessarily mean that result in covariance matrices, and that's because again, they're trying to estimate the covariances under the alternative, but they have to use their estimate of the null in doing that. Okay. I'm nice and ahead of time, so we're going to catch up. Chair's always like that. So, going back to our storylap for. So, going back to our soil application here, so again, these are just three of the pixels in the image. So, if you run the ratio method, which is the one that I proposed, then you get a number of change points, which I haven't plotted on here just for ease. But you can usually kind of pick them up more or less by eye. So, you've got something going on here, they kind of start to get. Here they kind of start to get in sync, and then this is where the kind of side kind of starts to get wet. So, there's some areas of the image that will start to become more homogeneous. And then, at kind of around 350, this is where we start to get our pooling coming in. And then at 4:50, this is when they've drained it. So, this is actually the end of the experiment. They weren't really caring about what was going on here. They went, yeah. About what was going on here, they went, Yay, we've done it, kind of thing. Um, so actually, in this part, this is where the trend filter estimation is really quite poor because there's a lot of movement, there's a lot more variability around the actual experimental site. But I wanted to include it rather than disclude it, just see if we could detect that, which we do, which is great. Okay, so the Wang method here only detects a single. Only detects a single change point at 445. So it was performing really, really well on the simulated experiments, but it didn't perform well on the actual data analysis. And the reason for that is because there's a huge eigenvalue that is affecting the threshold for saying whether it's a change or not. Okay, so that's kind of something that it's quite sensitive to. Whereas with our, I mean, we're using sums of eigamo, so arguably we will. Of eigenvalue, so arguably we will be similarly affected. But because we don't have to estimate that covariance matrix ends on the null, that is what really induces that huge eigenvalue is that difference from the null to the alternative. And so a lot of the stuff that we're kind of seeing here, they were really happy, which is always nice, always like to please our collaborators. But the key, I think, for me on this one is around when you're actually applying this. So they're currently collecting data where it's not an experiment. They're just doing field studies where basically they just have a camera constantly on the field site and they're wanting to be able to detect this pooling. So the challenges for me in that setting are around the fact that this one they were just looking at a very small area. In the next one, you've Small area. In the next one, you've got a very large area, there's going to be a lot more variation. And the simulated rainfall was underneath a cover. So, whilst we did get some light variation, you didn't get like the true natural light variation. So, that's going to be a challenge. Nighttime is going to be an issue. So, there's going to be a bunch of other things that we're going to have to deal with going forwards. And I'm not sure that Bayesian trend filter approximation is going to help me there, but at least for But at least for this experiment, it worked quite well. So, I'm going to summarize nice and early so that we've got lots of time to get back on track before lunch. So, the key thing for me that I want you to take away is that we need to be looking at ratios here of covariance matrices when we're looking at this. We don't want to be looking at differences, we need to be looking at ratios. And so, as far as I'm aware, this is the only work in the literature that actually does that. Only work in the literature that actually does that. So if you're going to do anything with that, please do that. There are advantages in that in that you don't require that estimation under the null. We have good false positive control. All the false positives that we do detect are at the boundaries to do with those small covariances. So the key kind of going forwards when we're trying to automate this is, well, do we actually need a different way of estimating my covariances when I'm working in those smaller. Variances when I'm working in those smaller sample sizes. So that's kind of where I'm going to go next. But the theoretical threshold actually works well. So there's a lot of change point scenarios where the theoretical threshold is either too conservative. But in this one, it actually seemed to work fairly well. So our future work is to extend this to mixed sampling rates, so whereby we maybe have images that are missing or maybe we're changing the sampling rate on the camera to say battery or things. Rate on the camera to say battery or things like that. And because these are actually in-field experiments where they rely on solar charging. So we're going to have to deal with that issue. And I did actually have a link to the GitHub on here, but apparently I sent the wrong version of the slides. So the code is on GitHub. The paper is published in the latest technometrics issue. That's it, issue. So, yeah, I'm just going to stop that sleeping. To stop that sleeping, it's bought on my talk already, and I'll end there. Any questions for Rebecca? Adria doesn't get to ask to talk, ask a question because she's in the talk before. So, from your primo, can you teach? From your framework, can you determine the number of the change point? Yep. You can? Yeah, so I'm not actually. So, in this, I'm basically just proposing a new test statistic. So, you can include that test statistic in your favorite search function. Then how do you compare your method to the cumulative sum approaches? So, one of those ones that I was looking at was a cumulative sum for the covariance setup. So, obviously, the cumulative sum is. So obviously the cumulative sum is usually directed towards the mean setting. So you need to kind of look at that in the covariance domain. So yeah, I think it was the Galliano was the QSM, if I remember correctly. Okay, okay. I think we can actually repormulate the ratio of cooperative matrix as a ratio of the distances. Because covariance matrix is basically a second order, like a Like a you couldn't distance scale, so you can actually compute the pairwise distance between all the time points and put a ratio, very similar to the ratio of the quasi matrix, and formulate everything in terms of the ratio of the distances. Okay, it's just a suggestion. Yeah, I mean, if you can, great. She said no, but I'm not listening. She said no, but I'm not listening to it. Fairly certain I haven't asked that question last time. You did ask a different one last time, so I hope I can. Maybe I remember wrong, but I don't think I asked this. So I've just been wondering because they were looking at ratio statistics and this is different, but there's this whole literature on self-normalization, so which is also a type of ratio statistics. So have you can you compare it with it? I mean, I know it's different, but yeah, so yes and no. Yes and no is the answer, I think. So, yes, in the sense that I'm sure you could reformulate this in terms of some of those test statistics that you're looking at. But I haven't looked at any of those that are looking at covariances. So, a lot of the self-normalization stuff that I've seen, I could just be not looking or not paying attention. But the ones that I've seen are more geared towards mean and trend structures. Meaning trend structures. I don't know if I'm on with that. Well, I think if we can reserve any more questions, you can corn our speaker during lunch when she's captive at her table. So let's please thank her. Speak one more time. So our last Our last