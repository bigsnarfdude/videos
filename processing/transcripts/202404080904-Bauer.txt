This morning we have two general talks. The first one is by Uli Bauer from the Technical University in Munich. And this will be an overview talk on political link analysis and persistent homology. Thank you, Keith. It's a great honor to be a presenter for this overview talk. Basically, mostly addressing the half of the audience coming from reputation theory, but of course, I also want to show something interesting and new to the TA world. So it's kind of a mix of the basics and maybe. Mix of the basics and maybe some recent results that everyone might be interested in seeing or learning about. But in a sense, I want to mostly focus on topological analysis, data analysis, and the theory behind this. And most prominently, that's persistent homology, which is one of the biggest topics in applied topology. And it originates from. It originates it maybe in the form like in the use as a data analysis tool from a project in the 90s led by Herbert Ellisbrunner. And biogeometry was the topic of the name of this project. And here you see a molecule. And one important aspect in that project was that the geometry of molecules play an important role in their biological function. Here, this is an Here, this is an antibiotic, a very simple antibiotic that functions as an ion channel. It attaches to a cell membrane, and the hole that you see in this molecule is kind of responsible for ions being able to pass freely from the inside of the cell to the outside, and that eventually kills the cell. So, that's how the in a very oversimplified description, how the Description how the molecule works. So, some of the geometry here, the fact that it's really a tunnel, is very important for the biological function of this shape. So, here's a kind of sketch of what we're doing here. So, you saw that this molecule is modeled in a simple way that you know molecules are often displayed, visualized as a union of spheres. A union of spheres, somehow the weight of the atom corresponds to the radius. And that's a simple principle that you can also use to assign a meaningful shape, a meaningful topology to a discrete point set. This point set itself has no interesting topology, but you already recognize by eye that this seems to be something like circular or annular. And the simple construction of blowing up this Blowing up this point set by considering balls of a fixed radius gives more interesting shapes how the topology of this union of balls changes. And here you see some isolated pieces. And then if you increase more, at some point you actually get a shape that has the homotopy type of a circle. So, in a sense, here at this radius, at this geometric scale, we cover what we see visually. See visually. And you also see something here on top, like in the backgrounds, you see these disks that are used to model the shape that kind of approximately is described by this point clock. On top of it is a simplicial complex. This is a sub-complex of the Delaunay triangulation. I'll show you a bit more about this in a moment. And this gives, as you can see from this picture, it is actually a homotopy equivalent, not homeomorphic, but homotopy equivalent. But how much of the equivalent version of the UN of balls in the background? And we typically study this at multiple scales because there's no natural choice of a scale. And depending on what scale you look at, you see different shapes right here. You still see the same shape basically as before. But if you increase the radius too much, then the whole middle will be filled up. And so this approach, like looking at a discrete metric space or a discrete point cloud, point cloud. This week point cloud, point configuration Euclidean space here in this setting, and turning it into a meaningful topological space by some proximity construction that associates a simplex basically to every subset of points that, or to subsets of points that are reasonably close, kind of in a simplified way. This is what kind of the basic construction we have. I like to call those constructions geometric complexes. Those constructions, geometric complexes or proximity complexes. And maybe the simplest version of this is often called a check complex. Again, the starting point is a union of closed disks. And then we simply record the intersection pattern. So whenever two disks have a pairwise intersection, then the corresponding centers of the disks are connected by a one simplex. And the same if you have higher intersections, you have higher dimensional simplexes. This gives you, in total, a simplicial complex. Total a superficial complex that will not embed in the ambient space of the point cloud. You see a bunch of intersections here. Another downside of this construction is if you increase the radius, eventually we get very many simplices. So the maximum number of simplices that is obtained is exponential in the number of the points, of course. If your radius is large enough, then every simplex will be there. So it should be simplices if you have n points. So it's a Points. So it's a simple but not very efficient construction. But that's kind of a good starting point. It's a special case of what's called a nerve. So if you have a cover of a space here, the space that we're covering is the union of these disks, and the cover itself is the disks whose union we consider. And then the nerve really of this cover records the intersection patterns whenever I have a subset indexed by some subset J here of Of cover elements that have a common non-empty interception, then this subset, the type of J here, forms a simplex. And this gives you a simplicial complex. And the nice thing about this is it's known under certain assumptions on the cover. That's often called a good cover if these assumptions are satisfied. And we can actually get a homotopy equivalent. Simplicial complex homotopy equivalent to the space that we're covering. To the space that we're covering. Good can mean various things. Maybe the most well-known interpretation of this kind of meta-theorem is that we have an open cover and that has a subordinate partition of unity, which is used to construct the homotopy equivalence. And all the intersections, therewith and high intersections, if they're not empty, they should be contractible. That's a typical assumption. Another setting that can also work, that's relevant for many constructions in topological. Many constructions in topological data analysis would consider closed covers. Here, for example, closed and convex covers. In fact, somehow this theorem is well known, but what's not so well known is how exactly these theorem, like there's actually two versions of the theorem already, and how they're made to work. So, for example, it was a bit difficult to find a kind of good description of the second case of closed convex cover in the literature. Of closed convex cover in the literature, especially when we are interested in infiltration. So we want to exhibit this homotopy equivalence as kind of a natural transformation. We are interested also in the connections between the scales. And that was not something that in 1948 people had not really thought about this. So, and then also the kind of overall literature of the different variants of the nerve film was a bit confusing. So, we recently wrote a I recently wrote a survey paper targeted mostly at the TDA community, but also other people where these NERFs play a role, trying to explain the common themes behind these different proof approaches and also some cases that you can't easily find in the future. Okay, here's one instance where we need this kind of closed convex and functorial viewpoint. That's the Delaware. That's the Lawning complexes. So, the starting point here is not a union of disks, but simply that instead we cover the entire space by the Voronoi cell. So, the Voronois cell consists of four points in the plane that have this highlighted point as its nearest neighbor. And so, these are closed convex subsets of Euclidean space. And you can play the same nerve construction if you have two boronoid cells that have an intersection. Boronois cells that have an intersection, this will be lower-dimensional. It's just a one-dimensional face. Then you can also connect them by an edge and higher intersections by a triangle. If the points are in general position, you won't see any more than three boronary cells intersecting. And in fact, you can prove that the corresponding simplicial complex, which is called the Launay complex or the Launay triangulation, and this complex is gonna be embedded in the same space as the point set. The same space as the points. It's not going to work if the points are not in general position, but in general position, you actually get a nice triangulation of the convex hull. That's a big advantage, and another big advantage is that these complexes are shown to be much smaller than the Czech complexes. So, this is actually very efficient and it's a very powerful tool that's used a lot in computational geometry and topology. And you can also Is a general position? General position means any in R in R D, any D plus one cardinality subsets must be affinely independent. So any subset that potentially could be affinely independent is also affinity independent. This is actually usually called general linear position because that's this linear condition. But in Only we need something extra, namely whenever you have Namely, whenever you have a set of subset of points having smallest circumcircle, then this smallest circumcircle should not contain any other of the data points. So there's a linear condition and then there's an additional spherical condition. So I would call those general spherical conditions. So yeah. But these are generically satisfied. So that's that's That's an important assumption to make. And actually, software needs to kind of artificially establish this condition by a symbolic perturbation technique. So now you can combine the idea of using the Delaunay triangulation with the chain complex idea of using the balls. Instead of looking just at the Voronoi cells or just at the balls, we look at their intersection. And this is going to be, again, closed convex subset. Be again closed convex subsets intersecting a closed disk with a convex polyhedron, and we can also form a nerve, and that's often also called the Lord complex at a certain radius. And these are also called alpha complexes or alpha shapes. Those have been suggested by Edelsborn and Sweiter in the 80s and are very commonly used in computational geometry. And you see here the point clone. Here, the point cloud should really look like a point cloud. And maybe you're unhappy about this triangle that appears in this complex, because maybe the shape would be nicer if it was actually a curve. There's actually another construction that I want to mention that's called the wrap complex. And in this special case, it will just take this triangle and this highlighted patch and collapse it away. And that's another change of the shape that doesn't change the homotopy type. And one way to explain how this wrap construction works is a theory introduced by Robin Foreman called discrete MOS theory. It is basically mimicking smooth MOS theory for compact manifolds, but it is based on simplicial complexes for combinatorial cell complexes. So here you see a function on a simplicial complex. So the function values, each simplex gets a function value. The function is monotonic, so The function is monotonic, so higher dimensional faces have to have greater or equal function values. And the functions have to be the fibers of the function, the pre-images, have to be grouped in either singletons or pairs. One and six is a singleton, and two, three, four, five appear in pairs where one simplex is a cofacet of the other. And if you have a function like this satisfying these two conditions, monotonic and grouping. Monotonic and grouping simplices into singletons and pairs. That's a discrete MOS function. And the power of this is this is kind of like non-degenerate smooth functions in the smooth MOS setting. So these two here are the singles or the critical points. And the fundamental theorem says if you have such a function, the complex is homotopy equivalent to a cell complex built only from the critical sequences. So only 6 and 1 are used, and basically, what you're doing with the non- And basically, what are you doing with the non-critical ones? You're collapsing them and deforming the complex, and you end up with this circulation. And so one special case of this statement can be used to, this is what we do a lot, we use discrete Mose functions and the corresponding partition of the simplices into single pairs. That's called a discrete gradient. And we use this as a way of encoding collapses. Way of encoding collapses, it's a way of organizing collapses. So, here's one example: a sublevel set of this function here at the level 5, right? I'm moving the simplex with value 6. And the range between 2 and 5 consists only of regular values. There's no critical simplex with any function in this range. And the gradient informs us that then suplevel zip 5 has to collapse to suplevel zip 1. Two sub-levels epsilon. And the gradient is an efficient way of organizing this collapse and encoding this collapse. How did you define this simple? Critical simplex means simply that the simplices are kind of the only simplices with a given function value. That's given a simplic value. The pre-images of the function have to be either single. Of the function have to be either singletons, those will be critical, or they are pairs and those will be the non-critical. So here, this edge and the triangle have value 5, and so that's not critical. But the edge here is the only simplex that has value 6, so it's critical. That's a simple problem. Okay, this is kind of useful to study the topology. We have very simple, but problem tools to. Full tools to make statements about homotropic type. And you can apply this to Delaunay and check complexes. We need a slight generalization to apply to this setting. Instead of allowing only these singletons and pairs, the clusters, the fibers of the function can actually come in arbitrary intervals of the phase poset interval, meaning in a poset, it's a sub-poset that has a common lower bound. That has a common lower bound and a common upper bound, the endpoints of the interval, if you want. So, for example, here, this is an interval, the face positive in green, that's that there's a vertex, that's the lower bound, and there's this triangle that's the upper bound, and all simplices that contain this vertex and are contained in this triangle as faces form this vector. And this is generalizing the situation from the slide before we were. From the slide before, where we had singletons and pairs, which are both special types of integrals. But we can have larger groups here, like larger clusters. And with this, we can express a, so we can use these gradients to express how these different geometric complex constructions are related. Czech and Lilani and RAP are actually all homotopy equivalent. You can have a sequence of collapses from Czech to the Nilani and then from the Jack to the Brevoni and then further to the bread. And all of this, for every scale, at once, it's encoded in a single dispeed gradient defined eventually on the check filtration. It shows how you can collapse to the smaller complexes. And I haven't defined the wrap complex yet, but maybe I'll show just an example of how this looks like. So it's a method that also has been. A method that also has been introduced by Herbert Illsbrunner in the 90s as a way of doing surface reconstruction for laser scans. So you have 3D point set data and you want to connect the dots by surfaces. The basic idea behind this can be nicely expressed in terms of this Fignon theory, which was born at the same time, but originally it wasn't scrapped in this way. But the definition would be: you start, so the red complex. You start, so the REP complex is simply the smallest subcomplex. It has the homotropy type of the Lenorny complex. So it contains all critical surpluses, and at the same time is compatible with the gradient of the Lenorny radius function, this discrete gradient. And basically, it's constructed in a way to be compatible with this gradient, but collapsing away as much as you can. That's the core idea. So if you start with a critical simplex, So if you start with a critical simplex, and then in order to be compatible with the gradient, you also have to include the boundary or the phases of the simplex together with everything that is in the same partition in the same interval in the speed gradient. So here you see an edge of the triangle, but that's paired with another triangle, so you also have to include that triangle. And so then you keep going with the curve. Going with the current set of trimmings, you also pass to the boundaries and you include the paired cells or the other cells in this interval. And you keep going, and what you see is this is a kind of gradient flow that's happening. You start from this critical cell and you flow downward. You do this for every critical simplex below your given threshold R, and that's what you get as a result. It's the graphical X. Smallest thing that you Smallest thing that you just include what's really necessary to include the critical point and make it a subcomplex. Are you saying that there's some sort of discrete force gradient that comes from the critical? Yeah, there's a discrete gradient. That's what's part maybe of this statement here. Yeah, the discrete Morse that's the Belloni function is the generalized disco-Morse function. And it is And it is the three the same is true for the check function. They're both instances of a generalized remote function, and we're using that radio. So what is that Delaunay function? So the Delaunay function assigns to each simplex the radius of the smallest empty circumsphere. That's how you can associate a generalized discrete Mohs function whose sub- This Greek Mohs function whose sub-level sets are precisely the Delano complexes. Right. So here, just to put this into contrast, this is the Lawny complex, the alpha shape of the same point cloud, and that's the wrap complex. And you see that in some sense, the wrap complex instead of removing some of these artifacts that finally loses its colour. Is there any theory that explains why the wrap cuff? That explains why the RAP complex is so much better in an example like this? It's hard to say. It is defined to be kind of minimal with respect to being compatible with the Delaware gradient. So in that sense, it's the best you can achieve if you force yourself to somehow follow the gradient of the Delaunay function. But I have a theorem that relates the Delaunay, sorry, the RAP complex to something else arising from persistent demonstration. Something else arising from persistent homology. Maybe you might also be interested in that. So, persistent homology is studying the homology of a filtration. And using the flat. So here, what you see is called a persistent sparcode encodes the structure of the entire evolution of the persistent, of the homology of the complexes across degrees. And the way to interpret it is when you look at a complex at a given scale, then you consider. Then you consider all the intervals. So, this is simply a collection of intervals, what you see here. Some of the vertical direction has no meaning, it's just I have to draw the intervals in some way, and I sorted them according to the length, from long and top to short and bottom. So there's five intervals in this persistence barcode containing the value 0.1. And this indicates that there are five cycles at the corresponding complex. Cycles at the corresponding complex that generates a basis for the homology in degree one of this complex. And you can read off, but what's even more important for persistent homology, you can read off not only the dimensions of the homology vector spaces. We always work with coefficients in the field. So we're close to representation theory. We actually get vector spaces. And you can also read off from the barcode the ranks of the map that are induced by equivalent. So there's, if you go from If you go from then scale 0.2 to 0.4, there's exactly one interval that creates both values, and that's the rank of the map, the induced inclusion-induced map between the complexes. So this is the one feature that's common to these two complexes. The computation is, this is an algorithm that you, well, you don't have to read it, it's basically Gaussian elimination expressed in terms of column operations. Expressed in terms of column operations instead of row operations. And we are not permuting columns because we need to maintain the order because the order is what encodes the filtration. So it's a variant of something that you know very well. There's another variant of that that you probably also know, Gauss-Jordan elimination, which was the kind of Gauss elimination where when you identify a pivot, you use it to clear out the entire column here, or row in this case. row in this case, if we work with column operations. So that's a variant that you can also use that we call it exhaustive reduction. And one interesting fact about this, it produces columns in the reduced matrix that are in some sense minimal. So these columns correspond to cycles in the complex. These are the cycles that we have just seen before. And let me just show you an example of this. This is a cycle. This is the cycle, this is applied to the Delaunay filtration. This is a cycle obtained using standard reduction to the left, and there's a cycle on the right if you apply this exhaustive reduction. And I mean, just looking at this picture, you see something, it seems to produce a very nice reconstruction of the surface that this point set was sampled from. And that's, in fact, I mean, something that we found very interesting. So, let me just show you an example here. So let me just show you an example here in 2D, like running an animation of the reduction algorithm. So this is my point set, a 2D data set of points. Here's the Delaunay triangulation. Here is the highlighted, the critical synthesis. This is the alpha shape, the Delaunay complex. And this is the RAP complex. You see, it is also not perfect, but we still have some artifacts. And these are there because these are critical simplicities. These are critical simplices where the situation gets stuck because these are still below our threshold. Now, let me show you something like I'm going to show a visualization of this exhaustive reduction. And I start with one simplex, basically killing a feature in the homology and then reducing it. Basically, that means I start with a boundary and I try to make the boundary as small as possible in this lexical graphical order of the chain. In this lexical graphical chain. And what you see then is kind of trying to make the edges in the boundary as short as possible, and the result seems to be a nice reconstruction of the curve. Here's a 3D example of the same thing. So I start with a chat reference. So, I start with a tetrahedron that is killing a large feature, and then you see as the reduction proceeds, the boundary of this tetrahedron is transformed into another boundary with smaller triangles. And at the end, you get a boundary with trying to have as small triangles as possible. And that also seems to be a good reconstruction of the surface. This is something that several people have studied in recent years, how to use this kind of optimal. How to use this kind of optimal chains as a method of flip-flops reconstructions. Here, some examples applied to point-cloud data sets. And we could prove something that relates this to the rap reconstruction that I mentioned. In fact, so the exhaustive matrix reduction, as I mentioned, it computes embedded graphic minimal cycles in a homology class given by a simplex boundary. And in fact, we And in fact, we could prove that any such lexicographic minimal cycle is supported on the corresponding wrap complex. So that gives you a relation here. On the left, you see the minimal cycle, and on the right, you see the wrap complex. And you see that there's some the support of this cycle is a bit smaller than the actual rap complex. We basically also got rid of these effects. So that's not hard. Okay. Um Okay. Um are there any reconstruction theorems of the like take a sample that is epsilon? Um I think so there's a claim in in one of the I think in a tech report by VOME and so but I think this is basically so in a general in arbitrary dimensions so there's there's a there's a statement about reconstruction in 3D but not in a published paper. In 3D, but not in a published paper, as far as I know. Yeah, there's a preprint that contains. And what is the conditioning for the sure about the sampling conditions, but I think it's something like an epsilon sample. So it's a reconstruction theorem similar to the ones that we know in the literature. But I think that's, yeah, that's a I think it's still kind of open and I'm very interested in trying to see if the techniques, the known techniques. The techniques, the known techniques, can be applied to prove something about this. The big advantage about this method is really it's very robust, it's very practical, in contrast to the methods where existing proofs are for methods that are not really practical. So only here have R squared x is in R squared. Yes, sorry, R to the D. Yeah, so this here in this example would be R squared. Yes, we call it R squared. Where yes, we point. So, this is the pointset in some Euclidean space, and you have a parameter. Okay. So, this is Czech and Delaunay complexes, which work well in Euclidean space. There's also a very popular construction that works in more general metric spaces, and it's also very simple. It's called peritorus-RIPS. It is the following simple situation: you have any metric space X. You have any metric space X, and for us, they're mostly finite metric spaces, and then you simply take finite subsets of that metric space with a diameter bounded below your threshold T, and the collection of those small diameter subsets forms the Ripped complex. So, if you increase the scale parameter, you see a family of such complexes. And again, if my metric space is in R2, I will not get an embedded complex, but these will be. Edit complex, but these will be contain simplicities of higher matching. But it's very simple, I mean, it requires much less geometric work to construct this complex because you just only need to check its pairwise distances. So you can write at least a very fast software to compute this. This is software that I wrote, and there's a nice web interface. It's quite popular, it has been used by many people, and it uses a It uses a bunch of tricks to speed up the computation. One is kind of keeping a lot of the information implicit, not storing it in memory. That contributes a lot to the speed up of our previous software. And there's another thing, again, discrete MOS theory plays a role here. There's a certain type of discrete gradient that can be used to, again, avoid storing certain things in memory that are called apparent pairs. Are called parent pairs. Parent in a sense of obvious. I mean, this is not seemingly pairs, but they're generally, like, they're the right pairs. And this is a very important fundamental way of assigning a discrete gradient to a complex. All you need is a filtration of the complex, simplex by simplex. And then, whatever you have in this simplex-wise total ordering, you have a pair of simplexes of codiment. Of codimension one, so the smaller one, sigma i, is the latest, the proper phase of the larger one, the interfiltration. Conversely, the larger one is the earliest proper co-phase. In other words, these two surpluses are closest to each other than to any other satellites. And then you can consider this collection, and they actually form a discrete gradient. This is a construction that has been shown. Show used in special cases many times in the literature. So it's something that people like to rediscover, and it also carries many different names. But the construction is very, very fundamental, I think, to discrete Mohs theory. It creates a bridge, in a sense, between discrete Mohs theory and persistence, because these pairs form a gradient, but they also form a birth-death pair in persistent form. It's always the smaller simplicity. The smaller simplex always generates a homology class when it enters the situation. The larger simplex bounds this homology class and kills it. And well, one connection, for example, it's useful when you deal with generalized discrete Mose functions because this apparent pairs gradient is one way to transform that generalized gradient into a standard gradient in a sensible format. In a way to to take the intervals in the phase probability. Take the intervals in the phase property and refine them into pairs. So, this gives you a way of translating generalized discrete MOS theory into the classical setting without introducing any additional critical points and basically carrying through the statement from the classical theory to this generalization. So, that's also another bridge that these parent pairs create. Here's an application that I wanted to mention. People have used. Wanted to mention, people have used like already more than 10 years back, I think, Vitor's RIPS complexes to study genetic evolution data. Michael Lesnick, for example, also wrote a paper about this. And when the COVID pandemic started, people started collecting lots of data about this disease. So, this is kind of the most complete data set of the evolution of a virus that we have so far. And the idea here is. Far. And the idea here is: I'll get to this in a moment. Like, if the evolution of this population is really tree-like, this is kind of what's sketched here, this is maybe a simple model of evolution that there's no mutation. Then, if it's truly tree-like and if every mutation appears only once in the entire evolution, then you could recover. Then you could recover the distance in this tree from the genetic distance, like how many nucleotides differ. However, if one mutation happens multiple times, then the actual genetic distance here would be maybe one, but the distance in the tree is three, so there's a shortcut. And now the observation is if it was truly a tree, like in that sense, a tree metric, we would not see any homology in the Torres Ribs filtration. In turn, if we see homology, it means that. If we see homology, it means that something is deviating from this. And most importantly, it could be that a certain mutation happens multiple times. And the reason for this could be that the mutation is beneficial to the fitness of the virus. So that's why identifying these features is interesting because it could point to certain mutations that provide an advantage to the virus. So we tried this on this data set and we observed that the code runs the price. Observed that the code runs surprisingly fast. It runs faster than usual geometric data sets. We could easily handle 25,000 data points. And also there's something interesting. After understanding why it's fast, we could bring down this computation time here. It's 120 seconds. This is after a certain pre-processing of the data set before it took a day. And after understanding what I'm going to show next, we could bring this down from a day to two minutes. Day to two minutes. Is this data set available? This is available. Yeah, this is a public data set. Yeah, it must contain now like tens of millions of RNA sequences of the coronavirus. So this is a, GIS8 is the project that collects and assembles these data sets. Is this with the full RIPS filtration? This is part of it. Yeah, so this is what maybe. Yeah, so this is this is what we're computing is only up to up to distance two, I think. So it's a very short prefix of the wind concentration. Because we're looking at, we're interested in these small situations, like very short genetic distances. And basically we're looking for features that, like this one, that are born at length one and are killed off at length two or three. So we don't go very far into this. All right. What's behind this? What's behind this? So, RIPS was actually or is working in geometric group theory. And the idea, the RIPS complex was prominently used in geometric group theory. And importantly, there was a statement about hyperbolic groups, hyperbolic spaces in general. So, Gronov gave this very simple definition of a hyperbolic space, basically, just a condition of four. Just a condition of four points in a metric space. And you can think of it in this way: like there's two diagonals in red that you can see here, and then there's two parallel edges, blue, and pairs of edges in blue and green. And the statement here is if you look at the sum of the diagonals, the length of the diagonals, this exceeds the sum of opposing edges, like either green or blue, by only an additive factor. By only an additive factor, right? In Euclidean space, you would have the multiplicative factor, like you could have a square, and the diagonals would be square two times the side length of the square. And in a hyperbolic space, the special thing is this can be, like, there's actually only an additive offset. So this is kind of what makes hyperbolic spaces special. There's one way to phrase this. The hyperbolic plane maybe is the example that gives name to this. But also, the trees that we're interested in when we study. Also, the trees that we are interested in when we study evolution, these are kind of the most extreme cases. They are zero hyperbolic. So, this delta appearing here as the additive offset is actually zero. And so, the important role of these in geometric group theory is if you know that you have a delta hyperbolic space, then at some point the RIPS complex for the parameter which exceeds four times this is a hyperbolicity. This hyperbolicity parameter, they become contractible. And this allows you eventually to construct spaces that have the group as a fundamental group. And we are, of course, interested in understanding when we have such a situation where the ribs complexes become collapsible, that's very important information for us from the perspective of topological data. The perspective of topological data analysis. The way this original statement was phrased was in terms of geodesic metric spaces. I think this is just because a lot of people working in this field came from differential geometries. So for them, geodesic spaces were more natural than discrete metric spaces. Even though the things that they study, the groups were actually discrete metric spaces. So the first thing they do is they turn it into a Cayley graph, which A Cayley graph, which then also worked with geodesic tools. But so we were interested in understanding this from a slightly different angle, making it applicable also to finite metric spaces. We care about collapsibility, like we've seen before. This is a more combinatorial version of contractibility, slightly stricter. And we care about the filtration and geometric group theory. You only care about that things collapse eventually. But we want to know when exactly does the collapse happen? When exactly does the collapse happen? And we also want to explore if this can have a connection that maybe explaining why the third computation of persistence runs so fast. So here's a version of Ribs' theorem that is more combinatorial, more discrete. I'm stating it here in the case of finite spaces. You can also have a statement for non-finite hyperbolic spaces, but we don't require the spaces to be. But we don't require the spaces to be geodesic. And in fact, again, similar to what we've seen before, you can write down a single discrete gradient that encodes all the interesting collapses that we might care about. So starting from this threshold for delta again, now there's an additional factor here which is called the geodesic defect, how far the space is from being geodesic. And then if you are beyond this scale, then the ribs collapse. Uh then the RIPS collapse uh con the RIPS complex collapses to a point, and also the larger RIDS complexes collapse to the smaller one, what this showed. So that's that's kind of the version of the rich collapsibility theorem, the contractibility theorem, that is most natural to us. Let me just explain what the geodesic defect is. In a geodesic space, when you have two points at a certain distance and then you look at two close And then you look at two closed balls whose radii sum up to that distance, then these balls will intersect in at least one point. It doesn't have to be just one point, but at least one. And if you have a non-geodesic space, for example, if you have a finite space, let's just look at the highlighted points here, this is no longer true, right? You may have to thicken the radii a bit further until you find the first common point of intersection. So, and how much you have to thicken so you can make sure that you have. Thicken, so you can make sure that you will find this point is one way of defining the geodesic defect. Something that authors from geometric in theory have identified before as an interesting concept. And that's a natural quantity for us to state this more general version of the RIPS collapsibility. Okay, and given this, I can also give you a statement that the Also, give you a statement that somehow has a similar flavor to what we have seen before. We said that the Delaunay and Shade complexes arise from generalized discrete Mohs theory. For Ribs complexes, this is not true in general. But we can say something, at least for the case that the metrics are tree metrics, so the metric is a path-length metric of a finite rated tree. And then this And then this uh if if I have a genericity assumption here that the all the pairwise distances uh obtained here are distinct, then indeed the diameter function is also a generalized discrete Morse functions. It can be related to the apparent pairs gradient. If I take the filtration and then I break ties lexicographically, then I get a the an apparent pair gradient and this actually defines the natural gradient. Actually, it defines the natural gradient formed by the diameters. And we can then also obtain the collapses we have seen before in a general setting. Remember that this is basically the case of zero hypermorbid spaces that we are studying here. And again, we have the kind of collapses that we want. The RIPS complex always collapses to the corresponding sub-tree of the tree, like when I filter out the Like when I filter out the edges below my threshold, so I have a tree, a weighted tree, and I like keep edges below my threshold, the RIPS complex collapses to that sub force. If I reach the maximum length of the edges in the tree, then the tree will be connected and then that will further collapse to a point. So that's kind of the state that we've seen before. The ribs at this level, the rib complex is is collapsible. Complexes is collapsible. And then also the S radium to them gives three further collapses from larger RIPS complexes to smaller ones. And this is true whenever between these two parameters there's no change in the tree. So there's no distance lying on this interval. What goes wrong if your graph has a cycloid is not a tree? If it's not a tree, yeah, so I can show you a simple example of a space that has no... So take four points and now this is my edge D. I'm interested in the edge length of this one. And now I want to have And now I want to have two more points such that this edge E is the longest edge of these two triangles, but on the other hand, this other diagonal here, that's called H, is longer than E. Then at the time that E enters the filtration, together with E, the two triangles enter, but it doesn't form an interval in the phase posters because the tetrahedron. The tetrahedron, which would be the common coface of the two, which would complete this integral interval, is not yet part of the filtration. So this is already an example, and this is not a tree metric. Metric I'm using to describe this can be based as a tree metric. That's that's basically um the simplest example showing that in general we won't have uh a generalized discrete Moore's function. generalized discrete Moz function. Okay. And yeah, one thing this shows directly, we pair all the simplices apart from the edges of the tree and the vertices of the tree. And so everything critical happens between 0 and 1. And this immediately shows us that we won't have homology in a higher degree. So this is explicitly given by this grade. Explicitly given by this gradient. It also shows this gradient shows if you set up the corresponding computation of persistent homology and you write down this boundary matrix and then you try to run Gaussian elimination and you realize you have to do nothing. The matrix is reduced from, to begin with, that's expressed by the fact that everything is appearing in this apparent place, which means the computation is completed already. So that's kind of the same thing. Problem. So that's kind of why these situations are very easy to compute because it requires no matrix operation at all. And yeah, maybe we are interested in this genetic data set. We don't have this genericity assumption, but you can still make similar statements work. You can consider arbitrary weighted finite trees. And one thing we need to do to make this work is we need to choose a suitable order on the vertices. Suitable order on the vertices. And so the right order would be to choose an arbitrary vertex as a root and then order the other vertices away from that root. So in a way, find the total order that extends the tree order. And if you use that, then actually you can also show similar statements. We get the same collapses that we have seen on the previous slide, also in the non-generic case. Let me just show you this example here. So this is the blue one, is the tree, and this is the Is the tree, and this is a RIPS complex at scale parameter where one is where, sorry, parameter two is where everything appears. Like points A and B, for example, have distance two. And this is the sequence of collapses that collapse this corresponding ribs complex. First, the tetrahedron at the bottom uh triangle vanishes, then we this edge and triangle disappear and then uh the the other remaining triangles. The other remaining triangles. And this is how the collapse from this full tetrahedral down to this tree looks like in this particular example. So the metric here is just the edge lengths within this tree. Okay? Right. Let me maybe just very briefly explain why we are often so Why we are often so much interested in the persistent homology. In particular, why it's important to look also at the maps that connect scales. So this is an important message because often people who are not closely familiar with how the systems work believe that it's all about like we consider a homology of individual spaces within the filtration. And that's often not enough to really get the information that we need. The information that we need. Let me just illustrate this by the problem of guessing the homology of a space from a sample. So we have a finite sample as we said before, and maybe the strongest version, what we had seen already, is we want to construct a space that has the given homology. And our approach was to consider these union of holes, thicken the point set, and then use that as a potential candidate for a space that we cover somehow. We have seen examples where this works. There are theorems. This works. There are theorems that prove under certain assumptions this is guaranteed to work. This is a very recent update to a classical theorem by John Seldt-Weinberger that provides a sharp bound for the conditions of this to work. So we have two conditions. The thickening of the point belt should cover the space for some parameter delta. And the delta should be chosen, on the other hand, to be small. Other hand to be small with respect to a geometric quantity of the embedding of the submanifold. So, if you satisfy these two conditions that are kind of opposing each other, then there exists a scale such that the thickening at this scale has isomorphic homology to the scale. And that isomorphism is actually induced by occlusions. That's also important. It's not just a random isomorphism, but it's really a geometric one. Really, a geometric one. However, these assumptions are quite strong, and it's not too difficult to find cases where you think it should be possible to guess the horror, but this method doesn't. Here's a curve, it's not too terrible, and here's a sample of that curve, also not too terrible, but it's too loose to actually satisfy these assumptions. And let's see what goes wrong. We browse through the scales and we try to find. And we try to find a moment when we actually recover the homotopy type of the curve, so a circle. And you see what the problem that appears is in these little artifacts that appear. They are very short-lived, but every time one of those artifacts fills in, another one shows up right next to it. And I designed this example such that you can't find the right scale that doesn't have these artifacts. Like by the time you hope to kill the last one, and maybe The last one, and then you recover the final hole here, you already create an exterior collision. So, this shows it's quite easy to find a situation where this is not enough. And the same situation can still be made to work with persistent homology. Now, we look at two scales instead of one and we just see how the scales interact. And then we only keep the common features. The assumptions now. The assumptions now are actually simpler than before. We have the same thing: the thickening of the pointer should cover my space. And then inclusions of the space into the thickened, yes, sorry, yeah, of the space into the thickened versions of the space should still produce isomorphisms in the hormonal. And if we have that situation, then actually we could read off the hormonality of the space from the image of an inclusive-induced map. So this is really just this information exists only. Just this information exists only in algebra. There might not even be a space, or we might not have access to a space that has a homology. It's really just an algebra. And here's the example, the same example as before, where this can be made to work. So the dark blue thing has two holes, and the slightly thicker one has three holes. But there's only one of them is a common hole. Maybe it could be viewed as this one. So the rank. Could be viewed as this one. So the rank of the inclusion-induced map is one, and that's exactly what we need to recover. So, and if you, this is actually very, very easy to see if you just write down the assumptions. The thickening of P covers X. Well, then we also have an inclusion of P into X. So we have this diagram of inclusions. And we just throw homology at the diagram. And we write down the assumptions that the thickenings induce isomorphisms in the homology. Induce isomorphisms in homology. And then we recognize: well, if this first horizontal map is an isomorphism, then in particular it fractals through an epimorphism. And the second one is an isomorphism, so it tracks through a water morphism first. And then we realize, okay, what's sitting here in the middle, homology of the thickening of x, is the image of the map from P delta to P2 plus delta. And also this homology is isomorphic to the homology of x. To the homology of X. So that's the proof. It's a very simple idea. And that's highlighting the reason why it's important to look at persistent homology and how we get much stronger statements than if we force ourselves to recover the homology just from a given single space. Right. Let me just say a few more words about the structure. This is something very important now for in the representation theory. For the representation theory perspective. Possession homology is basically the homology of a filtration. So we have a functor from the posit of the reals to topological spaces, and the internal maps are all inclusions. And we apply homology, we post-compose this functor with a homology functor, and that gives us now a diagram, a functor from the Bells as opposed to vector spaces. So that's an object of flexitation theory, and typically what Theory and typically, what representation theory do when they see a complicated object, they try to view it as they hope that it composes in a unique way into incomposable pieces. And maybe understanding this picture tells you more about what the module is that you're looking at. Thanks to Baison, who built this module here, always some very complicated ring. Okay, and here's. Okay, and here's this works particularly well for persistence modules because we know that the indecomposables are so simple, they're always interval modules. They're really just skipped by an interval of the real line that determines the support. And the barcode that we have seen is really just the, yeah, it encodes the isomorphism type of the module in terms of its composables. And this is very strong, but of course, this is something very unusual. We rarely have Very unusual, we rarely have this. And if we just generalize a little bit, if we go to two parameters, which is what a lot of people want to do. So if we instead we say we have a grid-shaped commutative diagram of vector spaces, then I mean, this is something people from reputation theory know very well. There's a kind of classification of what kinds of types of indecomposable classifications we have. And it has a very nice And it it has a very nice uh there's a very nice simple way of summarizing this for the case of of grid-shaped diagrams. You simply count the number of squares that you see in this diagram. If it's less than four, then you're finite type, so in particular the one primary case where you have no square at all. If it's exactly four, I think, and this is, I don't know, it's probably a coincidence, it's just a neat way of putting this, but there's two relevant cases for four, namely that the is m is four and n is one, or n is two and n is two. And I think the reasons why these are both same is probably very different, but they can be summarized in this very succinct book. But then larger than before, you have wild type. So that's a very nice little way of putting this. The problem that we're facing when we want to talk about parameter persistence, most of the time we are in. Most of the time, we are in a wide representation field setting. We were motivated, so maybe one of the basic motivations to look at multi-parameter persistence would be clustering. So, here is maybe the problem. You're asked to count the number of clouds here, and that seems to be very difficult because how do you differentiate when the cloud starts and another one ends? So, here's another sketch of the molecules in this cloud, and maybe you would recognize that. And maybe you would recognize that there are kind of two dense regions from which this data set is sampled. But if I just play the game that I have done before, I would not recognize the two. They would just merge very quickly and you couldn't tell apart that there's actually a denser region here and a denser region there. So you would also additionally want to filter by density and filter out the less dense region, and then you would actually recover. You would actually recover that there are two clusters at the scale. So it's very natural to consider more than one parameter, but that makes the story kind of challenging or interesting. That's why we need a representation in this setting. Because now we are facing a two-parameter filtration. But there's one important observation, like we still maintained one filtration filtering by geometric. By geometric scale. And in this direction, the zero homology will only have subjective maps. Like if you increase the scale, components will only merge. So we don't have fully general diagrams, but we kind of ensure that these maps here will always be epimorphisms. And so we we are wondering if that simplifies the picture, and indeed it it does a little bit, but not significantly. But not significantly. So the representation types of grids of this particular type, when we have epimorphous in the horizontal direction, is the same as if we had an m by n minus 1 grid. So if we increase the number of rows by 1. So in the case that we're going from 2 to 1, this buys us something, but most of the time we're still facing the same problems. All right. Okay, let me just say some words. I think I need to shortcut a little bit, but a very important fundamental theorem in ecosystemology is stability. Let me phrase it in terms of point cloud. We let a lot of the deltas in terms of point clouds. And if I have two similar point clouds, it could have a small difference in the hosted distance. In the host of distance, and we're guaranteed that their persistent barcodes are also similar. This is expressed in terms of delta matching, so we can match up the intervals in the barcode in a way that they have similar endpoints. And then if we have unmatched intervals in one of the barcodes, then they have to be short in the sense they can imagine that they are not with a zero interval. And again, the endpoints should be zero. And this And this is a theorem that kind of sits in a larger pipeline. When we go through this pipeline, we start with a set of data points. The first thing we do actually is consider the distance function in space to their data points, which then gives us a function whose sublevel sets we consider. The sublevel sets are the union of walls that you see. And this is our filtration. We apply homology to this filtration, and then we apply the structures. And then we apply the structure theorem to. So the oh hology gives us the persistence module, the structure theorem gives us the power code module. And the crucial insight is that each of these steps concerns a certain metric space, and each of the arrows between the steps is a one-dipsitz line. So stability is really saying that the map that takes a point card and sends it to its persistence barcode is a one-part card. Barcode is a one-litch step. And the first proof of stability that concerned the step, like kind of shortcutting is here, going from a function spray to the barcode. And later on, people realized that kind of the crucial, the difficult part in improving stability is the passage from persistence models to barcodes. And it's challenging because this is a non-functiliary step, right? The way that decomposes. The way that decomposing a persistence model is not something that you can reasonably do on the level of morphisms. It's really just an object-wise and non-canonical construction. So an important construction that I use to phrase this is the interleaving condition. So if you have one point cloud and another point cloud, then the blue one is contained in the thickening of the red, and then the thickening of the red is again contained in the thickening. Of the red is again contained in a further thickening of the blue. So you can have kind of this mutual sandwiching here. That's the geometric picture of interleaving. And in the context of functions, we can express this in terms of sublevel sets. If you have one sublevel set of the function f, that threshold t, then it's contained in the sublevel set of g, that threshold t plus delta. So you get diagrams of this kind of shoelace shape. Of shoelace shape, and then if you apply homology to such a diagram, and because everything you maintain commutativity, so you preserve to the leaning state when you apply a function. Fortunately, we can't, so this is how you go from filtrations to persistence modules in a way that preserves the interleaving distance, which is the smallest delta that emits such an interleaving shape. And unfortunately, the passage to The passage to systems barcodes can't be put in the simple term. We can do this in a tutorial way. But you can still prove that the passage from persistence modules to the barcodes is stable if you have a delta interleaving of two persistence modules. And you can show that also the barcodes emit a delta matching. So the match intervals have. So the matching tables have similar endpoints and the unmatched tables. I think I say something here. Yeah, so this can be seen as some sort of Krol-Schmidt, right? Krol-Schmidh tells you that two modules are isomorphic when they're in the compostable summons, can be matched one-to-one. This is saying that two modules are approximately isomorphic, so delta interlay, precisely when they're in the compostable summons can be matched one-to-one and are approximately. And are approximately isomorphic. So, this is an approximate version of Crochet Meat, and it's very remarkable that it holds. And it's remarkable because we kind of have a statement. Let me just skip a few things here and go to this. Because I think I have to. That's ending my talk now. But there's a definition of this bottleneck of matching distance that we use to prepare barcodes. It also applies to multi-printers. It also applies to multi-parameter persistence. And that says basically we look at decompositions of the two modules. The summons can also be zero, and then we try to find a bijection. So we may add zero summons so we can make sure that our bijection exists. And then we compare within this bijection, we compare each summon with the internet wing instance. Then that gives us a bottom line distance, like what's the best possible data. The best possible delta matching between such decompositions of two parameter persistence modules. So that's a natural way of talking about bottleneck distance also in the multi-parameter setting. And then we want to talk about a module being structurally stable if that passage from interleaving distance to the borderline distance is continuous at that persistence module. So we have seen stability can be interpreted as kind of an approximate co-Schmidt. An approximate Pro-Schmidt. And the interpretation of this would be: every one parameter persistence module is indeed structurally stable. The way in which one parameter modules decompose is a stable one. That's what the stability theorem says. And in contrast, in two parameter persistence modules, it's kind of the opposite. The only structurally stable persistence modules in the multi-parameter are the indecomposable ones. And for all the other ones, you always And for all the other ones, you always find that every other multiparameter persistence module has an indecomposable one arbitrarily close. So this is kind of the least structurally stable that you can move forward. And that's, I think, an important point for the perspective of representation theory. When we add this perspective on the leavings, the one and the multi-election. But the one and the multi-correct case behavior very differently. And we need to find new ways of exploiting the insight that we obtain from decompositions because we may very often have a module that doesn't post at all. Okay, maybe that's a good point to stop here. Thanks very much. Are there any more questions? Yes, I have a question. Is there a way to give a materialization to categorical data? Possibility data. Categorical data meaning? For example, there will be data that belongs to some uh vector space, just say, for example, this person has cancer. As cancer has blood the type of blood or that the not necessarily are numbers, just yes and no. I think that often people try to do something like they turn this data into geometric data and then they actually use Redis Conference to geometrically realize this. I mean there might be I mean that might be that that may in part be the most direct way of geometrically realizing certain things, but I think yeah, I mean if you say geometrically realizing that I think you really want to work with lengths and sometimes the obvious true right to find like this variable is something like one that's all any process but that's I think it works about it. Yeah, you have to have knowledge about where the data comes from in order to ensure that this is a robust construction. So I won't be loaded with this type of data because I want data. What you have to take into account is that when you start with categorical data and you embed them, the semantic of the data will be. The semantic of the data what they need is not anymore in the encoding but remember the embedding. As soon as you work, these data are not invariant by uh change of coordinates. And a TTA is invariant by change of coordinates. So usually to just get uh meaningless one to work with them, you have several choices. So first is to break the symmetry by fixing So first is to break the symmetry by fixing far away points that are fixed and or you don't work with the data by themselves but you work with the embedding of the data itself out of some information. Otherwise just copying I mean if you have a lot of numerical data and a little bit of categorical data that's fine but you have a lot of data set at least 