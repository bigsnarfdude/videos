So I'm going to do like the second talk on uh kernels. Uh so it will be a bit different uh different focus than the previous one. Uh so like yeah I hesitated about what to present this week but uh Linax said that it should be a tutorial so it will be like more tutorial. Like hopefully there will be at least one part of a proof you understand. And okay, so Nati and Road they talked about They talked about beneath overfitting. So, here I'll have like a slightly different approach. So, first, the reason we started working on that with Andrea, Son, Gordali, was to study the NTK. So, for NTK, you can't just take like a sub-gaussian in our model. And second and fold, I'd be interested in. I'd be interested in asymptotics. So I have like a non-asymptotic bound to compare it to these asymptotic predictions. But at the end of the day, I kind of bypass all the difficulty of doing the uniform convergence bounce. Okay, so I'll just go through this very quickly. So we look like in modern machine learning, we have this new regime for statistics. We have this new regime for statisticians that we don't really know how to study. Now we start knowing, but it's still kind of difficult. So we have overparametrized models, no explicit regularization, so balancing of the variance and the bias, like the capacity control of the problem. And so we train until interpolation, even with noisy data. And so we have like a new phenomenon. And so we have like a new phenomenology: so benign overfitting, double descent, non-monotonic error curves, and this only appears in linear models. And so in this talk, I'll consider kernel rich regression, so it is a high-dimension regime. And so the goal of this tutorial will be to show one how to derive quick asymptotics for these RKHS methods, and the second is Methods. And the second is how I cut these asymptotic predictions or give very precise explanations of what's going on with Bena-mover fitting. Okay, Ben-Loverfitting, again, so you interpolate the training data, but you still generalize well. So usually it will look something like this. So you have a smooth solution plus. Solution plus spikes, and basically the spikes won't contribute to the tester. And I will say that the reason we have been an overfitting in this kernel reach regression setting is that the non-smooth part of the kernel will play the role of an effective reach regularization. So it will be as if you have a non-zero reach regularizer. And this is like a had a And this is like a high-dimensional phenomenon. And so, like, second thing is like these double-descent curves with non-monetary behavior. So, this also I'll try to show to and explain to you how they can happen in high dimension. And just to emphasize that to capture this type of behavior, you need Like exact prediction for the tester that holds like for a given function and that is exact up to a additive vanishing constant. Yes? I have a great question. I saw this plot in 1D for the Network estimator. So in this case it's not a high-dimensional phenomenon. Oh yeah, because they take sigma to zero. Yeah it's not. But like for kernel switch regression. For kernel switch regression. So basically, if you're in low dimension, you'll have a solution that will be smooth and that will interpret the data and that will have a lower KHS now. So I don't know if there is an easy argument for that, but for LaPascal, Raklin has a paper showing that, for example, you can't have binary overfitting in low dimension. I think there are soft arguments to do. So for current evaluation, it's necessary almost necessary to again it's like you have more freedom to fit its bikes with low norm. Okay. So there's a lot of papers on these topics, just like a subset of them. And okay, so I'll start with a quick background on canon reach regression. So there are like a lot of So there are a lot of formulas, but hopefully it won't be too hard to follow. So basically we consider data on just a probability space x with probability measure nu. We consider a kernel function which is just a function from x dots x to r that is PSD and we consider the associated kernel operator, so which is an operator you know. Which is a operator, linear operator from L2 to L2. And so here's the definition. And so, with my conditions, like the trace of the operator to be bounded, this operator is compact. So you can always diagonalize it. So like the corner operator, you can s uh write it as a sum over the eigenvalues. over the eigenvalues lambda j times phi j v j. And so these phi j's are an autonomous basis of L2. And so like I'll take lambda j and one increasing. And so I can rewrite this kernel in terms of a feature map. So basically what you do is that you take your covariate, your data X, and you send it to a Tilbert space. So phi X, so here is just So phi x, so here is just n2 sequence, bounded sequences. So just you create this infinite-dimensional vector with square root of lambda j phi j x. And so now the kernel is just the inner product of the feature map evaluated at x1 and the feature map evaluated at x2. And so for the target function, so we want to learn a target function f star. And so f star. L star. And so F star you can decompose it in this autonomous basis in L2. And again, you can just write it as a linear model in this Hilbert space. And so I'll write it as the coefficients of F star in this RKHS representation. So like, okay. So associated to this kernel, you have this reproducing kernel library. Canalical space, so H, which is just all functions in L2, so that the RKS norm of F is bounded, it's non-infinity, it's finite, and Tsar RKS norm is just given by the I2 norm of theta star. Okay, so like just a bunch of definitions. So, like So, like, the main point of this slide is just to say that you can see kernel functions are just inner product of feature maps. And then you have a linear model, it is Hilbert space. Okay, so just now I define the kernel ridge regression. So it's just standard ridge regression into silver space. So I take IID data. I did data, where like XR just ID, it is a probability space. I take a YI that is a noisy measurement of the target function S star. So then kernel rich regression is basically just minimizing the square empirical loss plus this like regularization on the Hilbert norm, Haketchus norm of F. And so you can. F. And so you can write it as a more, I mean, simpler way. It is just a linear model in Hilbert space, which is just, okay, like now the RKS norm is L2 norm and theta. So it's just basically a rich regression model in a Hilbert space with a Hilbert feature map. And so can I just And so, kind of just FYI, so you don't need to evaluate this feature map. Thanks to the representer Ethereum, you can solve this problem that there is just a complex problem. Product problem. Okay, so the goal in this tutorial is to evaluate the tester, so with square loss, and in the high-dimensional regime, so X user. So, x, you think about it as a vector in Rd. And we'll take n and d, so the number of samples is the dimension d of the data to be related, like polynomially related. So, the number of samples will be polynomial in the dimension. Okay, so how do you uh compute uh this asynthetic test error? Asynthotic tester. And like in this rich regression setting, the good thing is that this tester only depends on this random matrix resorbent. And so we expect some universality to kick in. So if the feature map, this mapping of X to like the Hilbert space, is high-dimensional enough, we expect that we can Uh we explained that uh we can replace it essentially by a like just by a Gaussian vector with matching first and uh supernova. And so this would be the Gaussian equivalent model So you just take you feature map phix that is just an infinite dimensional vector, you assert that it is an infinite dimensional Gaussian vector with covariance, just just lambda j, diagonal matrix of lambda j. And then the data would be just this like, you know, evaluation of this linear model in this Gaussian model. You have a feature matrix Z, the kernel which is the inner product on these two Gaussian vectors. You get this prediction F alpha, and so you have like And so you have like a correspondence between the two, and you get finality risk taser, which is so the L2 norm of sigma one half theta star minus theta hat, the solution of future pressure. And so the universality that I mean here is just to say that the two test errors are the The two test errors are the same in these two models. Okay. So, okay, so this universality here. Okay, so you have already some people that considered it. But the point here is that it's slightly stronger than previous results in the sense that I do the universality of the entire feature map and in a polynomial scaling. So it will work for every So it will work in for n like that scales as the other power case and just the linear high-dimensional setting. And I just mentioned that such an equivalence is not obvious because the coordinates of the feature map are not subversions and are not weakly dependent. So in some cases you can just condition on d coordinates and you'll completely have the rest of the infinite vector determinism. Vector database, data map. So, okay, and here I will present some cases where it can be shown rigorously. Yeah, there's a subtleties that appear at natural numbers, exponents, like the previous costs also works. Yeah, yeah, so it's way more general than the first works we did with Andrea. So, okay, so like this Gaussian equivalence, I don't have to introduce it, but I think it explains most of the computation in this case. And yeah, it will work quite generally. Again, under some conditions on the future mode. So I'll give one example and then I can talk offline about some more recent work. So, yes, so now that we have this Gaussian provides model, we can do some computation. My point here is that this computation is behind all the works on bin and overfitting. Like Bartlett Erhag, basically the computation is this, but with Gaussian providers. So this will be. But so this will just explain the computation. So you take so you separate the tester into a bias and a variance. So the bias is just the expectation of the estimator over the noise in the training data, and the variance is just the variance over this noise in the training data. So this is a Gaussian design model, linear model. So it has been studied before. Studied before. The only thing I will say about it is that standard random matrix theory results don't apply here because you have multi-scale anisotropies. So the sigma matrix, the eigenvalues will have like very different scales. So you can't apply that directly. So here I just give an example where we can. Give an example where we can prove this equivalence very easily, and it's basically the same assumption as in Bartlett theta. So you take, so we'll assume that there exists a delta, so small positive quantity, arbitrary small, so that there exists a sequence. So here I'm looking at a high-dimensional limit. So basically I index my sequence of kernels by n. Kernels by n. So for each n. So, like the way to think about it is that you have a sequence of kernels that are indexed by the dimension of the data, and then you have a number of samples that scale with D. So here I just index it on M instead of D. But basically, here, like my assumption on the eigenvalues will be that there exists a sequence. Values would be that there exists a sequence of integrals mn, so that m is smaller than n at the power 1 plus delta, and lambda m plus 1 times n at the power 1 plus delta is smaller than the sum of the lambda shape starting at m plus 1. And so this I call it a spectral graph. And I'll explain it. I mean it will be more transparent later. And it will happen for models with a lot of symmetries. So for example data like inner products For example, data like inner product kernel on the spheres, each eigenvalues will have a huge degeneracy. And so going from one eigenvalue with degeneracy to the next one, there will be a big gap between the two. And so this I will give an example. So under this condition on the decay of the eigenvalues. So this is just a dec uh condition on the decay of the eigenvalues. So there's no gap, so. So this is like the k in my buffet. But there's no, when you say gap, it doesn't have to be an actual oh it doesn't have to be a gap, yeah. It can be like uh so the reason I say it's a gap is because of the phase condition, because m is smaller than uh n uh at the power of one minus delta. The power of one minus delta. So basically, if you assume just that the kernel has a finite trace, so the sum of the lambda j's are smaller than the constant, you can have n eigenvalues of only one over n. And so here, like, I'm not looking at this, so it is prevalent, this type of kernels here. So the trace of cannabis is not bounded. Oh, yeah, yeah, yeah. It's not bounded. Oh, it's yeah, yeah, it's bounded. But basically, uh it just uh decays faster than uh saying that you have n eigenvalues of one over. I will give a like a concrete example. Can you say that? I mean it seems that you should I mean you're saying that okay I understand that the covariance is allowed to vary within, but it seems that you should be able to satisfy this condition even with the fixed covariance. Yes. Yeah, if you have a yeah, okay, okay. This is like just a general condition under the so then you don't have any break or anything right, just a continuous decay, and the decay is such that it satisfies this condition. Yeah, and then I'll just add the condition on the eigenfunctions. Again, I have directly the tester for the RKHS retro Gaussian. But okay, so like here we see in the Gaussian model through RT's spectral. So it is spectral, yeah. It's not a new name, but okay. But so, like, what happens in this case is that, okay, so you have like the random kernel matrix, which is just the evaluation of this inner product of Zi Zj. So the feature maps. So here it is Bausions. And so here you can decompose your kernel as a part that depends on the m first eigenvalues and a part. Eigenvalues and the part that depends on the bigger eigenvalues. And so it is here I call it the low frequency and the high frequency part of the kernel. So for the high frequency part, so you have a j bigger than n, which is basically, you can write it as a covariance matrix, evaluated on this data, but just the part all the eigenfunctions that are bigger than m. And so this And so this covariance matrix has like so this matrix Z bigger than M as idingausian rows. And so in this case you can show that you have this type of concentration, just with like some very easy argument. And so you'll see that basically this part of the kernel will concentrate This part of the kernel will concentrate very quickly to just an isometry. And the isometry will have like this concept lambda bigger than m. So this is just like the trace of the kernel truncated to remove all the m first eigenvalues. So the low frequency part, so now this is like Z less than M, Z transpose less than M. So I remove like the I remove the covariance. So now Gm is an RID matrix with RID Gaussian entries. It's an N by M matrix. So M is much smaller than N. So here we can also use a very standard concentration argument and say that, okay, this is almost an orthogonal matrix. So, at the end of the day, with just this very simple proof, you see that you can decompose your resolvent, so like inverse of the resolvent, k plus lambda identity, as gm sigma m gm transpose plus lambda f identity. So, lambda f now is an effective regularization, which is this lambda bigger than m plus the rich penalty, the rich parameter. Rich penalty, like reach parameter. And uh, this part is just like a almost autoval function times the um eigenvalues, the covariance matrix, times like the auto-value matrix. And so if you just use this decomposition in the bias and the variance, you see that in this model, okay, you don't have variance, and the bias has this very simple formula, and so you can rewrite it as this test. Tesser. So you have beta minus sigma plus lambda f over n times identity inverse times sigma beta. And okay, so here I rewrite it for the kernel map. So basically it's a shrinkage operator. So each component that like each component phij. Like each component phi j is basically shrunk by a factor lambda j over lambda j plus the lambda f over n. So a simple formula in this case. Okay. So here, okay, I just mentioned that, so just to compare with Bartlett, Bartlett's paper, is that Bafflet's paper is that okay, just right here. So in Baffett's paper, they basically don't care about exact asymptotics. So they just need a banded conditioning number on this gm and like z bigger than m. So but like in their assumptions, so what they call m needs to be n over c with c large enough so that t s has bounded conditioning. T has garnered conditioning number, and they need also lambda m plus 1 times n less equal to c times the trace. So just like to compare like the two papers. So yeah, here we go. So yeah, here we we just scale uh M so that we have like concentration. So in this case like with spectrogap basically there's like this interesting uh phenomena that you can uh replace uh kernel reach regression with finite data by just uh uh like uh this uh effective problem. Effective problem, which is just like approximation problem with effective regularization. So basically, the effect of having finite data in this problem is just having lambda f, which is scaled by like bigger than lambda by an additive constant lambda bigger than m. That's all the effect in this model for finite data. For finite data. And so, yeah, so in this case, the components with lambda j, so the eigenvalues that are much bigger than lambda effective over n are perfectly fitted. So like using the shrinkage formula here, if lambda j is much bigger than lambda f over n, this is close to one, and so you fit completely these components. For lambda f, lambda generic. Lambda f, lambda j much smaller than lambda f, you don't fit these components at all. And so, like here, the phenomenology is that you have like this lambda bigger than m self-induced regularization coming from the high-degree part of the gravel. Here, in this case, because the tester will be better and better as lambda decreases, lambda equals zero, so the regularization equals zero is optimal. So, this corresponds. zero is optimal. So this corresponds to interpolations. And in this case, here, you basically learn this low-degree part very well and not at all not the high degree part. And so this high-degree part of the functional will be like the spiky part. That will help for the training, I mean for fitting, interpolating the data, but won't impact the tester. Impact the tester. Lucas? Yeah, but maybe we don't have time, but can you give a I mean uh um a glimpse of the proof of the high frequency part? Or you gave it and it was a pretty good question. How does it work? What are the tools which is pretty isymmetric? It's almost isymmetric. It's a decoupling argument. It's a version in matrix concentration. versioning, matrix, concentration, election notes. And basically, yeah, like you what you need, you need to basically show, okay, so you'll have hmm it will take some time, but I'm just if it wasn't. It's actually pretty easy, but I'll just like. Pretty easy, but I just like on the fly, and I'm happy to see you. Yeah, I give you after the talk, thank you, but it's not very difficult. When you say that lambda equals zero is optimal, optimal in some asymptotic sense. Yes, can you be a bit more I mean, do you does this allow you to be a bit more precise about the the gap between lambda zero and lambda star, like in terms of how it behaves? It behaves. Oh, so I'll give an example where you. I mean, like, this is exact asymptotic predictions. But then, like... So asymptotically, there's no difference in lambda star and lambda zero. Yeah, lambda star is equal to lambda. Lambda zero, right? But for any finite. There is a difference, right? In any finite one. And can you understand this difference? So I will give also a So I will give also asymptotic for a case where lambda star is not lambda causal. But for finite. You mean when asymptotically lambda star is not lambda zero? Yeah. But can you understand does this? I'm just wondering if this is fine enough in order to understand how this convergence. Or not the convergence of lambda star to lambda zero, but rather the convergence of the gap between there at lambda star and there at lambda star. They are a toughest part, they are a bump as well. So so basically you have uh like uh this are non-asym uh like you have non-asymptotic bounds between the prediction and this uh uh like a finite like finite D and this asymptotic prediction and it will be uh you know one over square to d maybe error. And okay, so basically the gain in lambda cell will be small and you don't have the penalty tota? So so not in the case I have in mind. But uh yeah. Um so basically like uh like you can okay so this is a bit uh incomplete story, but uh so you can remove this uh uh spectral gap and then you still uh have like this one over square to be. And it's just like uh some uh And it's just like some concentration of the resolvent. Oh, I think. Okay, let's stop for it. I think I see this is coming from the approximation part, actually. I think this one over squared P, not from the not from the high frequency. This one over squared of P is coming from the P. Okay, I'm actually more concerned with the high frequency part. Okay, we can. Yeah, anyway, but I know what you're asking about. Yeah. I don't have uh like uh you know uh lower order uh I mean higher order approximation of these results. Okay, so like the question is just when can we do this Berserk equivalence? So with Song and Honor we have like uh a paper where basically we had like some examples in mind and so we we we did like uh we we found uh assumptions. Assumptions that were allowing us to do like this two-matrix concentration. And so basically, so we have this spectral gap on the decay of the eigenvalues, but we also have this hypercontractivity of the top eigen spaces. So basically you look at the top, okay, this should be phi of S, but you look at the top M. Top M eigenfunction, and you have this property that for any H in the span of this top M eigen functions, the L2Q, like the moment of these functions are bounded by the moments, basically, by the L2 norm. And so it's hypercontractivity because like just from Jensen, you have the two Q moment of H. The two Q moment of H that is bigger than the L2. No? And so here is just, so this in some sense is just an assumption that tells you that the eigenfunctions are delocalized. So they're like high-dimensional enough to do this Gaussian economics. And examples of these spaces that are pi-contractive, low-W polynomials for X-Gaussian vector. X Gaussian vector, uniform on hypercube or uniform on hyper sphere. And so I have like other abstract assumptions for like this universality result. But yeah, still in progress. And one thing to say about this result is that it's just so you can always write the feature map. You can always diagnose the matrix. You can always write your rich regression. Write your rich regression in terms of this resolvent matrix. The only thing is that this property on the feature map are very difficult to check in practice. So basically if you take a you know if you take even like just an inner product kernel on Gaussian data, it's difficult to diagonize. So I give like some examples where we have access to the diagonalization. Diagonization. But basically, this is a abstract principle, abstract universality. It's just like checking it on particular cases that is out. And so this is the examples that we had for now three years with Andreas, like this inner product kernel on the sphere. And the idea is you can diagonalize this kernel over spherical harmonics. So why can't you? So YKS is just all degree K spherical harmonics, so it's polynomial from like polynomial of degree K of the sphere, it's just like alternative basis. And so it is psi k, so it's just like the eigenvalues with the HINRC BDK. And so BDK will be of order D to the power k, and so the psi k will be of order d at the power minus k. d at the power minus k. And so you have this decay and then you can choose n, the number of samples between d at the power i plus delta and d at the power i plus 1 minus delta. Then you can m, the one that I talked about, with a spectral gap, to just be like all degree L spherical harmonics and lower. So this will be of order d to the power L. So because we take N bigger than D So because we take n bigger than d at the power l plus zeta, we have like m smaller than n at the power 1 minus zeta, and we have like indeed this spectral gap. In that case, okay, so every straight coharmonics of degree smaller called to L will have lambda ch much bigger than the effective regularization over N, and all the higher degree polynomials that will be I get values much smaller than this threshold. And so we perfectly learn the best degree L polynomial approximation of F. And so this is for a given F star. And yeah. And okay, here you can compute like non-symptotic error, but they're not really good. Just one of those quotes. So you can ask, like, okay, what happens when n is proportional to d other power l? So, what happens is that, so you have like degree l minus one part that is still like perfectly well reconstructed. It's like basically a low-rank spike matrix. So, this has degree d at the power l minus one. So, and the eigenvalues are much bigger than one over l. This part, which is a covariance matrix. Pad, which is a covariance matrix, and we have a distribution that converges to a matching capacitor, and this part, which is just a self-industrialization. And so basically, you have this kind of test error when n is proportional to the other part. So basically, a load of repart smaller than n minus 1 is perfectly reconstructed. L minus one is perfectly reconstructed. I degree part bigger than L is not fitted at all. And in between, the degree L spherical harmonics are partially fitted following like basically this effective model. And so you have effective regularization psi L which is just mu bigger than L plus one double omega L. And you get this curves. And so if you add that with my So, what I presented previously, I'm like a complete characterization of the tessero of kernel retrogression on the sphere in the polynomial regime, meaning that n over d at the power kappa prime psi for any kappa app psi bigger than zero. Because in the first approximation, if you say the bias term is just small, but you also have enough explanation of the microphone. Sorry? I mean in the first approximation of the tester you just said the variance term is just yeah so in test case you have a variance term. And the interesting part of the test variance is that it doesn't have a variance in the classical sense. So here the variance like the high degree part of the function plays the role of an effective noise. So even if the noise in your labels equal to zero you have like a Like a double reset. You have to spike at each integer. Not necessarily. So if it would depend on the value of xi L. So the effective regularization. So like here I plotted for different xi L the test error. So if the Xi L is big enough, so it's heavily regularized, and so yeah, but it's a nice plot, just diffusing plot. And if Xi L is small, basically. Sail is small, basically. So here you have a matching capacitor, and so you have a lot of small eigenvalues that will make the files explode. And you have this normal receptor. Okay, so another model I will just quickly mention. Okay, like two minutes. So it's just the random feature approximation. So you get a kernel that is just expectation over a weight w of sigma x1 at w sigma x2 w with sigmas so in l2 x and v so this you can always write it in that way. Like you can always write your kernel as this. So like here you can think about it as an inner product. Think about it as an inner product in a feature space. And so you can approximate this cover by doing just like a parameter of the integral. So like use a sample WSID and you replace K by it is a finite sample approximation of the integral. And then you can fit a model that yeah, it's just Yeah, it's just a linear combination of this sigma XWS and yeah, you just do random electric regression of this model. And so when n goes to infinity, you get like this, okay, like the solution of this random feature approximation solution that converges towards the kernel rich regression solution. Solution. And so, here again, you can do in some cases this Gaussian occurrence model of the entire feature map. So, like you can diagonize sigma, xw, again like into basis, where p is the basis of L2 of x, m psi is the basis of L2 of V. And so you can write it as an inner product of these two feature maps. And again, Feature maps. And again, you can just replace this infinite feature maps in these Hilbert spaces by just Gaussian vectors with matching commands. And in some cases, you can indeed show that the tessera and the two are equal. And then you can get also like these tesser predictions in the polynomial. In the polynomial scaling. And so here, like you take X and W uniform on the sphere, and you take sigma inner product of X Wi. And there is like a normalization vector of missing here. And basically you get this high-dimensional curves. But basically, all of that is just like some computation, and this is Gaussian law. And so you do some linear pencils, and it's much easier than. So, much easier than with non-linear feature maps. Yep. And so, here, maybe the only thing interesting here is that, okay, so obviously you have a spike when the number of features is equal to the number of samples. So, just because you have a matrix that is almost squared, so you have like exposure of the minimum eigenvalue of the Minimum eigenvalue of the feature matrix. And otherwise, okay, so if the number of samples is smaller than the number of features, you basically follow exactly the kernel rich regression solution. So you basically the test error is limited by the statistical error. And when the number of samples becomes bigger than the number of features, then you're limited by the approximation error of the random features. Approximation error of the remote feature. So you're like the test error in high dimension is the max of the approximation error and the statistical error, which is not the case in low dimension. In low dimension, you have some results that say that you can take the number of features as small as the square root of the number of samples and still have the same tester. Or like tester rate in that case. Yeah, the constant would be large. Yeah, because I don't know. But basically they're in a low-dimensional space and like there's like some compensation. And I think I can quickly, okay, I have like some applications, but so I'll just conclude with the fact that, okay, we have this Gaussian equivalence and this like ridge regression model. Like ridge regression model that comes from this resolven matrix. And so this will be, yeah, I mean, this equivalence will be correct if the feature map is identical enough. So we just need to find like some properties on this feature map where T-sequence holds. So we can give very precise results to this regime. And so this is a more general type of. This is a more general type of universality than other works because here we take the entire feature maps and now we'll have linear sites for a bigger range of scanning, so log n proportional to log D. And the limitation is that it's out of specific settings. And okay, like some marginal directions is basically. Marginal directions is basically you can have like the same type of universality for other losses. So like max margin, you just also use this feature map and you can do this incremental. And that's it.