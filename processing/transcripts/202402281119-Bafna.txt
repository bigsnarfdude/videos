This might seem related to the workshop, but maybe at the end of the talk you will see how it's related. Or you may not. So okay, so let's define power-sum decomposition. So the input is given to you as a degree 6 homogeneous polynomial, which is supposed to be a sum of cubics of quadratics. And the goal is to find the quadratic numbers. Quadratics. And a slightly more difficult problem is where you're given the input, which is a sum of qubits of quadratics, added with some arbitrary error polynomial. And now your goal is to find some A hat p's which are close to the original quadratic. And your estimate of should go to zero as the error tends to zero. 0. And what we are aiming to do is to tolerate some inverse polynomial noise. So you should think of the 80s as having a spectral norm around n. So as long as the error polynomial is for variance norm 1 by n to the 100, this estimate is good enough. Okay. In general, this is a more general problem where you're given a Problem where you're given a sum of dth powers of de k polynomials, and again you have some error term, and your goal is to recover the sum polynomials that are close to the components. So, today we'll mostly think about the quadratic case, just for simplicity. Okay, so why study power sums? So, let's look at the easier problem where each of the A t's is a linear polynomial. linear polynomial. In that case you have that A t of x is some vector vt dot inner product with x and when you rearrange this polynomial you can write it as a tensor sum of vt tensor d in a product with the coefficient tensor x and as we all know this is equivalent to the problem of tensor decomposition. So, this problem is very well studied since the 70s, and there are a bunch of algorithms. And a lot is known about this problem. For example, that we can recover the Vts if they are linearly independent. And we can also tolerate inverse polynomials. And this has many applications to learning theory, such as learning spherical Gaussian mixtures. Spherical Gaussian mixtures, dictionary learning, etc. So, one question is: what about learning non-spherical mixtures of Gaussians? And that is one motivation to study power sums. So, let's look at this problem of learning non-spherical mixture of Gaussians. Here you're given a distribution which is with, let's say, for simplicity, it's uniform over Uniform over M Gaussians which have zero mean and covariant sigma t's. And now we can see how it's related to this power sum decomposition. For any x, the 2dth moment in the direction of x is actually this expression. And when you rearrange it, you can see that it's actually the sum of, it's proportional to the sum of xt. It's proportional to the sum of x t x transpose sigma dx to the data. Okay, any question about this? Am I supposed to understand why it's related to the workshop now? But there are various hints to okay, so uh this is um This is proportional to this power sum. And now you can see that to recover these sigma t's, one obvious algorithm would be to estimate this moment polynomial from the samples and then do a power sum decomposition of the polynomial. Okay. Um and now let's see uh before we see how to solve the problem, let's see why it's not already solved. Let's see why it's not already solved. So let's focus on cubics of quadratics. What if we did the same trick that we did for the usual tensor decomposition and rewrote this AT, yeah, so write X transpose ATX as AT inner product with X tensor 2 and the third power of it. And then we can see that this is equal to. And then we can see that this is equal to this. So, why is it not just tensor decomposing on sum of 80 tensor 3? So, if we had access to this, we could just perform tensor decomposition. But now you can actually note, we will see that we are only given access to the coefficients of P. So, we are not actually given access to this tensor. And in fact, we are given access to In fact, we are given access to a tensor which has very less information than the original tensor. So it's some symmetrized version of KP tensor 3. So let's see why this is true. So let's look at the coefficient of the monomial x1 to x6 in this will be actually a12 times a34 times a56. A3, 4 times A56 plus A13A26A45, etc. So you can see that this is some averaging of the entries of the tensor A, tensor 3. So this symmetrization operation is the averaging, and this loses information because in this A tensor 3 you have N choose 2 cubed many entries, whereas Many entries, whereas in the AX cube, you only have n choose 6 entries. So this is a linear operation that flattens, like that loses information. Okay, and one more qualitative, let me show one more qualitative difference. So for usual tensor decomposition, the linear indicator Tensor decomposition, the linear independence of the coefficient vectors implies that you can, there is a unique decomposition, but this is not true once you apply the symmetrization operation. So here are two sets of polynomials where a1, a2, a3, the coefficient vectors are linearly independent and so are v1, v2, b3. But this polynomial is equal to this polynomial. So, this is a qualitative difference from the usual density composition. Okay, so the moral is that we need more assumptions on the ATs. So, today we look at decomposing so called generic polynomials. So, what this generic means is that one model is just to look at random polynomials. Look at random polynomials A t's where each coefficient of A t is just IIT Gaussian. And we can also look at a more complicated version, like a more general version, which is a smooth polynomial. One such model would be to just take arbitrary vectors a hat t's and then add some perturbation to them. Here I've written Gaussian perturbation, but I Written Gaussian perturbation, but as long as your perturbation is smooth enough, this model is good. Like, for example, I can make sure that if I started with A T, which was a covariance matrix, I could probably do this as a possible covariance matrix. Like I mean symmetric, for example, if I'm not talking. Yeah, so yeah, we have to add some perturbation so that remains symmetric and PST. Like such a perturbation. Like such a perturbation would be fine for the yeah, yeah, any kind of perturbation So for simplicity just think about the random cases for today So let me say what was the prior work there was a nice algorithm of Gerhuang Karpay which decomposed some of cubics of quadratics and they could recover as long as the number of components n is at most root n. At most root n. And the upside is that this is a nice linear algebraic algorithm, and therefore it's also noise resilient. But the downside is that they could only recover when m is at most root n. So even if m is, let's say, n square, even if you're given more information, for example, you're given higher powers of the quadratics. Of the quota, their algorithm doesn't work. And the second work was of Garkayal Sa and they showed that if you're given generic dth powers, you can recover as long as m is at most n to pure. Sorry, the results on the previous page and tell it, is that like zero reduction to the tensor? Yeah, I mean I will show that. Yeah, so this is nice because if you're given more components, you can take a larger D here such that you can recover the components. So the upside is that if you're given larger m, you can increase your d such that you can recover the ease. But the downside is that this algorithm is not noise resilient. Noise resilient. So it's kind of only meaningful over finite fields. And also they need this large number of pieces. So they cannot recover anything like this. Okay. 2 to the 335 is not so bad. Another downside is that we couldn't fully understand the Okay. So ideally we would like to combine both the upsides of these results. So that's what we did. So sorry about this. So we show that if you're given p of x and with some arbitrary error polynomial, you can recover the at's up to some small error. And you can recover as long as D equals 3 and M equals at most Hotel N or NED and M is N to the O of P. So the same guarantee as Garkaya Sah. And this is also resilient to noise. And I won't show this result, but it generalizes to power sum of DK permanent. And uh this result is when uh the coefficients of A t's are uh each IIT function. I have quite a time yeah so that I was going to mention at the end that we don't know whether this is tight uh and actually uh we don't know any uh so um we don't know anything better if we just want to information credit recovery. Can I make a quick comment? So, I think there's a recent result by Alexander German Hoffer and one of his collaborators that they use some algebraic geometry argument to establish, identify for D equals V, M roughly above N squared. Oh, okay. Nice. But that's information theory. Yeah, but I mean, that's going to be way off from the computational piece, I think, right? Yes, yes. Yes, I think, right? Yes, yes. I'm not saying it's an algorithm, there's an information here. Because like basically, I mean, even if we forget the polynomial thing and we had the tensor thing, there's like some very easy algebraic geometric argument that like n squared is tight. But that would be pretty shocking. Yes, I absolutely agree with you. Just for it to maybe. Yeah, yeah, yeah. Okay, that's nice because I had mentioned that that was an open problem. So it's no longer open. Do not try it. Okay, we can take that offline, but it's hard, but okay. Okay. Yeah. So essentially we use the same algorithm when we want to decompose sum of cubic sum of powers of smoothed polynomials. So it's the same result when you are not given any error. But our analysis is Our analysis is not so. I mean, yeah, so this algorithm, when you have smooth quadratics, is only resilient to inverse exponential ones, not inverse polynomials. And this also generalizes to higher powers. So when D is four, it seems like you're sort of in this word spot where it's worse. Oh, yeah. Actually, I didn't mention here, but this is. Actually, I didn't mention here, but this is for multiples of 3. Degrees, which are multiples of 3. Kadi, can I make one more comment? Yeah. So there is this top 24 paper that shows that this algorithm, as of like whatever. Well, I guess I saw the accepted list whatever two weeks ago. And I think this is now designated to remotely not reverse. And do you remember the co-authors? I have been done. So what are the co-authors is Paramandra Gujarat and C. And CTAP doesn't know the others. I think Arvind, I don't remember the name of the student, but Arvind's student at Northwestern. Okay, excellent. Okay, thanks. I didn't know all of this. Okay, so two of the open problems I mentioned are closed now. Can't be more? Just one more, like the N got new version of N less than N square. Okay, so so Okay, so I was going to mention that our algorithm gives parameter identifiability for mixture of Gaussians. And this does not imply an algorithm because we need this inverse for non-invoice. But with the new result, you can apply it to learning mixture of cautions. Yeah, so any question about why this is true? Is this not true if the colour is not skinny? Yeah, that's because you can cook up weird examples where two mixtures will have the same moments. What happened if the mini knock, you know? What happened if we? Oh, I think in that case, can't really write it as this power sum. I think, okay, I think the previous paper, GHK, had some reduction. Previous paper, GHK had some reduction to the mean zero case, so it might be possible to do that. Sorry, I'm confused about the statement that you said two mixtures will have the same moments. I mean, can you just increase for any pair of mixtures with the same moments? So the problem is, like, if you do this for general mixtures, I can just have what's called this parallel pancake example, where I take a pair of distributions that matches in 1D, and then I extend it vacuously. And then I extend it vacuously in higher dimensions. So, sort of like one of the cool catchphrases for these types of smooth things is called the blessing of dimensionality, because once you smooth it, then the higher dimension you're in, the more interesting directions you have to find non-trivial moments, which distinguish between things. So, it's sort of really crucial that you're like smoothing in all of those directions and not just keep. And not just keeping the status quo. If that's a computational hardness issue, it's not a statistical thing. I mean, you can go to a high enough moment for CLWH and specific parallels. So it is both a computational and statistical issue. What Uncle just said earlier, you have to look at very high moments, you know, statistically speaking, to that's that's just a statement, right? We need at least whatever k moments because Whatever k moments because then, yeah, you're right, there's also computation issue. Like, you know, even given those moments, we just find like d to the k and not two to the k. So, this kind of depends on how you set up the goals. So, here, because the tensor decomposition is trying to find the actual parameters, it'll naturally turn into parameter learning. So, for things like parameter learning, when you match moments, then you have statistical overmoments. But once you moment match, you can also create pairs of hypotheses that can be distinguished information theoretically. Information directly, but it's hard to find the interesting direction, so it sort of leads to the both. But usually I think of these as closer to the identification to ask for just something in the pre-image? Just something that has the right elements of it. Yeah, so this is open in 1D. Okay, so let me give a technical overview of the algorithm for cubics of quadratics. So, this is the simplest case, and we should also think about there being no noise. So, the polynomial is given to you exactly. So, the high-level strategy, as Sleigh lasted a while back, is to actually reduce to the usual tensor decomposition. So, it's this operation called desymmetry. This operation called desymmetrize, which is to extract the tensor we want from the symmetrized version of the tensor and then apply the usual tensor decomposition on this tensor. So how would we recover something that loses information? It seems impossible, but the idea is to invert this operation. To invert this operation when restricted to some smaller subsets. So, let me tell you what that means. So, the main observation is to suppose that we had access to a basis of the span of the ATs, then we can actually invert this operation. So, let's see why. So, first look at this tensor sum of AT tensor 3. This is what we want to recover. And this can be written as sum coefficients times VI tensor. Coefficients times Vi tensor Vj tensor Vk. But suppose we know these Vi's, we know this quantity Vi tensor Vj Vj tensor. So, this tensor lies in the span of all these known vectors. And another thing we know is that we know the symmetrized version of this tensor. So, this lies in the span of these symmetrized vectors. So, we just look at this second line. Second line, and this gives us a linear system because for each entry on the left, we have one equation on the right, and Cijk here are unknowns. So we have a linear system with m cube variables Cijk and n to the 6, around n to the 6 equations. And if m is less than n squared. If m is less than n squared, that is like number of variables are less than number of equations, then we can hope to solve this linear system. And actually, you can imagine that intuitively when the AIs are chosen randomly, this intuition actually works. So then we just can solve this linear system, recover the CIJKs, plug them into the first equation and get this. Equation and get this tensor which we want. The main takeaway is that we need to find the span of the polynomials AT. And here we are thinking, I mean I will keep going back and forth between vector view and the polynomial view. So the algorithm outline is as follows. You have the input polynomial, you find the span, then you desymmetrize using that previous idea, and then you Previous idea, and then you just apply tensile composition. So, now let me tell you about the main step, which is how do you find the span of the polynomials. And this also is broken into two steps. First, you find the span of A t of x times xi xj for all pairs of variables ij. And then you kind of remove these extra xixj terms. Xi and J terms to get the span of the A t's. Okay, so let's try to find the span of AT times Xi Xj. Here T is ranging in M and Ij are in N. Okay, so I already went to the next bullet, but any idea on how you would go from a degree like how Go from a degree, like how you would find the span. So, one hint is that this is a degree six polynomial, and this is a bunch of degree four polynomials. Partial derivatives? Yeah, so the idea is to take partial derivatives. This is what GKS did. So, you can take this, you can see, will give you a lot. This, you can see, will give you a lot. You have a bunch of partial derivatives you can take because you can take one in every for every pair, ij. So you just compute all these possible partial derivatives. And when you write out what the partial derivative is, it looks like this. It has this form, nice form, where it is sum of 80 times some degree to polynomial. And it doesn't matter what this degree 2 polynomial is, really. But you can see that the span of the partial derivatives clearly lies in the subspace we want to find. But if the subspace we want to find is also lies in the span of the partial derivatives, we would be done. But the immediate issue is that the span of the partial derivatives is something like n squared dimensions. Dimensions and the space we want to find is around like m times n square. So this can never really hold. So the way GKS actually solved this issue is to look at restrictions of the partial derivatives. So consider these polynomials Bt's where we just set the last n minus n variables to 0. Minus n variables to 0. And then instead of finding the span of the AT's, we just find the span of the bt's times xixj. And these variables ij are now only ranging in L. Think of L as some small number like root n. And now we can do the same trick. So first compute all possible partial derivatives, i, j, are ranging in n, and then set the last. In n and then set the last n minus l variables to 0. And again, it's easy to check that the subspace we computed lies in the subspace v prime which we want to compute. But now we have gotten over the previous trouble because the dimension of u prime is like n square, but the dimension of v prime is just m times l square instead of m times n square. Instead of m times n square. So when m l square is much less than n square, we can at least hope that b prime will lie inside u prime. And in fact, we show that when a t's are random, then these two subspaces are actually equal. Okay, yeah. So now we are given this, let's say we are given the span of Bt times Xi Xj, where T ranges in M but Ij only ranges in L because now we are left with only L variables. Our new goal is to find just the span of the restricted polynomials P1 to Pl. So, here we also use some trick. So, we take a random quadratic V0 and compute V span of V naught times xi xj and then compute this intersection. So, why is this even useful? So, first let's see why this is useful. So, now note that Bt of x times B0 of X lies Of x times B0 of X lies in both these subspaces. So again we have that the span of Vtx times V0x lies in the intersection and when we divide out by the V0x, which is like the polynomial which is kind of extra, we get just span of Bt of X. Of Bt of X. The span of Bt of X lies in the subspace we have computed, but again we have the same issue that maybe the subspace we have is much bigger. So here we show that actually this subspace is not much bigger when the P's are random. So this is the crucial lemma that we show. So we show that the only solutions to this equation when we This equation when v's are random and r's are unknowns, is the obvious solutions. So, you can see that this equation has some kind of trivial solutions, which is like you set the first polynomial to be v2 and the second polynomial to be minus b1, and everything else to be 0, and then you can set the first polynomial to be v3, and second to be. Polynomial to be V3 and second to be, or the third to be minus V1, etc. So this has some T choose to obvious trivial solutions, but in fact these are kind of the only solutions to this equation up to taking linear combinations. And it's not too hard to go from here to here. This shows that the span of the intersection of the span. Of the intersection of the spans actually gives you the span of V naught times Vt. So earlier I told you that this lies inside the intersection, but we can show the other direction. And the corollary, again, once you're given this, you can just divide out by P0 to get the span of the repeats. Okay, so Okay, so this is like the overall outline that we did. We first computed partial derivatives, then we set a few variables to or most variables to zero. And then here we said that the subspace that we computed is actually the span of the Bt times Xi Xj. Then we sampled a random polynomial and took intersection and divided out to get the span of the Bi's. The PIs. And here we use the fact that there are no non-trivial solutions to this equation. So now going back, how do we use the span to get the tensor? So actually what we did is we, instead of just restricting the first, last, n minus n variables, you can restrict various subsets of. You can restrict various subsets of variables to 0 and you get various spans, and then you can aggregate all of this information to get the desymmetrized tensor. And in this aggregation, you can use some carefully chosen subsets of vehicles. And now, let me say just a few words about. Let me say just like a few words about how each of these lemmas is proven, like what goes into proving these lemmas. So, what I mean is like the lemma about why when you take random polynomials, you get the subspaces are equal. So, let's just focus on this desymmetrization step, which is kind of the easiest step here. So, recall that in this step, what we did. That in this step, what we did is basically solve a linear equation which looks like this. So, you have coefficient vector c and you have some tall matrix, and on the right-hand side, you have the symmetrized tensor. So, the crucial thing is that this yellow matrix is some random looking matrix because our AI s are random. And another crucial thing is that it's very tall, so the number of equations is more than the number of variables. Equations is more than the number of variables. And this inversion step will work as long as this matrix has rank m cube. So the main observation by this in the analysis is that each step of the algorithm, for example the desymmetrization step, is some simple linear algebraic operation and the correctness of that. And the correctness of that step relies on some particular matrix being full rank. And the matrix you can actually write out explicitly. It depends on various, on all the entries of A's. So you can get kind of noise resilience also because this each step the noise resilience corresponds to instead of full rankness of some matrix, it corresponds to Of some matrix corresponds to well conditionness of some of the same matrix. So instead of proving full rank that the matrix is full rank, all you need to do is prove a singular value and lower bound. And the entries of these matrices that we need to analyze are actually low degree polynomials in the coefficients of the x, which we have assumed are Gaussians. So now you may see the connection. So, now you may see the connection. We need to analyze structured matrices with correlated random entries. And to prove singular value lower bounds on these matrices, we reduce to proving singular value upper bounds. So, let's say this was your matrix on this tall matrix on which you had to prove that its kth singular value is lower bounded. Let's say the short dimension. Let's say the short dimension is k. Then you consider the matrix G G transpose and like if you think of G as a Gaussian matrix, completely IID entries, it will look like n times identity plus some error matrix. So the diagonal entries of this GG transpose are very well concentrated. This is not This is not hard to show, but the off-diagonal part of this matrix, so this error term, we can show that it's little off n with high probability. So just to reiterate this, like suppose you're given a k cross n Gaussian matrix G where k is much smaller than n, it's easy to show that this is n times identity and the off-diagonal part we can show. We can show that it's O tilde of nuk times n. So this is easy and this requires more work. But if k is sufficiently small, like n by poly log n, then we can show that the minimum singular value of g is bounded away. So basically the trick to showing these lower bounds is to These lower bounds is to consider slightly rectangular matrices, which doesn't affect the parameters of our algorithm in the end. So we just lose all these polynomial factors. Yeah, so most of our work is to count this the spectral norm of the off-diagonal matrices. And to do that, we use the trace. And to do that, we use the trace moment method. And this was introduced, or this was used a lot in the analysis of some of Square's lower terms, all the graph matrix machinery. And that's what goes into analyzing all these off-diagonal matrices. So finally, we use S singular value upper bounds using trace moments methods to give the noise reduction. Methods to give the noise resilience of a record. So, this is a summary. Yeah, and actually a thing to mention here is that we don't know how to prove that some matrix is just full rank without proving these singular value lower bounds. So, that like if there is Like, if there is a cleaner way to show full rankness without doing all of this, that would show at least a cleaner analysis of the exact case of their current bots. And two of these open problems are solved now. So you can try out this open problem. Wait, which one is still open? Sorry? The LGOR. So I guess Pravesh said that this is. Pravesha that this is known information theoretically and also this inverse form. Thanks. So for the worst case problem of loading mean zero mixtures, I guess we don't expect it to have the same kind of hardness as worst case problem. Worst case both. Worst case zero mean tausion extracts. Oh, same as we probably don't expect the kind of like parallel get things at all. How do you get the scan of the screen? Yeah, that's something we are working on. But the answer is Does anybody know what time we're supposed to come back? I think it's at 1.30. Sounds rather than. I thought it was at 1.30. That sounds weird to late, yeah. 1.30 would be here by 1.30 130 to 1 to 250. Yeah, yeah, yeah, 130. 130 to 230 to 330. So it's like a reasonable thing. Like Japanese. Yeah. No class in it. Yeah, we are. Yeah, no problem. Well, no, I think they don't do it. I feel like it's very pretty because you can design and mix and deploy the innovative. Whereas it's only one thing, you know, No, not both ice cream. Some others and then you can do it. You can all end up with the control. I really want someone who's not thankful that you had like a day to get back. I kind of had a data point. Okay, because basically the point is to like the set of points which take exposure to the microphone. Yeah, exactly. Yeah. This is sort of like, I mean, naively if you just look at what the mini-plag comes from, the only thing you know about more than worse is that just like probably one minute something. But like if you're if you're just if your initial screen is exponentially complicated, then like Because that's actually complicated, but like for all the really slices, they can talk really, really fast. They can be a lot of differences. If I look at the average, I don't know, but if you initialize a friend, the conditions are changing how long you can speed up. So that should just be because the fundamental way all things are. That'd take a long time to get this. I think about 15 minutes. The whole thing is true, right? Like, if I look at, if I look at, if I can identify within the line of space a set of integration functions, I'd recognize that. Yeah, yeah, that I should actually be able to. And I know that the probability of within the set, it's roughly. Within the set, it's like roughly let's say up to the less platform, but it's probably because it's not going to be a good thing. Yeah, but I guess in the worst case, you know, doing this uniform thing might not actually make you strong ten. You have to like you it needs to be very You have to like it needs to be very powerful, basically. Maybe it's certain sense of pictures, but it's not important to it. Yeah, I think we have to do that. So what I'm saying is I never started. So so at the end of that it's super simple, right? Oh well just uh that's what our maps work. That's what that's worked. So you take matrix groups versus symmetric groups have a plus minus one. So they say, forget it. Alternative group. You know, like, let's look at the alternative group. Even then, if you're not careful, you can, like, you know, there's a homomorphism between that and matrix groups. And matrix groups have this thing that, you know, we look at the characteristic homology of A times B. It's like an alphabet reduction, like the alphabet. Exactly. Exactly. Exactly. Exactly. Exactly. Yeah, this is the same thing. So that's right. I was glad you'll not look at what is that. So No, but it's also cool. But for general routes, you want to specify the same thing. We don't know the connection not to be not generic connection, right? The NC lies forbidden but I don't imagine it should be generic, but I was wondering if our techniques would overlap as well. That's a great question. Fantastic breakfast. That's what really is. So what like if you if you hope to get an improvement with computational, like which part of it will fail, right? Yeah, so it's tricky. I mean initially we were thinking of when you go through the argument and like general the PASIC information. Initially it was having this information and the disease itself. Yes. Just to give you some share of share possibility that I think right I think these are very very general                    