So I'll explain what that word means in a minute. But first, some sort of strategic philosophic background for where I'm coming from. So I'm a physicist, a biophysicist, and I'm interested in systems where you have geometric complexity and topological complexity at the same time. Of course, for this audience, I don't need to convince you that topological complexity networks, complexity networks are interesting, especially those with loops. But we're really interested in sort of bringing these things together somehow, so sort of including geometry. Somehow, so sort of including geometry. We've heard a lot about the sort of spatial embedding properties of being applied to networks already in many of the talks today in many beautiful ways. So, we're interested in that kind of thing too. We have a series of things where we're kind of starting from sort of more traditional morphogenesis, non-network-based morphogenesis, that of course doesn't include mechanical networks and junctional networks of apricot connections and junctions, things like this. I'm not going to talk about that today, but if you're interested in talking about that with me, I'm happy to talk about that with people throughout the question. Note that people throughout the rest of the time. But sort of to accomplish this kind of combination, you know, a lot of stuff needs to be done. And I think today I'm going to focus on new approaches and new techniques for modeling spatial networks and new characteristics. That installment thing, this word that sounds made up, it is made up in some sounds, is one of these things. But I'm going to sort of motivate looking at these network characters. Looking at these network characteristics with organ networks, so networks in developing and mature organs. So, this is an example of sort of a network extracted from segmented data in the liver. This is in the liver lobule. The purple networks here is the blood, the sinusoid network. Green network is a bile network, the bile canaliculi. And you can see that these are complicated, enmeshed structures that are sharing spaces. Structures that are sharing space in three dimensions. And so we're going to be thinking about some of the ways that we can build characteristics for networks that take advantage of or are sensitive to their spatial embedding and eventually are sensitive to each other. And so I'm going to start with a geometric one. I hope I have time to tell you kind of two mini stories in an association of data that's not at all complete. But I think it's interesting. But I think it's interesting, and I hope you'll find it interesting as well. So, the geometric angle first is going to be thinking about what you call your neighbor. So, taking a step back even from networks to begin with, you have some point cloud, and we'll reconnect the networks in a moment, but for the time being, let's imagine you just have some point cloud. And I ask you, let's look at this point right here in the middle, who are its neighbors, right? And so that asks. Neighbors, right? And so that actually, on its face, is an ill-defined question. We need to come up with a good definition of neighbor. And the working definition of neighbor that many people use, including, for example, persistent homology, which came up in the last discussion session, are things that are somewhat coarse, right? Things that are, you know, just you have some threshold and you look at, you know, everyone that's within that distance threshold is your neighbor. Or you have some number of neighbors that you want to find and you find the end closest ones and for that, that's those are your neighbors. For that, that's those are your neighbors. Or you do something slightly more sophisticated, like a go on your time relationship. And so, oh, yeah, I showed you against the slide. These are that particular point set, these are the examples of the most common neighbor assignments that people use these things, right? So, nearest n, threshold distance, and a Delano graph constructed over both points. And, you know, for this set of points, these two, depending on your use case, these two maybe are okay, probably not. Delaney graph. Okay, probably not. The monograph is much better, but you know, what if that's your point cloud? That's your point cloud, right? Perfectly legitimate point cloud. Who are its? What are its neighbors? Every single one of you in this room knows who the neighbors are of those particles. The Delana triangulation sure as help doesn't, right? And neither do the others. And so, you know, we think there's kind of space here to do a little better and to sort of come up with a characteristic. You know, for point patterns, we're really with an eye on networks because you can think about Because you can think about eventually these point clouds coming from the branch points of an existing network. We can make models of networks from the branch point locations. So there's this beautiful classical concept of the relative neighborhood graph from 1980 in computer vision, the really, really early days of computer vision, that said you can make a network, a neighborhood network, by making the sort of circular halos around two points and then looking at Around two points, and then looking at the loon intersection of the halos and seeing if there's another point in there. And if there is, then they're not neighbors, and if there is not, then you make the draw on the edge and you make the neighborhood. So now for the cat, it's not perfect, but it's much better than the others would do. And one can generalize this, there's a structure called the beta skeleton, that is, again, from the 80s and then has been sort of dropped for the most part, by essentially changing the relative size of. Essentially changing the relative size of those halos, right? You generate more and more either squat or elongated exclusion areas. And this is just a continuous function. So you have this continuous family of a one-parameter continuous family of this sort of neighborhood assignments that come from this beta. And so again, through the cat as an example, you can sweep through and you get sort of different, depending on what your magnitude is routing. How your magnitude better works on the beta. You do something terrible down here. And it turns out, like, some of the betas are associated with very well, well-studied specific graphs. There's a lot of really nice properties here. But I would like to sort of focus for today, because I don't have a lot of time and I want to tell you another story about this problem as well, on building these sort of properties. You know, you have a point pattern or you have the network branch points that you started with, and you network. You construct this family. You construct this family of betas, right? I should say that, like, as beta is growing, right, your exclusion area is growing, and it always includes the smaller exclusion areas that came before it. And so that means that every network, every network, as you go to smaller beta, it contains the networks from larger beta. You can think of you're increasing beta, you're removing edges from the same network. You're never adding more. We're never adding more. And then you can look at the network properties of this family and look at their scalings as a way to sort of determine and find sort of hidden information, spatial information about systems. So one nice thing to think about is the edge length distribution. And so one of the things that this beta spell approach is good at is discovering when you have an ordered arrangement that's perturbed. So this is obviously a square lattice that. This is obviously a square lattice. All the points have been moved around a bit. And at beta equals one, you get the square lattice, and you get a bunch of the diagonals, right? And then as you increase beta, the diagonals are start falling away. And then you get back to the square lattice, the pretty square lattice. And then eventually, if you go above two, it'll start falling apart. Started falling apart from here, but it'll start falling apart from here. But the point is that you pick up this sort of bimodal distribution as a function of beta when you have. Much in a beta when you have a perturbed, even a very strongly perturbed ordered lattice underlying your structure. Okay, so remember that. We're going to come back to that. Okay, and so let's think about, you know, for these sort of these spatially embedded networks, let's think, I showed you the liver already. We also are going to compare the liver to the pancreas. So the pancreas, this is the ductal network of the pancreas. The ductal network of the pancreas, then embryotic day 11.5 to 14.5 is the dorsal side and the ventral side. They're sort of easily sort of identifiable. And these are the experimental fabrication, Mark Ben Holin, Carl and Shohen from the Anglican Baton lab. And so unsurprisingly, it's growing. So the two plots just show that the total network length and the edge count are increasing bigly exponentially, no surprise. Not no surprise. More interesting is that this system becomes very, very cyclic. It forms lots of groups, right? So the percentage of the system that is cyclic by edge goes is between 60 and 70 percent for most of the embryonic time that I'm showing. And then it completely falls apart at the end. So it has this sort of two-day falling off a cliff of how much of it is simpler. So that's sort of an interesting topological signature. And then if you look at the distribution of the x-lengths, right, you find something really surprising. You find that, and I'm showing this is for one of the sort of middle periods of development, but this is actually true throughout, even after it falls apart into a tree, although the slope changes a little bit, the characteristic length changes a little bit. But you have this beautiful exponential distribution of. Distribution of etchel. It's really, really, really short. And that's surprising because many other similar networks don't do that. So if you look at, for example, networks coming from slime molds or the liver data, you get something that's very strongly non-exponential, perhaps with an exponential tail, but where the meat of the distribution is doing something very different. And so let's talk, let's go back to Deliver for a second and use the tool that I showed you, this baseball tool. That I should remove this basketball to see if we can understand a little bit about the sort of spatial embedding of the network. So, again, so we take the actual network. This is just looking at the blood vessel, blood blood network for the moment. You take the branch points of the network as your point cloud, and you can, of course, you can construct a Delaney triangulation. I already showed you that it's bad for the cat, it's bad for the cat, it's bad for the liver. Bad for the liver. You can do the Gabriel graph or the relative neighborhood graph. These are beta equals 1 and 2, turns out. Or you can find the beta. Remember, so beta, as you increase beta, you steadily lose edges. So you can find the beta where you match, exactly match the same number of edges as the actual liver network. And so, again, you see in the actual network, there's this sort of alignment of the edges that is missed by the dispersion of data. That is missed by this version of beta because it's spatially isotropic. But aside from that, the topology is pretty good. I had a, you know, there was a version of this with a Jacquard similarity. The Jacquard similarity is good. You're going to take my word for it. Talk to you about later. But the important thing now is to forget about the specific matching beta and think about what happens when we sweep the betas, right? To look at this family of betas based on the vertices. And so the edge length distribution from the empirical graph, it's the blue one, it's sort of strongly peaked. It's the blue one, it's sort of strongly peaked. It's a big tail, you don't see much. But if you sweep through beta, look at what happens. Right? There's this beautiful bimodal peak appears that you wouldn't have seen if you were just staring at the data itself, or even at the 1.8, the orange one, the best matching beta. You don't really see it, but you pick up this beautiful secondary peak. And so there's this hint that there's this sort of beautiful underlying, probably perturbed, but underlying. Properly perturbed, but underlying order present in the liver structure that is not at all present. Exponential distributions of egg lengths come from things that are totally, that are Poisson length. So one expects the pancreas to be somehow completely disordered and the liver to be somehow much more strongly ordered in a very interesting way. That's interesting, but it's also complete. And again, this is to remind you: this is likely coming from, you know, when you have an ordered structure, you have these sorts of. You know, when you have an ordered structure, you have these sort of secondary lengths that are kicked out that you can see in this beta suite that you couldn't necessarily see otherwise. Okay, and so that's sort of little tidbit story one. I think I'm doing okay on time. I thought I'll just look out a little bit. Okay, so I want to turn now to what are the topological effects, right? So, the sort of edge-length distribution, of course, is really in some sense about the geometric embedding. And so. And so, what about topological power? And so, I think one thing that's really beautiful about working on a 3D system, right? As a physicist, my inclination is, you know, make things smaller dimension, make things simpler, make things as easy as I can so I can actually analytic things and solve them. But as easy as you can is an important part of that, right? There are certain things that happen in 3D, don't happen in 2D, that only happen in 3D. One of those things. And one of those things is linkages. A topological linkage of two separate curves in space cannot occur in 2D. It can only occur in 3D. For those of you that are cognizant, it also can't occur in 4D or higher, but that's another story. Only 3D. And so maybe, these are both networks that with lots of loops and lots of cycles, you can't have bile getting into your blood, so they can't actually be interconnected in a real world. They can't actually be interconnected in a real way. But they're sharing space for functional reasons, right? Like the liver tissue, like anticytes, are filtering the blood of wild salts and things like that. So they have to be closely associated. Maybe there are linkages here. And so this is where InSNARL came from. So InSNARL networks are networks that are pairs of cyclic networks where many of the cycles of one are topologically linked to many of the cycles. Are topologically linked to many of the cycles of the other. And so this is something that we sort of expected when we first wanted to sort of dig into this. We expected that this was a metric that existed, but we couldn't find anything. If any of you know any historical, anything that sort of goes in this direction, let me know. But we couldn't find anything. And so we took an approach to build a characteristic for this. I'm going to go through the background on how to build it. But what you really need is two spatial networks sharing 3D space, the network. 3D space, the networks have to be distinct and separate. They need to both have cycles, and at least some of the cycles from one need to be top positively like from the cycles from the other. This goes back a little bit to the negative space that people were talking about in the discussion earlier. So you have a single 3D cyclic network that gets negative space is somehow maximally installed. Like every loop in the negative space will be linked to a loop in the positive space. And so the degree to which this isn't maximally installable. This isn't maximally installed, it's also telling you something about how one network relates to the mega space of the other, if you like. It's easier to think about it that way, perhaps. Okay, so background. For those of you that don't know what a link, you know, what a linking number is, so there's this nice way to count like sort of how what the multiplicity of linking is in space. This classical topology from mostly from the 19th century or early 20th century. Basically, you Basically, you assign an arbitrary direction to both of your curves, and then you give a plus one or a minus one to each class of crossing that generates, with this formula, generates a linking number. There are some very subtle things that you can do that don't have a licking number, despite the fact that they are actually linked. So, one, there are more complicated tools than this to deal with that, but we just want to make it easy for now. And also, sort And also, sort of algorithmically, we're going to do this computationally. You know, if you have a big data set, right, like many of the links maybe sort of side on, or in order to get this sort of nice projection of how the nice process, well-defined processings, you need the right viewing angle for every single thing. And so instead of changing the viewing angle at every single time, you just, you know, it's just easier to just do what Gauss did. Gauss was a genius, just don't reinvent the wheel. So Gauss, there's this beautiful integral. So, Gauss, there's this beautiful integral formula for calculating linking numbers. So, we just follow the curve and it's the two curves in space and calculate the sample of the danger vectors as you go around. And it gives you, it just spits out the one thing that's multiplied. Beautiful. Okay, so we'd use that. And then the other concept we need is we need to think, you know, what does it mean to talk about cycles in a network, right? And so, you know, this is a, I think you guys maybe, this is preaching with a choir, but you know, there's one could have. You know, there's one could have just the individual placettes, or you could have longer things, you get the whole thing, whatever. But it turns out that you can put this beautiful vector space construction over in a network that allows you, it's equivalent to the first homology group, that allows you to, it allows you to sort of generate a basis set of cycles in your network from which you can construct any cycle that you want. So, this is just an example. So, this is just an example. It turns out you can generate that. There's many, many, it's very degenerate, right? You can make many choices. There's an obvious choice of just all the plaquettes. This is just showing you could do this one instead if we wanted. The light blue is a spanning tree on the network. A spanning tree always gives you a unique version of your basis set. So, algorithmically, it's sort of easy to set up. Okay, and so what do we do to make this characteristic? We construct a basis set for the cycle space of the network. For the cycle space of the network. As I just said, all the other cycles are combinations of those basis cycles. We go through now pairwise of all of our basis cycles and check what their linkage status is. Not within a network, but from one to the other. So we go down all the cycles on one list and check if they're linked with circles on the other list. And then we just kind of throw them together in a table, right, and call it this linking matrix, L. And so for this linking matrix, L. And so for this, this linear matrix L is for this particular example. But L is like not covariant. It sort of hugely arbitrarily depends on the basis vectors that you chose, which is, again, sort of degenerate. But fortunately, there's this mesh matrix constructor, which is a map from the cycle space to the edge space that lets you sort of pull back this linking matrix from the cycle space onto the edge space on both sides. With an edge space on both sides. And that gives this new tensor object that I'll really call a tensor because it is covariant anyhow, because we got rid of all the queritus. Lambda, it's still not square, because of course the number of edges in one matrix and the number of edges in another may be different. And so then we work with just the square, the two-sided square version of it, which is the installment operator. It's a linear operator. Thank you. And so if you look at just the diagonals, just the dumbest. The diagonals, just the dumbest possible thing, diagonal values of the square matrix. The darker the color, the higher the magnitude of the diagonal. It really directly corresponds to the edges that are the most important for the installment. So it gives you, you know, this gives you this sort of super easy output on installment. One can do much better than that. I'm not going to go into much detail on it, but one can look at the spectral properties. At the spectral properties of this thing. And you find the sort of eigenvalue distribution set meaning and the eigenvectors are also telling you about what are the edge configurations that are sort of somehow the most important for generating instarment structures. Okay, and so if you then look at this instarment in, and this is super, actually, it is no super preliminary. Is the most supercliminary. We don't make it this yet. But if you look at Starlamet in the liver, you find that there's this, this is sort of postnatal weeks for a mouse. And the frequency of the non-zero eigenvalues is telling you about how unsnarled there is. It's a metric for sort of how many in-snarl loops there are, proportionately. And so it's dropping over time. Over time, sort of the networks are coming apart, and then it spikes way up, right? And then the mature state of the liver is one that is highly instrumental, it is not maximally instrumental, so it's not, one is the negative space of the other, but it is very strongly in snow. And so if one then looks, and again, I don't, unfortunately, don't have the thoughts because it's so preliminary, but so you have to take my word for it for the time being. But if one looks at the pancreas and the liver sort of as a duology here, organs that sort of. Here, organs that sort of have some, in some sense, similar function, in some sense, similar sort of cell types and similar sort of gross structure. Like, of course, a biologist is going to murder me for saying that, but a certain zoomed out scale, they're similar to organs. But one can tell, one can, of course, can tell from biology, they're different, but one can also now tell, you know, from these sort of network properties that they're different. The blood networks in both cases are very, very cyclo-rich, and they stay cyclo-rich. Very cyclo-rich and they stay cyclo-rich throughout all time. The other secretory network involved, so the bile network in the liver is cyclo-rich in embryonic development and remains cyclo-rich throughout maturity, and there's this funny non-monotonic process that occurs. The pancreas is different. The pancreas is cyclo-rich, you know, again I showed you earlier on, it's cyclo-rich in reactive development, but then it completely falls apart into a tree-like mature ductile member. A tree-like mature ductum number. There's this difference in the edge length distributions that I think is likely connected to this topological structure somehow. So there's this lattice-like or this echo of order in the liver that is completely missing in the pancreas that appears to be completely disordered in some sense. And then finally, from the installment, the sort of purely topological point of view, the networks are installed early and then become highly installed at this point of dip. Installed because now you know how to dip in the liver in maturity. And the pancreas, this is the one thing I didn't actually show you, but they are also installed at early times. And of course, when it falls apart from a new tree, a tree has no more loops. So a tree cannot be installed with, even if the other network has seconds. It's not installed. So the installment goes away. So that's the end of what I want to show you, two very quick advertisements. So we put together this nice simulation package for broadly speaking. For broadly speaking, active matter. This is together, worked together with a group of Frank Peeler at Bush Monsterin and Prism. Broadly speaking, shape programming, sort of tissue morphogenesis, polar active fluids and gels, 3D vertex models can also be used for mechanical networks, can also be used for, with some tinkering for networks. But I think if you're interested in mechanical networks, this is quickly ads. So just a quick ad for Coplospam. It's a topologically informed simulation platform. So there's a platform. Another quick ad for the really nice sort of quasi-independent postdoctoral fellow program we have in Dresden. So, if you have any postdoctoral students, or you are a post-doctoral student, you're interested in coming to Dresden. The next call is going to be opening in August. And also, there's a great faculty visitors program we have for, you know, stays of a few weeks up to long sabbatical stays. So, for those of you that are tuning in there, and then I have to thank many, many people. Probably not so great at the time. Time. But importantly, Felix, Nchimai, and Jim, Watchabah, and Leeorg, and the rest of my lab, and anybody else. And thank you guys for listening. Do we have one question from a couple? Sorry. You should have you seen it. Sorry, sorry, sorry. Okay. The dimension of the lambda lambda transpose mesh is the number of edges by the number of edges. The squared version, yeah. So it's the number of, so depending on which, like if it's like lambda transpose or lambda transpose lambda, it's the number of edges in network one by the number of network edges in network one. Or number two by number two. And so, okay, can you admit that the network itself has an idea of the Fine, I have some idea of that for it to set up. Ah, yes, so we are doing that, yes. And so there's a nice mapping, and there's a nice mapping to a, sort of unsurprisingly, but to a bipartite network structure that underlies some of this. But we're sort of extracting some of the, for the spectral fabric. So yeah, I didn't understand. I didn't understand. So you say this non-monatony