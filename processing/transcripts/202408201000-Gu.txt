Thanks for the introduction and I'm very happy to come to Hangzhou once again to give a talk on quantitative homogenization and interacting particle systems. So yesterday, Jean-Christopho has already given us an introduction about homogenization, especially in partial differential equations, and in the end, he mentioned. Question and in the end, he mentioned the application in targeting particle systems. So, today I will continue more. And uh so maybe let me give uh a brief uh motivation about uh uh the the study in this direction. So uh uh uh uh I believe everyone uh in our high school physics course has already uh learned like brain emotion for the first time. Like Browning motion for the first time in the course of physics. Um we know the Brownie motion is uh the the random movement of uh uh the small uh particles or maybe for in the water or in the in the gas. And uh but uh we also uh remember in our physics course um there is interaction between particles. For example, uh when two particles when two uh when two particles when two uh atoms are very close, they we have uh uh such a such a reporting uh behavior. Uh and later in our uh course uh of probability we learn uh Browning motion and uh the mathematical definition of Browning motion is in fact uh independent in query is in fact uh stochastic processes of uh continuous charge. Of continuous, trajectory, stationary, and independent increment. So in fact, there is some tiny difference between the discursion in physics and the definition in math. So we we hope to uh explain such uh difference and uh to explain uh why they coincide. There are a lot of models uh try to explain uh the passage from intarting particles uh From interacting particle system uh interacting particle system to pronounce motion. And today I just uh give uh one example. So maybe yesterday Jan K Stoff mentioned uh a model. Uh maybe you have already forgot the definition of that model. Uh it doesn't matter because today we focus on this model. Um and this model is called simple symmetric exclusion processes and uh the description is very easy. So look at this picture. Uh look at this picture. Uh on the on the one-dimensional uh segment, we we put uh one particle, at most one particle every site, and then we let these pa small particles to do simple random work. That is to say it has probability one half to jump to left and one half to jump to right. And uh since we pose a exclusion rule, that is to say at most the one particle. That is to say, at most the one particle every site, for example for these two particles, uh they they meet some interaction. The particle on the left uh on the left, he cannot jump to the right because his uh navel has already be uh taken. And for the particle on the uh on the right, he cannot jump to the left because his uh navel is also taken. So they'll jump uh So, their jump only has one possible direction admissible. So, this is the model of exclusion processes. And we call it simple symmetric exclusion processes because this is the most elementary model. But you can imagine some more complicated model. And for this model, we know the stationary distribution is a Bernoulli product measure. Product measure of parameter row belongs to 0, 1. And sometimes we also call this model Kawasaki dynamic. Okay, maybe I switch too quickly and you look at this generator, this generator, explain exactly we have one particle on X and the particle on Y is empty, so the jump is allowed. Okay, but the next allowed. Okay, but the next slide I switch a little, I just uh remove that term. Uh it's a little magic. If you see this uh this change men for the first time I should explain something more. In fact, the two equations, the two generators, they are same. It's very special in this model. So because for exclusion process you can uh you can explore all the possibility for the Explore all the possibilities for this case. For example, let me do a simple calculation: eta x, eta y. We have four possibilities: one, one, one, zero, zero one, and zero zero. Okay. So the idea is for zero. For the these two uh these two case, they are trivial. So keep in mind, in our in our model, we treat all the particles indistinguishable. That is to say, all the particles play the same role. So for the particle one, one and zero zero, we can uh think they they jump, but we can also think they don't jump. We can also say because they don't jump. Because uh before and after the configuration is the same. So here I pause uh I can pose a rate to to say we allow the two particles to switch their position but uh the but it doesn't change the value of function. Okay. So that that is uh so that is to say in the first slide, I don't allow these uh two particles to jump, it's cracked, but if I allow them to switch, If I allow them to switch, it's the same result. That's also the case for the situation 0, 0. And for the case 1, 0, and 0, 1, it is as a preview slide, 1, 0, and 0, 1, they can switch. So I spent some time to explain this small trick, to explain why it is called Kawa Psychodynamic, because later we will use this word several times. So this is the first exclusion process. is the first exclusion process model. And uh we study its uh larger uh uh its anthropological behavior in large scale and in long time. And here we study its uh empirical measure with a diffusive scaling limit. And this is called hydrodynamic limit. Uh it converge as our as we expect to uh heat equation. Okay. So and we can further So, and we can further x uh study the equilibrium fluctuation. That is to say we truncate the density measure, the density row, and we make a scale make a scaling like central limit theorem. And it will converge uh to OU processes. So, this result for these two classical results, we can find it in a lot of reference. So, that's Reference. So that's all the story for simple symmetric exclusion processes for the at the very beginning. And later today, our our topic is about this model. So we we can as I say we can imagine a lot of various model. But for such exclusion model, sometimes when we change a little the description of model, it will change the long time behavior. For example, in this For example, in this slide, I add a jump rate for the model. So the jump rate is something called CXY. And this CXY, it depends on the configuration. In the sense, this function is local and spatial homogeneous with some detail balance condition. So this is a little technical, but roughly when this particle is a very important thing, When this particle it jumps to left hand side or right hand side, uh it will look at its neighbor, uh not only look at its neighbor if it is empty, but it will also look at the local be local configuration to see okay if the local configuration is uh is uh is uh sparse enough, for example. So, for such model, we just change a little, we add uh at a jump rate, so sometimes At a jump rate, so sometimes we also create a speed change, cow psychodynamic. And for its long-time behavior, especially for the hydrodynamic limit, we see the equation depe uh change a lot. It becomes a non-linear partial differential equation. Because here we have a function called D and it is a diffusion matrix. And the diffusion matrix depends on the density. Depends on the density. So, this is a non-linear term. So, this result was at first proved by Professor Funaki, Ushiyama, and Yo in 1996. And then later, Professor Funaki also proves the equilibrium fluctuation. And in the equilibrium fluctuation, it is a generalized OU processus because here we also add some diffusion metrics. And here, Diffusion metrics. And here, uh, it has some dependence on the density, but uh it does not uh depend on time because uh we studied the equilibrium fluctuation. So the solution is a constant solution. So uh I usually when I uh mention this uh equation uh I say okay this equation is terrible for especially for probabilists. But today we are in a PVE uh workshop. I think uh most of you know very well how to Most of you know very well how to treat a nonlinear partial differential equation. Uh but personally, although this equation is nonlinear, sometimes I still consider it as a linear equation, okay? Non-regorous way. So because this diffusion matrix, it is uh uh matrix which uh describes the behavior in large scale. So that is to say if you look at one particle in very short time In very short time, he he only feels the the diffusion macro is like constant. So I I I hope you you you you support this idea. So because the the the partial differential equation describes the largest scale behavior, so if we only look at one particle, in fact the diffusion matrix is still uh locally constant. So that is to say uh more or less we can we can think every More or less, we can we can think every particle makes a simple random work or like a brownie motion. But of course, in this model, uh it has an exclusion rule, so we know the behavior of one particle is never browning motion. And uh okay here I add one slide to to to explain the diffusion matrix. So in last slide I introduced this quantity. I say uh it is involved in the definition of non-linear partial diffusion. Of non-linear partial differential equation, but I didn't give it exact expression. So this matrix is in fact the proportion between uh conductivity and compressibility. And the compressibility uh is just the variance of the density at one position. The conductivity is uh described by the following variational uh formula. And uh if you still remember If you still remember what John Kistoff explained yesterday when we defined the diffusion matrix in random work, random conductance, it also involves a variation formula. I believe it is the first connection we can observe between the two two domains. Because here the variational formula is ex is exactly like uh quadratic form. Uh it's a little complicated, but uh believe me. Little complicated, but believe me, it's quite articulate. So it's natural to think if we can make some quantitative homogenization in this model and to obtain some results. But I will go back to this point later. So here I explain in the work by Funaki, Yujiyama, and Yong how they prove the hydrology. How they prove the hydrodynamic limit. So, in that work, the classical way is to use entropy method, relative entropy method. So, the idea is very simple. Recall the relative entropy inequality. That is to say, okay. If I remember correctly, the relative entropy inequality, entropy inequality. Entropy in quality it's like So from this formula, roughly we can say if two if the relative entropy Two if the relative entropy between two measures is very small, then the uh the two measures are very close. So roughly speaking, relative entropy is a method to to uh define the the the difference between two measures. So here I use the word difference because it's not exactly uh a matrix. So so the idea is quite uh quite natural. So the idea is quite uh quite natural. We use relative entropy method and then it surface to to prove the impact measure at time t is very very small. So I believe in all the all the um proof using relative entropy method the first step and the last step are same. The first step is to say we use relative entropy method. The last step is to say the relative entropy is very small but the materials is always in the middle. How we prove they are small. So here I prove they are small. So here I give you a quick answer. So in the past in the paper we derive it is controlled by such a lot of terms. But these terms require us something like ground war inequality in our ODE course. So if we have this term we we can use ground war but if we we use ground war uh the final result it will be H N T is smaller than exponential some T is smaller than exponential something of initial condition. We suppose the initial condition will be small. But there is a lot of uh errors coming from the from the uh from the remainder and we need to show the remainder is small. So uh historically we can we can prove this term as more uh qualitatively, but the quantitative estimate is uh missing for a long time. Uh missing for a long time. Especially, this term uh explains we need to take a function f n which is local and use it to approximate the uh conductivity introduced in last slide. And we know if we take the support of f n as large as possible, it can give us uh give us a good uh convergence, but uh we we don't know before uh which function we should take and uh what is the rate. So so this term is a myth. So, so this term is a myth. And uh there is something more hidden in other terms, but I think the due to the limit of time, I should uh I should uh continue to next slide. So today our question is, so if you understand uh the the the rough rough idea in the proof of hydrogen limit, you can you can believe the to obtain the quantitative estimate, uh some key point is reduced to the estimate. Is reduced to the estimate of conductivity and the choice of function and something related. And maybe it explains some connection in previous talk by Jan Kristoff, because Jan Kristoff gave an introduction of homogenized coefficients. Usually we it's not obvious why the convergence of homogenized coefficient will give us Homogenized coefficient will give us a lot of results like convergence of function and homogenized function. It's not immediate, but this hydrogen limit example at least give us some feeling. It is the case. So I also mentioned some related work in various directions of exclusion model and so we have different So we have different models in exclusion and different research topics. So for example, the hydrogen limit is the most classical topic. And the first work we always mention is by Guo, Papa, Nicolo, and Vara Dang 1988. And Guo here is Professor Guo Mao Chen of Kiki University. And later this method is developed This method is developed in many models by a lot of professors. And especially in non-gradient model, it is a work by Vardan who introduced the some technique to treat the case we have non-linear equation. And then beyond the hydrogen limit, we study equilibrium fluctuation, and sometimes we study equilibrium fluctuation and non-equilibrium fluctuation. Here I mentioned Equilibrium fluctuation. Here I mentioned the work by Professor Fanaki and Chang. They are both for a non-gradient model. And recently, there are a lot of study about no equilibrium fluctuation. I mentioned the work by Zara and Menazez. And tomorrow, Ben Feldman will also discuss his release work in this direction, no equilibrium fluctuation. And of course, And uh of course uh in last in last slide we mentioned the non-linear partial differential uh parabolic equation. So you may ask me what uh well uh well defined of this equation. Is it uh does it admit solution or is the solution unique? And it depends on the regulatory of the diffusion matrix. So but uh here we do not uh we don't need to worry because the diffusion matrix is proved to be uh To be C infinite, okay, smooth and bounded from above and from below uniformly. And yeah, in these decades, people turn the interest to asymmetry model and study a lot of KPZ limit. And in my slide, I mentioned the definition of diffusion metrics, Diffusion metrics. It is related to the famous green-kuber formula. And sometimes we also ask if the two definitions are equivalent and Professor Sasata has a very nice work involving some algebraic structure in this model. And we can And we can explore the mixing time. And sometimes mixing time is another way to measure the difference between the initial condition and the evolution. And I believe sometimes the mixing time is a little more precise than hydrodynamic limit. But due to such precision, the model is always a little limited. For example, in previous work, maybe in the force of three, people can't control. People can treat non-gradient model. And in KPZ limit and mixing time, sometimes we meet some difficulty. Okay, maybe not difficulty. It's too messy to treat the non-gradient model and the model involving random conductors. Okay. What's the time? Can you tell me? 25. Okay. Okay. And okay, yesterday, Lanki stole. And okay, yesterday Jan Christophe has already mentioned the homogeneous thinking theory, so let me record some this result. So we have a, before we start, okay, we come back to the PDE case, we have an equation with random coefficient and we can replace it by a homogenized equation and we say the solution should be similar and in fact we can explore something more about this gradient and the flux, that's a derivative. And the flux, that's a derivative. But the derivative is always say uh similar in the sense of weak because uh we we see some small fluctuation of the function. And uh and uh and I believe uh in the audience you know you know uh a lot of researchers in in this direction because most of them come from PDE community like uh um Jacqueline Leon, um Baba Nicolo, the George. Um Nicola de George and Francua Ling, and they have a lot of research in virus direction of PDE, and maybe you know better than me. And in this decase, we, okay, I mean, this is young generation, they focus on the quantitative result of stochastic homogenization. Because in periodic case, more or less, once we know the information in one period, we know everything, but in random case. But uh in random case it's not uh it's not a situation we we need to know something more. And yesterday we also mentioned subadditivity ideal. I recall uh I recap some some information we define a duty energy and uh this energy is subadditive so it converted to a limit. So it was observed maybe in at first by the Motomotica. I say maybe because uh I do not exhaust all the reference. This idea is quite Reference. This idea is quite simple. I will explain you in the next slide. So this idea was observed, but in a sense, qualitative. And at the end, Armstrong Kusimura Smart, they developed a lot of analytic tool to make this qualitative observation, in a sense, quantitative. And now this method apply to a lot of model. We have a lot of reference here. And in the audience, we will Uh and in the audience, Wei Wu, where is Wei Wu? Wei Wu, yeah, Wei Wu is since there. He has a lot of application in statistical physics, like especially gradient phi interface model, and Professor Funaki has also a lot of work in this interface model. And Rensley and the Villa model, Coulon gas, and now today we talk about interacting particle system. So I mentioned in So I I mentioned in all these models, some models share the similar spread, like diffu diffusive behavior. But other models like Coulomb gas and guardium phi, I think sometimes they they have different universality but this method still works. So so so it proves the the homogenization method is is a is a very strong tool and it can treat a lot Tool and it can treat a lot of different situations. Okay, so yesterday we mentioned your normalization uh method and uh in audience some some audience asked me uh once we define this quantity why they converge? And uh it's a it's a key question. And uh uh and yes the Janke Stov mentioned sub additivity but I would like to say okay Janke also highlight the key point is to is not all The key point is to s is not only to to to know they are subadditive, but to know they are closed in a sense. So this is non-trivial. I will give you an example. So I will give you an example here. You will understand why it is the case. So imagine uh this is I I give two pictures, uh NBA uh season twenty sixteen and twenty seventeen. So imagine I pick the best player in every position. Best player in every position. And I form a team. So that is the first team on the right hand side. We have Russell West Blue, Kejams Harden, LeBron James, Kwa Yee Lenard, and Antony Davis. This is the first team. And on the left hand side is Champion of 2017 Golden State. Okay, we have Steam Curry, Kevin Durant. So which team is better? So the question is: if we pick the best player in every position, does it Proposition: Does it uh better is it better than the death T? So that's the idea in the last slide. In the last slide, uh in that equation, we just pick the minimizer in every domain and we form it together to to be to become uh a solution. And you have a very quick argument. The the team on the left-hand side is better because uh it it is a champion. Okay? It's a very naive uh answer. But uh the problem is we never know whether The problem is we never know what will be if we have gathered the five best players in our position. So I some the personal idea and I think the homogenization is to say if the correlation is not so strong, then uh the right hand side should not be worse than the left hand side, okay? In large screen. It's not easy to find this picture, okay? Because usually people ask when I find some examples. Ask, when I find some examples, people will ask me: okay, if we take the example of World Cup, in which side you will put Macy? On the left-hand side or right-hand side? Because Macy is enchanting and he is also the best player in that position. So I find a example to exclude this situation. Okay. Um okay uh so let me mention our result. So in particle system we also prove such situation. If we glue the If we glue the solution in every position, in every domain and perform a function, then it is quite competitive and it will give us a good approximation of conductivity. But here you should also so this is a joint work with Professor Fanaki there and my students in Chojun College. He didn't come because he Come because uh he's in uh he's preparing for his uh uh qualifying. Okay. And uh okay. It seems it doesn't work. That doesn't matter. So so in this uh in this question, uh please uh keep in mind uh in our approximation we also have an inf and soup inf and soup before and uh and it is a little different uh uh between the Different between the usual homogenization theory, because in usual homogenization theory, maybe I do not have that suit. So, our homogenization is, in fact, uniform in function of the density. And as I explained in the first slide, this result will imply the quantitative Hydroland limit, that is to say, convergence in probability, and we have the estimate of the probability. Probability. And tomorrow in the third lecture, Professor Funaki will turn to another model called global Kawasaki model in the non-Guyodian case. And quite impressively, we can also prove the convergence rate for such for other models. So that is to say now the the tool of homogenization is well integrated in the in the in the In the in the uh relative entropy method and it can work in different situations. So this is uh auto IO proof and we have uh a lot of block and I so this is in in our paper and I I think the the the block in center, section four and section five is more or less universal for all the quantitative homogenization theory. It's classical, but we have Classical, but we have other blocks around it. For example, the blocks on on the left-hand side, they are more technical. So they we develop several technical inequalities. They are very different from the classical things case. And on the right-hand side, we have some the block on the right. We use some some idea in uh interacting particle system to to improve our result. And finally, Our result. And finally, we also in give some input of in relative entropy method. And uh okay here the key word about our challenge is the excreting rule and the cost of dimension. So but uh let on let me also mention some something missing in the talk of Baron Christoph. So yesterday finally he came to the model. A lot of you complain th seems he didn't uh define very carefully the model. Very carefully the model. He has risen because to define this model, we need quasi-regular D3 form. Little technical, so uh I just give you a slide uh to some information how this model is defined. So it uses quasi-regular dual form, it also use intrinsic derivative with respect to the measure and uh so it's a little um heavy, okay. But sounds like a little bit of a data. But something interesting is in our first paper, we have already developed something different in particle system, for example, modified cultural Pauline equality. So the first culture polynomial is classical in PDE, in elliptical equation. But we found this inequality cannot be proved in particle system. Because for the particles, we have too many perturbations on the boundary. And we need to prove something uh to to replace this inquiry. To replace its inequality. And finally, we find the second one. And we have one term on the right-hand side. If you do not look carefully, you see it seems the second term in red is similar as the first term. And this inequality is quite trivial. But keep in mind, theta zero is between zero and one. So the term in red is in fact smaller than the term on the left-hand side. And this proof is quite interesting, although it's a proof of solution of elliptic. Of solution of elliptical equations. We use martingale and we also use Wiedemann's whole feeding technique. So if you are interested, please read this inequality. And once we develop this inequality, we believe we have explored all the mystery in Hartmann particle system, but in fact not. So I like my students to explore. I I was quite conf confident we can prove it, but uh it took a long time. But it took a long time because there is a second and a third difficulty. And the second difficulty is about multi-scale Punk-Henry equality. So classical Ponka-Henry equality is we get a factor of diameter. But this Ponka Henry equality is quite, I mean, it's not sharp in homogenization. For example, I give you a week example. So, Pontagon inequality, okay, I should take Inquiry, okay. I should take back my word. Ponga Hen quality is sharp. But for some fun specific function, Ponga Hen equality is not good. For example, for a function like sinews. So if you use use Ponga-hen equality for sinews, you do not need to add a factor of the domain, right? Because the derivative of sinews is continuous. Okay, so the left, so it's a it's L2 norm and it's uh H1 norm is norm and its uh h1 norm is in fact singular for for for sneer's function. And it is the case in homogenization. So we we believe in homogenization due to the spatial cancellation the factor of error is too large. So we develop multi-scale Ponka Haying quality and uh when the function has some cancellation you can see on the right hand side every term gain something. And uh this idea also met some difficulty where Met some difficulty in exclusion procedures, especially due to the exclusion rule. So in previous work, in the work with Jan Christophe and Irena, although the model is difficult, we do not have such exclusion and we can prove multi-scale Poincar√©. But now, But now our case we have we have exclusion and uh uh it's a little hard to to do cancellation. So uh at some moment we we have no idea for this problem and uh and uh and but finally we find an idea. Um we find a way to say okay we we do some cost screen. The idea of course green is quite classical. We for example for For example, for a function uh defined on some point, we just extend it constant by muscle to a whole domain. Um it is used a lot in numerical analysis. But here another difficulty is uh the cost of dimension. So our dimension is very very high. So if we pay some error in cost query then the error will explode. Okay. So finally our idea is So finally our idea is to say okay we should lift in topology we should lift our space to a large space. So here uh I do two pictures. The picture on the left hand side is like it's closing processes and the picture on the right hand side is in every point I can put as many particles as I I hope. And uh the idea is to say adjust the mark if the position has particle. The position has particle. So I don't care how many particles it is. So from the right hand side to the left hand side, it's like a projection. Okay. And in this projection, if I say, okay, the right hand side, the number of particle is Boson. So I have parameter like alpha. So if I choose alpha correctly, alpha equals minus log 1 minus rho, then I put a function on the two-hand side. It's equal. Okay? Okay? Uh or more more directly, I just use Poisson random variable to represent the Bernoulli random variable. The advantage is using this idea, we can prove the weighted multi-scale Ponca Hay inequality. So that is to say we we want to estimate something. So this is exclusion. We want to estimate We want to estimate something in in exclusion. And we know it equals to something as a function in the space of independent particles. And from the right hand side, I know how to how to do multi-scale Punk-Hair inequality. Okay. It works very well. Okay, I have one equation. Okay, I I cheated a little here. I didn't show you how to because because here if I I make some uh estimate in multi-scale point of hearing equality, it is in double bracket. The double bracket is under the measure of alpha. Okay. But finally I need to put it back. Okay. Put it back to the lamp hazard. But uh the the result is uh is here in the slide. Uh it's similar but it has another factor of uh empirical density and this empirical density uh is quite close to constant thanks to the uh large density. And thanks to the larger deviation or concentration or measure. So that is the first idea. The first difficulty with matching our proof. And the second d uh novelty and the difficulty is you you you see our proof is in fact a little stronger than previous work. So it's not like uh we are given a density, we prove a homogenization, but uh it's we find a function and this function gives us homogenization for all the density. So in fact we ask you So in fact we asked this this question a long time ago and uh uh but we didn't uh think about it very seriously because we never need to use it. But in this work uh if you remember the relative entropy inequality in the at the very beginning, it's in fact approximation for all the all the density. And during the project with Professor Funaki, we negotiated several times. And he told me okay if you if you can only prove it for one density then you You can only prove it for one density, then you need to treat it in a dynamical way. Okay, I say, okay, it's maybe more complicated to put the difficulty in other paragraphs. So maybe we should focus on this density uniform convergence. So the idea is like is also in the normalization. So here once again, I put the best pair of best pair on ever. Of best player on ever position together to get a super team. And usually we need to assign a density for the function. But here I replace density by the empirical density. I like our player to say, do not ask me the strategy in the game. Just shoot. Just play yourself. But then we have a lot of errors to pay, especially here I make a picture. We have error 1, error 2, error 3. Some error comes when the perturbation in the middle, some appears on the near boundary, and some appears in the boundary. And the key error, main contribution error is something in the middle. Because in the middle, when we switch the position, we will apply. Which is the position we will apply Kawasaki derivative. It's quite different. It's so in the middle, because we take empirical density. So in the middle, when we switch the position of particle, the empirical density does not change. Okay, roughly it does not change. But okay, let me. Okay, let me think. So it it does not change a lot, but the function will have some perturbation. For example, okay, I should go back to this once again. So the main error, in fact, is a term three. Okay, it's not one or two, it's term three. And in the middle, in fact, when we make the change, the perturbation is quite small. But in the third term, Quite small. But in the third term, when we make the change, there's may it may happen one more particle enter the domain and it changes the empirical density and we need to handle this case. So roughly I wrote an estimate of order. So this term 3 power minus dm is a perturbation of density. And at times we have some regulatory of our density. It's 3 power 2 dm. 3 power 2d plus 2n. And such contribution is surface volume because it's on the boundary last term. And we also need to pay some error of local equivalence because usually it is good, but if the density is too our impeccable density is too slow, it will be terrible. That is why I make some truncation of density to avoid the call case at endpoint. At the endpoint. And you see in the last line, if you manipulate the choice of epsilon, n and m, it can be smaller than one with some explicit weight. So that is how we obtain the uniform convergence. And here I also mentioned the estimate is up to the error of decoupling. So because it is in Ganonica ensemble and in the Ganonic ensemble, the function in different domain is not depe independent. Is not independent. But they are not far from the case of independent case because on the Guan canonical ensemble, they are independent, so we case an error from this decoupling. And that is our final result. And thank you for attention.