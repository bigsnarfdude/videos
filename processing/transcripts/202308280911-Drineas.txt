I'm going to touch upon, I think, two of the themes of this workshop. So I'm going to talk about optimization a little bit. It's going to be the easy part of optimization, linear programming. And then I'm going to talk about how randomized methods, basically randomized preconditioning, something that we have known how to do for well over a decade at this point, can be used in the context of interior point methods. I'll also discuss a couple of open problems. I think this is an area that is full of open problems. Is full of open problems. So there are lots of things to potentially do in the future. We've been working on this for a while, so this is not new on our end. We've been doing this for four or five years now. I'm going to talk about some of the more recent results of ours. And by ours, there's a list of collaborators here that I should acknowledge. Jaime Mavron from the University of Tel Aviv, this is an alphabetical author. Agniwais was my student at Purdue. He's now at all. Was my student at Purdue, he's now at Oak Ridge. Gregory Dexter is still my student at Purdue, and Pama London contributed to this work back at the time when she was doing that work. She was visiting us at Purdue, did her PhD at Caltech, I think then was at Cornell, and now she's at UC. Before I start talking about this, let me advertise the Jing Olum Sim Summer School next year. So in 2024, we're going to be holding the Jingold-Sheim Summer School. Going to be holding the Jean Goldman Shein Summer School on iterative and randomized methods for large-scale inverse problems. Some of the topics of this talk, some of the topics of this workshop, as a matter of fact, will probably appear there. It's going to be held in South America for the first time ever, I believe. So it's going to be in Quito, in Ecuador. Even though it's in August, we've been told that the weather there is fantastic throughout the year. It's at the equator. Organizers, Tia Chang, Juan Carlos de Los Vegas, Rosier. Carlos de Los Reges, Rosie Renault, Alex Darson, and myself are the organizers. And Carla and Jodi are going to join us. Carla from Cambridge and Jodi from She's at the NSF now at Ball Street. We are going to open up the application portal later this fall, so I hope that some of your graduate students, or if you are a graduate student, will consider applying and joining us in Kito next summer. Okay, why optimization? Why optimization? Well, optimization appears everywhere. My training has been in theoretical computer science and machine learning. So, in a lot of machine learning algorithms, you are going to use continuous optimization. Not the simple stuff, not linear programming, but some linear programming, but also, of course, a lot of non-linear, non-convex, non-linear stuff. In theoretical computer science, a lot of combinatorial optimization problems in the G programming. Problems in edgy programming, for example, can be modeled, can be relaxed to linear programs, and then you solve a linear program, and somehow you use some sort of rounding, typically randomized rounding, to go back to a discrete solution. So it's a very useful tool, obviously. It's ubiquitous. We used over the years, we've done a lot of on a separate, on the other form of the workshop, we've done a lot of work over the past 20 years. We've done a lot of work over the past 20-25 years on randomized numerical linear algebra, which is basically sketching, sampling, and so on approaches to reduce the size of the matrix. So you have your input matrix, you make it smaller by sampling, by sketching, by sparsifying. And you use this sample, this sketch, this sparsified matrix in computations instead of the original matrix, or perhaps in conjunction with the original matrix sometimes. And this allows us to sort of speed up. This allows us to sort of speed up matrix computations. At the same time, we can offer strong theoretical guarantees, and a lot of these algorithms work reasonably well in practice. So, the question in our minds when we started this line of work was how to sort of combine the two. We started with linear programming because it's kind of the simplest operation. And there have been actually many other groups working on similar topics, and we try to draw some connections later in the talk. We do learn a programme. It's Monday morning. It's the first top of the workshop. I'm going to take it a little bit slow. So we define the problem, we'll talk a little bit about the standard ways to approach the problem, and then we'll see where we can improve things by introducing some randomized numerator stuff. So we are talking about the standard form of LP. A lot of the things I'm going to be doing, we started with kind of textbook stuff, this kind of stuff that we teach undergraduate students basically. So this is the standard form. So, this is the standard form. I prefer the vector and matrix notation. Of course, this is a linear algebra workshop. You are looking to optimize minimization problem for the primal, the so-called primal LP. We are looking to minimize a linear function. x is the variable, the vector of variables, subject to equality constraints, a x equals v, that's a standard form, and x has to be non-linear. Think of A as being an n by n matrix, m constraints, m variables. M constraints, m variables, p is of course in Rm, and the vector of unknowns, your variables, is an n-dimensional vector. Both m and n in general can be very large. Now, linear programming, one of the nice things. So, if you want to think of a picture, this is the simplex, right? So, the constraints kind of define this convex body, and then you have this line, and you try to figure out where the optimum happens, and we know that the optimum is at the vertex. The optimum is at the vertex. So at least the optimum appears at the vertex. Linear programming has duality and there is a dual problem associated with a primal problem. If this is a minimization problem, this is a maximization problem. Introduces new variables. This is going to be important. So you introduce the vector of variables y. This is m-dimensional, the number of constraints. You are trying now to maximize another linear objective. linear objective, subject to a constraint that now includes the variable y and the new variable s. Again, these are vectors. y is m-dimensional, s is n-dimensional, same dimensions as the originals, as the original vector of vectors. This is the primal variable, this is the dual variable, this is often called the slug variable. The dual variable is unconstrained, the slug variable has to also be non-negative. Slack variable has to also be non-negative. Why do I care about this? Because, for those of you that have done work in IPMs, they're quite difficult here in this room. This is going to be a primal-dual solution. I'm going to try to solve the problem, the primal and the dual at the same time. Okay? So I'm going to be looking at x, y, and s simultaneously. I'm going to try to solve both of them. Why solve both of them? Because strong duality for linear programming says that at the optimal solution, C transpose X star is equal to B transpose Y star. Star is equal to d transpose by star. So when you reach the optimal, the primal optimum and the dual optimum, they basically compensate. They have the same. Well-known stuff. We published most of these papers in places like NERIPS, ICML, JMLR, so machine learning venues. So we had to talk about applications of LP machine learning. There are quite a few, as a matter of fact. I'm not going to discuss those anymore. Our objective was the following. We wanted to. Was the following. We wanted to take the standard of the self methods for interior point methods, from, let's say, Steve Wright's book, for example, on interior point methods, and see if we could use inexact linear system solves. So, let me expand a little bit on this, and I'll go into the math details over the next slides. We are looking for practical algorithms, so the kind of stuff that actually works well in practice. If you look at interior point methods, there are various families of interior. Point methods, there are various families of interior point methods. You can do potential reduction algorithms, we didn't do that at all, or you can do interior point methods that are solving the so-called normal equations. That's what we focused on. Technical differences will come up. Within that second family of interior point methods, we cared more about predictor corrector algorithms. We also did long step, Dan Skillmont had done actually short step a long time ago, but we cared a lot more about the predictor corrector IPMs because they are as good as get. They are as good as getting some sense from a theoretical perspective, and they also work well in practice. In the theoretical computer science community, people get around predictor-corrector IPMs by using different tools. They use stochastic path-following methods, which are very nice in theory, unclear how well they're going to work in practice. They haven't really been tested in practice. They are based sometimes in fast matrix multiplication. Fast matrix multiplication. By fast matrix multiplication, I mean algorithms for matrix multiplication that run in n to the omega time, where this exponent omega is 2.3 or something like that, the best known exponent for matrix multiplication. Again, those are nice theoretically, whether they're useful in practice is a different story. Instead of doing like linear system solves with conjugate gradients or precondition methods like we would do in Mechanical Algebra, they might be doing things like Computer algebra, they might be doing things like inverse maintenance. If you've never heard of this, it doesn't really matter. They're kind of more theoretical, computer science, a little more exotic tools. Whether again they are useful in practice or not, I don't know yet. But certainly, they are not kind of the standard, more practical approaches for interior point methods and linear programming that we would know. And we wanted to understand what happens if in interior point methods, as you will see at every iteration, I have to solve the system of linear equations, what happens if I use What happens if I use inexact system solves? That's what we are looking for. And again, we are looking at the standard predictor-corrector methods, we are looking at kind of the vanilla approach to solve systems of generic approaches, precondition quantity creators, things like that. Let's see what happens at optimality. So if you think of, again, I'm going to be doing primal dual interior point methods. So at the same time, I'm going to be solving both the primal and the dual. Solving both the primal and the dual problem. So I'm going to be looking at all three variables: primal, dual, and slack at the same time. And at optimality, we know that only three conditions need to be satisfied. Two of them are trivial. You need to be feasible. Primal feasibility, Ax equal to B with x being potential. Dual feasibility, A transpose Y plus S equal to C. Again, those are just off the cell and S being non-negative. And the only, in some sense, interesting condition is the last one. interesting condition is the last one which is saying that either the primer variable or the slack variable have to be equal to zero. This is saying that xi times s i, all entries, this is elementwise vector multiplication, that for all entries of those vectors x and z, xi, si has to be equal to 0. So either xi or si have to be 0. For simplicity, I'm going to be assuming throughout the talk that we are looking at a constraint matrix, the matrix A, and constraints and variables. N constraints, n variables. I'm going to be looking at the number of variables being larger than the number of constraints. I could do the other way around, the number of constraints being larger than the number of variables. I could also be doing Laurent matrices, Laurent plus noise. So a lot of structures would work, but arbitrary M by N matrices we don't. Okay? So some constraint on the matrix A would have to exist. Tall and thin, short and fat, exact Lauren, or exactly. Exact Laurent or exact Laurent plus a bit of noise would work, but arbitrary stuff not. The solution has been on empty, but this is just a solid flag. So again, I haven't told you anything new, just refreshing what is sort of known about IPMs. I'm going to continue on the same theme, and I'll say beyond interior point methods, of course, the first algorithms to kind of solve linear programs was the simplex algorithm by Danczik, that goes back to the 1940s. Back in the 1940s. It explores vertices. We know that the optimal solution must be somewhere in the vertices. So this only explores vertices. From a theoretical computer science perspective, we know that worst case it's exponential time. It would be a major result to be able to bring it down to polynomial time. But worst case, it's exponential, because of course the simplex has too many vertices and you might need to explore an exponential number of them. At least current state of the art, that's what it might be. Can state of the art, that's what it might do. Interior point methods were kind of the third family of techniques to solve linear problems, because in between there was also the Eritzoid method by Kachin, this is 1970s. Indian point methods were mid-1980s, pioneered by Karma Carr. At this point, they are the fastest in theory, and they are often quite good in practice. So, over the course of the past seven years. Over the course of the past several years, I've been talking to people who do Cyclex, who do Gorobi, and they told me that in general they use some interior point method. Maybe at some point they're going to switch to Syntlex once you are sufficiently close to the solution, but interior point methods are kind of at the heart of their approach. So they work well in practice. So it's something that you want to understand a bit better if we can from the theoretical perspective. I like this visualization for IPMs, and I haven't found many that are better than this. I haven't found many that are better than this. I haven't found any that's better than this. At least my mind pointed to other visualizations if you have them. So, this is your simplex, vertices. Interior point methods are not going to look at vertices. They're actually going to look through the so-called central path. They go inside your control. And this is the central path right here. Of course, they're not really going to follow exactly this central path. I'm going to define this talk about it in a few slides. They look around the neighborhood of... They look around the neighborhood of the central path. So, this is the neighborhood, and the moves that the interior point makes will be somewhere in this neighborhood. At some point, they are going to get close to the optimal solution. That's going to be measured, the epsilon neighborhood of the solution. This is where you want to get. That's the stopping point. And somewhere here you stop. And if you want an exact solution, I think most practical software actually switches to simplex at that point and gets you one. At that point, and get you one of the variables. So, a visualization of a path for an IPM. I'll have another one for predictor collector IPMs in a couple of slides. It's a small crowd, I'm happy to answer questions. I promise I'll be on time despite questions. Good. Okay. So, again, relatively standard things. This is coming from a paper by Lesata. This is not my visualization. How do you measure how close you are to the opt solution? Well, you are doing primal. Well, you are doing primal dual, that's the advantage here. So you are looking simultaneously at the primal and the dual solution, and you just measure the distance. Remember, the primal is a minimization problem, the dual is a maximization problem. When the primal optimum is equal to the dual optimum, you're done. So you measure the distance. So C transpose X primal optimum minus dual optimum, you want this to go down. Basically, I'm going to call this the duality measure, duality gap, new. And if you New. And if you want more definitions, we're going to look at what is called the feasible predictor corrector IPM. And we've done work on invisible methods as well, but I'm not going to get there, if they get a lot more technical. The feasible predictor corrector IPMs start with a feasible point. That's non-trivial to find the feasible point. There are methods to do that, but it's non trivial to find the initial feasible point. If you're willing to start from infeasible points, the methods still work, but there is a huge penalty in terms of the convergence In terms of the convergence complexity. And I'll tell you what the penalty is doing. So, this is your set of feasible solutions. You need to start somewhere where you are feasible. The central path is defined as the set of points where xi si primal times slag is equal to nu. So that's how you define the central path. Remember, this is kind of exact. But you are not going to be on the central path. You are going to be on a neighborhood around the central path. Many ways to define the neighborhood. This is a way to define the definition. The neighborhood, this is a way to define the neighborhood that uses a two-normal up. So you look at xi and psi, you take the whole vector, minus mu, two-norm of that, less than or equal to some constant theta, some scalar theta, times mu. So kind of relative error close, if you want to think about it that way, to the central path. That's again one way, that's why the subscript two is there. You could use different norms, you could have different definitions. So how do you make them moves? So, how do you make the moves? In order to make the move, you are at the current iterate, x, y, and s, that's your current solution. And you want to go to the next iterate. So you need to figure out basically the directions towards which you are going to move. So you are at x, y, s. You want to figure out delta x, delta y, delta s, those are vectors. Same dimension as x, same dimension as y, same dimension as s. And those vectors will tell you how to move, where to move. And you typically To move. And you typically also want to determine a scalar times those vectors. That's going to be your move. And that scalar is determined using standard methods. We are not going to change any of that. So whichever way we are determining your path, your step size, basically, just use that. We're not going to change them. How do you figure out delta x, delta y, delta x? This is where the work is. You solve this system, it's a linear system of equations, but it's much better to think of this system Better to think of the system like this. And what is this? The first one is the so-called normal equations. Let's look at this a little bit more carefully. You're figuring out delta y here. Your constraint matrix is A D square N transpose. I have a square here to emphasize that this is a symmetric positive definite matrix. There's a vector here that's obligated, includes this parameter sigma, your mu and other things, but it's a vector. Think of it as p right now. Of it as P right now. So you need to solve this system of equations. Is it bad? Is it good? Well, Asia for stained matrix, so it's M by N. The whole thing is M by M. D square is a diagonal matrix that has the ratio of the primal divided by the slug. So the entries in this matrix are Xi divided by Si. As you approach convergence, this becomes quite recommended. So that becomes a bit of an issue. But that's what you are solving at every step. Once you figure it out. You are solving at every step. Once you figured out delta y, delta x and delta x are really simple. You don't need to solve a system of equations, you just need make expected problems. You just need matrix. So, this is where all the work is. That's very nice. Because now you can focus on just this system of just this linear system. Now, okay, you need to solve this, then you update delta s and delta x. That's the work you do in each iteration. That's the work you do in each iteration. In predictor corrector, we'll see in the next slide you do it twice because you actually take two steps in each iteration, but that's just a constant multiple. The problem here is that you're not going to solve this exactly. That's where our work started. We're not going to solve this exactly. Most existing analysis assume that you need to solve this exactly. And if you don't solve this exactly, then a lot of things become, then there is a certain amount of trouble that you are going to have. Certain amount of trouble that we are going to go. And that's where we're going to start. Now, let me say that practitioners, if you talk to practitioners, they don't worry so much about the number of iterations in zero-point methods. They worry a lot more about this problem, solving this system. That's where a lot of the work happens. In theoretical computer science, you worry about the number of iterations. So, a very important problem in theoretical computer science would be to reduce the number of iterations in material point methods from the current best mode, which is root n. Destimate, which is root n, actually, it's the square root of the rank of the matrix, but think root n at this point, to like n to the one-third, something like that. That would be fantastic. We don't know if this can be done or not, there is no lower bound either, but that would be a great result. Practitioners, however, worry a lot more about how to solve this efficiency. Because a small number of iterations in many practical situations suffices. You don't reach root 10 iterations in mathematics. Predictor-corrector algorithms. This is a visualization of IPMs. There's a very nice book by Steve Wright, written in 1997. Highly recommended if you want to start working on interior point methods. And we took algorithms from that book in order to sort of improve them in the approach itself. Interior point methods, they work just like other interior point methods, long step, short step, but instead of taking just one step per iteration, they take two steps. Why? They take two steps. Why? Because it's kind of known that in the long step methods, the so-called long-step methods, you take a long step in an interior point method, but that long step could basically take you way away from the central path. In which case, convergence, theoretically, becomes worse. Short-step methods keep you close to the central path, but they don't make fast progress. Iteration count is good in theory, but in practice, they don't work so well. Predictor-corrector sort of fixes. Predictor-corrector sort of fixes both issues. It first takes, in some sense, a long step that could take you out of the neighborhood that you really want to be at, takes you at this larger neighborhood, but makes a lot of progress, hopefully, towards your prosolution. And then you take a corrector step, which takes you back to the neighborhood where you'd really like to be. So that's the advantage of predictor corrector, by taking those two steps at the same time and they just change the parameter so-called send. They just change the parameter, so-called central parameter from one step to the next. By just changing that parameter, you get the advantages of both long step, the experimental, the empirical advantage, and the short step, the nice theoretical component. Okay? So that's predictable correctation. Two steps at every iteration, this means two linear systems also iterates. Again, it's kind of standard material, but happy to answer any questions. And what do you do? You repeat the end condition here is until the duality measure becomes less than recognition. So once I get epsilon close to having the primal and the dual to be equal, I'm done. I scop right. Can I ask a question then? Who picks epsilon? Is that part of the input? Part of the input. It's part of the input. 10 to the minus 10, that it's going to be logarithmic convergence on epsilon. It's going to be logarithmic convergence on x on one over x. So, this convergence, I like to think of it as people call it inner convergence, but it's essentially exponentially fast out to the extra that you choose. So, you want to make it really small, local over x derivation. Does it make sense? Yeah. So, there is another issue about this episode stack. The system three, number three here, is not going to be solved exactly, correct? Correct, so we'll get about that. So, the number of iterations we have. So, the number of iterations will vary depending on the colour and so this is. We'll get to that. That's going to be the issue, right? So, the standard analysis, so if you go back to the standard, they assume this is solved itself, in which case there are no solvers. But of course, it's not going to be solved itself. Well, the literature, there's a lot of literature on solving it. Correct, correct, correct. But so, again, we did our best to figure out what exactly happened in prior work, and I'm happy to talk if we are misrepresenting some of Talk if we are misrepresenting some of the priorities. The kind of results I'm going to show, I'm not sure they exist, exactly the way I'm going to show them. Okay. So I'm happy to discuss that. Especially for the subtle point structure that you showed earlier. There's a lot of work in that. Okay, so let me let me show you. I'm not complaining, I'm just sharing. No, no, no, I haven't shown the results yet. Yeah, exactly. So he's a major expert in the business. Absolutely. I'm looking forward to. Absolutely. I'm looking forward. That's why I chose to present this here. Because again, maybe I'm coming from a different community and I'm not presenting some of the sterile. Very interesting. Let me tell you. So again, the stop and criteria so far this is only for the interior point method, the stopping criteria. So so far I'm assuming an exact solution, exact solution. No tolerance, no error, nothing. Okay? So that that's what I'm assuming so far. I'm assuming an exact solution. I'm assuming an exact solution. And again, just we're not going to solve this exactly, of course. We're going to go to precondition conjugate gradient of whichever method we have analyzed, like four or five different methods. You don't want to use direct solvers because this is expensive. You don't take into account sparsity. Iterative solvers, you are going to get ill condition near the optimal solution. You are also not going to return an exact solution, which means that you are going to invalidate the status. You are going to invalidate the standard theoretical analysis, at least the one I am aware of, and you are also going to lose visibility if you do approximate choose. So you are not going to get at the end of the day A executability. You are going to get close, but not exactly. So this is the first result. And again, when we came up with this result like four years ago, I actually put that on a back burner. I could not find this exact result in the literature. So if it exists, please. Literature, so if it exists, please point me to that. There is always saying the following: you're not going to get delta y, you're going to get delta y tilde. Any delta y tilde that is approximate solution, an approximate solution. As long as those two structural conditions I'll discuss in a minute are satisfied, you get exactly the same convergence guarantee that interior point methods are predictable correctly on interior point methods. So, this is the convergence guarantee for This is the converse guarantee for outer iteration complexity of IPMs. Root n logarithmic on the initial gap, forget about this, log one of the exponents. So this is the convergence guarantee for the standard IPMs, all the self-redictocorrector IPMs. If your approximate solution at every step satisfies a forward and a backward error guarantee, delta y tilde minus the exact solution with respect to the energy norm is less than or equal to delta. And the residual is also less than or equal to delta. Residual is also less than or equal to delta. So forward error, backward error guarantee. Then you don't break the iteration complexity at all. What is delta? Delta is epsilon over root. Epsilon is the target. I could have made this a different tolerance if I wanted. Just for simplicity, I keep the same tolerance as the target for the integer point method. So as long as you get epsilon over root ten, which is quite small, you don't break the directional complexity of integer point methods. These are feasible integer point methods. These are feasible in the report. You have to start with the feasible instruction. The final solution will not be exactly feasible. It's going to be epsilon v. Ax star minus b, the two norm of that is going to be less than or equal to epsilon. We sort of follow a paper by Dan Spearman back from 2009 or 2010, which had similar guarantees for a short-step IPM. We did it for long-step and predictor corrector, and we also analyzed in physics. Picture corrector, and we also analyze in Facebook, but we are not going to get it out of it. Does that result make sense? I think there's a paper by Wright long ago showing that you use something like LU for the A for system, then you are bound to get accurate results, which is similar to what you're saying. And then there are. I think I've seen that paper. I don't think. That paper, I don't think then there are other results but for the structured problem. For the structure, I mean, without doing the sure complement, you're doing sure complement here. No, no, no, I'm doing anything. Right, but I'm doing anything as a matter of fact. I'm saying that to get this type of structure, you need to complement the original system. I wasn't able to find this. I think I've seen the papers by Right. I haven't seen other papers, to be honest. And I've seen a couple of other papers. But for example you can prove that U would be stable uh um in spite of the high conditioning of the coefficients because of the behavior of the S and the X vectors. Okay, let's talk about this offline as well. I want to understand what exactly this is just to fix some of those things. But in any case, when we prove that, I have to admit that, you know, I felt it was nice, it was kind of clean, at least for my taste, you know, approximate structural condition. These are sufficient. These are sufficient. I don't know if they are necessary. Okay, these are sufficient. I don't know if they are necessary. So maybe we can do it. But I tabled it, I put that on a back burner because I felt that similar results must have existed. Maybe it was already ultimately to find. What was a bit more interesting to me, to be honest, is whether we can actually be exact feasible. So is it possible to solve the system approximately and not lose this? Does that make sense? Does that make sense? So instead of having a x star minus b, ax star close to b, I want to have a x star still exactly what. So that was kind of my question back then. Can we assume that the final solution is now exactly feasible as opposed to approximately feasible? And let's see if we can get different conditions, if we can do something better. To think about this, at least in my mind, the major bottleneck was the following. If you look at interior points methods, If you look at interior points methods, delta x, the direction towards which x, the primal variable, is going to move, delta x has to be perpendicular to the count space. a times delta x has to be equal to zero, otherwise you're going to lose visibility, and also the proof becomes more complicated. Now, if you only do an approximate solver and you get delta y tilde, and then you do matve x, you get delta x tilde and delta x tilde, that goes. So the previous analysis cannot guarantee it feasibility. Analysis cannot guarantee feasibility because delta x tilde is going to not will not be perpendicular to the country. So, my question was: can we fix this? Can we add some sort of correction so that my approximate update, delta x tilde, is still going to be perpendicular to A, and then I'm going to maintain this update. Does that question make sense? Okay. So, the standard analysis, let me repeat it again for people who might not have seen bigger point methods so much. Have seen bigger point methods so much. A delta x has to be zero. That actually guarantees that whatever your step size is, a times x plus eta, let's say there's delta x here, if a delta x is zero, then ax is going to be good. You're done. This is the whole proof piece. And it's used in other places in the proof, but this is kind of the feasibility part. So I wanted the update to be also feasible, to be also perpendicular. The approximate update delta is still perpendicular to the common space space. And in order to achieve this, And in order to achieve this, we had to modify the normal equations a little bit. So we added this correction vector v. This is under your control. So you control V, but you want, of course, V to be computed efficiently. Delta Y tilde is the approximate solution. It's not solving A D square A transpose delta Y tilde equal to P, so it's not solving the exact system. It's solving this system plus something else. And there are many ways to formulate. Else. And there are many ways to formulate this something else. We chose this afternoon with reviewers. This is the constraint matrix. This is the diagonal matrix of the inverses of the slug variables, basically. So one way to present this. So this is basically a linear combination of the common space. But you get to choose V, the correction vector, and you get to solve for delta y tilde somehow so that this equation is satisfied, then you obtain for delta x tilde, and you need to do something ellipse for. You need to do something and liquid for delta x tilde then an extra product to compute there. That's beautiful. Then, if you follow this structure, a delta x tilde is equal to 0. So you can maintain feasibility. You choose the correction base. You come up with a solution delta y tilde. As long as this is satisfied, you are done in terms of feasibility. We were very happy when we discovered this, or at least I was happy. My students were tired. I was happy, my students were tired. But the good thing, but then we try to do due diligence to the extent possible. I think, Jassip, we talked to a couple of your students. I think we talked to Kimonas at this point, and I think he pointed us to this paper by Montaio and O'Neill, which I was not aware of. This was a technical report as far as I can tell. I don't think this got published. They had a very similar idea. They had this idea of the correction vector right there, so we were a lot less happy after this. Happy after this. But they did not have a construction of V that worked well in practice, and they did not have the structural result that we have in the next paper. But still, that paper contains that idea. Whether it usually practices or not remains to be seen, I guess. Here's the theoretical result. The inexact solution, delta, y, delta, you construct any way you want. It has to satisfy this equation here. Remember, this is the original normal equations. You add this term here. You add these terms here. V is under your control. As long as this equation is satisfied for the V that you control, for a delta y tilde that you return, and the norm of V is order epsilon, this is your target accuracy. Again, I could have different tolerances here, but just for simplicity, I'm going to look at the overall accuracy of the IPM. I'm sort of following theoretical computer science conventions here, so maybe they don't make much sense for this audience. But there's always a norm of these smaller than a constant times epsilon, and this is satisfied. Than a constant time sepsis, and this is satisfied, then you don't break the iteration complexity of predictocorrect fine solution. And you end up with an exactly feasible final solution. So your equation 9 looks like a constraint on the residual? Yes, correct. You can definitely think of this as a constraint. That depends on S. That depends on S. That depends on S, correct. Magnitude. I mean, S goes to 0. I mean, I mean, S goes to zero, right? So it's some entry. So this definitely depends on S, but again, the norm of V, part part of the reason we expressed it like that is to demonstrate that S is here, and the norm of V does not depend on S. That's totally under your control. I don't need the norm of S inverse V to be scored. I need the norm of V to be scored. That was the back and forth with the reviewers when we were trying to explain if that's why we put it like that. Oh no. But you raised a great point because we were trying to explain that we precisely because S could go to zero, some of the slight variables would go to zero, this could be very large, we didn't want this to be inside as well, we want this to be outside. But S could have large and small, right? S could have large and small, correct. So that's why both are problematic potential. Both are potentially problematic, and that's why. In a way, you could relax your constraint and say that only those V's that correspond to the V. All the those V's corresponding. Some of you have brought that up as well, if that's somebody. I said, you know, I'll do whatever you guys want, then we'll convert it. But you're right, yes. All these things, because S at optimality, S is going to have zero entries, right? Some of the entries will be zero, supremely, they're not going to be absolutely. Again, let me just repeat this: that what we are getting here is. That's what we are getting here is a condition. U control V, the normal V has to be small compared to the target accuracy, what you are trying to do there at the end of the day, at the end of the IPM solvers. And you have to satisfy this equality constraint with equal. This is sufficient. I don't think it's necessary. I'm pretty sure there could be other conditions that you could come up with. I don't break at all the convergence of the original idea. Of the original IPM. So this is still root n log 1 over epsilon output directions, exact feasibility at the end. These are under your control. Any computationally efficient construction you can come up with, visible. Okay, this is structural result. I'll show you a construction, but it's not probably the only construction, probably not even the best construction. It's just a construction. So how can we resolve it? Again, we wanted off-the-self stuff. Wanted off-the-self stuff, so we did precondition conjugate gradient. Why did we do PCG? We asked Hein, and he said, do PCG. So we analyzed a bunch of other methods as well, but precondition conjugate gradient, we could pull stuff out of the self, basically. So Heim had basically already a preconditioner. It's a randomized preconditioner. It basically constructs, that's where the randomized linear algebra comes in play. You have this sketching matrix, it's n by. Sketching matrix, it's n by w. We are sketching in this case the variable space. If you had a tall and thin matrix, you could sketch the constraint space. For Laurent plus noise, you could sketch both ways if you wanted, but fine, we don't create it there. You sketch your input. This is ADW, that's the sketch version, and you use that as a technology, basically. And this is not unlike what Heim and Sivan Polido had done in their blended paper back in 2009, 2003. And the rest of it is just standard PCG. And the rest of it is just standard PCG. You run PCG for T iterations, and you just pull the results of yourself for precondition computing. And we could do it with other methods. Would you please repeat what you used as a preconditioner? So the preconditioner is basically your makeup matrix. Remember, if you want to solve AB squared, A transpose. You look at AB, these diagonals, so what's chip? You sketch, you basically compute ADW, where W is a. Compute A B W, where W is a sketching matrix. Sketching matrix, that's not what we use, but think of it as a plus minus one. Random plus minus one. Okay, plus one or minus one with equal problems. We do not use L. We actually use what is called L2 subspace empending matrices because they have the best properties theoretically at least. They're count sketch matrices. You could have used random Gaussians, the fast Hadamard, the fast Skipper-Sai transform. A lot of things should work there. Actually, any such Gaussian matrix should work. Actually, any subconscious work, any other. Is this making sense? My question was actually the next step. So, once you get A, B, W, who is the S V D of these matrix? Right, and what do you use as a practitioner about the S V D? Basically, so I have the exact question. Sorry? It's very technical. No, no, no, it's not. So, no yeah, so think of it A d W, W transpose D A transpose, okay? Transpose VA transpose. I take the square root of that. That's what I use. So the square root of that matrix. So I compute the left singular vectors of this matrix, the singular values, I invert the singular values. That's what I use. But essentially the square root of this matrix times its transpose, that's what I basically use, with an inversion. Okay, okay. And I can prove for that that for tall and thin and short and fat matrices or exact Laura matrices, the condition number of the precondition matrix is constant. Matrix is constant. Is that most a constant? That works very, very well. Not for arbitrary matrices. They have to have more rows than columns, or more columns than rows, or they have to be exact law, considerably less than M. This is not so new. If you've seen randomized linear algebra results, again, this really goes back 15 years. I think this can be traced back with lemon. So, you know, this is kind of the standard structure. Let me show what happens when you use the correction vector and you use preconditioned conjugate gradients. In order to satisfy these conditions, you can use this preconditioned conjugate gradient solver for log n over epsilon iterations. n is the number of variables, epsilon is your target accuracy. I made the target accuracy the same as the target accuracy of the IPMs. Log n over epsilon iterations of the two conditional steps. Of the preconditional set functions. So that's it. And this is PCG, so matrix. The correction vector is ugly. This is my formula for the correction vector. It involves the diagonal matrix of the current iterate for the primal, diagonal matrix of the current iterate for the slug, and so on and so forth. But if you have computed the preconditioner, this is free, because all the matrices here have already been computed. This is just matrix. This is just mathematics. So, as long as you've done a preconditioned method and preconditioned complicated gradient in particular, computing the correction vector V is just a few mathematics. So, it comes for free because the intermediate quantities, in particular this quantity here, have already been computed for vector quantity. And what do you gain? If you solve this exactly, Remember, I focused on the setting where I have fewer constraints than variables, m less than n. So it takes m square n time to compute a b square a transpose. So you have some dependency on n, basically. You then need to solve the m by m system. So if you do exact methods, let's say order m cubed, this is where theoretical computer size would come in with order m to the omega, because you can do basically system solves in matrix multiplication time. Matrix multiplication time. I don't think that's particularly practical. It doesn't take advantage of the fact that your matrix could be either tall and thin, or in this case, short and fat. This is exactly what we know how to take advantage of when we do the randomized preconditioners. The end result is that per iteration, my running time is going to be something like m2. I'm not going to get away from m2. Again, from a theoretical computer science perspective, I could have m to the omega here, but then m to the omega here, but then my other term is not m square n, it depends only on input square. If it's dense, it's n times. There are some load factors I'm ignoring, there are lower order factors, log load factors is the third factor, but I'm hiding here. So what do you save at the end of the day? You save here an order M factor. That's it. And that's in some sense, with this kind of methods, the best you can hold for. So unless you have some radically new idea here, you're not going to get much better. You're not going to get much of it. Okay. May I ask another question? Absolutely. I was expecting that of course. So when you say each generational since you compute a constant number of. So this constant depends on how well the embedding has fixed embedding. So, this holds with high probability, so there are all these things that I'm hiding in log functions. So, if you want 10 to the minus 17 probability, you will have 7, something like log of 1 over 10 to the 1. So, because of 1 or 10. Oh, yeah, yeah. The constant, yeah. Yeah, it could be tens. So I would expect. I have a question. In other settings, when people caught preconditioners, it kind of develops from some understanding of an application. Correct. And we're doing this black box way. I wonder if other people are trying to do that. Yeah, so that's a great point. And certainly, there is quite a bit of work. I don't know if you're going to talk about any of that, where the conditioners now could be specific to the application domain. Not done anything like that. anything like that. The only constraint I have on my input is that the constraint matrix has to be either totally thin or sort of fat or low-rank plus unknowns and standard model machine learning. But if you talk, this would break down for square matrix. We'll run square metrics development. That would get a great result if we could do that this market. But better run the channel. Better than a chairman, right? His class is not hard to say. Concern, but if n or m are large, and you have to compute an SPP for it, that seems pretty large. And just to make it very clear, the only thing again we are saving, you could do this exactly in m cube plus m square n time. I'm just saving a factor of m right there. A lot of this effort is there to just save that m factor. That's it. So you are absolutely right. Now, if the input matrix is sparse, I could do professionals saving the ball. But if m is enormous, I still pay m cubed there. Enormous, I still pay M cubed. Again, if I put my theoretical computer science hat on top of me, I'm going to call this M to V omega and say that it's close to quadratic, but I don't think anybody in this room or online would be. Okay. Okay. The pair has run. So what I'm hoping to do, and I'm happy of course to have discussions about how to position this in terms of the literature in the field, what we were hoping to do was to have approximate solvers, target action. Solvers. Target accuracy would be the same as the IPMs, but again, we could modify the target accuracy of the approximate solvers to be different if needed with some more notation, basically. Our objective was to keep the iteration complexity of predictor-corrector methods to be the same, not change the iteration complexity of the standard predictor-corrector algorithm, and not lose feasibility out. So get exactly feasible solutions, same iteration complexity, get structural constraints and see what we can do. Constraints and see what we can do. We were able to demonstrate a specific construction for this correction vector, for this corrector vector that works. I don't think it's the only one, I think it's a construction. Random LA, randomized linear algebra came to construct the strong precondition, but you're absolutely right, it was totally black box in a black box manner, so we didn't look at specific properties of the integrated matrix beyond the fact that it has to be tall for in short, fat, exact Laurent. Going beyond has its own challenges. Going beyond has its own challenges. Actually, in the workshop in March, we were talking about some of these challenges. And another family of matrices for which we could do things would be matrices that come from graphs, solar placians. We also have faster solar solar placians, but that's a separate current form, it's a microfactor. And we were able again to clarify again, we just got rid of a factor of n, future work. Can we do it for Can we do it for infeasible predictor corrector IPMs? And I think the point I want to highlight here is that these are predictable corrector IPMs that start with an infeasible point. I want to tell you that this paper, this was only analyzed for the exact setting only in 2018, and this takes all the n iterations, not through 10. So infeasibility comes from the penalty. So if you start with an infeasible point, you have quadratically many directions. I think that's a very technical project. Our structural Are our structural conditions necessary? I think that's more important. Could we relax them? Is a lower precision solver sufficient? I'm not sure, but I think that our structural conditions could change from one iteration to the next. So it's not obvious to me why we have to have the same structural conditions at every iteration. So the result I showed, because of simplicity, and because that's the only thing we could do, has the same conditions at every iteration. Maybe everyone you can tolerate more. Maybe early on you can tolerate more nodes, or later on you need more arguments. I don't know. So it would be nice to understand what happens over time. Would we be able to get similar approaches for non-linear, for beyond linear, convex problems, but beyond linear, like semi-definite problems? Now those are solved using interior point methods that do potential reductions. It's a little bit different, but perhaps some of these ideas can be used there. Okay. I was when I started this, I was going to. When I started this, I was again hoping I would talk about a separate problem. The slides are actually up there if you want to take a look. This is about approximating again, theoretical computer science, approximating eigenvalues of a matrix in sublinear time without looking at the full matrix. And the nice thing, kind of the interesting thing here for me, was that I knew how to do this in sublinear time for symmetric positive definite matrices whose eigenvalues are, of course, all non-negative. But when we move to symmetric matrices, that became a very Sematic matrices, that became a very difficult problem. That's a long paper published in the ICAP this year, and this problem actually attracted quite a bit of attention this year. There were a couple of papers in the Scott this year as well about this particular problem. I'll just mention the title and the collaborators. This was done with February at Purdue and Cameron Muskov's group at the University of Massachusetts in Amchest. And if we have another problem session or something, I can also talk about other problems here as well. So let me go to the very end and put out the references. I didn't The references. I didn't talk about this. I told you you have this. And these are the papers I've been mostly discussing today. Happy to take questions. Thank you very much. Okay, good. Thanks, beautiful. Petros. So just a couple of things. I think that ill-conditioning is characterized as benign or something in that pre-proverb, right? Benign ill-conditioning. People right denying ill conditioning. Yeah, that's what I'm saying. But I'm kind of wondering whether maybe as a follow-up to what Valeria was asking, you could kind of incorporate some of it in the large 3x3 system. You could still even do PCs. You can just form short complements, and then maybe you have to justify less. Yeah, you have to justify less this business of inexactness. Of inexactness. So I don't know if that's better. So, actually, your paper was also motivational for us. There is still a line of work, especially in the computer science, that wants to solve this problem kind of exactly. This became kind of difficult at some point for us to get exact solutions. I don't think that starting with your original system, you can ever get exactly feasible solutions. Maybe there is something we overlook. Why? Why do you say that? Why it's not possible? Oh, I I okay, I should I it's not possible for me to quantify but I don't know how to do it. Maybe there is something that we overlook. I mean the structure problem instead of the reduced problem? No, no. So I think the issue is that unless you do some sort of correction to fix this lack of organization. I think Frenda was mentioning only the fact. Fend was mentioning only the fact that you could use the structure problem, the three by three. Yeah, the three by three block. Instead of reducing, you can do some reduction into the what is called recomplement. People usually now work directly on the three by three block. Yeah, it's easier. Yeah, I'll be talking for this. Okay. But I think the thing about But I think the thing about a bit better. I cannot. It's beyond a philosophical thing, it's just that you have a mechanism for inexactness within preconditioning that does not require that maybe compromise of justifying inexactness. Yeah, it's all within a preconditioning framework. And there might be an advantage there. It will still be somewhat useful there to be. Exactly. And maybe a relationship. And maybe a related question, but maybe you answered it. Yes, I need to do it. The AS inverse V is that absolutely critical as opposed to if you just saw for a reasonably small residual, you will not be able to get visibility there you say. You added AS inverse, so that's critical for again I'm only showing the necessary conditions. Only showing the necessary solution. Whether it's sufficient or not, I don't know. We tried hard to simplify that. We do not apply the specific problem. We only work on the normal equation, then the update. Yeah, so yeah, I don't know how to get around. Is there equivalent ways of formulating this? So I had a couple of equivalent ways. We converted on this for whatever is easy to use in particular for movements. Yeah, so certainly, I would love to see other references. I think that there is more. Other problems. I think that there is more work. Michael, thank you for the copyright question. Just a few comments. There have been comments about benign indeed, there was something like that. The fact that you have developed the steps, the final developed space alone. And in feasible methods, they are perfectly feasible. But what are the methods is they explore two kinds of places. And because of that, which only others do traditionally not relevant in the sense that it it doesn't hurt and I suppose also. And I suppose also the thing that you can allow yourself to use S inversely, which looks very dangerous, uh having said that some elements of S go to zero, other elements of S are One, generally speaking. Burning the zero is something horrifying and but it actually happened.