A pioneer in developing software for experimenting and exploring manifolds, three manifolds in particular. And he's kindly agreed to give us a more or less introductory talk to start the workshop on knock theory, computation, and experimentation. Thank you, man. And before you go, do you know where the button is that raises you can use just the button? You can use just the back one? Yeah. Okay, thank you. So, welcome everybody. If you don't know me, my name's Ben. My pronouns are he. If you don't know me, my name's Ben, my pronouns are ye, him, yes. I gave them a title when they said we need to produce a program. And the title is perhaps a little bit inaccurate. I think what I would really like to do is give you some lessons that I've learned in computation and experimentation. And some of these have been learned through dumb luck, some of them have been learned through bitter experience, some of them have been learned through wild optimism. Through wild optimism, but anyway, so maybe I'll just talk through. I also, the plan, oh, I was told not to bang the boards because I'll break the technology. Is this someone said there's things behind the recording? Yeah, yeah, yeah, yeah. Okay, we're all good. I'll just, okay, the organizers can tell me when I mess up the technology and I'll fix it. So, right, so the plan for this talk is very loose. So, I should add: if you have questions, please butt in. That if you have questions, please butt in. I don't mind being sidetracked. I always get sidetracked anyway. But yeah, so let me start off. So, this is lessons in experimentation and computation. The framework for at least some of what I will be talking about is a recent project in knock tabulation. And the reason I mentioned this in particular is because with knot tabulation, Is because with knot tabulation, there's two things that you're doing. One is you're working with enormous amounts of data. So, up to 19 crossings, you have some 350 million knots. Up to 20 crossings, you have some 2.2 billion knots. And so, A, you're working with large amounts of data, and B, when you're working with 2.0, you just can't babysit 2.2 billion calculations. 2 billion calculations. So you need computations that run well enough all of the time, or if they don't run well enough all of the time, they run well enough most of the time, they bail gracefully when they don't, in a way that lets you collect everything and feed it into the next algorithm, then the next algorithm, and then the next algorithm. So you can't be dealing with every single case on an ad hoc basis. That's my sorry for my voice, by the way. I lost my voice two weeks ago. I mean, I think ever since the pandemic, something I've a voice can be. Ever since the pandemic, so my voice comes back a little bit slower than it used to. So, actually, I'm going to put these numbers up. So, the knot tabulation, so up to 19 crossings. These are only looking at prime knots. So, the numbers we're looking at here are 352,152,252, and sort of more recently, up to 20 crossings. And this is sort of And this is sort of somehow joint, but deliberately not joint, with more than fistlet. The numbers are 2,199,471,680. So these are the number of distinct prime knots that you have up to 20 crossings. When I say joint, but not joints, what happened is Say joint, but not joint. What happened is we were both working on this project with independent software, independent algorithms, and then when we both had tables and we both had numbers that matched, which was the first thing that you would really like. And history somehow is a little bit patchy in this respect. But so once you get the same numbers, you then actually want to match up the tables. And so we honest to goodness had the two tables lined up and found connections between every pair of crossings, every pair of knots and birds. Every pair of crossings, every pair of knots in both tables. So they are not just the same number of knots, but the same knots. Pardon me. By the way, if you're interested in hyperbolic knots, I should mention that all but 1315, all but 1,315 are hyperbolic. There's 15 torus knots and the other 1,300 satellites. So when people say the hyperbolic cases are the cases that really Say the hyperbolic cases are the cases that really matter. This is what you're saying. Right, so this was a project that, of course, involved a ton of different algorithms, a ton of different branches of mathematics, combinatorics, graph theory, parametrized complexity, hyperbolic geometry, computational algebra, normal surface theory, etc., etc., etc. And so it is kind of a nice one of the motivations for this project for me was to try and convince myself that computation. Convince myself that computational knot theory and computational low-dimensional oncology had actually reached the point where a project like this was feasible. Where you could do this and get all the answers, not just most of the answers. So that was a nice validation of the field in some sense. So that's the framework for at least some of what I'm talking about. Can I ask you a question? Yes, please. So you said that you and Wolwen put your tables side by side, your two billion knots, and you check that they were all the same. And you check that they were all the same knots. Yes. You did that by hand. No, so I mean, basically, there's a piece of code which is, it's a combination of, like, for the hyperbolic knots, you've got tricks like canonical triangulations that you verify that they're the same. For satellite knots, it was actually finding a sequence of Reidemeister moves that turned one into another. But there was really, like, for every knot in this table, there was a path to a A path to a nod in that table, and these paths had been computed automatically, but there was code to be computed. Does that answer the question? Totally. Okay, yes. The satellite knots are the problem, right? For hyperbolic knots, you've got all these nice tools, and chronicle triangulations in particular, a very powerful one. For satellite knots, like, there are so many different triangulations of the same satellite knots that are hard to. Of the same satellite, not so hard to connect to each other. And pulling apart the structure of the satellites is sometimes easier and sometimes harder. And there's a lot of bookkeeping, which is sometimes difficult to implement. And so there was just sort of, when I say it was one piece of software, it was like you'd run code that was increasingly aggressive and would reduce the number of outstanding cases down, down, down, down, down, and you would increase the sort of the level of aggressiveness, which made it slower, but there were fewer cases that you were dealing with and so somehow the balance worked. Yeah. The balance worked. Yeah, anyway, that was maybe the long answer to a simple question. There are more questions before I launch in. Okay, so lesson number one. Lesson number one. Don't be afraid to implement difficult algorithms. Don't be afraid to implement difficult algorithms. Algorithms. So, when I say difficult, there are two things that I can mean by difficult. One is that the algorithm is in theory extremely slow. So, not theory, three-manifold topology are full of algorithms which are exponential time, doubly exponential time, more broadly, superexponential time, factorial time, tower of exponential time. There are some problems where I think we still don't have good boundaries. Where I think we still don't have good bounce on the running time. And so, what I really want to say here is that should not stop you from actually just shutting your eyes and trying it. And there are caveats, but I'll come back to that. Four-manifold topology, of course, things are even worse somehow. Four-manifold topology is full of undecidable problems, where undecidable is like a halting problem, like Girdles and Linux theorem. So, theorem, there is no algorithm that can tell whether two triangulated four-manifolds are homogeneous. The two triangulated four manifolds are having different. So, having said all of that, you still shouldn't be afraid to implement this, right? So, I blame Ryan for this. Where's Ryan? So, when Ryan, and bless Ryan for dragging me out of industry and back into academia, and the way he did this sort of 15 years ago was by saying, oh, yeah, let's work on computational four manifolds. And it sounded ridiculous, and it actually works, right? You can actually get stuff done despite the theoretical limitations. Nowadays, Rodie Burke, who's finishing his Is Rodie Burke, who's finishing his PhD out in Queensland, is sort of taking the stuff that Ryan and I did years ago and building on it and actually getting concrete examples of exotic pairs, looking at the structure of triangulations, being able to simplify triangulations sufficiently well that you can just pull apart the manifolds and see the pieces in a way that you can't really do in three-dimensional. So it's quite nice what he's doing in this setting where everything is undecidable. So, well, where enough things are undecidable. So don't be afraid of that. So, don't be afraid of that. So, there's a second aspect of difficulty, which is that some algorithms, like the homeomorphism algorithm, the homeomorphism algorithm, I give you two triangulated three manifolds. I ask you, are they homeomorphic? So what I like to tell people is Matlayev has a textbook, which is 600 pages long, which describes some of the algorithms, right? And I'm exaggerating a little bit, but I'm not really exaggerating that much. There's a lot of moving parts in that. It's there's a lot of moving parts in that algorithm. It's never been implemented. I think it is implementable with the right ideas and the right simplifications. And I sort of, one of my dreams is that before I retire, someone will have done this. So I think it's within reach. Not this year, but maybe within our professional lifetimes. I don't claim to know how old everyone in this room is. When I say with the right tools, so for instance, you don't just sit down and You don't just sit down and implement an algorithm which is going to require a million lines of code, right? And expect you to do this in six months' time. So you do work to improve things. One very good example of that, it's a very old example, but it made an enormous change, was in three-sphere recognition. So I give you a triangulated three-manifold. I ask you, is it S3? And so Ribbenstein produced an algorithm based on normal surface theory where you would basically be looking for a Where you would basically be looking for embedded surfaces inside your triangulation that sort of were simple, except for in some bits and places where you could have little octagonal pieces or little tubes. And octagonal pieces are reasonably easy to deal with computationally. Tubes are horrible. Tubes are horrible. And so Abby Thompson produced an algorithm that didn't need tubes. So it also recognizes S3, but in an algorithmically much simpler way. Much simpler way. And being able to eliminate tubes and just focus on octagons, which somehow had a much simpler implementation. So Abby's work is somehow essential for this being implementable. So that's what I mean by don't be afraid, but there's caveats. You want to make your life easier when you can. What I should have done is switched off the screensaver. Save here. So another, I'm talking a lot, but I'm not writing. Okay, never mind. Another example of this, and this is somehow where I first learned this lesson. I apologize for stepping into three manifolds and out of knots from time to time, but this was an old problem from the 1980s. So first it asked, is the Vegas cipher-type space Harkin? Guess what, Bob? If you don't know what the Vegas cipher teth the hedge space is, it doesn't matter. If you don't know what Harken is, Spaces, it doesn't matter. If you don't know what Haarken is as a property, it doesn't matter. The point is that you had this question floating around from the 1980s, which had an algorithm that could solve it. There was an algorithm known, and all you had to do was code it up and run it. And somehow it took another 30 years before that was possible. And part of it was simplifying the actual algorithm. So you reduced from what algebraically were Hilbert bases, which is what Harkam was working with. Harkim is working with to what are essentially vertices of polyhedra, which was what Jayco and Otel simplified it to. There's lots of pieces in simplifying the algorithm. But also, and I want to emphasize this, there is software engineering there. So the reason, part of the reason I actually sat down, this was joint work by the way, with my room assignment, Jeff Arm Tillman, but part of what motivated me to actually sit down and start trying to make Actually, sit down and start trying to make this feasible on software was Nathan. And I blame Nathan for this because at some point in time, Nathan, yeah, sorry, so you sent me an email that basically said your normal surface code is crap. In nicer words than this, but basically he sent me an email that said, here, I'm running this code from Regina. If you don't know Regina, it's a piece of software that can do three manifolds and knots and some things with four manifolds. And he said, look, it's like a thousand times slower. And he said, look, it's like a thousand times slower than this simple C implementation that we've got. And so I thought, well, this is ridiculous. And so I sat down and actually sort of engineered the code, right? When I say engineered the code, I mean choosing the right data structures, choosing the right tricks, choosing the right heuristics, experimenting with the right heuristics, talking to people who knew things about operations research, talking to people who knew things about polytope computations, talking to people who knew things about doing Talking to people who knew things about doing fast bitwise operations, talking to people who knew about cache misses in chips, right? All of these things to kind of engineer the guts out of this code to the point where it became so fast that nowadays you can prove that the vague decipher directly is, God, which way is it, non-Harkin, in five minutes on a laptop. You just run the code and it takes five minutes and you no longer need this 20-page paper. But at the beginning, the algorithm, I should write something down. The algorithm that's under the hood here is e to the O of n squared time, where n is the number of tetrahedra. And by the way, the number of tetrahedra in this triangulation is 23. So e to the power of 23 squared is sufficiently large that if you looked at this boundary, you would say, no, I'm not trying. So my point is, you should still try. But try and engineer your code very carefully. To engineer your code very carefully. Another example, and I want to come back to the not tabulation, and this is snapping. So with the not tabulation, bearing in mind that almost all of the knots you get are hyperbolic, one of the very useful tools is to take a candidate knot diagram. Snappy is software by Mark McCullough, Nathan Dunfield, but Dunfield, Matthias Gurner, and my left name's off here. Jeff Wicks. Jeff Winkles. Of course, Jeff Winks. Originally based on Snappy with the same sound but a different spelling, but Jeff cooked up in the 80s and 90s. And what it does is, at its core, you give it a three-manifold triangulation and it finds a hyperbolic geometry sitting on that triangulation, which then lets you use all the tools that you've got from hyperbolic geometry. Got from hyperbolic geometry. So, what do you do with the candidate nut diagram? You feed it to Snappy, say, Can you find me a hyperbolic structure on this triangulation? In particular, for the tabulation, if you know these things, I was asked by Snappy to look for a hyperbolic structure where every tetrahedron had a positive volume. So these are called geometric solutions in the Snappy language. And then, if it found one of these things, you asked it for what it thought was a canonical triangulation. What it thought was a canonical triangulation. It might be wrong, but if it's right, it'll give you a triangulation of a correct manifold, but it's not guaranteed to be canonical. But a lot of the time it is canonical. Sometimes it won't give you a triangulation at all. Sometimes it won't even find that geometric structure that it's wanted for. But when it does, you can get this canonical triangulation, which might not be canonical, but if you've got lots of candidate knots and enough of the canonical triangulations are canonical, then you just see that they're the same triangulation. Then you just see that they're the same triangulation, you know that they're the same knots, and you've stripped away a ton of treatments. And this is fabulously effective in practice. The reason I'm saying this is because, and I believe this is still a correct statement, there is no known algorithm that, given a three-manifold triangulation, will produce a geometric structure where every tetrahedron has a positive form. This is, and I'm looking for Nathan to correct me, but I believe there's still no non-hour that's guaranteed to do this. And so, nevertheless, standards. And so, nevertheless, Snappy does it, right? In practice, most of the time. So, this is another example of not being afraid to actually code the thing up, despite the fact that there's no guarantee that it should even work at all. And by most of the time, what I mean is, of these 2.2 billion lots, on the first pass, Snappy found a geometric structure. It didn't necessarily get the canonical triangulation light, but it found a geometric structure on all but 400. On all but 418 million. So pass one, all but 418, sorry, 418,276. This is a large number of leftovers, but in the scale of 2.2 billion triangulations, that's not. So this is a measure of just how fabulously effective this code is, despite the fact that it shouldn't work at all. If you're interested, Snappy has. If you're interested, Snappy has this wonderful randomization button which just kicks the triangulation into a different triangulation of the manifold and tries again. I think Snappy used to have a hard-coded number of re-triangulations that it would try automatically before it gave up. Is that still a hard-coded constant? A randomized. Oh, okay. Okay. In which case, I'll take that back. So, certainly, what I did here was I hardly created my own consonant and randomized it. After pass 40, you will 40, you were left with. Oh! No, I lie. After 40 attempts, you were left with n minus 418 million. Right. That was what you got after randomizing 14 times. After randomizing 100 times, no, I lie, I lie. No, I lie, I lie. After pass one, you were left with this. After pass 40, after past 40, you were left with one. Out of 2.2 billion, you were left with one. And then after 100 passes, you were left with zero. What was the one? What was the one? That's an excellent question. It's buried on a tape drive somewhere in the high-performance computing facility, so I can get it to you by tomorrow, but I can't get it to you by this morning because the machine has to fetch a tape. Has to fetch a train. But it was a 19-crossing, not, I don't know. I've been asked this before, and every time I get stopped by the fact that it's buried in the archives of a basement of a building. Yes. What was your process? You didn't find hyperbolic structures, then did you ask Regina to find a compensible tori? Yeah, so to prove it's a satellite, you asked Regina to find a compensable tori. So then you've certified that they're satellites, so you know that they're not the same as any of the hyperbolic muscles. You know that they're not the same as any of the hyperbolic must still be to strip out duplicates. And the problem here was there were, oh, I've got a number somewhere here. Maybe I didn't write the number down. I think there were about something like 500 million. I think there were something like 500 million knots that you could prove were satellites. And by the way, you're aiming, at the end of the day, there's only 1300 distinct satellites. So most of the hard work there was in actually connecting up the duplicates. Connected up to jukeboxes. And with the satellites, what was sort of the worst thing that Snappy's behavior was it fairly uniform on the elder satellite? Oh, Snappy's behavior on the satellite was just I couldn't find a... Okay, so what I did to Snappy, Snappy, I understand behaviors better if you run it inside Sage. I didn't run it inside Sage because with these kinds of numbers, you can't afford the overhead of starting Sage each time. And if you don't start Sage each time, then Sage has a memory leak because of the way that Sage is glued to Snappy. And I think this is an error of 24. Sage is glued to Snappy, and I think this is an irretrievable feature of Cython. I think we all try to work this out at some point, which means that you can't run a single instance of Snappy without exhausting memory in Sage. So the answer was to toss Sage. So if you toss Sage, then very occasionally Snappy will crash due to there will be some numerical instability. And what you do is put something in your code that detects the crash. And if so, it just outputs it in a list of things that Snappy couldn't find a geometric structure for. So sometimes it aborted by not finding the structure. Occasionally it aborted by crashing. Occasionally, it aborted by crashing. Either way, you just put it into the other list of potential satellites. What was the nature of the crash? I don't know. I'm sorry, I don't know the nature of the crash. I don't know where in Snappy it came from. All I know is that you ran Snappy and then at the end, it threw an exception or it had a SIG fault on the other. And so I either caught the exception or trapped the SIG fault, whichever it was. After reading this, there's only order. There's only on the order of 1300 satellites. 1300 satellites. And 15 chores not so much. Well, no, chores knots. There's 15 tourist knots. Oh, yeah. That ratio approaches 1 as n goes to infinity. Sorry? The proportion of satellite knots will approach 1 as n goes to infinity. Okay. Okay. So that's um so lesson 4, by the way, is don't be fooled by the tyranny of small numbers. But that's um okay. That that one's coming if we ever get to the end of this talk. Coming if I ever get to the end of this talk. But yeah, that's a nice point. But actually, there is a nice point to make now, is that the behavior of small numbers is not necessarily any indication of what you get when n gets large. And that's a thing worth remembering. Look, I was going to talk about something completely different, but maybe, maybe I'll move on to lesson two. No, I will say one more thing. With the satellites, I do have the number, it was 500 million. Number it was 500 million with the satellites, right? The way to connect up the satellites, 300, around 500 million things for which Snappy could not find a geometric structure. Some of them were these 418,000 leftover hyperbolic knots, right? So you do sort of try Snappy with more and more iterations, but that's a small fraction. For moderations, but that's a small fraction. Most of these were actually genuinely satellites. You could use normal surface theory with Regina to prove that they were satellites, and then the task is actually connecting them up. And the bulk of the work there was using rider-weisted moves. So what do I mean here? I mean you have a pile of knots and you are basically building an enormous graph where each vertex in the graph is a knot. In the graph is a knot. And then you start doing Reidomeister moves. Sorry? Verte is a not diagram? Not diagrams, thank you. Sorry. Vertices are not diagrams. And then you start doing Reidemeister moves out from these not diagrams, and you start connecting up the components. Did you check for randomized move duplication? Did you check for duplicates by radarizing moves first, or did you prove they were satellites first? Um I believe I proved they were satellites first. I proved they were satellites first because the Reidemeister move code was expensive and it was expensive in theory and in practice. But in theory it was fabulously expensive and in practice it was just expensive. Proving that knots were satellites in theory was expensive and in practice it was very fast because that was using normal surface theory which the normal surface theory code inside of Gina is sort of being engineered to a sweet buggery, right? It's like it's very, very streamlined. Very streamlined. Thank you. Yep. So, yeah, so you're basically exploring out with Reidemeister moves until you've connected up as many components as you can, and each component in this graph then becomes a single model. Now, the problem with this is that the bound on the number of rhythmeister moves you could need. I am not aware of a general bound, and I believe there is one, and I was trying to find it this morning, and I couldn't find it. If your knot is the unknot. If your knot is the unknot, if your knot is the unknot, then Lac and B says there are n to the 11, at worst, Reid-A-Meister moves needed to connect up two diagrams of the M-par, where n is the number of cosmetics. This is polynomial, which was a fabulous result. But by the way, given that this is a polynomial, to actually find this sequence of n to the 11 Reimmeister moves takes time n to the power of. Time n to the power of n to the 11. Because you don't know which n to the 11 moves they are. So you try the first moves, then you try the second moves, and so on. And you get this tree with n to the 11 branches, where each branch has degree n. So you can't expect this to work. For general knots, the corresponding bounds I've seen for triangulations, Miadovich has a bunch of results on different kinds of three-manifolds, where instead of Of three manifolds, where instead of Reidemison moves, you use bicellar flips, and those bounds tend to look like exponentials, doubly exponentials, towers of exponentials, depending on what kinds of manifolds you're looking at. So you can't hope for this to work. Despite this, you shut your eyes, code the thing up, engineer it very carefully. Those 500 million cases, just with Reid-Meister move 3, which does not change the number of crossings, it becomes 1.3 million. It becomes 1.3 million, right? Which is like a bit over 1% of the cases. Once you start increasing the number of crossings, if you allow at most one extra crossing, which is very little, you get down to 300,000. If you allow two extra crossings, then you get down to 1,700. And if you allow three extra crossings and start dragging in things like coloured Jones polynomials, then you do it less. Than you do with less. But the point is that this was actually feasible. It was very expensive, it had to be parallelized, it had to be very heavily engineered. This data structure is enormous. We have a one terabyte RAM machine on campus. Only one, I think, or maybe two. I smiled sweetly at the HPC people, and they said, oh, I hear the biologist algorithms are rubbish. Please use our machine. So I hogged this one terabyte RAM machine because this all has to sit in memory. This whole data structure has to sit in memory. This whole data structure has to sit in memory. If you've ever worked with isomorphism signatures or not signatures in Regina, these are basically a way of taking a triangulation or a NOT diagram, encoding it as a piece of text. It's basically a base64 encoding. So it's using human-readable text, but not pronounceable, but readable, and typable is the point. And it has the feature that two triangulations are isomorphic combinatorially, if and only if the signature. Combinatorially, if and only if the signatures are the same. Two knots, the diagrams are isomorphic combinatorially on the two-sphere if and only if the signatures are the same. So they're a useful way to decant topological objects into a text file, but b they're a useful way to strip out combinatorials. But if you ever use that code, you'll see that Regina has an option for using a different encoding. So instead of human-readable, human-typable, base64, it lets you supply your own class which kind of does some other kind of Class which kind of does some other kind of encoding. And the reason for this was because to make this data structure fit in a terabyte of RAM, you had to make the signatures smaller. And by the way, basics before encoding only uses six of the eight bits in each byte. So in theory, by packing things more tightly together, you can compress the size of your signatures by a factor of three quarters, or a factor of one quarter. You can reduce it to three quarters of the size. And so this separate encoding did that to make it fit in the Monterabike RAM machine to make the cell with the mark. So this is what To make this help with the market. So, this is what I mean by software engineering. Data structures matter, implementation choices matter. Wait, did I just hear you say that you had some way of taking a general four-value length linear graph on a sphere and associating that if two graphs are isomorphic but in a non-obvious way, you get exactly the same string? Yes, but you're not looking at isomorphism of the graphs, you're looking at isomorphism. Looking at isomorphism of the graphs, we're looking at isomorphism of the planar embeddings. So, in particular, the key thing, because I think where you're going is surely this is factorial time. And the answer is no, because once you've worked out the image of the first vertex, everything else is determined. Because then you just follow the embedding around the sphere. Because you care about the actual embedding. Oh, but even if you store the places and stuff, there are lots of ways to write down. Right, right, right. Right, alright. So here's okay, like so I give you two diagrams of a knot. Let's say, okay, I'm going to. I'm trying to do the other trefoil. How does the other trefoil work? You do this, then you do this, then you do this. Right. So So, suppose I've decided that this vertex is going to map to this vertex, and I know that this outgoing edge is going to map to this outgoing edge. Then, I know that this vertex, if I follow that along, this one must match to this one. And this outgoing edge where I turn right must match to this outgoing edge where I turn right, and so on and so on. So, once you've worked out the image of the first vertex, everything else should be determined just by following things and remembering the left and right ordering and outgoing. The left and right ordering and outgoing arcs. How are we calling the orientation? Well, even without orientation, that's an extra factor of two. It's a single choice which is a factor of two. So then you have to check all possible mappings in one crossing? Well, the first crossing, you have to check all possible mappings. Fair. But after that, everything just falls out. So that's how it becomes polynomial time. And three-manifold triangulations, it's a similar picture. It becomes polynomial time because once you've got the units of the first set of. Polynomial time because once you've got the image of the first tetrahedron with the ordering of its vertices, then you can just follow along through. Yeah, yeah, so it's sort of, it surprised me at first when I realised that this was actually polynomial, not factorial, but it is because the embedding matters, not just the graph itself. Yeah, yeah, yeah. Cool. Okay, I have five lessons, and so far it's 9:52, and I've given them. And so far it's 9.52 and I've given you one of them. So I might, let me move on. Let me move on. Let me move on. Lesson number two. Okay, lesson number two. No one piece of software. Is best. And by which I mean is best at everything. So what I'm going through here, like for me, for me, I spend a ton of time working with Regina because that's the software that I'm sort of largely involved with. And so it is tempting, sort of, the first thing to do is to use Regina's implementation of whatever it is I'm trying to do. Regina's implementation of whatever it is I'm trying to do, and if it doesn't have one, to code one up if I need it, and so on and so on. The point here is that different pieces of software are better at different things. And there are different aspects. So one example is simplifying groups. So this could be the fundamental group of a knot, oh, sorry, the group of a knot or a link. It could be the fundamental group of a three-manifold or a four-manifold. And so when I say simplifying groups, I mean simplifying group presentations, generators and relations. Generators and relations. So, this is something that we've been working with for a long time in different aspects. Regina tries to be flexible about these things. It gives you a bridge into using Snappy, which has some very nice code for group simplification. It gives you a bridge for gluing into Gap. It gives you a bridge for gluing into Magma. Ryan Budney has some very effective simplification code which is buried inside Regina itself. And the point is that. And the point is that there is no one solution which is better than all the others, which is why Regina gives you access to all of these things. I think, that was Saul talking before, correct? Yeah. Through the ether? Okay, so Saul, correct me if I'm wrong. I think, if I remember correctly, maybe 10 years ago at ISEM now, Saul sat down in the kitchen of the apartment we were all staying in and decided to check whether Snappy's group simplification code or Regina's, which is Orion's work. Regina's, which is Ryan's work, which of these was better? And I think after doing a pile of code and running it through a pile of examples, what it came up with is: well, Snappy is better some of the time, and Regina is better some of the time. And Saul, correct me if I'm wrong, but. And Heguard is also better some of the time. Ah, Hega, of course. So Hegard is also better some of the time. So the point being that if you have a stubborn group presentation, you don't feed it through Regina and give up. You don't feed it through Snappy and give up. You try all the things. Snappy and give up. You try all the things. You try all the things. Because they are not all best at all of the cases. Not tabulation is a similar picture. Not tabulation, I was working with Regina, I was working with Snappy, I was working with Plantry, which is, that was one of the very first stages, which is a piece of combinatorial software by Brendan Mackay, which is generating triangulations of planar triangulations of the sphere and planar quadrangulations. Of a sphere and planar quadrangulations of a sphere. And by the way, a planar quadrangulation of a sphere is dual to a knot diagram if you forget whether the crossings are over or under. So that was a very useful first pass because he has engineered the sweet buggery out of that code. So you use what's out there and be open to trying different things. A good example of this for me was working with group invariants. So So the extra caveat I want to say here is: and don't be afraid to implement your own. So with the hyperbolic knots in particular, with the hyperbolic knots, so because the canonical triangulations are not necessarily canonical, Are not necessarily canonical. If you get two supposedly canonical triangulations of two knots, which are combinatorially the same, then you know the knots are the same. But if the supposedly canonical triangulations are different, it is not a certification that the knots are different. You need invariants that will distinguish your knots. You can work with things like Jones polynomials and Hoffey polynomials. These are fast enough, but they're not powerful enough. The Hoffe polynomial... Polynomial pulled out about half of the two billion hyperbolic knots as unique Homphyl polynomials, which meant you were left with a billion knots that the Honfling polynomial couldn't distinguish from other things, which is still a lot. By far the best tool that I found for this was algebraic invariant. So you compute the knot group and then you try and find invariants of the groups. And in particular, a useful invariant there was where your input here is a group presentation. Group presentation, and what you're looking for is a list of conjugacy classes of index k subgroups. And the number of conjugacy classes of index k subgroups for some fixed k is an invariant. And an even better invariant is to actually take those subgroups and compute the abelianizations of them. Because for abelian groups, you can actually test whether. Groups, you can actually test whether the things are the same. You do a Smith normal form, basically, right? And you can tell whether the abelian groups are the same. So, the hard part of this process is actually finding the index k subgroups. For the 19 crossing tabulation, I used Gap and Magma for this, because here were these big computational algebra packages which could surely do everything I wanted to do. For the 19 crossing tabulation, the index K went up to 7. Up to seven. I needed to go up to seven before I could successfully distinguish them from the other. Gaps and magma do. How did they do this? Sorry? How did they do this? How do they do this? I haven't got a clue. Possibly Nathan has more insight into what Gap and Magma are doing. I don't know what other things they think is. It's more efficient than that, but I mean, order of magnitude is there something else. Anyway, we come back to this. So. So, the first thing I'll mention is for the 19 crossing tabulation, I worked with both Gap and Magma. Because Magma, for the bad cases, Magma was faster. By the way, Magma I couldn't parallelize because Magma required a license, which meant I could only install it on one machine because we only had one license in the department, so I couldn't parallelize. Gap was open source, so I could parallelize. That was an important factor of why I used Gap most of the time. Fact of why I used GAP most of the time. But also, for the bad cases, GAP was faster. And what I really mean here is that GAP in general was more consistent. So the run-up time for a given index, GAP was sort of roughly taking the same amount of time for all of the cases. Sort of, sort of, kind of. Magma had a much higher variance, which meant that some of the cases it did really quickly, some of the cases it did very slowly. And if you're working with a billion knots, you can't afford very You can't afford very slowly. So I used gap to get rid of most of the cases and then fed the ones that were left into magma. And the very last pair for the 19 crossing tabulation, I think it took Magma about a day to completely build the variants that were distinguished for a single case. But for the 20 crossing tabulation, I figured this one case that took a day on magma, surely with 20 crossings, there were going to be significantly more than one that were in the same boat, so I couldn't have. That were in the same boat, so I couldn't afford to use magma. And at some point, I thought, screw it, I'll just try coding this thing. Now, I know nothing about computational algebra. Again, I know a little bit, but I don't know nearly enough. I looked at a naive algorithm, and a naive algorithm is essentially to find representations from your group into the symmetric group of order k. What this essentially requires. What this essentially requires you to do, and you want these representations to be transitive, and you want them to be representations, so they have to sort of faithfully maintain the relations, the good relations. And so what you're really looking for is that each generator has k factorial possible images in SK, which means the running time is the number of, maybe I've got this the wrong way around, the running time is k factorial to the power. k factorial to the power of the number of generators. So this is not a small number, it's not a small number, and as k gets larger, you can see that this gets much slower, which is why index 7 took a day. Nevertheless, you sit down, you code the thing up, and then you engineer the hell out of it. So by engineer the hell out of it, I mean, for example, making as much use of consciousness in classes as you can. So instead of mapping it into SK, where you can, It into SK, where you can map into representatives of conjugacy classes where you're allowed to do that without loss of generality. You exploit short relations. So you use heuristics to essentially try and organize the relations so that short relations are managed first, relations with fewer generators are managed first, but then you choose generators in an order in which you can cache as many of the computations as possible. Of the computations as possible to reduce the amount of permutation algebra you're doing. Hopefully, you can knock off some of the relations before you've mapped all the generators. So, all of these are just their algorithm engineering tricks, right? Permutation arithmetic. This is all, when you're trying to match up the relations, you're doing arithmetic with permutations in Sk. And to multiply two permutations in Sk is ordered K time. You've got to find the image of each element, right? So, because K was at most seven. So, because k was at most 7, and if you ever looked at Regina's code for solving this problem, you'll see that it has a limit on k. Exactly for this reason, the code was engineered to use lookup tables. So multiplying permutations became a constant time operation because you didn't store a permutation as a list of images, you stored it. This is the 31st permutation there. And you have a lookup table that says the 31st permutation times 57 permutation. First permutation times the 57th permutation is the 109th permutation. So all your permutation arithmetic loses a factor of k. And if you can divide your running time by k, that's a win, right? Linear algebra. So your group has to satisfy, your representation has to satisfy all of these relations, which means in particular, if you take a relation and then just look at the signs of the permutations, this becomes a This becomes a linear equation in Z2. Because if a certain collection of permutations multiply to the identity, then the signs of the permutations have to add to 0 or 2. That's a linear equation in Z2, which means you can write down all the relations, solve these equations in Z2, and then immediately you have restrictions on the sides of all your images, which, depending on how tight you're Which, depending on how tight your solution is, that strips out several more factors of truth for the running time. All of these improvements matter. So, all of this is basically someone with a background in computer science, but not computational algebra, trying to code up as good a possible implementation of this algorithm for this specific setting, which was finite index sub loops, coming from NOTS. And long story short, the running time of this code was so much faster than GA. So much faster than GAP or Lagma for these problems that were coming out of these knots that I, for the 20-crossing tabulation, I just used the home world implementation. And it was somehow, everything was just much better. And not only was it faster, you didn't have to deal with the overhead of calling out Madrid Gap. So don't be afraid to implement your own. The writer on this, by the way, is that if you look at the Snappy website, I think it's buried in Snappy, I'm not sure. So Nathan and Mark Culler. So, Nathan and Mark Culler and Matthews Goerner have an implementation of finite element subgroups. It uses a very different kind of algorithm. So, it's not finding representations of the SK. It's instead using Sim's algorithm, which is doing some clever tricks with matrices, and they have a nice paper that explains the algorithm. And someday I hope to read it and understand it, but I haven't done so yet. But anyway, so the point being that if you ever need to solve this problem, another place to look is like the low-index packaging scale. And the low-index packages happening. And again, at some point, they were doing benchmarking of this algorithm against the implementation in Regina. And sometimes Regina was orders of magnitude faster. Sometimes the Snappy implementation was orders of magnitude faster. It depended on the structure of the relations, the number of generators, the length of the relations, things like this. So, again, no one piece of software is best, right? So, anyway, so that's love. Oh, good lord, we've got 10 minutes, and I've got three more lessons. More lessons. I'm going to scoop through the others. Are there any questions, by the way? I'm sort of talking fast, but you're welcome to buy in. Okay, let me keep going. Lesson number three. Lesson number three. Exploit Exploit parallelism. Parallelism where you can. What do I mean here? Chips are getting faster. Of course, the laptop I have is faster than the laptop I had three years ago, which is faster than the laptop I had six years ago. But more and more these days, what's getting better is not so much the speed of the chips, but the number of cores. Course. And this is where the big improvements are happening. If you have access to a supercomputer, somehow you'll have a student who gets access to the supercomputing facility and they think, wow, now I can run anything. And then you realize that what the supercomputing facility has is not very, very fast machines. It just has lots and lots and lots of cores, lots and lots of CPUs. Which means you can run a billion things much more quickly on the supercomputer than you could run it on your laptop. Then you could run it on your laptop because you can do, if you have access to 500 cores, you can do 500 at the same time. But if you have a single algorithm, like if you want to prove that the vapour's hypothetical utilizes non-Argon, you can't do that any faster on the supercomputer than you can do it on your laptop if it's a single threaded piece of print. So because nowadays with hardware, what you're getting is more cores, what you need is algorithms that can use that. Now, for something like the not-tabular Now, for something like the knot tabulation, at the beginning, when you have 2.2 billion knots that you're trying to manage, this is really easy. Everything is what is called embarrassingly parallel. Embarrassingly parallel. And what embarrassingly parallel means is essentially if you have 100 tasks and 100 cores, you just give one task to each core, and they can run separately. And they can run separately, they don't need to talk to each other, and then you collect the results at the end. Which means that if you have n cores, then you get n times the speed. And when you have 2.2 billion things, each of which is running a very fast calculation, you can afford to do this. And it's easy. Once you get down to the hard cases, as you work your way through to having fewer and fewer knots that you have to worry about, but the computations are getting harder, right? The index of the Harder, right? The index of the subgroups is getting larger, the normal surface theory is getting more complex, whatever it is we're doing, then this becomes less useful to you. Then, what you really want is algorithms which can solve a single problem using parallelism, right? Where the algorithm itself kind of breaks the problem into separate tasks, and sometimes the tasks communicate, sometimes they don't need to communicate. The more complex the task is, the more they need to communicate. Communicate. But typically, this means that you don't get an end-time speed up because, as the tasks have to communicate each other, sometimes they have to wait for each other, sometimes they have to lock resources because they can't all be writing to the same piece of memory at the same time. And so then the improvements that you get from parallelization become less effective. And this really comes down to algorithm design. Algorithm design. So here's a takeaway problem for people to think about. Here's a takeaway problem for people to think about: if you find an algorithm to recognize the unnot, that exploits parallelization well. If you know normal surface theory, what I'm really trying to say is, for example, try to find a way to enumerate vertex normal surfaces in a three-manifold triangulation. That exploits parallelism well. If you have your different favorite way of recognizing the Favorite way of recognizing the unlot. If you want to verify that the fundamental group is trivial, do this with computational algebra, what have you. But at least in the broader mode of normal surface theory, which is what we're using to deal with the satellite knots in particular, we don't have a good solution for this. There are ways of parallelizing some of these calculations, but there's a lot of waiting, there's a lot of bottlenecks, which means you don't get that in time to speed up that you would like. If I had another hour, I would happily talk you through the Hour, I would happily talk you through some of the approaches which are promising but not necessarily realized. But let me just say there is one other thing that you can do with parallelization, and this is unique to topologists somehow. And this is a takeaway trick that I think is worth remembering, is that you can re-triangulate. If you're working with a manifold, you can re-triangulate, triangulate, or if you're working with a knot, you can. With a not, you can modify your not diagram. And what am I saying here? For example, many algorithms you will find have highly variable running times. So the algorithm to recognize the unbound, right? For some diagrams with n crossings, it's instantaneous. For some diagrams with n crossings, it exhibits the exponential behavior that the current implemented algorithms are going to have. The current implemented algorithms are known to have. Like I has a super, was a quasi-polynomial algorithm, but it hasn't been implemented. But basically, the point is that with the same number of crossings, you can get orders of magnitude difference in the running times that you see. And this is just a feature, largely, of the fact that these algorithms have been engineered to buggery. I keep saying this, because what you have to do, they've been engineered to buggery to exploit shortcuts where you can find them, to exploit data structures that behave mainly. Data structures that behave maybe badly in theory, but well a lot of the time in practice, which means you're relying on finding those shortcuts, avoiding those difficult cases in order to get running times, which are much faster than what theory says they should be. What this means is if you get stuck with the wrong diagram of a knot or the wrong triangulation of a three-manifold, then you could find yourself in one of these very sorry cases. And what you can do with parallelization is instead of running at once, you re-triangulate your knot. Say you've got 100 cores, you re-triangulate. I've said you've got 100 cores, you retriangulate your manifold 100 times, and run it on all of them simultaneously. And as soon as one of them finishes, you kill the whole thing. And if you've come across things like no-free lunch theorems in combinatorics, I think this works well. These no-free lunch theorems says, yes, that's not going to work if you're searching a space for an object or if you're trying to do some kind of center combinatorial thing. But we're not. This is not a combinatorial search. This is instances of an algorithm. Instances of an algorithm which could have vastly different running times. And the difference in the running times is well beyond the number of cores that you're dealing with, which means this trick can help you find one of the simple cases that works well in practice. So this is a useful trick, which we've done to great effect in a lot of different settings. So I do want you to sort of take that one away. Two minutes. Let me give you lesson four and lesson five. Let me give you lesson four and lesson five. Let me at least give you lesson four. I don't know if five minutes left. Yeah, but I should be the five minutes for questions, right? So that's sort of, you take and then you give away. Lesson four. Don't be fooled by the tyranny. The tyranny of small numbers. So, a good example of what I mean by the tyranny of small numbers, what I mean is that all the examples you have are small, and you look at them all, and you conjecture some behavior, which turns out to be nothing like what happens in the general case when the cases get large. So, an example of this is where we were saying that up to 20 crossings, there are only 1300 satellites, which is like nothing. Satellites, which is like nothing percent of the knots that you see, but in theory, as n gets large, the bulk of the knots. I think the probability goes to one, right? The fraction of satellites. Known to be proportional. Sorry? It's known to be proportional. Positive proportional. Known to be a positive proportion, conjectured to be one thousand times. Yeah, but even in that case, right, you're looking at primals, right? So you've already thrown out satellite. Okay, we have thrown out lots of satellite. Okay, so let me give you a different example because let me give you a different example. One of them is not untangling. Okay, so untangling knots. So this is with Reitermeister moves. So again, you have these results that say, well, like for an unknown, how many? You can count the number of Reidemeister moves that is needed to untie. That is needed to untangle and unknot. And Lackaby says that there are at most entropy 11 Reidemeister moves that you need to untangle an unknown. Perhaps you're not interested so much in the number of moves, but the number of extra crossings. And one reason for me to be interested in the number of extra crossings. For me to be interested in the number of extra crossings is because that's what defines the size of that enormous graph that I was talking about half an hour ago. So you allow yourself to explore right amongst the moves, but never allow yourself more than one crossing than you started with, more than two crossings that you started with, and so on and so on. And the reason I say this is because it is very difficult to find examples of diagrams of the unlot that require more than one extra crossing. It's easy enough to find something that requires more than zero, but requiring more. More than zero. But requiring more than one extra crossing, it is very hard to find excomplace. And this is where you can be fooled into thinking maybe this number is actually, there is a nice, very small bound. There was the history of lot theory is littered with littered with. I'm trying to find a nicer way of saying wrong results, but there are some wrong results. So, for instance, if you look at the literature, there's a particular example. Which I think was called the culprit, which was claimed to need more than one crossing. So I think it was claimed that it needed two crossings. Actually, if you run this search through this enormous graph with a height of at most one extra crossing, you can simplify the corporate. So it turns out, no, it only needs one extra crossing to undo. There was another example from maybe 10 years ago, which was conjectured to need, I think, one extra crossing. I think one extra crossing, and it turns out that one could be done, not conjectured to, claimed to. That was the problem, it was claimed to. Turns out it needed zero extra crossings. You could actually do it just with Reiner Meister 3s. So finding examples that need more crossings is very hard. But here, okay, so there is a paper. I'm out of time, so I'm not going to write the names down, but I'm going to read them out. There's a paper which has joint work with Chain and Loefla and Demesne and Maria and Schleimer. And Demesne and Maria and Schleimer and Sedgwick and Sprea. And the reason for all these names is because this came out of a Dutchville workshop in 2019, where we actually came up with some examples that need at least three extra crossings and proved computationally that they needed at least three extra crossings by running this enormous search through that graph and exhausting the graph. Exhausting the graph. These knots are bigger than anything you will find in the census. That's my point, right? Find in the census. That's my point. Don't be fooled by the tyranny of small numbers. These examples with three extra crossings had 28 crossings, 43 crossings, 78 crossings. You don't find them in the census. You can absolutely use computer-assisted searches, so you don't need to throw away your machine. But do not expect that what you see in the census is what actually happens in reality. It is a useful tool, but you should be aware of the limitations of the tool. I'm going to stop. I'm going to stop. I've got all this stuff I would love to talk about because, you know, let me give you lesson number five. Lesson number five, which I won't even write down, is if you are working with software in knots and three manifolds and four manifolds or mathematics in general, you will find yourself spending an enormous amount of time doing things that are not mathematics. And I'm sort of here, I'm really talking to the young people in the audience who are trying to decide whether they should devote serious time to computation or whether they should just churn. Serious time for computation or whether they should just churn out papers, right? And the reason I say this is because if you're spending an enormous amount of time doing software documentation, software porting, dealing with endianness on different platforms, dealing with the transition from C  3 to C  11, whatever it is, these things take a fabulous amount of time, and some people don't even realize that you're doing it, right? Some people think this is something that software engineers can just do in five minutes, and it's not. Just do in five minutes, and it's not right, it's like it's a major part of your life. There's the bell. What I want to say is, I think it's worth it, right? Everybody, like people who are theoreticians, spend a lot of time doing things that are not mathematics, that they don't get thanked for. And you do sometimes hear them complaining, but you don't often hear them complaining. My point is that I think it's worth it. And I think one of the reasons it's worth it is because nowadays we are finding more and more that. We are finding more and more that you can do amazing things with computation and experimentation. That not only are things that you can't necessarily do just with theory, but are things that can then guide the theory. And so I think being able to spend all that time on having software, and not just software that is correct and fast, but software that can last 20 years and is maintainable and can live on as a project, which is something that requires serious amounts of. Of there's a word I'm not for. The stuff that people teach in computer science courses that none of us do, right? This is worth the time because I think at the end of the day, what we get out of it and what the community gets out of it is something that is extremely valuable. And I think they will thank you for that. So look, I better stop. The bell's gone. It's 20 past 10. That's enough. Lessons this morning. So enjoy the rest of the conference. Okay, I think we've done a few questions. Let's see five. Sorry? What was less than five? Oh, it was what I just said. Like, don't be bothered by the fact that you spend all this time doing things that are not mathematics. It's worth the time. Yeah. So I'm just a little confused. So when you're doing the calculations for all the knots, let's say you get two knots and they have a volume of 27.21, blah, blah, blah, and the other one has a volume of 37. Blah blah blah. The other one has a volume of 32.9. Can you use that to then say they are different knots, or are you, because it's a numerical approximation, you have to go through these other. Yeah, I guess I think there's two answers. No, it's a very good question. So what I was doing, and this is just the way I was doing it, what I was doing was I was using the hyperbolic geometry to detect when things were the same. Because the canonical triangulations, even if they were inaccurate, they guaranteed to be triangulations of the same not complement, which means if the canonical triangulations did. Which means if the canonical triangulations did match, you had the same bet. If they didn't match, all bets are off. And then I ended up using, for most of the cases, computational algebra, for some of the cases, normal surface theory to distinguish, to distinguish the knots. So I wasn't using volume, I wasn't using any hyperbolic invariance to distinguish knots. I was using, to prove that things were hyperbolic. It wasn't the fact that Stabby found a geometric solution. The fact that Stabby found a geometric solution meant that, okay, it's probably hyperbolic, but then you. It's probably hyperbolic, but then you can use strict angle structures, which are essentially a discretization of some pieces of the equations behind these geometric structures. And because they're discrete, you can actually test for these things with exact arithmetic. And if you find a strict angle structure, that's a certification by Cassie and Riven. That's a certification that your manifold is hyperbolic. Doesn't help you distinguish them, but at least it certifies it's hyperbolic. So I think the longer answer is that if you're living inside Sage, then Answer is that if you're living inside Sage, then Snappy can do some things exactly. I didn't use Sage because of the problems with the memory links and so on and so forth. And also, there's this nice piece of code called Pigbot. What is that? The same thing that's now buried in. Well, so the TSPR reapplied Pigbot insiders to do the calculations we're doing. So essentially, you can ask it. I want to know the volume of asking the same place. To a thousand missing places, and it comes back on a thousand digits, which are proof of the point. So they really distinguish them or to prove that you really have about economic range equation. But it is expensive. And that requires Sage, too. You have to do this inside of Sage because it makes use of Sage's computational, I'm sorry, its interval arithmetics. Mark Pohl has recently made Sage much easier than Sage's because. Made Sage much easier. Sage has become much easier to install on Linux and Macs in the last few years. So been put off by Sage in class. Now easy. But not easy. As far as the options. You had this spider web of writer master one, two, three move identifications. Identifications. So these are all minimal diagrams with a minimum number of crossings. They're not known to be minimal. They're not known to be minimal, but if at any point you were doing your random master music and you dropped below the original number of crossings, then you just toss the entire component. Because you've already seen it at some lower level of the sensors. Because I know that at least for fifteen crossing diagrams, some of them some of these nodes have up to three and a half thousand different minimum diagrams. different minimal diagrams, one fifteen crossing naught. So if you even store only the minimal crossing diagrams, then you would have some 200 billion or 2000 billion or so on diagrams or even more, because they are bigger, which you have to be smaller. And these are just the minimum ones, not for just older ones, even the bigger ones. Yeah, so there's two answers. So the first answer is yes, the data structures are enormous. The data structures are enormous. Second answer is you do all the things you possibly can to avoid actually seeing this full set with that large boundary. So, for example, what you do is you don't just start with all the knots and dump them in the system, then try and explore. You separate the knots by invariants like Honfield polynomials. I'm sorry, for instance, right, take all the knots that are known to have the same Honkey polynomial. These are the only ones that you actually care about trying to join together. And then you somehow sort of aggressively try and connect things together. Together, and what you hope is that you manage to connect them all up long before you've explored the entire diagram. And if you are with good use of invariance and this kind of alternate interplay between using invariance to distinguish things and using white-emissaries to connect them up, you can get away with seeing just a tiny fraction of that full space. But it really is, I mean, there was this interplay of sort of increasingly aggressive work with writer-most moves, increasingly more expensive invariants to compute, more aggressive, more expensive. Compute more aggressive, more expensive, and you would sort of play that game. Okay, let's uh guess about the expense. I know this is hard to write the code, but what's your guess as to the expense of recording the complete JSJ decomposition and the gluings? That's got to be smaller than the Reid Max for the MS. Yeah, so the issue with the complete JSJ decomposition and the gluings, what scares me off with that? What scares me off with that? So I think you're well aware that I've never implemented it because you keep suggesting that I should implement it. The difficulty there for me is not the running time that scares me enough, it's the complexity of the implementation. I mean, particularly the bookkeeping with trying to track gluings, trying to remember maps, trying to... So that's what scares me. I'm looking at the roof because I feel like you're up there. That's what scares me about that. What's your estimate of the size? Right? So right as your. Right? So, right once your graph is large, what's your estimate of the actual size once this terrible algorithm is used? I feel like the size should be quite manageable. My gut says the size should be manageable, but. Are you talking about the size, like the actual size of the data structures that you're left with? Yes, yes. Yeah, I again, I'd say possibly this is a conversation for the coffee break, except you won't be at the coffee break. But That's the leading question, right? Because the Reidemeister graph is humongous and the JSJ decomposition has got to be very small. Yeah, so the answer is implementing the Reidemeister graph with very careful algorithm engineering seemed more feasible than implementing JD compositions with careful algorithm engineering. I'm not scared. You'll never get to 21. No, you'll never get to 21. Well, I've Never get to 21. Well, after 19, I said you never get to 20. And then I walked that back. Yeah, yeah. So, part of the answer there is that the Reidemeister graph was a first attempt at something that was easier to implement, and it was enough. So, because it was enough, at that point I stopped. Okay, let's thank that. Let's start recording and we'll have coffee. And uh we'll have coffee till 10.45. So