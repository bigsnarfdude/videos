Ricard from Builder, who will speak about improved computation of fundamental domains for arithmetic function groups. Okay, thank you. Thank you for the invitation to speak. So yeah, so I'll be talking about computing fundamental domains. There we go. So to introduce the problem, take gamma to be a discrete subgroup of PSL2R. And of course, PSL2R. And of course, this acts on the hyperbolic upper half plane. So when you take the quotient, you get a nice Riemann surface. So we assume it has a finite hyperbolic area. And we denote the hyperbolic distance function by D. So as normal. So the point is, if you take a point in the upper half plane that has trivial stabilizer under the action of gamma, then you look at the following space. Then you look at the following space D of P, which is essentially for every point Z, you look at its entire gamma orbit. It's a discrete orbit because the action is discrete. And you just take the closest point to P in that gamma orbit. And if you take that for every single orbit, you get a fundamental domain for the equient space. So there's other ways to get fundamental domains, but this is one of the most common ones. One of the most common ones. It's called a Dirichlet domain. So, what does it look like? So, it's a connected region, of course, and the boundary is a closed hyperbolic polygon with finitely many sides. Now, there's also a side pairing. So, every edge of the domain is also paired to another edge. So, using this pairing, you can kind of visualize. You can kind of visualize what the domain looks like. So, here's an example. So, here we're actually working with, it turns out that working in the unit disk model of hyperbolic space is a lot more convenient. So, the blue is the unit disk, and the green is the boundary of this fundamental domain. I don't remember what sides are paired with what, but I think this side is paired with this one, and this side is paired with itself, maybe, and a few others like that. That. So here's another example. This comes from a quadratic field. Got a few more sides. So yeah, fundamental domains look something like these pictures here. So of course, why would we want to do this in practice? So the first thing is having a fundamental domain allows you to compute a presentation for gamma with a A presentation for gamma with a minimal set of generators. So, of course, that's not a problem that's always easy to solve. Just give it a general group gamma. Furthermore, you can solve the word problem. So, if you're given an element of gamma, you can write it as a word in the set of generators completely deterministically. You can compute the cohomology of the Shmur curve and the action of Hec operators. Using this, you can compute Hilbert modular forms. Using this, you can compute Hilbert modular forms. So, the reason I became interested in having efficient ways to do this is you can compute closed geodesics on the surface, and then you can visualize them, and you can efficiently compute data related to them. For example, how many times two closed geodesics intersect. And there's many more things you can do with them. So, this has a lot of practical applications. So, again, in this talk, we're focusing on arithmetic fusion groups, so not all discrete subgroups. So, the way we construct these is you start with a totally real number field F and a quaternion algebra over F of discriminant D, and it has to be ramified at all but one infinite place. So, for example, a quaternal over Q that's indefinite. algebra over q that's indefinite will will correspond to an arithmetic fusion group so the way the way we construct them is you uh so there's one infinite place that's uh unramified so you take the corresponding embedding into two by two real matrices which is given by this unramified place you take a maximal order in b and you let o n equals one be the group of elements of reduced norm one Group of elements of reduced norm one. So of course, when you once you embed this at m2 of r, this group is now the group of elements of discriminant, yeah, discriminant one, determinant, sorry, determinant one in the matrix group. This is just definition. Gamma O is the image, the elements of norm one, and we just quotient by plus or minus one. And the point is, this gives us a nice discrete subgroup of PSL2R. So, we will be focusing on computing duration domains for gamma. Now, I should comment that technically an arithmetic fusion group is commensurable with a group like this. But if you wanted to compute a fundamental domain for that, the suggested method would be compute this fundamental domain and then use that to go from there. So, the general algorithm. General algorithm. So there already is an algorithm to do this due to John Voigt in 2009. And this was implemented in Magma. So the basic outline of the algorithm is the following. So you have to compute the area of the fundamental domain via, so there's various formulas for the areas of these. So you can do it completely theoretically. Then there's what we call the algebraic part. So you have to enumerate. Algebraic part. So you have to enumerate some elements of your group gamma O, and you store them in a finite set G. And then the next part is there's the geometric part. So essentially, what you do is you compute the fundamental domain for the span of G is in essence what this. And this is completely geometric. So for this part, it didn't matter that we were an arithmetic fusion group. It could have been any discrete subgroup. It's completely geometric. It's completely geometric. So you do that, and you get the fundamental domain for the span of G, which we call the normalized basis of G. And then you just check the area. If the area is your expected area, you're done because you know your group has to generate the entire gamma O. Otherwise, you go back to step two and you enumerate some more elements, you compute a new normalized basis, and you keep going until you get the correct area. You get the correct area. So, actually, most of these partial areas will actually be infinite. So, normally you'll stop as soon as you have a finite area, is typically the case. So, this is the general outline. And the running times, I mean, it works for pretty much every example. And they're good for small examples, but it's unpredictable and it didn't scale well. One of the main reasons. One of the main reasons for that is the second part here, the enumeration, was a bit unpredictable in how fast it would work. So part two to this, so this algorithm was actually improved by Oral Paj in 2015. So he generalized it to clagging groups. But I'd say kind of the main important part was he changed the method to generate elements. Method to generate elements to a probabilistic method. And this performed much, much better than the original. So this was a huge improvement to everything. So in these improved algorithms, both the geometric and the enumeration would run in basically mu squared time, where mu was again the co-volume or the area. But in his case, But in his case, the geometry generally had a slightly larger constant based on some of the numbers he gave in his paper. And there is a magma implementation for this that's available from his website. So it's not in the live magma, but you can download the code for this. So I wanted, you know, so when I looked at doing this, my initial motivation was to implement this in Was to implement this in Petty GP because that's where all my other algorithms were living in. So I said, well, let's implement it myself in PETIGP instead of moving everything of mine to magma. So that's what I realized that actually you can improve these geometric algorithms so that they run in mu log mu time instead of mu squared. Now both John and Orel have noticed the same thing, but unfortunately this improvement never made it into the live version. Improvement never made it into the live versions that are available in Magma. But you can improve the geometric algorithm. So now it is technically the bottleneck is the enumeration eventually. The second part was, of course, take Paj's probabilistic enumeration and just specialize this down to fusion groups. Again, the live magma implementation uses the old, you know, the 10-year-old enumeration. Uh, you know, the 10-year-old enumeration, which was a lot slower. Um, so in this probabilistic enumeration, you have to make various choices of constants, and these constants have a massive effect on the running time. If you choose bad constants, even though eventually things might be technically faster, your methods are just way too slow. So, you really need to make sure you have the correct constants to make everything as fast as possible. So, because things work a lot faster, So, because things work a lot faster, I was able to generate a large amount of data to kind of justify the choice of some of the constants. And then, just other various kind of more technical code optimizations to add a little bit more speed in various places. So, yeah, so the code is written in PADI and it's publicly available on GitHub. Of course, one of the great things about PADI is it's free and open source. So, you know, when I was at McGill doing this project, all we had was a very old copy of Magma on a slow computer. So you could run PETI on any computer. Everything is totally open. And there is one more thing. You compute this fundamental domain, but then you want to see the picture because that's part of the whole point of it. So there's a Python program to view and explore the fundamental domain. So you can click on an So you can click on an edge and see what it's paired to, and you can zoom in and you can just kind of look at the picture. It's kind of cool. So before we talk a little bit about some of these changes, I just want to prove that yes, things are faster. So there was only a single server at McGill that had magma, and it was kind of slow. The statisticians were always using it, unfortunately. So I re-ran the computations and pity on this office computer here. On this office computer here, that isn't very special and it ran about twice as fast. So, obviously, these timings are highly dependent on the machine. But all these timings were run on the same machine. So, hopefully, that would be consistent. So, here's just a few examples. So, here I just list the degree of the field we're looking at, the discriminant, the norm of the discriminant of the algebra, the area, and then the time it took for magma and Patty to compute these. Magma and Petty to compute these. So, these again, these are just completely random examples I just picked off. They're not cherry-picked anyway. So, this first example, very small. Magma took 13 seconds. In Petty, it took 22 milliseconds. So, quite a bit faster. Although this is not a huge problem. Stick's example is a little bit bigger. Magma took four hours 22. Patty it took under two. Patty, it took under two seconds. Here's an example. Magma only took five minutes for this one. Patty, under a second, 70 minutes to three seconds. So here's an example of some of the unpredictable running times. So this example, degree three area 400, took magma over a day. It took it almost 29 hours. Padia took four seconds, four and a half. Padilla took four seconds, four and a half. However, this next example has a higher degree field, bigger area, but it only took magma 41 minutes. So this is kind of some of the variations in the old enumeration showing up. So still faster and petty. And these last two examples, I just kind of set them to run and I didn't really expect them to finish. And one day I looked and it finished a month later. I was kind of surprised it actually did finish. And it takes 20 minutes in PETI instead of a month. And the same, this last example took 25 days and took about 20 minutes in PETI. So the computations are significantly faster now, I would say. So for the rest of this talk, just going to expand a bit on the geometric and enumerate. Expand a bit on the geometric and enumeration algorithms, and I'll give some graphs of running times for degree of F is at most four. So the geometry, because it's half an hour talk, I will skip through the geometry. I wrote all the slides just to get them all out, but I will skip through the geometry quite quickly. So I'll focus more on the enumeration side. So what do I want to say about geometry? So first, we want So, first, we want to map things to the unit disk. So, you pick a point, and this is the conformal equivalence to the unit disk via the map phi. So now your group acts via, you have to conjugate your group, and instead of living in PSL2R, we live in PSU11. So, if you have an element G of this gamma to the phi, we define what's called the isometric surface. We define what's called the isometric circle just to be the following set: mod Cz plus D is one, where G is, of course, the standard matrix. So the point is, it just looks just an arc bounding two points on the unit circle. There's an interior and an exterior. And the point is, if you have Z on the exterior, if you replace Z by GZ, you get further away from the origin. From the origin. If you have a point Z on the interior, you replace Z by GZ, you get closer to the origin. And if you have a point on the boundary, you stay just as far from the origin. So it's kind of the bounding point between do you get closer or further from the origin when you hit Z by G. So that's what this slide is saying. So the point is, the duration of the domain centered at zero is, of course, the set of Z such that Of z such that d of z zero is at most d of g z zero for all g in gamma. So, what that means is if you compute the isometric circles for all g in your group, which you know you can't really do because it's infinite, but if you could, you look at all the exteriors and you just have to intersect those exteriors and you get your fundamental domain. So, for example, here we have a bunch of G. I drew all the isometric circles. I think there's 13 circles. I think there's 13 circles here or something. And then you just intersect all the exteriors. And this is what we call the normalized boundary for G. So if you end up getting a normalized boundary that has the correct area, you know you're done because this must be the fundamental domain for your group. So what I'll say about this is: so we need to go from this. Uh, so we need to go from this picture here to this picture here. So, you need to essentially delete the unneeded circles and correctly arrange the elements in the order. So, this is very obvious how to do it if you're just given a picture and you said, do it yourself. Well, you know how to do it, but you need to tell a computer how to do it. So, what I'll just say is the old algorithm took big O of n squared time, where n is a number of sides. And if instead And if instead what you do is you initially sort the circles around the boundary and you do a different algorithm, the sort takes n log n time and the rest takes big O of n time. So the sum total is n log n time, where that's really just the initial sorting. So there's an example. I'll skip this slide. So this is just a few more small optimizations. So the next key part was: well, given a set G, we want to compute the normalized boundary of the span of G, not just G. So the normalized basis of G. So we can do this. So this is kind of one of the key points of Voigt's original paper. The essential part is you need to reduce an element to a given normalized boundary. You can kind of think of it as, you know, like over SL2. Of think of it as you know, like over SL2Z, you're reducing an element to the fundamental domain, is essentially what you're doing. So, all I'll say about this is each step in this reducing of an element and previously would take big O of n time, but actually we can do this in big O of log n time. Essentially, we have a sorted, an ordered list, and we just need to find where. And we just need to find where to insert a new element into the list. So it takes the log n time to figure out where to insert the element. So essentially, the improvement on the reduction is n over log n, big O n to big O of log n time. But again, I don't have time to describe all of this, but the sum total of it is the total running time of the gym. It is the total running time of the geometric algorithms should be mu log mu. Yeah, so that's the cost of kind of doing the geometry. So the enumeration we'll talk a little bit more on. I won't really skip slides here. So if you take g in m2 of r, we define this fg of p, which is just this quadratic equation. This quadratic equation in your point P. So, why do we do this? So, the point is: if G is norm one, we let its image in SU11 be ABCD. And then the point is you can express C in terms of this FG of P. It's the numerator essentially. And the radius of this isometric circle is one over mod C. So, the point is, if you have a G that contributes to the boundary of your fundamental domain, it in general Your fundamental domain in a general should have a large radius. The circle has a large radius, so that kind of corresponds to c being small, which corresponds to fg of p being small. So you're looking for elements of norm one for which fg of p is small. Oh, so this last slide is, you can define equivalently, you can define it for elements of u1. You can define it for elements of u11, which is really the space we want to look at. So here we have a bit of a complicated definition, but if you fix two points in the unit disk, you fix matrices in PSU11 such that they map zero to the Z1 and Z2, and we define this quadratic form here. So this term here is essentially you shift You shift, instead of taking G, you shift it by these matrices M1 and 2. So you're kind of like changing the basis in a way. And this term here takes into account the norm, the reduced norm of your element G. So I'll get to on the next slide why this is a quick form to take. But the first point is this is really exactly the same as the definition from Oral Page in the Klanine group case, just specializing it down to fusion groups. Just specializing it down to fusion groups. It's well defined, positive definite, and of rank 4n over Z, right? Because the point is your element G lives in this quaternion algebra, which is rank 4 over your field F, and F is of rank N over Q. So everything, you get rank 4N over Q. And actually, since we're actually really looking in an order, we get rank 4N over Z. order, we get rank four and over z. So the key proposition is that, is the following expression holds. So if you have an element of norm one, then the value of it is the cosh of the distance between GZ1 and Z2 plus some trivial factor n minus one. So this says if GZ1 is very close to GZ2, this quadratic form is small. The value of Q of Small. The value of Q is very small. Yeah. So the point is there's the Finkelpos method, which is a way to enumerate small vectors. So what you can do is you can enumerate the small vectors of Q, and you check which have norm one, and then you can recover G in this way. So this leads the algorithm. You pick a set of random points Z. pick a set of random points z enumerate small vectors satisfying q zero z of g is at most c so here we're fixing z1 to be zero and z2 to be z which which varies you keep all that have norm one and then you compute the normalized basis if you have the correct area you're done otherwise you go back you pick some more points you enumerate more small vectors you get more elements of norm one and you keep you keep repeating this until eventually you compute the fundamental domain Eventually, you compute the fundamental domain. So, there's a couple essential questions. How do we pick the random points and how many? I won't really talk about this much. The random points, you just pick them uniformly from a large enough hyperbolic disk. How many do you pick at one time? So, it turns out how many, as long as you take a reasonable choice, this doesn't have a very large effect on the running time. You know, there's not a clear choice for a There's not a clear choice for a precise number. Yeah. So as long as you pick something reasonable, things will work basically approximately the same. But much more relevant is what value of C we use. So we need to enumerate these small vectors. And of course, if C is too small, you're going to be finding this G less often. And if C is too large, you're going to be enumerating a lot more vectors. So you're enumerating more vectors. You're enumerating more vectors for the same success rate. So it's very important to get a good value of c. So, this is why we compiled a lot of data. So, for this last bit, I'll talk about kind of heuristics for choosing the best C and the data to support the heuristics. So, start off, we let D be the distance between g to the phi of zero and z. So, the distance between the points. So the point is, if we go back to this formula here, if you just manipulate it around, you see that the trial Q of 0, Z, G prime is at most C, this will recover G if and only if cosh of D is at most C plus 1 minus M. So if and only if D is smaller than cosh inverse of this number here. In particular, In particular, since we're not fixing a g, we will succeed in this trial if and only if the hyperbolic disk of this radius has a non-trivial point in the gamma orbit of zero. So therefore, the probability that we succeed is just the area of this disk divided by the total area of the fundamental domain. And if you use the area for the disk, it comes out very nicely, that the probability that this trial outputs us analysis. That this trial outputs us an element is 2Ï€ times C minus N divided by the area. So it's linear in C is the probability that we succeed. So does this actually hold? So for these curves, what I'm going to do is I'm going to label kind of the setup by N, where N is the degree of the field, the discriminant of the field, and the norm to Q of the algebraic of the craterian algebra. Of the crater algebra. Of course, this doesn't necessarily uniquely label a curve, but that doesn't really matter to us. So, this first example, I took a range of C's, did a thousand trials, and said, how many elements did we find? So, that's the blue circles. And then the red line is this heuristic here, this predicted probability success. So, we see it matches up quite well in both of these two examples. Quite well in both of these two examples. So that's good. It says, you know, our heuristic appears to be working at the moment. Okay, so that's the first part. The next part is how long does it take to run this test? So the point is this is a quadratic form of rank 4n. So the total number of vectors that we should get would be proportional to C. Get would be proportional to c to the 2n. Yeah, so we expect the time to complete the enumeration, assuming that the Finkapost method is relatively efficient, should be some constant time c to the 2n plus a, where a is like the setup time. Because running Thinkaposh requires some setup initially, and then you run it. So we expect there to be a setup time and then. Setup time, and then you know, essentially proportional to how many vectors we generate. So, of course, A and B are highly dependent on the implementation and the machine, but the ratio A divided by B should be essentially independent on the hardware you're using. You know, if you have a computer that's twice as fast, you'd expect kind of the time to divide by two. So, yeah, but it is highly dependent on the implementation. Yeah, but it is highly dependent on the implementation. So if you change, if you change how you do FICA post or you change to a different way to enumerate small vectors, a new mysterious way, you know, your values of A and B will definitely change. So here I fixed a curve. I did the computation time for a large range of C's. And then I also drew in the, I did a linear regression for A and B. For A and B to get these curves. So this matches extremely well. This is good kind of support that yes, there is no hidden time sink anywhere. These are the time does obey this heuristic. So the point is, we know how long it takes to get us to run a test, and we know the probability of success. The probability of success, so you get the expected time taken before a success to be the following object here. So, this is just a constant, and this is dependent on c so you just use basic calculus, and you see that this is minimized for the unique real solution to this polynomial here. So, the point is for a fixed algebra, you run this timing computation for a sufficiently large range of C's, you run a regression to turn to determine. You run a regression to determine A and B, and then you just solve this equation to determine the theoretical best value of C. So you can repeat this over a large selection of algebras, and you can get a heuristic for C by examining the data you get. So this is the result of the heuristic. So the optimal choice is given by a constant depending on the degree of the number field. Constant, depending on the degree of the number field, times the discriminant to the 1 over n, times the norm of f over q of the algebraic algebra discriminant to the 1 over 2n. So I should mention these constants Cn are extremely important. If your constant's off by a factor of 2 in, say, the degree 8 case, all of a sudden you're getting twice as many successes, but you're taking 2 to the 16th times as long. So this is really. So, this is really, really bad. So, you unfortunately, these constants really matter. I don't have an explanation for them. I ran a lot of data, and these are kind of the best constants approximately for n at most eight. So there's a big drop to two, and then it goes up again, and then it goes down again. So, I don't really have a good grasp of what they're doing there, but these are some constants that should work. And I should say, you And I should say you can also justify this with some more heuristics. But I'll just present kind of the data to show that this works. Yeah, getting near the end. So here you start by fixing a field and you take a bunch of algebras of different discriminants and you run the regression and you get tests. So again, the lines are pretty good. Then you fix. Then you fix the number, you fix the degree of the number field, and you vary the discriminant of the field. So you fix the degree and you vary the discriminant, and you run the regression over the data, and you get, again, pretty much exact matches. So this just kind of justifies the choice of the heuristic. Obviously, it's important, does this actually work? So I computed the time to obtain a thousand elements for different values of. Contain a thousand elements for different values of C. So again, if C is too small, it takes a long time. If C is too large, it again takes a long time. And this red dotted line is the heuristically predicted optimal value of C. So in both of these cases, you see that it pretty much lies at the minimum. So this is just saying that, yes, you know, all this theory does actually work to get the best value of C. So just a final couple additional comments. Comments. In Paj's paper, he also has a heuristic for C in the Kleining group case, which is different than the one I present here. But actually, it turns out it's actually the same. The paper has a typo. And the version in magma that's implemented has the same heuristic, but the values of C sub n were not done with the same level of detail in data. Next comment is, Comment is a subtle part of the algorithm is you have to check all these norms, and just using the built-in function check norms algeb norm is actually quite slow. So it's essentially better to Cholesky decompose a norm, i.e. write it as a sum of squares. Kind of like the same trick for evaluating polynomials where you don't evaluate x to the n, you iteratively go up. It's kind of the same trick. And this greatly speeds up the enumeration. This was a very non-obvious part. This was a very non-obvious part that was slow. And the final comment is: there's an improved, what I call improved Fincapost algorithm. So in the last step of Fincapost, you need an element of norm one, so you take into account this condition. Instead of getting a list, you can solve a quadratic equation over Z. So initially, I thought this would just be faster overall, but it turns out to only be faster over F. Be faster over F equals Q. So, all that to say, you know, very quickly that we can improve this enumeration of small vectors by a small constant factor, maybe one and a half times or two times over Q, but this improvement doesn't work over a general number field. So, to finish, the total running time is as follows. So, here's a couple of graphs for editing. Um, so here's a couple graphs for n equals one and n equals two. So these are all uh yeah, here and here. So uh in this case, the geometry always dominated for n equals one, uh up to co-volume 20,000. For n equals 2, it's around 3,700 that the so beneath 3700, the geometry was the bottleneck, and above it, the enumeration is a bottleneck. Obviously, eventually the enumeration well. Obviously, eventually the numeration will always be the bottleneck, but the constants cause the geometry to be the bottleneck in a lot of the small cases. A couple more running times. So degree three and four. So if you have a co-volume 1000 case in degree four, it should take about 40 seconds. And just here's just a couple slides, the code in action. So you initialize a number field, the algebra, you compute the fundamental domain. The fundamental domain spits out the answer, and here's just a picture of this fundamental domain. So, in this Python program, you can click on a side and see which side it's paired to. So, thank you for the attention. So, thank you for your talk.