So, yeah, this talk is about some stuff I've been doing for the past year with many collaborators you see listed here. So, Fabrizio, Francesco, Yuguanguang, Guido, Nina Otter, Ben Chamberlain, Michael Bronstein, and my supervisor, Pietro Leo. And overall, it can be framed as deep learning on topological spaces. So, yeah, let's get into this. So, yeah, maybe needless to say to this audience that geometric deep learning has kind of taken Geometric deep learning has kind of taken over machine learning, and that's primarily because data resides very often on these very non, you know, non-Euclidean spaces or more spaces with interesting geometry or topology. And this has led to a revolution, especially in the life sciences. And I think a particular kind of subfield that's very interesting of geometric deep learning is graph machine learning, which comes with like some particular challenges. And in general, Challenges. And in general, for graph machine learning, we work with this setting where we have a graph or multiple graphs with n nodes, let's say, and the feature matrix associated with the nodes. So basically, we have a vector for each node in this graph, and that's kind of the initial features. And we can kind of distinguish two sorts of tasks. So, first, there are graph-level tasks. So, here you have a data set that is made of graphs. So, each sample in your data set is one graph, and you want At this one graph, and you want to predict a vector for that graph. So, condense all that graph in a single vector representation for which you can, you know, you can do whatever you want with that, classify it, or maybe solve some regression task. So, a classic example here would be kind of predicting the solubility of molecules, for instance. Now, the other type of task are a node-level task. So, here your entire data set is kind of a single graph, usually. And what you want to do is And what you want to do is, for instance, classify the nodes or maybe do some regression on the nodes. So you might have some labels, like here you have these red and blue nodes, and you want to find some decision boundary to separate these classes. But you also have these gray nodes essentially that are kind of the testing nodes, the things you don't have labeled, and you want to kind of find the label for. And the classic example here might be fraud detection. So you might want to detect people committing some sort of fraud in some sort of network of clients or something of that kind for some application. Of that kind for some application. And most of the models that operate on these sorts of tasks can be framed in terms of this message passing framework. So apologies if you're very familiar with this. But basically, models in this framework can generally be described in terms of these two operations. So the first one is aggregate. So for each node, you sort of aggregate the features of the neighbors. Features of the neighbors, and that aggregation could involve some processing. And that result of that aggregation is called a message. So it's the message that this red node in this picture receives. And then you update the features of the red node by combining this message with the old features. And most graph neural networks kind of vary in the way they implement this aggregate and combine features, combine operations, usually with some neural networks. And this is a very active area of research. Area of research. Now, something that's very interesting is that graph theory and topology share a common history, and as with most things in mathematics, they start with Euler. And Euler had this work, Geometria Situs, the Geometry of Location, in which he kind of solves this famous problem of the bridges of Koningsberg. And you're probably familiar with this, like finding basically an olarium. Finding basically an Ollarian cycle or path through the city. So, basically, you can't cross any of the bridges twice, and you want to kind of visit all these islands while walking in a cycle and either show why this cannot be solved basically due to the degrees of this graph by modeling of the problem as a graph. And I think one kind of essential piece of intuition here that led to this kind of approach. That led to this kind of approach is that the geometry of the city doesn't actually matter, like you know, the particular shape of the bridges is irrelevant, like you know, the pavement of the bridges or something, it doesn't really matter. All that matter is kind of the connectivity. And it's similar when we draw a graph, we don't care about that sort of information typically, right? Like we don't care about the shape of an edge or how we draw it. Like it could be like super curly or something. It doesn't matter. What it matters is what nodes it connects. And this is kind of the perspective. Connects, and this is kind of the perspective topologists are taking. So, here's kind of one undergraduate course and maybe some postgraduate courses in one slide. And I think this is the kind of typical example people use to explain what topology is. And essentially, it's about studying continuous deformations of objects and potentially things that remain invariant to those deformations. And there is this joke you're probably familiar with that topologist, the mug and the donut are the same. The mug and the donut are the same thing because you can transform one into the other. Now, this might seem like super abstract and totally unintuitive, maybe, and consequently potentially useless because it's so abstract. And we're going to see actually we can apply some ideas from this field for improving existent models for both types of tasks that we've seen. So, graph-level tasks and node-level tasks. But the fact that kind of But the fact that kind of you can perceive this as being super abstract is not like something new. And there's this code that, if Bastian gave a presentation, he probably was in there as well. So maybe you've seen this already. So there's this code from this book by Alexander Solzhenitsyn. So topology, the stratosphere of human thought in the 21st century, it might possibly be of use to someone. And we're going to see that topology has actually become useful much earlier. So we don't have to wait a few more centuries. We don't have to wait a few more centuries. And this talk is about kind of seeing showing some examples of that. All right, so the first part is about topological message passing. And I'm going to explain in a second what that is. But what this is trying to do is solve some problems for graph level tasks that this message passing framework I described has. So the first problem is about group-wise interactions. So in a graph, you can, by definition, just encode. You can, by definition, just encode pairwise interactions in the graph structure. But you can't describe things like these three things interact together at the same time. And that shows up often in applications like in a chemical reaction, you might have multiple reactants that interact at the same time. Another problem is about higher order structures. So graphs or graph neural networks, they don't explicitly model higher-order structures. So if you have some structure that's potentially relevant, Structure that's potentially relevant for your problem, you kind of have to search for it, right? You need to do all this message passing and hope that somehow that message passing will identify that structure. And there is also lots of theoretical work showing these models cannot find certain structures in certain settings. So that's a problem. Another one is about high-order signals. So often you might hope you might kind of face some problem where your data is not necessarily living over the nodes, but might be living over the nodes. The nodes, but might be living over the edges or might be living on top of some higher-order structures. So, you know, how should you process that, or how should you take this message passing framework and kind of adapt it to that setting? How should you process those filter as those signals? How should you do signal processing on that? Another problem is about expressive power. So, maybe you're familiar with this line of research about the Weissfire Lemon test. So, this is kind of a graph isomorphism test, and people have shown that these message passing models are as powerful. Message passing models are as powerful as this test. And there's a whole hierarchy kind of these tests that are increasingly powerful. And people are trying to match that hierarchy with better models. But there's a question, how can you kind of be more expressive while not necessarily incurring the computational complexity of these tests? And finally, long-range interactions. So often the things you model have long-range interactions. So atoms that are very far apart in a molecule, they kind of need to talk in order to predict some relevant property. In order to predict some relevant property, so you need lots of message passing layers to make these nodes kind of aware of each other inside the model, and that might lead to all sorts of problems. And there's a question, how can we solve that? All right, and I think the common theme for solving this is to go beyond graphs. And the first thing to realize is that graphs are just kind of one thing in a whole hierarchy of spaces. So, first, you kind of have point clouds, which are particular types of graphs, so just like a set, essentially. Of graphs, so just like a set essentially. But then you have this generalization called simplicial complexes, where you kind of put these simplicies together and kind of glue them nicely. So besides having just nodes and edges, you can also have these 2D surfaces like triangles or tetrahedra and so on. But you also have something even more general. So these are called cell complexes. So here, your kind of high-order surfaces, they don't necessarily need to be like simplices. So they could be something like the Something like the square you have in here, essentially, that you have like this rectangular 2D surface. So, not just triangles, for instance. And in this terminology, we call vertices zero cells, we call edges one cells, and 2D surfaces two cells, and so on. So, that's just in case you're going to hear these words. And the main idea of this topological message passing is essentially to do message passing on these higher-dimensional objects. Higher dimensional objects. So you might do something like here in this diagram, and essentially you have a new sort of adjacency structure that's generated by kind of the incidence relations between these cells in your cell complex. And this might look something like here, where you don't only have nodes communicating like you have in graph neural networks, but you also have nodes communicating to the edges, edges communicating to two cells, edges communicating between the nodes. Cells, edges communicating between themselves. So you have all sorts of cells passing messages between each other and also between kind of different dimensions or layers of this cell complex. So first you might ask, like, you know, where do we get this cell complex from? Because, you know, I've never seen like a cell complex data set given to me. And you're absolutely right. And what we're doing is starting with the graph data set, and then we perform what we call the lifting operation. From what we call the lifting operation. So, this is a very kind of common way in algebraic topology to construct these things, and they're actually defined in this way. So, basically, you start with a graph and you put some two-dimensional disks, you glue them to the boundary of, for instance, some of the cycles in your graph, and then you obtain a two-dimensional cell complex. So, we will kind of follow a procedure like that on some graph data sets. All right, so why is this a good idea? So, a first reason is that Yeah. So, a first reason is that you have this domain alignment with certain applications. So, these structures are very natural to model things like molecules, right? Like you have nodes are atoms, edges are bonds, but then you also have these chemical rings, which are very important in describing the properties of molecules. Now you can model them as two cells. So, this is something that was not kind of modeled explicitly before in previous models. Another reason is that this naturally adds this inductive. That this naturally adds this inductive bias of having like a hierarchical message passing. So the message passing sort of reflects the hierarchy of the molecule, for instance. So you have this kind of a bottom-up or top-down perspective, depending on how you want to look at kind of going from microstructures to these macrostructures. So atoms, bonds, and then chemical rings. Another is we can finally kind of tackle this long-range interaction problem without actually. Range interaction problem without actually using a ton of layers. And this is because these higher-dimensional cells sort of kind of create shortcuts in your message passing flows. So instead of kind of going from left, as in the left image, from this red node to the green node by kind of jumping from one node to another, you just kind of jump to the two cell, and from the two cell, you jump back to the node. So you can kind of get there in just a constant number of steps, no matter how big this cycle is, as long as you model it as a two-cell. You model it as a two-cell. And finally, we can show that this sort of message passing is actually more expressive than kind of the regular message passing I described in the beginning, in the sense that it can distinguish some kind of pairs of non-isomorphic graphs that message passing cannot distinguish. So, if you perform regular message passing on two graphs, often you will get the same outputs, even if the graphs are very different. And we can And we kind of formalize this sort of results by proposing a generalization of this Weiss-Ferralman test to cell complexes. So, how this works, just to kind of give you a brief overview, is each cell in your complex starts with some initial color, and you kind of keep refining these colors similarly to how you do with the Weissfer-Lehmann test on graphs, by kind of hashing the colors of the adjacent cells. And you keep repeating this process. You keep repeating this process, and at the end, you kind of compare the histograms of two cell complexes of the colors generated. And if those histograms are different, then the cell complexes are not isomorphic. And it turns out that if you first start with a pair of graphs, you map them, you lift them to a pair of cell complexes, and then apply this algorithm, you're actually more expressive and you can distinguish more graphs. All right, and this sort of approach actually works. So we applied it to multiple molecular data sets. So zinc is. Multiple molecular data sets. So, zinc is, for instance, predicting solubility of molecules, and MOL-HIV is predicting the capacity of certain compounds to inhibit HIV replication. And at the time, we obtained state-of-the-art results. And I think one year later, these are still state-of-the-art on some benchmarks. I think at least small HIV is still outperforming even the transformers out there, and many models that have been proposed while using very few parameters. Parameters. All right, now in the second part of the talk, I'm going to talk about this more recent work on neural shift diffusion, but the pattern you're going to see is very similar. So essentially, you start with the graph, but you lift this graph in a higher kind of dimensional object with some additional structure compared to the original graph, and that additional structure will help you solve some problems. And one of the problems we're going to And one of the problems we're going to look at, so also here we're transitioning towards a node class, sorry, node classification setting. So basically you have a single graph here. That's your whole data set, and you want to classify the nodes. So basically, the second setting I was mentioning in the introduction. So in this setting, a problem that's kind of well studied already is this over smoothing problem. So if you put many GNN layers on top of each other, GNN layers on top of each other. What you notice is that the features become increasingly smooth. Now, this doesn't happen for all graph neural networks, but for many it does. So you have this problem that you can't recognize, you can't separate the features anymore because they all became too similar, so you can't separate any classes. And here's a diagram from one of the papers where they prove why this happens for this particular model GCN. Model GCN, and they measure the distance between the signal and a subspace where the features are constant. So you could think of this diagram as saying how close to constant are my features. So basically having a constant feature vector, basically. So all features are the same. And you can see the distance to the subspace empirically decreases. And they also prove a theorem like an upper bound showing that does indeed happen. Now, Now, another problem that we're going to show is sort of related is the heterophili problem. So, what happens is that many graph neural networks, so this is, I think it's slightly more universal than most graph neural networks, they struggle in what are called heterophilic graphs. So these are graphs where sort of opposites attract. So, you have nodes with different labels, they tend to be more connected than nodes with the same label. Same label. So this shows up very often in applications. For instance, as I was mentioning, in fraud detection, or maybe let's say spam detection, if you have some network with spammers and real users over some, let's say, email application or something, obviously spammers tend to connect to real users because they want to spam actually real users. So they're not going to connect to other spammers in general. So you're going to have like very high heterophyly. So that's going to pose some problems if you actually. So, that's going to pose some problems if you actually want to kind of classify who's a spammer and who's not a spammer. And this is a diagram you see here from a recent paper. And here they plot the homophily level of the graph. So that's the opposite of heterophily. And against the accuracy of the models. And you can see that pretty much all the models in like lower homophily levels are high heterophili. Their performance is relatively low, but it kind of keeps increasing as. Increasing as you increase the homophony. And obviously, there's already some more colored lines here show some more recent work that has already kind of improved the situation. But you can see originally, at least the graph convolutional network model was doing pretty badly. It's doing pretty badly, but other better models are out there. But it's not kind of a solved problem, neither theoretically, I would say, and neither empirically. Okay. Okay, and something to realize is that there is a very kind of a simple model that intuitively suffers from these two problems. So first, let's just introduce annotation. So we have a graph with self-loops. We have a degree matrix. So this is just a diagonal matrix with the degrees of the node. Adjacency matrix A. Then we have the normalized Laplacian, which is just identity minus this normalized adjacency matrix. Normalize the JC matrix, and we have, as before, like a matrix of node features. And now, this model I want to talk about is heat diffusion. So, let's just say you have some initial features, and suppose you just do heat diffusion. Now, if you're familiar with heat diffusion, you can kind of imagine having some temperature on top of each node. And as you run this diffusion process for longer, kind of the temperatures balance out and they. Temperatures balance out and they kind of become uniform. So, if you use this sort of model, you can imagine why it suffers from these two problems. So, like, if you're in a heterophilic graph and you start kind of doing this diffusion, you start to kind of blurry the features. So, you're going to blurry the features of like in certain neighborhoods, right? And then things that these kind of opposite classes that are connected, they will become indistinguishable because they kind of converge towards a similar representation. Similar representation, and you can also see, like, kind of oversmooting why over smoothing is a problem. So, if you kind of run this process infinitely long, you're going to end up with some constant features. And of course, you can't separate anything in whatever graph you find yourself in. So, this is kind of like a prototypical kind of a, you know, kind of counter model. Like, this is something you should not do because it suffers from these two problems in kind of like in a very canonical way. So, yeah, let's look at this model. So, yeah, let's look at this model and let's discretize this model by just in time by just using like an Euler discretization. And you get something that looks like here. And we can also use this common factor x at time t and rewrite this like this. All right, so I highlighted this in red because you're going to see it's going to show up again in a second. So if you look at the graph convolutional network, which is one of the most famous graph. Famous graph kind of models out there proposed by Kif and Welling some while ago, which looks like this. So you have this normalized adjacency matrix times the feature X, and then you have some weight matrix that does a linear transformation, and you put everything through some nonlinearity sigma. Now, by just kind of looking at the definition of the Laplacian, you can just massage these terms a bit, and you can see that what you get in like the discretized diffusion shows up again in the graphical. Shows up again in the graph convolutional network. So perhaps it's surprising that graph convolutional networks are just some sort of augmented diffusion where we've made it a bit more complicated because we put that weight in there. So we do some linear transformations and we've also put nonlinearities. But it turns out, despite these kind of additions, this doesn't, you know, doesn't make this model behave very differently because it still suffers from these problems you would imagine diffusion. These problems you would imagine diffusion is suffering from. So, then the question we're trying to answer in this paper is: how can we make this process, this diffusion process, more advanced or more interesting or more powerful? And then automatically, any sort of models we build on top of this diffusion should also be more powerful. And to do that, we're going to use sheaf theory. So, there are these things called cellular sheaves that have been sort of Sheaves that have been sort of discovered in, I think, in the 80s, and then Justin Kerry, in his PhD thesis, I think he kind of brought them back to kind of life and in the applied domain. And it's actually very simple on a graph setting, what they actually do and what they mean. So it's just a graph with some kind of additional structure on top, as I was alluding to earlier. So for each node, you have a vector space leading on top of that node. Of that node for each edge, you have a similar situation, so one vector space for each edge. And whenever a node is incident to an edge, you're going to have a matrix that maps you from the first vector space to the second vector space. So you have this F V triangle E denoting these matrices that kind of move you from one vector space to another. And obviously, if you use like a transpose matrix, then you move, for instance, from node E to node V. From node E to node V. So you move between the respective vector spaces. And these sort of constructions sort of arise by thinking about the graph as a topological space. And then you kind of have this general continuous definition of sheaves that shows up a lot in algebraic geometry. And you get this kind of discrete version I'm describing here, which is much easier to understand. So sheaves, like the continuous version, was invented or discovered depending on. Invented or discovered, depending on what side of the mathematical philosophy debate you are, by the French mathematician Jean Le Ray during the Second World War. And this is actually a very funny story because he was taken prisoner once Germany invaded France and he was basically sent to this prisoner of war camp in Austria. And because he loved math, he wanted to, he was an expert in partial differential equations. Expert in partial differential equations, and he wanted to start some sort of university, I think he called it in captivity. And it was probably like a reading group or something. And yeah, like he was scared that because of his expertise on PDEs, you know, you could do lots of useful things with PDEs, especially for building war machines and that sort of thing. So he was concerned that maybe the Nazis will kind of force him to, you know, work for their Work for their regime. So he kind of proposed a topic for this university that was kind of, you know, very pure mathematics, sort of like, and kind of seems useless to everyone. So he picked algebraic topology and he actually kind of revolutionized the field. I think he also invented the spectral sequences in those years. So lots of interesting things came out from there. All right. So with that aside being over, another way to look at the Another way to look at these objects is through the lens of opinion dynamics. So, there is this very nice paper by Hansen and Rob Greist, kind of constructing this mental picture of cellular sheaves where each of these vector spaces associated with the nodes are a space of private opinions of individuals. So, the nodes are kind of like individuals. And whenever there's an edge between individuals, they kind of meet in the middle in this discourse space. In the middle, in this discourse space, so that's the vector space of the edge. And what these matrices or linear maps are doing is mapping the private opinions into public opinions. So it's kind of like a filter that manifests how my private opinion shows up in public, basically. And it turns out it's a very kind of flexible model to describe all sorts of interesting situations. Now, a more kind of geometric perspective of the seller. Of these cellar sheets, you could see them as some sort of generalization of discrete OD bundles. So, what are these? So, these are some objects from differential geometry, where essentially you describe how you attach vector spaces to some manifold. And this is called a vector bundle. So, a classic example is just the kind of the tangent space of a manifold, right? You have a tangent space at each point on the manifold, which is kind of like Point on the manifold, which is kind of like a vector space. And whenever you have a setting like this, there's this notion in differential geometry of moving vectors from one tangent space to another that's called parallel transport. So you sort of move these arrows smoothly from one vector space to another. And what this kind of in our discrete setting, you could think of these vector spaces I was describing as kind of the tangent spaces. And here you can see this graph that's kind of sampled from this half sphere. Of sample from this half sphere, and what these matrices are doing, they're kind of transporting these vectors from one vector space to another, so it's kind of like a discrete version of parallel transport. So, that's kind of the intuition. Also, in the kind of differential geometric setting, these matrices are orthogonal matrices or invertible. But sheaves are kind of more general, so these maps don't have to be in general. So, you could think of it as a generalization, all right. Of it as a generalization. All right, so what we're doing in this work is going back to the equation we saw before with heat diffusion, and it looks pretty much the same. And actually, you might think it's the same. And the only difference is there's this F subscript in this Laplacian. And that's because we're kind of using another Laplacian this time. So this is called a sheaf Laplacian. And I'm not going to describe how it looks like, but essentially it's a block matrix. But essentially, it's a block matrix. So it looks kind of different from the usual Graph Laplacian. It generalizes the usual Graph Laplacian. So the Graph Laplacian is a particular case that arises from choosing some particular sheaf over the graph. But you could think of it as like, you know, some Laplacian operator that encodes all this sheaf structure or this geometry you've put on top of the graph. And by choosing different sheaves here, the diffusion process will look very different. So essentially, you can. So, essentially, you can pick different geometries that are suitable for your task. So, to give you just one example of how this looks like, so we have here a graph where the position of the nodes are the features. So, wherever they're positioned in this plane, that's their feature. So, they are two-dimensional features. So, you can see at time zero, this graph is very much entangled. So, all the classes are kind of mixed up and nothing is really. And nothing is really separable. And you can pick a shift, for instance, on this graph such that when you do diffusion, like here, everything becomes very nicely clustered and linearly separable. And you can show you cannot do this with kind of the usual heat diffusion, even if you assume something more general, like a weighted graph. There are certain settings where we proved you won't be able to separate the classes. So in certain heterophilic settings, if you just have like a weighted graph, Settings, if you just have like a weighted graph Laplacian and you do diffusion, you can't perfectly separate your data. But what we show in this paper, and this is kind of a series of results, but I've just kind of merged them in this very informal statement here. Essentially, for any node classification task you have over a connected graph, there exists a shift such that when you perform diffusion infinitely long, at the end you will be able, like in this picture above, like in this example, to separate very nice. Above, like in this example, to separate very nicely all your classes. And this is very nice because it doesn't matter if your graph is homophilic, heterophilic, or whatever, you can always find such a sheaf. Whereas with the previous Laplacians, you need to have some homophily assumptions. And this also relates to oversmoothing because here we're doing diffusion infinitely long. So it's kind of like having an infinite number of layers. And we're saying, okay, in the infinite. And we're saying, okay, in the infinity time limit, so you know, use an infinity of layers basically, you're still going to end up with some nice features that are separable. So in some sense, this is kind of how this relates to these two problems I was mentioning in the beginning. Now, this doesn't mean the problem is solved because, you know, what's that sheaf that actually, you know, we showed it exists, but how can we find it? Where do we get it from? You know, let's say someone gives you a graph with one. Someone gives you a graph with you know one million nodes or something. You know, how do I build that sheaf? And I think that's kind of one of the areas that this research will kind of go on. But what we do in this paper is we propose learning these shifts from data. So essentially, what does it mean to learn a shift? It means to learn these matrices I was describing. So we always assume we have some fixed vector space over all the nodes and edges. So in this case, it's just R2. Cases just R2, so just a plane basically. But you can have different kinds of spaces over each node more generally, but it's not necessarily useful for our purposes. So we just stay with picking some dimension D, and then all the vector spaces are R D. And what you do is like you're going to have some features at node V, this X V, and you're going to have some features at node U, X U. And we just pass this through some this neural network file here. This neural network five here that's matrix valued. So, what this equation three says: like, use the features of the nodes to build the matrix between from this F V E. So these red components in this diagram, those are given by these neural networks, essentially. So, once you've outputted these matrices, you can just build your Laplacian from that and process the features. So, it's a bit recursive because you So, it's a bit recursive because you use the features to build the Laplacian, and then the Laplacian will process the features. Yeah, so that's kind of the overall idea. And then we just apply this to like some by building a model based on these Laplacians that are learned over time. It doesn't really matter how the model looks like. But yeah, like we apply this in some heterophilic graphs. So there's this homophily level ranging from very low to very high. Ranging from very low to very high as you go from left to right in this table. And here, red denotes the highest score. I think blue is the second highest, and then the third score is in this purple. And in general, our models, which we have various versions, so depending on the type of matrices we learn. So, for instance, you could learn diagonal matrices or orthogonal matrices or just a general matrix. So, these are kind of three versions. So, these are kind of three versions of the model with different kinds of computational complexities and trade-offs. But in general, they all kind of score in among the top three models for all the heterophilic graph benchmarks. And they're also performing reasonably well in homophilic graphs. All right, so I think that's all I had. There's a bunch of references here if anyone's interested in kind of following them. And yeah, thanks a lot for your attention in case you have questions. Attention in case you have questions offline, contact details, and yeah, happy to answer any questions you might have now.