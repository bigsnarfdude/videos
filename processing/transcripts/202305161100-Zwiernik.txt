I really wish I was. I'm sure the weather is much nicer than here in the UK. I thought very long and hard about what I could say at this workshop that would be useful to a good number of people. So I think a few people in the audience probably know me and know that I do persistent homology, and this is what I spend most of my time on. But I'm also aware that. But I'm also aware that a large portion of the workshop isn't involved in this kind of applied topology, persistent homology world. So I really thought about what I could say that would be kind of useful to as many people as possible. And given that there is a lot of talk about computation and algorithms in the talk so far, I thought, you know, we've been computing. You know, we've been computing persistent homology in one way or another for let's say about 20 years now. We're still kind of working on it, and I think there's a few lessons that we've picked up along the way that I think, you know, again, for a good portion, some portion of the audience, this will be completely obvious. I hope there's at least kind of one new or surprising thing in the talk for everyone. But I wanted to kind of share this and To kind of share this, and I wanted to make it as general as possible. So, the examples I will give will obviously be from persistent homology, but I think they are more general algorithmic points and kind of implementational points that I think are more broadly applicable. And I'm also aware that I'm standing between you and Koffe, so I will make sure to finish on time. Okay, so first I'm just going to Okay, so first I'm just gonna say a few things about computing homology. I think, again, I'm sure that most people in the audience know how to do this, probably better than I do. But just so that we have the same kind of vocabulary and that everyone knows what I'm talking about, then really I'm going to go into persistent homology and basically divide it up between theory and practice. And I'm going to highlight kind of three main ideas. Going to highlight kind of three main ideas. There's a few tangents in there, but depending on how quickly things go, I can easily skip over those. And there really are kind of three take-home points that I want to kind of hit. And then I'm going to say where the field is going right now, how I think it interacts with things that other fields such as yours think about, and also kind of the takeaways, what I hope you remember from this talk. Okay. Okay, so again, you know, given especially, you know, the very nice talk we just heard, I don't think I really need to say very much about homology or just kind of this general viewpoint. But we want, right, we generally start with some topological or combinatorial object. There's a whole thing, right? There's a whole kind of set of procedures on how to go from topological to combinatorial. Maybe you're already given. Maybe you're already given the combinatorial. It's not something I really want to kind of go on today. So we can just start off with some combinatorial object and we want to replace it with some algebraic object because we believe this is a good thing to do, mostly because we believe that the algebraic object is somehow easier to deal with. Now, I'm going to be giving kind of a lot of examples and these things generalize to different degrees, but the running, you know, if I But the running, you know, if I don't say anything explicitly, you can kind of think that I have a simplicial complex, and the algebraic object I care about is a vector space or something very closely related to a vector space. So very, very simple algebraic objects. I will talk about kind of where the complications as you start to add more algebraic structure, or I guess less algebraic structure, depending on your viewpoint. And, you know, I will kind of. And I will kind of highlight those things and what those mean for the actual algorithms. Now, the key point here, and the whole point of this slide, is really the entire talk is really just about matrices. If you don't care about homology or persistence or anything else, you can just think of these as matrices. They are just kind of a special class of matrices that we're going to deal with. But everything will fundamentally look like linear algebra. Okay. Okay, so I think everyone here knows kind of the general pipeline, right? If I have a simplicial complex and I want simplicial homology, what do I do? Well, I first create this chain complex, which, you know, again, thinking of it as a matrix is just an identity matrix, which with each dimension indexed by simplex. And these simplicies fit together in some nice way. And these attaching maps between the simplicities. And these attaching maps between the simplicities are going to be given by another matrix, which is just the boundary operator, a boundary matrix. I will use the two interchangeably. And of course, the key property that makes this a boundary operator or matrix is that if you apply two of them in adjacent dimensions, you get zero. Basically, this thing is why everything works. I'm not going to point it out explicitly because, again, I think it will be obvious. Again, I think it will be obvious to many of you why these things that I'm saying are true. Okay, so before getting into persistence, back to homology. We all, I think, know the definition of homology. It's the kernel of one boundary matrix quotiented out by the image of the one that's one dimension higher. Okay, so you know, it So, you know, it's worthwhile taking a step back and thinking a little bit about what this actually means. So, you know, finding the homology in a sense, especially over vector fields, it's really just a problem of finding a suitable basis, right? I want to find a suitable basis for the kernel and a suitable basis for the image, such that these are comparable in some way that I can kind of read off what the homology should be in some suitably easy. In some suitably easy algorithmic way. And, you know, the one thing that for whenever we have to deal with basis, especially for vector fields, right, the simplest thing that we know is Gaussian elimination. And a lot of the talk will be kind of centered around this. And it's kind of a surprisingly rich topic for something that's been around for so long. Now, again, just kind of as an example, it really depends on, you know, it's always worthwhile, again, You know, it's always worthwhile, again, taking a step back and asking what is it that we really want? Well, again, if I'm dealing with vector spaces, all vector spaces are really just defined by their rank. So if I want the rank, I don't really even need a compatible basis. I just need to know the rank of the kernel and the rank of the image. And I subtract the two, and the algebraic structure takes care of the rest. And I can just take this, and I know the rank. And, you know, I, it. Know the rank, and you know, if I'm being very glib about it, I can say I know this up to isomorphism, right? I know the vector space up to isomorphism. Now, clearly, this doesn't have kind of this isn't really always a satisfying answer. So we often want to do something a little bit more. Okay. And again, I'm going to kind of interchange this because if you come from a topological background, this notion of vector Background, this notion of vector space will probably be bothering you because topologists like integral homology. So, why am I using field coefficients or why am I doing kind of vector spaces when I could be doing just coefficients over integers? There's many reasons, most of them having to do with persistence that I'll mention. But there is something, just if I forget about persistence for a second. Forget about persistence for a second, and I just want to do kind of homology in one over the integers. Well, this again is a very well-studied problem, and people have done a lot of work on it. But, you know, essentially, it's the same procedure I just described, right? I find the basis for my kernel and a basis for my image. These two things are nicely behaved. They're freely generated, so I can write down a basis. And then, you know, now it's worthwhile thinking about what it means to actually write. While thinking about what it means to actually write down the quotient, right? And here we're getting into kind of how you represent things. And again, from a mathematical perspective, I can just write down a short exact sequence that's, again, more on the topology side rather than or a finite presentation, if you will. And, you know, this basically, these three, I have a basis and kind of a matrix between them. And I can say this is a Them and I can say this is a reasonable representation of the quotient module. Now, here, when I say representation, I don't mean it in the mathematical sense. I mean it in the implementational sense. I mean it as a data structure. It's, you know, this is a reasonable data structure for a quotient module. Okay. And really, you know, it's very redundant, right? Because in principle, all I really need is. In principle, all I really need is this one matrix here, and I can kind of tell you everything I need to know about homology. But you know, if I want to do more complicated things, or if I'm trying to figure out what it is that I want to do, you know, I may want to think about these things. And even algorithmically, it makes sense because from an analysis standpoint, dealing with three matrices is no harder than dealing with one. In practice, it matters, but kind of at least for the analysis. But kind of at least for the analysis, sometimes it can be easier. Okay, so just kind of as a very simple illustration, right? Let's say that I give you just two bases, right? So how do I actually find this M, right? So finding a basis is kind of easy. That's part of the input. But I think it's worthwhile just saying this in a few minutes. Saying this in a few minutes, right? So, what I can do is I can take this basis, I map this image, right? So, I take an element in the image, I map it back into my chain complex. So, I explicitly write down the chains, and I know that this is in the span of my kernel basis, right? That's kind of the whole point of this boundary mod, boundary composed boundaries equal to zero. So, that's again given in terms of these. Again, given in terms of these chains. And so I can just reduce this new vector in terms of that basis. And there's going to be some linear combination. And, you know, here's where I write down my linear combination. And I do this for all the basis elements. And I get m, right? I get this map between them. And then, you know, I'll probably do some reduction on this where I'm going to try to find some specific form. Now, the reason I kind of mentioned this again, I'm not saying. Reason I kind of mention this again, I'm not saying anything that's not terribly obvious, but number one, I like to think of these matrices as columns and vectors as columns, just to make sure there isn't any confusion. And I think it's worthwhile really thinking about how you represent these things, right? So, this is one way to do it. It's very explicit. Maybe it's not the most efficient way, but this kind of thought process, I think, at least for myself, has been very helpful in all. Myself has been very helpful in ultimately finding more efficient ways to do it. So I guess this would be kind of the lesson zero that I've learned over doing this. Okay. Now moving on to persistent homology, right? So rather now than one chain complex or one simplicial complex, I have a sequence of simplicial complexes that are increasing. Again, one can generalize this in all sorts of interesting ways, but again, But again, for the purposes of the algorithm, this gives you an increasing set of these chain groups. And the way to think about it, more importantly, it induces this increasing set of matrices that are the boundary operators, right? So you can write the final boundary operator. And if I've ordered things correctly, all of the previous boundary operators are just restrictions of this matrix. Okay, so this is a very useful picture to have. Useful picture to have. And what, you know, there's a lot more to it, but kind of algorithmically, what we're really asking for is some sort of consistent basis across these sequence of across the sequence of boundary matrices. Okay. So, you know, kind of more mathematically, you start off with the chain complexes, you get homology groups, you know, you apply a functor, you get the homology groups being. Get the homology groups. The inclusion maps become linear maps, right? So, this is these inclusions become linear maps. These are vector spaces. Again, all of this that I'm going to say now only really works over fields, right? And everything that I'm going to mention in terms of computation should be considered over finite fields. So, you know, but algebraically, what we're really dealing with is a persistence module. So, that's for each point. So that's for each point, I have a vector space, and then I want to encode all the maps between any two comparable vector spaces. So, you know, in most cases where we're computing, again, if for the more algebraically minded in the audience, you can think of this as monomials over a finite field, okay, in one variable. It's not Variable. It's not exactly equivalent, but for the purposes of algorithms, they can be thought of as almost one-to-one. Okay. Okay. And again, these things have been studied. You know, modules over fields like KT or ZPT have been studied, are very well studied objects, and it's known exactly how they behave. But, you know, But the nice part about this is that there's a nice decomposition of this. And actually, it's just the standard decomposition for things over PIDs, right, over modules over PIDs. It's a very standard kind of thing. But there is kind of a little bit of a subtlety here in that I think one of the nice observations was that, you know, it's not just a nice algebraic decomposition, but there's this nice combinatorial structure that's. This nice combinatorial structure that's hiding in there. Specifically, you have your boundaries and your cycles, or your, you know, again, relations and generators. And there's a nice matching between them in such a way that respects the filtration, right? So you have this sequence of things and you can find essentially the structure tells you that there exists this nice kind of consistent matching between these things that looks combinatorial. Okay, and this matching is really what we call the persistence diagram or the barcode, at least in the one-dimensional case or the one-parameter case for filtrations. I'll kind of say the more general things a little bit at the end. But we have this nice algebraic object that we know exists. We know that there's reasonable algorithms to compute it, very standard. But and why we would want to do that, I think, is why we care about persistent homology. Why we care about persistent homology is, I think, a very different talk from this one. So I'm not going to kind of say very much, but just kind of say that, you know, for let's just say for one reason or another, we care about computing this. And so we know there are standard algorithms for it, right? But why am I talking for 15 minutes about it, right? Or 45 minutes about it? Well, and why have we spent so much time? And why have we spent so much time in the field trying to make it so efficient? And I think it's worthwhile to kind of mention kind of the scales that have been achieved. So again, this isn't over everything. This is just kind of two examples that I've recently been involved with. So I've been studying kind of geometric random complexes. So what kind of topology arises from randomness? And, you know, we like to run experiments. And, you know, we like to run experiments to see if we're kind of in the ballpark. And, you know, there are things that pop up there that scale like log n over log log n. So this was this. So this is kind of this bit. And, you know, the input size, not even the simplicial complex, but the number of vertices just was here went up to about 50 to 100 million points. So it's really, really massive. Okay. And it took a lot of work to kind of get. Of work to kind of get experiments to run this way, and each point is an experiment, so a lot, really, a lot of computation. More recently, we've been looking at, again, kind of points that are sampled from different distributions. And again, we see phenomena. We need to do a lot of experiments to kind of make sure that these things are really there before we try to prove it or see how well it works in practice. You know, see how well it works in practice. We want to kind of run these experiments. So, we really did a lot of these things. And, you know, again, when you're talking about matrices that have sizes in the billions, you really care about efficiency and you really want to make sure things work the way they should. Now, one thing to say is that, you know, recently, again, part of the reason this isn't kind of a completely solved And this isn't kind of a completely solved problem, is you know, these very large instances require a certain set of optimizations. Where recently, you know, one of my students and various other people have started putting these persistent homology computations inside neural networks. And there, it's very different, right? The instances aren't that big, but you need to do them very, very fast because you're going to be repeating them many, many times as you're running the optimum. Many times as you're running the optimization. So, again, for the second thing, there's still very pertinent, open questions as to what the right optimizations are to really make this work. Okay. Okay, back to the algorithm. Okay, so again, we have this ordered boundary operator or matrix, and the algorithm is very simple. It's just Gaussian elimination, but it's Gaussian elimination with a slight twist. There's an order involved. Twist, there's an order involved, right? So the basic algorithm is I've ordered my simplices left to right, top to bottom, by time, right, by when they appear in the filtration. And essentially, I'm just, you know, instead of just doing the standard Gaussian elimination of finding a pivot and doing reductions and row and column operations, I'm just going to do column operations and I'm going to proceed left to right. Okay. And we're going To right. Okay. And what I'm going to do is I'm just going to find, you know, I'm going to reduce with respect to previous basis elements. And I'm just going to find the lowest entry, non-zero entry. And, you know, if there's a pivot, if I can get rid of that by using something before, I will. If not, I make this a pivot. So it's a very simple kind of thing. So the first column is just going to have a pivot. The second one maybe has a pivot. A pivot, the second one maybe has a pivot, the third one, but maybe the fourth one, right, has also a non-zero entry here. So I'm going to add this to this one to get to make this thing zero, and then repeat, right? And either I will find a pivot at some point, or I will zero out the entire column. And in which case, it's a cycle and I just move on. Okay. And the surprising thing or The surprising thing, or maybe not surprising, but the thing that takes a little bit of insight is that these pivots, if I choose this lowest one, I get exactly this, the matching that I would expect or the matching that I need to get from this kind of algebraic structure. Okay, so this is already kind of much more efficient, right? I just have one matrix, I'm not doing row operations and column operations. Row operations and column operations and just doing column operations. So already here, this was, you know, again, almost 20 years ago that this was implemented. And, you know, it still works pretty well, to be honest. And now, what's the computational complexity of this? Well, the space complexity is quadratic, right? The matrix can fill up. And by the way, n here just means the number of simplices. And then the running time is cubic because. And then the running time is cubic because this is the standard thing for Gauss elimination. How do you see this? Very simply. I add two vectors. It's order of n, right? The vectors are of length n. Each vector might need O of n reductions, right? I might need to add n order n vectors to that vector. And then there's n total. So I multiply the three and I get a cubic fact. Okay. Okay, so the first thing to notice is that the cubic bound, by the way, can be achieved, but often isn't. And this is an example of an output-sensitive algorithm, right? Which means that if I don't have very complicated, if my answer isn't very complicated, it won't take very long to compute. And this is just kind of a basic thing that was, you know, that was observed in 2002. two um and and basically you know in this original paper where they introduced this algorithm uh they already have this they already have this kind of bound in there although granted it is it is buried within the paper okay so you know as i said before these persistence modules are you know quote unquote equivalent you know for the purposes of this talk they are equivalent um and And we're doing something that's very kind of standard and that's well understood. As I promised before, right, if I'm doing things over integers, there's two problems. Number one, this matching doesn't really exist in the same way. It's slightly more complicated. And the other part of it is there's another bit that if I'm working over integers, each coefficient, the amount of memory I need for each The amount of memory I need for each coefficient might grow kind of a lot. And this does become a problem when you're doing homology over the integers. Okay, so here we're going to get into these things. So really the first point, can we do any better than cubic, right? So another very kind of classical thing from algorithms from, I think, late 70s, early 80s was the Gaussian limit. Early 80s was that Gaussian elimination is essentially equivalent to matrix multiplication, and we know that matrix multiplication can be done in sub-cubic time, right? So, you know, if you want to be practical, use Strassen. If you want the greatest, there's a, you know, the exponent is smaller, but this is, again, if people aren't aware of this terminology, it's something called a galactic algorithm, which means that the input size needs to be roughly on the order of a galaxy in order for it to be better. Galaxy, in order for it to be better. So it's kind of more of theoretical interest than actual practical interest. Okay, so, you know, okay, well, Gaussian elimination looked very similar to this. So why can't we use this? Well, if you think about how this works, it's not exactly clear, right? Because here we need to process things left to right, or things don't really work. Don't really work. So, for a long time, it wasn't really clear how to get around the fact that we have this prescribed order, right? And again, if you want to think of this more algebraically, you can think of it as how do I apply Strassen when I don't have really field coefficients, but I have monomial coefficients, right? So, I'm about to answer that question, but this. You know, this wasn't an obvious thing. This took about a decade. Okay, so really, this was a paper with Nikola Milosalovich and Dmitry Morozov in 2011. I have to give credit because this insight was really Nikola's insight. I mean, he's the one who really kind of figured out the key idea. I was just kind of lucky enough to be there for this. Of lucky enough to be there for this. So the idea is really rather than kind of doing reductions one by one, we're going to batch them in some smart way. And there's a nice observation, right? So let's say that, you know, again, because we're dealing with algorithms, we need these kind of little subroutines. So if I want to multiply an n by k in a k by k matrix, right? So I have this very nice drawing. Well, how long does that take? Well, I can see. Does that take? Well, I can split this up into little k by k pieces, right? Let's say there's n divided by k, and let's say everything is divisible. You know, one can make this work. And, you know, you just multiply each one separately, right? And then you kind of concatenate them together. So, how long does this take? Well, there's n divided by k of them, and each one takes k of matrix multiple. takes k of matrix multiplication time, right? So if I multiply these two out, I get n times of k omega minus one. Okay, so well that's nice, but it's not clear why this is useful. Okay, so here's the algorithm. And rather than kind of give this the pseudocode, I just want to run through the first four steps of it because I think that's more insightful. So Insightful. So, you know, I again, I look at the first column, it's reduced. So I can just read off whatever the pivot is, right? Okay. And in fact, whenever I have a reduced column, the insight is that I can just read off whatever the pivot is. I just look for the lowest non-zero entry, and that's it. Okay, so now what I can do is for the second column, I can, you know, I know where my pivot is. I take one step over. And if this is non-zero, I'm going to apply this one to this one. I'm going to basically. This one, right? I'm going to basically zero this out. I don't have to, it's maybe more work than I need to do, but I'm going to do it nonetheless, right? So, this is going to be O v. In the worst case, I might need to do it anyway, so I might as well. Okay, so now I've done this, but and now the second column is reduced. I can read off the lowest entry. Now, what? Right? Now, the third column, and this is really where the insight is. Rather than do the first and the second. Than do the first and the second one to just the third column, I'm going to apply both to both the third and the fourth column at the same time, right? So, you know, let's say I have these two pivots here. I have to look at these four entries, right? And I'm going to permute the rows, kind of not actually, but kind of conceptually, because it's much easier to picture it this way, right? So I push these two down here. I push, make these four this B, and then everything else I leave here. And then everything else I leave here as DNA, right? Okay, so now what do I want to do? Well, I want to try to zero this out, right? I want to zero these four things out using these two. Well, what I can do, right, I can just multiply this thing, which is just identity times b, and subtract it. But again, I'm just doing column operations. So, you know, I multiply this d by b the same way I did here and subtract it from. And subtract it from A. And I've done this is exactly doing column operations. Okay, if it's not obvious, you know, go home and think about it for a little bit. It's, you know, if I can figure it out, it really can't be that complicated. Okay, so now when I've done this, well, I've applied the first and the second column to the third column, so the third column is reduced, and I can just read off the third column, right? And then, right. Right, and then right, I'm going to apply the third column to the fourth column and kind of go this way. So the general pattern is: I apply the first to the second column, then I apply the first two to the third and fourth, then the third to the fourth, and then you know the first four to the next four, and you know, and go onwards. Okay, so this is a very standard algorithms problem of what the complexity of this is. This is. And the most costly part of this, and actually, the total cost, you can kind of show is that it's really dominated by this last bit where you take the half of the matrix and apply it to the second half. Okay. And what's the cost of that? Well, n over 2 omega minus 1, and that's just the order of n to the omega. And that's really how you get matrix multiplication time. Multiplication time. Now, this isn't the algorithm that's used, but this idea of zeroing out things that are to come actually is used in algorithms, although it's implemented in a slightly different way. So, you know, it turns out that even though we're doing more work than necessary, doing it kind of upfront tends to actually be better. Okay, so that's kind of the first bit. The second bit is: well, you know, this is again. Well, this is again a special, this is the worst case, kind of a general bound, but can we do better for special cases? And an important one is whenever we can divide and conquer, right? So this is, again, a very standard thing. But there's a very nice class of problems that I think are more very general, which is when matrices can be represented by graphs. So you take each column, say, and you make a vertex. You make a vertex. And then, if two things kind of interact, right, if they have rows that are non-zero, then you make an edge between them. This is one way to represent a matrix into a graph. And if this graph has some nice properties, specifically if it has something called a graph separator, then you can do something that's better. Okay, so I'm not going to go through kind of the detailed thing of the definition, but the point of it is that you can separate. But the point of it is that you can separate this graph into three pieces that are kind of two big pieces and a small piece in the middle that if you remove it, then it's disjoint, right? So here's kind of, I'm going to skip forward a little bit to the example, right? So the typical example of this is when you have something that's a planar graph, then you get kind of very small intersections, right? Intersections, right? If you remove square root n, and I did the diagonal here, I could have done this or this. If you remove this, you end up with two big equal size pieces. And it's not just that you get this once. For any kind of subset of this thing, for any subset of planar graphs, you can always kind of subdivide it this way, which means you can recurse. And once you can do this recursion, that means that you can improve the running time, right? You can improve the running time, right? So, again, this is not a new idea. This was first suggested by Lipton and Tarjan. And again, we adapted it to this persistent setting, and there's quite a few little details to deal with. But this underlying idea of finding settings that fall into this is really helpful. Okay, so again, kind of the key bit. Know again, kind of the key bit here is that, you know, okay, we had to show that graph separators lift to simplicial complexes. This actually, these constructions of separators often bring in a lot of geometry, which is something that's, I think, always useful. And the key bit here is that what this tells you is that I can easily find a good ordering for doing reductions. And more importantly, this is kind of a result that implicitly gives bounds. Of a result that implicitly gives bounds on how much memory I'm using. Okay, so if you wanted to find kind of an optimal ordering, this is known since the 70s to be, you know, ever since MP-hard was defined. It's not one of the original problems, but it didn't come soon after that this is also very, very hard to do. So we have heuristics, but you generally don't want to try to do things optimal here. Okay. But here we're going to get to kind of the last bit. We're going to get to kind of the last bit that I want to get to, right? And this is the memory usage. So, you know, all of those things have theory associated with them that were useful in practice, but now we're really getting up to kind of the practical aspect of it. So, you know, when you're scaling up computation, we always think of running time, or at least, you know, I do by my training that that's kind of the first thing I think about. But realistically, if you know, if you talk to people that really People that really do this for a living, right? That kind of code for Google or real software engineers, they know that it's not really about processing power anymore. It's, you know, the ability to use fast memory or kind of these small amounts of memory that are very close to the chip that can be accessed very quickly. And the more you use that, the faster you're going to be. If you're not using that, all of the processing power you have is essentially going to. Processing power you have is essentially going to be wasted, right? Because the processor is going to be sitting there. Okay, so the goal is then to reduce memory usage. And again, the first kind of example of this is to use co-homology versus homology. This is, again, a bit of a strange beast that I don't want to kind of really get into. But the core of it is, you know, ultimately you're doing reduction. You know, ultimately, you're doing reductions on just the transpose of the matrix. And the reason it tends to work faster than homology is really that you just end up using more. It's not really that you're doing less work. It's just that you end up using less memory. You have to keep track of things at one time. At least on average, you have to keep less things in memory over time. So, again, the idea of using duals for computing things. Using duals for computing things, I think is very, very general. So I'm really not, don't want to kind of say too much on it. There's a lot of other things that are specific to homology that people have done to minimize memory usage. Again, I'm not going to go through them because I don't think, you know, they're very interesting, but they are very specific to homology and persistent homology rather than more general algebraic computations. Now, the last bit that I want Now, the last bit that I want to get to is this, right? So, okay, one of the most efficient packages to compute persistence is this thing called RIPSR, which was written by Uli Bauer. And it's a very nice piece of software. There's a lot of optimizations. It does something very specific, but it does it very well. Now, I speak to Uli relatively often, and there's one thing that he put in there that really made no sense. He put in there that really made no sense to me, and I think it's very, very surprising. And I think it's something that everyone should know about. Okay. And it's essentially a memory saving technique that I think is generally applicable. So what I'm going to call this is persistence in linear space. So, okay, the first thing to notice is that if we're not using the full symptom, you know, generally the boundary matrix when you start with it is very, very sparse, right? With it is very, very sparse, right? Because each vector has at most, you know, if I can limit the dimension of my simplic shell complex, I can limit the number of non-zero entries by constant in each vector. And the problem isn't really the input, it's that it's going to fill up as I do reductions. Okay, and you know, realistically, one reason why we like simplicial complexes is because we don't really even need to store this matrix, it's generating this vector. You know, generating this vector is not something that's terribly difficult. Okay, so how am I going to do this? Well, you know, here's my picture of a reduced matrix, right? I have all of my nice pivots here. And the insight that Uli had was, well, you know, I'm not going to even store these bases. I'm just going to store the pivots. I'm going to remember that the first column has a pivot here, the second one has one here, and these. I'm just going to remember these places. This is. These places that this is clearly linear, right? Each for each column, I remember one number where the pivot is, or that there is no pivot, and that's it. Okay, so I'm just storing this position. And okay, but how do I do this? Well, you know, whenever I need to use a pivot, I'm not even going to, I'm not going to add the reduced thing. I'm going to just add the vector as is. Okay. Now, this means that the pivot. Now, this means that the pivot can go up and down, but implicitly, what's going on is I'm recomputing the reduced basis vector each time, right? So, you know, if all of these, if this one had this and this, I'm going to first have to add all of these things again to this one and then all of these to this one. Okay. So I'm essentially recomputing the same basis elements many, many, many times. Okay. Many, many times. Okay, and this is actually O n to the fourth, which is terrible, but in practice, much, much faster than actually doing it the other way, right? This recomputation ends up being faster in practice than kind of doing the same thing, which is to store the things that you've already computed. And if you don't remember anything else from this talk, remember that, because I think that's kind of a very Because I think that's kind of a very surprising practical bit. Okay, and why does this work, right? Well, many basis vectors, like to recompute them is very, very quick. And it's actually quicker to recompute than it is to bring it up from the slow memory. That's kind of the insight. And if you talk to people who are doing visualizations, for example, very large data sets, they will tell you the same thing. Sets, they will tell you the same thing in that, you know, most of the compute, most of the things that you compute, you actually have to throw away, you don't store because there's just too much of it. And, you know, I think this is an important insight that's very non-intuitive, but it is very important as we're trying to get kind of things to be computed in practice, right? So I guess the, you know, the takeaway would be: it's fine if you do more work. It's fine if you do more work as long as you do it kind of in the right place. Okay, so the other kind of aspect of it that I'm not going to go into is there are a whole range of approximation algorithms, which from an algebraic standpoint raise interesting questions in what does it mean to even be a reasonable approximation of an algebraic structure. You know, in persistent homology, we have a very Persistent homology, we have a very clear indication or kind of some idea of what that would look like, but I think it's in general a very interesting question for kind of more general algebraic structures. I think there are very, very efficient random and approximate algorithms that computer scientists have developed. And I honestly, I haven't seen very much of it kind of permeating. Very much of it kind of permeates into more algebraic computations. Or even the symbolic community, really in general, hasn't kind of taken it up. And for good reason, because it's not at all clear what it means to approximate a symbolic answer. But I think it is something that one should keep in mind and try to kind of think about. Okay. So, in the last five minutes, I just, you know, hopefully you. I just, you know, hopefully you will have at least remembered that last bit. But, you know, what is the current direction? Well, we've been doing one parameter things. There's still questions that we're trying to answer there, but more often than not, we can take it as a kind of solved problem. So the next question is: what happens if I don't have a filtration, but I have a multi-filtration? This obviously is a more This obviously is a more complex. This leads to a more complicated algebraic object that doesn't have all of this nice structure. And, you know, there are a lot of, there's a good number of people who have implemented a lot of things with respect to this. I am not one of them. But, you know, all of these things, I think, are at this point unsatisfactory. And here's kind of the reasons why. Here's kind of the reasons why. And I think this is something that this community will appreciate. So, you know, the standard approach is: well, you know, I have a module, an algebraic module, so I would want to compute its decomposition, right? Everything's finite dimensional, so I know these things exist. Or maybe I want something that's much less, right? Like supports of these decompositions or some other related construction. Some other related construction. But actually, it turns out that these things end up being very, very expensive to compute, as I'm sure many of you know. And the problem is it's not clear whether the things that we want to use them for in the persistence community, whether this is all really necessary, right? Whether the decomposition of a module, whether the basis, this canonical basis that we know exists. Basis that we know exists, whether it's really the thing we want, right? And it's really not clear, right? So, you know, whereas in one-dimensional, everything, you know, in this one parameter case, everything kind of came together very nicely. Here, we were really as a community asking ourselves, you know, what is it useful, you know, what would be useful to be able to compute, but that we can't, right? What would be really useful? What will be really useful to scale? Where should we concentrate our efforts? And again, it's not completely clear. So, you know, there are alternate approaches to this. So the simplest one is if I want to, rather than do homology or some very complicated thing, I just compute a function or a functional if you want, right? The Euler characteristic is the simplest one if I'm doing things topologically. I had a student who implemented this, you know, that was basically looking. You know, that was basically looking at distributions of topological structures, if you will. And, you know, for the most part, if things come from distributions, the Euler characteristic is going to capture a lot of the behavior that you want. So, you know, in these cases, again, maybe even homology over vector fields is too much, right? Is more work than we really need to do. You know, the alternative thing and the thing that I've kind of come around in this. And the thing that I've kind of come around, and this is the last thing I'm going to say, is really, you know, rather than focus on the algebraic decomposition, we've kind of gone back and thought about other ways of defining this barcode or persistence diagram that's not necessarily algebraic, that's depending on, that's dependent on more kind of a functional approach to things. And again, we don't know what the answer is, but essentially we're building. But essentially, we're building kind of non-complete invariants of a module, and we're seeing whether that fits our whether that fits our needs or not, whether it's kind of a strong enough invariant that's computable that can actually say something that we want, right? And again, this is very much an ongoing discussion, but these alternate characterizations and these different viewpoints. Characterizations and these different viewpoints have led already to more efficient approaches to even doing kind of things that we would want. And as always, these kinds of things always raise interesting questions. So I'm hoping that some of this was useful to people. But essentially, we're dealing with very simple algebraic objects, but we've matched. Algebraic objects, but we've managed to, I think, again, by we, I'm talking about the community, has managed to really make the computations exceedingly efficient, right? And there's, I think, you know, once we know what it is that we want, I think we can really kind of scale things in very surprising ways. I think, you know, all of these things have interesting algorithmic aspects, this notion of Aspects: this notion of space versus running time again is ever changing as our computer systems change. You know, we don't always need a theorem. A lot of times, very good heuristics are even better. And again, every once in a while, if we hit a wall, it's helpful to take a step back and really think about what it is that we're trying to compute. At least, you know, this is something that, at least from my This is something that, at least from my point of view, is something that I really think is a useful bit. Okay. And with that, I'm going to finish and let everyone have coffee. And thank you. Yeah, thank you for the talk. So are there questions? Yes. Hi, uh, he can't see me. I am somewhere. I go here. Okay, okay. I where is the camera? Somebody helping. Okay, I can see you. Okay, no, it doesn't matter that you see me. I just wanted to sort of not be the ghost in the room. So I really appreciated the one thing that you said. Well, many things in this talk, but I wanted to point you to a couple of things that maybe we should connect over afterwards. Over afterwards, but I want to take the opportunity to tell you now. You mentioned how the need for randomization and approximation, which I 100% agree with, and it's been very successful in many fields, but somehow in symbolic algebra, we're like lagging behind. So I just ran a BERS workshop in Banff on random algebraic geometry, and there were several takeaways. And a couple of them were: there is a community trying to do randomized computation in. Do randomized computation in, we call it algebraic geometry, but I'm going to call it commutative algebra today. Okay. And one of the important aspects is part of it is approximation is doing numerical algorithms with these randomized homotopies. And there's not a good notion of what random means in that sense. So part of the reason we're behind the rest of mathematics or computer science or whatever you want is because there are no good notions of what it means to randomize appropriately to get the correct symbolic. To get the correct symbolic answer, if you will. But perhaps we should connect over something that I worked with Jesus de Lera a few years back called Helley numbers and violator spaces. So we imported LP-type algorithms that are randomized into an exact symbolic computation. And so I'd love to hear your thoughts about that, but perhaps we'll talk later. I don't know, clearly not here because you're online, but we should make sure to connect. We should make sure to connect. I wanted to say that. So, thank you. No, I mean, I think that that's, you know, that's a very, you know, I can't really disagree with anything that you've said or really add any significant bit on it. I mean, the notions of randomization within algorithms, I think, is very important. But, you know, again, you know, I think randomization is in some sense the easier, the easier. The easier, the easier question to answer, right? Because all of the optimization tools are, you know, there are kind of very interesting and hard questions there, but I think the approximation, right? When is a symbolic answer approximately right? That's, you know, that's a much trickier question to even set up. But absolutely, you know, please, please do email me and we can set up a time to talk because it does sound, you know. Does sound, you know, everything you've said sounds kind of very much in line with things that I've been thinking about, if not made, though not making very much progress on. That's great. I also had a question. So when looking at your algorithm at this kind of Gaussian elimination variant, I was thinking of, well, we like Grubner bases, right? And there is this Buchberger algorithm. And there is this Buchberger algorithm, which in the linear case reduces to Gaussian elimination. So I was wondering if there was a way to interpret your algorithm in terms of like elimination orderings, like block orderings when you're doing these several blocks. Absolutely. I mean, if you run this on monomials, like as I said, over you know, over one variable, you're going to get exactly the same answer. This is kind of specific. People, you know, not, I have. I haven't worked on this, and so I honestly find that algorithm rather intimidating. But people in the field have done a lot of work on kind of optimizing the Grogner basis or Grogner. Let me not say Grogner basis, but Grogner basis type algorithms to these kinds of more, let's say, persistent setting where they're trying. Persistent setting where they're trying to do something kind of, you know, that's something that's very sparse. The input is very sparse, but it's quite specific, right? So I think the main name to look up is Tamal Day. I think he's done the most work on these things. But if not, by all means, send me an email and I will be glad to give people references if anyone's interested. Okay, thank you. Okay, thank you. And more questions? Also, online, maybe? I have Ines, go ahead. I was actually going to say that I actually was going to say something related to something that Ines, who's my PhD student, and I were thinking about. So probably Ines, beat me to it. She might have the same question since we're, you know. I don't know. We'll say. Okay, go ahead, Ines. Okay. So I guess, I don't know, maybe, maybe it's not. I don't know, maybe, maybe this is not true, but like from Hinjon Sog, I got this impression that understanding very well how they're in one parameter persistent, how this relation in between how you're matching your relations and your generators and how that relates to the barcode decomposition was very helpful to have a good algorithm to compute barcodes. So, I was just wondering what do you think about this? If we are missing something similar in multi-parameter persistence, if we are missing something Multi-parameter persistence: if we're missing a good understanding, yeah, sorry. So, I don't think we're missing a good understanding. I think the problem is, is just that the, you know, when I mention this output sensitivity, it's just that the answers in practice tend to be very complicated, right? And what we don't have, and when I say we don't know what to compute, I think what ultimately that boils down to is we don't. Boils down to is we don't know how to represent the answer in a nice way, in a nice compact way, and so you know, because it doesn't have a nice compact representation, if nothing else, you're going to have to spend a lot of time writing it out. And this is the problem. It's just that, you know, again, if you want the complete answer, I think it, you know, even writing it down is quite complicated. So the trick that at least to me, That at least to me, and again, this is possibly a religious answer because people have different viewpoints. But I think the point is at what level of, on one hand, we have the complete, let's say, Grobner basis, and on the other hand, you have just the Euler characteristic. Now, in what sense, where's the sweet spot in which we maintain a lot of the algebraic structure, but you know? But you know, don't run into this horrible complexity of the output. Yeah, I see. I understand. Yep. Thank you. Anthea? Yeah, I mean, it was kind of related to that, but also, I mean, Ines and I, together for her PhD, are sort of trying to explore like computational aspects of multi-parameter persistence. And so this is actually quite new to me as well, because I've mostly been working in single-parameter. Mostly been working in single parameter. But back to sort of the theme before we were talking about randomization and algorithms. Do you know of any sort of like, you know, specific restrictions in implementing randomization in computing multi-parameter persistence? Or like, is it just something that hasn't been yet because it's still relatively new? Or is it just because there's some kind of like hard wall that we're running into? So the algorithm of Kerber and The algorithm of Kerber and Chen would extend to multi-parameter without any significant problem. Again, the problem is not applying these methods to multi-parameter persistence modules. The problem is what is it that you want to compute, right? If I actually want the basis or the NDC composables, you know, again, even if your input is random, right? So forget about the algorithm, but just. Random, right? So forget about the algorithm, but just if your input is random, there's plenty of experiments that show that, you know, things aren't going to look simple. But maybe that's okay. And maybe we don't care. And maybe we just want, again, some notion of approximation, right? That will give us a good enough answer. But again, I think the problem is that people haven't focused enough on what is. Focused enough on what is, you know, with the barcode, we knew what we wanted. I think for multi-parameter, the problem is more that we don't know what we want. Yeah, yeah. So, yeah. I mean, in S and I are like looking at rank functions specifically, but rather is sort of like, you know, the, you know, the sort of manifestation of the rank functions as the generalized persistence diagram from like, you know, Emit's work. So this is sort of the thing that we're kind of after, but kind of looking at more kind of Of looking at more kind of probabilistic slash statistical slash randomized perspectives to that is something that I think is kind of foundational if we want to look at these things. And yeah, so this is something Ines and I are interested in thinking about it. We should talk on Tuesday. Yeah, yeah. Okay, so I would say let's thank pretty much again. And we'll have a coffee break. And we'll have a coffee break, and then we'll be back for a moderated discussion at 11:30.