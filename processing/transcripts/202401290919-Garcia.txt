Is an associate professor in the Department of Biostatistics and Bioinformatics at Duke University. He's also affiliated with the Duke Clinical Research Institute and the Consortium for the Holistic Assessment of Risk and Transfer, or TART. He holds a master's degree and a PhD in biostatistics from Harvard University. He also completed a two-year postdoctoral training in causal inference at Harvard. His methodological research focuses on non-parametric, semi-parametric, and causal inference methods for comparative effectiveness. Methods for comparative effectiveness studies, clinical trials affected by non-compliance, not-so-perfect experiments, and observational studies. And his overall goal is to develop statistical methods that make the best use of the data collected to answer scientific questions while applying principled methods to minimize bias and ensure fair assessments. Please welcome our first speaker. We started with a puzzle, so I didn't want to get stuck with this. And thank you, Tanya. Thank you to Tanya and all the organizers to have me here. I've known Tanya for quite some time, so I was surprised when she invited me. I thought she forgot about me. That's been very sensitive for all those years. But I've worked on sensor code. On essential coverage with my colleague back then at Harvard before if I was never UTUC. So, what I'm going to talk about really is going to be how to handle sensor coverage. I'm going to use mostly white sensoring regression models and the methods I'm going to propose this evils by waiting before I show some simulations and the GitHub application. So, if you're really interested in this. If you're really interested in these methods, there are a few papers that are publicly available. I'm going to show three of our papers, and the last one myself was in the room, so if you have questions, you can ask and Dania or the team. It's really a good review of the methods that are currently available on this topic. So, I work in cardiovascular disease, and so the main study we used for our paper was the French. For our paper, was the Firmingham study. You know, Firmingham Study, we're way back, Framingham, just not far from Boston. They collected data not only on the original code, but they also have data on the offspring, they added minorities, and they started enrolling grandchildren in 2022. And so it's a very rich galaxy. And every two, four years or so. Every four years or so, they are updated to health status of those participants and potential risk factors. So we use that actually to look at onset of cardiovascular diseases on the children of the original cohort. As you know, cardiovascular disease, and I've been working on this disease for quite some time since I came at Duke. And so it's very complex, and this is. So it's very complex, and I think this is sort of like some of the words we hear here, but it's more than that. And it's a latent cause of death, as you know. And there are really a lot of risk factors, among which are family history. So for this study, we wanted to look at the blood level of LDL and as a strong predictor of CDCD. And the goal was to see whether the age of unsettled parents for cardiovascular disease can. Cardiomascular disease can be associated with LGBT orphanosophyllum data. So, why censored data? Because not all parents at the time were going to conduct the investigation to have C V D, and we're not going to consider it as missing data because some of them actually had already C V D, some of them are going to have C V D, but some of them may not have C V D. So that's the So that's the premise that we're going to work with. And we're going to consider censored covert, not because they're missing, but that's just we have incomplete information. We know actually until the time of the study they did or didn't have, the parent didn't have C V D. So COVID sense, sensor COVID are mostly used in the limit of detection. That's where mostly That's where the most literature found before we start working on this. But in our scenario, we're going to use random sensory. So, when you don't account for sensory covert appropriately, it affects your type 1 error. Mostly is going to have a huge impact on your power and also going to lose efficiency. So, there is actually a good reason to think about these methods and. Methods and now, with the papers I presented in the beginning, you I think have a good idea of where to look for what to read if you're interested in these methods. So, for quite some time, people were using complete case analysis. And the complete case analysis is just okay. These people in my study, I don't know whether they have the predictor of interest. So, I'm willing to draw them and consider those who have a predictor of interest. So, it works. So it works sometimes. It works when you have a large sample size, just a small number of people without the predictor. And actually the most important part is if you think that the sensoring mechanism is independent of patients factor, which is now something you can think of most of the time, and if also the sensoring is independent of the outcome of interest, then you're going to have problems. Then you're going to have problems even when you have some of these conditions where you have a large number of sensory observations, right? Because heavy sensor observations are going to lose, you're going to lose power. You're going to even lose information of other coverts because you delay participants. So you're going to lose information on other coverts before you measure it. And you also lose information of the outcome of interest. Right? And you really get. Right, and you really get into serious problems and bias actually when the sensoring is very informative. So there have been also other methods actually people who have used where you assume maybe a certain value you're going to include in lieu of the sensor observations. It has the advantage of using Intel data set, so at least for the covert it did measure. For the covert indeed measure, you're not losing them, you're not losing information on them. And so, what people have done is either they replace the sensor observations by the mean, the median, or some constants. And some people, instead of using the covert as a continuous covert, for instance, for our scenario here, you just dichotomize the covert or use a specific. Cover it or use a specific threshold to dichotomize. So I read this actually when I was reading these methods. This is for limit of detection. So for this author, they consider it just making up data when you sort of impute values based on the mean, the median. I know we do this very often for missing data, and even in missing data, people don't really like. People don't really like doing simple imputation, right? So, most of the time, we feel like doing multiple imputation, and all the algorithm Rubin's approach is the way to go. So, here also, you don't just need to include any number to call it quit, you need to be more principled than that. So, there have been a lot of alternatives. You can use parametric. You can use parametric methods if you happen to believe the distribution of parametric assumptions you're making. And the only thing with parametric metrics, it becomes questionable when the data set is small because you don't really have a way to justify that the the the the distribution fits the data very well. You can use semi-parametric methods to make it less stringent. Methods to make it less stringent, and we also use non-parametric methods. For non-parametric methods, we have published papers on conditional sample and multiple imputation. And so most of these methods are in the literature, they are for tap on sensoring, when you know exactly you follow for your sign duration and you know exactly when you go you go into sunset and observations. You're going to sunset the observations. We also, there is also reversive variable regression. That is, if you want to assess whether there is an effect of the COVID, the sensitive covert on the outcome. And it works very well, especially if you have a continuous binary data, but it may not provide you something to interpret. Yes. Clarified, is a tough one. Clarify, is the type one sensor maybe is uh the corral is is uh under detection? Uh is below it that's one example, yes. Okay, okay to exchange. I wanted to let Sarah speak on it. Uh you have type one, type two, and you have interval sensor. Sensor. So, type one is when you have sort of like an example limit, the detection limit. Type two, which mostly we use is if I have analysis. Accessing or accessing void. Yes. The same idea, but instead of being on the outcome, you're thinking about it and recover it. Also, have two sensoring. Type two sensoring? Yeah. It's when it's when I'm sensoring that, you don't have the same follow-up time, you don't have. Don't have the same follow-up time, you don't have the same direction, but people are being sensored at different times. And in terms of sensoring, as I say, it can be left and right, but they are in the middle. Other questions before we continue? Yes. So, are there two ways that we're classifying the types of sensoring? There's like random or fixed and then left versus right versus interval? Yeah, yeah. Yeah, yeah, you're right. So maybe you can clarify what do you mean by fixed sensory? Fixed might be like with limited type of detection. There's a fixed cutoff value below which you can't measure. Okay. But that could be left or right. You can't be left or right, that's what I guess. Type one, random versus fixed or left versus right. That's a good one. I think it's random versus fixed. The classical examples I was The the classical examples I always think of for type two, isn't it? It's like my study's gonna go on for an indeterminate amount of time until I get 50 cases of the disease. So it's like at outset, you have no way of knowing when people are gonna be censored versus type one is like I have funding for five years. I'm going to watch everyone and they all get censored on the same thing five years from now. So I think we're all saying that helps. And then random censoring means. Okay, that's better. How do these methods perform with like multiple sensor covariates? Oh yeah, you can extend it to multiple sensor covariates. Or like even the reverse survival where you look for that? The reverse survival, I haven't worked on it. I mean I never worked on it. Ajay, you publish a paper on this, maybe you can see something. Technically we should, but it's more involved, right? Even like for my people working in causal inference, we just use the two treatment groups and make it simple, but in practice we may have more than that and it's going to require more thinking to make the I think it's possible. I think it's possible. But I never worked on it. I just listened to that. Those are the people, the methods that exist out there. Even on this list, I didn't talk about the Bayesian approach, but there is a Bayesian approach to this. Questions? So, for my case, I use inverse probability wearing and inverse probability in the context of GLM. So, the GLM is pretty straightforward, and I don't want to. And I don't want to waste your time to remind you this, but the GLM is also an integral generalized estimating equation. Once you can solve the generalized estimating equation, you can solve for beta. And beta has all the nice properties we have. We have software packages for that. I mean, this is what we do every day, where you have linear regression, logistics regression, and Poisson regression for these models. You've been familiar with quite some time. Familiar for quite some time. But when you have a sensor covariant, actually, you may actually break down into two parts. A part which you're based on the observations you see, and the part, yes. Yeah, I didn't have a chance to absorb the implication. Well just okay, so it's just a GLR data set for questions. Can you give us an example about YI in your exam, in your application? YI is? Oh, YI can be a continuous outcome. Okay. In that case, you use G is going to be the identity function and you use a linear regression model. Yes, so in the CBD applications, what's the YI? CBD applications, what's the YI? The CBD application. In our application, we use log LDL. That was our outcome of interest. And we were interested actually in an XS, which was essentially covariant here, which is the age of answer. And Z here was any other covariant we want to include. I'm not familiar with part. I'm not familiar with cardiovascular disease, so log LDL, what is that a measure of what is LDL standard? But LDL, I think. Sorry if I'm saying if you're a doctor in the room, don't believe my words. I say crazy things. But I think. But I think that's what we call bad calls. For those that don't know, LB, are you not old enough? What are the standard timing input? I see, man. Thank you. Most of the commercial you see on TV about Coruso is bad. Oh, that's what it is. Okay. So, are we good here with the GLM? Yeah. Yeah, so when then. Yeah, so when then, obviously, when you have a sensory, so your U is breaks down into two pieces, and you don't have all the nice properties you know about the DLM. So, you need to find a way to analyze the data. So, if you do the complete case analysis, you have to restrict the data and the subgroup of people observed the covering. And as I was saying, it may be consistent primarily, and it may have. Inconsistent parameters and may have a problem with your analysis. So, the way around it is to use the probability of being observed given the outcome and all the outcome. So, I'm using H here instead of Z because H, you can include not only Z, but you can also include auxiliary variables. The variables that may help you explain the sensory mechanism, which is not always the variable you have internally. The variable you have intended for your outcome, the operational outcome model. Your interest is just about the regression model of one given x and C. Does the CC analysis give you a consistent SML? It gives you a consistent estimate, as I was saying here. Let me go back here. Under these two conditions, that you have assumption that's independent of the risk. A sensor that's independent of the risk factors, which we call independent censoring, and it doesn't also depend on the outcome of interest. Which also determines the limit of detection to one form of independent sensory. But in many scenarios, especially in medicine, you're more likely to have a sensoring that depends on the coverage of interest. Depends on the covariate of interest, or at least the patient's use factors. Thank you. And it should be independent of the other covariates? Not really other coverts. The most important factor here is your covert of interest. Because you have a covert of interest, for instance, age onset of C V D, then if that age is independent of getting. Is independent of getting CBD for the father, the parents is independent of any other measures, any other factors they may have, or the kids may have in that case because it's independent, also independent of the kids getting a higher LDL. It should be completely independent of all the predictors you may think of. I think it's also as long as your X, your central covariate, is independent of your. Your sensor covariant is independent of your outcome Y, then it will need to consistent estimate. But the interest will have to be the regression model itself. If the interest is just a y data xz model, oh, you mean with his weights? I'm saying he's asking if you are not interested. He's asking if you are not interested in estimating those parameters. Yeah. Esimating something else. Okay. Like the median of the Y or things like that. Yeah, then it's okay. Yeah. Okay. Yeah, they're interested in Y given C and X. And then so what we're going here is with the premise that X can be censored. And if X is censored, which it sort of becomes a selection bias problem, your selection bias is not going to be a selection bias. Selection bias is not going to be a selection bias if the reason those are missing are totally independent of everything that's going on in your data. And which is sort of the reason you can use a complete case analysis. And the complete case analysis, even in those situations, you have to have a good sample size so that you're not losing data. Because when you're losing data, you will be losing efficiency. So even under this scenario, Under this scenario, it's always helpful to use other methods because you can improve your efficiency. Answering covariates is in the age. The age, yes. Okay. And by the age, because you're taking the kids up the the off thing of of those first courts, you're looking at LDL. You're looking at LDL, and you want to say whether the age the parent had the first C V D can be a good measure to predict the LDL of the children. And then you know by the time you're doing your analysis, you don't have to see all the parents to have C V D. So for some you don't, you know the AD has C V D, for some you don't, because you may have in time for me to happen. So in that situation, you have sensitive information. I have a question about the outcome here. Yes. But measured what time point? I'm going to give specific when we're going to look at the data analysis. Yes. So for goal number three, you have I, Y, H, theta. And one of the assumptions is that the sensoring process is independent of Y. Why does it make sense here to add y as a conditional and this probability? Yeah, that's a good question. I never said essential has to be independent of the outcome, though. It's one of the scenarios when it's independent, you may drop the why, but it's not always independent of the outcome. Yes? Yeah, how do you incorporate if a person has died without C D V? Have CDV. Have we in company with that before having C D V? Yeah, that'll be also a reason for censoring, right? But it's a different type of censoring. That's different for the normal censoring because we're hoping, well, now hope maybe we're projecting that they may have C V D, right? And if the person does, then they didn't have C V D, that's a different problem. You may even exclude that person from. Even exclude that person from the study, or if you use want to use that information to impute other participants' observations, that's helpful. Somehow you're mixing in the same censoring pattern those that didn't have CBD but they're still alive together with those that died before CBD. Yeah, but in the context of events public wedding, we're not including those people in the analysis. We're not. Yes, question, Mark. Um yes question about that for you for you like on the screen is two different U's so delta UI plus one minus delta UI are those two different UI here and here just the on that I think he's just saying the UI can be viewed as delta UI plus one minus That's delta ui plus 1 minus delta ui. We have ui, right? Ui can be written as delta ui plus 1 minus delta ui. So then whether your delta is 1 or 0, you have a ui. Right? Now, if you're i if you you don't have anything regarding the sensing, then your your UI won't be included. That's where the problem comes in. That's where the problem comes in. And because in that scenario, you're only looking at these people. And that's the reason, maybe I should go to the next slide. That's the reason you're going to need a different UI, that's a weighted UI, so that you can do your inference. So as long as this is positive, which is called the positive assumption, you can recover your UI and have this expectation to be zero, which is. This expectation to be zero, which is one of the core principles for GLM. And when it's zero, then all you need to know is to solve your beta, and beta has these property sensors for proper consistency and asymptotic normality. Where you can get not only the point estimate and also the conference interview. And you can extend that to multiple random sensitive covariates. HI is Z. Z is the covert you have in the model, but you can also include it in any covert you think may help explain the reason for sensory. And we'll call those alternative variables, which you can include in the model, especially for the model for sensory, to be able to have a better way to account for. Have a better way to account for the selection bias here. Because when you censor, technically it's assumed to be a selection bias. And why selection bias? Because you started with a group of people and you're artificially removing people from the study. So that's that selection bias, right? Because by removing certain people, you don't know whether you still have all the conditions to apply the methods you have. So you need to go back to credit for that selection bias. Go back to qualify for that selection bias by using bikini. I have a practical question. So, in your application, do you ever encounter a model that H contains something other than Z? So, coverage, you have a good reason that it should be in the delta model, but not in the Y model. Yeah, this is something we do, especially if you are a survivor now, if you're running. So, think about it from the perspective of survival. So, think about it from the perspective of survival analysis. If you're doing survival analysis, sometimes we have what we call informative sensoring, which are the observations or the cover we can measure after you've given the treatment that may explain why people are quitting the study. And if you have that information, you can include that to the sensory mechanism to be able to analyze the data. The reason why people are not adherent to the treatment. To the treatment can help you write a better sensory model so that you have a better control for your selection device. Why given? No, no, no, you're not giving, you don't include them in the why model. You're including them in the sensory model. So, technically, at the end, we're going to have two models. I don't know if I did that. So, you're going to have. So, you're going to have two models. The model for Y is going to be Y, X, and Z. But the model for Y, where you're going to use either imputation, because we didn't use imputation, or you're going to use invasible weight, that's where you're going to include those extra covariates that have explained the sensory mechanism, but you're not going to include them in the account model. A better way of asking questions. Better way of asking questions. Is there a reason that I do not include? Because it's like wasting resources. I could have a Y given X, Z, and that additional correct, maybe it's called W. I could also include it in the Y model, can I? No, you don't. Oh, okay. You were given the choice. You remember these things they call it because of infancy diets? Oh, okay. And the other thing. Good at that diet. I hated that pain. Right? So, if this is your treatment, let me put C. If this is your treatment, what you just need are this peak. Right? And sensoring, because we're sensoring on the X here, it's something that takes away from this. So, as long as you can adjust for the covert that fits into X and goes. Feeds into X and goes to Y, that's fine. But if you adjust for something that's here, you are in trouble that comes after your treatment assignment or after your conflict of interest because these are called media, right? Because they are in the causal pathway between X and Y. So you have a way to include this in the model, but that's a different type of analysis called mediation analysis. Called mediation analysis. And so the variables we're thinking that can help explain the sensoring here, right? As I'm done, some of these, right? Instead of including Y, you may include this to explain the sensory model. But we're not going to include this for the model of a Y. So the only things we're going to include are C and X. Otherwise, these are going to create a lot of problems. Create a lot of problems. You may not have the connection between X and Y, but by including these people, you may create this connection that didn't exist. So you need to be careful there. That's why they're not part of the alcohol model. Although it's like a waste of resources, but actually that's clever not to include them. Any other question? Yes? So this recovers consistency when the assessment is a Covers consistency when the assumptions for a complete case are not met. Do you have any? Do you know how the efficiency of this compares to complete case when the assumptions for both are met? Yeah, we're going to see in the simulations that even in that regard, if you use inverse quality wearing, you recover a result that as good as the complete case analysis when you have a small number of sensitive observations. Number of sensor observations. And when you have a large number of sensor observations, it's even better than the complicated case analysis. Although the complicated analysis is good, but you lose efficiency in terms of the fairness estimation. So how to estimate pie? So you can use logistic regression. In that case, you just think about sensor, not sensor. You can use a compromise because it becomes like a survival type of measure. Survival type of measure, you can use a calculation of half-side model, and you can use other models because calcs we use we go with calcs because that's sort of what's popular when you're doing survival analysis, but you don't have to use a calculation housing model. And this is one result that's pretty known in cause of inference, that even when pi is known, estimating pi gives you a better result than not estimating pi. It seems counter counterintuitive, but uh Counterintuitive, but there is a reason why you can estimate a pie, a pi gives you a better result. And the reason is like it uses all the data you have, it uses the data efficiently and effectively. So even when you know the pi, although most of the data is unknown, simulating it's very helpful. So then we run the simulations analysis here, pretty simple, three core, two core. 2 covariate plus X, where we use independent sensoring, so we were thinking about large sensoring and heavy sensoring. And we also use outcome-dependent sensoring here, and why outcome-dependent sensoring here? You're going to see that the sensoring is an epsilon, actually, which is the error term for the outcome model. And the outcome model we're using the linear model. Right? And the goal is to look at how the sensing is impacted. How the sensoring is impacting the data analysis, and also how independent versus outcome-dependent sensoring can also play a role here. And in addition to outcome-dependent sensing, you can also run the model where the sensoring depends on the X's or the Z's. That also, a model you can look at. And we also consider the case where we have 65 sensors instead of just 24, and we also. 24, and we also consider a case where you have contraction term between X and C. Yes. The interaction case, does that mean you assumed that there was an interaction in the model or that the model was misspecified? No, you assume there is an interaction in the model. Yeah, if you go to misspecification. That's a different experience. That's a different problem. Sorry, what's the C and what's the H? C is sensing the covert for sensory. For the covet for sensory. So we're using, we determining sensory using the wildlife distribution here. So C is sensory. So C cannot appear in the linear models? No, C doesn't appear because it's not the atom that's sensor that's covert X. Right, so C is what perturbing X. So we're going to generate X and then we're going to To generate X and then we're going to generate C, and C is going to be considered observers, not observed, depending on the criteria we're going to specify for C and X. So C is the sensory mechanism for X, not Y. Y is fully observed, and Y doesn't have any problem. So then that's also the difference between when you do survival analysis where the outcome is censored versus here where the covert is censored. And the probability of sensoring. So when it's independent sensoring, as you can see, this is where the full model is the model with that sensoring because we know what we can get, so we can calculate everything. When you have light sensing, really not many problems. Actually, when the samples are came out, you have pretty much results. you'll have pretty much similar results. Although you see the bias here for IPCW and the purple mire are a bit highly here, but overall everything is fine. But then when you have a heavy sensoring, you pay a price when you do a complete case analysis, especially in terms of the standard error key. Right, so the measure I improve with The measure improved with increased sample size, which is what expected. You have marginal bias when it's large censoring, and you have really a problem with bias when you get large proportion of people being censored. The complication analysis are going to have larger standard error with increased censoring rate, but these are going to have similar performance. Similar performance, and I'm going to ask you why they have similar performance. But overall, you have the invisibility where you have the couch computer as a model that perform very well. Now, the question is, why these have similar performance? Well, on your proof, you show that the expectation of the estimated function is equal to zero quite a few minutes. Is equal to zero, but if you misspecify the model for the weight, then you will not achieve unbiased results. At least here, the IPW with the cost model, you're not assuming any distribution for the sensory mechanism. So that's why here's the least unbiased. But with the other two IPWs, you're getting biased results because you're putting a, you're specifying a You're specifying a model for that probability that may not be correct. Is that right? In part, I I let the teacher do you know the answer? Looking at that, they're coming so well. They're going to have a similar performance because nothing really here depends on the outcome. And although most of the time the complement is going to be slightly better, but since it's independent. But since it's independent sensoring, so the only thing you actually use in here is the fact that it was sensor. And the PACS model is actually leveraging all the coverts you have in the model, although the sensor doesn't depend on the covert, to really improve the selection bias. Upper table. Right, right. So I cannot see very well what the So I cannot see very well, but in truth, the full data analysis variability is even larger than the IPCW and the IPCWK actually. Here, here. The full should always be the best, right? The full should always be the best. Right, does it even have smaller variability than the Smaller variability than the first row? Yeah, here, yes. How come? Comparing the full to IPCW, the standard error. I think the IPCW is biased, so it is not good to directly compare the errors that they have. IPCW is going to have a bias. No, no, in his case, it's not biased. It's the point. It has some bias. Is this a biased question? Is IPCW biased? No, it's not biased. So don't keep in mind it's just tiny numbers, actually. That's what we're saying: that the bias is only marginal. But the percentage bias is 20%. Yeah. Yeah. I'm not confused. So, in theory, all three methods should be consistent, right? Yes. And the bias what we are seeing is just finance on our bias. Yes. Okay. So, really, nothing profoundly related to is that my turn? Nothing profoundly related to the band that he's just a small star like. Small star, like a finite sample by SU. Do you have a larger sample result for this same setting? For the same setting, we have 500 and 600. Do you have in biased also? Is it the biased? The full or the IPC ones that have smaller variability than this one here? Yeah, the small. Here? Yeah, the third row and the fourth row. Here? Yeah. The fourth row? The fourth row and the third row. So that's IPCW and IPCWKM. Yes. I would have loved to see them diminishing the bias, but maybe as I'm just saying. Diminishing bias going from here to here? Yeah, and if it's not, if 600 is not big enough, maybe we need to go larger to see. Yeah, so is it is it diminishing? No, not diminished. Because it's 2020. Yeah, I hope I hope we're all statisticians. Maybe we need a larger sample size. Yeah, we need a larger sample size, but this is not unusual. And there is nothing wrong here. It could have been a problem if you want the o the opposite direction, like it increases maybe by forty or sixty. But if it remains twenty, twenty, I don't see a problem. But what's your problem? It seems a little bit high for me. Twenty percent? Yeah, very accepted kind of. I would feel more confident if I make it 6,000 and it goes way down. I don't have to record it, but I agree with you, but that's not the point, though. The point is comparing the four methods. For methods, it's not seeing at which point you get it perfect. Because if that was the case, then I would have done that, increase the samples until you see everything. I think that concerns me a little bit is first is the bias seems not very small. And the second is the standard deviation is better than the full data. The standard deviation here or here. Here. Which one is the sum? Yes. Which one is the sample version? Which one is the estimated version? If the sample version i the is better than the full data, that worries me. What do you mean by the sample version? Which is the estimated variance and which is the one empirical variance. So the variance here, first of all, was estimated using the pushclock because of using IP diagram. So is it SE or SD? I guess the estimated wagon is not very precise, would you say? Yeah, it's not very precise, yeah. And I'm okay with that. I'm not okay with that. I want to know what the sample size of matter. The samples as we get this, I mean, get these problems away, but we hope so. We hope so. We would like to see that. I mean, I'm not an expert in this physical rating, but I guess I'm not surprised. I guess I'm confused. It seems like bias variance trade-offs. Yeah, that's what I was thinking. Yeah, we don't have a bias with various trade-off issue. But the variances are getting smaller and the smaller. But the variances are getting smaller and the bias is just getting big. Our goal is is just like the very verification of the theory. The theory says the method is consistent. Oh, the theory says it's consistent. Yeah, I realize for the IPCW line. For all three, I suppose, yeah. For all four, actually. So in your understanding, 20% bias, that's too much. It's the combination of the bias not decreasing. Of the bias is not decreasing, and the SE is too bad, too good compared to 4. That's the point. So I think it's decreasing slowly because I think you're looking at 20, 20 here. 36 and 36, you feel like it's not decreasing. But if you look at here, it goes from 38 to 36, from 18 to 16. And even this, I mean, although this is what you said, there's methods. What you said, there's methods going from 4 to 2, from 6 to 6, so it stays at 6. But eventually, if you increase the sample size, you're going to get smaller. Some of the dice is very small. It's the IPCW and IPCW capital minor that is a little bit. But maybe there's no problem. It just needs a large sample size. How sorry are you? Won't you remind us quickly what's the difference between IPCW and IPCW PR? So IPCW, yeah, let me provide that. So IPCW is just using magistrate regression, and then you use the way the progressive scores as a way to basically get KSWA. IPC the IPC capromario, you're going to get Calculum value, you're going to get the pi's using the Caproman estimated in the supervisor analysis, and the Cox proportional hazard model is going to estimate the pi's using the Cox proportional housing model. I love all the questions, I really appreciate it. Just for sake of time, let's allow him to just complete the talk and then people have plenty more time for questions. So, we did the same thing when we did the dependent sensing, so it's more dramatic here, and I think. Dramatic here, and I think you're also going to have your same questions here that is getting done slow here. And at the end, also, we have the IPCW cards was the best performance overall. Now, we looked at the Framingham data. So, we consider the data on people who have at least going to the regular visits, the fourth five visits. We looked at Visits, we looked at the offspring were smokers, and we considered a measure at exam seven and eight because they are examined regularly. And so, exam seven and eight is just based on what time the exam was scheduled. And then so, in Los Angeles, we have 36 people who have C V D, 52, 82 beer consumers, and we have almost And we have almost roughly half of them were male, and 67 of the covert was censured. And so, using that, I just recorded here the coefficient for 8. So, you don't see anything in terms of the statistical significant using the complete case analysis as using the Bausistic equation. But when you use a Caltrumaier or a Caltrum School model, you have slightly different shear, but you have a significant. Here, but you have a significant result. Then, just to wrap up, and I can take your questions later. Using complete use case analysis can be misleading for all the reasons I've mentioned here. And even when you have all the conditions to apply the complete case analysis, I think it's better to use other methods because as long as you can use all the data, that's okay. And censored coverts and censored coverage. Censored coverage and censored observations cannot be ignored, and they cannot be trailed as missing data because that was one of the questions we even had from the reviewers asking, okay, why can't you use this multiple imputation, which we know from the missing data literature? But we can't because, unlike missing data, because missing data, all you know is whether they're missing or observed. But with the sensors and observations, you know at least. You know, at least that up to a certain point they didn't have the event of interest, right? Up to a certain point for the parents, at least if at the time of the study, the parents were 60, you know, at least up to 60, they didn't have an onset of C V D. And that's a very good information to include in the model. It's not just saying that they didn't have C V D or not. didn't have CVD or not. So in this talk, we consider three methods paused to complete case analysis. And so the propensity scores are just for covert, but just carry less information because it's looking at just sensor the non-censored. The couple may consider the time to censoring sort of time where the person was actually censored, but they don't adjust for covariance. That's for the so that's sort of the drawback we've So that's sort of a drawback with these methods because you don't adjust for covert. And then RPCW with the cost proportionality model does both and can include Y in the model or if you don't want to include Y, you can include auxiliary variables. And we show that it performs better even under the the independent censoring and uh it's more robust, especially you want to use it when you know that the sensoring depends on the outcome of interest. On the outcome of interest. So, Joseph, just