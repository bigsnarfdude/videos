To present this recent work in this workshop. I'm sorry I could not make it, but it looks that it's going very smoothly. So I would like to discuss this work, which is not what the title suggested, I decided to change at the end. I think this one fits a bit better the workshop. So it concerns the analysis of the fundamental limits of principal component analysis. Principal component analysis in a case where what I will call the noise in a kind of probabilistic setting for PCA will have statistical dependencies instead of the usual assumption of Gaussian noise. Right, so some motivation. What is the first thing you do if you have a large array of numbers that is data and you want to extract meaningful information about it? You simply diagonalize this matrix. Let's assume for Diagonalize this matrix. Let's assume for simplicity it's symmetric. And you look at the dominant eigenvalues and associated eigenvectors. So this is called spectral PCA. And so let's first consider the ideal world where we like to do theory and where everything is extremely clean, which is in this context the so-called spike model or spike-Wigner model, introduced by John Stone around 2010. It's a very simple model. It's a very simple model for probabilistic PCA, where you have some matrix of data denoted y, which is the sum of a low-rank matrix containing some supposedly interesting information. They denote x star, x star transpose here. So for simplicity, let's say it's rank one, and z is the noise. And so, in this case, where the noise is a GOE matrix, just a symmetric Gaussian matrix. Symmetric Gaussian matrix, you have this well-known phenomenology of the Benau-Beik-Pesch transition. You have a bulk of eigenvalues corresponding to the noise, and if the signal-to-noise ratio lambda is large enough, you have an outlier eigenvalue that escapes this bulk. And if you do spectral PCA in this case, you will realize that the associated eigenvector will correlate non-trivially with this hidden direction x star. Okay, so this is the BBP. Okay, so this is the BBP transition. All right, and instead of looking at this problem from a random matrix perspective or from a spectral perspective, if you're interested in doing information theory, you can consider this model through the lens of Bayesian inference. And the kind of status now is this kind of so-called phase diagrams, this picture. Kind of so-called phase diagrams, these pictures that are very similar to what Cynthia has presented previously. You have these computational gaps. So here on the x-axis, you have the mean square or reconstruction on this hidden direction x-star as a function of the signal-to-nose ratio. And you see three distinct phases. The impossible regime is the one where even the base optimal estimator leads to poor performance. So the black curve is the MMSC. So, the black curve is the MMSC estimator, which is bad in this case. If you increase the signal-to-nose ratio, you cross the so-called information theoretic transition, which is when the vector, the hidden vector is uniform on the sphere, it actually corresponds to the BPP transition, but not always. For example, in this case, the hidden direction is sparse, it has many zeros, the picture is a bit different. The picture is a bit different, and this lambda IT does not necessarily match the BBP transition. So you increase lambda and you enter this hard phase where no algorithms that is efficient is able to recover the signal, but we know from information theory that the MMSC should lead, should be smaller. So there exists an estimator in the universe that gives something good. And then if you continue to increase the SNR, you finally enter the easy regime where we The easy regime where we know efficient algorithms. Here, their performance is indicated by the blue and red curves that starts to give non-trivial performance. In this case, PCA will give something non-trivial but sub-optimal. Instead, AMP, this approximate message pressing algorithm, will match the MMSC estimator. All right, so this is for what was known. Let's now look at what happens in the At what happens in a slightly more realistic setting where we don't have so many tools to analyze what is going on. And so I want to deviate a bit from this idealistic hypothesis on the noise being completely Gaussian independent. All right, so going towards experiments, if you open some, for example, neuroscience paper, you look at the kind of matrices they're dealing with and you see And you see very clearly that whatever you would call the signal in these matrices, in this case, these are neural spikes, so the spikes of different neurons as a function of time. Whatever is information in this matrix and whatever is noise, it is evident that the noise is not Gaussian. It is structured. There are strong statistical dependencies. And the point of this work is to try to design some model in which we can take into account such statistics. Take into account such statistical dependencies. And in particular, you take this kind of matrices, you maybe construct a covariance to make it symmetric, and you look at the spectrum of this kind of matrices, and maybe the kind of picture you would get looks like this. It is much less obvious what you would call the bulk that you would naturally associate with the noise, like in the BBP picture, and what are the outliers, what are the eigenvalues. What are the eigenvalues that correlate in some way, whose eigenvectors correlate in some way with the signal? And these complications are intrinsically connected to the statistical dependencies in the noise as well. Let me further motivate why structured noise makes sense from my viewpoint. Consider a simple supervised learning task. I give you these four pictures. I'm almost sure that most of you will have separate. Almost sure that most of you will have separated these pictures in these two classes, but actually, I didn't explain you the task, right? So, this is what you should do if the task was to label dogs and cats. But now, if actually my task was to understand the notions of outside versus inside, you see that the linear classifier in this case is actually very different. And for that second time, For that second task, every feature in these pictures that correspond to what is a dog or what is a cat are what I would call noise, okay? Because they are not related to the outside versus inside notions. And so this, the point here is to emphasize that noise is task dependent, the notion of noise, and can be highly structured, and so this should be taken into account in the analysis in some way. Account in the analysis in some way. All right, so this is what we've done with my colleagues here: Francesco, who's a postdoc with me at SATP, Marco Mondeli at IST, who presented this week as well, and Manuel, who used to be my postdoc and is now actually moving to UK in this recent paper published in PNAS. So, what we studied is a kind of variation of the SPICE model because it's a kind of bottom-to-top approach in the sense that we wanted to stay. sense that we wanted to stay close enough, let's say, we want the model to still be fully analyzable from a theoretical perspective. So we try to deviate in a meaningful way while remaining tractable. So the model has basically the same form. You still have a spike plus noise. The data is y, which is given to you. Just for simplicity, I'm going to consider that the entries of this spike of this. Entries of this spike of this vector x star are drawn iid from some prior px that I know in this Bayesian context. And also, by making this signal very simple, we're going to focus on really the effect of the noise structure instead. All right, and so what changes is now the model for the noise, which is now a so-called rotationally invariant matrix. Concretely, it means that if you diagonalize it, its eigenvectors are. It its eigenvectors are uniformly distributed in the space of orthogonal matrices, so they are so-called R-distributed, or said differently. Z is drawn from the so-called trace ensemble, which is what is written there, where this V of Z that I will call potential, think of it as a function that really applies entry-wise to the eigenvalues of Z, and that leaves the eigenvectors invariant, okay, due to the simplicity of the trace here. The measure is invariant to. Is invariant for any rotation of Z, so that's why the model is rotationally invariant. This was studied from the RMT perspective by this milestone paper not so long ago, a bit more than 10 years ago. But what I want to consider is the Bayesian version of this. So just to fix ideas, consider as a simple case a potential which is quartic. In this case, if you write this probability measure explicitly in terms of matrix elements, you realize. Terms of matrix elements, you realize directly that this measure is no more factorized over matrix elements. Okay, so only when you take a potential which is quadratic, you recover the Gaussian, the standard Gaussian orthogonal ensemble. And this is the only ensemble which is at the same time rotationally invariant and that generates matrix and entries which are independent up to symmetry constraint. For any other potential, you induce Potential, you induce necessarily statistical dependencies among the matrix entries. Okay, all right, so because this model is rather complex to deal with, we focused on a kind of simple version where this potential is constructed as a kind of interpolation between the purely quadratic potential and the purely quartic. And a purely quartic potential. Okay. And the interpolation is kind of tuned by this parameter μ. And so as I will decrease μ, I will deviate from the pure Gaussian orthogonal ensemble to a more structured ensemble. Okay, so if you look at what happens when you change μ at the level of the spectral density of these noise matrices in the limit of large size. Limit of large size. This is what happens. So when μ is 1 and γ is 0, you have the pure GOE ensemble. So the spectral distribution is the standard semicircle law in green. And as you increase mu, you go through this kind of double wedge shape like this. And when mu is zero and gamma is some fixed constant, you see that you have this blue shape and just Blue shape. And just to give you some intuition, why do you have this kind of double whale potential like this in the spectral density? Simply because if you write down the PDF of the eigenvalues of this noise instead, so you do the change of variable from matrix entries to eigenvalues, you have a Jacobian. And the Jacobian that appears is called the von dermond determinant. And it's a kind of pairwise repulsion between the eigenvalues. You can read it here on the right. Read it here on the right. And so this repulsion between eigenvalues, in a sense, forces they repeal themselves. And so, what the picture to have in mind is a kind of gas of particles with the same charge, which are confined in a strongly confining potential, a kind of quartic potential. And if you compress them too much like this, and at the same time they want to repeal themselves, they will start to climb the wall of this potential. And that's why you get this weird curve. That's why you get this weird curve here. Okay, so here is the Bayesian framework that we studied. Here is the posterior distribution, prior times likelihood, which is now this likelihood is a bit more complicated. You have this potential appearing there. And the two main quantities of interest that we studied are the Shannon entropy of the data and the minimum mean square. And this Shannon entropy. Minimum mean square and this channel entropy contains the location of the phase transitions in the problem, in particular the information therapeutic transition that I discussed before, and the MMS is the fundamental lower bound to reconstruction of this spike. Okay, so this kind of problems were studied in different contexts. The setting where the noise is Gaussian is standard. Instead, Standard. Instead, the spectral approaches to these problems are studied in this paper there by Benet, Georges, and Nada Couditi, where the spectral estimator is proportional to the top eigenvector of the data. And then there is an algorithm, an AMP algorithm that was proposed some year ago, which was thought, at least we thought, would be optimal. That was actually the motivation of this paper, is to show its optimality, but it turns out that it's not. Optimality, but it turns out that it's not. And this AMP is aware of the prior of the spectral distribution row of the noise, but it does not exploit it in the correct way. And I will tell you how to do so instead. And there is this other line of work that shows that if the noise is not necessarily Gaussian anymore, but the entries of the noise remain independent, you have a kind of Gaussian universality that tells you that from the information theoretic perspective, as long as the noise has independent entries, As long as the noise has independent entries, it's equivalent from Gaussian noise, but with a variance profile. Instead, the model we study does not fall in this class because the noise has statistical dependencies. All right, so the results come in the form of a so-called replica symmetric formula. So one of the main tools is the replica method from spin glasses. So it reduces the computation of the Shannon tropy into a varietal formula, a low-dimensional variational formula. Formula variational formula in terms of so-called variational free energy. I will not ask you to parse the formula, but it's a rather complicated object that depends on many quantities that you need to optimize over. But you can do that, okay? And you can compute this asymptotic limit of the entropy. And as a consequence, you can extract the minimum mean square era, which looks seemingly simple, but actually the complexity is due to the fact that this m parameter. Is due to the fact that this m parameter is actually the solution to this complicated variational problem which is there, okay? But you can plot it, and as a consequence of the analysis, we also show that non-rigosely that this interesting fact, I think, is that as soon as the signal that you try to infer this spike has a measure which is rotation invariant, so basically it's Gaussian or spherically distributed. Or spherically distributed, then you cannot beat the performance of spectral PCA. So no Bayesian estimator, no estimator can beat PCA independently of what is the structure of the nose. From the algorithmic perspective, what we show is that what I would call the natural approximate message passing, the one that you can find in the literature on this problem. Later on this problem, for structured PCA is suboptimal. So here is the form of the AMP. It's a kind of generalization of the power method where you have application of your matrix to a vector. You remove a so-called Onzager correction, who simplifies, who helps the algorithm to converge and turn some statistics of this high-dimensional vector into Gaussian statistics, and that allows for rigose analysis. Analysis. And so after this matrix multiplication and this removal of on Zager term, you have a nonlinearity that is applied. So that's why it's a kind of generalized power method. And so what we show instead is that the correct AMP looks similar, but you see that instead of multiplying this U vector in the algorithm, which represents really the estimate of this spike that you're looking for. Spike that you're looking for. Instead of being multiplied by y, which is the data at every step, it will instead be multiplied by some polynomial of the data matrix. And this polynomial is in the matrix sense. And in the case of a quartic potential, it is expressed here and can be deduced from the analysis. And this is some hard to guess polynomial. And there is no real intuition behind this form. The only thing I can tell you is that if your The only thing I can tell you is that if your potential behind your noise is of order k, this polynomial will be of order k minus 4. And we have one rigorous result here, which is that this algorithm can be tracked rigorously thanks to so-called state evolution analysis that essentially tells you that these high-dimensional vectors that are iterated, this u and f appearing in this algorithm, can be tracked through kind of low-order Gaussian. Through kind of low-order Gaussian statistics, which allow to access, in particular, the mean square error associated with this algorithm. Okay, so let's check that I'm not lying and that you can indeed improve and therefore exploit the noise structure in a non-trivial way. Let's start with the picture when μ equals 1. So, μ equals 1 corresponds to the noise being a pure Gaussian orthogonal ensemble, a pure Gaussian matrix. Gaussian matrix. So the solid curves are the state evolution analysis and points are experiments. And the black curve is our replica prediction. And so you see that when the noise is Gaussian, everything collapse. The old, the previously known algorithms in blue and green give the same performance as our proposed algorithm, as it should. But now, if you increase the noise structure, so let's look at mute. So let's look at mu equals zero when the structure is the strongest. You see a gap emerging, and you see that these previous AMP algorithms are kind of stuck on a line which is suboptimal. They are still tracked rigorously by their state evolution analysis, but they're suboptimal. Instead, our algorithm is improving rigorously. Improving rigorously because it is rigorously tracked by the state ocean. The conjecture here is that the curve, which is in black, this replica prediction, is really the one that predicts the minimum mean square error, the minimum mean square estimator, but we believe it's true. Okay, there is also strong universality kicking in. So, what does it mean? What we've done is to repeat the experiments. To repeat the experiments. But in the case where now, instead of generating the noise in the experiments using eigenvectors, which were taken as those of GOE, meaning really R distributed. We took these eigenvectors as being those of certain covariance matrices from real data. Okay, so we took, for example, the CIFAR data, which is a standard. Data, which is a standard data set for images, or we took a data set coming from biology, a kind of gene-counting data bank. And we see that basically the theory predicts still extremely well what is going on. And so there is the point is that despite this rotational invariant assumption may sound restrictive, it seems that in practice it is not so much. Not so much. You can even increase more the amount of structure in the noise by considering cystic potentials instead of quartic ones. So if you want the statistical dependencies between the noise elements becomes longer, they have a longer range. And so you see now that the gap is at the linear scale, before the plots were at the logarithmic scale. And our algorithm perfectly falls. Perfectly falls on the replica prediction and on its state evolution. This is a theorem. And again, the universality here is very present, even more than before. Also, it's because the fluctuations look smaller due to the fact that we are in the linear scale now. So we were very satisfied with this picture. I think I have maybe five minutes left because it started a bit late. Because it started a bit late. So let me say a few more words about this kind of magical function that is applied to the data before being plugged in this Bayes optimal AMP, this conjecturally Bayes optimal AMP that we propose. Let's try to see what's its effect from a spectral viewpoint. Okay. So here is on top, so you can read again this so-called optimal pre-processing, this Optimal preprocessing, this polynomial function we apply to the data in the special case when the potential is quartic, which is assumed to be known. We are in the Bayesian optimal setting. We know all hyperparameters of the problem. So on top, you see the spectrum of the data, okay, with an outlier. And you see this kind of double well shape. And the orange curve is the asymptotic. Is the asymptotic prediction and so it matches well. Now, the picture in the middle is this pre-processing function that will be applied entry-wise to all the eigenvalues of this matrix. And below, you see the effect, you see the resulting spectrum. And what you can observe is that all the eigenvalues that were part of this bulk of noise, eigenvalues, have been sent to the negative axis. Negative axis, and instead the outlier has been pushed even more on the right. So, what you see is that the point where this function kind of crosses the zero axis is kind of close to where the outlier is. So, this function is aware of where should be the outlier, and it sends everything corresponding to noise on the right and pushes even more on the left. It has a kind of cleaning effect for the spectrum. If you increase the signal-to-noise ratio, the function is even more. Signal-to-nose ratio: the function is even more confident in this cleaning effect. The outlier is further from the bulge, so the function is more confident. And you see again the same effect, but now the outlier moved from a position of around five to something which is closer to 20. So it's a kind of very strong cleaning effect. Let me now just say in one minute, just a few words about. One minute, just a few words about what we understood. Um, so what is the reason behind the fact that this previously introduced AMP, what I called the natural AMP, is suboptimal. So, what is going on is so we've redone analysis in a slightly different setting. It is a setting where the statistician that is performing in France is not, so it's a misconception. Is not so it's a mismatched inference problem. The statistician is kind of trying to sample a posterior distribution corresponding to Gaussian noise. So even if the noise is not Gaussian, it is actually structured, it comes from another trace ensemble, the statistician assumes so. And so this person writes down this mismatched posterior distribution, which is that you can read of here, where you see the line. Of here, where you see the likelihood now is quadratic, meaning corresponding to a Gaussian node. And so you can do the replica analysis for that setting, and you see that indeed this AMP, when the noise is structured while the statistician assumes that it's not, the replica prediction will give exactly what the AMP gives as well. So there is again a perfect match in this case. So what does it mean? In this AMP, you can actually AMP, you see that the spectral distribution of the noise is explicitly used. So that's why we conjectured it should be good. But actually, the point is that this spectral distribution is not used in order to improve statistical accuracy. It is instead used to improve the convergence of the algorithm. So, without the knowledge of this spectral distribution, the algorithm would simply be not convergent. It becomes convergent. Convergent. It becomes convergent, but it does not sample the correct posterior measure. That's the point. Instead, the algorithm we propose does sample the correct posterior distribution. It's not really a sampling algorithm, but at the end, morally, this is what happens. All right, so we have exploited the noise. So that's great. Take home. We offered, we provided a potentially base optimal efficient iterative. Optimal efficient iterative Bayesian AMP algorithm for low-rank information extraction from large structure noisy matrices. And the potentially emphasizes here that most of the analysis is based on non-rigorous techniques, but we think that it is indeed Bayes of T. It exploits knowledge about both the signal and the noise combined. It comes with rigorous guarantees and it is empirically very robust to non-rotational invariants. So you have strong universal. Non-votation invariant, so you have strong universality effects. All right, that's the end for me. Thank you very much for your attention.