I'd like to thank Leila and Lang and Peter for organizing this wonderful workshop and inviting me to speak here. So I'm going to talk about something different, which is about survival model with measurement error in covariates. This is a joint work with my recently graduated student, Li Pang Chen, who is now a faculty member in National Chen. Faculty member in National Chenzu University in Taiwan. I'm going to start with a motivating example for this project. So for this Leatherland Cancer Institute breast cancer data, we collected about 295 women who were followed up from 1984 to 1995. And when those patients were And when those patients were recruited in this study, so they were 52 years or younger. And in this data set, we found about 79 patients died before the end of this study. So this created about 73% of censoring. And among those patients, we collected a whole bunch of gene expressions, which includes 25,000 genes. 25,000 expressions. And among those gene expressions, many of them are not really expressive. So, for this Vanderwejemar paper, they found about 70 genes which have been previously determined. They have predictive ability for tumor diagnosis. So, with this setup, a natural question is, natural question is how would those gene expressions would be associated with the tumor development so this is something of our interest here and before answering these questions i'm going to introduce our general framework quickly i'm going to use t tilda and c tilta to represent a finger time and sensory time for individual patient and we also have a collection of covariates which could be potentially Covariates, which could be potentially related to failure times. And traditionally, we could introduce any types of survival model to describe the dependence of failure time on covariance. Here I'm going to particularly look at Cox proportional hazard models. So this model has a good property in a sense. In a sense, we can separate the dependence of handler function on the time and the covert effects. So here I'm thinking about the baseline covariates x. So here lambda 0 represents a baseline handled function, which involves a time only, and it is unspecified. And alpha represents the covariate effects, which is usually of our interest to estimate. Our interest to estimate. And the G function could be specified by the user. If we specify this, many times people may just use a linear form to indicate its form. So this is just a standard framework for analyzing survival data. Now, in this process, we have two quick comments here. First, in terms of the covert vector capital vector capital X, we treat all the components XJ equally, and pretty much we assume they are uncorrelated or not associated. And another assumption we usually made implicitly is this covert X is precisely measured. But those two assumptions in practical settings could be violated. For example, if we go back to our If we go back to our motivating examples, we can see there are several concerns. Number one, are all those identified 70 gene expressions important in explaining the survival process? And also, do all those gene expressions affect the survival process in an equal way? Would they work? An equal way? Would they work in a network pathway or structures? So we want to study these kinds of questions. And another concern related to this data is because of laboratory errors and the actual measurement of gene profiles involves measurement pair. So instead of having precise value of X itself, very likely our act. Very likely, our actual measurements could be something different. So, here I'm using notation x star to indicate potential difference from the true gene expressions x. Now, our objectives becomes, number one, I want to select relevant gene expressions in order to explain my survival process. Number two, I want to identify potential network structures among those covert components. Among those covert components. And number three, I also want to address potential measurement effects in our analysis. So in the second part, I'm going to extend our traditional Cox proportional hazards model to a graphical proportional hazard model. And before doing that, I want to quickly introduce this notation, Camp to G, which involves two components. So this is Two components. So, this is basically used to represent graph structures among all the covariate components. So, I'm using VTuta to indicate the connection of the index of my components in covariates X. And I also want to express potential connection between any two components in X, which represents a conditional dependence. The conditional dependence given the other components in X. So if I'm able to collect all of them, then I'm going to use capital E to represent the collection of all those edges. A quick example here is, so left and right hand side gives us a different dimensional variable structures, which has different kinds of pairwise. Pairwise dependence or network structures. So, if two dots are connected, that means they have a connected dependence given everything else. Now, with these preparations, we can start thinking of the simplest case. What if all the components in my vector X are connected? So, I'm going to start with a saturated model by considering all. model by considering all possible compilations, pairwise combinations among components of X. So I can form this saturated model. And here, basically, this beta represents a parameter vector and theta includes all the pairwise connections. And B function is basically given and A function represents a normalizing constant. So I'm going to use this search rating. To use this saturated model as a starting point to study the pairwise dependence structures among my covariates. So again, I'm using this way to represent all those important individual components. In other words, if the corresponding coefficient beta r is not zero, so this represents a main effect associated with the Associated with my individual covariates. I'm going to use E to include all those conditional dependent components XS and X mu given everything else. So essentially, to look at the importance of individual components XR or pairwise connections, we only need to examine the associated covert effects, either beta R or theta S Lu. beta r or theta s u whether or not those coefficients are zero or not zero so i'm going to group all those components if the corresponding coefficients are not zeros and this would give us our graphical structures among the components of all my covariates and once this is ready then the remaining part is straightforward so i only need to combine the graphical structures with my traditional cox With my traditional Cox proportional hazard models to form graphical proportional hazard models. So, in this case, I would end up with this particular model. And for simplicity, then I can just assume my B function takes a certain form. For example, if I take my B function as an identity function, then I would end up with this hazard functions for my graphical proportional hazard models. Proportional hazards models. Now, with these preparations, then the remaining part is: I want to do inference about the associated model parameters, beta and theta, right? So what I'm going to do? So you need to make use of whatever sample information you collected. So here I'm assuming I have a random sample of n subjects. Then I use X superscript X to present a covert information. say covert information for subject i and i also introduce associated quantity n i and which is related to counting process and yi which is the anti-risk factor and with these preparations then the remaining part is straightforward i only need to make use of available inference techniques related to cox proportional hazard models so one particular thing is i can One particular thing is I can imply a partial likelihood approach, which allows me not to model baseline handled functions. So in forming this log partial likelihood for my graphical cox model, the only thing I require is how to identify my structure E and individual components in V. So this becomes our following focus here. How can we Focus here. How can we know the form of V and E? And to answer these questions, essentially that means I want to identify the covert network structures. I'm using script N to indicate the information consisting of V and E. And if this is possible, then I only need to do inference based on induced model by using a partial likelihood approach. Partial likelihood approach. But to do this process, there is a challenge. As I mentioned earlier, we can't guarantee all the covariates would be precisely measured. For example, like for gene expressions, the data collection process involves measurement tear. So we can't guarantee all the components would be precisely measured. So the question becomes how to address potential if Potential effects induced from air-contaminated covariates here. So, this is our objective in the following development. I want to identify my network structures in my covariates. And in the meantime, I also want to address associated measurementary effects in my covariates. So, this would be the material in the following part. Analysis for graphical proportion. Is for graphical proportion hazards models with mismeasurements in covariance. And now I'm going to look at my problem in a more general framework. So in any case, I would think my covert X is p-dimensional. And in this P-dimensional coverts, it could include either continuous or discrete or both. So I'm going to separate those two different features by using. Those two different features by using different notation X subscript C representing the sub-vector of continuous components and X sub D represents a vector of discrete components. And the reason of separately describing my coverts by continuous or discrete coverts is I'm going to use different approaches to describe their potential mismeasurement process. Miss measurement process. If I'm going to, so before doing that, now I'm going to use again x star to represent observed measurements. In the literature of measurement error models, we also call this as a surrogate measurements. So here I also have X C star and X D star, which represents observed version or surrogate version of X C or X star. or x star now i'm going to describe the link between surrogate measurement x star and my true covert x so i'm going to do this separately according to my coverts are discrete or continuous for continuous case i'm going to use this additive form i would think my observed version xc star is my true xc star plus some random noise Thus, some random noise. Here I'm assuming this random noise epsilon would be independent of everything else and follows a normal distribution. And key here is we assume this epsilon has a mean zero. In a sense, in the long run, my surrogate measurement x star will recover my true value xc. And similarly, for discrete case, now I would use a different way to express a link between surrogate xd star. between surrogate x d star and true value x d so i'm going to use a misclassification matrix capital p to represent their relationship if i can work out this relationship then i can easily recover the link between surrogate x star and true x star by this matrix product and notationally i'm going to use a simple form and like x d A simple form and like x d star, so which is a middle part x d star expressed as a function of x d if I can work out this misclassification matrix P here. And in the following development, I'm going to assume my measurement error degrees in continuous coverts x is norm and misclassification probability is also known. And I know this is. Is also known. And I know this is not necessarily realistic assumptions, but to highlight a key idea, I'm going to make this simplistic assumption here. Now, in the next part, I'm going to introduce how to correct mismeasurement error effects in my process of selecting relevant covariates and in the meantime, identify network structure. Identify network structures among my two covariates. So there are two issues I want to address here. Number one, I need to accommodate mismeasurement effects in my entire inferential procedures. The basic idea here is: I'm going to use simulation-based technique to understand. To understand how different degrees of measurement error could affect my inference results. So, the basic idea here is I'm going to artificially generate a sequence of surrogate measurements. And in doing so, I'm going to artificially add additional measurement errors. The rational here is: I'm thinking if is I'm thinking if I have one air contaminated surrogates, if I still use traditional inference approach, I would expect my results would be biased. So that means I would end up with a biased estimate. Now, if I'm going to do this multiple times by introducing a sequence of artificially created surrogate values and then repeat a traditional inference approach, I would end up with a sequence. Would end up with a sequence of biased estimates. And if I'm able to examine the trend of this sequence of biased estimates, then I may be able to extrapolate this trend to the setting which corresponds to error-free scenarios. So in that case, I would be able to address how different degrees of mesh and tear could affect, could induce different Could induce different degrees of biased estimates. So, this is a basic rationale for addressing these measurement error effects. And the second issue is I want to identify important covariates in compilation of determine the underlying network structures when I'm doing my first step to look at different degrees of measurement effects. Degrees of measurement effects on my bias results. So, the remaining part is: I want to combine these two rationales together in our inference procedures. So first, I want to artificially simulate data. And when I simulate this data, I'm going to use my original observed version to generate a sequence of data. So here, For data. So here I'm using WI and CB to indicate this would be the artificially generated surrogate for my continuous components. And this B and C would be user specified. And similarly, I can generate a sequence of discrete surrogate value denoted by W D B casi for each subject. For each subject. And you can see if I can count, if I can, if I specify different values of casi, then that would give me a different degrees of mismatch in my artificially selected surrogate values. And you can see if I'm going to let cosi equals negative one, then that would recover the scenarios of no measurement error. If casi is getting b. sign is getting bigger, then I would end up with artificially generated surrogates, which involves more errors. Now I'm going to pretend those artificial selected surrogates would be my true covariates. Then I just repeat my ordinary inference procedures. So in here, I want to identify important covariates. So that's why in here I'm going to formulate To formulate, pinnerize NOG partial likelihood for either beta parameter or theta parameter. So, here I separate from them because I may introduce different tuning parameter, numbda1, namda2, all different pinerlize function here. Then, my idea here is I just repeat ordinary procedures and do iterations. And so, I'm going to keep doing this many times. Doing this many times until I get convergence. Then I'm going to. So basically, when I do this, I just do this for each generated artificial surrogates. So that's right here. I have dependence on cosi and b. I'm going to use beta hand and theta hand b casi to represent the resulting limits for the escalators obtained from my step one. So these procedures would These procedures would give us a sequence of biased estimators. And now the next step is: I want to study how those biased results would depend on the degrees of different mismeasurements. So that's why for the final step, I'm going to group all those biased estimators, and then I'm going to fit a regression model to each sequence. And then Sequence. And then I'm going to extrapolate my regression fitting to the scenarios corresponding to mismeasurement free settings. So in that case, I would end up with two estimators denoted by beta hat and theta hat. And this is whole procedures of identifying important covariates and also address measurement error effects and also identify. Identify potential network structures for my covariance. So it turned out this procedure is valid in a sense. It will give us consistent estimators. So we can theoretically prove under certain regularity conditions, then my resulting estimators and their true values will differ by such quantity. One over square root n in probability sets. So this basically indicates my resulting estimators would be root n consistent. And the second properties tells us, because one of my goal is try to identify underlying network structures among my covariates, right? So it turned out under regulatory conditions, they estimated network structures in network structures and hat identified identical to the true network structures will have these properties. It will converge to one if my sample size approach infinity. And also in terms of individual estimators for my parameters beta and theta, their sign and the true parameters will be identical when my sample size approach infinity. Sample size approaches infinity, this would happen with high probabilities. So, this essentially says if my sample size is big enough, then I have a high probability to identify my true graphical structures among my covariates. Now, I'm going to ramp up this talk by giving you a quick idea about how the proposed methods would perform. The proposed methods would perform if I apply this to my initial motivating examples. And as we said, measurement there is basically present when we connect gene expressions. And this has been acknowledged by many authors. Even if we know measurement is present in our data set, but unfortunately, we don't really have a precise information to characterize. Information to characterize the degree of formation pair in our process. So, to circumvent these issues, now we are going to conduct sensitivity analysis to assess how our analysis results could be affected by different degrees of measurement error. To do so, I'm going to use this so-called reliability ratio R, which represents a variance of individual Of individual covariate components, like a variance of the true covariates divided by the variance of its surrogate version xj star. And in our studies, we just for simplicity, we just assume every component have the same types of measurement pair and the same degrees of measurement pair. So here we assume a common reliability ratio. And on the rightmost graph, this is the results for naive approach, which means I just use my available data without addressing any potential measurementary effects. So I would end up with this kind of connected nine segments to indicate network structures among those same genes. If I suspect my measurement their degrees, My measurement there degrees will give me 0.85 degrees of 0.85 reliability ratios. Then the middle graphs will be the network structures among those same T genes. If I consider reliability ratio is 0.65, then I have a different identified network structures among our gene expressions. Our G expressions. So, comparison of these three graphs shows our final identified network structures does depend on measurement, different degrees of measurement here in our gene expressions. And this table shows a sensitivity analysis for our data for those estimated coefficients under three settings. Under three settings, naive approach or different degrees of measurement degrees. And you can see, if I don't consider measurement theory issues, it turned out among 70 genes, 34 of them will be selected as important. If the underlying Bayesian theory degree gave me the liability ratio 0.85, then 29 gene expressions will be identical. Gene expressions will be identified as important. If R equals 0.65, then 28 of them will be identified important. So again, these results demonstrate our inference results would be sensitive with respect to different degrees of formation there in our covariance. So, here I'm going to ramp up this talk to accommodate survival data with both network. With both network structures and potential air-contaminated coverts. In this project, we propose a graphical proportional hazards measurement air models. And definitely, the same principles could be extended to other survival model, which does not have to be restricted to proportional hazards models. And in our development here, we focus on vision pair degrees, norm, and this sum. Known. And these assumptions in many settings is not realistic. But in two typical situations, these assumptions are reasonable. For example, if we can use previous studies, which provide the information for the degrees of measurement material in coverts, then we can apply this approach. And another setting says if our focus is on sensitivity analysis to try to understand different degrees of To try to understand different degrees of mismeasurement effect on our inference results, then our measures could be directly applied. And of course, in other settings, the parameters for measurement air models need to be estimated. And in that case, usually we would require additional data, such as repeated measurements for our covariates, or we require validation data available, or sometimes. Data available, or sometimes we can also try to apply the information conveyed by instrumental variables here. So, in that case, we can modify the current development to incorporate induced variability for estimating parameters related to measurement theory process. So, this is the whole idea about this project. I'll stop here. Okay, great. Thank you, Grace. Any questions? Yes. Hi, Grace. It's Lily here. Hi. Hi. Just to make sure I understand what you are doing here. I end up very much like it's a nice talk and the way you handle using sensitivity to handle. Using sensitivity to handle the extent of mismeasurement because then you don't need that gold standard data to calibrate. But if we take away the measurement error layer, if you look at your data example, so am I understanding so you basically, because you are penalizing, so you put all the 70 genes in there as the main effect, and then you put Effect. And then you put all the possible ages there, which are 70, choose 2, that many. And when you actually throw into the analysis model, are those interaction terms of the gene expressions? Yes, you can think of this as interaction terms. Yeah, so for example, like in here, if we have like S and Have like S and V who are connected, then the corresponding coefficient theta S V could be treated as interaction terms between two genes. Okay, great, thanks. Any other? Yeah, I mean, like the main point here is if we start with a saturated model, then you are going to consider like a simptoose to all possible interactions. Interactions. But it turned out if we do this selection process, then we only need to include those two interaction terms by not including all possible combinations. Okay, other questions? Okay, I yeah. Oh, sorry. I was just going to say, I thought I saw maybe a question from Peter on Dean's talk. question from Peter on Dean's talk. Peter, if you want to. I want to stop sharing now. Okay. Yeah, hi. Well, you know, oops, sorry. Yeah, so, you know, Dean, I think it's really ingenious to try to use a pure mediator to help us with vaccine surrogates. Yeah, so my question is: if we think about infectious diseases that are highly genetically diverse, And so, a monocle antibody or a combination of monoclonal antibodies, it's going to have a certain profile of the ability to neutralize certain viruses. Some virus would be resistant, some would be sensitive. There'll be some distribution of potencies of those. I'm just trying to think about, like if you go to the Moderna study, it was kind of a homogeneous study where basically the vaccine was Wuhan, the antibodies were Wuhan, the exposing viruses were Wuhan. Is ReWuhan, the exposing virus is ReWuhan. So it's kind of a homologous correlate setting where one can think about the monoclonal antibody being specific to the Wuhan and one can kind of think about it in a simpler way. But what if you had the trial where the set of circulating viruses had a lot of genetic diversity and a lot of different strains, and then the monocle antibody that's put in the trial, say, you know, it has one specific profile of neutralization against some specific population of viruses. Population of viruses. It's just really just a question: how do you think about your idea in this context of a more genetically diverse disease during the trial where you would randomize the M equals 0, 1, and 2? Yeah, that's a great question. I really haven't thought about that much. I was thinking more for SARS, where I expect there'll be a dominant strain for six months, and then maybe another one. Six months, and then maybe another one. And so we can keep playing catch-up with this sort of payathal licensure where, yeah, it works against Omicron in the test tube, so we're good for six months, and then rinse and repeat, basically. And so it's different if we say, like in July of next year, there's 13 different strains sort of competing with a different neutralization profile for this antibody we're given. It works well against the first four, bad against the next seven or whatever. So I've Or whatever. So I haven't really thought about that. I guess you could try and just try and develop an assay that sort of has pseudoviruses in the proportion of July of 2023 and then use that as a kind of proxy. I would say, like, another thing we're thinking about is maybe doing accelerated approval where we would have field evidence of how it's working. Evidence of how it's working, how the monoclonals are working in the field after it's sort of been provisionally licensed. And there we would use kind of a self-controlled case series. So basically, if we get breakthroughs sort of at the time when we know antibody is waned and not at the start, right after they've just been infused, that's sort of evidence that in the real world, the antibodies are working as predicted. So anyway, I think it's a more complicated. Anyway, I think it's a more complicated setting. We could try and do a more complicated pseudovirus neutralization assay, and then maybe it would need augmentation with field trials or post-marketing kind of studies. And there are ways to do that. Thanks. Okay. Any other questions before our break? Yeah, I have a can I ask a question for Can I ask a question for Dennis? Of course. So I had two questions. This is Dave, by the way. The first one was about your longitudinal surrogates. Are they, so how many time points do you need? And do they have to be balanced, so measured at the same time for all of your subjects? Yeah, so the theoretical work assumes you have some sort of a grid, but Have some sort of a grid, but you don't, there's no minimum or maximum, really, number of measurements. And so, we do some ad hoc stuff to fill in if you have sparse measurements. So you only have two or three time points per person and they're at different, they're not all measured at the same time. There are like pretty standard methods to sort of like fill that in. And presumably, a lot of the asymptotic results go through, though, I'm not sure. Go through, though I'm not sure I believe it. So that's the there are certain some sort of standard stuff that we do, and it seems to work fine in simulations. Okay, so just with two or three and like not equally space for each patient still is okay. Okay. Maybe not two or three, but it's definitely dependent on the number per patient. And I think, you know, once you're getting up closer to five or 10. Closer to five or 10, then it's starting to work better. I see. Okay. And then a second quick question. So you showed in your second, the second part of your talk, the high-dimensional results, your sort of double machine learning estimator worked a lot better than, for example, ours. So there's a lot of machinery there that I don't quite understand. I was wondering if you had just an idea of what the core reason was, that it worked better, you know, had better distributional properties. Distributional properties and had smaller confidence intervals? Yeah, I think that there's a few things. There's interactions between treatment and the surrogate, which you actually talked about in your talk, but I didn't think was implemented in your package. But then there's also inclusion of covariates to control for confounding, which I believe your package doesn't do. And there's also sort of like model misspecification. So like deviations from linearity. All three of those things can. Linearity. All three of those things can break the strictly linear model-based approaches, and that you know, being able to take advantage of the superleaner seems to buy us something. I see. Okay, awesome. Thank you. All right. Well, thanks, everyone. I just want to thank again all our three speakers, and we're going to go on a break until 11 Central. So thanks, everyone. Thank you. Thank you. Hi, to the Zoom participants. Please wait a second. I'm going to take a snapshot to attach to the group photo. So