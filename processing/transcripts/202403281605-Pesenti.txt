Descriptive says optimal transfer divergence is induced by sparring function, and an alternative title could be Is there an alternative to the buses fan distance? And it kind of bothered me for a long time, so why do we use the buses fine distance or particularly in application mostly the two buses fine distance? I think mostly or kind of a lot of motivation is also kind of mountainous tractability. And then the thought was: is there an alternative? Is there an alternative to the two bosses time distance that we can work with and that gives us something else than the two bosses time distance? Right. Okay. So here I just wrote down kind of the simple version. So this work is going to be on the VL line. So I only want to work on the VL line. And then of course we can write the two buses line distance as the L2 distance. Distance as the L2 distance between the contact function. And as we know, there's lots of different applications on the bus stand distance in general. And we've seen kind of works on worst-case risk measures. You can do uncertainty quantification in the sense that you start with a reference distribution and then you look at deviations from that distribution. So, but in many applications, if you look at the business fair distance, this is symmetric distance. So, in symmetric distance, So it's symmetric in the sense that it penalizes all different parts of the distribution equally. So especially if you do kind of financial risk management, if you think you have a baseline distribution which is your P ‚Åá L, then you might want to penalize gains and losses differently. Or if you do treatment effects, maybe you have, you say, okay, I care about one tail of my distribution much more. And of course, the buses then. And of course, the vast land distance doesn't distinguish between left and right necessarily. And this is kind of the idea that that came from. Is there a way to do something that is asymmetric? That I can do saying, okay, I have a distribution. Let's say I'm in a financial setting. I really care about the tail that describes my losses. And can I then infer a distance on the space of this? On the space of distributions, that actually kind of is tailored to what I want to do, or where I want to quantify my uncertainty. So, this is kind of the idea that we were looking into. So, this is kind of to summarize the motivations that we have. So, is there an alternative to the particular two-vaste distance? And of course, we want to stay in the OT literature because of the also of this workshop. But, of course, we want This workshop, but of course, we want to keep the nice properties of optimal transport divergences. But the key part here is really we want something asymmetric. So what we're going to do is we're going to choose not a cost function that is asymmetric. Of course, we want to be able to do something that we can work with. So we'll be able to compute, so we calculate, actually, get a formula that is computable. And again, this is in R, right? So as long as we can find the optimal coupling, we can. Can find the optimal coupling, we can actually calculate this, and it should have some interpretation from either a statistical perspective or a risk management perspective. So, the way that we're going to do this is we're going to connect a couple of different literatures. So, we're going to connect the OTI literatures with risk measures that we've seen kind of in this week quite a lot, and also kind of a notion from statistics that probably many of you already know, which is called elicitability or kind of the theory. Or kind of the theory of strictly consistent disport functions. So it's, I kind of, what I quite like about this is that it comes together and uses these different properties. Yeah, so of course I put this slide here just to fix the notation because we want a slightly different notation. So this is a very classical setting. And again, I'm looking at distributions of the view line. And what I'm going to do. Line and what I'm going to do here, of course, if I choose the cost functions to be this, I recover the buses, that distance. And the idea here is: well, are there kind of natural ways of choosing a cost function that gives me something asymmetric? And the natural way that we thought is natural is kind of borrowing something that is well studied in statistics. Right. And these are what is called scoring functions. So, what we're going to do is we're going to look at So, we're going to do is we're going to look at the statistical literature where it is kind of a plethora of kind of literature on elitibility and scoring functions. We're going to use those and choose them as our cost function in the optimal transport call. Great. So, for those of you who are familiar with elisibility, I'm going to talk a little bit about statistics here. So, what are scoring functions in statistics? So it's a very big field. So I'm just giving a slide on a motivation why statisticians look at scoring functions, and then I'm going to do a slide on what is actually the mathematical definition. So let's assume you have some observations kind of y1 to yn of a random variable to y. So, and then in a statistical prediction is you want to forecast a functional. So as well, you kind of in a simple setting and say, okay, we have a Let's say, okay, we have all is on the real line, so univariate random variables. So my functional t here is a line varied functional, so it only depends on the distribution. So I can write t of f. So it's either the mean, a quantile, or a risk measure that we want to kind of estimate from the data or predict. And one question is, and statistic is, okay, let's say I have a model A, right? So my predictions for this functional is my set one. This functional is my set one to set n, right? And maybe someone else has another model which might be better and gives some different predictions of that quantity. And then the question is, which model is better? I mean, it's a classical question in statistics. So what they do is saying, okay, you should actually look at a loss function or a spore function, s, and compare the average loss. So here, what you compute here is you compute. What you compute here is you compute the empirical loss of your score with model A and compare it to the one with model B. And the one that has the smallest loss is the better one. And of course, what is crucial here is that the score has to be related to what you actually want to calculate. So you cannot just randomly pick your functional that you want to predict and the score. So they have to be kind of linked together so that you can actually say, So, that you can actually say the one that minimizes the score is a better model. So, this is kind of one of the examples that statistically use R function in this. There's really a lot of areas that this comes into. And the one that you're probably familiar are with R regression. So, if you've done any regression, use scoring functions. We'll go into that in a minute. So, mathematically, what does it mean? So, you have here a functional t. So, you have here a functional T, so that goes from a subset of a probability measure to a set A, which is a subset of R, which is field of R. And so, what this score does, it takes in your estimate, compares it to a realization, and gives you a value, a non-negative value. And the idea is, the smaller the value, the better your prediction is. Right, so then there's just some couple of properties, right? So, you say you have you've started with your functional. You start with your functional, and then that score is consistent for t if the following inequality holds here. So, on the left-hand side, you calculate the expected score for the true estimate. So, t of n will be the true value. And so this is the expected score if you actually predict it perfectly. And you say that this expected score has to be smaller or equal to any other arbitrary set, which is an arbitrary set prediction. So, really. Prediction. So really, S is consistent if the expected score under the true prediction is small and scaled. And there's also a notion of strict consistency. That's if the equality in this equation only holds if z is equal to t of n. So this is kind of the notion of scoring functions. And what we can do is we can. What we can do is we can say, well, in statistics, there's a kind of there, a lot of people are interested in, okay, if I start with a functional, can I say if that, if there exists a sporing function that is strictly consistent for that functional, and once you have kind of a, what they say is that a functional is elicitable, if there exists a sport functional. So that this works, which means you can actually do model comparison and forecast. So if you have a functional that is elicitable, then you can write it as a functional that is a listable, then you can write it as an argument of the expected score. And of course the one that we're very familiar with is the expected value point. And this score is used if you want to do, for example, regression. So here's a little table of functionals that are elicitable. So of course we know the quantiles like the median, mean, the value at risk, any quantile is elicible. And then we have like quantities like the variance and the expected shortfall that are not elicitable, at least not. Elicitable, at least not in the strict sense of this. So, what they have is there's an extension of, like, let's say, k-elicitability. So, you can only elicit jointly the mean invariance. So, if you elicit them both together, then you can do that. Okay, so here's a couple of graphs of how this scoring function looks like. So, I plotted here, for example, it puts a zero, so you predict zero. Predict zero and you plot kind of how much you penalize if your realization y is away from zero. So the blue one would be the square loss that we're familiar with. If you look at, for example, the 80th quantile, then you see this gives you something asymmetric. And what you can also plot is like the alpha by the expectiles and 0.8 expectiles, which looks a little bit different. So these are kind of illustrations of sport functions. One of the motivations of this work, as Stephen actually already said, was when I was talking to Caleb Leonard, and they were kind of excitingly telling me how the Bregman-Wasterstein divergence. And I was like, okay, this is kind of nice because it's a nice symmetric atomic distance. So of course, if you choose the scoring function to be the Breckman divergence, then you get the Breckman-Muslim divergence. So and here, what the Breckman divergence is Here, what the Bergman divergences are, there are the scoring functions that elicits the mean function. And this is a well-known result from Knighting that says any, this class of strictly consistent score for the mean are given by the Berkeley project. So, maybe a kind of a small note is that here, if you look at the arguments, I'm swapping them, this is the statistical notation, and this is, if you use this, then you. And this is, if we use this, then we will get the Berkeley-Mussel standard regions in the OT literature. So, this is kind of a little difference there. So, here, what you can do, here, there's different scores for the mean. So, you can, the blue again is with the squared mean, but you can do something like homogeneous scores, which would be this yellow dotted line or the red dashed line, and you can also do something what's called Poisson or gamma deviation, as used in statistics. In statistics. So, what we're going to do here is we're going to, as I said, we're going to use the scoring functions that are relevant in statistics and plug them in as our cost functions. And then the question is, can we actually compute this in FEMOM? Can we find the optimal coupling? And then can we therefore use them as a distance between distribution that informed by statistics? So here, the idea. So, and here the idea is really: if I have an elicible functional that I'm interested in, then I can choose this foreign function that elicits this functional, and then I can choose this foreign function to kind of give me a distance on the space of distribution. So, actually, I connect what I'm interested in optimizing with the choice of uncertainty that I'm going to create. So, this is kind of the motivation that we have here. So, some properties that are So, some properties that are kind of quite quickly to see, so it's non-negative because the scores are non-negative. And of course, it is zero if you plug in the same distribution. The other way around is not so simple. You need some more assumption on the scores. And of course, what I'm hiding here in the notation is that this here is different for any choice of the scores. So we've seen that the expected value has infinitely many scores, right? Any Gregman divergent elicits the mean. The mean. So here, any function, if you start from a functional, then you have infinitely many scores and therefore infinitely many distances, right? That you can create. And the question that we're interested in here is, what is the optimal couple? So this is the Bergman-Masselstein divergence. And as we've seen, and Stephen already mentioned, is that the optimal coupling is the common atomic coupling, at least in dimension one. If you want to know what the optimal coupling is, If you want to know what the optimal coupling is in our end, you have to talk to Letter Lake Hill. They will be able to tell you. So, what is kind of interesting is that this optimal coupling, the comatonic coupling is the optimal coupling for any choices of the Red Man divergence. So, this is kind of quite an interesting result. So, any choices of scores that is consistent for the mean gives rise to the comatonic coupling. And then, here you can actually rewrite this. And then here you can actually rewrite this in this form, which is what I meant with it's computable. So you actually calculate the ethereum, and once you have data, you can actually calculate this. What is P? P of X? P of X? Oh, that's from the Bregman divergence. That's here. So the Bregman divergence is induced is the. So any convex function phi describes your Bregman divergence. Yeah. Yeah, didn't mention that. Sorry. Yeah. Didn't mention that, sorry, but yeah, yeah. And if here, if you choose phi to beta squared, then you may provide a Wassenstein instance. Yeah, so okay, this is for the expected value where we already, you know, we get the von-Breggman Wassenstein divergence. And so, what about other LSB functions, right? The other thing that we can think about is quantiles, right? So, it's known that the class of strictly consistent scores are given. Of strictly consistent scores are given of this form. So here, any increasing function gives rise to a score that elicits the quantiles. And what is interesting here, you get this penalization. So if y is smaller than z, you get 1 minus alpha, right? And then the other, if it's larger, you get the alpha component. So you have a really asymmetric way of penalizing the distance between the distance between. You know, the statistics between prediction and realization. So, again, here, if you plug this into the OT problem, right, so you can say, okay, what is the optimal coupling? And what happens is that the comatonic coupling is optimal. And again, for any choices of scores that elicits the value address or the quantile. So, this you can also generalize to kind of lambda quantiles or other words. Or other more generalized contracts. So, there's also a notion that I think is more popular in statistics and recently also got popular in risk management, which are the expectators. And they're actually, I think, from the originally defined in the 70s in statistics. And they're defined through elicitability. So there's not a nice closed form solution, that kind of formula for. Solution, that kind of formula for expectants. So you have to actually solve an equation to get them. And there's a generalization from the quantiles. So the class of strictly consistent scores for the alpha expectile is if you choose, you take the Bregman divergence, so you can choose any strict convex or strictly convex function phi, get a Bregman divergence, and then you have this azimutric part on on the left or the right side. On the left or the right side. So, if you see that it's a generalization of the alpha quantile, it's because in the quantiles here you have just increasing functions and now you're kind of penalized with quadratic function or quadruplex function. Here what we find is also the chromatonic coupling is optimal in this case. So, in many cases, we find that the chromatonic coupling is optimal. And again, that's for any choice of That's for any choice of phi of the scoring function. Yes. But some are these supermodels are super modular. Some of them, yeah. Some of them are. Yeah. Most of them. Probably need all of them to be zoomed out, but otherwise the component copy should not be called. Yeah, but there's a lot of scores and not so but modulo. Yeah, if you've if you look at the relative errors, for example. In that case you should be able to find something that has higher Find something that has higher or smaller error than components. Yeah, I mean, here this is submodular, that's right. That's why you get the common atomic company. Yeah, I'll show you an example where we don't have the common atomic component. But there's a lot of scores that are not submodular and that we haven't necessarily found the solution. You can also do what is called short-full risk measure that we find as this infemon. Risk measure that we find as this infimum, and they also kind of elicitable with this specific function. So here, L is increasing and non-constant, and the local increase smallest value that makes this negative. And then again, here, you can again get the comatonic properties. So, now to the question: are there any kind of distances or divergences where these? Distances or divergences where the comatonic coupling is not optimal. And so, this is their work, this result is inspired by Osman's principle, and it's in statistics sometimes also called Osban's validation principle. And it's very well known, this principle. And it basically says, if you start with an Elizabeth functional, can you use that to construct new functional that are also Elizabeth? So at some point, I was like, okay, I'm going to read this famous paper, but then I realized it's actually not a paper, it's his PhD thesis. Actually, a paper, it says PhD thesis. But luckily, I didn't have to read too far because it's on page 9, which is still in the introduction, and it's a footnote. It's not even a statement, it's a footnote. So we're on the left though. So it's kind of quite remarkable that a footnote was a famous. So what does this say? So you start with a list of functional t tilde that has a score s tilde, right? And then you're interested in monotonic transformation. And then you're interested in monotonic transformations. So g here is strictly monotone. And then what Austin says is that, well, t is elicible with this function. So you just kind of transform the scores and then you get a score out of it. Right. So if you assume that in our setting you choose a score that is in the optimal transport problem, the common tonic coupling is attained, then we can use. We can use this one here, the score, this transform score, as our cost function. And then it depends on what monotonicity G has. So if G is increasing, then we get the conatonic coupling. If G is decreasing, then we get the antitonic coupling. And that's the result here. So if you have a decreasing, you get the antitonic coupling. And examples of that is if you want to look at kind of the inverse of a risk measure of a quantity. Of a risk measure of a quantile. Great it. Kind of a nice result that I find is if you start with any coherent risk measure, so you let T be a coherent risk measure that is elicitable and that is normalized, then you choose any strictly consistent score, S, then Then the optimal coupling is the humatonic coupling. Only includes expectiles, right? Yeah, these are the expectiles, yeah. So we could just say for expect couples it's true. Because that's the only invisible coherent. Yeah, but it's I think I think the connection goes a bit deeper in the sense that it's just not so expectile. My guess is that if you have some type of translation invariance and monotonicity, Variance and monotonicity, then you will get an elicitability, then you will get the chromatonic coupling. So that means you prove this without using that fact that it's actually expected. No, no, we use the fact that they're expectiles, yeah. Yeah, yeah, yeah. But I think you can generalize this result to an extent. Because if you look at the spores, as soon as you have some, there's a very interesting result also by Steinwart on that. Steinwort on that. If you have some properties that are not necessarily all of the coherent ones, so if you just have translation invariant and monotonicity, then it implies specific structure on your score. And I think you don't need coherence. I think monotonicity and translation invariant will be actually enough, or maybe some other properties, to get some type of submodularity of the sport. If you start from a non-disciplinable measure, say, spectral risk measures are not, right? Spectral risk measures are if you write in terms of the gamma, and if the gamma is a step function. Expected shortfall. Well, I mean, they're k-elicible. That's right, that's right. Yeah, yeah. They're not one elicible. The question was: can we compare them in the way that you showed in the beginning? Yeah, so. Yeah, so that's kind of one of the open questions that will be very interesting, is how to generalize that to multi-dimensions. And I think there's two directions. One is you mentioned, is if you have joint or ellicible with more than. But what then happens is that let's say you have two elicible, the value trace can expect the choice. The value dress can expect the chart. Then you have a score that depends on three components. So you have like Z1, Z2, and Y. And in the statistics literature, you would basically, the first Z will still give you your prediction of the model, and then Y would be realization. So it's not quite clear how you would define this as a scoring function, of what should you actually specify the first two distributions as the same? Or so it's for me. Or so it's for me, it's not quite clear how you translate that statistical property into defining a score. I mean, the most natural one would be looking at elicitability of a vector of functionals. For example, the expected value, if you look at a vector of expected values, is elicitable, and these are the Bregman averages in RN, right? So there, we already know what it is, but you could think about doing that for quantum. Think about doing that for quantiles, right? But then in multi-dimensional is always a bit more challenging on what is actually the definition of a multivariate quantile. So, I mean, this is still kind of an open question. And that's a good point to kind of conclude. As I said, I mean, there's still quite a lot of open questions that I'd be interested in working on. I want to say kind of the multi-block. On, one is the kind of the multi-dimensional parts, but also how does actually these, if you would choose now one of these divergences and create like uncertainty sets, how does they actually look like? What distribution lie within them? And how are the difference to the muscle span distance, for example? And I mean, the key point here is really: if you say you're interested in a functional that is elicible, maybe you should tie the way you measure the uncertainty or your deviation tied to what actually. Your deviation tied to what actually the objective that you're trying to connect objects. Well, thank you very much, and I'm looking forward to maybe one teaser from the hike yesterday was a slippery sli slippery occasion. Yeah. I seem to remember, but I'm pretty sure I'm wrong. But anyway, I seem to remember that the essential job that the cohomanotomy properly is equivalent to supermodular construction. So I guess then when you consider, you know, construct your square room, then you can just check super modularity on some modularity. Yeah, there's a reason why this is a short term. Yeah, there's a reason why this is a short paper. Because the proofs are not. I think the novelty of it is the idea of bringing scoring functions and cost functions and creating this and combining that. It's not, you know, the contribution are not necessarily proofs. I think that's why that's interesting in the economics are the placements. Whenever we talk about energy transport costs, and people always ask how can we choose a cost function. Always ask how people choose a cost function, right? Why are you creating calls? Maybe this could be anyway. So, thanks.