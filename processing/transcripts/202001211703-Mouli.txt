Uh so uh this is joint work with my advisor Russell and uh Tonik Taft. Uh so an important open problem in proof complexity uh right now is uh uh lower bounds for AC0P Freighter which is the proof system that works over constant depth circuits with uh modular counting gates. And uh so in the circuit world uh we have lower bounds for ACCROP circuits uh through Rasporov and Solensky and Through Rasporov and Smolensky, and so they use algebraic methods for their lower bounds, and so that pointed towards algebraic proof systems. And so, one of the first algebraic proof systems, Null Stilnthats, was actually introduced to show that it can give lower bounds for AC0 for counting axioms, which is a weaker notion than counting gates. And polynomial calculus was introduced as a generalized. Polynomial calculus was introduced as a generalization of null cells at so here is just a quick overview. So a tautology is represented as a system of polynomials P1 to Pm or over a field. And essentially the system lets you work in the degree D pseudo ideal which means that given two polynomials you can derive their linear combination or you can multiply by a variable as long as you don't exceed degree D. And the And the goal is to derive a contradiction. And the size of the proof is measured in terms of the total number of monomials. And we have strong degree and size lower bounds for grass boundaries for PHP and sighting. And so here's a quick overview of semi-algebraic proof systems. So these since lines in these are thresholds, they are not believed to be in ACCROP Frega because In S0P frega, because again in the circuit world, we don't have that majority in S0P. So here is a quick intro. So in cutting planes we have a system of linear inequalities and we want to derive a contradiction using the linear combination or multiplying by a constant or dividing by a constant and rounding up. And in positive cell tax calculus which is a generalization of sum of squares. generalization of sum of squares it's a dynamic system. So you are given a set of non-negative polynomials and you again want to derive a contradiction using linear combination or a multiplication of non-negative polynomials or just introducing a square and accepting that it should be positive. And so now back to the question of AC0P Freud bounds. So right now the state is that we have strong bounds for nulls and that's Strong bounds for Nelson rats and polynomial calculus, but they have not been pushed to a bounds for AC0P frequency. And so the natural next step would be to say, okay, let's try to strengthen these systems, give them more power, and maybe lower bounds for those will give us more insight. So and the main message of our work here is that this approach may not work because some very intuitive ways of generalizing these systems Generalizing these systems are much more powerful than AC0P frame, or they do things which we don't believe to be in AC0P frame. So what is the natural way to generalize polynomial calculus? So there have been many different generalizations proposed but many of them are not Cook-Recall systems which means that it's not known whether the proofs in these systems are verifiable and deterministic. In these systems, are verifiable and deterministic polynomial time, so they usually require the use of polynomial identity testing. And so, here we focus on generalization by Grigoriev and Hirsch, which is Cook Recovery. And so, the idea here is to introduce extension variables, which are functions of the original variables, xi. So, I'm going to use x size for the original variables, and by z and so on for the extension variables. Variables. So, so, so, for instance, you can introduce extension variable y, which is a linear combination of the original variables, and so you can introduce a second extension variable, which is a product of these yi's and so on. So, this is just working with circuits. It's equivalent to working operating with a straight line program. Yeah, yeah. Yeah, I mean, this recursive definition is very. Yeah, I mean this recursive definition is very uh reminiscent of uh yeah certainly. So uh and uh so so the system depth GPC uh we define it as uh uh allowing extension variables which uh have up to d minus to alternating layers of this sum and product. Uk. So, and the proof line any proof line would be a depth d arithmetic circuit in the original variables. So, say these variables are d minus 2 each, so the whole line would be a depth. Each. So, the whole line would be a depth the arithmetic circuit in the original variables. And the size here we measure in terms of the number of monomials in all of these variables, which is again how you would measure size of straight line programs, for instance. So, what is known so far about depth3PC? So, we know that depth3PC is quite powerful. So it is a bit of notation. So I use A arrow B for proof systems A and B to denote that B effectively simulates A and I will clarify what is the effective simulation in a bit. So when Grigori and Hersh introduced this system, they showed that depth GPC over FP simulates depth order D AC0P frequency. AC0P frequency so here the key thing is that these two depths have to scale together so proportionally so and they also showed that PHP and sight in tautologies are easy for a depth 3 PC over appropriate fields so PHP would be over say rationals and sight in would be over a field which contains the right root of unity for instance so so and So, and finally, Raj and Samarit showed that cutting planes with bounded coefficients can be simulated by depth 3PC over the rationals. And so, here first result, so we show that depth 3PC is in fact much more powerful. And we yeah, sorry? A fortius in the fortis it's a type oh uh no it's not uh so uh so uh we show that in fact depth 43 PC can simulate uh cutting planes and some of this so so the the the upper bound on the constant is 43 uh and uh uh yeah so uh Yeah, so uh sorry any any more comments? Uh so and this is over any extension field uh which is large larger than the uh bit complexity of the proof lines. And by bit complexity I mean uh the number of monomials times the bit length of the largest coefficient. So it's just how you would naturally represent a proof line. So and so here I again want to make So here I again want to make a comparison to Ido's talk in the morning. So here we start with the same idea as them is that if you want to do semi-algebraic reasoning over algebraic systems, you want to move from the polynomial to its bit representation. So in particular you want to say that its sine bit is 0 for instance. That would say that the polynomial is positive. Is positive or non-negative. But there are a couple of key differences. So the first difference is that they consider simulation in IPS because IPS is a really strong system. So they are concerned with going from this representation to the bit representation within the system. So they are concerned with deriving the bit representation of the polynomial from the polynomial inside the system. And so essentially they show that the binary value principle is both necessary and sufficient for making the switch from the polynomial to its bit representation. Whereas since we work in much weaker systems we don't want the burden of switching these representations within our system. So here we assume that this switch is made offline, which means Which means this system is already given the bit representation of the polynomial to begin with, and that is what it has to refute. And this is essentially the idea of effective simulation where you say if you want to refute a tautology in a system, so before representing that tautology in the new system, you are allowed to pre-process it a bit and represent it so that the new representation is morally equal. The new representation is morally equivalent to the original one. And the other key difference is: so, once you've obtained the bit representation either online or offline, so now you have to actually use it to prove stuff inside the system. And here, again, since I have a really powerful system which contains Fragor and extended Fragor, so this is doing bit arithmetic in IPS is less of an issue. Is less of an issue here. But for us, since we work in this weak system, it's really the most non-trivial part of our work comes from showing that you can do bit arithmetic in constant depth. And so these are the two key differences. And so for this theorem, there is an alternate proof known. So, there is an alternate proof known in the literature for the case of cutting planes. So, it is known that cutting planes simulate polynomial threshold, propositional threshold calculus of bus and load. So, this proof is due to cry check. And since for our next theorem, so we essentially simulate the rules of PTK in say depth 9 PC. Depth 9 PC. So combining these two results, we have that cutting planes is in depth 3 PC. But the problem with this proof is that it's non-explicit and so from this proof you cannot get the value of the simulation like the 43 that you saw earlier. And so this is the source, only source I could find. And so I just want to highlight that they ask for a more direct proof and which is able to. Direct proof and which is able to determine the value of a depth. And so, this is from the textbook Boolean functions and computational models. And so, our simulation is more explicit and gives this bound of quota. What is the cry check load result? So, actually, the cry check result is just this. So, the cutting plane. So, the cutting planes is in this system PTK. So, this is without unbounded work. So, and this is apparently like a sketch of the proof. So, I'll try to sketch the proof of our results. So, first I'll just give an overview of how we simulate cutting place bounded coefficients. Cutting bounded coefficients in depth 3PC. This is a result of RAS and Samrit. So say you have a line which has small coefficients. So you want to, in depth 3PC, you want to introduce a linear form which is the same as the one being thresholded here. And then introduce another equation in terms of this y, which essentially says that y only has to take the values which make this threshold 2. So this is just a big. 2. So this is just a big R saying that Y is only taking the values that satisfying this threshold. And so let's denote this constraint by Y belongs to C, where C is this range of values. So then really the key idea for the simulation is that given two constraints in this form, Two constraints in this form, y in C1 and Y in C2. You can derive in depth 3PC this line which says y is in the intersection of both of the sets. And this happens to be sufficient to simulate cutting planes with bounded coefficients. I won't go into the details here. So now let me try to sketch the proof for the unbounded coefficient case. The unbounded coefficient case. So, again, in the circuit world, it's well known that if you have a high-weight threshold gate, it can be computed by small-depth majority circuits. And so, a tight simulation was shown by Goldman, Hastad and Rasborough for this. But the problem is for any such construction that we want to use, we really need to prove the correctness of the construction. Prove the correctness of the construction in depth DPC. I'll say more about this in a bit, but essentially, what it means is that given the construction, we need to be able to take it apart and kind of play around with it in order to be able to use it to prove stuff. So, therefore, and the issue with these constructions is they usually use stuff like Chinese remindering, which Remindering, which is really hard to reason with in these bounded depth systems. And so we use this other much simpler but maybe less optimal construction by Maciel and Theian. So what is this construction? So let's say we want to compute the threshold of n numbers a1 to an. So first we just try to compute the sum of these numbers in low depth. So let's say each number is represented So let's say each number is represented in binary and we choose a block length of about log n and we split the bits of each number into blocks of size log n. And now for each number we think of it as being made up of two parts, the odd part and the even part. And so in the odd part you have that you only pick the odd blocks and zero out the even blocks. And 0 out the even blocks. And in the even part, you only pick the even blocks and 0 out the odd blocks. So now it's clearly a number is equal to the sum of its odd and even parts. And so the sum of all the numbers is just the sum of all the odd parts plus the sum of all the even parts. So let's look at how to compute the sum of all the odd parts in low depth. So let's say we are summing up all of these blocks. Say we are summing up all of these blocks. So each block is of log n bits and there are n blocks. So the sum is at most n squared or 2 log n bits in length. And so that's exactly 2 blocks. So when we sum up all these blocks, so we never get the carry to the next stage. So we always end up with exactly the right number of bits. And then s so then we can move forward and compute the sum of the next stage and so on. Sum of the next stage, and so on. And because there is no carry, so each bit in this sum can be represented in a brute force way in terms of these. So for instance, maybe, so the last bit would just be 1, or it would be 0 if the sum of all these numbers is even. So you can just write it out brute force because these numbers are really small. And similarly, we can do the same with the even parts. Even parts, summing up all the even parts. And now we have this even sum and the odd sum. And so for the final step, we just use one step of carry save addition to compute the sum of the even and odd parts. And since so, carry save addition for just two numbers is low depth, so we are okay. And now, so if you want to say now you express a say now you express a threshold, you you can just represent all these numbers in say two's complement uh representation and uh carry out this sum and uh say for the sign bit of this just say that it is equal to zero. So that would that would just say that okay the th the threshold uh of this so these the sum of these things is uh greater than or equal to zero. Um okay. Um so uh okay so um so So again, as I said, it's not sufficient that we just have this construction. We also need to be able to use it in the system. And so actually really the most important property we use repeatedly in the simulation is this. So here this S represents the sum obtained using this construction. So this is essentially This construction. So, this is essentially saying that if you sum n numbers using this construction, it is the same as summing the first n minus 1 numbers using the construction and for the last one, just using your usual definition of addition. So, this is essentially saying that if you can prove this in Depth3PC, so that's essentially saying that at any point you can interchange this construction with the usual addition. And I think that's intuitively That's intuitively proving the correctness of this construction in our system. So, because it's saying that you can always swap with the usual addition whenever you want. So, this is one of the key things we needed to show. And we also needed to derive some other basic properties within the system. Some things like to represent multiplication, we just use this construction and do shifted addition. and and do shifted addition uh and uh w we have to again uh prove that uh okay this is actually equivalent to uh multiplication and uh and again uh some arithmetic into complement with the sign bits and so negating stuff and things like that. So so for our other results, so we improve upon Gregoria when her result Even first result in two ways. So, first we show that depth 9 PC would contain AC0Q frequency for any depth, but it would simulate it quasi-polynomially and the constant would go into the quasi-polynomial here. And more importantly, this works for any prime q over this fixed extension Fp. So, once again, so this is. So, once again, so this is kind of going in the opposite direction of what happens in the circuit world. Where, so, in the circuit world, the exact lower bound is that a mod q cannot be computed over FP. So, and here we have this counterintuitive result that for any prime q over a fixed Q over a fixed extension FP, you can do this simulation. So, again, there is a proof of, alternate proof of a part of this result in the literature. So, depth 3 AC0P fraga is known to be collapsed into depth 3 AC0P fraga with a quasi-collominal blow-up. This is due to bus at all. This is due to bus at all. And again, so once we have this result, if you use the result of Grigoria and Hirsch, which would scale the depths proportionally, so maybe it would put depth 3 S0 Frega in some say depth 10 PC. So combining those two, you could get the result, our result for the case of Q equals P. But again, as I mentioned, our result is more general in fact. In the fact that it works for any Q, and maybe it's a simpler alternate proof for the case of Q equals P. So I just briefly sketch the proof of this result. So again, in the circuit world, Alender showed that any AC0Q circuit can be converted to a quasipolynomial size what I call flat circuits. What I call flat circuits, so a circuit which looks like this, which has a threshold gate at the top and mod Q gates in the middle and gates at the bottom. And so Maciel and Pitasi extended this to the proof complexity world when they said, so if you have an S0 Q Freaker proof, we know that you can convert each line into a circuit of this type. But they also build a proof system which is essentially a sequent calculus over these kind of circuits, and then they show. These kinds of circuits, and then they show that you can actually complete the proof, fill in the gaps, and get a proof in that new proof system from any AC0Q frame proof. Okay, and so essentially what we do is we show that we can simulate this proof system of Maysl and Pritasi rule by rule. And so again, the key thing is again the C P star. is again the CP star uh the CP star simulation uh that I mentioned earlier. So we just use that to simulate the threshold and parity queue gates and we also need that to simulate like the cut rule in sequence capital. So and these are just some more of Just some more of our results. So, if you allow the depth to scale proportionally, we can show that TPC can actually simulate TC0 freighter where the depth is allowed to grow proportionally. And finally, for the case of depth 3, we extend the result of RAS and Samarit to say that it can simulate semantic cutting. Semantic cutting planes with bounded coefficients over the rational numbers. And so here are some open problems. So one initial open problem that we considered was to prove lower bounds for PC over the plus minus some basis, but this has already been answered. So you should talk to Dimitri if you want to know more. And so the other problem is proof lower bounds for depth 3 PC. So this definitely continues. PC so this definitely contains resilient but it could be also more powerful and it would also be interesting to prove for a very restricted version of this depth 3 PC. So this is the system where your extension variables are only linear transformations. So i and it's interesting even to prove for this S6 version which doesn't really contain uh doesn't it's not known that it's con it contains polynomial calculus in them. So It contains polynomial calculus again. But the CP star simulation still works for this case. And lastly, so our results need a large enough extension field. So can we do the same over the base field or show that it's not possible? So thank you. So uh you don't uh simulate uh semi-algebraic side. Cutting points and so and like positive self-science, calculus. Yeah, I mean so then it's always but uh it's also can be yeah possible. So how do you know? So we don't have a question, but an ultimate out to the question. Uh how do you get the end uh uh assuming you start with this big bit vector, right? You start with bit vectors and then how do you actually you simulate the SOS for instance or the cutting plane with bit arithmetic. How do you actually get to the uh to the refutation, to the end? Uh so you you just say that uh um You just say that so so for instance for SOS you just get this equivalent of minus one is greater than or equal to zero and so minus one is really in two's complement is the all ones vector and so and since inequalities we were representing just by the sign width so then we would get one equals zero right. So so we we want to say that basically the sign the sine width of minus one sine width of minus 1 is equal to the sine width of okay so is equal to 0. This is the equivalent of so and the sign width of this is just 1 so you say how do you want to make another I just want to say about this uh hike to Social Mountain. So who is interested in