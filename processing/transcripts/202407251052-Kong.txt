That is the code inference method work rather than a real mandatory randomization. So the scenario we consider here, we get actually one simple mandatory randomization with individual level data. So this is dot work with my former PhD student, Sing Yi Zhang, and my colleagues in WoWong and study stuff coach. So as there are a lot of talks talking and discussing about the magnetic randomization, I would About the magnetic randomization, I will just skip this background. So, basically, the magnetic randomization is actually an instrumental variable approach. So, suppose we are interested in estimating the code effect of x on y, so this is the red arrow here, and in observational studies, we have some observed component u, and one cannot identify the causal effect directly. So, instead, one needs to find an instrumental variable c, which needs to set. Variable z, which needs to satisfy three assumptions. So the first assumption is that the instrumental variable z needs to affect the risk factor x. So this effect should be done there. And the second, the z cannot have a direct path to y. And the second condition is that the covariance between the z and the u they must be actually uncorrelated. And so to summarize these conditions, so so basically if the variable is an instrumental variable, Variable is an instrumental variable, it can only affect outcome y through the treatment of the risk x. And if we assume all these relationships are linear, then we can write the factor of z to x to y will be equal to z to x times x to y. So the causal effect of interest x to y can be identified as z to x to y over z to x. So we should call the word ratio. Of course, if we have finite samples, their corresponding estimator is called a two-stage instance. Is called a two-stage system. And so in mandatory randomization, we actually use all generic variants and treat them as instrumental variants. However, so in the real world, most generic variants are actually irrelevant to the exposure of interest. So in the past, one can often just find a few genetic variants based on a prior study or some expert knowledge. But of course, one can also use some performance to find those important. GWAS, right? So find those important generic variants associated with the risk factor. And once we find some SNPs that pass the GWAS test, and there will be a second problem. I think Tenrien has just mentioned it. So for some of those SNPs, those genetic variants will be called invariant due to prairie. So here I just give you an example. So now suppose I have a genetic variance C1 which can affect multiple traits. C1, which can affect multiple traits, including one of the mutes S. So, suppose there is another risk factor where this C1 affects this variable, and this variable also affects Y. If we omit this variable, then Z1 has a direct class of Y. That is to say, the Z1 can affect the outcome Y, not only through the steps. And if we, so this C1 is called the variable choice. If we use a ratio to estimate the causal effect, To estimate the color effect, then the corresponding colour effect estimate will be best. So there are some actually existing solutions to solve this problem, basically to make the valid causal inference with some valid instruments. So these are either statistics and academic origin research. So their ideas are actually quite similar. So here's what their idea is. Here, I just use the example in this pool and absorb. Example means it's full and absorbed 208. So, if an instrumental variable satisfy all those assumptions, it's called a valid instrument. Then, the corresponding ratio will be just a two-call effect. However, if it's invalid, then it will be biased. So, what is their assumption is that for those invalid instruments, the bias are actually quite different. That is to say, the magnitude of the violation of the instrumental variables can be quite different. And it can be quite different for these valid ones. So, as long as these valid instruments, so their corresponding two-color flat actually form a mode, then one can use a mode-finding algorithm to identify these valid instruments. And finally, one can do two-state each value fit to estimate the color effect. So, here is the basic summary of the existing solution if we have one sample measure randomization in individual level data. So, the first step is to use a GWAS to find those important samples. Use a QAS to find solid space. And the second, we have to take the ratio and apply a mode-finding algorithm to identify the valid IBs. And finally, we do a two-state squared. And two-state B squared. So now we actually are forced to use a variant, a toy simulation to evaluate the performance of this procedure. So here in this example, we actually have just 50,000 genetic variants. Genetic variance. And for simplicity, here we do not consider any LD structures, so they are generally independent. And for Z1, V2, they are invalid instruments because they affect both the factor X and also have direct effect outcome 1. For Z3 to V7, they satisfy all the instrumental variable assumptions, so basically they are valid. And we also have V8 to Z50,000, so they are unrelated to the respective S. So here we also just So, here we also just have a single American founder we just generated from a standard normal. And for all these relationships, we generated them using based on linear models. And the sample size here, we just consider only 500. And the two code effects, x to 1, the same as 2. And now we will perform this procedure. So this procedure, actually the GWAS actually contains two steps. So the first step is based on module screening. So basically we just write based on the module correlations. Rank based on their marginal correlation. So, as in kinetics, it's equivalent to rank the marginal p-value. And after we find those significant success, then basically, our second step is to fit a device passed model, basically fit the model jointly, and then we can actually do a joint thresholding and find those important dynamics. So, what we do here is we repeat this step for 1000 multicolored rounds, and we find that on average, there will be around 20. There will be around 21 candidate IPs tests. And the good news is that we always include C12C7 in every multi-color. So all the signals remain after this joint. And then our second step is to try to use a modifying algorithm. So we have like 21 ten IBs, so it's corresponding to 21 IB. Then we just uh use the modifying algorithm to find the mode of the ratio. To find the mode of the ratio and find the corresponding value IVs. And the second, the last step is to use these IVs and do to steady this square with it. And it turns out that the code estimates are biased. So this is actually the average bias across 1000 multicolor rough. And the average bias is negative 0.22 with a standard error of only 0.02. With a standard error of only 0.02. And we also calculate the cover probability of the 95% confidence interval, but it's only 0.07. So there's something wrong here. So you may wonder what went wrong. So recalling our step one, like on average, each multicolor run with like 21 can at least. It always includes Z1 to Z7, right? Also signals. But we also include around 14 Iran ones. So they are. Iran ones. So they are actually sort of force positives. But these false positives are actually not randomly selected from Z8 to Z57. But we have a selection procedure, we have a ransom procedure. So these variables actually cause those variables with Spirit's correlation. So here, the Spirit's correlation here, I refer to a statistic term that there are two population correlation with respect to zero. But because we have a ranking, a selection procedure, there are underlying sample. They are underlying sample correlation with the respector is actually non-neglected. And because of these variables, it actually causes our problem. So to see the problem clearly, what I do here is that, so for each multicolor round, we have 21 ratios, right? We have one multicolor round. So we stack all these ratios across these multicolor rounds together in this sort. Now we actually plot these ratio values. Actually, plot these ratio values. So, for this green colour, they actually correspond to the ratio of those invalid instruments. And for these blue ones, they actually are the ratios corresponding to the value instrument. And for these orange ones, they are actually the ratios corresponding to the variable ones. Now, if we try to find the mode in this histogram, then we can actually see that the mode are actually the orange ones. They are actually corresponding to the ordinary ones. Corresponding to those irrelevant ones. So, this is the reason why we have a problem because we are now actually misidentifying those irrelevant ones instead of those valid ones. So, this is actually a very interesting phenomenon. So, the first question we want to ask is that why all those irrelevant instruments, their ratio are also alike. So, here I just use a simpler case to illustrate this. So, now suppose we consider the case where we have an Of the case where we have an extreme confidence. So here we have a single American configuration and we also assume that the risk factor or the exposure x is completely determined by u. So you can interpret it as x equals u. And now suppose we have a spirit instrument. So this spirit instrument means that this instrument, the corporation correlation between z and x is zero. But the sample correlation due to this selection procedure is not neglected. So we use The non-negative level. So we use a dashed line to denote this non-negative level sample correlation. And because x equals u, so it's equivalently, c and u have a dashed line to have an organic non-egregible sample correlation. Now, since this z is treated as a value instrument and we use this ratio to identify the cause of fact, we know that this ratio is z to x to y over z to x, which is covariance between z and y. Which is covariance between  and y over covariance of z. So, to calculate this, we actually just need to calculate the numerator and denominator respect. So, for the numerator, we actually need to know how many passes are from z to y. So, in this case, we have two passes. One is z2u2x to y and another is z2u to y. And for the denominator, we have it in z2u2x. So, if we take a ratio here, So if we take a ratio here, we can see the ratio is actually x to y plus u to y over u to x. So the first term here is a caudal parameter of interest. But we have a second term, which is a bias, which is u to y over u to x. And interestingly, we can see that this bias is actually the same for all ARAM instruments because it does not depend on how strong this sample correlation between Z and X for all three C and U is. For all three CMUs. So, this illustrates the reason why all those ratios of those iron ones are actually also lacked. And interestingly, this bias is actually also the same if we just perform linear regression y or x directly without considering these almightical models. So, this explains the reason why we have a mode here, because the ratio of all the irrelevant instruments are also alike. Are also alike. So, this is a very interesting phenomenon and actually one of the novelty in our work. So, now we need to actually solve this problem. So, what we know so far is that all the ratios of those value instruments are alike, the ratio of irrelevant instruments are also alike. But for those invariant instruments, they can be everywhere. And also, we know that if we misidentify those irrelevant instruments as varied ones, there will be a severe problem because there will be a bias. Real problem because there will be a bias in causal evaluation. So, in order to solve this problem, we need to distinguish value instruments from irrelevant. So, to solve this problem, we actually consider a pseudo-variable approach. So, I believe yesterday there is also a talk related to the pseudo-variables. So, there are different ways to generate pseudo-variables. Here, we provide one-way, simple way using permutation. So, suppose we have sample size n and we have p generic variables. So, we have a dynamic matrix D, which So we have a dynamic matrix D, which is by P. And then we randomly commutate the rows of the C, we end up with another dynamic matrix, C star. Then we put C and D star together and we perform GWAS on the exposure on the C and C star together. And after GWAS, there will be actually four groups of several left. So for the first three groups, there are actually valid, embedded, and irrelevant instruments in the original CE. And for the last group, it will be all those irrelevant. Group, it will be all the relevant instruments in this star. So the reason is that for this star, because we actually shuffle their samples, right? But for the X, it's the sample in still a period of order. So we actually destroy the relationship between Z star and X. So basically, all these Z stars, those variables, are irrelevant to X. And if they pass the GWAS, then they are still irrelevant. So the good news is that for all those stars, they are generated by. For all those stars, they are generated by ourselves, so we can track them. And even if they pass through us, we can still track them. And another good news here that for the ratio in this group 4 and the ratio in the group 3, we have shown that they are actually alike. So based on the behavior of the ratio in group 4, we can actually know the behavior of those ratios. So here is what we do here. So we still have a crystal graph here, but now we add this gray part, which are correct. Gray path, which corresponds to the ratio of those group four. Okay, so now what we essentially do is that we calculate the range of the ratios and then we remove all the ratios within this range. And after we remove this, we can try to find a mode which will be the blue one, and then we can use a two-state blue square root fit to ascent color. So this is our methodology. And we of course we provide some theoretical. Of course, we provide some theoretical theoretical guarantees for this phenomenon and the procedure, but at time constraint, I will just skip it. So, now I will simply go through the data application. So, now we are interested in whether overweighting people will affect the quality of life. So, basically, we consider all of the BMI value grades are equal to 25, which is the overweight group. And we also have a HUI3 index, which is a self-supported index. The higher the index is, the better the light. The light resposal fields. And we first perform a crude analysis where we just regress the outcome on the exposure while just filling for the result of funders, H, Jenna, and HP. And then the result is that the code effect is negative 0.011. And then we perform our procedure. So basically, after pre-processing, we have about 3.7 million SNPs. And then in this case, we actually adjust for one more appropriate. Just for one more observed confounder where we observe the formulation specification. So now we actually perform our procedure. So what we do here is that we have a V matrix which is 3000 times 3.7 billion. So we actually random perimeter load end up with another V star that will form 2 us. And after 2 us there will be 44 SNPs in Z left and 42 in the V star. So here what we do here is that we plot their ratios. And on the right Their ratios. And on the right-hand side of this yellow dashed line, they correspond to the ratio of a pseudo-quariant. Then we take a range, which are these black dashed lines, and we remove all the ratios within this window. There will be four left. And if we try to find a mode, the mode is here, and finally we expand code effect, which is negative 0.39. And we also compare with the result where we first do a GBAF X on V and perform the mode of sign. And perform the mode of finding algorithm directly. So it's the corresponding code effect is negative 0.408. And to further support our finding, we actually perform the GWAS or S and Z and Z star together. Okay, so in this case, these Z stars are URL-wise. And then we apply the modified L. So in this case, the code of FET is like this. And however, in this case, we know that for all those points on the right-hand side of this red dashed line, they are actually direct. This red dashed line, they are actually irrelevant because they are from this data. But if we use a modified element to find, we can identify these points as valid, so actually they make a mistake. So this actually shows us the code effect estimate here is likely to be wrong. So here is just a summary of the comparison. So our proposed method, the core effect estimate, is like 1sp, but for all the previous comparison method, including OLS, the corresponding code. What we call the fat has made is 0.2 SP. And what we also know so far is that this method makes a mistake. So this indicates that call the fat aspect here is likely to be wrong. So this means that these two are also likely to be wrong. Also, this coincides with our previous argument that if we ignore those various instruments, the corresponding colour estimate will be similar and doing our estimate without adjusting for the American function. Just before the American point. Of course, we have done extensive comparisons, say basically with those MR methods, such as you first do some LD clumping, and then you can do those MR, perform those MR methods. And their results are all similar to yours. This is because they have those false positives. So finally, I will end up with this discussion. So here, we actually find these spirit IVs. Spirit IVs is actually a severe problem based on these false positives. And this phenomenon is actually very severe in small or moderate sample-size scenarios. This applies to the case where we want to answer those causal questions within specific subgroups. And of course, if we have a very large biology study with sufficient large sample size, this type of problem may not be severe. But of course, if you apply our method directly on that, it's still fine, you will yield a similar result as those traditional MR methods. Traditional MRMS. And also, our work is actually connected with those instruments of very well as well as a time constraint. I won't talk about details. So, basically, here is a summary. So, the MR is actually a powerful tool for colour distribution. So, we first, there are some challenges in MR studies. So, we first need to find those IVs. We can use GWAS, and we need to deal with priority, then we can try to use those modifying algorithms. But if we combine these two procedures together, If we combine these two procedures together, there will be extra bias due to those spherical variables to our study. And this spheres IV is actually a potential serious problem because ignoring it has a signal effect and ignoring those harmonic functions. So we actually propose a procedure, five noise-wide noise, use the variables create for this first device. And finally, here, of course, I said our method is actually now only applicable to the one sample, and then the minimization will be very. Magnetic minimization with individual level data. But in, say, if you have a two-sample magneticization with summary-level data, actually this spherous IV is still dangerous. So you will observe a similar phenomenon if your sample size is not sufficient large. So as long as you believe there is some potential false positive, there will be a problem like this. But of course, the way to solve the problem is more challenging. So yeah, we will add even false. So, yeah, we will add even for the future search. So, that's all for my talk, and here are some references. And our paper gets posted on Archive T. Thank you. So, we have time for the question. Yeah, so I'll just follow up with your last comment. So, have you actually tried to two samples? Because my intuition is it should be less of the problem. Because you don't have this U influence X or what exactly the same. So, if you have a two-sample, the phenomenon is like this. So, of course, this is theoretical, yeah, if you have a lot of possible. So, actually, I didn't show the little theory here. If you have one sample problem, so spherous one, they will actually concentrate around the OLS aspect. But in a two-sample problem, so spherical will concentrate on zero. It will be zero. Basically, yeah, if you have a spherical one, the corresponding ratio will concentrate. The corresponding ratio will concentrate on the phase. But in some sense, it's good. It's attenuation, right? It's less post-positive. Yes. But the phenomenon is different. Hopefully, quote questions. I really like the idea of how you do it with these logarithms, I think. But in the example you show for an AMI, so you have this two, you know, after you remove this hand, then you have sort of like two plots, like one for invalid, one for valid. For invalid or for valid. But both seem very smart. So how decided which ones are valid and which ones are invalid? Okay, yeah, this is a very good question. So here, we actually rely on the assumption of the previous literature where they believe that those two IDs, right? They actually will form, the corresponding ratio will form a whole. But of course, in reality, whether this is true or whether in reality, you can really find it small. Here, you can really find it's more dependent on your noise signal-to-noise ratio. So, yeah, I want to say this is 100% true. Okay, thanks uh, so 