Okay, let's start again. This is like on the top of the morning. So it's a pleasure to introduce Ivan Campo that will talk about epochoerce exponential decay in entropy for Hamiltonian monitoring. Thank you. Thank you very much, Yulilia, for the introduction and the organizers for inviting me. I'm really happy to be here. Had a fantastic week. Here. Had a fantastic week. Lots of new things I've learned, and I hope I can maybe teach something new today about Monte Carlo methods. So this is joint work with Alain Dermous, Pierre Mamash, and Gabriel Stoltz. They're all in Paris. I'm at Colorado State. And the overview for the talk today, I'm going to go over a lot of motivation, right? It takes until step five to get to my main results, but a lot of motivation just to kind of see what. Of motivation just to kind of see what's going on here, why we care, and maybe what we're looking to prove. And I'll finish with proof techniques if I have time. And so our overarching problem is simply that sampling from arbitrary distributions is not an easy thing to do. I say arbitrary in the sense of very large dimensions, perhaps some very strange behavior in the distribution. And here's a And here's a nice example, just in two dimensions. We're looking at, okay, we're Gaussian, but we have these modification terms that are almost singular, but not quite, not quite singular. So you can think there's a big X in the middle of the X1, X2 plane, which is almost preventing particles from transferring from one to the other within this distribution. And it looks like the following, right? So we kind of have four. Right, so we kind of have four peaks. It would look Gaussian if I didn't have the almost singular terms. But it turns out that to kind of, in some sense, construct an infinite die, which we want to roll and grab, you know, a number in the x1, x2 plane with probability approximately corresponding to these peaks is not an easy thing to do. So, how would we construct maybe 10,000 samples which are distributed according to that distribution? According to that distribution, there's a pretty large class of solutions to this problem, but to kind of motivate this large class of solutions, let's make the problem easier. How would we construct 10,000 samples distributed according to the two-dimensional normal distribution, which is probably a very easy thing for all of you to do, right? This is a very standard process to sample from. You could think about it as the product that's. You could think about it as the product distribution of two independent one-dimensional normals. You could find maybe samples of the one-dimensional normals with the central limit theorem on some random walk or something. Alternatively, you could think about it as the distribution of a two-dimensional Browning motion after one second. But I'm going to pay attention to the bottom part here, and that's going to be the invariant distribution of an over-damped Langevin dynamics. Delangevin dynamics, also known as stochastic gradient descent. So it's the invariant distribution of that dynamics, and that's what's going to be really important for really where we're headed. So we're going to consider this third bullet and say, okay, let's compute 10,000 sample paths of this dynamics. And to numerically compute some samples, we're going to use maybe the Euler-Mayama method. And to do this, you know, you just. And to do this, you know, you just kind of directly set up the Euler-Mariyama method, sample from some one-dimensional normals, and all of a sudden, you know, for a small enough delta t and iterating enough, you get an approximate distribution. So I'm going to show you some examples of this just so we can see maybe what does it look like when these sample paths are computed. So, of course, with one iteration, I've put all 10,000 samples at zero. They start to spread, they start to spread. They start to spread, they start to spread. But this is really important because here we are sitting at 100 iterations. We've moved in some sense forward in time, I guess, like 10 seconds or something like that. But if I modify this from 100 iterations to 10,000 iterations, we really don't see much of a change, right? So, in some sense, iterating this 100 times was good enough. There's no reason to move on to 10,000 times, right? And somehow we want to know. Right. And somehow we want to know what that cutoff should be. Yeah. So I guess that would be a question of how much how much error would need to increase the iteration. Perfect. Yeah. So that's that's in many ways the entire point of this talk is an answer to that question for maybe some more complicated situations. Yes. So going from 100 to 10,000, we really don't see a change. And of course, there's the true normal. You know, I'm able to sample from the true normal just because we. Normal, just because we know how to do that as well. And you know, you kind of see between the three of them, there's not much of a difference. Okay, so this Euler-Mariyama method, because we've discretized time, we actually produce a Markov chain, right? And this is, in some sense, this is what Markov chain Monte Carlo is all about. Can I construct a Markov chain for which I can produce samples from some prescribed probability distribution if I iterate the chain long enough? Chain long enough. So, is there a method which works for maybe more arbitrary distributions? Because what I showed with the overdamped launch of indynamics was maybe a little bit too restrictive. We want a stochastic dynamics which has an arbitrary distribution, as a stationary distribution, and in fact, something where we converge to it over time. And so, this under-damped Langevin dynamics is a really often used choice. And all it is, is it's the sum of a Hamiltonian dynamic. It's the sum of a Hamiltonian dynamics. I've displayed that in red, with an Ornstein-Ullenbeck velocity damping, right, which I've displayed in blue. And the stationary distribution for this exactly corresponds to our potential of the Hamiltonian dynamics. And in fact, it's really just the exponential of the negative Hamiltonian, right? And that potential function u, if I decide to just project onto the x-marginal, that, that. Onto the x-marginal, that is really somewhat arbitrary. That's where I'm going to get my arbitrary distribution from. Is you know, for some sort of a potential function u, I can run these dynamics and get really close to, in the first case, you know, the product distribution of u and v, but then if I project, I can just recover just the distribution I'm seeking. So, Hamiltonian Monte Carlo, which you've probably seen in the title of this, is the approximate. Of this is the approximation scheme to this dynamics that we're going to use, just like we used Euler-Mariyama earlier. So, this method follows a splitting scheme, and then we've got a couple orders of the splitting scheme. We've got a Verlet or leapfrag scheme, which approximates the Hamiltonian dynamics. That's why I've displayed that in red. And then similarly, we have a stochastic velocity update, which approximates the Ornstein-Ullenbeck dynamics in blue. So, we're going to split between Hamiltonian and Ornstein-Ulheng. Between Hamiltonian and Ornstein-Uhlenbeck. And then, in what people will call an adjusted Hamiltonian Monte Carlo, we have a metropolis step where we either accept or reject wherever we've moved at a certain point. So mathematically, we can describe the Verlay scheme as the following. We're going to, because this is just the Verlay portion, I've described it with the initial condition within the Verlay step itself, even though my true iterates. Itself, even though my true iterates are these subscript n's, the Verlay scheme itself iterates through this, what I'm calling time, but it's still discrete, right? And we basically iterate up to some time k delta, where we choose a time step delta and maybe a number of iterates on this single scheme k. And it's called leapfrog sometimes because we're kind of leapfrogging between velocity and then space and then velocity again. Space and then velocity again. So the Ornstein-Ullenbeck approximation is really this velocity damping. So I'm going to preserve x, right? So my next update in x moving to n plus 1 is preserved from the relay step, but now my velocity has been given a damping with eta between 0 and 1, some parameter. And this, you could basically solve the Orange Sein-Uhlenbeck dynamics. Ornstein-Ulenbeck dynamics explicitly, right? Because it's a linear system. And if we decided to approximate some time steps of that explicit Ornstein-Ulenbeck solution, this is what we're going to receive from that. So that's where this comes from. Then the metropolis step is just an accept or a reject where we're going to pretend, you know, we're going to move forward. We're either going to say, okay, are we going to accept what we had above, but we're only going to do that with a certain probability. But we're only going to do that with a certain probability, otherwise, we reject it and we go back to where pre-verlay step everything occurred. Now, in fact, today I won't be considering the adjustment case because it turns out we don't need to consider the adjustment case. So just think of steps one and step two for what we're going to be looking at today. So, as an example of this in action, let's go back to the first one that I started with today, where I have my kind of my four peaks in the two-dimensional plane, and I want Plane, and I want to approximate a distribution throughout those four peaks. And so I went through and actually computed this. And once again, I've set everything at zero after zero iterates, but we move time forward, and what we see is exactly a distribution at the four peaks, right? And in fact, since they weren't quite singular, you can still technically approximate that, or you would get perhaps a single sample in between. A single sample in between the two of them, but it's very, very unlikely, right? And so we do see the true X show up there. Mixture of... I mean, you could argue, yes, a little bit, but I mean, if we jump back to what the actual distribution looked like, it's there it was. Was it's it's really Gaussian centered in the middle, and then that's been split. It's been split by the singular term, so it's not you know, it's not quite the same as maybe a mixture. I think so, but you you can you can show me afterwards and we can talk and see. I would be able to tell you right away, maybe, if you actually wrote it down for me. Okay, so this is our overarching solution to the overarching problem, but maybe the specific problem is what do we mean precisely when we say maybe our time steps are small? What do we mean precisely when we say we iterate enough? We don't want to run our dynamics for 10 years and then get just a perfect sample. We'd rather run it for five to 10 seconds and get something that's. Five to 10 seconds and get something that's very, very good, but maybe not perfect. And we're just content with the little bit that maybe it differs from the true sample. So the way we solve this is with explicit rates, right? We're going to ask ourselves about a quantitative functional decay, which depends on maybe my parameters and the potential, right? And I want this functional decay to kind of describe the To kind of describe the approach of some initial probability distribution to the desired probability distribution mu. And so knowing these rates basically just says, okay, it tells anybody actually running these simulations, you have to iterate 10,000 times or something. And if you want your threshold to be 10 to the minus 3 over whatever this functional is measuring, then you just make that. Then, then you just make that choice and you're happy. So, the solution depends on a few of these definitions. We have kind of this large definition of an F divergence, which basically considers the difference between two probability measures, but not in a metric sense. This is a little bit more vague than a metric. But what it does do, and so maybe I shouldn't denote it with a lowercase D because it's not quite a metric. With a lowercase d, because it's not quite a metric. But what it does is it measures, in some sense, only the difference of ν to some fixed μ. We want to think that the right-hand side of these F divergences are fixed, and then the left-hand side, the input, measures some sort of, it's not quite distance, but think about it as distance. Because what do we require out of the function f? We require that it's zero when the input is one. So, you know, if new in So, you know, if nu and mu were the same measure, then we do get zero out of this. It seems related to entropy. Exactly. So that's where we're headed with the next statement. So this is a very general version of maybe these measures, metrics, almost metrics. You know, they're called divergences for a reason, but some way of measuring the distance between two probability distributions. Now, entropy is kind of the classic way to do this. Some people Way to do this. Some people will call this the Kohlbeck-Leibler divergence or TL divergence or relative entropy. But this is exactly the F divergence corresponding to f of x being x log x. And this is the one we're going to use. We're denoting it with h, but we're also going to use the Fisher information, which is a kind of a modified version of this. The way I like to think about it is that this is the Sobolev AM. Is the Sobolev analog to the above KL divergence, right? We're going to compute the gradients on each of the terms that were multiplied here and take their inner product. And there are two ways of representing this, right? We can truly represent it as the gradient of each of them, but we can also represent it in this sort of a way that really looks much more like a Silvolev norm, right? Okay, so these are particularly well suited to. Are particularly well suited to what I'm searching for, which is non-asymptotic MC-MC convergence rates. And really, the first results were in 2017 by Delalian and then by Cheng and Bartlett. And then more recently in 2022, Mamarsh proved some results for an idealized HMC, where what we mean by idealize is that the Verlay scheme isn't actually used. We assume that the Hamiltonian dynamics can be solved perfectly. Do we still have some sort of decay rates? Still have some sort of decay rates. That's what Pierre Mamarcha approved basically a year ago. And then I decided I wanted to join him. So that's what these results are once we do actually include the Berlay scheme. And something that's really important to note before I get into this too much is that our Fisher information is stronger than KL divergence. Same way we would think that maybe a Sobolev norm is stronger than an L2 norm. And in fact, the KL divergence is stronger than the... Divergence is stronger than the two Wasserstein distance. So we can compare these to true metrics on probability measures, but for many reasons, we would rather work with these divergences. And there's the inequalities, just so that we keep in mind. Maybe I should have mentioned so long as mu actually satisfies a log-Sobolev inequality, right? So the CLS is my log-Sobolev constant on μ. And that's the inequality we can use. And that's the inequality we can use. And so, what does it mean for me to say when I introduced in the title hypocoursive T? Hypocoursive is basically a modification of a Sobolev norm, right? But we want to jump our way out of L2, we want to jump our way out of Sobolevness into entropies. So it's now a modification of the entropy, but the Sobolev version of the entropy, which is some sum of the entropy plus its Fisher information. Plus its Fisher information. And how do we modify it? Well, you modify it by a positive matrix. So this is kind of the key property of hypocoerosive systems is that you modify the Sobolev norm by some matrix, the norm with the gradients in some matrix, and the interaction of some of the gradient terms with each other through the matrix is what gives you a decay rate or some sort of explicit decay. Yes? What does it mean by all? What does it mean by coercive? Yeah, but its true definition is simply that there's a norm, let's say, hypocoersive. The true definition of hypocoursivity is just that there's some modified norm for which a semigroup is, and I'll call this mod has the property of being. Property of being a contraction has a property of being a contraction C e to the minus lambda t has a property of being a contraction eventually in this modified norm. There's some c greater than one here for a hypocercive system, and this is in the modified norm, but in fact, these modified norms. But in fact, these modified norms in hypocorrosive systems are always required to actually be equivalent to the original norms. And so you actually gain, you gain, you gain, oh, sorry, it's truly a contraction in a modified norm, but the modified norm is equivalent to the original norm you're considering. And then by the properties of equivalence, you can get rid of the modified norm with the inclusion. Norm with the inclusion of a constant. So that's generically what hypocorrosivity describes as these semi-groups, which are contractions in some sense eventually, right? It takes a little while for usually noise to spread the behavior of a system to all directions where then in each of those directions we can actually be a contraction. But by Vilani, you know, this is kind of the generic approach is that. This is kind of the generic approach is that you always just, our modified norms are just like a modified Sobolev norm with a positive matrix. Because, you know, how to, it produces an equivalent norm because the matrix, if it's positive, is bounded from below. If the matrix is a matrix, so it's certainly bounded from above as well. And so that's where we get the equivalent norm. So if mu satisfies log Sovolev, just like we were kind of seeing before, actually this entire were kind of seeing before actually this entire this entire entropy that we're working with is actually equivalent to to h the same way that you know we would think in maybe a more traditional hypocroer sense so we want to think that fine this is this is some modified entropy but um but it allows us more freedom when we're working with the interaction of a lot of these a lot of the the trajectories more or less and so the main results are the following And so, the main results are the following. We're going to take any potential, and this can be in any number of dimensions as well. It just needs to be twice differentiable, and in fact, the Hessian needs to be bounded. And we have a few other commonly satisfied conditions, but none of their parameters are showing up in the statement I've made, so I've omitted them here. But what we get is for any n, For any n, and if we talk about the Markov transition operator as p, then if we're looking in the entropy, the entropy of n iterates of whatever my Hamiltonian Monte Carlo is will actually decay with with the following rate, 1 plus kappa k delta to the minus n. And then here's maybe a version of the modified entropy. But you can imagine with enough choices of n, we. Choices of n, we make this very small, except for maybe some quartic dependence on delta, right? So here you can tune delta very small if you want to. Unfortunately, that term never disappears, but you can tune delta very small and then you iterate enough and you know that the difference between the two measures is very, very small. In particular, this A here is chosen. This A here is chosen as the following. It's not that important, but what is important is kappa, because in many ways, kappa is our rate at which we're going to converge, and kappa is very proportional to A in many ways. And so we want to think about what is the dependence of our rate on gamma? Well, it looks like gamma, maybe if gamma is very small, think about gamma as like one minus eta. So if eta is close to one, then One, then gamma is very small, A is very small, that makes kappa very small. So it's bad when eta is close to one. That would be like a very, very noisy system. If eta is, or I guess eta close to one is a very, very weakly noised system. Eda close to zero is a very, very noisy system. And then we have gammas very close to one there. And then maybe we get something better with higher noise, right? But something to Noise, right? But something to keep in mind that we should expect the behavior in Gamma, and this is something you can check for a lot of these systems, is anytime you have Launch of N dynamics, let's see if this plays. This is just a quick simulation of Launch of N dynamics. You really want gamma somewhat close to one in these systems, right? If you pump gamma too large, then in fact, Then, in fact, the solution or the samples won't spread very far. Right, the samples don't spread very far if gamma is really, really small. They're just kind of going to stay where they started. Gamma is very, very large. There's too much noise, and they all kind of just sit around. There's too much. Gamma is related to the damping. It's the damping in the Langevin dynamics, exactly. Yes. So, but the reason it's bad if it's very, very large is that there's so much noise, because noise is also proportional to gamma. So it's minus gamma v is plus root to gamma Browning motion. And so when there's a lot of Brownian motion, in fact, when V is very low. When V is very large, the damping is velocity damping. And so your speed is really slow. You have really low speed. So this would eventually get there. It would just take a very, very long time to get there. So I just didn't run this long enough to get there after a really long time. But that's something that you can think about all the time with just to kind of check one's results when it comes to any sort of Langevin dynamics is you want this sort of gamma being on the order of one. Gamma being on the order of one behavior. That makes sense. And that is what we have here. That's just an extra little way of us verifying that. Okay, so let's talk about the proof techniques very quickly. We're going to set A just to be this matrix. And in some sense, what do we want to think about this matrix? It's just it mixes the spatial and the velocity parameters, right? That's the important thing here. That's the important thing here. Because if I got rid of the ones on the off-diagonals, that means that I'm not going to consider any mixing between spatial and velocity parameters, but we kind of need it for the hypocerosive approach. And we're going to assume that mu is fixed and that the initial distribution is absolutely continuous. And we're going to set h to be the density of mu with respect to mu. And what that allows. Mu. And what that allows us to do is kind of play these games with respect to a function rather than measures. And so we're just going to define the entropy of H, because a function really is the entropy of the measures and the entropy of, or the Fisher information of the functions similarly. And so we have a couple lemma here that we're going to use. And what we want to think about for a few of these lemma is just that this. Lemma is just that this quantity of the difference in entropy after maybe one damping step, we can represent the difference in entropy as we try to represent it as an integral of the derivative of the entropy. And well, the integral sits out front, but then the derivative looks like a Fisher information. And so this is where hypocrisive approaches give us extra ability. us extra ability because we can represent just differences in one entropy as actually an integral of a derivative where the derivative now pushes us into like the Fisher information or the Sobolev norm. And so this is very standard that you would get from some velocity damping, but you'll notice the B V that I have in my Fisher information says that really I'm only dissipating in velocity. I'm not dissipating in space at all. In space at all. And then the second case is just: okay, so how bad could it possibly get in space with the eta here? And it can't get that bad in space from the damping step. We also have a lemma on the Fisher inequality on the Verlay step, right? Because our Hamiltonian Monte Carlo contains both a, this is our Gaussian damping step, as well as a Verlay step. And so if we look And so if we look at a relay step, we can see that it grows the Fischer information only basically on the order of one plus epsilon, right? It's not giving us too much extra Fisher information. And there's an error term, which depends on delta. And we kind of have to think about this Jacobian flow as well. And then we also consider this. And then we also consider this is the same sort of idea. If I'm looking at the difference of the entropy under the Verlet step, I'm going to do the same sort of process of considering that as an integral of its derivative. And so there you can kind of almost see the integral of the derivative show up. And we still pick up some error terms. But more or less, all of these inequalities you're noticing, we're bounding them by the Fisher information. That's the important part. That's the important part. And so, when it comes to the true difference in the modified entropy, we have four terms, and two of the terms are differences in entropy, and two of them are just Fisher information terms. But you'll notice from the previous LaMada that these were all accounted for. And so, what we get from that is kind of this big collection of garbage, but think about the top line. Top line, the top line versus the bottom line. Notice that the bottom line is either on the order of epsilon one and epsilon two, or it's on the order of their inverses, which is maybe not good, but delta to the fourth, right? Actually, a delta delta to the fourth. And so it's the top line that we're going to care about because by tuning epsilon and delta, the bottom line can get small for us. Okay, so the nice thing about our modified Fischer information is that actually it's Fisher information is that actually it's linear in its matrix parameterization. Right? So if this parameterization of matrices, they actually distribute with constants. And so they're linear. So actually, in fact, this top line, I can add it all together. And by adding the entire top line together, we have something that I'm calling negative I sub S. And I really, really want this S to be positive definite. This is also very common for hypothesis. This is also very common for hypocorusive approaches. And so, kind of the last important lemma for this proof is then saying, fine, it should be negative of some positive definite matrix. B sub V is perfectly fine for being somewhat positive. This A B A to inverse is perfectly fine for being somewhat positive. So it's this, this A psi is kind of the nasty term out of those three. If I'm going to add them together and hope for something that stays. them together and hope for something that stays negative. And so really what I need to care about is the behavior of psi. Recall psi was kind of this Jacobian flow of the Verlay, the Verlay part of the scheme. And so we can show that psi is close enough to this matrix. And in fact, that gives us some dependence of psi on the parameter t, which is just k delta. And that means if k delta is small enough, And that means if k delta is small enough, all of a sudden psi is close enough to one, and we get uh and we get that s is um s is positive if we tune a correctly. So yeah, that's all I have. I think it's good timing for me. Thank you very much. I'll take questions now. Okay, thank you, Ivan, for this interesting talk. Any questions? Thank you. Can you explain one more time what was the appearance of using Hamiltonian dynamics and some different sampling techniques? Yeah, so I think the important thing is that with Hamiltonian dynamics, it's well, let's just start with saying, fine, we're going to work with Langevin dynamics. And Langevin dynamics is nice because we know that the invariant measure is this. The invariant measures is this, right? D, I should use Q, DX, V V. So since it's a very well-studied stochastic dynamics, which has a very easy to calculate invariant measure, then Launch of it dynamics is like, fine, that's a really good choice. Actually, and we have, you know, nice convergence properties of this to the invariant measure, right? There's a whole wealth of literature on convergence. Literature on convergence of Langevin dynamics. Okay, so then the question is: if I don't have a perfect Langevin dynamics because I'm running these things on a computer, can I and I decide to discretize this? What is maybe the best way of discretizing Langevin dynamics so that it converges fast? And why Hamiltonian Monte Carlo is maybe a good choice is that the Hamiltonian is actually preserved under the Verlay splitting. So if I use a Verlay splitting, that does actually preserve the Hamiltonian, which is Does actually preserve the Hamiltonian, which is not true of other splittings. Any questions? Okay, thanks again. Thanks to the money.