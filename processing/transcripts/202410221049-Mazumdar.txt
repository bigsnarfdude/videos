Well, optimizers are fortunate. Yeah, so I mean in optimization, like how do people deal with non-congality? Well, they say, okay, you cannot find global opt, but you can find a local opt. And since in practice, local op, we're fine. All right, so that's an approach that actually has been quite popular on this problem. So let's try to think about what are local versions of these solution concepts. Maybe these will make it tractable. A lot of this work. Tractable. A lot of this work was actually on two-play zero-sum games. So, one, I mean, there are many different versions of what local equilibrium means. So, one particular liberal version is called this first order local Nash. Basically, what you have here is that you say, okay, let's say I have a strategy profile. Now, everybody takes the gradient assessment. Okay? Okay, that's it. And if you get back something that's close to where you start, fine. Okay, so that's what the local Nash means here. So this is a very liberal version. And it turns out this does exist, but still intractable. So this is resulted by Dasukaki, Skolakisa, Tsamataki. Alright, so even this kind of very liberal version of Nash equilibria, local Nash equilibria, is intractable. I'm not allowed to be one break in this area. All right, so we'll talk about some results with one drink only. So, if you want to do local versions of Nash equilibrium, the problem is still intractable. So, really, what we want to think about in this project is can we actually come up with some solution concepts or equilibrium concepts that are tractable and also universal, meaning that they always exist in any game. Whether it's a lunch, okay? So, of course, one natural So of course one natural attempt will be try to do local versions of like coordinate equilibrium or coarse coordinate equilibrium. We want to think maybe a bit more general than that. So let me tell you what type of equilibrium that we're thinking about. So we want to think about this very general notion. It's called phi equilibrium. And this is very tightly related to what John talked about. So um what is the phi deliver? Okay. So first uh limited phi was the strategy modification. So you have a function phi that maps any possible strategy to another possible strategy. Okay, so think about this as a possible deviation you can apply. And now if I say a distribution over the strategy profiles is a phi equilibrium, I mean the following. mean the following. So this is a type of generalization that basically tries to generalize on C C or C. So you look at correlated strategies. And if you have a phi equilibrium, what it means is that once you get the recommendation for this coordinating device, it's better to follow the recommendation than trying to deviate using one of the modification functions from the set file. Okay, so if you choose this file to Okay, so if you choose this phi to be all possible deviations, that would be coordinate equilibrium. If you take it just to be constant functions, that would be system. Okay, so this is a very general way to think about what type of correlated equilibrium you can have. Okay, so they always exist. Yes, I said, CC correspond to having all constant functions. And this is closely related to this notion called phi regret. This notion called phi regret, which is basically the swap regret that John mentioned. But here, so John allows all possible swap functions in this finite action game. Here, I'm taking one particular subset of functions that you can apply to swap your strategy. Okay, and the five regrets say in hindsight, if you apply the best possible swap function, how much more do you get? Function, how much more you get? And it's known that if you can get sublinear, 5-regret, you can basically let all players run these algorithms and their average will converge to approximately 5. So this is a concept that has been around for many years. And the concrete version that we are studying this. We are studying this project is we want to think about what kind of phi that makes the phi equilibrium trackable. Okay, so let me start with a simple case. So the simple case is maybe my phi is actually finite. Okay, so I only think about a finite set of deviation functions. A finite set of deviation functions. So let's say you have n deviation functions, and the result that we could prove is that even you have arbitrary sequence reward functions, non-concave ones, you can have a randomized algorithm that gets you the following phi regrettable 1 minus beta. So you basically get t times log n, n is the number of possible swap functions you have. Possible swap functions you have. You also pay this log one over beta here. And also, the algorithm doesn't depend on beta. Okay, so you can run the algorithm and then you say, oh, I want to have a beta to be 0.1. You just plug it in and get a corresponding value. Alright, so that's the first result of the share. And I want to quickly, oh yeah, so it runs in time. That's square t times n. So I just want to quickly. So, I just want to quickly compare this with what we know about fire regard minimization, even when the rewards are concave. So, I think John already mentioned there's this blonde and sword paper, which says if you're in a finite game, you can actually reduce swap regret minimization to no external regret minimization. There's a follow-up that generalizes. That journalizes that result. Roughly speaking, so they can get a similar bound, fire graph bound. So it's a deterministic algorithm they have. But they all need an oracle that does the following thing, which is you need to compute a fixed point for functions in this family. Okay, what's this family? You take all convex combinations of functions in this family. So, you require, like, you need to be able to do such a computation. And in certain settings, you can really implement this oracle in polytime. For example, you can find a number of actions. But in general, this is what you need. Okay? So you need to compute this fixed point. And there is actually the paper by Hassan and Keo actually showed that in some sense this is tight. If you want a deterministic algorithm, it's really the same as computing a fixed point. Same as computing the fixed value. Okay. So what we did is that instead of having this remissive algorithm, we have a randomized algorithm. We got quite comparable bounds and we don't need even the functions in phi to have a fixed point. Of course, we don't need any oracle to compute them. I actually prepared two slides for the proof, but I think maybe I'm too expensive. Yes, yes. So if you... Right, exactly. Because for our functions, you might not even have a fixed point. It's uh actually quite simple. Uh Actually quite simple. What kind of random like how do you randomize algorithms? Yeah, so let me try. Right, so here I said, so basically in their algorithm, there's one step. They say, hey, you need to give me a fixed point for a function here. Okay? So Okay, so instead of giving a fixed point for a function there, we kind of come up with a distribution, a stationary distribution over the strategies. It's stationary in the sense that if you apply this deviation to any point in this distribution, you get basically the same distribution. So you need to come up with some kind of stationary distribution under that transformation. And so it turns out the one we came up with has exponential support. You cannot explicitly represent it, but there is an efficient way to sample from it. And then you have to do some martingale analysis. That's why we pay this one a bit. So there's a way to do it. Okay. So let me skip the proof and talk about okay, so the final one, I find it quite The final one, I find it quite clean. But really, we're looking at this case where x is a continuous set. The phi function maps x to itself. So you could try to discretize a set and apply this finite result there. You're going to get some explanation depending on the explanation. So we don't want to pay that. So the next thing I want to do is let's just directly look at some infinite view. Look at some infinite deviations. And you cannot look at global deviations, as I mentioned, because that would take you back to finding global maximum. So now we'll try to use this local deviation algorithm. So I want to think about infinite sets, but the deviations are all local. And for this part, I will introduce this. I need to assume the game is actually G. ellipses and out. Actually, G-llips and Auschmoos. By that, I mean for utilities. And so here's one type of very general local deviation you can think about that really just says the deviation doesn't move you far away from where you start is. And let's say this is L2. And I claim that this is really some kind of local coordinate equilibrium. If you can guarantee to To find an equilibrium with respect to this deviation, really is a local coordinate equilibrium. And so, when you think about this kind of local solution concepts, it's all about what regime you're thinking about. By that, I mean the dependence between epsilon and delta. Delta is how far away you're allowed to move, epsilon is the error in your equilibrium, how far away. How far away you're from the equilibrium. So there's a very trivial regime. If you say epsilon squared, then g times delta. So five delta is the supo over all x's. Say again? Is it five delta is the supo over all x's? Because there's a variable x in x in this definition. So these are functions from x to x. Okay. So I'm looking at all functions from x to itself. Right. Such that. Yes, no, exactly. Such that, yes, no, exactly, there's a sum over x. You're right, good. There's a sum over x, yes. You never move any point doublet from the cell. Right. So that's the type of like general local deviation that you can think about. There's a very trivial regime basically. You just say, okay, epsilon is g times delta, of course. Yeah, because Euryptions. So that's quite trivial. The next one is more interesting. So we call this first order. More interesting, so we'll call this first-order stationary regime. Basically, you get epsilon to be in the range of L times delta squared. So, we'll call this first-order stationary regime just to be consistent with the optimization. That basically you get this in optimization when your gradient is zero. So, for this one, there is an algorithm that computes some approximate equilibrium. Approximate equilibrium in this time. Okay, so let me parse this for you. So you're going to pay delta squared times L. On top of it, you're also going to pay gamma. So that's epsilon, the sum of two things. And the running time of the algorithm is exponential in one over gamma, but polynomial in the other things. And this uses this recent result by these two different groups. They came up with this algorithm called tree swap, algorithm for reducing. Algorithm for reducing swap regret to external regret. So if you apply that, you will get this algorithm. Alright, of course it's interesting to see whether you can go beyond first-aller gene. So it turns out this is impossible. So we'll show that if you want epsilon to be lower order, say you want delta cube. Okay, epsilon to be delta cube. Epsom to be delta cube. This turns out to be handy. Alright, so if you want to think about this very general type of deviation, the first order stationary regime seems to be the one that's, you can get some possible results. Although I wouldn't say this is very efficient because you've got this exponential dependence on it. All right. So So let me tell you, alright, so this one I have here, I think of this as a local version of correlated equilibrium. So on the next slide, I would like to discuss some local version of coarse correlated equilibrium. Okay, so if you think about the previous, like the one I have in the previous slide, really the type of, there's no restriction on the five. There's no restriction on the phi other than that you have to be close. For different x, you can apply completely different phi, like you can move it completely different. And that feels really like a coordinate equilibrium. But for coarse correlated equilibrium, basically the type of modification you can apply to different points should somehow be related to each other. And it's not so easy to come up with such a definition, especially. Especially when you want to restrict it to be local, right? So I think about in CCE, you say you think about all constant functions. You all give it to one constant function. But now you want the points to only move local. How can you all give it to the same function? That's it, the same point. So you have to come up with some other ways to say the deviations are sort of related. So here's one attempt that we have. That we have, we find it quite natural and also related to this local NASH that I talked about in the beginning, where you take just a gradient, a set step. So what we allow the five functions to be is that you say, okay, so you're allowed to basically, all the points are allowed to move in this one direction, V. And you're not allowed to move more than delta. Maybe that's going to take you out outside the feasible region. Outside the feasible region, just project back. That's the type of deviation that we allow. So here's one example, right? So if you both, x and y, two points, you want to move in this direction, then after applying the function, you get these two yellow points. Maybe you want to both go on this blue direction, then the y will be outside, so I'm just going to okay? So that's the type of modification we allow. Modification we allow here. And for this one, you can actually get quite a bit better than what we have for the correlated equilibrium. So this one, you can actually get, so you get this epsilon plus delta squared times R over 2 approximate equilibrium. But this one, the dependence on epsilon is much better. So you only pay epsilon squared. Alright? So that's for the course of. So that's for the coarse local deviations. And we also think about some other coarse local deviations. Another one that you can do is you can say I'm going to take a fixed point x prime and I'm going to interpolate that x prime with x. Basically, again, this idea that all the points want to move to this one particular strategy, but have to be local. But I have to be local. How do I do that? So I'm just doing some highlight interpolation. That's another version. Or we can think about this like complex combination-based multiplication. You just say I have some finite set of local deviations. You're allowed to pick any complex combination. That's another version we thought about. So, for both of these deviation families, again, we think they are very efficient on coupled learning dynamics. So, in the same domain, so you pay this error that's L times delta squared, but it's efficient. And also, we have a lower bound saying that even for this interpolation-based deviation, if you want to go beyond this first-order regime, it's still MP1. So, if you want to think about even this sort of coarse deviations, still might be challenging to go beyond this delta squared. How crucial is it that like the bent in the same direction for the previous set have like roughly the same order of magnitude? Because you might imagine that the points are further away if you care if you move for the points that are closer. Some of these sorts of things. Or some of these sorts of things, or like, you know, basing this on the utility function. Further, in what sense? Further. So suppose we were talking all about swapping to action X, right? And I'm looking at a collection of different actions. Some are closer to X than others. In like, I don't know, Rn, right? Like, if we're talking about these actions as closer to X, meaning? Like, X is the one you want to go. Uh-huh. Oh, I see. So that's more like this interpolation. Oh I see. So that's more like this interpolation I think. Uh right so there is a point that you want to go and I'm just gonna interpolate. Right right but do we all have to interpolate with the same depth? Yeah that's the same point yeah. Yeah but is that crucial? Like could we could we interpolate different amounts based maybe on our dystopistics or something? Yeah I think you're talking about a lambda. Yeah. Right. Whether you want to a good question Good question. We thought about this. Okay. The lower bound bias, so you cannot go beyond the stationary, first order stationary. I think the algorithm, yeah, might work out, but I'm confident that we can get something for that. Yeah, yeah. Running out of time, I know, thinking, so I'm done. All right, yeah, so that's the last thing. That's the last slide. Okay, I just want to mention one open problem, which I think is related to what John talked about, or at least like the technique that you mentioned in the end. That is, I think it's important. So we have the result saying that if phi is finite, you can get this sort of regret. And then if phi is infinite, in general, you cannot do anything. But if you m make some structural assumption about phi, like the ones we thought about, you can also do something. We thought about it, you can also do something. So, one interesting question is that maybe there's some kind of complexity about this family phi that you can define that allows you to characterize when is phi represent minimum attention function. An engine curves. Yes, exactly. Exactly. So, that's one thing we also mentioned. So, I think that's kind of an interesting question. Okay, very good. Okay. Okay, very good. Okay. Yeah, there we go. Cool. Yeah. Yeah. Thanks for having me. This is, I'm looking forward to it because I think Jar did a little set up for me, which is great that I can kind of free ride on it. So today I'm going to be talking about principled multi-agent reinforcement learning by bringing in ideas from behavioral action on. Like to help the A V folks.