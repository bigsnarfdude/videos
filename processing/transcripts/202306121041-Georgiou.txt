Thank you again for great comments and participation. And so we move to the second part. This is again continues with some of the key ideas from the previous, especially the connection, the link between entropy dissertation. And the discussion will be about thermodynamics, how dissertation and power in physics and biology. And power in physics and biology linked to optimum mass transport and short-term bridges. This is joint work with a number of my former students, colleagues, and Olga, Moria Mangolara, received PhD this year. Amir is here in the audience, and John Shin and Avon. So much of it, all of it actually is. Of it, all of it actually is joint work. So, here the figures of the top people change slightly. The first one is Carnot and the last one is Boltzmann. And in the middle is again Gaspar Monge. Carnot, of course, you know, was one of the founders of thermodynamics. At a very young age, he wrote this treatise, which he publishes in his own money, and died shortly afterwards from Shortly afterwards, from some disease, and that ended up on the desk of Claudius, and the rest is history. He tried to quantify efficiency in engines. Efficiency requires quasi-static transitions, long times, so there's no way to quantify power for even models of engines until recently. So it was in the 70s that was an effort from the There was an effort from the community that from the physics community to develop this. And it turns out that there is a very intimate link between Okama transport, precipitation, and finite time transitions, which became very apparent with the work of several people in the late 90s, fluctuation theorems, development of the subject called stochastic homogeneity. So, this is what I'm going to talk to you about. How all of these things come together. And again, the Newton paradigm of moving one particle somehow becomes now the new paradigm where you are thinking of the collective response of many particles and described by thermodynamic states, the thermodynamic states being a distribution, a density. And I would like to note that some of the ideas in different ways can be traced back or very related ideas. Trace back or very related ideas in some of the earlier prophets in our subject, Roger and Jan, who are not with us. And it's very interesting, and myself also I'm trying to see all the possible gems that they produce and they relate very much to things that happen later. Of course, the advantage of having new tools and new Of having new tools and new concepts. So, this is a talk about the interlinked themes of gradient flows, optimizing principles in Gush's time, where work and dissipation play a key role. That's what we're interested in. In mathematics, it started off, and I explained this in the last hour, 200 years ago with Moz Kantorovich in the theory of probabilistic theory with large deviation. Probabilistic theory with large deviations, Schrodinger, Sanov, and so forth. Then, 120 years ago in physics until now, there has been a lot of development starting from thermodynamics, which is really thermostatics, quasi-static transitions. And even in the development in the 50s and 40s, 50s, 60s, when we say thermo non-equilibrium, it's still quasi-static. A lot of ideas were not able to deal with. We're not able to deal with the questions such as what is the power. So you need finite time transitions. It's all about energy transduction and stochastic models that emerged more recently. Then 40 years ago, much of what I'm talking about becomes very pertinent in biology, where you have one tries to understand mechanisms that sustain life. Life is Life. Life is sustained because we have anisotropy. We have the 6000 Kelvin photons that are coming from the Sun that we emit back in space as 300 Kelvin photons. So how in this environment you get a sustained energy transfer through cascades of different energy transduction mechanisms is of great interest. Is of great interest. So, I'm going to talk to you about one such. And I'll explain a little bit anisotropy and the relevance of this. And now, how geometry and structure enables all this and gives us some sort of a setting we can actually think about it. So, the subject started blossoming when it also goes back to Einstein, when he made a very He made a very simplistic model of particles inside a heat path. But the recent development of the theory started much later, about 20 years ago, and it's known right now as stochastic thermodynamics or stochastic energetics. There's a beautiful book by Sekimoto, Zazinski, Seifert, great contributors in the subject. This is a nice book, 2012. 2012. So, this allows you to look at thermodynamics at the level of individual particles, and then the transitions take place in finite time. So, you have stochastic models for dealing with this. And it gives you new insights, and of course, enduring directions, how do biomolecular motors work? Like, for example, you have flagella. Here's a picture of the flagellum, and we could talk more about this because it's really a beautiful structure. This is because it's really a beautiful structure that here you have some protein structures that, due to chemical potentials and gradients, rotates at 3000 rpm. Absolutely huge. It's going, you know, a bacterium can go 17 body lengths per second. Imagine Mark's speech swimming that fast, you know. So there are very interesting things that at some point we should be able to understand. To understand deeper, and that is kind of the ultimate goal. So, here let's go back to simple models: optimal transport dissipation. So, the model that we are using for stochastic particles is this classical model of a heat bath that of smaller particles bouncing against the particle we're interested in. And this is This gives, of course, goes back to Einstein, where he explained that the drift current of the is due to diffusion balances against the current due to forces. And he came out with this famous expression for the fluctuations and basically understand that diffusion relates to the headwind if you tie a rope. Wind, if you tie a rope and you start pulling it. So that's basically the idea. And you will see this expression popping up everywhere. I don't simplify Kb to be equal to 1 because it's really nice to see the underlying physics. But you can feel free to do so. So the model for such a particle is the underdamped Langevin equation where you have position B. Be the integral of the velocity and the acceleration due to potentials, some potential that this is subject to, there is friction and there is a thermal excitation. The strength of the thermal excitation is exactly goes back to Einstein. If you notice here, the temperature we allow to be time varying. dB is a standard Brownian motion, stochastic excitation, everything is Stochastic excitation, everything is very standard. Thank you. Then we will focus more on what is called over-damp dynamics. This represents colloidal particles, particles that are, the friction dominates, inertial forces. Analysis is much simpler for those, but many things can be done and said about underdamped as well. So the overdamped particle. As well. So the overdamped particles, if you look at the previous equation, a simplistic way of looking at it is the dx, so the dm goes to zero. In simplistic way, again, you can justify it, so the dx is vt. So if you substitute here v dt by dx, you get the equation we have in the next slide. So you get gamma dx is equal to that. So you have Equal to that. So you have forces and stochastic excitation. So, right. So when the inertial effects are very minimal, so the thermalization is faster, then the over time dynamics dictates the movement of the individual. The movement of the individual particle and the probability distribution of this stochastic process, we view it as a stochastic process, obeys the Fokker-Planck equation. Now, the Fokker-Planck equation, which typically we write it as the gradient of raw time drift, and then you have the Laplacian, it can be brought together into a continuity equation form. A continuity equation form where the velocity field is due to the gradient of the potential and a stochastic gradient due to the density, the logarithm of the density. And this is very insightful. So now the most important thing is to be able to put your hands around energetics. How much work is transferred between the potential and the particle, and possibly. The particle and possibly the particles, and so on. So, the energy of a single particle, the internal energy, is dictated by the position of it with respect to the potential. So, if you take the derivative of that, you get a first component which can be identified as the interaction of the particle with the potential. And this represents work, and this is the stochastic interaction between the two and represents heat. Between the two and represents heat. This is Statolovich. This formula is a key, and it has been debated in the literature, very recently, in the physics literature, and to a degree verified even with some experiments where they have a colloidal particle and tried to quantify. But again, intuitively, it looks fine, but if you are puzzled. If you are puzzled by it, many people have been before. So you are in good company. But this is the key: that this represents actually heat that is brought into the energy that is transferred by the heat draft to the particle industries. What is U? What is U? U is a potential. So this is X is moving Under the influence of a certain potential. So you can think of it as a quadratic potential or a string tied to it with a spring constant and so on. Now here I indicate lambda of t is this is some scheduling parameter, but I simplify it to just t. So it's a function of t and x. So it's a time-varying potential. So you can think of a collection of particles that you try to group together. You try to group together by applying forces to them. Very much like you have a cylinder and you put, you know, it's an oversimplified thing when you have a gas inside a cylinder, but kind of gives you the pictorial of this. So then the work and heat over a window of time for a single particle is the integral of the first term and the heat is the integral of the term. And the heat is the integral of the second term. This is against Atomovich. And you have the first law of thermodynamics: that the sum of those things are equal to the kinetic, the internal energy. Now, in the physics literature, there is discrepancy as to the convention of what is minus and what is plus. You know, the heat that's going into the system, coming out of the system. So, this is just a choice. At the level of the ensemble, the At the level of the ensemble, the work is calligraphic and is the expected value of W over a collection of particles. And the same with the heat, and the same with the change in the internal energy of the collective. So this gives sort of the basis for doing some computations. Now, you can think of the entropy of a particle, and in fact, you can And in fact, you can the entropy of a particle is log rho and depends on the x where the particle sits with respect to the density of the collective. If you take the expected value of that, you get the usual expression for entropy, which is the entropy of the collective, the whole ensemble. Now, the free energy is the difference between the internal energy minus KTS. KTS. And this represents the energy that you can get out of the collection of particles by suitable change in the potential if you keep the temperature constant. If you take the derivative of F, you get two terms. And one of them is the derivative of the work, which depends on the path. This is what the bar here indicates. This is not a perfect. This is not a perfect differential, and then you have another term here. And if you integrate it, you get that the change in the free energy, the work into the system, is the change in the free energy plus this term. So this term is again positive no matter what direction you are following. And this represents the situation. And this represents the situation, and of course you recognize it. This is exactly the Wascherstein length between the rho 0 and rho t. This is exactly what we had in the previous hour. It was the integral of the kinetic energy. So expected value of V squared, integral of rho V squared. So it's kinetic energy of the ensemble of the particles. So this This is exactly the point of this slide. And to understand how this is to be used, you have the ensemble from an initial distribution to a final distribution, which is affected by the change in the potential. So you change, the potential is shown here with the blue. So you're shaping it, and you open it up or close it. It up or close it. So, for example, here you have a biomodel and it goes into a unit model. And exactly from the previous slide, the insight is that the average amount of work that you're putting in is changing the free energy plus the dissipation. And the dissipation is the masses time length of the trajectory of row from beginning to end divided by the duration. So it's like the wasted work. So if, for example, So if for example you take infinite amount of time, then that term disappears and then you have the work you put in is exactly the change in the free age. So now this gives insight into an age-old question that goes back to probably 50 years or more. What is the cost of computation? What is the cost of computation, of carrying out a computation, and famously was understood in a certain way, which is what is the consensus now by Landauer almost 50 years ago, where he explained that they work. So if you want to do computations, you need to move bits around. But you don't do them in a zero temperature environment, you do it in a thermal environment. So if I flip the switch here, so from zero to the temperature, The switch here, so from zero to one, well, it's unlikely that will flip on its own, but if we wait a hundred thousand million years, you know, random fluctuations, presumably, you know, you're going. So even inside the computer or anything, you need to hold the information in a bimodal distribution. You have a zero or a one. And it turns out that carrying out the computation is not Is not, it doesn't require energy. You can do this with a unitary transformation, but resetting the bits, setting the register back to zero is what it costs you. So the real cost is erasing your memory. So you have a one or a zero, and by a process of changing the potential, you bring them both to zero. So you lost information on the past. Very much. Very much as is portrayed here. And Lambda House bound was that the work is greater or equal to the change in the free energy between the pi model and the Green model. And that is KBT log2. In general, if you do it in finite time, you have an extra dissipation which tells you the actual cost if you do it in whatever time is allowed. Okay? So this So we are now, we have tools to be able to do finite time transitions, to do budgets between and transfer between the heat bath and an ensemble. So the first thing you can think of is trying to replicate a Carnot cycle and see how, you know, what is the story? You know, what is the story? How much efficiency we have, how much power we can generate. So, here is a very simple example of a Carnot engine. You have an ensemble that is distributed according to raw inside this potential. The potential at 0 plus has this shape. Let's say quadratic. It's in contact with the heat bath of temperature H. This means high. This means high. So intuitively, the particles are pushing against the potential because they are in a very excited state. So then you open up the potential and in the process, very much like when you open sort of a cylinder where inside it has a hot heat bath. So work is generated. So you can quantify that. You get work out here. Then you bring the collective of particles in contact with the Particles in contact with a cold heat path. And then instantaneously you open the potential to sort of the one corresponding to this distribution. Now the particles have less stochastic excitation, so you can afford to push them back by doing a little bit of work. And then you repeat the cycle. So this is an adiabatic transition, this is isothermal. This is isothermal at this temperature, this isothermal, and this is at the abatic. So you can budget how much you gain, and you see that along any path, again, you change, you know what the free energy is at the beginning and at the end. You can compute the work, you can compute the reversible loss, the versus time length. So then you can You can look at different scenarios, fixed temperatures, varying temperatures, different potentials, different scheduling, and you can find, assuming that you're doing optimality along the isothermal, you don't accelerate and decelerate, you don't do something strange, you can show that the efficiency, for example, in the simplistic engine that I have. For example, in the simplistic engine that I have in the previous slide, the efficiency, meaning the workout over the heat that enters into the engine, has this expression. So this expression, the right-hand side here, if this is zero, gives you Carnot efficiency. So if the time it takes for all the transitions is infinite, that disappears, and then you get exactly Carnot's formula. Exactly Carnot's formula. Delta S is a change in the entropy between the various states. But you can play the same game and look at the power because you have finite time. See how much works. So this is the expression for the power. Again, these are the times of the different steps of the cycle. And this has been worked out for many different scenarios. Different scenarios. Now, this is a nice exercise and gives us a lot of insight, but it doesn't answer the question that I started at the very beginning. How, for example, cells or engines work. You know, inside our body, we don't have valves and pistons going back and forth. So, how does exactly we get the energy transaction? So, it turns out that So it turns out that we need to understand metabolic processes by molecular motors, thinking of them as working within a thermal anisotropic environment. In other words, there's no hot heat bath and cold heat bath at separate times, they are together. You know, this side of the wall is hot, the other one is cold, and we're sitting in the middle. What is going to happen? There's going to be a tornado here. There will be circulation of air. There will be circulation of air that is taking the heat from the hot wall, bringing it to the cold wall, and transferring the heat. So, if there is such a cyclone in here, we can put a little cell and move it around because it will produce torque and we get workouts. So, that's exactly the way mechanisms that transduce power from anisotropy in thermal or chemical gradient. Or chemical gradients. So, this is what I'm going to talk to you about, and we're going to analyze. A very simple model for this was put forth by Chiang and Al and Fillinger and Reinhard. And to understand it, you can see this schematic where there are two resistors, one of them is cold, the other is hot. The other is hot. Then there are capacitors, and if you choose these parameters properly, what you get is a circulating current. So even though the cold heat bath and the hot are separated, they are linked through the degrees of freedom of the electric circuit in it. So this is again a schematic for the charges in the different capacitors. The two resistors produce The two resistors produce thermal fluctuations. This is Nicholas-Johnson noise. So you can think of as Brownian excitations, which come in different densities at the two ends. And this point represents the charges in the two capacitors. And what you see is you see a fluctuating positioning which circulates. So you get at steady state, you get a stationary distribution, but it's not equal. Distribution, but it's not equilibrium. In other words, this is sort of the stationary distribution for the charges here. So these are the voltages in the two capacitors. But if you observe any instantiation, you see that the circulating current. That is the key. Stationary doesn't mean equilibrium. Equilibrium means that you have a You don't have probability current. You have detailed balance. So to speak, you can think of it as equilibrium means that there is a lot of movement around, right? Let's think of them as particles. So you can think of what we're doing here as some sort of a particle here that is attached to a spring with a wall, and there's a second spring. And there's a second spring. So it has two degrees of freedom. So we go in, grab one of the springs and shake it, and the other one we shake it less. So the particle is going to be jumping around like that. Now, if we look at it long enough, we will see that it goes on a circular motion. That is not equilibrium. If there are many particles, there's going to be a special distribution, but all of them, if you look at magnifying glass, we will see that there is a tendency to rotate. So, this is the model. This is a control Brownian gyrator. Some of these ideas go back to Feynman, on ratchets and so on. So, it very much is not, you know, this is a model that has been studied more recently, but it's, as you say, as they say, you know, we stand on the shoulders of giants. So, let me explain this. So, you have two degrees of freedom. This. So you have two degrees of freedom, x and y. You have a potential that couples the degrees of freedom. If it doesn't couple them, then you have an equilibrium distribution in different directions. But if they are coupled, then we get a randomly moving particle within the potential. Then there is a direction that is excited more than the other. So this is the Tx. Than the other, so this is the Tx and this is Ty, one is hot, the other is cold. These are independent Brownian motions from the two capacitors, for example, and K, which is the spring constant of the potential, is our control parameter. So the idea here is you leave this particle there, it keeps circulating by rotating. Rotating the potential, you change the density, the uncertainty, the position of the particle, or if you have many particles, you change the distribution of the ensemble. By doing that, you're going to get work out or put work in. So this is what we're going to be doing, understanding how you calculate, how do you do the energy budget. So just to make sure I understand, if you go back to your previous slide, he Understand if you go back to your previous slide, there is no potential that describes the Brownian ratchet that you had on the previous slide, right? No, you're adding your control. So here, there is no control. Right? You're going to see the control coming in. The control will come in by changing the capacitances. Okay, so that's what's going to realize the K. Yes, the K will be realized by changing the capacitances. You can change the capacity, and you will see how. And you'll see how. It's quite interesting. So then you have a potential here, and the potential is again described by a parameter k. So this is like the A matrix of the system. Tx and Ty are fixed. Tx and Ty are fixed. They're done. Yes. You only play with the K. Now, when you play with the K, if K is fixed, the system will reach a steady state. The covariance is going to be sigma of T. Is going to be sigma of t, sigma. But if you change the k, the covariance will be changing and will obey the Lepuna equation. That's the one, I mean the standard Lepunov equation. K is symmetric because it's the spring constant of this two degree of freedom potential. And the T here is the temperature, which is the stochastic excitation, and it's diagonal, but It's diagonal, but it's not scaling of the identity. So the idea is to apply a stochastic control action and see how much we're going to get out. Now, because we have too many things on the board, we have k, we have sigma, we have sigma dot. The simplest way to understand this is you solve for k as a function of sigma, sigma dot, and then you don't worry about. And then you don't worry about what k is. You try to find a path on the space of covalences, which is the same as a path in Wasserstein space, because you are moving a density. It just, it happens, it so happens that here we have finite parameterization. But we are moving in a Wasserstein space and we want to see, you know, k can be solved from this equation and you can write it actually in this. And you can write it actually in this form. But that's not so significant. It will come significant later on. If you haven't seen this formula, that's a linear equation for k. So if I give you sigma and sigma dot, you can solve it. And this includes sigma here, sigma dot there, and so forth, and some integral representation. It will come handy if you remember it in loopbags. Okay? Now, so we are going to look at Going to look at what are the optimal paths in Washerstein space. We need to parametrize the sigma in a nice way so we can think of it. So sigma is a positive definite matrix. It has a representation with some unitary orthogonal transformation, rho of theta. That's an orthogonal matrix. And here is the transpose. And this has the diagonals of sigma. Of sigma, and we assume that the determinant is constant. There is not really a big loss of generality in doing that. So we have the luxury of working on a two-dimensional space. Because we now have two parameters, the ratio of the eigenvalues here and the theta. For reasons that, you know, again, we don't, it's not so important, instead of theta, we look at two theta. Instead of theta, we look at 2Î¸, that's 5. And instead of the ratio, we look at the logarithm of the ratio. So these are the two parameters that we dictate the position, specify the position of our distribution on the two-dimensional manifold, and we look at them as polar coordinates. So here is a schematic. So this is the Euclidean space. R and theta represent position of our Gaussian. Of our Gaussian distribution. So if r is zero, the logarithm of r, r is the logarithm of the ratio, so it's basically the identity matrix. So you have the double sets will be circular. If you squash it in this direction, R increases. The theta represents the tilt of the Gaussian with respect to the axis. So if you go around a cycle like this, So if you go around a cycle like this, the upper half plane, then this oval is moved to this position and then you can squash it back. And that's a cycle. So you can change your potential to move around any trajectory you want in Wascherstein space. So this is again a two-dimensional representation of some slice in Wascherstein space, which we're only dealing with Gaussians. The idea is, and there is Idea is, and there is more work that, again, we're not going to talk more about that, but you can think about this in general. You can think of a Bascherstein space with more, you know, infinite, of course, dimension, and you say what's the optimal. Okay? How do you decide whether you're going to counterclockwise or partwise multiple? You'll see. You'll see. It depends on whether you want to get work out of it or you want your engine to work as a refrigerator. There's going to be an energy density. There's going to be an energy density coming in. So that's exactly the point here. We look very much like in the earlier part. We have the internal energy of a particle that sits at position x and y, and it is the trace of k times sigma. If you take the derivative, then you get heat and work differentials. And you can trace it back to the formulas. And before the derivative. When before, the derivative here comes from the sigma, and the derivative here comes from the k. This is the partial change of the potential. Now we get rid of the k as a function of sigma and sigma dot. Because we don't want to have sigma, sigma dot, and k too much. So after you substitute the expression we had in the previous, you see that the heat, this term here, breaks into two parts. Two parts. This part has sigma dot, and this has sigma dot squared. All of these are matrices. You can't commute, right? But nevertheless, this one, if you take infinite amount of time, this will be zero. So you can identify this with the Wascherstein length of the trajectory. So this is actually the dissipation, the the the the amount of heat that you waste to the environment. You waste to the environment. And this is the quasi-static heat. This is the heat that you suck in from the environment if you want to produce work. And this doesn't depend on the timing of the cycle. Do I have this right? Yeah, I think here it should be TF the time of the cycle. Because we we could do this in finite time. Doing this in finite time. So, if you look at these two expressions, the Poissar style here, then we can replace the sigma dot. Again, these are different coordinates, transformations. We have a movement on the Wascherstein manifold, which now we represent with the covariance. In the next one, we replace this with the R and the theta. Replace this with the R and the theta, or the phi, that reflect what the values of this covariance is. So, by doing that, the quasi-static heat can be expressed in a certain way. And using Stokes' theorem, we can write, and this is the work that we're going to get out, quasi-static heat, and it turns out to be a surface integral on the Wasserstein manifold encircled by the path. Encircled by the path we take. This path here is dictated by the choice of the potential, but conversely, if we draw a path, we can find the potential that will carry us along this path. So the surface integral is the work we get out, and the dissipation can be written as a length along the perimeter. That's exactly. That's exactly the Passenstein length. The Passenstein length in traversing the cycle. So you can write it with suitable expressions for the metric. It induces a metric in this two-parameter space. So the dissipation is bounded below by this expression. And then we get to the we realize that we have an isopymetric problem. If we want to optimize the operation, we are not going to go like this, but we might try to cycle through areas where there is more energy density. So you have an area. Have an area integral where you integrate again on this particular Wascherstein sheet, which opens up the question for many things later on. Like we want to understand not just geodesics in Wascher's time, but surfaces in Bascher's time. And this is more for a lot of future work. So this is a surface integral, and this is a line integral. This is the Waschestein integral. A line integral, this is the Bashstein length. Depending on how much time you allow yourself to go around the cycle, you get some quantity here denoted by mu. It's some physical characteristic that has to do with dissipation and so forth and the time of your cycle. So the mu, whether you want to go around the cycle fast or slower, dictates what happens. So the work over a cycle is this expression. Is this expression? So you have area mines mu L squared. So if you want to maximize work, and you choose this to be fixed, you have isotiometric problem. How much you dissipate. And these are isotiometric curves. The height here represents the energy density, that expression here. Expression here. And if you want to go around the cycle too fast, then you end up here. And then, if you go faster than something, then you're not going to get any workout. You're basically wasting everything in dissipation. So there are trade-offs. Just to make sure I understand correctly, so here the work you're extracting comes from creating a distinct temperature you have Tx and TY, which are kept fixed by some very large heat path. by some very large heat path, which are isolated. Yes. And then the U the that was the potential here, you use it to actually extract work. You use it to extract work. But how do you do that? When it's under the influence of the hot heat bath, you are allowing it to pull you and in the other direction. So intuitively, that's what happens. So you have some circulating set of particles, or you might have a very single particle in an isotropic potential, and the interaction. And the interaction with the control action that you apply gives you the ability to extract work. So the... Okay, if you go very fast, of course, the energy you extract gets very small, and that has to depend also on the delta T, right? Yes, yes, on the delta, the time of the cycle. Yeah, and that depends on the variation in temperature here when I make delta T. It depends on the time. Yes. It depends on the time. Yes, everything comes in in this constant. So, this constant brings in friction coefficient, another physical constant, L C, which has to do with the determinant of the entropy of the ensemble. And again, all of this is compressed into one. And here for different And here for different mu's, you see what are the superior curves. I don't see why the max of W is equivalent to max of A with L2 constant because I thought mu plays a role in here, no? Mu plays a role? Yes. In the right hand side, I don't see how. Like, here, Q plays. Yeah, no, that I see. In the map. So again, I mean, the work of a cycle is given by this formula. Now, pick your poison. Choose what you want. You choose a cycle you have a mu. Does the amount of dissipation into the environment is important to you? Then you can fix elsewhere. And then you get to this. You might have different priorities. You might want to maximize, like the way it is right now. Like, like the way it is right now, you know, I mean, that's the formula. And then it suggests isoperiometric problems. So, for example, you can talk also about efficiency. So, if you go through a cycle, then there is dissipation and there is work that you get. So, the ratio of the workout over the maximum work that you get. Work that you get in. It depends on the speed. So if the speed is too fast, then you will not get anything. So you can quantify, you know, exactly. But again, as I said, pick your poison. This is the formula. Whatever you decide is, in what ways do you want your engine to be optimal? So that is again a simple model, but explains exactly, yes, go ahead. So if you wanted to have. So if you wanted to have a mechanical example, what would it be? Some sort of spelling engine? The next, next, exactly. Next slide. Thank you. So can it be built electromechanical, sterling concept by molecular metabolic processes? So I want to give you, you know, to spark everyone's imagination, and this is at least some of the things. At least some of the things we tried. Okay? So, in this one, as you observed, there is no control. So, how do you put control? You have to change the capacitances. You have to mediate the process of transferring work from one heat bath to the other via the electrical degrees of freedom. And you do this by changing, for example, the electric. For example, the dielectric, which you can couple to a wheel and thereby extracting work from it. So, for example, when the voltage increases here, it will suck in the electric and produce a torque. So, you can analyze this, and that's what we need. So, the first one represents the charges in the two capacitors, namely this one and that one, because if you know Is this one and that one, because if you know both, you know exactly the third one. You only have two degrees of freedom. So Q is a two-dimensional vector, is exactly the two stochastic differential equations we had earlier, in just different parameterization. So you have here the spring constant, which becomes capacitance of the two things, all of these things together. So this is the it means the capacity I have it before. Yeah, it's this capacitance. It's these capacitors of the triangle capacitors, right? You can write it down. Then you have the inertia of the wheel. So the wheel rotates at some angular velocity omega, and the acceleration then depends on the torque that's provided by the electric component. And there could be some outside friction. Be some outside friction used in order to transfer the work to an outside mechanical component, right? I mean, there will be torque applied to the wheel, so in order to get to steady state, I have to apply to hold on to it and get to work out. So it turns out that you can show that this works, you know, under certain conditions it works. Certain conditions, it works, can get to a steady state. So you get a limit cycle oscillation. So if you get above a certain level, you will keep going. And this is in phase plane. So you can get the image cycle oscillation. If you stall it, it might not be able to go. Not be able to go on. This reminds us actually of another very well-known example, the Sterling engine that we just mentioned. And you can buy one thing like that with maybe, maybe I shouldn't do advertising here, for like $50. You put it over your coffee cup, and if it's hot and you give it a little push sometimes, so you need that. Sometimes you need that, will start rotating because it has two sides, one of them called the other heart. And there's almost a similar principle. The way this works is it's almost like a pendulum, but because of the anisotropy, a torque is applied to it. So if you let it like this, the pendulum will stay there. If I touch it a little bit, it will just oscillate. But if I go over the hump, But if I go over the hump, you know, the little torque will start sort of spinning it. So it's like you have your positioned in a tilted potential, and therefore you need either enough motion to get through, or you need a collection of such things that you put in parallel. So everyone produces a little torque, and then you no longer stay in the hut. In the valley. So, this is exemplified here. And intuitively, you kind of level the potential. So, these engines can be parallelized. And it seems that that's exactly what's happening in flagella motor. So, if you look at it carefully, this is not understood, but it is truly very, very carefully mapped. Carefully mapped by biologists. So if you look at it, there are collections of protein structures that produce some torque and they're all working together. Then you have a very, very fast operation here. So again, this is some sort of a theory, but it seems that flagella motors may be working based on this principle. Yes? Yes. Um there might be a perhaps even simpler example of this, which is uh A C to D C converter. There's a s there's a sinusoidal wave coming in, the top part you can think it's hot, the bottom part is cold, and you have a switch that lets you decide how current goes through. Which is kind of the same thing, alternating in the middle. And then you can switch to go from one to the other. So no, again, there's probably more examples. There are probably more examples, and we need to get further insight. But somehow, this set of tools allows you to see within the parameters that you specify in terms of the period, what you want, how you extract work. Now, maybe the example while you're saying could be even simpler. It needs some thinking. So, thanks. And basically, not Now basically that's all I wanted to share with you. And the whole idea is we go from Newton to Mons. We have coordinate space, Newton's equations, minimizing actions. Here we have the same thing for collections of particles. This is all in Washerstein space. The description that I had here is deceptive because we're looking at covariances, which are in two dimensions, right? Or three dimensions, or five. Right, or three dimensions, or five dimensions. But in principle, you can start thinking about this as motion in a much larger Pacific mind, some surface. And you would like to integrate, you would like to extend Stokes' theorem to infinite dimensions. And that's all I had to say. To say, I shortened the places because I thought I didn't want to. Is it about right? Oh, three minutes past. Three minutes past. Your timing is heading in the wrong direction. Yes. So thanks to the younger, maybe members of the team who carried and had all the ideas. I was probably the Probably the holding them back, always saying, Oh, let's understand this or that. And Alan, of course. And thanks for Sar and NSF for finding me this part of the support. And thank you for your attention. Sir, I have I have one question which which is actually related to the first part of the talk. So you said that you construct an almost Riemannian manifold. Oh, the first part. Yeah, the first part. And on a Riemannian manifold, you have an exponential map. A Riemannian exponential map. Yes. Can you write down or can you is there a form? Yes, yes, yes, yes. It's basically the the modes map. It's basically the the Mohn's map. You have a velocity field and the geodesics are exactly those. So you have a density and you have a velocity field. The velocity field needs to be a gradient of a potential. So then from there on you launch a geodesic. The geodesic is exactly the mechanic geodesic, which is this. Now the genesic again can only extend it forever. You even get to sing it. And this is at the heart of many of the problems that one can have when you try to think, for example, to the dimension surfaces in Wascherstein. So for example, the interesting question, I'm just posing it here, is to think of a Wascherstein triangle. To think of a Wascherstein triangle. Let's say I have three densities and I want to have something in between. Unless the Monge maps commute, the triangle becomes thick. So the geometry is extremely rich. So more on that, hopefully soon, but I expect many of you to, I don't know, people, this is really fascinating because, again, this is an example. This is an example of how you bring in our new insights. The insight is that lengths correspond to dissipation. And then movement of thermodynamic systems have to be understood in terms of how their state. What is a state? It's a distribution. It moves in a Washington space. So and and we can ask lots of questions here, I'm sure. Questions, and I'm sure there will be a lot more. Yes. Oh, one more. No, please do it anyway. Yeah, just going back to the spelling engine. If you really wanted to make a perfect analogy with the recorded circuit, could you do it? I mean, for the spelling engine that you have in the example? Yes, I mean the torque is not the same. I mean, the torque is not the same. It's just a difference in how, you know, you have basically something like this here. So you have some inertial terms and the heat producing a certain torque. And there are models for such thing in the literature. So, but for instance, the capacitor, which is the case of the Sterling, and would it be the size of the capacitance? Would it be the size of the map? Be precise from the formula. I have to remember this. Yeah, I have to remember this. I have to look at the formulas. They are not exactly, you cannot do the exact correspondence. But it's very similar. Yeah, and probably your example problem might be cast exactly in similar terms. So I think one was more n non-linear than the other. Maybe I'm mere remembrance. Other, maybe Amir remembers. Do you? No. Okay. Yeah, but it was, you know, they're both very complicated. When you go to physical systems, they are complicated. It's not simple. Okay then, it's lunch. Remember, for the guided tour, we meet at the lunch place at 1 p.m. and at 2 o'clock here. 1 p.m. and at 2 o'clock, here we'll have a group work. Alright? Thank you for the channel. Just to give you a couple of chats, can this be out of the continuity point?   Yeah, I think you studied the bunch of the features that we're interested in.  Colleges are also in Europe colleges, I mean in Switzerland colleges, right? Yeah, exactly. Don't turn it all. University is university. And we have a lot of schools. And I think not everybody should go to school. Absolutely. My son definitely should not go to school. I don't know. No, but not like mathematics. Oh, yeah. So you can file that. Oh really now we're thinking about it. So Ames the publisher wins like the full board part of our windows. So I was the first one. And then everybody else the next day. It was behind the scenes, it was orchestrated. And frankly, I'm out of here. And so we're starting as a new journal. Yeah, but completely different theme, right? The name of the journal is going to be one script. Okay. So that should be starting some charts. Right, so the offering for presentation is only voluntary and if you volunteer meet the promise that it's open access, otherwise it's fine. You can keep it for the journal. So I know GGMs have something. Yeah, what do you want to talk about? Yeah, that was really nice. I was hoping that by now we would have sort of told you somewhere. Right, I was actually having that because it's all fun disappear. So, you know, I mean, Matt Well, he kind of. He's the one who started it, I guess. That's right. I hope it's really coming. Yeah, but the new one looks like it should be more or less people. I'll see you in a bit. Yeah, I think a lot of people were like that. I feel bad for all of you who were in the middle of the process of publication. It was in review and there were bunch papers in review. They do publish in a very different journal, actually. Yeah, I do remember, right? So the answer is that. But something that worked that they were doing stories that are changing the other story. So, I mean, so for example, I'm not going to store it. That's right, that's right, exactly. Everybody on the editorial book got this email. So those emails were not single hours, but they might not arrive. So that's how we're going to show you. So it's because we are very concerned. Because we are very concerned, exactly, about this quarter. If we had a finance support, no problem. Definitely no problem because Angela's GP editors. So you're going to follow the colours. Yeah, well we have some of us we were concerned about ourselves again sometimes that in the name of it. He wasn't really working. It's glad we talked about it. I'm glad we did it of course. So basically, yeah, I mean, it's also listening to a journal. So now what's happened is they go basically when it's there.  I think I didn't mention some of you. That's right. The book by the man. There's two books. You know, I would say if you are interested in fine communities and And uh, I don't know why they have some versions on it. But is the next number one? He says so now he's a fine anarchist. You know that's in the UK. Yeah, it's second because of war. This one is a drink. Yeah, which I wanted to do with the next one and then they was in the last physicist. Didn't they try to write this? Because you were referring to Doctor and the other name. But I forgot about that. And then looking at COVID-maybe more exclusive. So it's not all evidence. But the point I wanted to make, maybe is that this is a gradient flow in the touch of everybody that we define. Thank you. So because I thought like the the uh Maybe also written. Yeah, there is something like that. Yeah. Yeah, right? So we had a first paper, which I'm not excited because we tried to do So in this course, we try to do cantologist. Cantorous type or point. So cantologous type of counting means you have a density matrix, you have a density matrix, and you're looking now for a density matrix in the product squared image. That the marginals, what are the marginals, the partial traces. You have a very big matrix, and this partial trace. So every part of this component tracing out procedures. And we tried to do something. But we didn't manage to get it or something. So that was 2007. This one is also English. When it has been taking our steps, and then in 2016 we passed that anywhere that was objective or not. And for that, we had insight from some work with the added cover a few of the languages, and I had three references. They covered mass. I was telling you next I can make it mine. Within a week, uploaded to the one that we have to the other. And there's a long story together. And you know, that's, you know, so actually. So actually, we were, that's why I put ourselves third. So the first one was Carl. We played there with the other two guys and then it was us. Yes, I think. So it was not aware of the work. And by the way, the approaches are slightly different. That's completely different. Sometimes we can open the physical temperature. They require something which you call the GNSS. Once they have this GNS thing, they can switch to a different metric. And with this metric, I think they can derive this as a kind of break. And so this is the copy result of that work. Maybe it's my unit kind of pretty deep. Yeah, when there were 10, I had to connect. Then there's another. So we don't extra speak. He tried to do a very similar thing. So all of this has to be on science. In other words, that they wanted to have this. Don't ask me, I don't understand the steam, but it's like is there a connection? I know I it was just about our remote. Yeah, yeah, I remember that this genus so far it's but yeah, I mean maybe to tracing it through the work of Herrick Carlos. Far, Gary Carl has tried on this. Ken, we benefited from it because he had done a paper before. Ken and his work goes into much more detail. We were doing the other stuff on General Podcast and Parallel. We were trying to complete the things I was doing earlier. By the way, another thing that might interest you just Might interest in just not, I didn't discuss. There is a quantity, and this is in a paper average that we have with telephone. There will be twenty guests of this I went to North Coast. So in fact, if you have the simplest fancy versions, it will more likely do. You know? You said, okay, I can be D, you know? Okay, oops, it's a different story on the box on the IA. So the sequence algorithm is. I just talked to a friendly thing that I've seen. Let me explain and then I actually also mentioned that. So let's say you get to the state. So you know, for example, here's something. Let's say here's a story that relates to the. This is the shadow I expected all from Anton. And you have some correlations between this. But you have very home, Swedish alcohol. This problem is very far back in statistics. It's like 52 years old. It's not static. And it's called come to me. So they have basically no joints. They have this and this and this and this and this and that. And they have some joint. However, this joint distribution, to sum it up this way, is not consistent with data, it's not consistent with the data. Yeah, but we will leave Friday after. So basically, what do you have? You have a matrix of data. That's called a park. And if you sound in the direction of the minus and if you sound during the direction of our attack, you don't get the ten hours plus the additional difference. So then the problem is, how do we collaborate data so that it digits? So, what they did is, and they were doing it well before Singapore. So, they give the money or preparation and they don't get moved. So, what do they do out of it? They take time to get something. So, they apply whatever they got. Multiply with a diagonal which inverse what he does. Then you multiply with a variable that's the mean. So now, if I multiply by one, our class is ended by new role. So I corrected the role. At all. So I'm both of us in it. But now I go back and I see does it match? No, it doesn't. So then I do the same on the other side. This is called the diagonal scale. And you multiply it by this, you multiply it by that. And then you get it so it's consistent with the modules. So they were trying to do this thing and they were doing it with many, so this is tension. And this is called task force. This is called task called this chat again. So then SIMCOL actually proves that this component discold. And that's why it's called the SIMCO algorithm. The SIM algorithm means you can give it a data on each side. It's exactly the short gear algorithm. Because the Schroeder algorithm is the Schroder algorithm. So the name, um it's called the shyword mobile read, whatever it is, for example. So if you go to all three, so two. So this is this is W. Initially we were trying to do W stochastic. One, one, only the direction. But this is this is a installation portal. You can do whatever you want. Now this is so that's that was the original thing and it can be scaled And it can be scaled through other things. So, this is another setting, which is the quadruple. And this is the paper, Positive Contraction for Classified Quantum Systems. This is my work with Mintella. So it goes like this. This is really the Schrodinger bridge. They shorten your bridge for a quantum system. So, what is so? Here, this is a little bit deceptive. You can do it a little bit differently. You can look at, this is like a joint probability here. That's what it is P of pi pi. Because pi usually denotes this process. In this presentation, transition probabilities. So the single iteration amounts to starting from a row zero, you need to factor it as phi zero hat, phi one, phi zero. This is entry-wise multiplication. Entry-wise multiplication. And then here, the rho one is entry-wise multiplication. Entrywise multiplication so that in the forward path, the zero hat gives you the one hat. And then here you compute the phi one, and then you get the phi zero. So then Schrodinger's problem converges. Now you can do it infinite. These are matrices. So you can take a vector here, multiply. That's a transition probability. So when you are So, when you are done with this, and it converges because of the Hilbert metric. So, here you compute that and you know sign this. Now, if you look at it, if you go here and you multiply with the inverse of the diagonal that has, oh no, you multiply with the diagonal that has phi one then Then you get rho one, because that's what the multiplication is. And if here you start from row zero and you multiply with the inverse of the diagonal that has phi zero, if rho zero looks like the product, you divide by that, you get phi hundreds. So this maps to that, and in the reverse And in the reverse direction, the identity vector with the transpose of this, which is d phi1, gives you phi 1. And if you have phi 0, the transpose of that