My co-authors on the project and director Assigned to John Hossel, Alexander Koresnikov, and Noma Siglietika Maya and Arkansowic. I will report particularly on joint papers with John Hossell and Koznikov and a papers with Koresnikov that are recent. There is some notation that most of you hopefully are familiar with. What's important here is, so this is Minkowski sum, it's just pointwise sum of two sets. Sum of two sets. Support function of a convex set, again, is something that hopefully most of you are familiar with. It's a norm. Support functions of Minkowski sum of convex bodies add up. Importantly, we denote unit, I like to denote unit normal by n with sub-index x. It's a normal which sticks out of the point x. And the support function at the point nx is going to be equal to the scalar product of x with an x, as according to the x. product of x with an x as according to this picture. Second fundamental form which is Jacobian of the minus the Jacobian of the Gauss map will be denoted by this Roman Ii. Its trace mean curvature is Hx. So the Bruminkowski inequality is something that again hopefully most of you in this room are well familiar. So I'm going to go through it very quickly. It tells us that Lebesgue measure is log concave. If you take a convex combination of sets in general Combination of sets, in general, Borel measurable sets, it's going to be bigger than something. It's going to be bigger than the logarithmic average of the volumes of sets. And again, as hopefully most of you know, it itself improves to 1 over n concavity. So a priori, this second line here is stronger, but because Lebesgue measure is homogeneous, those two statements are actually equivalent. And in fact, Lebesgue measure is 1 over n k. The way to think about it, it's an isoperimetric. Think about it, it's an isopyrimetric statement. For instance, it implies the classical isopyrametric inequality that tells us that overall Borel measurable sets with volume fixed to be equal to the volume of the ball, the perimeter is bigger than the perimeter of the ball. And here is the proof using the Brumingowski inequality. It's just indeed one line. This is the definition of the perimeter. And if you apply this statement here, we get immediately the inequality that one can check is exactly the right thing. One can check it's exactly the right thing. And in principle, it implies many other parametic things. I'm going to primarily talk about the local versions of the Primikovsky and other inequalities. So the idea here is as follows. Fix pair of convex sets K and L. And let's try and move continuously the convex set K into the convex set L. Namely, we move, we push convex set K at every point along the normal vector. At every point along the normal vector with such a speed that in one minute we get to the body L. Namely, we measure the difference between the support function of k and the support function of L here in the direction u, which is the normal at the point x. We fix this function, let's set this function to be psi. And we will also, yeah, so then the convex body kt, which is this guy, 1 minus t k plus t L. This guy, 1 minus Tk plus TL, has support function HK plus T psi. At the beginning it's HK, at the end it's HL. So because of Bernikovsky inequality that we know is true, we know that the volume of this convex body KT is a log concave function. So if we let capital F to be the volume of this intermediate sort of interpolation body, then the second derivative of the logarithm is going to be negative. Going to be negative, and the second derivative of the logarithm of a positive quantity is negative if and only if this situation happens. So let's summarize this. This is what we discussed on the previous slide. If you consider this continuous system, the function f, which is this volume, has this property. Let's consider a function on the boundary, just for convenience, capital F, which is nothing but psi at nx. psi at nx. So it's a function at x with which we push along the boundary. Then at 0, f of 0 is the volume of k. We haven't pushed anything. The situation at 0 is just k. The first derivative, it's something pretty intuitive, right? It's a formula for the mixed volume that many of you know. That the derivative of the change of the volume is precisely just the integral on the boundary of this function little f of the speed with which we push. Of the speed with which we push. Second derivative is a little harder to compute, but it has been done. So this is the second derivative. It's a mean curvature and this is the second fundamental form. So the Bremen-Kovsky inequality implies the following inequality. And also it follows from this inequality. One can integrate in the specific case. It's not hard to show that if for any K and for any psi for any F, say we can show this inequality, then conversely Brunlinkowski inequality holds. Prim-Linkovsky inequality holds. So, this is what I'm going to call the local version of the Prim-Linkowski inequality. It was established by Khalesanti and by Khalesnikov and Milman. And if one wants to be honest, one has to add 20 more names, such as Bakovli, Du, Bakri, many people. Okay, so I'm going to make a very abstract observation, just something completely ideologic and abstract. Suppose we have any bilinear form. So on the previous slide, we had an any quality. So, on the previous slide, we had an inequality with some bilinear form, right? If we have an abstract bilinear form on some algebra that's also a linear space, in this case, it's an algebra of functions. Suppose we know that on the diagonal, it's always non-positive. Suppose we now fix any designated special favorite element in this algebra. Then, because we know that this inequality holds for any element a, it also holds for a plus tz, for any t. Right? Right? Because A plus TZ is also an element of our algebra. But it's a symmetric bilinear form, so we can open this up and just get a quadratic polynomial here that we can subsequently then optimize and get the sharpest possible inequality in this family of inequalities. So, what's written here is a family of inequalities. Let's get the sharpest one, the one that happens at optimal t here, which happens to be equal to this guy. This is, of course, Equal to this guy. This is, of course, nothing particularly impressive. It's something that's called Schwarz inequality or Borsch inequality in the case when it's just the usual scalar product. But what's important here is that, as I said, with respect to any element, we can get a sharpening of this inequality that corresponds to this element. What is so strange? Our our Berlinian form is negative, right? So this is. So, this is you don't know. I mean, but okay. But also, because I downloaded this picture from the internet, it just looked parabola. But anyway, yes, this is a negative number. Could be a negative number. Smaller than zero. Yes? Okay, so it's sharper inequality, stronger. And what's important, it's invariant under the transformation plus a plus tz. If now here we try to optimize, we're going to fail because we've optimized everything we could already. Okay, so let's apply. Okay, so let's apply this completely abstract observation with our favorite T. It anyway should be negative. Yes? Can you come back at Z, it should be negative. Okay, very important, yes? So it can be 15. Why can it not be 15? Because it's negative. What if QAZ is positive? No, no, but at Z. The value of this quadratic at Nt is negative, already. It's just a picture from. Asham, do you understand what I'm trying to say? Yes, but it must be 15. I don't quite understand Ajam. U of A Z can be anything. That's okay. U of A Z can be 15. It's that T, so Q of A plus T Z. No. Okay, let's. Okay. I don't know what you're saying. Let me continue. So this is okay, right now. He's saying that this is negative. He's saying that this is negative. Okay, yes, you're right. This is our favorite quadratic form. Okay, so we want to optimize this. This is our inequality. It's a quadratic, I mean, I'm not going to write it as a quadratic form of two elements, but you can easily do it. Obviously, write it as Q of F. Let's optimize it with our special, absolutely favorite function, which is the support function of a convex body K, right? This X scalar product with an X. Which corresponds in terms of this flows, corresponds to dilate in a convex body. Flows corresponds to dilating a convex body. Lebesgue measure has some invariant properties in terms of dilating convex bodies. So we might hope that this thing, if we do the Schwartz inequality with this function, going to have this nice look. And of course it does. If we optimize with respect to, yes, this special function z, we're going to have something that looks almost the same, but is sharper. And this is, of course, sorry, this is the local version. This is the local version of the one over n concavity of the Lebesgue measure that we showed on the first slide. Non-surprisingly, by the way, if we plug K to be the Euclidean ball, we're going to get something that we know as a Poincar√© inequality on the sphere, where it's a boundary of the ball is the sphere. If we cancel out correctly, it's the spherical gradient local with respect to the sphere. We get this inequality. It's a completely sharp inequality. 1 over n minus 1 is the first eigenvalue of the Laplacian of the. One is the first eigenvalue of the Laplacian of the sphere. So one has to appreciate this Bruninkowski inequality. Its proof I didn't mention, but the proof by Lusternik is like two lines. You can explain it to a high school student. But it implies something really serious. Typically, sharp estimates on eigenvalues are not easy to come by. Okay, so because this additive version of the Primikovsky inequality came from this optimization using the Schwartz, the Straitz inequality. The Schwarz, the Schwartz inequality, it's now invariant under the transformation f to f plus tx and x. If I plug f plus tx and x here, I'm not going to get anything different. Also, obviously, it's invariant on the deviating the function f. So let me recall the notion of mixed volume of convex bodies. They're just normalized derivatives. If you have convex bodies kn, we take k plus tm and take derivative. If we take k plus tm and take derivatives of this quantity normalize with appropriate constant, we get something that's called mixed volumes. I like to denote them with sub-index k here. So if we plug f here to be the support function of some condominium body m, then we get nothing but Minkowski's quadratic inequality that we will hear from Virgo a lot more about, of course, very soon. Maybe not, actually. And what's important here is... Minkowski is second inequality. is a second inequality. Not the quadratic inequality. Quadratic inequality, you would have like K, L, and M. Right, that's it. So you mean in the partial case of Minkowski-quadric inequality, okay, you're going to have to bear with me throughout this talk. It's going to always be called, okay, Indian second inequality, whatever. And Brimenkovsky inequality is equivalent to this inequality because of this invariance. Because it's enough to assume that the function f is just a support function of subconvex set. Otherwise, we add a lot of support functions. Otherwise, we add a lot of support function of k and get a convex function there. So, this is the option. It's equivalent to the Brun-Minkowski inequality. Okay? Very good. Now, could we prove Brun-Minkowski inequality by proving direct this is Minkowski second inequality? Well, so there is how to prove that it appears, for instance, in a recent paper of Kolesnikov and Milman from 2015, I believe the first paper with this method. With this method, is, but it also they do it in Riemannian set, and some of the first results were done by Cabrel, also in Euclidean set. And so, the idea is we want to prove this inequality for any k, for any f. Consider the function u on the space which has normal derivative, little f. Then, the first derivative term we can relate to u via just the divergence theorem, as follows. And when I think of a minimum. Follows. And Kinesnikov and Milman showed that also the second derivative can be from above estimated by something that only depends on U. And of course, these things appeared previously in the works of Gree Mri and others. Here this involves integration by parts twice and also some elementary inequality point twice that uses convexity, by the way. So everything basically points down to showing that for every f on the boundary there exists a function in space such that this inequality holds. Where variance and expectations are taken with respect to Variance and expectation are taken with respect to the uniform measure on k. And so, if we want to find best possible such u, of course, there is a multitude of functions in space with given normal derivative, we want to kill this variance. We want this variance to be as small as possible, preferably zero. So, in order for this to happen, we can take Laplacian of u to be constant. Luckily, so this is the main ingredient of this proof, the solvability of a system with Neumann boundary condition for Laplacian. The solution exists. The solution exists by the standard theory of Sobolin spaces. And so, yes, such u indeed exists. And the remaining inequality is an elementary point-wise fact. So, this is the argument. Now I'm going to switch to Log-Brumenkovsky conjecture. Are there any questions so far? Okay, so this this was the story about the Brumlinkowski inequality. Let me now discuss the logarithmic sum of convex bodies. Sum of convex bodies. So it's a sharper version of the Minkowski sum of convex bodies. Basically, what we do is we take intersection of all subspaces of this form. So the support function of the set is smaller everywhere than the mixed, this logarithmic geometric average of the support function of K and L, and it exactly equals to it, by the way, on this point of the surface area measure, by arithmetic, geometric mean inequality. Arithmetic geometric mean inequality, this set is strictly smaller than the Minkowski sum, because geometric average is smaller than arithmetic average. However, Bernotsky, Ludwig, Young, and John conjectured in 2011 that maybe this much smaller set is still large and still bigger than the geometric average of the volumes of the sets. And so, not much is known here. It's more or less everything that's known about this global formulation of this conjecture. It is indeed important that there is a lot of connection. There is a lot of connections of this conjecture to questions that go back to Fiery, to some questions from the 80s. It is particularly very important because it's a quality cases. I'm not going to say actually almost anything, basically nothing about it. I'm going to say a few words in the end also about things that it implies. It's known in dimension two, as was shown by Baris Borosky, Lodochiant and Janke. Alina Stank also showed it for polytops earlier, basically. For in the digital sets, it's known to a bit two, as was shown by Christopher Sergulou, and actually Padora, Delizze and Marie had a similar argument. Pedrofer, Delisi-Marie had a similar argument, and it's true for complex convex volumes, as was shown by Leon using complex interpolation. So, this is our conjecture that we will discuss. I mean the conjecture that we will discuss. So, let's discuss the local version of this conjecture now. So, I'm going to go through it quite quickly. Basically, not much. So, what changes comparing to the Brun-Minkowski situation? When we were discussing Minkowski sum, we were going additively. We took a convex body and the load in every normal we pushed with some speed to achieve. Will be pushed with some speed to achieve L at the end. Here we have to go multiplicatively. And instead of the difference, we take the ratio of the support functions of k and l and then instead of adding it to hk we take power. But if we linearize up to the second term, still we could look at it as hk plus t phi, plus something that depends on t squared, where phi is now different, and therefore f is also now different, but still Lodbrun and Kowski. Now different, but still Lodbrun and Kovsky conjecture: if it was true, it implies an inequality of this sort. And still, the first derivative, the first, the value of f at zero is still the volume of k. The first derivative is not going to change, actually, at all if we denote f correctly, because this t squared addition is not going to influence the first derivative. So it's t is still going to be integral of our function if we correctly choose it. Second derivative is going to change a little bit. The first term is going to be exactly the same as before. Exactly the same as before, governed by this part. But because we have this t-squared addition, there's going to be some additional term that one can compute. And so I wrote it down here. This is a result of our joint work with Kelly Seintern Marcy Greater from 2016, where we showed that this inequality that already kind of popped up on the previous slide follows if the Logrim-Mikovsky inequality is true for any symmetric convex k and for any function f. It's very important. Function f. It's very important. I'm not sure if I paid enough attention to this. All these conjectures for symmetric functions and for symmetric sets and for even functions, everything is symmetric, or else it's not true. I think I kind of answered it a bit quickly, well now, isn't it? Okay. So we verified that in particular level, we observed that it's true when k is a ball for any even function f. Indeed, if k is a ball, then this rewrites as almost as Poincar√© equals. As almost as Poincar√© inequality, but now instead of one over n minus one, we have one over n. And of course, it's not true for all functions anymore because one over n minus one is sharp. But this is true for even functions. And moreover, for even functions, it's true with the constant one over two n. Because the second eigenvalue of the Laplacian on the sphere is one over two n. So basically the this concavity near the sphere is even better for even functions than Log Remikovsky predicts. So this is true. Predicts. So, this is true. Next, Kalestiko and Milman moreover showed that the local version of the Bruminkowski inequality is true for all L P balls for P between 2 and infinity inclusive. So they generalized this result much more. And something that we didn't do, we were not able to do in our paper, and Klesnikov and Minman also did not do it in their paper, is we were not able to show that this local version implies a global version. This local version implies a global version. Initially, it seemed to us, I know, maybe remember that something's gonna just, of course, work, you're gonna write a few definitions, down, it's gonna work, then it didn't work. Then I remember talking to Sasha early when they started their project, and he was telling me some of the stuff they were thinking to do, and I told him how we're gonna show that this local version applies to the global version. And he was like, is there a problem? God, we just write a few definitions down. Anyway, it turned out that it's quite difficult. But luckily, after both of those papers were written, Both of those papers were written. Chen Huang, Li and Liu, and Eli Puterman, who will talk about his work here, showed that indeed it's true the local version implies the global version, and this means that this whole approach is actually hopeful, like there is some hope in this approach. Once again, I want to say that it's not easy because in the Brun-Minkowski, in the case of Minkowski addition, the reaction is very different, which is much, much easier. I want to emphasize, of course, I want to emphasize, of course, for some reason people like to make wrong assumptions about what's known and what's not known. So if we know that local implies global, and if we know the local form for a fixed body, such as the ball or L P ball, of course we don't know the global version for this L P ball. And because we have to go along the system, right? It doesn't work like this. So it's actually the global version of the conjecture is not known for any fixed set. There is no fixed set such that we know that for There is no fixed set such that we know that for any other symmetric set, unless we impose some unconditional assumption, obviously. Right, so this is not known. And what would be really interesting, in my opinion, is to understand for what functions f could we do this sort of, this prove the local version for all k. So if we fix k, some examples are known, but it's true for all even f. Now, if we fix a specific one good f, could we say One good f, could we say that it's true for all k? And one such answer I will give on two slides. In the meantime, let me comment on the invariance properties of the Log-Berminkowski inequality here. So Kolesnikov and Meinman observed that this local version of the Log-Berminkovsky inequality is actually invariant under this f plus the support function addition, something we kind of ignored initially. And in particular, this means that if you add to this function f any This function f, any multiple of the support function of k, it's still going to be exactly the same inequality, but our function is going to now be a support function. And this reduction played a major role actually in the work of Edin. He will talk much more about it. So actually, in order to prove Lobby-Minkowski conjecture, it's enough to show this form, just convex geometric form in terms of mixed volumes for any K and for any M symmetric convex volumes. So this is an strengthening of So, this is an strengthening of Minkowski's second inequality. Also, it's important that all those things are invariant on the linear transformations. Okay, I'm gonna go ahead. Let me make a quick remark about the case of the cube. Like I said, Kalesnikov and Milman showed that it's true when the convex body k is a cube. Here's, by the way, a simple, very simple way to show it. Because you can write that inequality. You can write that inequality. If k is a cube, the inequality I wrote becomes just some kind of simple, simple guy, yes? Because of symmetry, because this is a norm, it's symmetric. We can boil this sum down not over two n elements, but over n elements only. This is important, it makes it true. And what remains here is to estimate the second mixed volume from above. And the cool thing about mixed volumes is that they're a monotone. So if we just simply So, if we just simply do nothing more intelligent than inscribe M into a parallel pipette of the smallest possible parallel pipette, then the mixed volume is bounded from above by the mixed volume. This is parallel pipette. And that guy, just by differentiating determinants, you can see it's just going to be some sum of mixed terms of Hm at EI and EJ. And this is going to make it actually just a quantity. So, this is a very easy thing, as it turns out. Thing, as it turns out in the case of the cube. And now let me tell you one example of a situation when the local version of the Log-Bermingowski conjecture is true for any k, for one fixed function. I formulate it again in terms of m. So it's true when m is an interval. It's true sort of in one direction. If we take k and we start pushing it in one direction, in the direction of an interval, this low concavity with respect to zero addition is going to approach. Respect to zero addition is going to occur. So, how do we prove it? So, this is again, this is the version of this conjecture. So, first, our life in the case when m as an interval isn't believably simple right away, because actually the second mixed volume is an interval is, of course, zero. Namely, the sum of k plus t interval is just a linear function. It's the volume of k plus t times multiple of the projection of k. Multiple of the projection of k in this direction, as shown on this picture. The known geometry. So if second derivatives of linear functions are zero, so this guy is zero. I remind you the formula for the support function of an interval. That can be an exercise. Now, the first mixed volume, of course, it drives the projection of the convex body in this direction. Again, it can be derived from here or from, well, anyway. Can be derived from here or from any other way. So basically, our goal becomes to show that this integral is bounded by four times the projection squared over k. So this is what the whole statement rewrites. I'll rewrite it here. And I'm going to separate these multiples. So what we're going to do here is actually estimate this term point wise for every u in terms of the projection. How does How does it go? So, first of all, let me remind you of Fubini's theorem that tells us that the volume of k can be recovered as an integral of sections in this direction. And because of that, we can estimate 1 over the support function of k from above. In terms of the maximal section, which for symmetric section yes, which maximal section in the center, blah blah blah. Okay, so we estimate the value of the support function from above by Support function from above by this quantity because of Covini. And since the projection of a smaller set is smaller than the projection of a larger set, we know that if we write this quantity, this is nothing but just the projection of the section. So it's the section with u per projected on v per, which is smaller than projection of k, and so therefore we get everything that we wanted. In the last line, there was also Gaucher projection formula. So again, it's Formula. So again, it's quite simple elementary argument. However, it makes me or anyone perhaps wonder if there is some application of the Fourier transform technique in a more intelligent way in all of this. Lord Briminkowski story. Here is something we don't know to do. And that's ridiculously simple, again completely embarrassing. So next step, suppose we know it for an interval, can we do it for a square? Just two-dimensional, two-dimensional squared. Two-dimensional squared, two intervals, and add them. This is how everything rewrites. The second derivative then becomes double projection. One can check it again in a variety of ways. Question, is it true or is it false? It's one of the other. So we don't know. It does not reduce to the case of just one. So basically, two directions don't become one direct. It's kind of a little bit like this localization stuff. We cannot localize to linear functions, but we can localize to quadratic functions or whatever. It does not. Functions over there, it does not reduce back to one interval and one can find examples. You see, you can estimate this quadratic terms and this inequality, but then what remains is already, unfortunately, not negative. If it was true, then we could show the local version for any k and for any zonoid m and convert, and moreover, for any zenoid k and any m. Because it's invariant and the linear transformations, and we can add it up, and everything basically if you write it for zonoids, it's boring. If you write a fusonoid, it boils down to the pairs of intervals. But we don't know if it's true. So, okay, we don't know anything. Let's talk about a simpler conjecture. I will be Bromikovsky conjecture, which was also formulated by Burtsky-Ludwac, Young, and Junk. It's a equation that interpolates continuously between Log-Brominkowski and Bruminkovsky. When P goes from 0 to 1. When P is 0, it's Log-Briminkovsky. When P is 1, it's usual Bruminkovsky. So. So, and this inclusion indicates that the conjecture is stronger when P is smaller. And also, again, by homogeneity involves the Schwartz story, itself improves for every P. So, Kolesnikov and Milman developed the local version of Alpi Branikovsky for general P and verified it for some range of P, between one minus n minus one, one half and one. Half and one. And Chen Hong Li and Levan Puterman again showed that this local implies global, and therefore the conjecture is actually true for this range of p. Which makes you kind of wonder if the log-Bromikovsky inequality could be false, if this is already definitely true. Well, who knows? First result: I want to say a joint work with John Kossel and Sasha Kalesnikov. If that's KNL, If sets k and L actually are such that k is contained in L, then we could slightly improve this exponent from minus one one half to minus sorry, three halves to minus 0.75. However, it's a multiplicative form of LPR-Minkowski conjecture, so there is no dilational variance. We cannot include the more general statement, but it slightly improves this interval. In other things, I just want to say unbelievably quickly. Quickly is that the P. Bron-Ninkowski conjecture for Lebesgue measure automatically implies it for all log and k measures, as was shown by Serro Glouces, but in the multiplicative form. In particular, the story about the Gaussian measure and sets K and L B in dialects corresponds to the B conjecture formulated by Banaszik and popularized by Rafo Latala, which was verified in the Gaussian case by Kadorafer Delaisi and Marie, but it's not known for general local concave measures. Measures. For measures, this inequality does not self-improve automatically because we don't have homogeneity, but randomly it turns out that it does. And the first such result was obtained by Versegdi Nejazvavich and myself. We showed that if log-Broming-of-C conjecture is true, then this inequality holds for any even log-concave measure in symmetric convexes K and L. It's 1 over n concave. More recently, we showed also that, in fact, gamma. That in fact, gamma was done. In fact, it's actually true that it implies for every p, and moreover, this inequality strengthens when p decreases, so it actually always kind of self-improves. This was conjectured independently by Garnier and Zwaivic, we proved that with exponent 1 over 2n. With Hosel and Kalesting of this, we obtained a collection of results. One of them I will highlight now in the absence of time that the Gaussian-Log-Berminkowski inequality holds for all products that contain a sufficiently large pole. Sufficiently large pole, maybe not so surprising. And unfortunately, I really don't have time for some other interesting things. And they are interesting, and I'm very excited. I don't have time for them, but I will thank you for your attention. Any questions, suggestions? From the internet. I think one? Yes, of course. It's one panel, yeah, it's one panel. But it's not negative, right? Can you go back to slides? Yes. How many slides? One to the one with the P one more, probably? No, this one. I really want to talk about that. Okay. I want to give you this one for the moment in big more slash. Run for a moment and become a slowly get. Okay. So, yes. So if K and L contain a sufficiently large pole, you can always get this P-Brom-Minkowski inequality for Gaussian. This is all for Gaussian measure only. We have also more general results, of course. And yes. This inequality, if mixed with P and Q, we also can get in some range. And if one of the sets is inside the others, Of the set isn't in the inside the other, then you have something a little better. Yes? So on another two, so Gaussian and Brominkowski, so Gaussian Brominkowski then. What do you mean by Gaussian and Brominkowski? So the log Brominkowski inequality with ma it's um here. So then you have it for the regular zero near Gaussian, that's what I mean by Gaussian log Brimminkowski. I mean, the Gaussian dot Vimekovsky. Do you have the number under with P equal one? With P equal one is what we got with Sasha, and we just recovered it. And you said it is true also if K contained a ball of radius squared of n b to n. Okay, this is always true if k and all contain the origin. In spite of the fact that Koch and Nayar showed that you can't have one origin if they just contain the origin. We showed a long time ago, and I talked about it so many times already at conferences. So, yeah. So, yeah. This is true if they just contain the word. So, basically, the next theorem generalizes it. Any other questions? Then let's think about can we start soon? One minute. 