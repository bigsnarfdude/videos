So, the topic for our discussion today is the stochastic heat equation. This has the Laplacian term. This is kind of like the diffusion term. This softens noise. This is the linear feedback, makes the noise bigger, makes everything more wild. And then we'll be talking about intermittency in a minute, which comes. Intermittency in a minute, which comes from the linear feedback with driving noise, W dot. This is a centered Gaussian noise that's white in time and colored in space. This is because you cannot solve the stochastic heat equation with white noise in dimension two or bigger. You have to use a regularity structure for dimension two and dimension three, and you can't do anything in dimension four. Anything in dimension four. So we have some problem though, which is that we want to do this on a manifold. So how do we make any sense of this noise on a manifold? And this, yeah, this is a joint work of my advisor and our collaborator. And what we did here is that we're doing it in the Mali Avan sense, like a Wiener space, right? So we have to define the inner product on this L2 space, right? On this L2 space, right? If we just use the L2 inner product, that would make white noise on the manifold. So instead, what we want to do is we take this basis. Every single L2 function on the D-dimensional torus has a Fourier expansion in the Fourier series. And this is the inner product. It's also the L2 inner product because of the Plancherel theorem. And what we want to do is soften it. And what we want to do is soften it. We want to make it softer by dividing by k to the two alpha. So if alpha is zero, then this is white noise. And we just get the regular L2 in a product. And for alpha bigger than zero, it's colored noise. And you're going to be dividing by k for higher oscillation terms of larger k. Those correspond to higher oscillation, more singularity. And we're going to divide those out. And this. Divide those out. And this parameter, alpha is the main parameter for the noise. And this parameter, rho, what that does is it controls the correlation. We want to have a little, we have a floor on the correlation, but it could be negative. So we're going to talk about that in a second. This is the heat kernel on the Taurus. This is essential for solving the SPDE and the way that the heat kernel works. The way that the heat kernel works is that it starts out highly concentrated at one point, similar to Rn, but instead of approaching zero as it does on Rn, it approaches a constant function. Yes, and this is sort of a key qualitative difference that we're going to make use of for our proof. This is the formula of the heat kernel on the torus. I like to think of this as the starting point X, and then you're kind of And then you're kind of evaluating it at this other point Y. And it's going to be a product of heat kernels from Rd. Your typical formula here, e to the negative x squared is Gaussian. And then it's a product, right? It's a product right here of G's right here. So this is for each. This is for each dimension to get sort of a d-dimensional heat kernel, right? You take a product of d terms, and that is the d-dimensional heat kernel. And now we have to sum up over Zd all the equivalent points of R D that correspond to the same point on the torus. So if we want to look at X and Y on the torus, then we look at all the images of Y on R D in the same equivalence class of points, and then we And then we add them all up for all those contributions. And that's sort of going to mean that when we integrate over the torus of this heat kernel convoluted with something, it's going to be the same result as doing the same thing in RD, more or less. And so this is a kernel representation for our noise correlation. So this, the inner product we defined before for the defining the Mallevan space, the Wiener space to do. In space, the Wiener space to do stochastic integration in. That is now the inner product here can be expressed as an integral over the torus of two functions with this kernel F alpha rho, which is here. So, and you can sort of make sense of this because you are subtracting out the constant term of the Fourier transform of this. And then this gamma function here is set up to produce. Here is set up to produce at the end the same, uh, the same kind of Fourier weights that you saw right here. This k to the two alpha, that's going to come out of the big formula. And then instead of having the, we subtract out the constant term here, and then we put it back over here with rho. So, uh, this is important for controlling the noise is this bound on the heat kernel that you can control for large. For large time, you can control the difference between the heat kernel and its constant term with this function here. And so the F alpha, thanks to this estimate, we can reverse the dx and dt integration over in the definition of this kernel. So the integral is going to be zero. So that means it takes both positive and negative values. Positive and negative values. And so it's going to be bounded from below by a negative number in general for when row equals zero. But we can make rho bigger. And if we make row bigger, then the correlation can be made so it's greater than or equal to zero. And then we can deal with it that way. So the whole point of creating that correlation formula was so that the noise correlation was Correlation was this thing here: the one over two to the alpha. So we can divide out the highly occilatory parts. Okay. Okay. So this is the formula for the PDE right here. And this is familiar to everyone. I'm sure this is identity that the solution must follow. It's a mild solution, should be predictable. Should be predictable. And this is the Ito-Walsh isometry. So we can move this expectation inside because of the way the Walsh integral works. And that's the same as the stochastic integral from the Maliven calculus. And this is very nice because this thing here is the same thing as this. So what if we just take this whole formula, this whole formula here, and we This whole formula here, and we just put it back in to here, right? We want to keep putting the formula back into itself, and then we can sort of eliminate this self-reference, and we can just have a big long formula of series. So, the mathematician La Chen, who is our collaborator, this is the subject of his PhD thesis. And this is what he came up with, is this triangle operator, which we've modified a little bit to make it work better on the torus. On the torus. So basically, this is kind of, I like to think of this as a starting point, X0. And then when you do one triangle convolution, you put an intermediate point Z in. So you go from X0 to Z, and then you go from Z to X instead of just going from X 0 to X. And then you kind of take the correlation along the way between Z and Z prime. And yes. Program. And yes, question. Does this operator have a zero associate? Is this an associative operator? Yes, this is an associative operator. Yeah, yeah, it's a little easier also to show us associative in this new way. It's a little bit different from the Chen's PhD thesis, but it's an excellent thesis, I rest. But it's actually an excellent thesis. I strongly recommend it as a light reading. And here, this is now what we're going to use. We're going to have this infinite series LN, and it's going to start with just the heat kernel. And then you recursively put the heat kernel back in through the triangle operator to the Ln minus one operator. And what you get at the end is you have the initial condition, which is the heat. Condition, which is the heat kernel convolution with the starting measure, and you get this infinite sum of ln times lambda to the 2k. And this is the two-point correlation of the solution. So by showing this thing converges, then you're going to get the existence and uniqueness by some standard methods. And that is basically our goal in this. Basically, our goal in this situation, and it's very nice also because you can kind of separate out the initial condition very neatly, and so we can work with measures instead of just functions or something like that. Yeah, so these are some examples. So, it starts out with the heat kernel. Then, essentially, yeah, I like to think of it: you start at X0, right? Then you go to Whether you start at x0, right? Then you go to z, then you go from z to x. So then it's from at time s, you're going from x0 to z, and time t minus s, you're going from z to x. So it has a sort of a natural relationship to the pinned Brownian motion, which is which has this density right here on the on the torus as well. And so if you put it as the pin-Brownian motion, right, then what happens right here is that you can pull out this. happens right here is that you can pull out this these factors these factors right here uh g bar the two g bars uh now they're deterministic they don't have they're not integrating over it really it's not deterministic but you're not integrating over z and uh the whole thing is deterministic because it's an expectation but never mind about that uh yes so this this is nice and basically we can control the density of this object right here as like a single thing and the thing and the density of the noise and that kind of gets us everything we need to control these term by term ln so for um uh large we had to break it up into uh two regimes one regime which is large t that corresponds to the case that we showed where it's a uh the the density the the brown the uh heat kernel density is like between a minimum and a maximum Like between a minimum and a maximum because it's a compact manifold, so that's qualitatively different from the RD and makes it very, very easy to deal with because essentially you can control a pin-Brownian motion just with a heat kernel because you can control it from time zero to T over two, and then you bounded T below. So you have lower and upper bounds on the heat kernels for the larger T, and it makes it very easy to deal with. Very easy to deal with. So we define this thing K1 basically, and this is the part of the contribution of the noise that we need to sort of control at each step. And we have this for L1, we can show that L1 is less than or equal to the integral of K1 of SDS. And then for very small T, we have to do something different, which is that we have to control using the methods from Rn, which are from a paper. From RN, which are from a paper that LeChen and Kim used for the heat, the color noise heat equation on Rn. And the problem, though, is that when we kind of try to relate the torus to Rn, we could end up with multiple images of the same point. So let's say there's a singularity at zero for the noise correlation function. That singularity could also be at negative two pi and two pi. And depending how we coordinate, we might end up with multiple images. So we have to take a copy of a Reese kernel for each. Copy of a Reese kernel for each image of the singularity. And I didn't include this, but we also have to do something very similar with the pin-Brownian motion density. We have to control the possibility that there's multiple images of the peak. But so essentially, what we end up doing is we have this term K2, which comes from, it's very similar to the work that's already out there on the RD because for the small time, it's pretty similar. And this thing we control. It can be controlled if the if the other one that K1 is controlled, this one's also controlled because when you're integrating this thing, you just end up with a certain series has to be finite. I think there's a slide about it in a bit. And what we end up with is L1 is less than or equal to g bar times integral of k2. So, what we want to do is make h1. So, this is going to be k1 plus k2. So, we're going to control for small time, small time, and large time. And then Large time, and then we keep doing these iterative convolutions of this thing with itself, and it's going to be iterating over simplex, so we're going to end up with a constant to the n over n factorial. Yeah, this is the De Lange condition that falls out. So we need this thing to be less than infinity, and that works out because of the construction of the noise if one over k to two alpha plus two converges. And this, it also, if this And this, it also, if this factor is also finite, it's going to give us that this one's finite. These are all equivalent. So if 2 alpha plus 2 is less than d, then we get this convergence, which is nice. And so this is the result that ln is bounded in this fashion. And this is the Taylor series for the exponent. And so we have exponential upper bound on ln, on, I mean, on. Ln on aiming on the solution. So that'll give us the existence and uniqueness that we want. Then we want to get exponential lower bounds too related to the intermittency. So this the idea here is that this expression, you want to have an exponential lower bound for the pth moment. And that will lead to a situation where there is a lot of unpredictable behavior because you have these big moments. So these rare events are making most of the Events are making most of the noise. And yes, so you can do it by using the lower bound on the noise correlation, but you have to mess with this row factor. But we can actually use this Feynman-Kotz formula for the parabolic Anderson model case in order to hold that too well too long there. Uh-oh. Same. Okay. Thank you for your assistance. When I saw you do it before, I thought, oh, I could probably figure out how to fix that. Okay, thanks. Yeah, so this is going to. Yeah, so this is going to be ergodic. So basically, this is a, you can find the solution, make a formula for the solution with the pin-Brownian motion. I mean, not the pin-brown, just a regular Brownian motion. And you take a difference of two Brownian motions and you can get the solution, right? So this integral here, these two Brin-Brownian motions, they're going to be ergotic on the torus. They're just going to zip around and just cover the whole thing. So they should converge to the space average of the noise. Average of the noise correlation times time. And our space average of the noise correlation is just rho. So if rho is bigger than zero, this should be bigger than zero. And then we should get an exponential lower bound. And yeah, use the Jensen's formula to show that if this expectation is bigger than zero, right, if we show the F alpha part is zero, then the only core contribution is from rho, and we'll have the exponential lower bound. And we can do that. And we can do that working in the Fourier mode. So you have okay, sorry. Do you just hold it down? Okay. All right. Well, yeah, so this is, you can, you basically, these terms. Basically, these terms, you want to show that this part goes to zero. So that is possible by showing that this thing goes to zero here. And then this is just the Fourier transform of the heat kernel, which we calculated pretty easily. And that thing, it has a constant upper bound, basically. So we do, in fact, get. upper bound basically. So we do in fact get that this is bigger than an exponent of c to the t. So when we get our existence, which has some dimensional conditions, we are also going to get intermittency, largely because the manifold is compact. That's a big contributing factor, is that it's a compact manifold. So these are some future directions we might go and we want to look at a Hong Ye next presentation is going to talk a little bit about more general manifolds in the torus. And then we want to start, yeah, and he and these. To start, uh, yeah, and he, and these are some of the factors that come into the genus of the manifold. Something we could look at: Bourier expansion of the noise, uh, and the Laplacian eigenvalues. Does that have some effect on what's going on in the situation? And maybe look at the fractal dimensions of things, and maybe someday look at the alpha less than or equal to zero or the white noise case for manifolds, regulatory structures on manifolds, and things like that. So, thank you, everyone. Thank you to my advisor. I just graduated. I'm very happy. And I'm And I'm a collaborator with all the conference organizers. Thank you. And thank you. Okay. Any questions? Oh, yeah. So, and he asked me what's the difference between TD and RD. So, the biggest difference is that the torus is compact, so the heat kernel converges to like a finite number instead of zero. So, the case with large T is pretty simple and it's sort of very elegant to deal with. And then for very small T, we kind of have to compare everything back. We kind of have to compare everything back to Rd because the heat kernel concentrates at the origin. It's like a direct delta function or something like that. And then, so there are a lot of complications come in mapping everything back to RD because, for instance, if everything's not centered exactly right, then you can end up with multiple images of singularities. And yeah, something that adds a little flavor to the whole thing. Flavor to the whole thing. Is that what? Did I answer your question? Oh, whoa, did you have some more precision? Okay, yes. So there is phase transitions in RD. So the parameter, so for certain, the noise is for dimensions one. Noise is for dimensions one and two, I think there is always full intermittency. But in dimension three, depending on how the parameter of the noise, so if the noise is very rough, it's always going to be full intermittency. But if the noise is softened and pink, then what you'll find is that for certain values of the parameter lambda, there is no intermittency. There's no exponential lower bound. It goes to the that the x exponential. the that the x exponentially it goes i mean it doesn't go to zero but if you look if you try to make exponential the thing the exponential lower bounds will be going to zero something like that that the the parameter but uh for for lambda the the parameter from the equation if it's big enough then you get full intermittency so you get a phase transition there's no intermittency and then there's um there's intermittency so that's the main difference yeah Yes, that's correct. Uh, yeah, I think. Well, I mean, um, the first the alpha part Yeah, so just this the exponential growth, the exact constant, is just rho. At least maybe I think we're working on for rho equals zero, if maybe there's intermittency or not. But for this part, then this part we showed that I think there should be This, yeah, this part here that comes from F alpha, it should have this, this, actually, maybe I should put back on the yeah, this part is the part that's related to F alpha, and I think that that should be sort of like constant lower bound, yeah. Basically, it implies that the integration is zero, so that the constant in front of t is zero. That's why we at this moment we need a rho to be positive in order to have this exponential lower bound. So, a delicate question is actually when rho equals zero, do we still have an exponential lower bound? Yeah, uh first order the the coefficient is sensitive to geometry. The order does not, so it's always exponential. It's just but but what is the coefficient in front of that t? That actually depends on the geometry. So maybe it's more like a general question, but what prevented you to go to general compact manifold? Yeah, because I mean as long as you have a heat kernel estimate, spectral theory for the heat kernel you'll get these lower bound, you know, for the second moment. Lower bound, you know, for the second moment of the solution, like we did, for instance, in the in the fractal case. But I mean, this is uh, yeah, okay, but I think this can be done, you know, if you have the regularization property of the of the semi-group, you can start from a measure of function. That's not that's not a big difference, I think. Yeah. But the Fourier is, I mean, the Fourier decomposition on the terrace is the eigenfunction expansion for the Laplacian. It's exactly the same. So, yeah. Okay. Okay, we can discuss more. So fast is the idea. Okay, so uh thirty thirty four less than okay.