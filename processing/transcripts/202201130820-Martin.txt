So, actually, today I'm not going to talk a whole bunch about the modeling work going on in my lab, but rather I'm going to start briefly talking about why I think that human language is a really interesting sort of extreme value or boundary condition for both the brain, for brain computation, but also serves as a nice test case for any kind of artificial neural network modeling you might want to do that then is going to relate to brain computation. Is going to relate to brain computation. I'll try to make the case for that, and then I'm going to switch to showing some instances of what we are calling more and more neural readout from the group. So these are like observations we have from EEG and NEG during natural like comprehension of spoken language. So people listening to stories as the only task to use that really as sort of a starting point to build our theories and models of how the sort of, I think, the most. Of how the sort of, I think, the most challenging part of brain computation can work. So, let me first make that case to you. All right. So, come on, computer, I can hear you computing. Okay, so the human brain's capacity for language requires a neural system to have two seemingly opposing properties. And I think the puzzle of how they go together for me is endlessly fascinating. So, we have statistical properties of language that we know are super important because there's no debate that's. Super important because there's no debate that statistics are really important for language. So, these are things like associations or statistical regularities that you can extract from any level of linguistic description that you might want to apply to your language. And of course, predictions that you or the brain can make based on what was above. But in contrast, we also can do things like understand utterances we haven't heard before and produce utterances we haven't heard before. And this point is to actually. We have not heard before, and this points to actually a deep fundamental interesting property of language that some aspect of what we represent has to go beyond our experience or to go beyond just association or associated information. So to get back to the chase, I want to know how the brain can represent both types of information. And that's why language is actually an excellent test case for this problem. So the contrasting sort of other kind of information that has to be represented is something like variables or structures or rules. Or structures or rules, or structure dependence, as we say in linguistics, that would allow things like production, so being able to produce novel sentences, and things like generalization, where you can understand things that you haven't heard before based on your knowledge of the rules of your language, and things like developing new words or new uses of words, which we see all the time. Exactly, you know, we've always seen in human history, but now so much with means. You can see that a lot. And also, there, of course, predictions-based. And also, there, of course, predictions based on your knowledge of language, like this. So, like I said before, the starting point for modeling this system or this capacity, the way that we approach it is starting with the neural readouts that we can obtain from the brain and then try to understand how those readouts not reflect these two kinds of information as a way of guiding us to say, okay, how would we then instantiate that in a model? All right. But you'll probably already are aware that most of language Already are aware that most of language research these days focuses on what we've come to know from experience, right? So, that's associations, statistical regularities, and predictions that I just talked about. So, I want to be totally clear. We know these are super important, both for language as a system, but also for the brain. We know that the brain is really guided by its past experience and is very good at predicting things in the environment. In fact, that's like a crucial, perhaps guiding principle of brain computation. But, in any case, in my Computation. But in any case, in my subfield in psycholinguistics, there's half a century or more of research establishing things like priming at all levels of linguistic representation. We know that distributional property statistics are super important for language acquisition and development. But at the same time, one of the coolest things about language acquisition is that kids go beyond the statistics of what they get in the input and do interesting both. Do interesting both grammatical and not quite generalizations based on that experience, right? We also have things like speaker adaptation. So, the way that statistics of the people we're speaking to, these things often converge together in interesting ways. And more and more in language models, we know that things like surprise and entropy and the many different ways that you can extract that information and things like closed probability in experiments really shape both behavior and the neural response. Response. So it's clear we know the statistics of our world for sure. Those have a clear role in our models, but we know more too. And that's what I want to focus on. So putting the statistics of our world into models has been really powerful. It's been a game changer. We get better and better at it, but still we can niggle because language models, you know, every year they get better, but they still don't get everything quite right. And I think that's actually because they're missing some of the core aspects of our knowledge of language. Of our knowledge of language. And that comes down to how they represent language. And the fact that they don't get things quite right, that can sometimes be very entertaining, can be funny. And in the extreme, it can actually be dangerous. But getting back to how they represent information, sort of in the limit, it's always as a sequence of associations. So no matter how high-dimensional or conditionalized, whether it's one word related to another or the probability of one word given another or distributions in a large corpus. Another or distributions in a large corpus. But in all these cases, it's not yet as a structure or grammar, although there are some interesting works coming out using RNNs to create grammars. But in any case, what a statistics-only approach or an association-heavy approach misses out on is actually what's miraculous about language. And that's that we can understand things that are not highly probable or predicted. And we can understand things that we've never heard before. So, for example, like the phrase. So, for example, like the phrase, a marmoset getting a toothbrush head rub. So, you can't be sure, but it's unlikely you've heard that particular phrase, marmoset getting a toothbrush head rub before. I can be sure that it was not a predictable utterance for you to hear. So I would like to argue that even without this picture, if you can hear that sentence or someone can say that to you in speech or sign, you can understand what it means, even though you didn't predict it and probably never heard it before. didn't predict it, you've probably never heard it before. But the point is, models based on associations alone can't and don't explain how we do this, nor do they explain how we say things that are novel, unexpected, or even just variants of what we've heard before. So it's obvious, like I've said already, that statistical association and prediction are super important, but we also need this other component in our theories and models. Yet, we don't really have that so much. So, in the history of cognitive science and psychology, there are many. Of cognitive science and psychology, there are really important attempts at this, but bringing that now into our sort of modern approaches in neuroscience and AI, I think is more and more important, right? Because it's really our knowledge of language or the structures that we build as we hear the utterance that explains how we know what a toothbrush head rub is, even though we've never heard that particular string before. And this ability or this fact about language as a system is actually due to the, we can describe it more. The you can describe it more succinctly than I've given in this intro. It's actually because language is mathematically very interesting, so it's an example of a system that's both algebraic and statistical. And we don't pay enough attention to that wonderful fact, actually. All right, so that leaves us with developing theoretical and computational models that try to be faithful to these properties of language. So both the facts that statistical association and prediction are very important, but also that there are formal properties. Are very important, but also that there are formal properties of language writ large. So, that is, in this case, focusing on structure and not only on statistics, that are important that we need to not overlook those. And also trying to stay faithful to what we know about the brain. So, for the instances of this, the purposes of this talk, I'm just going to start from the principle that brain computations are extended in time and are distributed across neural networks or ensembles in the brain, and the dynamics of those ensembles reflect their computation. Ensembles reflect their computation. That's what I'm assuming. And to sort of foreshadow, it's that, those dynamics, which in turn express linguistic structure in my accounts. So the rest of the talk is going to focus on how neural readout from EEG and MEG might establish the sensitivities or the capacities that a model of language must have or must account for to sort of spam this whole space of information. To try to see, okay, to demonstrate that both structure. That both structure and statistics shape readout, to what degree do they do that? That's what we're now going to try to investigate. So, now before I move on to that, I'll just emphasize: so, the things that I think or that I'm trying to add to models, and I say, and I hope other people would be interested in doing this too, is components for time, timing and time scales of activation, focus more, especially for language models, on what is actually being computed and how it's computed, not just on performance, and then more in the cognitive modeling. And then, more in the cognitive modeling perspective, using things like oscillations and network and whole brain dynamics in our computation, having those as components of models, because I think those are elements of brain computation that we can't, that we shouldn't leave out and that could be very powerful for language modeling. And also more now for looking at interpretation of utterances, not just predicting the next word. That's a whole other can of worms. Okay, but back to step one: identifying the relevant neural readouts. Identifying the relevant neural readouts. So, can we observe the modulation of brain rhythms related to linguistic structure and meaning over and above the response to sentence prosody and lexical content? And this you can see as a proxy of can we observe that things like abstract linguistic structure modulate neural readout more than, for example, something that you could extract from distributional information in the stimulus. And this is the PhD work of great. Is the PhD work of Greta Kaufeld, actually, Dr. Greta Kaufeld now? She's a recent graduate of our lab. And here she asked whether the mutual information between the brain response and the stimulus increases the function of linguistic content. So mutual information here is between the speech input or the continuous natural speech stimulus and the brain response. And it's used as a measure of the brain's tracking of a signal. And so the signal processing pipeline for stimulus and response in this case is the same in order to be able to relate essentially. To be able to relate essentially just two time series at a small lag, and this is a standard way of assessing the relationship between a stimulus and the brain's response in speech processing. So Gray used three conditions to try to tease apart the perceptual aspects of naturally produced speech. So prosodic jabberwalkie is one condition here in the middle. It has a sentence like prosody or rhythm. So the modulation spectra of the stimulus in that condition is indistinguishable from that. Is indistinguishable from that of the sentence condition, which is sort of our goal condition, where everything comes together into sort of a natural, coherent sentence. You can see the jabberwalky condition is sort of a prosodic control. Then the word list condition has all the lexical content, but a different prosody or rhythm. And phrases and sentences weren't easily reconstructed from the sentence, from the physical stimulus of the word list. So we know that the brain closely tracks the physical properties of the input, which has The input, which have a statistical distribution that is predictive of the next input, right? So that's helpful for the brain. But what we don't know, or that there's not so much evidence for yet, is whether the tracking response for structures, which obviously you can describe the distribution of structures as well with statistics, but the point is, can the brain also track something that's not got that same next moment predictable distributional information driving it? And of course, the path. And of course, the power of being able to track structures, even if you're not predicting them using distributional statistics, is that then they can occur in lower unpredictable cases and the brain can still construct them or extract them. All right. So Grayte found that indeed brain rhythms track linguistic structure, even when you control for the effect on the brain from prosodic, acoustic, and lexical distributional properties of speech. So Gray has shown that when you cut up the brain response into phase, word, phrase, word, and syllable length, Word, phrase, word, and syllable lengths. That's what you see in these columns here. That on the time scale of words and phrases, the brain is more attuned to linguistic structure than to prosodically and lexically matched control. So you see this by looking at the fact that the sentence condition, the green dot in distribution, is always higher than the orange, the prosodic control, and the purple, the word control. So this shows that it's not the acoustic, prosodic, or lexical properties alone, which we can describe very well. Alone, which we can describe very well with statistics, that are driving the brain's response. There's the structure and meaning of what's being said in the sentence, although it doesn't have a direct physical correlate, is actually what's driving the brain's tracking response. So this pattern is true even when we use a hyper-reduced annotation of the structure of the sentence, suggesting that we don't even need to model the fine detail of the acoustic response to set the role of structure in shaping the brain response. Response. So there's more mutual information between the annotations here, which you see is sort of a pulse train or a step function, and the brain data when structure is present compared to when it's not. So even though we did lots of work to carefully control our stimuli and make sure that they were prosodically and acoustically as indistinguishable as possible, now we have evidence that maybe that even then, even if we didn't do that, probably we could have detected the signal. But that's all right. I still think it's important to have done the well-controlled experiment. Okay, so to sum up, the most structured and compositional thing we presented to participants organized the brain response the most beyond the acoustic and prosonic or time frequency matched controls. So that means that the tracking that we're seeing in these readouts is not just about the time scale when things are occurring, which is a big debate in the neural oscillations literature. Is it just sort of tracking an event that happens at a regular period, but actually about the content. Actually, about the content. So, without the presence of the content, you don't get the same recruitment of the tracking response. So, that means that linguistic structures, not only distributional information, is tracked by the brain. But I just told you that phrases are tracked more strongly when they're meaningful and bare linguistic content. But we need to know a little bit more than that. So, what aspects of linguistic content are actually affecting the tracking of phrases in natural speech? So, sort of In natural speech. So, to sort of pick up where Greta left off, a current PhD student in the group Kuskovmans set out to try to answer this question. So, as I've just presented to you with Greta's PhD experiments, it also connects to a large body of work in the speech processing neuroscience literature that shows that temporal regularities in speech are tracked by the brain. And now we are beginning to accumulate more evidence, including Raja's work as well, that Racha's work as well: that this is true, even when those regularities are abstract and have to be inferred. But the strong signal that we can get also really reflects the fact that content, the linguistic structures themselves are important here. But still, like I emphasized before, we don't really know exactly what. So I don't know that we'll ever be able to answer that question definitively, but we can get one step closer, and that's what Cuss is trying to do. So to this end, he created a complex experimental design with a parametric modulation of linguistic context. Parametric modulation of linguistic content. So comparing sentences to idioms, syntactic prose to Jabberwoki, all these different instructions in Dutch. I won't go through them in detail for the sake of time, but I will denote how parametrically varied linguistic content across conditions. So all of the conditions except for Jabberwocky had real words or lexical semantics that are known to the participants listening to the speech. All of the conditions except for the word list had the same kind of syntactic structure. The same kind of syntactic structure or phrase structure of some sort. The only condition that has meaningful propositional interpretation is the sentence condition. Again, that's sort of our reference condition. So what we mean by this is that the combination of word meanings and their phrase structure is what yields the meaningful interpretation of the sentence, which is what we call the product of composition in linguistics. But it's possible to derive. But it's possible to derive semi-compositional or arguably compositional interpretations of the idioms, syntactic prose, and jabberwalkie conditions. But all these interpretations that you can get here are not really the intended meaning, or in the case of idioms, they're semantically odd. So I guess to maybe give the best loss in English, it's sort of like, well, you can combine the meaning of kick the bucket, but you don't, even if you just put the meanings of the word together, you don't get the meaning of the idiom from that. Medium from that. So that's the difference. So, in contrast with Greytest work that I just showed you, the recordings of the sentences and wordless here were acoustically quite different. So, we weren't able to get that perfect kind of envelope matching control that we could there. But that's because we wanted to have the variation in the relationship between how linguistic representations are combined together compositionally and the output of meaning. But we did other controls to try. But we did other controls to try to compensate for that. Right, so for each recording, we manually annotated the onsets and offsets of the syntactic phrases and converted these annotations into an average frequency at which the phrases were presented. So it's like a phrase rate. And we also annotated for each stimulus how many syntactic phrases were opened and closed at each word. And we use this annotation of bracket count as an abstract representation of syntax, which doesn't contain, oh, sorry, doesn't contain acoustic information. Oh, sorry, it doesn't contain acoustic information, so a bit like the pulse chain that's great to use. Okay, so Cus can show here now that idioms and syntactic prose, the two conditions which have less straightforward one-to-one relationship between structure and meaning, for these we find stronger mutual information between sentences in both and syntactic prose and jabberwalky. Right. Then when we compute mutual information between the E's. Compute mutual information between the EEG and the abstract annotations of the bracket count that pulse stream. We find stronger mutual information for sentences than for Java Walking word lists, but no difference between sentences in idioms or between sentences in syntactic prose. So at first, these findings seem to suggest, I'll try to summarize them for you, that the meaning of the stimuli doesn't really affect the cortical tracking of phrase structure effect, that the tracking effects that I've been talking about. But we wanted to check that participants were sensitive to Participants were sensitive to the oddness of the meanings of the non-sentence conditions, and that they actually noticed that the conditions were different in terms of compositional content. So, we computed event-related potentials, which is an older way of summarizing brain responses, basically, elicited by the word at the end of the sentence in all of the conditions. And we find that there's an N for 100 response for sentence final words and sentences that's similar to the N for. Words and sentences that's similar to the N400 for both prose and jabberwalky, but larger for idioms. And these ERP findings show that participants did notice the semantic differences between conditions. In other words, despite clear differences between the conditions, the extent to which compositional interpretation ends up with a sensible semantic interpretation, we don't see that in the cortical tracking response, but we can see it in other aspects of the neural readout. So that indicates that the tracking is really something we think. Is really something we think that has more to do with assembling and generating the structures or sort of the perceptual aspect of parsing language, and not necessarily with whether that interpretation fully aligns with either your expectations or the rules of your language. Okay. So the answer to this question is still a bit complex, but as to which aspects of linguistic content affect oracle tracking of phrase structure, it seems to be the properties of the It seems to be the properties of the input that affect that are not the compositional interpretation of the output, at least. So, the fact that it's the input sort of to the structure building computation that seems to be driving the tracking response, that's something like word or your argument structure, really the building of the structures, but not necessarily the output of that. So, whether that structure ends up having a meaning that totally aligns with either what you expect or Aligns with either what you expect or with the semantic and thematic constraints of the language. And this is consistent with models of language processing in which structure building is a partially lexicalized process. So if you have like a head-driven lexical grammar, that's something that we could account for, arguably with this finding. All right. But the plot thickens. I'm going to try to get through two other interesting readouts in one model before I run out of time. We'll see if I can do it. But the plot thickens when it comes to what neural readouts can reflect. So, we've just seen from the mutual information work of Grey and Cuss that structure is reflected in the neural readout, right? That's what I've tried to make the argument for. And that's a crucial component of brain computation, and we should include it in our models. But the readouts are more complicated than that. So, this is the work of Von Bai, another current PhD student in the group. So, his work was really inspired by this seminal work in MEG by Ny Ding and colleagues, which adopted this frequency tagging paradigm. Adopted this frequency tagging paradigm, which I won't go too much into detail, but you'll feel free to ask me questions if necessary. Um, where you synthesize speech that was presented at a sort of fixed isochronous rate. And you can see that in the panel B there, that's the in the in the spectral, in the power spectrum there. You see this peak at four hertz, and that's the syllable rate of the stimulus. And you can compare that to the neural response below. So below, you've got MEG, you see also a peak at four hertz, but you also see peaks at two hertz. But you also see peaks at 2 hertz and 1 hertz. And in this study, that is arguably due to the fact that in the syllable stream, you also have phrases and sentences that are being constructed. You see that in panel A, right? So there's nothing in the physical stimulus from B that's driving the response that you see in C at one hertz and two hertz. And this has been used to argue the brain is tracking linguistic structure. And while we don't dispute that conclusion, we also wanted to understand more about this tracking. We also wanted to understand more about this tracking response. So, Fan did she set out to replicate this cortical tracking effect. And here we replicate it in a first language. So, these are Dutch participants listening to Dutch stimuli. And you can see that we had three conditions. So, you have two syllable pairs that form a word. So it's something like Taiher in Dutch, which is tiger, or tare, which is wheat. And you compare that to two other conditions where we have random syllable sequences, or those random sequences played backwards. Sequences play backwards, so two different kinds of controls. What you see here is that we get the response at four hertz for all the conditions, showing that there's a regular rhythmic periodic stimulus. And for the case where you can actually perceive a word in that otherwise physically identical thing, we get again that two hertz peak, showing that when you know a language, in this case, it's your mother tongue, you're able to pull out the words from that stream, no problem. So that's the in principle replication. That's in principle replication of that famous finding. But we also could observe this tracking for two-syllable nouns in Chinese, a language unknown to the participants. And Tibetan, we have another set of experiments where we have our Chinese participants who don't know Dutch listen to Dutch. So we've balanced it that way, but I'm not presenting that data right now. So this is possible because participants could learn the statistical distribution or transitional probability between syllables. Between syllables, which is systematic in any language compared to a random sequence. So it doesn't take that much exposure for the brain to be able to learn that, right? But so you can see that same thing. So these are Dutch participants who don't know Chinese. They're not going to learn it from listening to these syllable streams, but their brains can still extract the transitional probability is different from a random sequence or a random sequence played backwards. That's fine. That's just a fact about our perceptual systems that's important and interesting, right? It's really important for the brainstorm. Right, it's really important for the brain to say, okay, that's not random. So that's what you see in that peak there for the type 1 condition. Right. So then we wanted to sort of go deeper into how much can we play with these transitional probabilities. This is also very much inspired by the work of Jenny Safran, also with infants in the 90s, and Fan was really into that. So, anyway, okay, so we trained Dutch participants on novel compound phrases. So these are things like, in the first example, Tai Her Darb, which is an Example, tiger tarbe, which is something like tiger wheat. But in Dutch, you would have to modify to say something like tigery wheat. You would have an inflection. Also for like the two-syllable noun sequence, if you had something, the first example there is Levai Ruvir, which is noise river. So in order to say something like a noisy river, you'd have to say Levaych Rivir. So we don't have that. It's just basically words that people learn to group together into pairs. So they're not really a phrase because they're not grammatically in fact. Because they're not grammatically inflected. But the point is, in these sequences, which Fun then carefully controlled, so he used a markup chain to hold the statistical relationship between different levels of structure, so between syllables and between words and between pseudo-phrases, to show that even without grammatical inflection, people in their own language can learn to track these different levels of transitional probability from the syllable level to the word level to the syllable. Level to the word level to the pseudo-face level, fine. So that's not controversial. But interestingly, they can also do it in a language they don't understand. So it turns out that there's the sort of brain scope or ability to learn transitional probability from one syllable to the next can be expanded to actually incorporate what would be a word lengthening or a pseudo-phrase lengthening, right? So basically we can get this one, two, four hertz pattern, even though there's no comprehension of what's being said. Of what's being said. And that's because we actually think this frequency tagging paradigm really reflects the kind of perceptual grouping effect that the brain can do. And again, this is with arguably some training or exposure. So they have to be exposed after a couple of minutes for this to happen. And another thing that we're looking at next, which we haven't finished, is how that response changes over the course of the experiment. So obviously in the beginning, it's probably not going to be a very strong response, but towards the end, it should be stronger. Okay, so what does that mean? So, what does that mean? That means that this cortical tracking or frequency tagging response can be driven by statistical regularities in the absence of linguistic structure. But we've also seen that linguistic structure can elicit tracking. For me, that's not, maybe that's a paradox. I actually don't think it is. I think it just means that we need to take a broader view of how we bring this information together in our models and theories and how we understand it in our neural readout and our data. What does not follow is that linguistic structure is purely statistical. That's the conclusion that I. Purely statistical. That's a conclusion that doesn't follow from this, and I think we need to be very careful about. But what it does tell us is that theory and model building is a capacity like language also requires a theory of the readout. So we have to know what is it that's causing this frequency tagging paradigm to look the way it does. If we only looked at linguistic structure, we'd think it was only linguistic structure. We only looked at perceptual grouping or transitional probability, we'd think, oh, it's only transitional probability. It's only because we've now looked at both that we can see. Because we've now looked at both, that we can say, okay, actually, you can get this kind of neural response for different aspects of what the brain might be tracking, and that'll actually change the kind of inference you'll do in your theories based on the data you observe, right? So that's how we have to have a good theory of what our data actually are reflecting. So you have to do sort of a lot of modeling and dipping in and out of it and coming back to try again. Okay. So I'm going to try to wrap it up in the next seven minutes, is my goal. Okay. Okay. So I do want to say one thing. So the next thing that we, after we did this project, then I sort of said to Flan, like, but I really want to know where is it in the brain is something that is a signature that's really about structure building. Like maybe, is there, can we ever say that there's one readout that's like one-to-one related to this, you know, if there is this dichotomy between association and statistics and structure? And, you know, you can go deep into that philosophically and think about, you know, what that, you know, is it sort of like a data type? What that you know is it sort of like a data type or what kind of format you would use to represent this information. You can you can dive deep, but in the meantime, I wanted to know what is it in the readout that might reflect this. So the way that we decided to address that question is to say, okay, what happens when we force a phrase in a sentence, which is different in structural ways that are interesting in syntax and linguistics, if we force it to be physically identical, that is to take up the same amount of time and be To take up the same amount of time and be as physically similar as possible in the speech stimulus, can we then extract some part of the neural readout from the brain that can then tell us, well, was it a phrase or a sentence or that modulates with the difference between the two or the number of constituents? All right. So in order to do this, Fan made synthesized speech stimuli that are about a second long at four syllables each. So in Dutch, this is comparing a sentence like the ball is you. Comparing a sentence like the ball is you with the blue ball, and because of the inflectional morphology of Dutch, you have to say the blauer ball because of the grammatical gender of the word ball in Dutch. And that makes four syllables, which means it can be matched with something like the ball is blau, the sentence version. So we've got a sentence and a phrase that's essentially meaning the same thing. Okay, now this is to try to show you that the speech. That the speech stimuli were as physically identical as possible. So they have a cosine similarity that's always greater than nine. We've done multiple different comparisons to try to show they're maximally physically similar. The same number of symbols, the same duration, the same intensity, the same root mean squared power. There are many different ways to show this. But the key thing is that native speakers still classify them as being good exemplars of a phrase or a sentence. So we've made them as similar as we can. Now we're going to look at the brain response to this. Now we're going to look at the brain response to this. So the sort of key finding when looking at many different readouts is that it was phase synchronization in each trial that's looking at the phase synchronization between trials. That was clearly different between phrases and sentences. So there was more phase synchronization for sentences than for phrases. And you can see that by the sort of strength of orange color in the bottom plot, time frequency plot there. Plot, time frequency plot there. And although I haven't had time to tell you about the modeling work going on in the group, I'll try to briefly explain why this is interesting. So, phase synchronization in neuroscience can relate to just how some neural network models that I've worked with organize information during computation. So, in a connectionist model that can implement symbols called DORA, there's this notion of phase sets or distributed codes that fire in a temporal pattern with one another. Fire and a temporal pattern with one another. For example, to create a word in relation to another word, both in relation to a phrasal representation. The other way you can think about this in more neuroscientific terms is that you have these sets of activation functions, and the way that they relate to each other is how information is encoded. And so that particular model makes the prediction that as you add more representation, so in this case, because the sentence representation has an additional constituent, Has an additional constituent. So the ball is blue versus the blue ball. Can't, you know, if I had more time, I would show you all these linguistic imitations of it, but in any case, the fact that you're adding more linguistic structure in that case makes the prediction in models that use phase sets to discriminate and add things together that you should see some difference in how phase information is being modulated in the system. And that's what we found here. So the idea is that. The idea is that the phase, the sentence condition has more fate of these phase relationships. The organization of the neural assemblies, as much as it could be reflected in this measurement of phase synchronization coming from EG, is increased for sentences, even when they take up the same amount of time and have the same physical shape as the phrase stimulus. So that's something that's purely being added by the brain. It's not something that's in the physical stimulus. Okay. Juan was also working on some encoding models. On some encoding models, these temporal response functions, basically just showing that relevant frequency bands for this are theta and delta. I will actually skip going into the details of this because it's not so relevant. The other sort of last interesting finding when you ask, okay, so where is the signature of linguistic structure in the brain if it's not in frequency tagging? One of the other, another sort of prominent model in speech comprehension and neuroscience is the Giraud-People. Neuroscience is the Giraud-People cross-frequency coupling model that makes the claim that phase amplitude coupling, so a relationship between the phase and power across frequency bands, particularly theta and gamma, is crucial for segmenting syllables and decoding phonemes from speech. So here we actually tried to test that because there's not so many instances in the literature that's being tested. And interestingly, we didn't find any difference. So you see the plot for the Gaul Fame between theta-gamma coupling for phrases and sentences. So that's not. For phrases and sentences. So that's not a metric or readout that actually discriminates between these two cases. We do find increased data gamma coupling when participants were listening to speech compared to resting state. So it's probably really important for speech parsing and sort of aligning with speech in order to segment it and begin to get into language processing, but it's not making a distinction between these levels of representation. So sort of the short answer to that is I think where we need to look. I think where we need to look more for linguistic representations is really in the connectivity and in these sort of phase relational phase measurements of different kinds, like phase synchronization, but also whole brain dynamics. And that comes from this idea, and I'm going to begin to zoom out a bit, that we've got this internal language model that is firing ensembles that represent different aspects of information in time, and that the synchronization of that phase together of those different populations. Phase together of those different populations is really contributing to the activity that we measure coming off the brain. So, in various modeling papers, trying to talk about how this would work conceptually, talk about it very briefly at the end. Okay, so the last thing I want to tell you about today is one brief modeling project led by my postdoc, Dr. Sanaten Uber, who is wonderful and is about to launch herself off into her into an assistant professorship. So, you can tell I'm already very Very proud, but also very sad. But also, very proud. But Sana is great. So she's a postdoc in my group, and she's really interested in modeling how spoken corpus of Dutch conversations with lots of natural speech. So these are actually all customer service calls in the corpus. So she wants to explain how the timing in speech and computation relate. So the ultimate goal is to relate the model that she's developing with neural data from natural speech. So so far, she's begun by incorporating linguistic structural construction. By incorporating linguistic structural constraints as a feedback source in an RNN. So these constraints were extracted from the Spolecan corpus. At the same time, she incorporates neuroscientific principles. So this is what I was talking about before by adding things like inhibition and an internal rhythm or endogenous oscillation to computation that tries to mimic how the brain computes in time in a very coarse way. So she wanted to see if adding these aspects, inhibition and oscillation, can improve performance. Performance. So she took speech signal, added some neural oscillations together. But as we've seen, speech isn't isochronous, so it's pseudo-rhythmic. It doesn't have a fixed period. And she wanted to then know, is that pseudorrhythmicity is a consequence of actually the brain taking these internal representations and deploying them while you're listening to speech? So that means that the shorter word is next, but it's now my. But it's now my thumb backwards. Okay, here we go. So we've got the shorter next word is predictable. The idea is that this can help with accounting for pseudo-rhythmicity in speech. So we know that amplitude modulations of syllables fall within the theta range, so four to eight hertz. You can find these theta rhythms in the brain, but speech isn't fixed, right? It's not as an isochronous rhythm that's always at four hertz. So you can actually extract the onset time of different words based on these temporal features. Different words based on these temporal features. So it's an open question how we can track this more natural pseudorhythmic aspect of speech. And what one possibility is that it could involve aligning these highly excitable neuronal phases or activation sort of potentials, functions, that could be a consequence of the content of speech. So basically, the more predictable a word is, the lower the threshold for activation it might be. So this is what we tried to investigate in our recent paper. In our recent paper, whether predictable words in the sentence context had a shorter word-to-word onset difference. So, to find that out, we first correlated word-to-word onset differences with the predictability of a word in a corpus. And then we estimated this using a deep net, and we found a negative relation. So, that indicates the more predictable a word is, the earlier its onset was. And another way to say that is that words are shorter when the next word is predictable, but longer when the next word is less predictable. Next word is less predictable. And so that she wanted to add these content predictions to her oscillatory tracking model. So this is a very brief summary of that. So we used a model with the following components. We had stimulus input, the main processing level with words consisting of oscillations, inhibition and feedback. So then this is just how we would process a very basic sentence like I eat cake. And then when you add an adjective like nice, that becomes less predictable. You can make it even less predictable. Predictable, you can make it even less predictable by putting an adverb in. Um, and uh, right, this the output that you then see in these next uh panels is that the sensory input, i eat, at the feedback level after the word eat, after three notes, gets feedback, but at different strengths. So basically, you get feedback about whether something is predictable or not. And then we can look at the final activation of the model and see that the oscillation gets to a more excitable phase that is more red. That is more red when the word is more predictable. So the word node cake is active, followed by another node that creates a phase code of predictability. So this is a way that the model can sort of pass information about forward, right? So this model can explain, we show in the paper that how it can account for behavioral findings and also some standard things in speech perception. All right, let me just start and set that right. So actually, oh, yeah. Said that right. So, actually, oh, yeah, the last thing I want to emphasize is we actually get a stronger rhythmic response from the RNN when the timing, input timing of the stimulus, matches the internal model's predictions about words coming up, how short they're going to be. But interestingly, when you combine the internal model making the onset predictions with an endogenous oscillation, so when we add this sort of sensitivity to when the activation function is going to come, which is basically what the oscillation is. Which is basically what the oscillation is. Then, the non-isochronous sensory input, that is the pseudo-rhythmicity of speech, that combined into the model can actually give rise to a stronger rhythmic response from the network, right, which is our model of the brain here, because of the contribution of the internal model, right? So that's actually how having a pseudo-rhythmic input actually results, so that's the speech, results in the brain having a more rhythmic or isochronous brain response, which has always been a puzzle that we hadn't actually known how to work out in speech processing. Known how to work out in speech processing. So that was an interesting thing. Okay, so now I'm going to take a few minutes to wrap up. So I'll try to point at the theoretical model that I've been developing for the last five or six years now. And in this model, I try to talk about the knowledge of language as being your internal language model that you then combine with a speech signal. And lately, I've been focusing on how gain and inhibition might be able to package this information in time. Information and time with the goal of trying to explain how hierarchical structure and compositionality might arise in a neural network without the sort of the minimal assumptions that we can make about that. So we want to use known computational mechanisms in neuroscience in the very sort of coarse green race inhibition, while taking into account time and incrementality and trying not to deny any of the facts about language and what I explained. So the latest version of this model has gotten a little bit more. Of this model has gotten a little bit more trippy, so now it has manifolds. But anyway, but the core itself, I'll just do the core claims of it. So, delays incarnation is out in GSCN, actually now two years ago. But the core claims are that linguistic structure itself arises in the temporal dynamics of neural networks. So, that's again what I was talking about in the readout. That's where the representational structure is. And that process is a form of perceptual inference. So, I don't want to invoke anything special in order to get that, right? Special in order to get that, right? So, again, the fact that you can get frequency tag infrastructure and for transitional probabilities on this account, that's fine, that doesn't violate any principles because you're just using the infrastructure of brain computation. But then the rub is then how do you then get the compositionality and structure out of that? Okay. So the sensory percepts that you get from speech or sign, Q, and internal model of language, which are all distributed codes. And the temporal distribution of these activation patterns is what results in the Patterns is what results in the functionally linguistic structures, right? That's where the magic happens, and this is very similar to work I've done this logic with Alex Dumas and the DORA model, which I'm happy to point you to if you're interested. So again, it's gain modulation and crucial inhibition that are the assembly level computations that produce representations of linguistic structure from the sensory input. In the paper, I have things like pseudocode and a glossary and a range of predictions. So please, I know, feel free to tell me how I'm wrong there. Free to tell me how I'm wrong there. Always interested in feedback. But anyway, so now I described linguistic representations in the space as trajectories in a manifold. We can fight about what that means. I can question myself about that. But that's what renders linguistic structure building then actually just as another expression of coordinate transform in the brain, which is a very, I think, broad notion that you can have in many different areas of neuroscience. That idea that you've got some coordinate system where information is represented, you've got to transform it across. Represented, you've got to transform it across a gradient. And that operation now is multiplexed in time. All right, now you're in the wrong window. Okay. All right, so to sum up, the things that I find that help me in this process of trying to create out their theoretical model, but also do experimental work and computational work is that taking independent or orthogonal evidence for a given computation has to go beyond the read-neur model. Has to go beyond the region or model alone. So you don't, again, you don't want to be narrow. You don't want to only look at transitional probability, you only look at structure, then you get this sort of very like biased view. And one way to do that is try to look beyond just doing a particular task when you do your experiments and try to come up with something that, you know, where you're measuring the capacity of a system. I think what that more and more points to is that we have to brain notions from linguistics and psychology. So it's not, you know, not to slag off. You know, not to slag off box models too much, but now we have to think: okay, what could possibly be the kinds of formats where something like a syntactic tree or linguistic representation could actually work in the brain? Because in order to not deny that, we have to think in a more, you know, I think radical way. And it's, I mean, it's easy, but I think it's possible not to deny what we know about linguistics and also not to deny what we know about the brain and also how we do things in neural networks. I think those things can all come together, even though sometimes it can be very challenging. It can be very challenging. Right. So, again, I think this calls for reimagining these concepts, not denying them. I won't talk about modularity. Right. So, yeah, the last things that we always focus on too in my group and we tell each other is that, okay, just because we've predicted something doesn't mean we've explained it. And that our model performance is not necessarily evidence that what we think is being learned is actually being learned. So you have to be very, you know, strict with yourself about that. And that. That and that just because we found it made it work this way in this model or this analysis, that it definitely means that's how it's going on in the brain, mostly because of multiple realizabilities. Okay, so that's everything that I wanted to tell you about. So, again, thank you very much for your attention. I'd like to thank my group for being my wonderful group. Thank you.