Uh, I'm the next speaker in this session and uh really happy to be here and uh be part of uh the organ organizers for this workshop. And uh, today I'll present our recent paper on the recipe for a general powerful and scalable graph transformers. It's a joint work with Michael Galkin, that is also here in audience, Vijay Divadi and Antoine from NTU Singapore, my postdoc supervisor, Guy Wolf. Postdoc Supervisor Guy Wolf and Dominique Berini from Valence Discovery and also University of Montreal. So I'll briefly spend just a very short time talking about the message passing compared to graph transformers because we've already heard a lot of great talks about that part. So I'll try to be quick. Some kind of pros and cons of using the graph transformer. Of using the graph transformers. And then I'll jump into this Graph GPS framework that combines node features and edge features that we divide into local, global, and relative features. And how we put that all together from kind of existing parts where we combine these pre-computed features with local message pass. Features with local message passing and global attention using the transformer-like attention. And if I'll have time, I'll try to conclude and show you some new benchmark data sets to test in practice whether these models really can capture long-range dependencies in the empirical data. All right, so we've heard many times that the basic message passing GNNs are limited. Basic message passing GNNs are limited by the Weisweller-Lehmann test as their expressivity sealing, unless you extend the node features. Also, these models have the problem with the over-smoothing and over-squashing, and all that results in their poor ability to capture the long-range dependencies in the data, as they can't effectively propagate that information. And there are several ways how to try to circumvent and break through this glass ceiling of first-order message passing, extend to higher-order GNNs, or to better understand the topology and bring that perspective in. There has been several great talks on that. Then, graph rewiring. And finally, what we've done here is using global attention using transfer-like attention. Like attention. All right, so briefly the pros and cons of running a transformer on graphs. First of all, if you try to tokenize the graph, we have several options. The one most common so far has been to consider all the nodes as tokens. There's been also works that also consider the edges as individual tokens. So then you don't have n tokens that you feed into the transformer, but n plus e. N plus E, and which has some computational limits. But even when you, let's say, when you consider all the nodes as individual tokens, the standard transformer attention looks like this. You do the linear projection to get your query, key, and value matrix. And then based on the product of the query and key matrix, you You get the so-called attention matrix, which is a softmax of that product of the two matrices. And with that, you pull the information that is also just a linear transformation of the original node features. So that's the value matrix V. The problem is that if we just do this naively, then obviously we, oh, first the benefits. First, the benefits. So, the benefit is that we are decoupling the computational graph from the input graph structure, and we are no longer limited by propagating the information only along the existing edges in the graph. We can propagate the information arbitrarily or learn how to do that. And with that, we are shortcutting the diameter of the graph and improved the ability to. The ability to propagate the information across long paths in the graphs. So, and now to the problems, as I mentioned, the nodes are not identifiable if we don't provide any kind of positional encodings. It's just a set, right? So, the way how the transformers handle it in their original applications on natural language processing. On natural language processing, and then later for vision and so on, they add positional encodings that are based on sines and cosine features, which really has the interpretation that it's derived from the Laplacian of a path graph. Not sure whether that was also the original insight of the authors, but there is a very nice connection. So it's kind of then, as we'll see, natural to Then as we'll see, natural to derive graph Laplacian derived positional encodings for graph transformers. Next, so the problem is that even if we provide these embeddings, we are losing the locality inductive bias that message passing GNNs have, which is true for a lot of real-world data, where you really have for most of the data sets pronouns locality that the Pronounce locality that the nodes that are close to each other somehow are more important, right? It's kind of the point of the graphs when you think of it. And with this, we are throwing that away. So we don't even have that inductive bias. We rely on the model to be able to pick up the distance between the nodes based on the embeddings alone. And the final And the final kind of issue is the computational complexity. Since message passing GNNs operate on edges, they are linear in the computational complexity is linear in the number of edges. And in most of the graphs, this is comparable to the number of nodes since. Number of nodes since most of the graphs are sparse. However, for the complete attention in the transformer, we have to compute attention between every pair of nodes, which gives us the quadratic complexity. So there has been booming literature on different kinds of positional encodings, and already in the past, let's say, two years. In the past, let's say two years, various distance-based ones, or so on. And so we decided to try to categorize them as well, have a little bit of a look of what's the common and different aspects of these existing positional encodings and what one can, roughly speaking, use as an encoding. So, one category that we bin these encodings to are this Encodings too are these positional encodings that are intended to provide as if some kind of quasi-coordinates space for the graph. So into this category very naturally falls, let's say, the eigenmap. So using the eigen decomposition of the Laplacian and the corresponding values of the eigenvectors for each node. Eigenvectors for each node. And then the structural ones, they don't provide this global kind of or actual position in the graph, but more encode whether nodes are part of some common substructure, let's say, into this category would follow also the pre-computation of various subgraphs, stars, cycles, and so on. And next, we also categorize these positional and structural encodings into local, global, and relative. So the global positional encoding is, as I mentioned, try to provide the absolute kind of position in the graph, while the local one would perhaps give you a coordinate within a cluster or within some kind of substructure of the graph, and the relatives. And the relatives, relative positional encodings, we can think of as another kind of distance between two nodes. So let's look at a few of the positional encodings. So for the local ones, let's say it would be distance to the cluster center or looking at the landing probabilities outside of the diagonal. Of outside of the diagonal of M-step random walk. So then for the global positional encodings, it would be the eigenvectors of some distant matrices, the adjacency metric or the Laplacian, next to the, let's say, the distances of two graph centroids and so on. And for the relatives, it would be the pair of eyes distances derived from heat kernels, random walls. Derived from heat kernels, random walks, or gradients on the eigenvectors. So, in our experiments, we mostly focus on the eigenvectors of the Laplacian. And let me tell you a little bit more about the issues that come up there. So, we already seen the graph of Placian, so I'm not going to introduce that one. But it really has this nice property that. Has this nice property that it provides with the global position in the graph and at various frequencies, and the eigenvalues are providing this frequency and that way we can encode the global position. However, the issue with the eigenvalues and eigenvectors is that. Use in eigenvectors is that if you want to featurize them into a vector embedding that we can append to every node, we run into the issue that the eigenvectors are sign invariant. That means that they are invariant to flipping of the sign, and that makes it difficult then to engineer as a node feature. Next, there's multiplicity of eigenvalues there are. Multiplicity of eigenvalues. There are commonly a lot of eigenvectors that have the same eigenvalue. And quite often, then the solvers give us these eigenvectors in stochastic order. And next, depending on the size of the graph and the number of components, we can get a variable number of eigenvectors. So we have to somehow deal with the fact that we want to be able to derive this position line coding. Derive these position line codings for also graphs that are maybe too small or and have to somehow pad it or do something reasonable with that. So the very first work that started to use this Laplacian-derived eigenvectors for positional encodings was the work by Divadi in the original graph transformer paper. And there they dealt with design invariance by simply By simply randomly flipping the signs during the training, similarly to like input augmentation done in many deep learning applications in Vision, where you crop the image in different ways or you flip it and so on. So, here that's one way to do it. The multiplicities, I think, were not dealt with, and finally, they Finally, they just set the number of eigenvectors that they would consider to be less than the minimum size of a graph in the data set so that they don't have to deal with the fact that they can get a variable number. Next, in the SAN transformer, which is the spectral attention transformer, they retain the random flipping of the eigenvalue. random flipping of the eigen of the eigenvectors, of the sine of the eigenvectors for dealing with the sine invariance. However, at least they dealt with the multiplicities by also using eigenvalues and then using invariant model, in this case transformer, to embed this set of eigenvectors into a node representation. And then the variable number was nicely handled by the transformer because it doesn't expect a fixed number of tokens, but nicely scales with a different number of the eigenvectors that the particular graph might have, and you can reuse it. In our work, we use deep sets because we just found it to be a lot simpler, a lot less parameter-hungry, and Parameter hungry, and that we haven't observed any performance degradation. I think the transformer there was not particularly important. And most recently, there has been several particular two works that I'm aware of on principally handling the sine invariance of the eigenvectors. One of them is the sine network by. Is the sci network by Lynn et al. that is accepted for the ICML this year? And for that one, they have a nice background and theory, like why the way how they process the eigenvectors is actually sign invariant, and it boils down to running a message passing GNN like SYN, like GIN on the On the positive of the eigenvector, and then summing the representation that you get out of that together, and then using either MLP or deep sets very similarly to what was done by the previous methods. And lastly, I would just very briefly say a few words about the peg layer by Van Gedal, also a recent publication. And this one turned Application. And this one turned the Laplacian positional encodings into edge attributes and essentially uses it as an attention during the message passing. So this is an example of the augmented message passing of GCN, where they gate the message passing based on the MLP transformed square distance of the eigenvectors between the Eigenvectors between the two neighboring nodes. All right. Then, when it comes to structural encodings, so these were the positional encodings. We also differentiate the local, global, and relative. I would also say that some of the more kind of advanced encodings recently don't quite fall into just one of these categories. Fall into just one of these categories because they combine different aspects. And so this is more like a view of the different characteristics. So just a small note that the organization is not exclusive. So here, for the interest of time, I'll skip through enumerating all of them. But what we are most interested are these diagonals of random matrices. Random walk matrices. So you take the random walk matrix and power it to consecutive powers from 1 to some k, typically 16, 20, or 32. And then you take the diagonals from those and construct the node features. It's good practice to run some normalization on that, but this gives you. But this gives you a good idea about the local structure of the neighborhood. Particularly, if you have a graph, let's say without self-loops, it can tell you a lot about whether a node is a part of an odd-length loop cycle or not, because let's say with an odd number of steps, you won't be able to return back to the starting node if it's not part of some loop. And this And this encoding was introduced by Divadi in the recent paper at iClear this year, and they called it Randoke positional encoding. We kind of renamed it to structural encoding since it's more capturing the local structure. All right, so getting to our framework that we call general, powerful, and scalable. And later I'll explain why it is general, powerful, and also scalable. Is general, powerful, and also scalable. It boils down to a very simple recipe. In the first step, you choose the positional or structural features with which you want to augment the original graph. As I mentioned, we consider the random walk structural encodings, the Laplace in the right ones, and also benchmark the novel sign net and peg layer. But it would be But it would be super interesting to try many more and see whether we can also do better than just consider this as some kind of a hyperparameter that one has to test on individual data sets. But this is the first step. So, augmenting the original input graph. And the second step is that Step is that we are using a local message passing network to encode the local neighborhood of each node and also real features of the existing edges in the graph. And then under an assumption of efficient coding that this information is then encoded into the node representation, we don't really have to consider the We don't really have to consider the graph structure for the global attention, and we simply run a transformer or a linear transformer on top of the nodes where we consider them as tokens. But now these tokens have these positional encodings and also the local structure of the nodes embedded in them. So, for the local message passing, as I mentioned, it provides this locality bias. Provides this locality bias that is missing or very difficult to put back into Transformer. By expensive, I mean that mostly you have to then condition the attention matrix that is n squared, and for that you have to most likely materialize it, which results in the n squared complexity. If we don't need to do this, then we don't need to materialize the attention matrix, and we can do some of the tricks that Some of the tricks that scale the transformers to linear time. And for that, we use either gated GCN or gene extended with edge features and updating also these edge features of the graph as we go. And for the transformer, we really use it as the global attention mechanism. So in the next part, I will just refer to it as a global attention mechanism that either can be that of Either can be that of the n-squared transformer or of those linear architectures such as Performer and BigBird that have been proposed recently to scale transformers in NLP and other applications. And so, when we have these two representations, the local one and global one, this kind of two ways how to do the updates. How to do the updates. We perform them and then we combine them. In practice, we saw that just doing basic sum aggregation works very well in practice. So we apply in transformer fashion first dropout skip connections and then a batch norm that has a learnable bias component. So there's essentially a gating that can go on between how much of the local and global kind of a view. Uh, kind of a view is maintained in the updated node features, and then we combine them and run them through a two-layer MLP. All right, so let me just quickly spend a few words on the linear transformers, like where the efficiency comes from, either it's from some sparsification. Some sparsification of the attention matrix A that we saw before, that is the matrix that is the result of the softmax product of the query and key matrices. However, for the graph transformers, they need to really materialize it and condition it on the graph properties. And in our case, if we do rely on the MPNN to do the local embedding. Local embedding, and we don't need to materialize and condition this attention matrix, then we don't need to materialize it. And we can use the approach, let's say, from Performer that scales up the attention by kernelizing this softmax operation. And then you can switch the order in which you multiply those matrices. So instead of doing the key query product first and having the Product first and having this in this case, it's L times L because L is the length in their case for us. That's that's the number of nodes. So that would be the n times n matrix. So that's what you would have to materialize. But if you can kernelize that attention and have it as a product of two matrices, then you can multiply this K prime, which This K prime, which is one of the low-dimensional matrices from the kernelization with the value matrix, you can store this matrix and then multiply it with the modified query matrix. And this way you avoid ever multiplying along the length dimension, which is the number of nodes. So all of these products never involve this quadratic term. And that's where the And that's briefly the idea of the performer. And I'm not going to go into details on the kernelization. That's the key part of the performer paper. So let me get quickly to the results. This kind of relatively simple idea of taking some of the best positional encodings that make message passing probably more powerful. Probably more powerful because some of the isomorphic nodes and graphs are now distinguishable, as we saw before, and also allow us to scale up the full attention in the model, result in really good performance, and we can achieve state-of-the-art on data sets like Zinc, MNIST, pattern, and cluster from the benchmarking GNN data sets in case of zinc. In case of zinc, this is a relatively large improvement. But and moving on to also OGB data sets, there we see also relatively good performance with state-of-the-art on one of the data sets when it comes to this type of models, definitely beating most of the transformers out there while being much faster. While being much faster. When it comes to data sets like MOL HIV and MOLPCBA, what I've seen is that these data sets are relatively easy to overfit. And most of the models that perform well are very efficient with the parameters or employ very complicated regularization or pre-training schemes. And then finally, when we use a very large data. When we use a very large data set of over 3 million molecules, this PCQM4M data set, in this case, the V2. So, what Amea was presenting was the V1 from around a year ago. So, here we achieved also the state-of-the-art performance currently. I think I've seen a paper that improved by 1000 or and And yeah, this is also currently ongoing challenge. So we'll see how it's going to work out, what kind of state of the art performance we're going to have at the end of the year. It's currently ongoing long-range challenge at Nurips 2022. All right, so quickly about some of the ablation studies. So since we have multiple components and we can kind of Multiple components, and we can kind of pick and choose different components and how we implement them. It's really interesting to see what actually helps and what doesn't. And surprisingly, for a lot of the standard data sets, the global attention really doesn't do all that much, even though it does help in several data sets, in a few data sets, but it's not like you need the global attention for every benchmark out there. So, let's say for zinc, we haven't seen really any performance. We haven't seen really any performance gain from having the global attention. It really comes from mostly the architecture and the positional encodings. For large graphs like our larger graphs like CIFAR 10, there we've seen some gains in the performance by using the transformer. And then when it comes to using these linear transformers such as Performer or BigBird, we see some degradation in the performance. In the performance, and also, but they allow us to scale to much, much larger graphs. Though the other side of that message is that there's not really benchmarking data sets that would require such scaling. Most of these graphs are fairly small. The largest is this malnet tiny, where we have graphs up to 5,000 nodes. All these others have just low few hundreds, and n square attention is. It's and n square attention is perfectly fine for that. And then, when it comes to the message passing component, the message passing component is very important. Without that one, we see the most the large or the largest loss in the performance. And then depends on the data set, whether you need to actually learn edge embeddings and the task and the featurization of the graphs really. Featurization of the graphs really benefits from it. You can choose either the Gini or the gated GCN or PNA layer, since all these graphs, they come with quite a bit different original featurization. All right, and then when it comes to the positional encodings, we've found that the random walk structural encodings work very well for mostly molecular data because it provides this kind of structural. It provides this kind of structural information, so it's kind of a cheap way how to do this kind of substructure-enriched encodings. And overall, also the CyNet performed very well, albeit that takes a lot longer to run. And the peg layer is only used in the message passing, and it's not actually then represented as a node feature. Represent this as a node feature. So perhaps that's why we haven't seen much of a benefit of that type of encoding in our architecture. All right, so quickly, why this is general? Because we can pick and choose several of these components to match various data sets. It's powerful because with several of these positional encodings, it's probably more the message. Probably more is probably more powerful than one WL test. Additional, if we assume the efficient coding of the local message passing and exponentially growing dimensionality of the representation in subsequent layers, then the information can be stored and effectively retrieved from the nodes by the global attention. And through that, we can also Through that, we can also argue that the architecture has a potential to be a universal approximator of functions on graphs. And the scalability, again, comes from the fact that we can switch out to the transformer for the linear transformer. So that scales linearly if we need that. And I'm kind of running out of time. So I'll just quickly run through the few benchmarks that we designed. Few benchmarks that we designed to actually elucidate whether models really perform well on data sets that do have some kind of long-range dependencies, because many of the data sets really don't have this property. And for that, one of the ways we took Pascal visual object classification data set and its equivalent that is like 10 times larger than Microsoft Cocoa and runs. And run select superpixelization on these images and then assigned each superpixel a class based on the majority class of the pixels in this superpixel. So then the task is to predict what type of an object each of these superpixels represent. And we derived a region boundary graph from this kind of superpixelization and then turned this into a node level classification. To node level classification task. Then another one is based on peptides, where we take a data set of around 10,000 peptides and don't consider them as being a sequence of amino acids, but actually consider the molecular graph. So the neural network actually has to learn to recognize the different amino acids and what we are. And what we are trying to predict is either the class of the peptide or properties that are derived from the 3D shape of the peptide. So, that's something that is not available in the original features. So, for that, the model needs to be able to capture dependencies that are far beyond just local neighborhood. And staying in the computational chemistry. In the computational chemistry, we took the data set of PCQM4M and took like a half-million subset and look at the 3D structure of these molecules and defined atoms to be in contact if they are close in this 3D space, but far in the original molecule. So, if you need more than five bonds to travel from one atom to the other, but in 3D space, But in real 3D space, these two atoms are actually very close. Then we define that as a contact, and this task is then a link prediction task. And again, this requires the model to reason over longer ranges than just a local neighborhood, which we believe is kind of the case for zinc and so on. These are roughly the sizes. These are pretty big. Also, the graphs themselves are. So, the graphs themselves are relatively large compared to a lot of the benchmarking data sets out there. And we see most of the difference between the message passing GNNs and transformers on the first few that are derived from this visual object classification data sets. Our GPS then here showcases the scalability because these data sets are really big, and for example, graph transformers such as SEN really doesn't. Such as SAN really doesn't scale to this data set. And even after 60 hours, we were not able to converge to somehow great performance. And then in the other data sets, the differentiation is still there between message passing GNNs and transformers, showing that the long-range connections matter, but the margin is not as large. So that brings it to the end. So let me conclude here that our general power scalable approach combines this idea of augmenting the node features or edges with positional or structural encodings that make the local message passing more powerful and also enable the global attention as, let's say, implemented by Transformer. Implemented by Transformer to reason over the graph in a more efficient way than a local message passing could. And with this approach, we got SODA in seven out of the 11 existing benchmarking data sets. And we also, in the other paper, introduce these five new benchmarking data sets that are, I think, a good test bet that is more challenging than the existing benchmarks. So, with that, I Marks. So, with that, I thank you and also want to again acknowledge all my co-authors that contributed to this work.