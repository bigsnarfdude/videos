Okay, thanks a lot for the invitation. Thanks a lot for this nice introduction. Okay, so I will talk about the method of fundamental solutions in the context of Laplace equation. So, I'm going to go ahead and do a little bit of a detailed data. Yeah, okay. So I will consider this problem. I will assume that omega is a bounded linear region. As always happens in the context of the method of fundamental solutions, we typically choose an artificial surface that surrounds the domain, something like we have in this picture. So this is the region of interest, and just take an artificial surface. An artificial boundary where you put some points for singularities. So, our approximation, so this is the fundamental solution of the PDE. So, essentially, the method plays with the fact that we know the fundamental solution of the PDE. That's why we build traft type approximations. So, this is the fundamental solution. We just play with translations of the fundamental solution. These points, for which we have singularities, are placed on this admissible curve that surrounds the domain. So, by construction, So by construction, we have no singularities at all in the domain, but we have a trap type approximation. So this approximation satisfies by construction the PDE. So we have to focus just on the approximation on the boundary data. We can use a collocation method to determine the coefficients and so on, because we have some coefficients to play with. So we just solve a collocation problem in order to determine these coefficients. Okay, so a first question. Okay, so a first question. What about completeness? In other words, are we able to approximate an arbitrary solution of the problem? Actually, just with this simple formulation, it is not the case. It is straightforward to build a counterexample, for instance, this one. Let me assume I take as artificial boundary the unit circle. So by construction, since the fundamental solution, it is a logarithmic function, it vanishes when x is equal to 1. When x is equal to 1. So, in other words, if we consider this setting, essentially each one of the basis functions vanishes at the origin. In other words, the same does for the linear combination. This means that we are able to approximate just functions that vanish at the origin. Of course, this is not a general property of harmonic functions, so essentially we have not density in this setting. Okay, but the remedies is quite simple. Actually, it is enough to put here a constant. In other words, it is enough to consider. In other words, it is enough to consider an extra basis function which is a constant function in order to have density. Okay, so in this case, in this general setting of adding a constant as another basis function, we are able to prove density results. In other words, we can pick an arbitrary solution of this problem. There's always a linear combination of this type as close as we want from the solution. Okay? Okay. Let me just, yes. Right. Yeah. Uh there are criteria for for the for the for placing the the singularities. In principle, they should be points placed outside the domain. That's it. But of course there are better choices. There are literature about how to choose the source points. Okay? I will not discuss this in this talk, but we can discuss later things like that. Okay? Good. Okay, let me just mention some classical results concerning a process. Some classical results concerning approximation made by the method of fundamental solutions in the context of this problem. So, Laplace equation. Okay? So, let me assume omega is a unitary disk, and g is the trace of entire function. So, we have something like this. My region of interest is the unitary disk. So, actually, this is r equals 1. And then I take the artificial boundary to be also a circle with radius r larger than 1, in such a way that we have no singularities in our. Have no singularities in our domain. So there's a result by Katsurada that actually says that, of course, you can, this is Laplace equation, you can measure the error by maximum principle, you can just study on the boundary. And if we bound the error on the boundary, always we bound the error in the theory of the domain. So what does this result tell us? Essentially, the error is bounded by something like this. There's a positive constant in such a way that we can. Constant in such a way that we can always bound the L2 norm of the error. Of course, this says that we can expect to have exponential conversions. Okay? Just a naive comment. Actually, it is not exactly the correct way. But just looking at this upper bound, we can suggest something like this. Okay, it is good to take capital R to be really large, because in that way we have, it is not the case. Why? Under similar assumptions. Under similar assumptions, essentially, if we look at the condition number, we realize that actually the conditional number also increases the exponential. So there's a kind of compromise. In terms of accuracy, let's say it would be good to take R to be really large. In terms of conditionality, it's really, really bad. So in some sense, for really large values of R, the MFES can be highly ill-conditioned, and this prevents to achieve in practice exponential compactness. Okay? Okay, so this is the motivation. Okay, so this is the motivation for my work. Essentially, I will try to work in the setting of the method of fundamental solutions, but not with the direct basis that is defined by the direct method. I will try to find different bases that span exactly the same functional space, but they are well-conditioned. So let me start from this. Okay, let me assume this is my region in red, and let me assume I will put all the singularities on the circle. This is the first approach, actually, this is. This is the first approach, actually. This is I called MFESUR. There's a huge constraint here that is, we must assume for this construction that actually all the source points are placed on a circle. It is not the best choice for placing the singularities, but let me start with this approach. Under this assumption, let me write each one of the singularities in this way. So each one of the source points can be written in this way, okay? For a certain angle, okay? So essentially, I just let me. Okay? So essentially I just play let me drop the constant of the fundamental solution. I can assume it go it goes directly for the coefficients, the linear, for the coefficients of the linear uh combination. So let me just play with this function. Okay, using or writing the fundamental solution in terms of polar coordinates and playing with the properties of the logarithm as elementary properties, essentially I can write this. Each one of the MFES basis functions can be written in this way. Right? Okay. Right? Okay. This is the angle at which we take the each run of the single element. Okay, good. Okay, so let me assume now we consider equally spaced nodes on the circuit. We have n basis functions to play with. Let me take the point sources equally spaced on the circuit. If I consider the previous expansion and put everything together, essentially I can write this. What is this? This is a vector field. This is a vector field made by the n basis functions defined by the method of fundamental solutions. So essentially, in the method, we are playing with linear combinations of these functions. And we can write all these functions in this way. Of course, this is an infinite expansion, but at a certain point, we will truncate this. Sorry. We will truncate this for a certain integer in such a way that we don't miss anything in terms of double precision accuracy. Okay? So. Okay, so essentially I truncate this in order to get a factorization. If you want, B, so this is the vector field made by the n basis functions for the method of fundamental solutions. B is a discrete version of this discrete version of this metrics, which is just evaluation of Fourier functions, if you want, in terms of the angles that you took for the points for singularities. Then there's a diagonal matrix, T, and then this vector. And then this vector field made by harmonic polynomials. Okay, so here we can immediately realize the problem of field condition comes essentially from this diagonal matrix. We have, so epsilon is smaller than one. So this means that we have increasing powers of epsilon, so of course a lot of quantities going to zero if you want. So if you want to correct the problem of field conditioning, we should tackle these quantities that go to zero. We should correct these effects somehow. Okay? Correct these effects somehow. Okay? Good. This is the most important remark in my talk, and that actually it is very, very simple. If you multiply from the left any invertible matrix, what we are doing essentially, we are modifying the basis without modifying the functional space. So the issue is just what could be a good matrix to be applied from the left. That's the the question. Okay? Good. So for the MFESQR, I just consider uh the QR factorization of matrix B. QR factorization of matrix B. So this allows to write B as QR, Q is unitary, R is upper triangle. So essentially, I get this. So the vector, again, this is the vector field made by and basis functions for the method of fundamental solutions, and I can write it in this way. Good. Now what to say? I will try to convince you that this is a good matrix to be applied from the left. At least formally. At the end, for practical purposes, we will not do the multiplication. Proposals will not do the multiplication. But formally, this could be a good matrix to be applied from the left. Why? Just look at the definition. If I multiply from the left this, I use the fact that Q is unitary. So this gives just the identity. Then I can write this. Don't forget, this is a vector field made by harmonic polynomials. This is upper triangular matrix. And then there are two diagonal matrices. Diagonal matrices, one at the left, the other at the right. So essentially, I can compute this product in a convenient way and said, look, D, this is matrix D, so there are a lot of quantities going to zero. Here, we consider the inverse, so there are a lot of quantities going to infinity. So in order to avoid numerical problems, we should compute this in a convenient way. But actually, this is straightforward to be done. Why? Because R is upper triangular matrix and D and D one. Matrix and D and D1 are both diagonal matrices. So it is just a linear algebra to write this in this way. This is Adamara product of matrices. So I mean just product component-wise. So essentially, this is the matrix that you should introduce that depends on epsilon, the radius that we took for placing the singularities and that's. So actually, we built new basis functions. Formally, we multiply the matrix on the left, but... We multiply the matrix on the left, but for practical purposes, we just do this. F is a vector field made by harmonic polynomials, and then we just apply at the left this matrix that we have just introduced. And that's it. Okay? Okay, now second approach. The first approach, MFSQR, as I told you, had a constraint of putting all the singularities on the circle. Let me try to drop this assumption. Okay? So I will consider again. Again, the expansion of each one of the MFES basis functions as a series. Now I can assume that my artificial boundary can be something different from a circle. I have just a geometric constraint to be imposed. Actually, I should have a ball like I have here that is in between the two boundaries. For cover, let me show you the expansion. For convergence of this expansion, actually, from the geometrical point of view, I need to... Actually, from the geometrical point of view, I need to have a ball in between the two boundaries. This is the constraint that I have at the moment. I will try to drop it in the future, but for now, I must live with this. So don't keep in mind, this is the constraint that you should impose. There's a wall in between the two boundaries, the boundary of the domain and the artificial boundary. So again, we do more or less the same construction. So this is a vector field made by MFES basis functions. And I can write using. And I can write using the previous expansion in this way. So there's a matrix like this one, and then there's a vector field made by harmonic polynomial. Okay? Good. Okay, so I will truncate this in such a way that we have this factorization. And now the same construction. I will try to find a good matrix to be applied from the left in such a way that we cancel that effect of all the quantities going to zero. Okay? Good. Let me identify here two points. Let me identify here two possible sources for ill-conditioning. The matrix M is ill-conditioned. So there are, again, quantities of increasing powers of epsilon. So a lot of quantities going to zero. On the other hand, in the vector field made by harmonic polynomials, we have also an increasing power of this type. So this also generates zilt conditioning. This is that at the end we get something like Van der Moon matrices. And all know that that these are And know that these are ill-conditioned. Okay? Okay, good. So what to say? Let me assume for a while, and later I will show you that actually it is the case. Let me assume for a while that I can write something like this. The vector field made by harmonic polynomials can be written as a certain function. For now, I don't care too much about this function. Apply it to a set of well-conditioned particular solutions of the Laplace equation. Let me assume for a while I can write all of these, okay? I can write all these. Okay? So, assuming that we can write these, essentially we just plug in this in the definition of the MFPS basis functions, and we get something like this. M1, a new matrix that we have just introduced, apply it to a vector field made by well-conditioned functions of Laplace function. So, what to do? I just consider the singular value decomposition of this matrix. So, actually, I can write something like this. Something like this. Okay, u and v are unitary. S is diagonal matrix with non-negative entrance. Okay, and so I play with the definition of these matrices. Okay, so I use the fact that this product is the identity, so I just keep this product. Now we know that S has the same dimension as M1, so essentially I can write S in this way. S1 is a square matrix, 0 is a square matrix, zero is just a block made by zeros, right? So this product actually, this product S V star is just S1 V1 star. S1 is a diagonal matrix with all the components going to zero. But now it is straightforward to tackle this problem. Essentially, I just need formally to multiply by the inverse of this matrix. In praxis, we don't do this, but formally we are multiplying by this. Formally, we are multiplying by these metrics. And so we get something like this. So, what is the new set of functions? Essentially, we define a new vector field, which is this one. Look, this is a set of well-conditioned solutions of Laplace equation. And this is V1 star. What is this? Just a block made by the first 10 rows of one of the unitary vectors, matrices that we obtained from the SVD decomposition. V the composition. So essentially, we have two approaches. Direct MFES that plays directly with shifts of the fundamental solution. A second one, which is MFESQR, made by the basis that we get from the first approach. And finally, the MFES-SVD, exactly the same. In three cases, we have linear combination of particular solutions. So how to determine the coefficients? By collocation. For instance, in this paper. In this paper, I consider the number of points on the boundary to be twice the number of singularities. Of course, I can write, for instance, for this system, it is the same for other situations. So essentially, I can write a system like this one. g is a vector containing all the boundary data. I just solve this least squares problem and I get the coefficients. And once we have the coefficients, we have the solution everywhere. That's it. Let me, okay, of course I have a maximum principle because I'm playing with Laplace equation. Principle because I'm playing with Laplace equation. So I just need to compute or to control the error on the boundary, and that's it. Okay, let me just show you a first example. I would not advise to use this in the context of the MFES, but since the vector field made by Armonic polynomials is orthogonal for the disk, let me try this strange situation, for which I take a strange artificial boundary with this boundary data, and let me show you the results that we get. Essentially, this is a typical situation. Essentially, this is a typical situation. On the left, we have a convergence curve. So the direct MFES is plotted in red, and the MFES SVD is plotted in blue. As you can see, for small values of n, let's say up to 40 or something like that, the two points coincide because we are playing with the same functional space. We are just playing with the different bases. But at a certain point, let's say for instance, n equals 40, something like that. Is something like that. Essentially, the convergence breaks down, the convergence of the direct MFES. Why? The right plot is the condition number. So essentially, the condition number blows up. And since it blows up, you cannot expect to get something better. On the other hand, for the MFES SPD, we can exhaust machine precision, and the condition number is always of order one, independently of n. Okay, let me just say something. At a certain point, I said that, okay, let me assume I can write f as a product. Assume I can write f as a product of a matrix times a set of well-conditioned functions for the Laplace equation. Okay, it's time to check that actually I can do this. Actually, this relies on the work by Trefet and collaborators. Essentially, I can write the vector field made by harmonic polynomials in this way. So this is, there are two blocks, Z and W. Z is a van der Moon matrix. W is a van der Moan matrix, provided we exclude the first column made by one. Column made by ones. Okay? Okay, so based on the work by Trefeten, essentially, since this is a van der Monde matrix, we can interpret this as a sequence of vectors like this one, where x is a diagonal matrix. And our null deliveration allows to build a sequence of vectors spanning exactly the same space as the columns of Z. So essentially, playing with Trefeten's method, we can... With prefetance method. We can write this for the two blocks. Let me skip this. And at the end, we get exactly this, what we need to write. So this vector field is a certain matrix that depends on R. I don't care too much about this. Times apply it to a vector field made by well-conditioned solutions of harmonic find of Laplace equation. Okay? Let me just show you some numerical results. The first one, okay, this. Okay, this is the region of interest, the boundary of domain. Let me took as artificial boundary the circle because I need to compare the three approaches. And for the MFSQR, I always need to put the singularity on the circle. So let me take as a first example this situation. This is the boundary data, okay? And again, convergence curve. So the direct MFES goes up to 10 to the power minus 5, something like that. 5, something like that. But for this, the condition number blows up. So essentially, the convergence breaks down. The MFESQR can improve a little bit, up to 10 to the power minus 9, let's say. And also, the condition number also increases exponentially, but at a lower rate. But finally, for the MFES SVD, essentially we have the condition number always of order 1 independent of n, and we can exhaust the machine precision. Okay, what's the problem of? What's the problem of essentially what's the reason for the ill conditioning in this situation? Essentially, let me consider this case. This is the region of interest, so I'm considering a circle, and this is the artificial boundary. For determining the coefficients, I need to solve a least squares problem on the boundary. So essentially, these are the functions that I'm playing with. Essentially, it looks like the same. You understand my point? So in some sense, we are trying to find solutions from... Solutions from the linear, from the span of these functions, but they look like the same on the boundary. On the other hand, with the approach made by the MFES SVD, these new bases that are complex functions, complex valued functions, span exactly the same functional space as those. So essentially, but these are well-conditioned bases. So there is no issue in solving the problem on the boundary. One last example, in this case, I took this to be the boundary, and this is the artificial boundary. To be the boundary, and this is the artificial boundary. Again, I have this boundary condition, which is not a harmonic function. This is okay. Let me okay. And again, the convergence curve. So the direct MFES with this method, we can get something like 10 to the power minus 2 or something like that, because the condition number is always huge. On the other hand, for the MFES SVD, we can see, this is log scale, so we can check that actually. Lock scale, so we can check that actually we are having this seems to be exponential convergence up to machine precision and condition number is always of order one, independently of n. Okay? One last example, I just changed the boundary data. Now it's this theory function. And again, this is the solution, and again, this is the convergence curve. So the direct MFES, we can get something like 10 to the power minus one and not better than that. For the MFES SPD, we can go up to. The MFPS SPD, we can go up to machine precision. Okay? And that's it. So, future work. I would like to extend this to completely general boundaries. Now for convergence of the series, I need to assume that there is a ball in between the two boundaries. I would like to drop somehow this assumption. Then the extension to the recaps. Actually, I have a PhD student working on this. We have the paper submitted right now concerning the 3D case. And also extension to other PhDs. And also extension to other PDEs. We are considering Helmholtz equation and Nafier equation from linear LSDs. Okay, so thank you. Thank you. Can I get confirmation? Can you hear me online? I know there was a problem with the microphone. Okay, wonderful. We get the right microphone. All right, so do we have any questions in the room or online? I just want to make sure that I understand. So you take these functions and expand them into a sonic functor and then you re-scale sonic functions by some powers or some fixed conditions of the current number. So you could in principle look at optimal yes, of course. In the context of the MFES there are two big issues. One is the optimal location for the One is the optimal location for the source points. And at the point, I would say this is well studied. We cannot expect to have very good an optimal choice in the sense that it is always the best. We understand? This depends on the domain, this depends on the boundary data, this depends on a lot of things. At this point, I could say there are some choices proposed in the literature that are generally very, very good for general domains, for and for general boundaries. I mean it's not crazy to think that you could add some other constraints and just find I mean there's not one optimal, but basically I guess you could do better than just place them if your shape has a certain director. If you want you can include the placement of the singularities like one of the degree of freedom in your system. At the end we get something that is non-linear. Do you understand? And you lose the simplicity of the method. So I believe that one of the attractive points of the MFS is the simplicity. We just fix some source points, and that's a linear combination of the functions that we generate with the fundamental solution. Do you understand my point? So I prefer to stay in this setting. That you fix a setting for choosing the source points, and you just play with the coefficient. But if you prefer to get the, or to obtain the optimal allocation. The optimum allocation for the artificial boundary, you can do something like this. You can consider optimization, or include the placement also as a degree of freedom in your system. At the end, it is a non-linear system to be solved. For instance, yeah, why not? If you have a corner, if you have a corner, No, if you have a common that's that's a different case. Yes, sure, but but uh yeah, there are two issues. If you have a corner, uh if you have a corner, do we have a different behavior on the neighborhood of the corner? So probably I would guess that the the best situation is to enrich the functional space with some particular solution that that can tackle that problem. You understand? So if you want, you can assume that the solution to be a regular The solution to be a regular part plus a singular part. This deals very well with the regular part. If you have some singularity introduced by a corner, for instance, you know particular solutions that can deal with the correct behavior of the solution in the neighborhood of the corner. So you just correct the solution. Somehow you enrich the functional space with some similar functions that can tackle, can approximate correctly, the, reproduce correctly the behavior of the solution in the neighborhood of the kernel. That's it. Problem. That's it. Okay, can we go up here? I don't see any questions online. So if not, let's thank our speaker again. They need the microphone.