So, today I want to talk about isotonic subgroup selection, which is some recent work with three fantastic collaborators of mine. Manuel Muller is my PhD student. Henry Reeve is at the University of Bristol, and Tim Cannings was my PhD student and my postdoc. And after a short stay in the University of Southern California, he is now at the University of Edinburgh. University of Edinburgh. Okay, so I want to begin by thinking about a very hypothetical situation. You have to imagine that there's a global pandemic. Sorry? Okay, so here are my collaborators. I'm sorry about that. So the hypothetical situation I want you to imagine is that there's a global pandemic, and you're in charge of the vaccine trial. Of the vaccine trial, and in order to get past the regulators, your vaccine needs to be 80% effective. And you do your randomized clinical trial after an enormous amount of research, and you find this disastrous outcome. The trial failed, you needed 80% efficacy, and it's only 70% effective. And this is a very common situation in a lot of randomized controlled trials. And fortunately, in this case, And fortunately, in this case, your clever collaborator saves you and says, hang on a minute, for women over 50, it's 95% effective. Can we just give it to them? So this is a very tempting thing to want to do to rescue a clinical trial. But somehow our reaction is something like this, right? We know that there's something dodgy going on here. Maybe it takes a moment to put your finger on it, but that this is something. That this is something we sort of instinctively know isn't right. So, the reason that this should feel uncomfortable to you is that the subgroup that we're considering here, the set of women over 50, was chosen after seeing the data. And so that invalidates any type 1 guarantees that we might want for our procedure. Procedure. And so this is actually what's called a post-selection inference problem. So we are choosing a subgroup after having seen the data, and yet we still want to have some guarantee about the performance of our method on the selected subgroup. And in fact, if you look in the clinical trial literature, there are huge numbers of papers saying you shouldn't do subgroup selection. It's almost like the first commandment of clinical. The first commandment of clinical trials: thou shalt not do subgroup selection. And yet, this is exactly what I propose to do in this talk. So, our challenge is to identify a subset of the covariate domain where our regression function exceeds a predetermined threshold. So, in the vaccine example, it would be 0.8 or 80%. I want to talk about an application that's very close to my. An application that's very close to my heart, quite literally. In fact, just over two years ago, I blacked out in a swimming pool and I had to go to hospital and I was diagnosed with hypertrophic cardiomyopathy, which is a congenital heart issue. And whilst I was in hospital, I was having a lot of different scans and they were taking a lot of mycovariates with the aim of risk stratifying me. So they're trying to determine or try to estimate my risk of sudden cardiac. Estimate my risk of sudden cardiac death within the next five years. And if this is deemed sufficiently high, then you have to have a defibrillator fitted. And in fact, this is what I do now have. So if we think about this problem, the challenge the doctors face is to identify the subset of hypertrophic cardiomyopathy patients who are at low risk of sudden cardiac death, so they don't need. So, they don't need a defibrillator. But there's definitely an asymmetry to the two sorts of errors you might make here. If you have someone that's high risk of sudden cardiac death and you assign them as low risk, that's a much more serious error than the other, making the error the other way around, which is still a non-trivial error. It still means you have to undergo surgery and you have to live with a defibrillator. But I want to emphasize that there's definitely an asymmetric. That there's definitely an asymmetry here. Okay, so here's the statistical setting I want to think about. I've got covariate response pairs. My covariates are going to take values in r to d. And eta is going to denote my regression function, and mu is going to denote the marginal distribution of the covariates. So together, eta and mu determine the distribution p. And given a predetermined threshold level torr, Determined threshold level Torr, we're going to seek to identify what I'll call the Tor super level set of ETA, the set of points where ETA is at least at level Torr. So Tor would be 0.8 in the vaccine example. And of course, we don't know ETA, we don't know the mu, but what we're going to assume here is that we have an access to a training data sample of n independent copies of the pair xy. Okay, so what do I mean by type one error control in this setting? Well, I actually want to have a really strong notion of type one error control. As I've said in the clinical trials literature, there's a huge skepticism about doing subgroup selection. And so, well, the guarantee we're going to seek is that the set we report, the subset of the covariate domain that we report, should be a subset of the torso. Should be a subset of the tor superlevel set of ITA with some high pre-specified probability. So this is giving guarantees on an individual level. For every individual in the selected set, we can be confident that the regression function is at least at the level tall. It's not averaging over subgroups. In that sense, it's a strong type 1 error control guarantee. So, based on my experience in hospital, I think that the threshold where they decide to give patients an operation and implant a defibrillator is around a 2% chance of sudden cardiac death in the next five years. So, in this case, that would correspond to TOR is 0.98. Of course, it's very easy to satisfy. Of course, it's very easy to satisfy the type 1 error guarantee by reporting the empty set. That's not a very interesting outcome. So, on the other side, in a sort of power guarantee we're going to look for is to seek to minimize the expected regret. What we're going to do then is to look at the mu measure of the set of points that we emit in our selected set. So, what's in the Tor super level set, but we don't select. Look at its mu measure and take its Look at its mu measure and take its expected value, and that's what we'll call the expected regret. So, you might already be thinking, well, why don't you just do sample splitting? Why don't you just do a first stage trial where you seek to identify a subset of the covariate domain and then seek to verify that on a second stage? That certainly would give you type 1 error control if you pass the selection at the second stage. At the second stage. But at least in a clinical trial context, it can be extremely time-consuming and potentially expensive. And of course, the sample splitting may mean you have an inefficient use of data. But my main objection to this is actually this last point here, that if at the second stage you don't manage to verify that your candidate A hat does satisfy the property you want, well, it might be the case that there's a further subset of A hat on the There's a further subset of a hat on which you do have the regression function at least a level Tor, but you can't verify that with any type 1 error control in this process. So that's definitely a drawback of sample splitting here. Okay, I'm going to be mainly thinking about a setting where the regression function is increasing. That's what the isotonic word means in isotonic subgroup selection. So we'll say eta is increasing. If it's increasing coordinate, Increasing if it's increasing coordinate-wise. So, whenever I increase in each coordinate, then the regression function goes up. And in a lot of medical examples, it's easy to think of covariance with respect to which we would expect a health outcome to be monotonic. So, for instance, in the case of HCM, age, family history of sudden cardiac death, and wall thickness measurements in your heart are very relevant. Are very relevant covariates, and you would expect the regression function to be monotonic in those covariates. So the main class of distributions I'm going to be interested in, this curly p mon, is the set of distributions p for which the regression function is increasing. And I'm also going to assume that the error distribution, in fact, the conditional error distribution, given the covariance, is sub-Gaussian with variance. Is a sub-Gaussian with variance parameter sigma squared. So here's going to be our high-level strategy for selecting a subgroup. So for each x in R to the D, I can define a null hypothesis. That's a set of distributions where the regression function is strictly less than torr. And I'm interested in whether or not I can reject that null hypothesis. So first of all, I'm going to just take a subsample of my covariate vectors. You're not losing very much in this talk. Vectors, you're not losing very much in this talk if you just think of m as being equal to n, although it turns out that the optimal choice of m may be a little bit smaller. And then for each of my data points, I'm going to try to construct p-values for testing the null hypothesis of whether or not the regression function at the point xl is less than tau. And then we're going to look for a multiple testing procedure for DAGs. We're going to look to exploit the To look to exploit the logical relationships induced by the ordering of the covariate domain to reject a certain subset of the hypotheses with family-wise error rate control. And then having done that, I'm going to report as my overall selected set the smallest upper set containing all of the rejected hypotheses. So as I like to joke, if you don't know what an upper set is, there's no need to get upper set about it. No need to get upperset about it. It's just the smallest set, it's the upper hull of the set of rejected hypotheses. So the set of points X, which are larger than at least some XL in this ordering on R to the D. Okay, so this is a sort of picture that I have in mind. We have this staircase-like regression function, and the true super level set is this sort of darker bluey. Set is this sort of darker bluey-gray green set here, and then our algorithm is outputting this red set here, which in this case is a subset of the true super level set of the regression function. Okay, so this is our high-level procedure. Let's put a little bit more flesh on the bones here. Let me talk about the specific case of the univariate setting. So, we're just thinking about one-dimensional distributions where the regression function. Distributions where the regression function is less than tau for the null hypothesis. So here we can just take m to be n. I'm only going to look at the points that are to the left of x and let n of x be the denote the set of the cardinality of that set. So then ordering to the left from x, I let x j of x denote the jth nearest neighbor of x amongst that set. X amongst that set and it's got corresponding response y brackets j of x and a crucial observation for us that is that under the null hypothesis and even conditional on the on the covariates this process SK is a super martingale. So on average it's going down because under the null hypothesis the eta of XJ is smaller than Taur. So it's got a negative mean. Than tau, so it's got a negative mean, and what that means is that we can obtain p-values by inverting time-uniform upper boundaries that are obtained in a paper by Howard et al. from a couple of years ago. And you get a picture that looks something like this. So here we have our gray line is our regression function eta, and we're interested in where it exceeds this green level torque. We've got our point x here. We've got our point x here, and this is the hypothesis we're interested in testing: is whether or not the regression function at x is smaller than tor. And we can see here that actually here, eta of x exceeds tor. We're only looking at the data to the left of x, so z zero, z corresponds to x y pairs, z zero doesn't get looked at here. And if we look at the first point to the left of x, well, actually, its y value is below tau. Its y-value is below torr, so there's certainly no evidence against a null hypothesis here. Then we go significantly above torr, below again, and then above. And on average, the regression function is, of course, increasing, which means that these points down here are going to tend to have a smaller mean. And then we construct this process SK by these partial averages. And so initially we come down because we've got this. We come down because we've got this negative increment, delta one, and then delta two, this z two, gives me some evidence against the null, but not quite enough to reject the null hypothesis here, because these are the time uniform upper boundaries, this VK of alpha. But when we get to point four, we see that we do exceed this level, and that does allow me to reject the null hypothesis correctly in this case, because eta add x is bigger than tor. Bigger than tor, and I can obtain a p-value as the level which just allows me to touch this maximum value here. Okay, that was in one dimension. In D dimensions, we just need to have a mild adjustment where we just look at the jth nearest neighbor amongst this set of points that are down and to the left of x. And we look at the jth nearest neighbor in the supremum norm. And here's a constant. And here's a complicated formula for how we can obtain these p-values, but it's at least a closed formula. And the reason why this complicated definition makes sense is because of this lemma that says that under the null hypothesis, even conditional on my covariates, these p-values are indeed p-values. Okay, that's taught me how to construct p-values. Now I've got to combine these p-values. So I want to use Values. So I want to use some multiple testing procedure for DAGs to try to obtain a selected set with family-wise error control. So we're looking to select a subset of 1 up to m with the property that the probability I reject any of these null hypotheses falsely is less than or equal to alpha. And the algorithm is easiest to describe are an example. So this is a situation. So, this is a situation where we've got seven points in two dimensions, and to each of these points, we've got an associated p-value. The p-value is written in the bottom here. And you might like to think to yourself, if we were looking for family-wise error rate control at level 0.05, how many of these hypotheses could we hope to reject? Okay, well, the algorithm proceeds as follows. There are two relevant graphs here. One is Graphs here. One is the induced DAG. That's the DAG that includes both the arrows and the solid lines and the dashed lines here. And these encode the logical relationships between the hypotheses that are induced by the increasing regression function. So if eta is bigger than torr greater than or equal to torr at node five, then it certainly is at node seven because of the increase. Is at node seven because of the increasing nature. And so of the regression function. So if I take the union of the solid and the dashed arrows, then this gives me an induced DAG. But I'm also interested in a sub-DAG of this, which is called the induced polyforest. And that I obtain by deleting edges in the induced DAG. And I only get to, I do this in such a way that each I do this in such a way that each child has only one parent. And where it has more than one parent in the induced AG, I only keep the one that's closest in supremum norm. So three has two parents within the induced AG, but I only keep the one at five because this is closer in the supremum norm. Okay, so that's why this line here becomes dashed. Okay, so the algorithm proceeds as follows. We want to have family-water error rate control. To have family or arrogant rate control at level 0.05, and we split the budget, the 0.05 budget, in proportion to the number of children that each of the root nodes has in the induced polyforest. So this node here is itself a leaf node, so it gets signed weight one. Node seven has two leaf nodes. Seven has two leaf nodes within the induced polyphores, nodes two and three, so it gets twice that weight, and node six just has one leaf node within the polyphores. So having done that, I then compare the p-values with each of the with the budget levels that have been assigned to it, and I see what I can reject. Well, I can reject node one because 0.01 is below the 0.0125 here. Then I reassign. Then I reassign the budget by imagining that this node one didn't exist. And now I split this in level two to one. And that allows me to reject node seven. Then I pretend that node seven didn't exist. And so node five is now a root node. And I've got this split to two to one again. But now I can reject not only five, which is lower than 0.0333. Than 0.0333, I can also reject its parent within the induced DAG because of the logical relationship in the ordering. Even though the p-value at node six is really quite large, I still get to reject that. And then I can't reject anymore once I've got to that stage. So this tells me that the multiple testing procedure for DAGs that I have in mind. And the important lever. And the important lemma is that this DAG testing procedure controls the family-wise error rate, which means that our overall selection procedure controls type 1 error at level alpha over this class of distributions, curly Pmon. In fact, it controls the type 1 error over a larger class of distributions. All I require is that the Tor superlevel set of eta is an upper set. So that's a weaker assumption than saying the regression function is actually increasing. In fact, saying it's increasing would say that all of the Tor prime super level sets are upper sets. So I'm just asking for a single level Tor to be an upper set. Okay, so it turns out if we let A and hat denote the set of data-dependent selection sets that control the type 1 error at level alpha over P, then you've got no chance to have any power. No chance to have any power until you make some more assumptions. So, this is a very strong result, a negative result that says that notice the ordering of the sup and the inf here, they're the other way around from what you normally expect in minimax lower bounds. And that's because it's the same distribution p that's destroying every data-dependent selection set with type 1 error control. Indeed, this is saying that you can do no better than the rule that outputs. Than the rule that outputs the empty set with probabilities and the whole space with probability alpha. So we need to assume to impose some more assumptions. And these are the two restrictions I want to make. The first is quite a mild restriction that just says that the covariate measure of a ball of radius r is not too different. Is not too different from what you'd get from the Lebesgue measure of the ball. So, up to this level theta. I mean, if you had a Lebesgue density that was bounded above and bounded away from zero on the torso level set intersected with its support, then this condition would be satisfied. And this second condition says that as you move from the boundary of the Tor super level set inwards, so sorry, you're sort of moving up into the Sorry, you're sort of moving up and to the right, then you have to have a non-empty intersection of that with this higher level. So it's really imposing a rate at which eta has to grow as you move away from the boundary of the Tor super level set. And under those conditions, so if I look at a distribution that satisfies a monotonicity assumption and The monotonicity assumption and these two additional assumptions, we can get an upper bound on the finite sample upper bound on the expected regret of the procedure A hat ISS of this form here. And this is very interesting because it's saying that there are two terms here, one of which, the first one of which is reflecting the difficulty of deciding whether each covariate, whether I can reject the null hypothesis of each covariate, and then the additional term is dealing with the regions in between the covariates. With the regions in between the covariates and the error that I might make there. You can see that there's a cursor dimensionality effect in the way that the dimension appears here. And you can see the importance of this parameter gamma that is controlling how quickly I move away from the level Tor as I move away from its boundary. So that's a kind of main result on the power. And that's complemented by a mini-max lower bound over the same. Bound over the same class of distributions, which says that a no-statistical procedure that controls the type 1 error at level alpha over this class can have an expected regret that's smaller than of this order. And this is the same level as we had before up to logarithmic terms. So it's kind of interesting. We're saying that our procedure is minimax optimal, not only in the sample size, but also. Not only in the sample size, but also in this parameter alpha in the sigma squared and in the lambda, at least up to a polylogarithmic factor. So that's, I think, quite a strong justification in terms of the power. There are various extensions of the original procedure that we can handle. We can have one that's adaptive to sigma squared under a Gaussianity assumption for the errors. We can deal with bounded responses with sort of tailored p-values, and we can deal with. Values and we can deal with very heavy-tailed observations if you deal with if you handle sort of quantile regression instead of mean regression. But I want to talk about what I think is perhaps the most important extension here, which is towards heterogeneous treatment effect setting. So where in addition to covariates and response, you have a treatment indicator, a binary treatment indicator. And then you can define a heterogeneous treatment effect as the difference between the regression function for those that are. The regression function for those that are treated and the regression function for those that are untreated. If we write pi of x for the propensity score, then we can take our original responses y tilde and compute these transformed responses, inverse propensity weighted responses, y, in this way here. And the reason why we do that is because then the expected value of y given x is exactly equal to this heterogeneous treatment effect eta of x. X. In other words, if we compute alongside our original covariance, if we compute these inverse propensity weighted responses, then at least provided this distribution P belongs to the curly Pmon, then our procedure does control the type 1 error at level 1 minus alpha, or type 1 error at level alpha, I should say. So let me finish with an application. Let me finish with an application. This is to an AIDS clinical trial, the ACTG study 175. And so this was a study that involved about 2,000, just over 2,000 participants who are assigned to one of four arms. I'm just looking here at one of the arms corresponding to a baseline treatment of zidavudine. The primary end point of this trial, so the point you don't want to reach, is a reduction. To reach is a reduction of your CD4 cell count by 50% or the development of AIDS or death with a median follow-up duration of 143 weeks. So here are the outcomes for the 500 or so patients with the baseline treatment as a function of age, but age is plotted in reverse here so that the regression function is sort of increasing. This is the isotonic regression function that we see here. And what our procedure is. And what our procedure is reporting is this red line, and it's saying everyone to the right of this red line, you can be confident that their regression function is at least at level 0.5 there. And this on the right is a bivariate example where, as well as age, we take into account the CD4 cell count at the initial onset of the trial. So on the left, for instance, we can say that with alpha as 0.05 and Tora. Alpha is 0.05 and tour and a half, not reaching the primary end point is the more likely outcome. We can be confident that not reaching the primary end point is a more likely outcome for patients who are 39 and under. And then a sort of second aspect would be to look at comparing two different treatments where you compare a baseline treatment with a combination treatment of zitavudine and another drug, zalcitabine. And what's nice here is that. And what's nice here is that the propensity score is known, it's equal to a half. So we can compute our inverse propensity weighted responses, which has got a simple formula in this case, just 4t minus 2 times the original response y tilde. And again, you can see that for patients who are 25 and under, you can be confident that the combination therapy is at least no worse than the baseline treatment. So this is something that, for instance, This is something that, for instance, the sort of technology that Novartists in particular have become very interested in. Okay, so what I've tried to present here is a method for doing subgroup selection in a non-parametric way where you have an isotonic regression function. We control type 1 error by combining local p-values with multiple testing procedure for DAGs. A nice thing is you don't have to choose any smoothing parameters, and it's certainly computationally feasible. Computationally feasible, and the method gives minimax optimal regret pounds up to polylogarithmic factors. And there are lots of different extensions which you can find in the paper. And that paper is available in the archive. And if you're interested in trying this out, our package is called ISS and it's available on CRAM. With that, I'll stop and thank you very much for your attention. Okay, thank you very much for the very interesting talk. Very interesting talk, and the floor is open for questions. Hi, Richard. Very nice talk. So I have one question about the monotonicity constraint that you have. I wonder if you can comment on if it's possible to relax it and also in your real data analysis. Also, in your real data analysis, because you applied the method, do you have any recommendation on how to validate this monotonicity assumption in real data? Thanks, Yin Ying. That's two very interesting questions. Let me deal with the first one first. So, I think that there is, I spoke rather quickly about this. You can relax the monotonicity, at least. Lacks of monotonicity at least in terms of type 1 error. So, if we go back to this result, this DAG testing procedure controls the type 1 error at level alpha, not just over the class of distributions with a monotone regression function. All I need is the Tor super level set of the regression function is an upper set. So it means that the regression function can oscillate, but sort of at least in one dimension, once it's gone above Torr, it can't come down and dip below Torr again. Can't come down and dip below Tor again. So, I mean, that is, you know, you're going to need some sort of structural assumption on the regression function to have any kind of type one error control. But for me, I think this is a pretty good, strong robustness property for the procedure, at least in terms of the type one error. For the power guarantee, then I do need the monotonicity here, as well as. Sentinel here, as well as these additional assumptions. So that's one part. On the second question, how would you try to validate the monotonicity assumption in real data? I mean, I think there are situations where you can definitely expect health outcomes to be monotonic in covariance. You don't expect health outcomes to improve as you smoke more. Improve as you smoke more cigarettes, for instance. But then, I mean, it is certainly a valid question about how you would test for monotonicity of a regression function. And I believe there are some papers on this, but I'm not sure I could give it any precise references off the top of my head.