Alright, so I'm gonna talk about the ATI. Yes, not the ATI we were hoping to talk about, I guess. Yes. So actually, the idea for this talk came up when I saw this picture on Twitter. This is a map. There's a lot of references, so you can check the resources later if you want. So this is a map somebody built when Auto completing the sentence the movie was filmed in, for the country. And this Tilberg was in charge of creating here follow-up sentence and then sentimentalizes. Sentence and then sentimental analysis were conducted on this part of the sentence. And clearly, there's a bias against movies made in Africa, Asia, South America probably, which indicates a heavy bias on the model. Also Germany, which might not be a bias. So the question is: who blame here? The model itself, we can hardly blame it. And this is another example. And this is another example which somebody asked GPT-3, which is supposed to be state-of-the-art, one of the best models out there in language modeling, which is heavier to compensate, and it fails miserably at trying to have a response. The theory here is that the model is enabled to understand the relative information between two objects that probably don't usually appear together in a corfus. Together in the corpus, and also the concept of heavy, heavier, which is rather hard to represent in language. So with this idea, let's start talking at where we are right now. So this is going to be not technical at all, but so I think this image pretty much summarizes the the situation of the field right now. So we have um a lot of power, we have uh built and designed this powerful And design these powerful models that are capable of processing things that we never thought would be available, but it's still not quite working as we expected because we have no control over there. So I think part of the solution comes from understanding the nomenclature and the definition that we are dealing with here. So we have AI, which is a field that is hard to bound, it kind of overlaps with other fields like statistics. Then within AI, we say Then, within AI, we have machine learning, which is the subfield that uses data to learn. And then, within machine learning, we have deep learning. But that's it. I mean, we cannot further categorize deep learning beyond that. Essentially, because deep learning is not a supervised method, it's not an unsupervised method. The best way to describe deep learning, that's my opinion, is as a representation learning technique. So, essentially, deep learning just gets a piece of data and learns a vocabulary to transform this piece of data into something deep. To transform this piece of data into something different so that this representation will be useful for whatever you're trying to achieve. That's it. So, deep learning is this handle that we can use to change the shape of things so that they look, they have a shape that is more useful for another purpose. And kind of like the really big hammers. So, right now we have models that are getting bigger, not necessarily because they're getting better, because sometimes people make They're getting better because sometimes people make an effort to demonstrate that bigger does not degrade performance, which is a bit supreme. But instead, they're going bigger because it's easier and cooler to show off. And we can actually other kill problems because it's as what was her name? The Google lady. Sarah, yeah. So Sarah said also that we have this, we are overparameterizing. We are over-parametrizing our models because it's easier to. And this is an analogy that I think works here. So you're trying to build a shirt to put on, and it's much easier to make a shirt that is four sizes too big for you, and then kind of try to wrap it around your cells so it doesn't move too much, then just build a shirt that goes straight perfect for your body. So we have this huge regularizing method, a lot of research dedicated to regularizing. Dedicated to polarizing models that they don't overfit because it's easier to actually make a model that tackles the problem we are trying to solve. So the good examples are transformer models, which came kind of related with fully connected networks in the sense that they don't have inductive biases, they don't assume any property or relation in the data. They just explore everything possible in there. Even though you can obviously encode some stuff. And then you have language models which have no purpose at all. So the language models just give me the data and I will try to figure out something to cover. So this is a trend that is being led by some parts of industry, particularly those that are using huge amounts of power, computational power, that cannot be replicated autonomically. And it's being endorsed by too many scientists, I think. By too many scientists, I think that it's facilitating this look is more important. In the end, the contributions and the advances that we are making in this sense are not reliable in the sense that they are not properly thoroughly tested science, so we shouldn't be too confident on anything that we find in the field as long as we are working in this in this manner. So we have the the probably the most Probably the most loud exponents of this trend is OpenAI and DeepMind, which build these huge models. Most of them are general purpose models that they don't have a specific task to solve, though there are some exceptions. These models are usually opaque, they are not probably released, we don't have access to their source code, we don't have access to the data, we have no clue of how they are built, they are business-oriented. We have our We have also, they have no accountability, so we cannot, this is who watches the watcher, so we cannot review whatever has been done with decisions were taken. They don't have reproducibility at all, so we cannot know if what they claim is actually true. How character picked are all those Dally images that they keep showing around. And I don't think we can call this research in the proper sense. In the proper sense. So it's something else, but quite sure what it is. So from bigger and color usually goes together with these general models that have no purpose at all. This is just reading data and data and data because they can. In a sort of race for visibility, I think this is kind of trying to gather the attention of people. But the image shows, for example, language models are so hot because everybody can understand them, everybody can see text that. Understand them. Everybody can see text that is written by me. I can say, hey, I can understand that. I mean, it makes sense. So that's accessible to pretty much everyone, and that makes it a big hype. But it's not quite useful. I mean, language models have not changed that much the state of the art. The applications and the images, they have the intelligence to change the state of the art on the metrics and so on. But the applications are the bots that still suck. So forget this approach is forgetting. This approach is forgetting purpose, it's forget the scope of research, it's forgetting the task you're trying to solve, the advances you're trying to make, and it's just prioritizing general, broad, and undefined purposes. And in this sense, I think the relationship between industry and academia still has to figure out how to interact with one level. We have to figure out how to get a better and more balanced outcome of this news relationship. Okay, so I will now talk about general purpose models a bit more in detail, but try not to be so critic. So this is a video of a guy who tries to put it larger, just filling data to GPT and trying to maximize. It keeps trying to make TPT not be racist, but it's apparently possible. Alright, so the purpose of this example is to show that the bias is deeply within the model. So it's not something that happens arbitrarily sometimes, but it's actually the main representation of Muslims in this model is terrorism, violence, and death, and so on. So well. Okay, so general models are a very powerful thing. They can be used for anything. I mean, they have this task-free definition, which is actually the same limitation that they have. Because since they are task-free, it means we cannot debias them for every possible task because we don't know the task we will be applied to. So it's virtually impossible to debias a general purpose model, as long as you don't know what you are going to be using before. So this is the kind of So, this is the kind of artificial general IG or the idiot. So, when I was thinking which terms, like, so maybe I would run a call for the next talk which I will use. So, bottom line, I think there's no safe release of these models. I think it should be avoided to release these models without the purpose, just for showing up, because it's it's uh dangerous and it doesn't have a good reason for deployment. Don't have a good reason for deploying it, then it's a bad decision. So the thing is, I actually, one of the main research lines is transfer learning. So general models and very big models are great for transfer learning, which is very unfortunate. So this is the principle of transfer learning, which is don't be a hero, always try to reuse what it does. Don't try to create everything from scratch. And it works great when you have a very large Great when you have a very large model train or a very large and a data set that is capable of reading a large representation of things that you can produce and pick and build your model for very touchdown trends. It gets better results, gets faster results, and it's cheaper as well. So you use less data, you use less hardware, you produce less footprint, and you use less human hours, which is also a key component. Because the amount of time you need to tune a model Time you need to tune a model from scratch by a trained engineer is a ridiculous compared to the amount of time you need to do transfer learning. Three parameters and your model works quite well. But if we are going to use models for our transfer learning, we have some requirements. I mean we cannot use models blindly. I mean we need to know which are the training details of these models. We have to know which is the data details, or what what kind of data they will train on, and which devicing efforts they Biasing efforts was made for them. Because these things are going to be affecting our models, and the biases that they have in their models is going to be encoded in ours. And it's our responsibility to understand what we are dealing with, and we are building models and we see. So you cannot really make safe general models. I think that's pretty clear. You can try to make safer general models because, well, still we want general models. Well, still, we want general models because they are important for fundamental research, they are important for transfer learning applications. So, maybe there are ways around it. So, one way possible is, for example, defining a disclaimer and intended uses of models that really list. So, which is the type of data that you should be filling this model, what's the purpose of this model, and also adding warnings on where should I be filling? So, things you shouldn't be doing with your model and adding phase age. So, for example, I think a good idea, something that I'm not sure if it hasn't here before, is trying to build models that prevent an output when something goes beyond what they are expected to be seeing. So instead of just responding, I'm not gonna respond to that because that's going to be harmful. Regardless of my confidence, I'm not Regardless of my confidence, I'm not showing for this distribution of it. And this reminds me of the story of the cat and the microwave. I don't know if you know that microwave instructions manuals have a disclaimer where they say that you cannot draw your cat in your microwave because there was a lady that did so and sued the microwave company. So from that point on, they had to go address disclaimer for technical purposes. I'm not sure if this story is from the other one. I don't Google it. I don't Google it. I believe that I don't Google it because it doesn't work. I mean, the warning at least the other way. Anyway, so it's a long way. Doing this, it takes a lot of effort and it implies sacrifice because we are going to have to sacrifice performance, top line metrics, as Sarah was saying. And one of the ways of doing it is limiting the data that we are feeding it. So we know that deep learning can deal with a lot of data. Learning can deal with a lot of data. That's not the point. We already know that. It's established. It's not necessary to fit it with a lot of data. Let's try to limit the data we fit it with. Let's try to limit the modalities of data we fit it and the distributions of data. Be as narrow as possible to the domain of application that we're looking for. Because if we deviate from that, even though it works, apparently, then so in the test set, it becomes more and more dangerous, a small device. Well, so we have to make an effort to limit the complexity of the models. I think we are overparameterizing by definition models too much. It's so easy to make a huge model and then drop out a much more than everything and it just works. Eventually, if you manage to find the right size for your model instead of for reparameterizing, as it has been said already in this workshop, you obtain better for your models and they are safer because they encode less viruses. Less viruses. They transfer less as less usable, but you can't have everything less. And regarding the release of deep learning models, and particularly of general models, we should go for a policy of either in full release, that's releasing the code, the model, the data it was trained on, and the biasing effort that was done, or not releasing at all. Because this is the only way that we can do science. Do science in the essential meaning of the word and a safe science. And regardless of releasing or not releasing models, always try to include an ethics loop in your assessment of which data you're using and how you work with the value speed and so on. And this don't release models without purpose, without purpose, so it's just for showing off, which is happening more and more this year. Showing off, which is happening more and more these days. We see a lot of people just in social networks using the cherry-picked examples or APIs for uploading your image so that you can do whatever. And this kind of thing is not healthy for slides. One more, sorry, this is a bit of a bummer, the presentation is kind of depressing, but particularly because I so what I was thinking about this when I was on the plane, come here. It was very difficult. Here, it was very bad. So, essentially, this-I don't know if you know this paper, they compare the cost of training transforming model, including all the hyperbarometer search in CO2 emissions, compared to, for example, cars, the whole lifetime of a car, which is roughly four point-something cars. So, it's it's kind of I mean it it's it's it's um and it's not only that, I mean we have And it's not only that. I mean, we have a lot of papers on science that has been trying to achieve ridiculous gains in specific metrics at a huge price, which in this case is money, but this could also translate to silo too, and so on. I mean, the the scale of this thing is getting also the outcome. Okay, I will now talk about bias to change every topic. To change the topic, maybe keep it close to the purpose of the worship. So, I think when I talk about biases, I try to explain to my students that biases are not something bad. Biases is why we are here. Biases is why machine learning works. If there's no bias, there's no learning of anything. There's no pathable, there's no correlation there. So, it's not that we hate bias, it's that some biases are undesirable. Bias are undesirable, that's it, but we need them and we need bias. So, on one hand, we do not want to specify which biases the model should learn, because that's the point of using neural networks and machine learning methods, that we don't have to specifically design which are the features that we should be looking at. But on the other hand, we don't want the model to learn biases that are not the ones we want it to. So, it's kind of a We wanted to. So it's kind of a start. We haven't. It's hard. It's a hard way out of this problem. The only solution we have found is actually interpretable AI, which is kind of a half-a solution. So when you're thinking about your model, thinking is it biased? Yes, it is biased. For sure. Never trust your model. I mean, even when you understand, even when you think you understand. And even when you think you understand the bias of your model, you're probably wrong. And if you were right in the unlikely probability that you are right in understanding the biases of all the biases of the model, which are, I don't know, how many can there be, you might be biased. So even then, it's not safe. So, yeah. The only solution we have to do to this is explainable AI. So we're So it's our way of not defining which biases I want you to learn, but defining biases I don't want you to learn. It's kind of the way we are going right now. But we need to do it all the time, and we have to understand that explainable AI is not magic wand, so we have still to use we need the magic, the the magician itself that is part of the process and we cannot remove from the equation because eventually the the the Because eventually the the one who decides if something is undesirable bias or not is going to be the person. So how can we deal with bias? Let's see. This is an example from the focus, which I'm sure you're familiar with, methodology. So we use it in the mountain data set. We were trying to say, so this is the P stands for polychrome wood. These are images of polychrome. These are images of polychrome wood that are art, and the O stands for OM Campus. So we have here, we are asking for why this mosaic belongs to the polychrome wood class, and the class kept the model kept looking at this image, and in this case, it kept looking at this image. So we showed this to the experts, and the experts told us, well, it's looking at people because at the time where polychrome wood was most prominent art form, Prominent art form the religious figures were the most popular art representation. So you have a lot of people in polychrome art objects. So whenever the model sees a person, something that looks like a person, it thinks it's a polychrom. So the thing is, experts know this, and experts use this for classifying the artwork they have. So the question is, should the So the question is: should the model be using it? That's something that's hard to respond to, at least from one who is not an expert on the domain. In the end, I think the expert on the domain, in collaboration with an ethics expert, should eventually take this decision. And overall, all who use the model should be aware of this bias that is included, and if it's kept in there, especially Yeah, so how can we debias models? So the first thing is, I think it's a three-part effort. So we need domain experts, we need technical experts, and then we need the ethics experts. And all of them together, I mean, it's three people that speak three different languages, so it is challenging to put them together and to come up to an understanding. And maybe Understanding and maybe some work needs to be done for sure. So, first we need to debias at the beginning the data. Here we should debias, this is an idea I got from the first talk, from the Microsoft talk. Yeah, because he said that you shouldn't eliminate the bias, the lightly biases variables at the start, because otherwise you won't be able to see the effect and understand how it works. See the effect and understand how the colour effects model. And I think it's right that unless you find variables that you clearly don't want to use like gender or racial issues or this sort of stuff on your own stuff that has nothing to do like financial assessment or this kind of stuff. So one thing, one limitation here is that even after reviewing the data, we cannot think like a machine, so we cannot anticipate which biases might there be the data, we cannot think that maybe this variable is. Think that maybe this variable is can be used to detect, to use, to create a bias against or for a certain partition of the population, but we cannot really anticipate all the possible, because if we could, we wouldn't be using models anymore. So some biases are going to go through. Then we have to be biased during the development of the model. We have to keep detecting this, particularly in the model selection. I think something that's missing in this community severely. Is missing in this community severely is integrating explainable and bias detection methodologies in the model selection process. So while you're training, while you're deciding which of the two models are key, using these variables also into account. Because right now it's just accuracy and it's not healthy in the end. Because in the end we get up with a model which is really biased. Oh, why is this bias? Maybe because we didn't do anything wrong. Also, Also, detecting bias is fixing and retraining. So we keep purging our data set to be less biased, our model to be more. And finally, of course, finding biases at the end, so no big surprise to us, that means at the end. And even so, we can't hope to catch them all. I mean, eventually some biases will run away, which suggests that we should run also periodic sampling assessment of... Assessment of models that have been deployed in order to validate that no biases have been housed. So, just one last happy slide so that I can lighten the mood. Why we should keep working on this, it's because it's useful, it's fun, and it's unprecedented. So, this is two examples of models creating Pokemon stuff. This one just generates Pokemon based on images of Pokemon, and this one is a bit more. And this one is a bit more weird because there is a model that they use to produce these descriptions of Pokemon based on our description of Pok√©mon, and then a human person interprets the machine definition of a Pokemon and creates the visual description. So it's kind of a creative loop between the human and the machine that is something rather unprecedented and cool. And one of the reasons why I'm working. So that's it. So that's it. Just a summary of the main points. Try to limit the scope of the model. Add disclaimers and warnings. Fail saves. So the model doesn't cause harm. Don't be non-reproducible or show off. Remember the cost every time you submit a job. I know this hurts. And I'm working on a small computing center, so it's something that I have to deal with every day. And when you're writing papers and producing experiments, I don't need to. Papers and producing experiments. I need more experiments because the reviewer is going to complain because I only run one combination and one set. I have to scale everything. Fuck. Never believe in multi-script bias. There's always going to be bias in there. And I do explain a little bit all the time, particularly for motor search. And that's it. References. And the thing, the HBAI team, uh the missing picture here is there. The missing picture here is there. Thank you. I have a comment on the first part, because I also found that the the too much on Twitter. So so my my my thought was that if in a court I would like find this PPT. I would like to find the visibility. I would say most of the data rather than the model. But I want to hear your opinion. Like, if I go to Twitter, I see some stuff and then I want to delete my account. So this is not like the problem of me reading Twitter and being erasing them, but more on data. And it is very interesting to me because I don't think it's doable like 10 years because we have to decide what is good to train and what is not. What is good to train and what is not. We cannot even distinguish fake news from from news. So, but I will say, like my ten second list on the blame, that it's more data, probably. I wouldn't give you a percentage, like 70-30. Yeah, I don't think I mean, blaming data is easy because it comes right back. I don't think there's anyone to blame here. So, somebody built a model with a data that came from somewhere. With a data that came from somewhere, and somebody used it to do something. And nobody took into account the consequences of what was happening. So, the one that released the data, the one that treats the model, and the one that applied the media analysis. So, I think that just there's no one to blame because nobody took responsibility. And I think that's the point. Nobody is taking responsibility for this. And uh the moment we stop doing these things, uh it will stop happening. And there it doesn't need to be uh something to someone to blame. Someone to name yeah, I mean from that that perspective, I'm obviously with you, so I I I will say the same for self-driving cars at some point that but my question was like I really think that data is that on these huge models because we we don't we have to like define an ethics on how to select data for these models and it's it's not I think doable in E. Global in ERS, because yeah just about it. So, like huge models, I'm not defending them, but they work in some way. It's hard because, I mean, even when you are so current, current societies are still racist, but not so much as we used to be. So, I can take responsibility for being racist for the amount of racism. Racist for the amount of racism that I hold now, but not for the one that I hold 20 years ago, that's a different person. So it's yeah, it's the data, but I'm not responsible for that anymore. I'm responsible for the person that's standing here. So we are defining cancer culture for the models now. Yeah, yeah, yeah. Sarah? Yeah, I was wondering if you have like looked um much at some of the restrictions in Dali The restrictions that DALI has put in place for the kind of problems that they will allow people to generate, and if you have any thoughts on that, because it's kind of a safety measure. Yeah, I try not to play too much with these things that I cannot reveal. I mean, what's the point of getting excited about something that I cannot understand if it's fine? Yeah, it works great for some cases that they show. Okay, thank you. Okay, anchor. Like papers and archives that are not peer-reviewed. I mean, what's the relevance of that? That's a crisis of the sector because even peer-reviewed right now are poor quality because of the huge amount of papers that we have. So we get terrible reviewers that have no idea what they are reviewing about. So there's no no clear solution right now. But non police reviewing is clearly not a solution. We already tried that at least magic in the twentieth. Magic twenties and uh fifties. How do you see the the role of uh regulators and all and all this? Because I think also imagining a future in which you know politicians uh legal uh featuring figures and I need to play longer to regulate of this that you percent, I mean you need the scope and to make uh you know actual actions on uh on what is done to guarantee ethics and everything, but still uh I But still, how do you see this? I mean, it's better a movement from the scientists, as you said, to be more aware of those problems and do something more internal before reaching a point of really obligation for your community. So maybe here I only put domain expert, tech expert, and ethics expert. Maybe I'm missing the politician. Maybe the role of the politician is making sure that whatever comes out of here. That whatever comes out of here aligns with whatever society is looking for at that particular moment. Because politicians come and go, so they are just a brief representation of people's will, hopefully, at a certain time. But I'm not too much, I clearly prefer to rely on these three politicians, because these three are are there because of a knowledgeable reasons, the other ones just of a popularity uh contest, which is exactly what I'm uh going against in this three. Going against this thing. So I have a follow-up question about that because I can have some experience and find these people around the table and we sit down and talk. And the problem is that we all, as you were mentioning, we all talk different languages. Like even the same, the simple world of interpretability, it's understood differently. And I really felt a barrier with lawyers and philosophers talking about interpretability because Talking about interpretability because what for us is maths and rules and statistics and something that we can put down in paper and we all clearly see in the same way for them it's completely something different. For them it's something that has to be put in the legal scheme, something that has to be reported to the European Commission that has a particular way of working, right? So they expect things in a certain way. We cannot provide that. I couldn't, but I would. But I wonder how we can actually help and facilitate the communication with these people because they don't have the knowledge that we have and they're trying to make an effort, they're really trying, but it's difficult to share with them the knowledge of all the years of school that we went through to get to this discussion where we now are here. I was wondering if you had some experience. I don't have the answer, but I have experience similar to yours, so it's really frustrating when you're speaking with an expert. When you're speaking with an expert in another field, and you're unable to understand a single sentence of the introduction because you're using words that you have never heard before. So, my idea here is that I think we need to align all roles for a shared purpose. I think when we sit down with an expert, one of the roles has a different purpose, a different goal, and it's trying to pursue their own agenda. And as long as we don't share the same, I mean, as long as we cannot get down from our pedestal and Get down from our pedestal and move our ego aside and say, Okay, I'm gonna talk simple, I'm gonna try to figure out the essential problem here, not the scientific point or not the exact medical terminology, but just what we are trying to achieve here with the three or four or five of us. And once we are aligned with the problem, I would expect things to move more smoothly. Maybe there is the need there in in academy to Academy itself of information for people that is done in a different way. I mean, dealing with machine learning, we all have this imposter syndrome, right? Because we all come from a different field, essentially. Like in machine learning, I met a statistician. I mean, I'm information technology engineer at Bezos, but I don't feel myself like wondering because I haven't been doing real engineering for a long time and I work with statisticians that are completely far away from industry. Are completely far away from engineers, but already within machine learning there is struggle. So I was wondering if maybe we don't have to define a different type of character that actually has the knowledge of the three of them or can understand the three of them well enough. With statisticians, it's always a very good example because it's always a very tension message. Because they feel like we have eaten the space, so they are always trying to no, no, this is the way to do it, and this is what's statistic, this is my p-value, and then that's it, I'm gonna die. I'm going to touch it. I'm going to die. That's good. It's humorous. Because you say that we are increasing carbon footprint. So somebody needs to be there to say, hey, we have done this before. Don't overdo it. It's done. I mean, you have seen that Trevor and ST, I mean, they have done a generalized editing model so long ago. Now it's coming back again. Now it's coming back again in AI context in new forms. So many things were done, really. We are talking in different languages. Being a statistician, I have to. You have also to recognize the advances of the field and the ways it has gone beyond statistics and univariate and multi-variate analysis. And I think we have achieved this. I don't mind giving the discussion and then we finish the minutes later. We are already the minutes and time to catch. We already have minutes to find what they do is something else that we answer again to talk about your question on what they would recognize. I mean, as an alteration, I was what I really liked was a like voluntary along three axes. So perhaps correct. Second was practical, so you would go to the humanitarian people. Whatever you thought you came up with and they coated and how certain this one was ethical and miserable. And it's true that often those three parties will likely be able to do that. That somewhere wasn't a good idea. And yes, it was voluntary because you cannot force it. Voluntarily says we go through this activity. Also, want to try practical peer review group. I can do it. I think the answer is interesting about maybe just a different perspective. I think you really need for starting to discuss. Otherwise you have a PhD student. All the PhD student thinks about this, how to impress their PI while talking to them. That fight while talking to the clinician. Then you have this language coming up, which is so, I mean, we all know it. The best example, I think, is the Rennon Walks with Gross. Just restarting Random Wall.