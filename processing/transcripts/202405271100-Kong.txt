First, I want to apologize. Basically, I'm like a party crusher. This is more like high-dimensional data network, but my talk, well, you can put it at high dimension, but not quite related to network. So I'm going to talk about this double adaptive spatial quantile. Double adaptive spatial quantile regression models for neural image data. This work was done quite early. Actually, this is the only work I did with Hong Tu when I was doing my postdoc, which like 10 years ago, or more than 10 years ago. So first, I would do some introduction, then I will introduce this double adaptive spatial quantile regression framework. Spatial quantile regression framework, and then I will do some simulation studies. Okay, so image data, and usually we need first acquired those image data. We need a scanner. And for example, if we want to get this MRI data, we need people to do the scanning, then get this. The scanning, then get this neuroimage data, then we started to analyze those neuroimage data. And usually, we need quite some process in order for our statisticians to be able to analyze it, like image reconstruction, processing, segmentation, alignment, and so on. And also for For the image data, the challenge lies on we may have multiple subjects and also we could have multiple images from a single subject. For example, for adding data, you may want to have this sort of longitudinal data to see how this Alzheimer's disease gets prognostic. Now, let's now assume everything had been done. The house cleaning work had been done, the registration pre-processing, and so on. Everything had been done. Then we have a clean image data that mathematicians or statisticians we can analyze. So basically, we would have a sort of response. For example, the green matter or white matter intensity, this basically it's a Jacoby matrix and after do all the registration and so on. And this is a 3D image. And for example, here we can have a 64 by 64 by 48 3D volume. But of course, nowadays, the But of course, nowadays the resolution can be much higher, could be like 256 by 256 by 198. Now we have some coverage. Okay, let's say age, gender, diagnostics, and we may also have some genetic variation. So what's our task? What do we want to do? We want to find We want to find these sort of coercive effects. It's always one thing we care about. We want to find how the coverage affect those image. Why we want to do that? The key thing, especially for image data, the final goal is not to just estimate this covariance effects B D in terms. B D in terms of the location D. The key thing is we want to identify meaningful regions of interest. We want to find those regions that may be related to some particular disease. That's the second goal. We want to find those meaningful regions of interest. But then, as a statistician, find those regions. And find those regions. It's not the end of our work. We also want to see: okay, the region we find actually is really the region significantly related to this disease. So we want to quantify those effects. We want to see, okay, those coherency effects are actually significant. They are not. And they are not just found by charts. So that's our goal, our three goals. Now, let's first review typically how or traditionally how this would be done. The first kind of method would be, okay, a volunteer method. Let's ignore the sort of spatial correlation within the brain. Using the brain. We just treat each voxel independently. Then we have this voxel-wise method. So even D can be very large. The total number of D in those regions can be tens of thousands. And we just run the individual regression. And very simple, very fast. But of course, the drawback is very obvious. We ignore the spatial correlation. We ignore the spatial correlation and we ignore the structure, we ignore the variability of this empty data. And that's not what we want. But it's actually a very good step. Now, in order to respect the data, respect the spatial structure, we can actually We can actually consider those kinds of correlation structures. So, we have this spatial modeling method. We explicitly model the spatial correlation. For example, we can use conditional, auto-regressive, and macro random field. This is quite a popular method to model this spatial structure. structure. But for this kind of method, the computation is very intense. The computation was very intense. And this project was done, like I said, 10 years ago. And recently we are doing the revision and we rerun the previous, so you can see we actually downsize the We actually downsize the image to 64 by 64 by 48, previously 256, 256 by 190. And then even in our computational power, we rerun the real data analysis. It's still quite slow. The computation is still very challenging. So another method is this propagation separation method. separation method. This method can adapt really and spatially small things. Just small things of image. They might be the key underlying assumption is the image actually is piecewise constant or piecewise moved because we have certain region of interest. And supposedly within that region of interest, the response should be similar. But for this kind of method, it still ignores the variability of imaging data, imaging data. Now, that's some traditional method. When we start this project, we feel like, okay, we want develop some method which are more powerful. Then for image study, if you want. And for image study, if you want to develop those kinds of methods, you have to have a better understanding of the whole process. And so we try to go over the whole process again, which including typically, like I said, we have the image acquisition and image processing, and then we can do data analysis and interpretation. Interpretation and the left side is what we want to do, and the right side is what we actually are doing in order to achieve the goal. For example, in order to do the image acquisition, we need to use some mathematical model like signal model and trying to respect the noise sources. For the image processing part, we need to do a representation. To do a representation and segmentation registration and so on. In order to do this analysis and interpretation, we need to develop the statistical modeling and then do the inference. That's the whole process. So in order to make this project successful, we actually go through the whole We actually go through the whole process. The data we are using is ADHD data. And nowadays online, there are many process ADHD. But for our project, we actually go through the whole process. We did this segmentation, registration, and the whole thing ourselves. One of the reasons. One of the reasons to do that is, you know, for this various image segmentation and restriction, when we do the processing, pre-processing, we may result in some non-consistency. A lot of, if you read this neuroimage papers, a lot of time they would have this A lot of times they would have this sort of in-house processing, or sometimes you get it online, and people process the data arrive. And if you run the same data set, a different pre-processing may have different results. So we may have this non-consistency with error. And also, a lot of times during the process, we would assume, okay, the error is Gaussian. It's Gaussian for communities. We all know Gaussian is easy to do. We have a lot of tools available to deal with Gaussian. But in image data, really is Gaussian. It's debatable. Another thing is within the brain, the variation of the error actually varying spatially within the brain. So we need to account those spatial correlation spatial correlation. Consider all these things we want to try to overcome. We actually propose to use quantile regression. And quantile regression, we call it the cousin of the ordinary least square. So, why quantile regression? Quantile regression can give a full picture of the data, not only in the center, the mean situation, because we know in Gaussian, and if you just study the mean, then quite enough already. But we don't know whether the underlying error is Gaussian or not. Actually, a lot of time it's not. Then a quantum regression. The quantum regression is able to give a more complete structure about how the response change with the coverage. Another thing is we actually propose sort of a composite quantile regression. Through this composite quantile regression, we can learn this structure, can estimate the currencies back more efficiently. Currencies back more efficient. Therefore, we propose this double adaptive spatial quantile regression model. And we update the estimate by borrowing information from nearby voxels. Because nearby voxels are sort of correlated. They have this spatial structure. And meanwhile, we borrow information from We borrow information from different quantile levels, different quantile levels. Okay, and more specifically, what we have here is for this voxel-wise quantile regression model, we are trying to model the conditional quantile given some covariates. We assume it follows a certain structure here. Certain structure here. We have this structure mu xi beta d. beta d depends on the location and also depends on the quantile level. And this q tau, y d given x i satisfy the torque conditional quantile. And this d is in the in a 3d space, 3d space. For example, it could be 32 by It could be 32 by 32 by 6 with 3D space. And this mu is a small function in both tor and d. And here, maybe it's more precise to say, okay, it's piecewise. Now, for the quantile loss, we would have this. We want to minimize this. we want to minimize this and the difference between the response and toss quong tau toss conditional quong tau and this is the check function is the check function and in order to do this there are a lot of algorithms available we can use linear programming interior point simplex method and so on and there are also a lot of packages available for example in Available, for example, in SaaS and RMLab, and you can use those software to do this. But this is work-wise. So for our DHQRM, the object function will be written in this form. Here, we actually get two widths: this WDD prime i. this W D D prime H and this W tau torque B torque B and the rest would be the same and this torque is in a pre-specified quantile level set and this D prime in a bar and related to this center D related to this center D so basically this is two weight sort of you can call it weighted Of you can call it weighted composite quantile or weighted quantile, and the weight is coming from two parts: one is from location. We want to borrow information from nearby vauxhall, and one is from a different quantile level. We want to borrow information from a different quantile level, different quantile level. Now, how do we borrow information? How do we borrow information? do we borrow information and to borrow information smoothing is one simple way so for this spatially in term of voxels to borrow the information they're coming two parts coming to coming from two parts From two parts. It's two kernels. One is related to the location and how close is D to D prime. The closer, we borrow more information. The other one is this sort of similarity kernel. Okay, if two voxels are close, but if the estimate quite different, then we should borrow less information. If the estimate are close, very close. Are close, very close, then that means we should borrow more information, right? We should borrow more information. So that's how these two kernels plays in determining the weight, determining the weight, location and estimate, location and estimate. The closer, we borrow more information in terms of location. The closer, the estimate, we borrow more information. I think I didn't write it here. Basically, we actually use R2 distance of the two asthma. Like the estimate at like estimate at D and estimate at D prime, bet D and bit D prime it's a vector, then we look at the R2 distance yeah yeah and adjust by of course adjusted by its variance Yeah, I will specify the whole process. Basically, at the beginning, we use Volkswagen method. Volkswagen method is not efficient, but it's still a consistent estimate already. So we start that and then we do propagation, then we update. So this kernel is based on the dissimilarity. Based on the similarity measure, this similarity measure, just asked, we actually look at this. There are many ways to do it, but we simply use this sort of R2 distance, adjusted R2 distance, adjusted by this variance. Okay, then the objective function, initially, we have this. Initially, we have this three summation, and we need weighted by two weights. And it actually can be rewritten in this format. And here we assume those weight summation equal to one, then we only need to adjust this torso star. This torso can be rewritten as this way. This way. This way, and we can replace this y as a weighted variant and this feature also by its weighted variant. So, in this way, when we do coding, it's easier. Now, in this sort of double adaptive estimation, we have three key steps. The first step The first step is this sort of initialization. To do the initialization, we just do it volunteer. Volcano-wise, it's not efficient, but it's consistent already. So we start with the volcanoes method. And then from there, we can update our weight. We can calculate the weight. And then we do weight reverse. Um, then we do wage world. Okay, we do wage world. And for the next step, we will update our kernel, decide how many voxels we are going to include to participate this smoothie, participate, participate, smoothie. And also, because we have initial estimate. Because we have an initial estimate, we can estimate the weight, then we can get an estimate, updated estimate. And then from this updated estimate, we can do it again to update or to do the sort of propagation separation again. We can include more voxel, more voxel. And of course, we have a sort of a stopping checking, stopping checking. You don't want to. Stopping checking. You don't want to do over smoothing, over smooth. So here is this sort of propagation separation, propagation separation. Initially, we have this voxel-wise, and we estimate at each voxel. Now, in the next step, we have a radius, H1. We decide to include nearby voxels. The nearby vauxhole, and then we have the crease funding weight. We decide how much information we want to borrow from nearby vauxhall. And then we update our estimate. Then we can increase our radius. We decide to do further smoothing, borrow more information from larger neighborhood. And then we update our weight again. And then we update our weight again until we stop. So this is the propagation separation. So then we have this sort of stopping rule. And when we stop, if the updated estimate is too far away from the initial estimate because the initial estimate Because the initial estimate is already consistent. We do this sort of double adaptive smoothing. The reason is we want to respect the spatial structure. We want to consider the correlation. And by considering the correlation, by doing this weighted smoothing, we actually can reduce the variation of. The variation of our estimate. Therefore, we can increase our detection power to detect this region of interest. So, this is the stopping rule. We have this constant C is the distance is the dissimilarity is two far away greater than this C then. Greater than the C, then we will stop. We will stop. And why we choose this constant C, that actually have a theoretical guarantee so that under certain conditions, and we can choose this C in order to recover this sort of piecewise small thing or piecewise constant structure. Okay, and then this is a Okay, and then this is the whole procedure. And we have the three key star initial estimate, individual, the two individual vauxhall and individual quantile level. And then, once we get the initial estimate, we want to borrow information from nearby vauxhall and also through this. Through this, we can also borrow information from nearby quantile level, nearby quantity. And then we update our estimate through this propagation separation procedure, then we start. Now, what kind of theoretical guarantee would we have for this procedure? Of course, the first one is the the first one is the sort of consistency and asymptotic normality. So the first theorem is under certain condition, our estimate will converge to this two estimate in probability. And also we would have this asymptotic normal asymptotic normal Asymptotic normal property asymptotic normal now there's no free lunch, right? We we did the smoothie and of course we would we would be introduced by us one way or the other, right? And in order to study that, we first look at how this Look at how these dissimilarity measures look like, dissimilarity looks like. Under certain conditions, this location dissimilarity can be approximated by this eventually, and this dissimilarity between functal levels will eventually be approximated by this. And based on that, we would have this would have this kernel, how this kernel behave, how this kernel behave, and why it's related to voxel and why it's related to this functional level. And if you look at the current, you can see the language. If you look at those things closely, and you will find essentially this part, the first part is whether they are same or not. And then it's also related to both smoothing. This is a location smoothing, smoothing part. And also we have a read. Also, we have a read log capital N. Now, we also did some small simulations. The model here we consider is kind of spatial location scale model. We have a We have a mean structure here. This YID equals XI transpose BD. This is the mean structure. And in order to show the advantage of quantile regression, we actually consider this a skill part, this skill. And this D is in this 3D space. And those features including three paths. Three paths. The first one is just intercept, second one is follow binomial. And then it's basically we are trying to mimic this category variable. And this third one is uniform. And this is continuous variable. And the beta i is in 0, 0.2, 0.6. 0, 0.2, 0.6, 0.4, 0.6, 0.8. This is just to mimic the strength of the signal at different level. And we have gamma 1D like this, gamma 2d and gamma 3d. And this ethn d either follow normal. And this is a regular case. Usually in least square, we would assume this area follow this one. Follow this one and all this p-distribution. This is a situation for your botany. I don't know how to make it and then under those conditions, the conditional The conditional quantile can be written in this format. This is the key structure, and this part is coming from this sort of spatial correlation, this scale part. And essentially, it can be written in this format, in this format. Now, let's see how it looks like. We have two situations. One is the taught equals 0.5. The tau equal 0.5, which is the media situation on the left, and the right is tau equal 0.3. And we have three rows. First row is beta 1D, the incept. Second row is bet 2D. Third row is base 3D. And for each panel, we have three. No, this is beta one, beta two, b3. Beta one, B2, B3, Beta 1, B2, B3. And this is the initial estimate. This is our estimate. This is just Volkswagen. And as you can see here, initially, we, especially for this part, we will miss a lot of the pattern, miss a lot of pattern. Well, after we use After we use double adaptive smoothing method, we can reconstruct the pattern quite well. Quite well. The left one is midian is the easy to do. And this one is taught equal to 0.3. And it's a little bit to the lower part. And that's the information available. As you can see here. Available. As you can see here, we barely see any pattern. However, through our adaptive estimate, we can actually recover those structure quite well. And in our revision, actually, while the reviewer asked, okay, what if we just smooth this? Uh we are we also did use the simulation. If we just smooth this, of course, we can sort of get better result, but it's still not comparable to this one, comparable to this one. Because in our method, when we do the smoothing, we actually consider different locations and also different quantile levels. And we use more advanced propagation separation method. So our result is. So, our result is much better. And not only in detect those patterns, but also in further when we do hypothesis tests, right? Because remember, the essential goal is we want to claim, okay, this pattern actually, not only we have this kind of pattern, but we can actually claim this pattern is significant. It's really a pattern. It's independent, it's pre-specified, pre-specified. And the key thing here is it's a very good question. We actually also get similar comments, review comments on this ask us to comment on why you do this pre-specified. We actually did either Did either including, let's say, in one direction, five to ten would be good enough. And why is because actually within those regions, the signal is quite strong. And some very simple things that you do will largely increase the power. You would be able to detect those patterns. Those patterns. And this is, we look at this root mean squared error, this standard deviation, and this RE, which we define this RMS divided by S D. Those patterns are not clear as this one, and also in terms of those kind of measures, like root mean square error and standard deviation, and I basic basically the two things. Basically, there's two things. One is in terms of recovering the pattern, that won't be able to clearly recover those things. That's one thing. The second thing, the variation can be still large, can be still large because we want to have a good variation estimate in order to do the next hypothesis test. Okay, this is another measure to quantify our estimate. And here is the signal region we detected. We detect this is tor equal 0.5 and this is this side is tor equal 0.3. Tau equal 0.3 and we have this initial one and we have this adapted one. We plot this mass log 10p mass log 10p. And as you can see, we actually be able to detect more, correctly detect more regions than this initial one, than this initial. Of course, like I said, in this revision, they asked us just to do this simple smoothing to compare with our method. And it's certainly better than this one. You do a simple smoothing, but not comparable to this, to our method. Okay, here is some numerical method. a numerical method a numerical result i'm not going to uh i'm not going to show it and uh um so uh this is uh um the um the first simulation and the first simulation um it kind of uh okay we have this uh uh two two situations the variance one is normal the other one is uh The other one is T. But this only represents one issue in this neural images study. That's under the assumption, okay, everything is perfect process. And those errors only coming from the image itself. But what if in our processing, we have some maybe even slightly inconsistent result? For example, when you do the race. Result, for example, when you do the registration, when you do the alignment, and you cannot guarantee they are aligned quite well, right? And we all know, okay, and there are actually no underlying truths at all, right? Your image or your brain and my image and brain, how you are going to learn them quite well. So in our second simulation, we add the model. The model is the same. However, for the response of the structure, we have 20% random select subject shift of one, two, two, three volts. They're basically trying to mimic, okay, in the alignment. And there may be some distortion and how it works, how it works. And so here is the result. Here is the result. And everything is a little bit from the initial part is a little bit worse than without this distortion. But still, our method can largely improve those, largely improve those. I'm not going to go to details. And yeah. The third situation is the model is the same as simulation one. However, we have a 20% random select subject. They are the response multiplied by 10. This is mimic to this to the To the series, distribute underlying distribution. And also, when you do the normalization, then maybe for various reasons, we didn't do well, then how it looks like. And here is the result. And again, both initial and adaptive estimate be worse than perfect simulation. Than perfect stipulation, but still our method can largely recover the response. I think I will stop here, and thank you. Yeah, yeah, I did, but that's like a top level, but I think it's a little bit more like a message. In my framework, the control level we only choose like five and a half. And also, they are quite different. So, and they are at least 0.1 different. And also, because it's doing the smoothing, right? And we actually didn't find the crossing. Yeah. Yeah, you know, simulation started. But in reality, I understand that could be the the the the crossing. The crossing. But we actually, you know, if we have this crossing situation, I probably would use this Victor's method, just reorder. Yeah. Yeah. Yeah. Yeah um that's a good question and we didn't think about this um but and the key thing and of course if we have better estimate then everything will be better. Then everything will be better. But the kitchen at the beginning, we only need a consistent essence, and then we iterate to that's one of the advantages we do quantile, right? So we can still get a We can still get a sort of a consistent one, but may not be as good as no contamination, no this shift. Yeah. Especially you only get 20% and then you add some median then. Yeah, that's also why I only show 0.5, 0.3. In that situation, if you go to very tail, then it can be. Kill, then it can be your problem. Yeah, that's a good question. So basically, this whole process is very complicated. And if we add penalty, then it can be very practical. Maybe you can do this. But But in terms of to look at the theoretical properties, basically we are quite overwhelmed already. Yeah, but it can be. And also another thing is when we do the propagation separation during this small thing, it's kind of equivalent to this penalty one way or the other. But that I think this is the beginning of the topic that the office general, you know how to take what be the par square, what be the Part square would be the, you know, when I try to use gender, I need to use genetic predict the, you know, the box. So, how easy it is, like a how good is like where you predict the box. This gives me information. Do you have a do you have a idea about the I already know this area very well, but I'm just curious, like, you can general people to computer. My understanding is in practice, we identify certain regions, right? We are able to, the key thing is identify certain regions. Then we talk with expertise in this field to see whether the region we identified. The region we identified will make sense or not. And that's how we verify whether we did a good job or not, because it's really the area depend. And if the and sometimes we, even for ourselves, we are able to find the literature, okay, the region of interest we identify actually consistent with the literature. And sometimes if we find something new, we may talk with experts. They may talk with experts and see whether it makes or not. But I thought, I don't know. That's usually how, at least for me, I do it. It's ADATD data, Alzheimer's disease. Yeah. It's a uh it's it's a no, it they have a health control. Yeah, um health control and uh MCI and AD, Alzheimer's disease. Yeah. We actually didn't explicitly consider the correlations. And it's it's a bit challenging in quantity. Be challenging in quantity. Yeah, yeah. Yeah, yeah, kernel, but kernel function can capture some of the dependent structure. But theoretically, we can't say, okay, we did something to, we can just vaguely say, okay, we use kernel, so we can capture some. We use kernel so we can capture some spatial. Yeah, yeah. Kernel function, basically just the regular kernel function would be, would work. Like for example, you can use Gaussian kernel. Like, for example, you can use Gaussian kernel. Yeah. That's also a good question. We just very ad hoc choose some. Like we can vary a little bit because the whole computation is quite computationally intense. For the simulation, we actually need to let's we at start. Um, we I started project 10 years ago. We rerun the simulation in between like five years ago, and it takes like several a couple weeks to run the whole simulation. So we actually only choose like for some other parameter, we only choose like three to five candidates, and then we use cross-gradation to choose some. Some good candidates come by, and we unfortunately cannot afford to do too much on that. Yeah. Yeah, thank you. Thank you.