Thanks so much, Diana. There's just a comment question in the chat from Leonid, which you can consider. But I think in the interest of time, let's move on to our next speaker, who is Itzik Per. I hope I'm saying your name. Please correct me if I'm saying your name. All good. All good. And yeah, we can. And yeah, we can see your screen. So it looks good. Please take it away. Awesome. So, thanks for inviting me and giving me the opportunity to speak. I wish I would have been able to actually be there in Oahu. I want to tell you a bit about the work that we've been doing in the lab over the last couple of years on building probabilistic models for modeling the dynamics of microbial communities. The dynamics of microbial communities. And the picture I want you to have in your mind is of taking samples of each patient in this case at multiple time points. This is from now a 10-year-old data set of stem cell transplantation therapy, where this particular patient was sampled at multiple. Patient was sampled at multiple time points, and you can see a few days after the transplant and following some antibiotics treatment that this patient was on, there was ventromycin-resistant enterococcus taking over the community. So each bar, colored bar, represents the composition. And this kind of data suffers from many challenges that we may be familiar with from other contexts. So, data are noisy, so you need to think of them probabilistically. You want to take advantage of the table nature of the data, and are often limited by the fact that you're only sampling reads, so you only have composition. Composition that's relative to the overall abundance of the bacteria in the assay. Sometimes the data is sparse and when you're relying on stool samples, you're getting some kind of a mix of the different very different communities all along the gut. So there is a special spatial component that Spatial component that you're often ignoring. So the first tool I want to talk about is Luminate that tries to take the counts of reads that we observe and smooth them out because even if the ground truth data here, so along the time. So, along the time axis, you have bar charts that describe the composition of OPUs. Even if that's really smooth here, describing some kind of convergence to an equilibrium state, what you will observe is very noisy because you're sampling reads and there are all kinds of other effects. There are all kinds of other effects in the data. Ideally, we'd be able to use this tool to smooth out the observed data points for each taxon and get back some output that resembles the ground truth. So, how should we do that? Well, the probabilistic model that probabilistic model that Luminate uses is a vector across OTUs that represents the composition and we use a Gaussian process between different time points. So the vector state at each time point is normally distributed around its state at the previous time point. At the previous time point, and the covariance matrix is scaled by the elapsed time between these two time points. Now, we use Gaussian processes because they're very convenient computationally, but when you think about that, bacterial growth processes are exponential in nature, and the Gaussian process adds an additive perturbation. So, this is not what you Additive perturbation. So, this is not what you really want. You want the additive perturbation to occur in log space. So, we exponentiate each element of this vector at each time point. And on top of that, model a technical noise that adds on another normally distributed variable that actually describes composition. So, these are the quantitative part of the model. Of the model, we realize that it's not enough because there's also a qualitative part that is needed in order to describe basically zeros, actual biological zeros in the data because of different OTUs becoming eliminated or being introduced at particular time points. It just didn't exist. So the log space really has a hard time dealing with them. Dealing with them. This means we have, in parallel to this quantitative process, we also have a qualitative binary process that has binary vectors that describe the presence or absence of each OTU and allow modeling of appearance or elimination of particular OTUs. When we combine these models, we get Combine these models, we get a theoretical compositional vector, and the observed counts are just multilingually drawn with these probabilities. Now, what's nice about this model is that it lends itself to approximate procedure inference, which allows an iterative inference of the parameters of the model in a In a relatively rapid fashion, and we'll see how that helps us in a minute. How well does that work? So we took a couple of data sets, one that introduces C. diff infection and another that's a temporal change of diet. We added noise at different levels to these ground truths. Levels to these ground truth data sets and try to reconstruct the original data and compare ourselves to other methods that are trying to do sort of the same thing in Mallard and TGP Coda, as well as to a straw man of just a Dir Schlet distribution multinomial that's maybe unsurprisingly performs worse. Higher is better. worse a higher is better a r squared of one is ideal and luminate is performing better and usually significantly better from a compared to a the other the other options a now doing a bit better in terms of accuracy may or may not matter to you what would definitely matter is whether you can run the analysis at all and here's where the ability to The ability to use variational inference helps us a lot. So, if you're looking at multiple time points, suppose you want to follow patients with daily samples across a month or a year, and you want to do that not just across the top five taxa in the data, but rather include top 10. Broadly include top 10, top 25, top 50. The models that are based on sampling, for instance, using a Monte Carlo Markov chain, really scale exponentially with the number of parameters. So when you're looking across large data sets, multiple time points, and multiple species, so multiple taxa, you're You're getting into invisibility territory, so very long computational time spent on each sample. And obviously, if you have large data sets of hundreds or thousands of samples in a healthcare system, this would be problematic. Whereas the variational inference principle allows us to Principle allows us to be very effective even when we scale to hundreds of time points and several dozens at least of OTUs. So this is Luminate and that deals with the noise and temporal aspect of such data sets. The next challenge we tackle The next challenge we tackled is the fact that we don't observe absolute abundances of the different bacteria, but only relative ones. I think this is well explained by a couple of other papers in the field. Even if you think of a cartoon example where you only care about the three different phyla and choose. And change only one of them here, bacteroidis, between community A in red and community B in blue, what you would observe is change in all percentages because the data that you measure is just a is just relative and have to sum up to one. It's actually not three-dimensional, but rather collapsed into two-dimensional under the constraint of all percentages sum up to. All percentages, some up to 200%. Unfortunately, some standard ecological models do not take that into account, but rather try to model the absolute abundance. And I'm talking specifically about the generalized logical Volterra equations. These equations consider the change d to dt of each abundant. Of each absolute abundance at time t of toxin I, and observe that it is, first of all, proportional to how much of this toxin you actually have, but then the change depends also multiplicatively on the growth rate of that particular toxon as well as interaction terms. So, the way any other taxon J affects. J affects our favorite taxon I, as well as any other perturbation, which is very useful when you're considering treatments that may be affecting this taxonide. So this is nice in general, but requires us to actually know the absolute abundance. We try to figure out how to change. To change this kind of equation to fit the most common situation where we only have absolute abundances. And the nice thing about this is that when you divide absolute abundances, you get the same thing as when you divide relative abundances. And the resulting equation reminds the original one, but you have this. Original one, but you have this nuisance parameter, which is the overall total abundance of the entire bacterial community at time t. We are trying to approximate this as a constant, and that's a natural scale for this equation, giving an equation that's very similar to the original one. To the original one. But it's wrong. It's only approximate. And how bad this approximation is, well, that depends on how variable this total community size actually is. We checked how bad making this approximation would be in practice when inferring the parameters of this generalized This generalized computational Lord Covaltera model. Again, using this data set that I mentioned earlier, there is some variance around the average total abundance of the bacteria in this data, in this data set, as well as several others that we explored. That we explored, the investigators actually took the time and effort to measure the absolute abundance of the different taxa using quantitative PCR. So we actually have groundproof answers. We realize that with this kind of level of variability, we're actually A variability: we're actually able to observe the growth rate as well as the effects of interaction terms and perturbations with reasonably good accuracy. In other data sets, when there is more variation, at least the growth rate suffers significantly. We have much more noise in terms of Noise in terms of inferring the actual parameter. But when you think about that, you may or may not care about the formal parameters of the model. What you might want your model to do is really to predict missing data points and the next data points. So we try to evaluate our ability to do that and realize that we're actually doing quite well. Quite well and significantly better than other simplifications of the model that have been proposed by the additive log ratio transformation or by just using the general Lotka-Volterra model on the relative proportions. Curiously enough, we may be doing Enough, we may be doing better than the generalized Alca-Volterra model on the actual observed absolute abundances, likely because quantitative PCR has its issues and it has sufficient inaccuracies for the equations to suggest. Equations to suggest that that may not be the right way to measure the data. And we're able to do that across different data sets that have ground cruise information. I have still a few minutes, so I would also want to talk about the ability to deal with this. The ability to deal with this with the sparsity of information where we want to infer the dynamics between time points that are actually measured. And here we're switching contexts from 16S sequencing that the data sets so far have been using to metagenomics and leveraging the principle of speak to. The principle of speak to trough ratio. So, work by my now Columbia colleague Talk Rem a few years back observed that when you're looking at a metagenomics reads across a genome of a growing bacterium, you have many more reads near the replication origin compared to Compared to a trough near the replication terminus, because you're catching the replicating DNA in the process and you just have more of it to observe. And the theoretic formulation is that the peak to trough ratio is exponentially related to the replication timing and To the replication timing and generation time. So we realized that the original implementations of this model were not making optimal use of the data and in particular required knowing the genome. The genome and the replication origin and terminus. We developed a system that does not necessarily require that. So if we want to compute the peak to cross ratio, when we do have a reference, we can fit this curve to the To the known peak in trough, but do that in a maximum likelihood framework that does a bit better. But even if we don't know the genome, we just have an observed contig, it turns out that just sorting region, sorting bins of the contigs based on the abundance across. On the abundance across multiple samples, gives you sufficient signal in order to estimate slope that approximates the picture trough. And we show that using this framework, we're able to obtain the growth rate with little Growth rate with a little error with as few as 5,000 reads per genome. We apply that to an inflatory bowel disease data set where multiple samples had been sampled across multiple times. This was part of a larger experiment that include include larger experiment that include included more than just the microbiome data points and we're able to observe that while we didn't find any of the particular features or the particular disease or age or sex to explain a lot of the variability between individuals. There is significant variability between individuals. So that's between individuals. So that's something that affects the picture trough profile or the growth rate profile of different communities significantly. This is one example of a particular OTU where regardless of the Regardless of the disease status, there is very high variability between individuals, whereas within individual across the different time points, the growth rates are relatively consistent. So, the last thing I want to mention, and I'm going to just highlight it without going into details, is Without going into details, is a work on modeling the spatial structure of microbial communities along the gut. So this is using data from the Harris Wang lab that developed MAPSeq that basically shreds a gut to a mouse gut to small pieces, barcodes that, and therefore gets with one. For gets with one shot the composition of not a single microbiome for the gut, but the microbiome composition for each and every one of these pieces. Now, the unfortunate side effect of this paradigm is that you're losing the origin of the different pieces. You don't know where. Pieces. You don't know which piece is which barcode might be coming from which part of the gut. So we try to reconstruct this. We basically build a Gaussian process model along the gut and try to fit the observed communities to this particular axis. We experimented Experimented with this single-axis model that goes along this hypothetical gut, as well as a two-dimensional model, because the communities may be coming from different places, not just along the gut, but also there is a radial access of being further inside or further. Further inside or outside, depending on the location of the particular shred with respect to the different cavities along the gut. And so what we observed is a distinct linear structure along the ilium where Where communities come from different discrete states along that axis. But the distal colon actually has a very distinct signal that's two-dimensional that also includes this radial axis. Radial access. I want to highlight and thank Tyler Joseph, a recent graduate, recently graduated, a PhD student from the lab now at Regeneron, as well as Philip Schlensky, current PhD student in the lab, who've done most of this work, along with multiple other students. Let me also point out Ami. Let me also point out Ami Professor Carr, a wonderful undergraduate now at Princeton. Of course, lots of thanks to your collaborators at the Quram Lab and the Mind Lab. And with that, in the interest of time, I'll thank you for your attention and take any questions. Thanks for the. Thanks for the really interesting talk, Itzik and Diana as well. So we're at the break, but that means that we can, there's time for questions for Itzik or for Diana as well.