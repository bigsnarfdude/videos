And yeah, thank you very much for having me. My name is Regina, and I'm a Flatiron Research Fellow at the CCM. And today I'm presenting some work that I've done jointly with Alex Barnett on a numerical solver for oscillatory ODEs. So the problem. So, the problem we set out to solve is defined here in equation one, is just a general linear second-order differential equation with some initial conditions. So it's an initial value problem. We assume that omega squared is non-negative and real-valued, and then gamma of t is also real-valued. And of course, the difficulty in the numerical solution of this arises when Solution of this arises when omega, the frequency omega is very large. Because conventional time-stepping ODE solvers, when solving this, will need to lay down kind of several time steps per oscillation period in solving this. And so their number of steps and runtime will scale with omega. There have been a couple of efficient solvers developed for the high frequency. For the high frequency case, which I'm listing here. But there are some issues that remain. So, first and foremost, is that most of these methods really only deal with the high frequency case, even though the other extreme when U is slowly changing can occur within the same problem as highly oscillatory U. Another one is that a lot of Another one is that a lot of the methods focus on the case where gamma of t, the gamping term, isn't there. So it's just zero. And naively, this shouldn't be a problem because there is always a variable transformation that transforms away the gamma of t, but it may not be numerically stable to do this transformation. So it is of interest to have a solver which can deal with gamma not equal zero. And lastly, Jim. And lastly, Jim Bremer's work is the only method that is arbitrarily high order. All the other ones are fairly low order. So, if the user wants the solution to more than six digits of accuracy, then they're inefficient, which is just not sufficient for some applications. So, in this work, we aim to create a method that is arbitrarily high order, can deal with a non-zero damping term. Deal with a non-zero gap damping term and is applicable in both the highly oscillatory and the slowly varying phases of the solution. So, the general idea is that we need two different methods to tackle this problem, one that is valid in the slowly varying case and one that is valid in the highly surgery case. And then we need to somehow couple these and switch between them automatically depending on the behavior of the solution and adapt the step size to fit to some. The step size to fit to some sort of user-specified tolerance, which can be really low. For the slowly varying phase of the solution, we use a spectral method based on Chebyshev notes. And for the slightly more interesting, highly oscillatory case, we opted for an asymptotic expansion, but not for the solution, rather the phase of the solution, which is a slowly varying quantity. Is a slowly varying quantity in the highly oscillatory regime of the actual solution. So we get to this phase function via a series of transformations. We first do u equals e to the z, u was the dependent variable, and then z prime will be our phase function. It obeys this ODE, the Riccati equation in equation two. But the issue is that the solutions, the vast majority of solutions of the Riccati equation are also. Riccati equation are also oscillatory. Luckily for us, Heitman, Bremer, and Rocklin have shown that there exist non-acidory phase functions for an analytic omega. Their phase function, by the way, is a little bit different. It is called alpha and it's defined like this, which is just the easiest way of writing down an amplitude that changes with time and a phase that changes. That changes with time and the phase that changes with time. And then they proceed by solving for alpha by solving a non-linear third-order ODE that it satisfies with the appropriate initial conditions. And again, these initial conditions that yield a non-estability alphas are hard to find. So in this work, we don't do that. We don't make any references to initial conditions and we create an iteration, some sort of Some sort of like a correction scheme for x that is by construction non-asymmetry. And this is how that happens. So our zeroth order approximation for x is just going to be plus or minus i omega. There are two solutions here because they're linearly independent because it's a second order ODE. And because it is linear, we can just linearly combine them to fit any sort of initial conditions. Of initial conditions. Then let us define the residual of the Riccati equation like this, which is just the right-hand side that it would give with an approximate x instead of zero. And for the kth iteration over the non-escalatory phase function x, we define a correction function delta and write down what the residual of x was plus delta. What the residual of x plus delta would be, and we get this equation 4, of which the square term we're going to neglect, assuming the correction is small. So we seek a function delta that gives us zero residual or reduces the residual. But we again got an OD that delta obeys, which surprise is again oscillatory. So So instead of solving it exactly, we note that if the phase function is indeed non-osyletry, then the first term and the third term are order omega larger than the delta prime. So we can neglect the delta prime if omega is large. So we do this, and then we get an algebraic equation. So that lets us derive this iteration five, which Which is going to be valid over a range A to B, and the selection of which I'm going to talk about later. This will essentially be the step size of the method. And we can perform a sanity check and write down the zeroth and the first iteration over x, compute this residual, and convince ourselves that indeed there is an on order omega reduction in the residual. Reduction in the residual. In practice, this will contain numerical differentiation, which we do based on the Chebyshev grid again. So as I've shown you, there's an order omega reduction in the first iteration, but this continues. I've also said that this was an asymptotic expansion, which I'm demonstrating here. Which I'm demonstrating here. So, initially, so on the left-hand side, I'm showing in black an analytic solution of an ODE over some range A to B. And then in orange, I'm showing these iterations in X, or well, in the actual solution Y. And you see that it gets better, but only up till a point, and then it displays this asymptotic behavior. And more rigorously on the right, I'm And more rigorously on the right, I'm plotting the maximum of the residual of the kth iteration of x, the phase function. And the maximum is taken over a time step, so that interval a to b. And this is plotted as a function of k, the number of iterations. And among these curves, I am varying omega, essentially, omega at the start of the interval. Of the interval, and we observe this order omega reduction for the first couple of iterations, or more rigorously, a temporary geometric convergence. And then there is this optimum kind of truncation point of an asymptotic series is reached, and then the residual starts to blow up again. Yep. So these, the two methods, this Riccardi, I'm going to talk. I'm just going to call it Riccardi method and the Chebyshev spectral are coupled in the more general algorithm, which I'm presenting here. So, if you want to step from Ti to Ti plus H, then the first thing to do is to get a step size estimate for both of the methods, which I'm indexing as slow and OSC. And the step size just depends on how quickly the solution, how osylitric the solution is. How oscillatory the solution is, and how quickly omega is changing, how well it can be interpolated with a Chebyshev grid specifically. After this initial estimate, we refine the step sizes just to be sure that we'll get the answer to within a user-specified tolerance. And then we compare them. If the oscillatory step size is larger and the solution And the solution is indeed oscillatory, so we step forward, we would be stepping over at least one oscillation, then we perform a Ricutti step. But because of its asymptotic nature, the residual may not be reduced to the user-specified tolerance epsilon before that optimal truncation point is reached. If it doesn't, so if it has not converged, So, if we don't, if it has not converged, then we just revert to a Chevyshev spectral step, but with the step size H slow. H slow is halved every time the user-specified tolerance isn't met. So, yeah, once we have performed either of the two steps to within tolerance, we then advance the solution. And that's really it. So, let's see it in action. So let's see it in action. The first example that I'm bringing here is the air equation, whose frequency goes the square root of time. And I'm plotting four quantities, the numerical solution and the analytic solution on which it is superimposed, the step size as a function of time, the numerical error in the method, and the number of periods of oscillations the method steps through each. Method steps through each in a step, within a step. And yeah, the method demonstrates successful switching between Chebyshev and Riccati. And in the highly oscillatory phase, it can indeed step over many oscillations in a single step. But this plot, the numerical error, perhaps requires a bit of explanation. So the user-specified tolerance, epsilon, was 10 to the 12. Cylon was 10 to the 12. And the method does keep to that for a while, but then the numerical error starts accumulating. But we have to keep in mind that for this problem, the conditioning of the problem gets worse and worse as the solution crosses zero. And the theoretical maximum accuracy or minimum epsilon is just the condition number of the problem times machine precision. Number of the problem times machine precision, which using double precision is about 10 to the minus 16. So out here, when we have traversed 10 to the 12 oscillations, we cannot really expect more than about four digits of precision. And the method does perform really well in comparison to this theoretical minimum. The next example. The next example is an ODE with a parameter. It's called the burst equation, or I call it the burst equation, and it has a parameter n that controls how tall a spike in the frequency is around the origin. So it's slowly changing and then omega spikes and then dies down again. So it's slowly changing again. And again, it's I'm plugging the same four quantities. There is successful switching. This is the This is the step size progression, and there is again an impressive number of oscillations traversed per step. And the numerical error is kind of as expected most of the time, but clearly there is some refinement that needs to be done on the step size update of the method. Still, this is very much work in progress. Right, so I made a claim about some geometric conversion. About some geometric convergence in the residual, which I want to back up with some theory which I'm presenting on the slide here. So, put into words, the theorem is that if some conditions are met on the bounds of omega and omega prime, which end up being translated to omega being sufficiently large and omega prime being sufficiently small in comparison, so omega being large and slowly changing. So, omega being large and slowly changing, then we do indeed get this temporary geometric convergence in the residual at a rate r, which deteriorates with k, k being the number of iterations that we end up going out to. So for a large, slowly changing omega, it might be that we can perform many iterations. We can perform many iterations with geometric convergence before we hit that optimal truncation point. But a practical note is that we don't really need to do that because we only need to go out until the residual reaches a user-specified tolerance and no further. So, in practice, even though the rate deteriorates with K, we don't really need high K. And this is And this is a somewhat general property of asymptotic expansions that, you know, among others, Jim Bremer talks about in his paper. Right, so this is basically the theorem being put into words. And the proof in a nutshell proceeds by defining some concentric circles in the complex plane with kind of a linearly growing. Growing radius, and the number of circles will be the kind of number of iterations we perform in this Riccati asymptotic expansion. And we can bound a function's derivative on the j plus one ball or circle and express that bound using Cauchy's theorem for derivatives with With via the infinity norm of f on the previous ball. And using this, there is then a kind of an induction proof for the following bounds, for following statements 12 and 13. And then these are applied to the residual iteration, which is just the residual of the k plus 1 iteration r of x k plus 1 as expressed. xk plus one as expressed in terms of the residual of xk and then that gives us the the theorem um all right so i haven't uh you may have noticed that i haven't presented any runtime results for this for this method um and that is because it is the code that implements it is is work in progress so it needs to be optimized um but i wanted to talk about the kind of theoretical The kind of theoretical minimum runtime, so the kind of the complexity. So, for steps that end up using the spectral method based on Chebyshev nodes, they involve a linear solve, sorry, not numerical solve, a linear solve based on n Chebyshev nodes. So that would be an n cubed operation. But, oh, this should say n squared, but with some tricks, including Some tricks, including using an ultraspherical polynomial basis, this can be reduced to n squared. So, overall, this can have n squared complexity times the number of steps, of course, that the ODE performs over the range of integration. And for the Ricardi absentotic solver, there are two things that work. There's the number of steps multiplying the complexity and the number of iterations. Of iterations. So for each iteration of the Riccardi equation, there is a matvac, which is n squared, when n is the number of Chebyshev nodes used. And there is also a linear solve because we defined the phase function as the derivative of something. So we do need to perform a numerical integration, which is just going to be a linear self. Which is just going to be a linear solve for the kind of the differentiation matrix. But actually, this can be done only, this can be performed only once. We can arrange that it is performed only once for the ODE solve. So even though that's n cubed, the overall complexity of the Riccati steps ends up being n squared. Right, so some kind of ongoing application. Kind of ongoing applications and ideas for how the method can be applied are in the fields of cosmology and quantum mechanics and within mathematics, evaluating special functions like Legendre polynomials. In cosmology, and I have already done some of this, there is an ODE of interest that describes the extremely early time evolution of space-time curvature perturbation. Space-time curvature perturbations. And this OD needs to be solved about 10 to the 9, 10 to the 10 times when embedded in a kind of a Bayesian inference framework where we want to infer quantities about the early universe. So it is of interest to, of great interest, to have an ODE solver that can deal with highly acidic ODEs. Oh, I think it was implied that this is a highly acidic ODE. Was implied that this is a highly oscillatory ODE. And of course, in quantum mechanics, we can think of the time-independent one-dimensional Schrodinger equation, which takes the form of the ODAs that we can deal with. And we can use the method to compute the energy eigenvalues of highly excited states for a given potential well by kind of try and error or an optimization process. An optimization process. But for each of the each of the optimization, each of the steps within the optimization will need to do an ODE solve, which is a highly oscillatory ODE for high energies. Okay, I'm not sure how I'm doing with time, but I'm going to wrap up. Sure, yeah, I think one or two minutes wrapping up and you're good, Roshina. Okay, so I've, yeah, to summarize, I've presented. Yeah, to summarize, I've presented an efficient numerical solver for general linear second-order ODEs, whose solution may be oscillatory. And the method is unique in that it can deal with a non-zero damping term and both non-acidory and oscillatory regimes of the ODE. There exists another method that, well, I've developed that is kind of similar in sentiment. It also switches. Is kind of similar in sentiment. It also switches between two different methods and it also uses an asymptotic expansion for the acidry phase. It's called the WKB or Wenzer Kramer Spirulin expansion. But without going into details, a major difference is that because the WKB expansion, the iterations from term to term are much more complicated. It involves a recursion relation with all of the previous terms. Terms, it's much more difficult to make it adaptive in the number of terms included in the WKB expansion. Here we can go out to the optimal truncation point or we can adapt the number of iterations k, but OSCODE actually uses a fixed number. So it is, and it also ends up being a low order method. So this method importantly is arbitrarily high order because of that. In the immediate future, In the immediate future, well, the code needs to be optimized. The step-size update algorithm needs some refinement. And I have plans to use the ultraspherical polynomial, an ultraspical polynomial basis for the Chebyshev steps. But we are quite excited about it because the method is very efficient and it has the potential to generalize to PVs, which is a much wider range of problems. So, yeah, I'm going to leave you with my references and yeah, have questions. Great. Thanks very much, Rashina. We've got, you finished perfectly on time, 7.25. Sorry, 11.25, 12.25. I'm not sure what times we're in. I'm not sure what times we're in. But, anyways, we have a few minutes for questions. I have a question. So, when you're solving a Ricardo equation, Ricardo equation is non-linear. How do you guarantee that you always have a global solution? I couldn't quite catch the end, but so was the question. Right, since it's non-linear, how do you guarantee that you have a global solution? Oh, we don't seek a global solution. So we basically iterate over the integration range of the ODE. And for the ODE itself, the original one was linear. So we create these two. So we create these two linearly independent solutions for it, even though the Riccati equation isn't linear. But the solutions will end up being linearly independent for the original ODE. And then at each of the time steps, we basically match the Cauchy conditions. We basically just linearly combine them. It's a solve for 2x2 solve to match the answer at the previous. The answer at the previous at the end of the previous. So essentially, you write these non-linear equations sort of as a first-order system? So we don't deal with the non-linear equation directly. We match the initial conditions of the original ODE. Yeah, okay. If that makes sense. Which is linear. Yeah. All right. Thanks. No worries. No worries. Any other questions?