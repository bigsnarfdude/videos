Cool. So today I want to talk about some recent work with some fantastic co-authors. Joseph Cheryon, who, as you all know, is a professor at the University of Waterloo. Logan, who at the time was a student at Cornell, he's joined the industry recently. And Sheridan, who's a postdoctoral fellow at the London School of Economics. And I'm going to talk about, yes, I do augmentation again, but beyond on cross-border components. Let's get into it. Obligatory introduction, network design, we want to find the cheapest subgraph that satisfies certain connectivity requirements. MST is the simplest case, minimum span of trees. Very well-studied problems include survivable network design, capacitive network design, more recently, flexible graph. And a lot of these problems are quite complex. So you need to solve certain subroutines, the most common of which is the edge augmentation problem. So what's the problem? Problem. So, what's the problem? You're given a family of cuts and you want to find the cheapest cover of these cuts. So, cut is just a subset of the vertices, an edge covers the cut if it's incident to this subset of vertices. And you want to find the cheapest and most subset. Now, there have been very specific cases studied: tree augmentation, cactus augmentation, as Vera was pointed out, standard tree augmentation, matching augmentation, a lot of these have constant factors. A lot of these have constant factor approximations. But in general, the problem is as hard as that cover. You can show that the two points are equivalent, which means that you can't approximate it to better than law of feeding. But we can do better if the family that you want to augment has some sort of structure. In particular, Williamson, Gomens, Michel, and Wazzirani in 95 showed that if the families satisfy the so-called or possible properties, then you can get a tool. Then you can get a two-approximation. So, what's an uncrossable family? If you take two sets in a family, you either want the union and the intersection to also be in the family, or you want the set differences to also be in the family. And this is a very useful definition because it captures a wide variety of cuts that we are actually interested in. For instance, it captures min cuts. And what that means is for all the augmentation points that I just showed you, you can get to approximation algorithms very easily. Approximation algorithms very easily using the techniques by Lucy Edgel. And they observed two very key properties that allowed them to do this to approximation. One is that if you take a minimal set in your family and you look at any other set in your family, these two sets don't cross. So essentially if C is minimal and S is any set in the family, either C sits inside S or they're destroyed. Said S or they're destroyed. The second property is a bit more technical. It's what we'll call the dual laminarity property. And it states that there exists an optimal dual solution whose support forms a laminar family. So if you look at all the sets S where this dual solution is positive, those sets S form a laminar family. A laminar family is essentially disjunction of chains that cross the S. And this dual laminar to property is very essential to it of Rounding methods as well. To it, rounding methods as well, to most quantum factor approximations in the context of natural design. And for many years, it was believed that the dual laminarity property or a possibility is essential to obtaining constant factor approximations for this eigentechation problem. And Williamson et al. left it as a challenging open question as to whether we can obtain constant factor if it didn't have such properties. Challenge accepted. But the first thing we should ask is: why do we actually care about extending the results to larger classes of cuts? Turns out there are naturally arising families that are not captured by uncrossable families. For instance, the family of four-Neilman cuts, which can be described as you take a graph G with some capacities on the edges and the threshold alpha, you look at all cuts that have capacity less than. cuts that have capacity less than alpha, less than or equal to alpha. And alpha could be any number, so it's not Neumann in the traditional sense where you're only looking at the constant factor from minimum cut, but really any alpha factor. And this family does not satisfy the incrossflow problem. Very simple example here, let's say alpha is 5, the numbers on the edges of the capacities, the red and blue cuts are violated, they are in the family. They are in the family, but it's only the intersection and one of the set differences that's also in the family. So C is in the family and D is in the family, but the union is not, and the other set difference is not. So this is not an uncrossable family. But it lets us generalize uncrossable families in a very natural way, which is what we call appliable families. So the definition of applied family is when you take two sets in a family. That's when you take two sets in a family, you need at least two of the four corner cuts to also be in the family. So earlier we wanted either the top pair or the bottom pair to be in the family. Now we allow you to mix and match. So it could be A union B and B minus A that's in family or any combination of. So it's a very natural generalization of pliable families. And the hope would be that the primal dual method developed by Williamson et al. Well, before that as well, but shown by Rosen at all to provide a two constant for uncrossable families works for this family as well. So let's look at the primal dual in a bit more detail. The primal is fairly straightforward. It's just one of minimized cost, subject to each cut in your family has at least one edge crossing it. The dual has this constraint. So you have variables for every cut in your family, and the constraint essentially says that. And the constraint essentially says that if you look at an edge E, you sum over all the cuts that have this edge E crossing it, then this should be at most cost of E. And primal dual works as follows. It works in two phases. In phase one, you take all the minimal elements of your family, you increase the duals from the formally until an edge becomes tight. You add it, and then you repeat. When you repeat, you need to update the minimal sets in your family. And you repeat until your solution is feasible. And then you perform a reverse delete phase two, where you take the edges in reverse order of additions, and you delete an edge if it's no longer required. And this provides a two approximation for uncrosable families. Does it provide a two-approximation or constant approximation for piable families? And the answer, unfortunately, is no. In fact, we have In fact, we have a counterexample where using this method to augment appliable families can do as bad as root of n times the cost of the optimal solution. And this picture here is that example. I'm not going to go into what it is. It's a bit complicated. But yeah, this example shows that the primer dual does really work well. And a major issue seems to be that active sets or minimal sets are now created. Minimal sets are now crossing your other sets in the family, which didn't happen for the crossable families. So that seems to be a big issue. So, what do we do? We, of course, look for more structure in the family of cuts that we're actually interested in. It turns out that the family of cuts that we're interested in, like Neoman cuts, have an additional property that we can leverage to obtain constant factor approximations. And we give this property a very creative name. Creative name, property gamma. And informally, what the property states is that the number of crossings between minimal sets and other sets is roughly proportional to the number of minimal sets. But more formally, if you take a minimal set C and crosses two sets S1 and S2 such that S1 is contained in S2, then the shaded region, which is just S2 minus S1 union C, minus S1 unc is either empty or lies in your family. And turns out that with this additional property, we can prove that the primary dual does give a constant factor approximation. And I'd like to point out that even with this additional property, we still lose the two properties that were sort of essential or thought to be essential for constant factor approximations. We still don't have non-crossing in all sense. Crossing in all sets. And we still don't have dual laminarity. And I don't know of many examples where constant factors are available for network design without these. Ramin and yes. Yes. You can see that S1 and S2 here are enemy. Yes, so S1 and S2 are two sets in your family. Not minimal. Not minimal. Yes. C is minimal. And if they cross in this way, where S1 is Cross in this way where S1 is contained in S2 and C crosses both of them, then this shaded region here should be either empty or also in your family. You can weaken that a bit to state that this shaded region is either empty or contains something in your family. That's also okay. But yeah, so going back to these two properties, Back to these two properties, this is one example where you can still obtain constant factor without these properties. Ramin and Zach recently had a result for directed standard trees under some conditions, and they can show constant factor, and those also can have those properties, which Ramin might be talking about later today. But okay, let's go with the proof briefly. The goal is to show that in every iteration of the primal dual, the average degree of the remuneral sets. Average degree of the minimal sets is bounded in a final solution. Why is this useful? Well, if, so let's forget about average degree. Let's say we could just show that the degree of minimal sets is bounded, let's say 2. Then you're essentially a factor 2 away from complementary slatness. You do maintain dual complementary slatness using dual. So you get a two approximation, and you can just show that this works in average as well. So that's the goal. We want to show that the average degree of minimal sets is bounded. Of minimal sets is bounded when you find your solution. So you're also doing reverse belief. Yes, reverse belief is very important, yes. Do you want me to spend more time on why this is sufficient? But yeah, essentially the point is you do have dual complementary slackness, which is whenever your primal is positive, x is positive, this is tight. It's positive, this is tight. We would like it to be true here as well. So whenever your dual variable is positive, we want this to be tight. But let's say this was not tight by a factor of 2. So let's say summation xc was greater than or equal to 2, or equal to 2, or less than or equal to 2 greater than or equal to 1. Then that shows that you get a 2 approximation. And you can, it's fairly, once you write it down, you can see that it works on average as well. See that it works on average as well. If you can show that on average it's bounded by two, then you'll get a two-approximation. Okay, and the way this is generally done is to assign a witness set to every edge in your final solution. So a witness set has certain properties. Firstly, the witness set should be a part of your family. Secondly, it shouldn't witness any other edge. So the only edge So, the only edge crossing the witness set in your final solution should be this actually. And ideally, what you'd want is the family of witness sets forms a laminar family. And this is simple to do for uncrossable families, because if two witness sets cross, let's say S1 and S2, you can uncross them into S1 intersection S2 and S1 union S2, or the set differences. And this would strictly reduce the number of crossings by 1. And you can keep repeating this. By one, and you can keep repeating this to show that there exists a number of this family. However, this is not, this particular step is not true for pliable families. If you do this sort of uncrossing for pliable families, the number of crossings need not reduce further points. But using a slightly more complicated argument, we can in fact show that you can get this lambda family. And we haven't used property gamma so far. So for any pliable family, when you perform the primal dual with reverse delete, we can find With reverse delete, we can find these witness sets. But things go wrong when you want to now use these witness sets to come up with a charging argument. So, what's the charging argument for a cross-world family? To every minimal set C, we assign the smallest witness set as C that contains it. And then, what we can observe is now you take an edge E that's incident to your minimal set C. So let's say this is C, there's an edge incident to it. Let's say this is C, there's an edge constit to it, and you look at the set that is witnessing that edge E, then this set S E is either the set S C itself or it's a child of S C. And when I say child, I mean in the laminar tree that you can construct based on the laminar family. To see why this is true, essentially if that was not true, there would be a brown setting between here, a brown. set in between here, a brown set, but then that set is witnessing this HB as well, contradicting properties of a linear set. And the same is true here as well. So if you look at any minimal set C and an edge that is incident to it, it's kind of paid for in the degree of S C in your laminar family. So the degree of C is paid for by the degree of S C in the laminar family, and then a simple hand shaped laminar family. And then a simple handshake and lemma essentially tells you that the average degree of million sense is 1 by 2. So you get a 2 approximation for uncrossable families. For pliable families, the picture is on X. The set S E could actually be a distant descendant of your set S C. In particular, you could have your set C with an H incident to it and the witness S E, but you could have these brown sets now crossing the set C. Crossing the set C, which are also witness sets. And you could have a huge chain here, a really long chain. And this happens because now minimal sets can cross these witness sets, which couldn't happen for the cross-border families. And this is essentially the reason why Prime-Dur doesn't work for platform families in general. And this is where property gamma comes in. Because property gamma ensures that if you look at this region here, At this region here, that's either empty or it's in the family. If it's in the family, then it contains some minimal set. And so that set S1 essentially branches off and creates a new minimal set somewhere. If it's empty, we can show that it can't be empty for too long, and this happens regularly. So even if this chain was really, really long, we can show that at regular intervals, the chain is going to be At regular intervals, the chain is going to branch off. And then, what you can do is we can then pay for the degree of this minimal set C using these branches, essentially. And this is what allows us to get our constant factor for piable families with property. Let's look at some applications now. Now, before moving on, any questions about what I've shown so far? Yes? Yes. Is it easy to think about this property? Um not not very easy, but it's yeah, essentially if you if so we you can show that it's applied quite easily. No, it could be any any factor still has property problem, yes. Yeah, so it's quite easy to show that it's pliable. That it's pliable. And then, after you show that it's pliable, essentially, so you look at C and you look at S1 and S2, because it's pliable, we know that C union S1 and S1 minus C also have to be near-minimum cuts. And the same holds true for S2 as well, because we know two of the four cuts have to be in a family. Two of them are actually subsets of C, so they can't be because C is minimum. So it reduces the number of kinds of minimum cuts, and then using that, you can. And then, using that, you can show that property gamma also holds. But it's not too straightforward. Okay, so some applications, flexible graph connectivity. If you heard Ria's amazing talk in the morning, you know all about flexible graph connectivity now. But essentially, you're given a graph, you want to find a pH-connected graph that's tolerant to Q unsafe failures. And when Q is 2, And when Q is 2, Boyd, Cherian, Haddarin, and Ibrahim Poor showed that you can get an order log V approximation algorithm, essentially solving the underlying set cover instance. Checkery and Jane recently, and this is Rhea Jane, showed that an order P approximation is possible by dividing the problem into augmenting P on crossover families. We can show that you really just need to solve for the augmentation of one pliable family. For the augmentation of one appliable family satisfying property gamma, and so we can obtain a constant factor approximation. So the factor 20 here looks quite bad. Recently knew to have improved into a 7 plus epsilon. And then we showed that similar methods actually work for p, 3 at flexible graph 92 as well. So we can get a constant factor for p, 3 as well. Both of these have recently been put up on archive. The Newton paper, I'm fairly confident it's accurate. I'm fairly confident it's accurate. This paper hasn't been externally verified. So, if someone has some time and wants to do that before we send it to Jensen's desk at IPCO, that would be great. But essentially, the idea there is when P is even, it forms a pliable family. So you can just augment it. When P is odd, it's a bit more complex. You need to carefully analyze the structure of the cuts. And you can show that you can split into two uncrossable functions. And then you can. Patterns. And then you can just augment across both patterns. Another application is the one that I've been talking about, which is augmenting neo-minimum cuts. So the problem of augmenting neo-minimum cuts can be done now with a constant factor. And the 16 as well, in our follow-up paper, we will be improving it to something like 709 with the tighter analysis of the charging argument. What does it mean to augment neon cards? Right, so let's say you're given a family of neomin. So let's say you're given a family of neon minimum cuts. So you're given a graph G with capacities and this hash rod alpha. So you take all the neon minimum cuts. And let's say now you also have some additional links which you can purchase and you want to augment these neon minimum cuts. So essentially the edge augmentation problem where the family that you want to augment is a neon minimum cut family. For instance, so you only want to increase the Want to increase the connectivity of the cut value across these near-minimal cuts? Exactly. Exactly. For instance, Vera talked about the connectivity augmentation problem, where you want to only augment all minimum cuts. Here we want to augment all near-minimum cuts, so it's a larger family, and you can still do it in conspiracy. And an immediate application of this is for the capacity to educate. This is for the capacitated edge connectivity problem, where you're given a graph G with capacities and you want to find the minimum or cheapest K-edge connective subgraph with respect to these capacities. And here Gomitz et al. showed a 2K approximation, Boyd provided a K approximation, Boyd et al., Gomitz et al. provided 2K and K respectively. And we can show that you can obtain a K over U min, order K over U min, where U min is the minimum capacity of match, which is better in. Of an edge, which is better in some cases. And the main insight here is, for instance, for the 2K, the algorithm is essentially augmenting minimum cuts one by one until you get to kh connectivity. Here we can augment a family of neo-minimum cuts at once, and then you can do that again and again. Okay, so I'd like to end with just some takeaways and open questions. Folklore beliefs need not be true, the Bigfoot is not true. True, the Bigfoot is not real, and uncrossability may not be required to obtain constant factor approximations. Neither is dual lambda t. And some very interesting open questions that arise from this is, can we get rid of property gamma? As Neil pointed out, it's not very easy to verify if property gamma holds for the class that you're interested in. And we'd really like to get rid of property gamma. We show that the normal primary dual. We show that the normal primary dual doesn't work. Maybe iterative rounding works. Iterative rounding really relies on dual laminarity, so we're not sure if that would work either. But is there some technique that allows us to get rid of property gamma? Property gamma is also not essential for primary doing to work. In our follow-up paper, we will be showing a different disjoint property that also works, which Neominka is also satisfied. So property gamma. Satisfied. So, property gamma is really not necessary. So, is there an exact characterization of a property under which prime material, the normal prime material works? And somewhat related to these points is, so let's say you're given a general privable family, can you decompose it into order one or a small number of privable families with property gamma? Because then that would give you a constant factor for private families in general, as well. Thank you. Happy to take you. Happy to take it. I have one question. So you say if you can so you're asking whether it can be decomposed with that property gamma, to constant number of pliable families. If it can be decomposed, then the rest of the analysis goes through? Oh, yes. So if you can just express any pliable family as. If you can just express any pliable family as the union of a constant number of pliable families with property cover, then everything just goes through. Because then you can augment each of these families separately and you lose additional factor on how many units you have to take. So what is your counter example show again? The counterexample shows that the normal primary dual doesn't really give you a constant factor for planetary. Factor for plantable families in general. But I should point out, so that example actually has an integrality gap of 4, because you can split it into two uncrossable families. And in some sense, vaguely we find that this structure is kind of unique, and you need such a structure for prime ideal to fail, which gives us a lot of hope that there should be a way to overcome the structure and get it comes back to. That would be nice to know. I should also point out that if we relax the pliable property further to state that only one of the four should be in the founder, then it's as hard as set colour. What's the constant that you get? What's the constant that you get at the end? Uh 16, currently, 4 16. But we can improve that to 9 mix 7. Oh I make this out the recording with that. 