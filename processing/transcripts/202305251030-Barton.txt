Yeah, okay. Well, thanks to the organizers for having me here. And so this actually, I swear I didn't like coordinate anything with Flores, and he can confirm that. But it's going to have some overlap with what Flores just said, but not too much. So I wanted to start out by basically the purpose of my talk is to propose. Is to propose or suggest or recommend or whatever that we, meaning somebody who's good at formalizing things, to formalize a specific theorem. And so I wanted to start sort of start off with a kind of like overview of homotopy theory, explain like what the point is of the theorem. So from a really big picture point of view, what is homotopy theory? Well, first of all, it's about these things, and they go by these several names, and it's sort of a strange thing. Homotopy theorists Homotopy theorists tend to use this word spaces, but there are some issues with the word spaces that will become clear in the next couple of slides. Some people coming from type theory or higher category theory also call them affinity group points. And then recently some people decided to start calling them anima. I don't really know what to make of that word. I don't even really know what the plural of the word is, so I just throw it anima. It's the same. It's the same. Okay, great. So that was easy. I should say anime. Yeah, yeah. So I sort of like about it the fact that it's kind of mysterious and opaque and foreign sounding. So I want because I want to encourage that perspective. And so that's the sort of first thing that Hope With Theater is about. And then it's about all these other things that you can build. And then it's about all these other things that you can build out of spaces or whatever, like pointed spaces and spectra and the infinite ring spectra, and then infinity categories, and then we could take sheaves of like any of the previous thing, our infinity topos, and we get like lots of other things that are also of interest, nominop theory, not just spaces. But really, the basic object is spaces, and so that's the building block for everything else. And to sort of connect this to previous talks on the subject of this workshop, I thought I would mention. Of this workshop, I thought I would mention that there's this sort of slogan or philosophy that maybe not every cohomology theory, but certainly lots of things that sort of get known as cohomology theories can be viewed somehow as some sheaf of spectra, or maybe some object like a spectrum on an infinity topos, or maybe some object like an infinity topos. So, you know, we had topological K-theory that is represented by some ordinary spectrum, the K-theory spectrum. We've had, nobody actually ever talked about a topic cool mod yet, I sort of assumed somebody would by now. So many wood by now. But if you have atol cohomology or pro-e-tau cohomology, then you take your favorite pro-atoll site and take your favorite coefficient and take the Eilen-Bring-McLean spectrum, and that's the thing. So it's unified. This is a picture that unifies a lot of cohomology theories, but in order to understand that, well, we probably better start with just this blue thing, right? So, okay. So what are those, what is this blue thing? What is this space? So there's a funny thing in homotopy theory that, like, in a lot of math, we study, I don't know, fields or topological spaces. Fields or topological spaces or graphs, and these are like things you can describe as a set of some extra structure. It's really easy to describe them in like set theory or in Lean's type theory. And it's not true for this blue spaces from the previous slide. Okay. And how do I deal with that? So one way we can deal with it is we sort of just say, well, there are these things called spaces. And this is sort of the approach of homotopy type theory. Well, in set theory, we had set as a primitive notion. So why not just have sort of spaces? Primitive notion. So, why not just have sort of space theory where space is a primitive notion? The only attempt that I'm familiar with to do this of any kind is some kind of version of pot. So, there, a type, instead of being like basically a set, a type is now basically a space. If you have two types, there's the type of functions, that's the mapping space. And then you can define if you wanted, and it was written on the board before I erased it, what it means for a space to be a set. So you didn't increase the number of printed notions, because you sort of defined your old notion in terms of this sort of dancier notion. Notion in terms of this sort of dancier notion. And in particular, something you can see already from this is that a space is not like a set with extra structure. Actually, a set is a space with extra property, but a priori, an extra structure. And that's very confusing especially if you see presentations of simplicial categories where you might think, oh, it's like a simplicial object. No, this sort of, it's upside down from the way you think it might be. Okay, so this is sort of. Okay, so this is sort of the synthetic approach. But if I just want to use sort of ordinary set mathematics, how am I going to talk about spaces? And this is sort of the traditional way. And I'm going to sort of argue that what people do is they study these spaces in terms of these other things that I'm just going to refer to as presentations, and which I'll explain on the next slide. But I've sort of set this up as a kind of opposition of two things you could do. But really, you should do both of them. You know, you should do both of them, and as Flores was also suggesting, like at some point, what you should do is formalize the simplicial set model of homodomic tech theory and then figure out how to take your theorems up here and like turn them into theorems down there. I don't really know how to do that, but if you were going to try to do it, you better start by understanding some pushial sets and just their homotopy theory before you could connect the synthetics up to it. So that's basically what I want to do. Okay, so what do I mean by this presentations business? So I really mean presentations. So, I really mean like presentations. So, a group is something you can describe. So, I picked the cyclic group of order four, and you can describe it via presentations. And I picked two different presentations. I want to emphasize that they're different, right? Because the first one has one generator, and the second one has two generators. Okay, good. So we agreed that these are different. Okay, good. So, similarly, there is this weird blue thing called a space or an animal or whatever, and we can present it by different topological spaces. And they're really different topological spaces. And they're really different topological spaces. Like Rma Z, sort of real, one real-dimensional space. But the unit complex numbers, that's like a two-real dimensional space. They're really different spaces. Well, they're different topological spaces, but they're somehow the same of this blue space word. And this is the problem with the word space, is that it appears on both sides of this line, and that's why I picked these colors to try to keep things straight. So, okay, this is the first thing to understand about Homotopy theory. To understand about homotopy theory, you take algebraic topology, you sort of understand, okay, there's a map here, and it's homotopy equivalence, and that's sort of what we mean when we say they present the same space. Okay, good. But the story doesn't end there, right? So there are other ways that you can also present a space. For example, I want to show a picture. So I have this simplicial set that everybody knows the boundary of delta 2. Whatever it is, the boundary of delta 2, and it looks like a triangle like this. Okay? And you can kind of see it looks sort of like a circle. But here's another simple set where I take a one simplex and I identify its endpoints. So I just get a sort of edge that looks like this. Okay? And again, they're different. The first one has three vertices and the second one has one vertex. So again, I have two different presentations of the same space. But not only that, I claim that this space is the same as this space, even though here I use the same The same as this space, even though here I used some button sets, and here I used topological spaces. Okay, and this is sort of the point of: well, you know, there's some problem here. How do we explain what's going on if this sort of doesn't fit into our world where you have rings and a ring is a set with a bunch of operations? Something different is going on. And so you could step back one level and say that, well, there's this entire blue infinity category spaces that we're trying to describe, and we can do it. Describe, and we can do it in different ways. One by using topological spaces to try to describe the objects, another by using simplicial sets to describe the objects. So now both of these orange things are like presentations of the same blue thing. Okay, and so there's a lot to explain here. Condense sets, another allowable stuff. Yeah, I think you could if you wanted to. Yeah, you sort of do the same thing for top. Or sorry, yeah, you do whatever you would do to top to condense sets and you would get another. Yeah. But I don't know if any, if there's ever really. Yeah, but I don't know if any if there's a really much I don't really do swallow it, yeah, no, neither do I. Yeah, okay, so this whole setup is organized by a subject that's called abstract homotopy theory. Like what is to sort of answer the question, what is going on here? What do we mean when we say that you know these present the same infinity category? What do we mean when we say that like this topological space presents the same space as this infidel set? One thing I left off this slide, but is also important. All of these objects on the left. All of these objects on the left, right, they each have like some automorphisms. So phi4 has an automorphism that swaps one and three or whatever. And these guys also have an automorphism that switches direction to circle. So when I give you two presentations for the same object, I better tell you like how they're the same, right? So there's really a map that sends X to Z or something. And here there's maps both ways. And here's there's a map in one way, but there's not really a map in the other way because I'm sure there's not enough edges or something. And that means likewise here. And that means, likewise, here there should be some kind of relationship between these categories that expresses the way in which they present the same hometown. Okay, good. So this is the theorem that I want to... Oh, I see. That's how people did that. So that's the theorem that I want to present and propose that we prove. But this is sort of an explanation of the meaning of the theorem. Okay, so just to recap, we need to explain what these orange things are. We need to explain what these orange things are, right? I was sort of vague here. I just sort of, I mean, they're categories, but maybe more than just categories. So, what kind of things are they that make them presentations of something that home-topy theorists care about? And secondly, what is the actual relationship that says that they are presentations of the same infinity category or whatever? Okay, and the sort of first successful attempt, I guess, to do this in a systematic way was by Quillen. In a systematic way, was by Quillen in the 60s. He introduced these things called model categories. That could be the answer to the first question. These things are model categories. And then the second, and also there was this notion of, I mean, he probably didn't call it equal and equivalence, but that's what everybody calls it today. So there's a junction where we have the geometric realization and the singular set functor, a form of Gohan equivalence. And that's what it should mean to say that these represent the same thing. This isn't the only notion of equivalence. Like the notion of equivalence of homotopy theories today, but it's the oldest and for a long time most successful one. Okay, so what I want to suggest is that this theorem, first of all, we should prove this theorem. And secondly, it's not really that much work, although it is a difficult theorem. And in fact, I just really want to talk about the proof of this theorem. Okay? But before I do that, actually, are there any questions so far? And then I'm going to talk I haven't told you what a model category is. I'm aware of that. So maybe questions on other topics. So, maybe questions on other topics. Okay. So, what is the subject like? Well, my experience, there's this sort of subject of general category theory where you already know you have model categories and then you, whatever they are, and then you sort of operate according to the rules of model categories. And all that business is, first of all, easy on paper, and then when you try to formalize it, it's still easy. But we did do we have model task grades in paper? No. Task groups and thing? No? Well, sort of. I mean, so I have a very old repository, and then Joel has another nod method. They're not in Mathlib yet, but it's sort of, there are a few design decisions, but not really so many, and then you just sort of do them. So all the hard part of the theory is always in building the model category. I like to say that, like, every time a model category exists, it's because of some miracle that occurred that wasn't supposed to occur. And then you get to exploit that over and over again. So the strategy here. So, the strategy here should be that you build the model category, then you forget about all the hard work you did, and then you just sort of get consequences from the fact that the axioms are satisfied. And for example, more or less, once you've proved the theorem on the previous slide, then you, I mean, you still have to do some work to get out like singular homology, but it's not like work on the level of thinking about simplices and how they fit together. It's like a different kind of more formal. It's like a different kind of more formal, like, oh, I know this is a weak equivalent, so I know this is a weak equivalent second part. And I wanted to include at least some attempt at describing what a model category is, but the point I want to make is that it's really just three, like prop, the only data of a model category is like these three classes of maps. And they're just, so it's just giving some families of propositions. And if you describe the weak equivalences of a model category in one way, but then you prove that they can also be described in another way. But then you prove that they can also be described in another way. Well, that's great because now you have two propositions. You could rewrite all your assumptions along that equality propositions, and you're never going to run into trouble doing this. So it's a very amenable to kind of like lean style formalization sort of concept. And then there's a bunch of axioms, and they're all propositions. Okay, so there's no like issue about, oh, do I really, you don't have to look into the construction because it's all hidden as the sort of, it's all contained in these axioms. There's one small. There's one small twist in that. So, some people like to include this functorial factorization business, and that's not what the things I said. It's really data. So, there's a choice for every map of a way to factor that map into this blah, blah, blah. And it's really useful in some constructions. You never seem to actually need it. Like, you can always do what you want to do without it, but sometimes it's a lot harder. So, there are some design decisions. There are some design decisions to make here once you get far enough, but it's not a very pressing issue. I just wanted to include it as a, I don't know, as a to be honest about this point. Okay, and so yeah, so you asked if we already had multiple categories. So some of you might have, so Kevin, for example, was here, yeah, so he'll remember my talk in Amsterdam in 2019. And there I gave most of a so the model categories that we're interested in. Oops, I'm using the market. We have some special sets, and we have topological spaces. So I constructed most of the model category structure on topological spaces because it's easier to do than some positive sets. But somehow it's also less efficient in the long run, right? So if you build the one for some positive sets, then the one for top bottom spaces is really just like a matter of pushing some axioms around. Some axioms around. Whereas, if you do the one for topological spaces, it doesn't help you as much with the one for some special steps. So, this is sort of my, well, I'm suggesting that you should do it in this order, okay, based on my own personal expertise. Joel has a bunch of general model category theory that is sort of like along the lines that I was saying before. You sort of just like write down what you write on paper, and it all flows very naturally and smoothly, and it's fine. I didn't do very much of this because I had like a different, I was working. I had like a different I was working with this other notion of cofibration category, there's kind of like half of a model category structure. And I had the same experience with co-fibration categories that they were just very smooth and I just knew that or assumed it would be the same for model categories. So the general model category theory is really easy. It's not a math library, but it's just, you know, it could be PR'd probably like next week or whatever. And then the other sort of stuff that already exists is, well, I guess the categories, some social sets and topological spaces, and the adjunction, right, between them. And the adjunction right between them. So I think the functors go by the rather bland name of like top.ofset and sz dot of top or something. So they're called sing, and I don't know why they're not just called sing. Great. Okay, so now I want to talk about like what are the actual ingredients of this proof. So there are a lot of proofs, like maybe half a dozen different proofs of this theorem, and you can do it in different ways. You could start with topological spaces and go to some special sets. I think that the one that I'm going to present is. I think that the one I'm going to present is, well, among the proofs I understand, it's like the most sensible one to formalize. So I want to explain what that is, and so then people can sort of like, you know, I don't know, well, either believe me or not, I guess. So the strategy is we're going to deal with some potential sets first, and that's the hardest part. And then we're going to, once we've done that, we're going to build the one on topological spaces. And this, I'm not even going to say anything about it. It's just like, it would be like 100 lines of encode or something. And then at the end, you're supposed to prove that there's equivalent equivalence, or that the specific assumption is equivalent equivalence, and that's sort of like medium-level difficulty. That's also not very hard. Most of the work is really here. And the specific proof that I think is good is, well, some version of the proof using the properties of the x-infinity functor. And that will come up eventually. Okay, so I know I'm not telling you to go to model categories, so I'm just I know I'm not telling you to go to model categories, so I'm just going to dive into the proof. So, how do you usually construct these things? The first thing you do is: well, there's really like three tasks. First of all, you've got to build some weak factorization systems. And then secondly, you've got to build some class of weak equivalences. And then finally, you've got to check that they like line up correctly. And that's usually the hard part. So the weak factorization systems, there's just a way that you always pretty much always do it, and it's always the same. Pretty much always do it, and it's always the same. At least in favorable cases, it's always the same. So you just write down some maps. So these are these I maps, they're the boundary, the inclusions of boundaries of simplexes into a whole filled-in simplex. So this is the case n equals 2. Okay, for i. And then with j, what we're doing is we're removing one of these edges basically, or one of the Removing one of these edges, basically, or one of the faces of the domain. And so you can see that in the second case, we have something contractible here, we have something contractible here. We want to be weak-lungs. That's the way this theory is supposed to be set up. And so we just plug these maps into this machine, and it spits off these weak factorization systems. Okay, it's using this thing called the small object argument, which maybe, if you really remember my talk from before, it was pretty hard to do the small object argument. So I'm going to review how this works. So, I'm going to review how this works a bit. So, when you have classes of maps, you can use them to generate weak factorization systems. And you use the small object argument. I'm not going to say exactly how you instantiate all these things. There's this abstract situation. But basically, the idea is that the construction works by making this really long composition. So, you start with some object, x, like Some object, x, like, which is like the map you want to factor, let's say. And then you have some functor you want to apply. And the functor takes your, in this case, it's going to take your map and make it like one step closer to being factorable or whatever. And there's those comparison maps. The old map will map, or the old object, let's say, will map to the new object. And the idea is you just repeat this over and over again. And so what you're supposed to do is, well, you start with your object x. Well, you start with your object x, 0, that's x. Or, yeah, x0 would be x. And then you apply, to get x1, you apply f, right? So f alpha plus 1 is f of x alpha. And there's a map theta from x0 to x1. And then to x2, you apply f to x1, you get x2, and so on. And you do this for a while. And then you run out of natural numbers, so then you call something x omega, right? And that's like the co-limit of this whole diagram that came before. So that's the first case on this last part. Case on this last part. If beta is omega, you take the colon. Now, in some cases, like actually in the case of composure sets, you'd be done. But in other cases, you're not done. So maybe it's a good idea to apply f again. You get this x omega plus 1. And then you can continue that for a while until you get to another limit ordinal. And you just sort of do this for as long as it seems appropriate. And so you need some a priori estimate on how long is a good enough time to do it for, which is somehow coming from L. Which is somehow coming from LSAN or so you just assumed actually that, okay, this is also like part of the input at this point. And at the end, you just want to take this sort of composition of the whole thing. That's the whole objective of this construction. Okay. Do you know why it's called this? Yeah, the small object is not visible here, right? So it's the map that you want to map out of. Does it make sense? I agree. Because the idea is that at the end, you're going to apply. At the end, you're going to apply, you want to solve some lifting problems. So you have a map into this x gamma. But because it's a collimate, if A is small, then it factors through some stage, and then this will factor through the next stage because of the way we built up. Okay, so really, but that's sort of contingent on the specific choice of F and so on, but you can easily just eliminate it from the kind of task. It from the kind of task at hand. Okay, so I just copied those two lines again for reference. This is actually surprisingly hard to do in lean, even though when I encountered it on paper, I never imagined that there could have been any problem because it sounded very kind of like, I mean, once you're sort of at ease with transponite constructions, it's just sort of like another transponic construction. So, why is it hard? Well, because the reason somehow that it's hard is not that it's like some transponic construction, but there's two Transfinite construction, but there's two different phenomena that are at odds with each other. So, one is that, well, the issue is that you need to build both the objects of this diagram and the maps of the diagram. And they're sort of mutually dependent on each other. So, for example, you cannot just first build all the objects, because in order to develop x omega, well, it's supposed to be the co-limit of the diagram up to there. So, you also needed the maps between all the things that came before. And it's going to be the same problem at every limit ordinal stage. Okay, but so that would suggest that, well, maybe you do some kind of simultaneous recursion where you build both the objects and the maps, right? But then the problem is that the maps and objects are not like easily combinable into one thing. And the reason is that when you talk about the maps into a given stage, they have to come from every previous stage and not just like, I don't know, a fixed object or something. And so if you try to translate this setup. Translate this setup into like a you know a pi over all beta of something, it just doesn't have that shape, and so it's not easy to build it by induction on beta. Okay, so I mean I spent a long time like dealing with this and coming up with various ways to try to do it. And maybe you have to sort of try it for yourself to see that like there seems there's a problem here that is not easy to just make disappear. Could you use alpha and beta? Well, but then the object doesn't depend on alpha. Yeah, yeah, exactly. Yeah, so if you could express it somehow, then, but I don't, yeah. Yeah. Yeah. So it's tricky. At the meeting in Amsterdam, I presented a way that actually worked in the sense that, like, you know, it was sorry free and it worked. That was, here's some other ideas that I'm not going to talk about here. Use some other ideas that I'm not going to talk about here. But I wasn't really happy with it. It used a bunch of equality of objects and stuff, and eat to comms, and a bunch of if-then-else. And it was really pretty gross. And as soon as I think updated Mathlib to a different version of Mathlib, it just broke and I never fixed it because I couldn't figure out what I had written. But just on Monday, some of us were talking about this issue, like the fact that this construction that looks kind of innocuous enough, like turns into this big... It turns into this big problem in Lean's dependent type theory. And then I saw, I guess, yesterday, I was poking around Joel's repository, and I noticed he was trying something very similar. So I don't know if, oh, maybe the last version of this. So I don't know whether there were some sorries near the end. I'm not sure if he ever finished this construction completely or not. But I'm still pretty convinced that this method should work. I haven't also finished it yet. Is that the inductive-inductive thing? No. Oh, okay. No. Oh, okay. So, yeah. So, here's the strategy we want to do. So, the problem is, it's sort of hard to just build something, but maybe we can build a unique thing, and that might be easier. So, the idea is, let's try to describe what kind of thing we want to build. Well, we want to build a diagram where, so first of all, a diagram. And then I want to do the whole thing without talking about equalities of objects or anything like that. It sort of gets gross. So, I'm going to ask for a specific isomorphism from. I'm gonna ask for a specific isomorphism from the first object to X. Yeah, rather than an equality, because who really cares? And I'm also gonna ask that, well, if I look at any stage, any map of my diagram from one object to the next object, it should be isomorphic to the Ada object, the Ada thing that I was supposed to do. So there's like a triangle. Yeah, you get the idea, okay? There's like a triangle between x alpha x alpha plus one and f x alpha, and that's also specified in this data. And then finally, at each And then finally, at each limit ordinal stage, I was supposed to, those diagrams were supposed to be a co-limit. So let me just write down the collection of all these things. And now they form a category because I could look at like maps of diagrams for each specific gamma. And our goal... Is it maps, is it isomorphisms? What's is that? Yeah, you could do either one. So you're gonna need both. Yeah, so maps just that commute with everything and that commute with these isomorphisms as well. Commute with these isomorphisms as well, and like the naturality of f and blah blah blah. And then you want to also want isomorphisms. You're not just determined every object up to versus one. Yes, that's exactly the idea, right? So the point is, rather than just being, you know, trying to construct a diagram of this sort, now we're going to sort of strengthen the inductive hypothesis and prove that there's actually an essentially unique diagram of this sort. Okay. And why does it help? So I didn't try to draw. So I didn't try to draw this is a thing called a huge diagram. I don't want to just draw into the board. Is essentially unique up to isomorphism or unique up unique workflow? It means what does essentially unique mean? Yeah. It means the category of these things is inhabited and any two objects are uniquely isomorphic. Okay, yeah. Oh, but maybe, yeah, sorry. Is this your question? That, like, also. Also, all the parallel maps are equal as well. So, like the whole category is contractable. Just a contractable recording. Yeah, yeah, yeah, yeah. Yes, that would fall from the last axiom. The last axiom? All the limit coordinates have to be called as the previous name. Well, and you also need the, you know, well, like, these things being isomorphisms, sorry, this being an isomorphism forces like all the naturality squares to. Yeah. Okay, so what's the idea? So, the problem is, let me try to first. The problem is, let me try to first just do this one without trying to prove this. So I want to know that for every gamma, there is some diagram of length gamma. Okay, so if I'm doing this for a successor, like if gamma... I said proof by induction. Yeah, proof by induction. So they're going to prove this statement by a well-founded record, but maybe just induction on the... Or maybe just notion on the order type of gamma or something. So if gamma was alpha plus one, yeah, then it's easy because you just take the previous date and you attack on the thing that's just attack on. But what if gamma is a limit, or denote beta? I don't know why I changed the name. So the point is, there's no reductive hypothesis you can use because you need like the whole sequence up to like you need to take the union of all the diagrams that came before. And if you just have Before. And if you just have a diagram of length 0 and a diagram of length 1, another diagram of length 2, and so on, maybe omega, maybe gamma is just omega. Well, they're just unrelated diagrams, right? So what are you going to build for the diagram? I mean, you need a diagram that goes on forever, and then you want to attach the covalent to it. And how do you get it? Right, you can't get it. But if you're inductive. If your induction hypothesis includes the fact that the diagrams are unique, then you can make the following argument. So let me look at my diagram. I know I've got some diagram of length 3, some diagram of length 2. And I can also easily prove that if I took my diagram of length 3 and then just chop off the end, it's going to be another good diagram of length 2. And that means my inductive hypothesis says that all these guys are isomorphisms. Or there are unique isomorphisms that fit here. And likewise, there are unique isomorphisms that fit everywhere here. That play everywhere here. So, this claim you're proving by induction is that there is an essentially unique good diagram. Yes. Yes. And I'm going to use the essentialness of the previous stages to prove the existence of the next stage, because now I can just sort of take this zigzag, like this is a thing, a diagram that I can continue all the way up to, but not including gamma, and then I can attach a less thing. And that is going to be how I built the thing about the gamma. And so this argument doesn't involve any, like, I don't know. It's really actually very clean. It doesn't involve any strange. Actually, very clean. It doesn't involve any strange stuff that you don't want. Yeah. Okay. Oops, here we go. So you don't take the co-limits of the object, there's no point. Oh, the co-limits of the objects. I'm expecting you to just take the limits under all the objects. Yeah, you probably could do that too, right? Is that easier? It's probably harder. I don't know. It's just a trick I've seen in number theory. Yeah, yeah, yeah. No, I've seen that. If you're tempted to pick one, you just take the limits under a bunch of isomorphisms. Yeah. Either one would work fine. You need the co-limits, anyways, because you're. You need the co-limits, anyways, because you're assuming there are all sorts of co-limits. What are you suggesting? Just to take the co-limit of this triangle thing? I mean, just to find the zeroth object, just be the limit of a diamond model. Yeah, like the co-limit of the diagram that consists of all these isomorphisms. You could just take the co-limit of the triangle, this kind of limit triangle. Yeah. Yeah, probably. But then you still need to fit in a diagram with the rest of the game. This is varying gammas, so there's variant gamma. Is varying gammas and there's varying categories. Yeah, you have to like paste them all together. I think it's easier to do it this way, but okay. Okay, so this is this is to go back a bit, this is explaining how we like can do this small object argument more efficiently than before. Okay, so now once you've done this, there's a bunch of simplicial homotopy theory nonsense. And really all I'm gonna say about it is that it's like not really that hard. Like, not really that hard. And so, but I'll try to point out the parts that are more annoying. So, we need to check that the model, that these refactorization systems are like the things that we expect to get. So, the first thing we expect is that the co-fibrations are the monomorphisms. This is, I mean, the cofibrations are monomorphisms because you saw that, like, the generating ones were monomorphisms. And to go. And to go the other direction, you need to understand the way that this fact that every simplex of a simple set is uniquely the degeneracy of a non-degenerate simplex. That's the content of this lemma. And that's something about in the category delta, every morphism of delta factors uniquely as a surjection full nine injection. So it's really elementary stuff. The next lemma is this business about the pushout product. I'll just draw a picture. If you have a If you have a one simplex, the inclusion of a boundary is this thing that builds these open hordes or open cylinders or what have you. So here's the inclusion of a boundary, and then the box is this notation for the push-up product. Take the inclusion of one endpoint. That's going to be something that looks like an open sort of box including into. Sort of box including into a product of two one simplices, which looks like this. So you need to understand or figure out how to get from here to here by like gluing in this simplex probably and then this simplex. That looks like a guru. So this is again, this is some bunch of combinatorics. It's written down in many places. I don't actually know. I mean, it's simple enough on paper, except that it's like full of notation. So I guess maybe Alina would also be. I guess maybe Alina would also be simple, but full of... I don't know if there's a smart way to organize it, or if you just have to kind of get your hands dirty. But it's not, there's nothing deep at all about this physics. Okay. And once you know these facts, they sort of tell you that the covariations and anodyne covariations interact with each other in the ways they're supposed to. And then you can start applying a little bit of model kind of theory and things like that. If you have a con complex, which just means something to the total object is a. The total object is a vibration. Those are sort of like the good simplicial sets. Then they're sort of closed under exponentiating by anything because you're sort of transpose and pushed in get back here. So that's more or less all there is to say about these factorization systems. And then the really harder stuff is to do with these weak equivalences. So how do we define the weak equivalences? There are various ways you can do it. There are various ways you can do it, depending on, like, basically, depending on what facts you already know. We currently basically know no facts about the theory. So, this is like the probably most efficient definition in that situation. So, I just said that, well, oops. Oh, here we go. Kahn complexes, they, because they're now fibrant objects and we also understand how covariations work, it means we've sort of like the Hobotopy theory of con complexes kind of under control at this point. At this point in the argument. And so that means if we can test whether maps are Wiki complexes by, well, first of all, we know that if we map them into con complexes, then the result is a con complex, so they have a good Homotopy theory already. And so then we can say that the, you know, f is a weak equivalence if the sort of hold back whatever restriction of maps along f is a homotopy equivalence for any concomitant stacks. Homotopy equivalence means like a thing with the interval delta one. Like a thing with the interval delta one used as an interval, and you can find home to be. And it's an equivalence relation already because you already understand your business with co-vibrations and so on. So yeah, by definition, okay, that's a little bit of a why. It's by everything I came before. But the hard part is that, well, sort of in this sentence, there's implicitly two definitions, right? So you would really expect the con complexes to have homotopy equivalences of salient equivalences, but I also just gave you a different definition of them. But I also just gave you a different definition of them because of the things that when you map into other con complexes, okay. And so you need to sort of resolve this, and the way to do this is, well, one way that I suggest is to use this cons x and fitness monster. Okay, so I'm going to say a little bit about what this is. And again, I've written a bunch of stuff, but I'm just going to draw a bunch of pictures as well. But in the meantime, you can't really draw pictures, right? So you better figure out how to write it now. So this is. So, this is the basic operation here: we have a simplex and we want to subdivide it. Because somehow, what's the big problem with some special sets? Well, you often don't have as many maps as you want, but you could fix that by sort of subdividing your sufficial set, and then there will be more maps. And so, the picture of the subdivision, let me take n equals 2. So, I could take a simplex here. 0, 1, 2, that's delta n. And then I want to tell you how to subdivide it. So it says here it's the nerve of the postet of non-empty subsets of n. So the case emphases are these chains. So I just wrote it down because you can see visibly that it would be roughly as long in me to write it down. But the picture is the thing that you want to understand to me out. So it looks like this. And where now the vertices, so if k is zero, then I'm just giving you a single non-empty subset, right? So I could label these by subsets. So how does it work? Zero, this is zero, one, and two. And then along all the edges and in the middle, we're just gonna have like which vertices appear on that, on that edge or face. So this is this. So this is the subset 01, 1, 2, 0, 2, 0, 1, 2. And all the arrows point to like bigger subsets. So that's what the subdivision does. And then I need this other construction called the last vertex map. So with each of these subsets, I could take the maximum of the set. And in this triangle specifically, I have each of those numbers appearing, like each number appears. Appearing, like each number appears as the maximum of, sorry, zero appears as the maximum here, one appears as the maximum here, and two appears as the maximum there. So there's a map that goes back this way that kind of takes this triangle, that hits the entire triangle here, and then everything else is going to get collapsed in some fashion that's hard to understand. It's really important to know there's no map that goes this way, even though in topology there would be, and that's sort of the whole point, is that there's no map that goes this way. So, on geometric realizations, this would be a homotopic equivalence, but not a homomorphism, right? Like it's collapsing. Yes, exactly. Yes. There's a different homomorphism. Yep. I mean, the homotopic, of course, because it's attractible. Okay, so that's sort of all the contours we need to write down this map. So then we define this x thing as sort of the right adjoint. It's just, I wrote it by the formula, but it's the right adjoint that comes from. It's the right adjoint that comes from the UNITA restriction or whatever bond. The subdivision functor, and then you do another one of these transfinite constructions, and you get this thing called x of theory. And all these transition maps, well, okay. I mean, you just sort of work out if you have a map from the subdivision of delta n back to delta n, that gives you a map from delta, like hom from delta n to x to half division from delta n to x. That's these maps. So these trends are my Trying to automate constructions. I understand that to actually construct or show that such a thing exists, you need to actually do some work. But presumably, it's possible to write down the universal property of just this co limit at the very end. Yeah, maybe it was like the universal. So the universal thing where x is it the like initial algebra? Yeah. So we were actually talking about. Yeah, so we were actually talking about this on Monday. It's very confusing. There's like different maps you could use, because another map you could use is the functor x applied to row x as the second map. And now I'm very confused about the relationship. Like those are not equal maps. I'll talk to you after. Okay, great. Yeah, I'd like to know that. It's very special in the case that the normal zone mega, you can just like use the university property before a construction. But my own rules, because the diagram depends on the previous stages, you can't just say the universe. In the previous stages, you can't just say the universal property. Right, if you need to have omega in the indexer diagram, you need to do this transmitted construction in order to get the diagram to say the universal property. Yeah, so the equivalent small object argument doesn't have any universal property, except maybe if, yeah, it's okay. But I guess this one might still. Yeah. Okay. But in the end, we're kind of going to use the way that we constructed it this way. Because we want to do the same sort of argument where, you know, if we map something into the co-limit, then Something into the co-limit, then it maps to some finite stage, and then we solve a problem in the next stage. So, this is really what we use, anyways. We need to know what's the co-limit of some diagram that, if I mean that definition before of a good diagram, is actually like that, that's usable as the definition. Okay, and now there is another just pile of complex works you need to do. So, I've summarized many steps here, but the idea is that, well, this mysterious X thing is somehow This X thing is somehow what you should imagine is like adding solutions to some lifting problems that weren't there but in a very like controllable way. And so the result of doing it omega times is that you end up with a con complex which you wanted. That part's easy, but the hard part is that the composition of this whole thing, right? So x in the end is going to map to the colonate, which is x infinity. Is x infinity, but that's also weak equivalence. And for this, you need to understand this picture a lot. So, for example, one thing you need to understand is: suppose that I erased this edge and then I subdivided. Yes, I'm sorry, maybe it's been purposeless diagram. Suppose I subdivided these guys as well. As well. Okay, that I can map here, right? It's the subdivision of the horn, and I can map it to the subdivision of the whole thing. You need to prove that you can take this thing, you can take this map and build it as by a sequence, as has been a sequence of cells. So probably it looks like I should start by adding a cell like this. When I say a cell, in this case, I mean like a vertex. A vertex, sorry, a simplex together with like one new face of the simplex. So I'm going to add this first, and then I'm going to add like this guy together with this simplex, and this guy together with this simplex, and then I'm going to keep doing that until I get here. And it's some comatorical fact, commentorial fact, that this works. You don't prove it by Achilles Hobbit space or anything. You just figure out the formula somehow and you write it down. So that's that's one of this bunch of contours with some division of simplicity. Bunch of contours with some measure of simplicity. It's not, I think it's not the only thing you need to prove, but gives you the flavor of the argument that is needed to prove this. And once you've done this, then the rest of the argument is sort of like just, it's the kind of like general, general model category theory logic that you do, but you sort of where you have this partially constructed thing and sort of want to finish the job. Yeah, so this is pretty. Pretty much a very sketchy summary of the entire construction of this model structure of superficial cells. And yeah, what I want to say here is just that it's doable. And maybe, especially if you find someone who's maybe unlike Brendan, from what I understand, really loves the comic 4x sympathies, then you could probably get them to do this. So then there's one more, yeah. So then there's one more, yeah. Well, there's two more parts, okay? So I said I would skip the business about transferring the model structure of television spaces because it's really very, it's just general theory of model categories because of like it's a just very favorable situation. And then finally, you need to prove that this junction is equivalent to a one. So that's telling you that like you can sort of reduce questions about how blogging spaces to sufficient stats. It also tells you that That, like, simplicial homology and singular homology, they're going to be the same because, well, it says some unit of some junction is a home topic component, and you apply homology to that, and we're done. So, that's mostly just sort of general machinery, but then you need to, at some point, use some input about topological spaces. And this is like the minimum input that you need. Well, so the geometric consideration involves things like, okay, simplexes are complex. So, if I want to build a homotopy between two things in a simplex, I can use a straight-line homotopy. That's the kind of level of Straight time mode will be. That's the kind of level of thing I mean by geometric considerations. And then this is the only thing, this is like the hardest thing you need. That if I have a topological simplex, like in some vector space, then, and I have some open cover, then if I subdivide it enough times, then, well, the simplices sort of get smaller, and so eventually all purposes are in some member of the open cover. And so I understand, Brendan actually already proved this, right? So I think you mentioned. I think you mentioned this that I proved an abelian version of it. It's a little controversial. The barycentric subversion turns a chain into a sum of chains, right? Yeah, yeah, I see. Yeah, it's probably you could have factored your proof through identical proof of this factor. Yeah, yeah, exactly. This is the Lebesgue number argument together with some annoying estimate on the diameter of a simplex that I never bothered to understand. Okay, yeah, that's basically. Okay, yeah, that's basically all I have to say. That this is something that you can do, and maybe we should just do it. Great.