Thank you very much, Nathaniel. Can you see this? Yes. Okay, wonderful. Thank you. I almost missed my own talk because I forgot about the daylight savings. And when I put it in my calendar, I think there was one more hour of time difference. So I just jumped to it last moment. Sorry about that. All right. So what's this about? It's about the entropic optimal transfer problem. And at the end, I'm mostly focusing on Mostly focusing on the convergence of the so-called synchronous algorithm, which is an algorithm to compute the solution of this problem. And basically, the theme will be: you know, try to communicate that whenever you have a stability result for this problem, you can expect to also have a convergence result for that algorithm. So, I'll be talking about these two aspects and how they go together. Okay, so I'll be drawing from various joint works with Espen, who has left academia, Stefan. Who has left academia, Stefan, who is now at ETH, Promethe who is at MIT, and Johannes, who is there in Banff and otherwise is at Columbia. Okay, this is just for reference. These are various papers that results are drawn from. If you're interested in an introduction, then you may look at the lecture notes, which are on the homepage. They come from a course taught in Paris last fall. Okay, so just to fix some. Okay, so just to fix some notation for the usual Monchkantrovich optimal transport problem, most of the time my marginals are called mu and mu. They live on spaces X and Y. There is a cost function C of X Y. It is continuous throughout the talk. Okay, and as you know very well, the problem consists in finding a coupling pi of those marginals, minimizing the transport cost. So transport cost we integrate over the cost function. integrate over the cost function with the coupling. Here I call C0 the optimal cost and my notation for the couplings is capital PI of mu nu. Okay so much of this entropic stuff is driven by applications of optimal transport in the last decade or so where people are mostly interested in computing Wasserstein distances for relatively high dimensional problems. High-dimensional problems, for instance, in image processing. And the reason why this has become a thing recently is the existence of approximations to actually compute the Wasterstein distance in a reasonable time, even for a large number of data points. And one of the key approaches, or probably the key approach in this realm, is to use the entropic regularization. So that's what I'll be talking about in the rest of my talk. Okay, so what exactly is this entropic optimal transport problem? Essentially, it's the exact same problem as before. This one here, Monch-Kantrovich optimal transport. And then there is a panelization or regularization term, which is a number epsilon, the regularization parameter, times the relative entropy between the coupling and the product of the marginals of the coupling. So information content. Of the coupling, so information content. So, P capital P in this talk will stand for product, P as in product of the two given marginals, and H is the KL divergence or relative entropy, which is given simply by taking, you know, integrating the log of the relative density with respect to the first argument, or integrating x log x of the density with respect to the second argument, the reference measures. Okay, so when this problem is finite in the sense that the inf is finite. The sense that the inf is finite, so there exists some coupling with finite cost, then there will be a unique minimizer denoted pi epsilon, and it's automatically equivalent to p in the sense of measure theory. Okay, so this is of course a linear problem. We add a strictly convex problem, that's where the uniqueness comes from, from the strict convexity. So, generally speaking, I would say this can be seen as a trade-off between the two terms. Basically, when epsilon is very large, you're mostly trying to minimize this guy. This has a unique solution, so the unique pi with zero entropy is p itself. Okay, so when epsilon goes to infinity, the solution should converge to the product p. That's the less interesting limit for the application that I just mentioned. The more relevant one there is when epsilon is small, then this should become not very important and you're most. Become not very important, and you're mostly trying to minimize the optimal transport cost. So, then we expect that pi epsilon would converge to an optimal transport. Okay, that's a very important subject, but it's not the subject of this talk. Okay, so in that sense, pi epsilon is some sort of interpolation, which is illustrated on this slide here, which is basically copied from the recent book of Perry and Kuturi on computational optimal transport. So, what are those pictures? Transport. So, what are those pictures? There are two marginals here. The two lines are the two one-dimensional marginals. And then, this is for quadratic cost. And the solution is showing the entropic optimal transport for various parameters of epsilon. So here, the left-hand side, epsilon is large. And indeed, you see the solution basically looks like the product of the two marginals. And then going to the right, epsilon gets smaller and smaller. And as you would expect, so. And as you would expect, so we know here the optimal transport would be just the Bernier map without entropic regularization. And so, as epsilon gets small, you see that the solution, you know, this starts to look like the graph of what apparently is the Bernier map in this specific context. Okay, some high-level properties. So, for what I'm, you know, gave as a motivation, I guess, the most important property of EOT. Important property of EOT is that you can find the solution through Sinkhorn's algorithm, or also called IPFP. I will detail the algorithm later, but I guess from the applied point of view, the main thing is that you can implement this very efficiently on GPUs because essentially you have to be able to carry out matrix vector multiplications. And that's something that computational people happen to be very good at. Okay, so in that motivation, the basic idea is you're going to solve. Basic idea is you're going to solve the EOT problem, the entropic problem, for some small epsilon to find an approximation of optimal transport and the optimal transport cost, in particular, maybe some Wasserstein distance that you're trying to compute. Okay, so on the flight side, there is a pretty vast literature doing this type of thing in the last few years. Okay, even without sending epsilon to zero, EOT has some desirable properties. Maybe it's much smoother than the usual. Maybe it's much smoother than the usual optimal transport, and various applied people have started to appreciate those properties and exploit them, essentially, trying to define quantities similar to optimal transport quantities, but keeping the regularization term in there and using that object directly. In particular, one thing that's very nice is that the EOT cost would give a gradient that can be used for gradient descent, whereas that doesn't really work out in optimal transport. Okay. Transport. Okay, then completely different direction. So EOT is actually an incarnation of a much older problem that some of you are familiar with: the so-called Schrodinger-Bridge problem in its static formulation. So the EOT problem I wrote down here, this can be reformulated into a problem which is without the integral term, but absorbing the integral term into the measure. So then the pi epsilon is also the argument. Also, the argmin of this entropy minimization problem, which is over the same set of couplings, but now the p has been changed into a new reference measure r, and r has this Gibbs kernel relative to the product of the marginals with the e to the minus c over epsilon. So this is just some simple algebra where you imagine if you take the log of this, somehow you get the c in front, more or less. Okay, and so of course this falls into the general reality. Course, this falls into the general realm of entropy minimization problems, which have been studied from several perspectives. Okay, so I will not be talking about the Schrodinger-Bitch problem in this talk. There is a paper by Leonard, which has a very long list of references. And I guess Leonard's survey is sort of the classical reference in this field. Okay, so now what am I going to talk about? These two sections. These two sections are about results on the stability of the value as well as the solution itself relative to the marginals μν. These first results are purely qualitative about weak convergence, and we want to be able to say something more quantitative, and that will be done in Wasserstein sense. Okay, and then after reviewing those results, I'll explain or recall Sinkhorn's algorithm and use the stability. Algorithm and use the stability results to gain convergence results for Synchorn's algorithm. So, in all of what I'm doing, I'm especially interested in costs that are the quadratic costs, because that's the most relevant one for applications. And I'm interested in marginals, which should be hopefully general enough to cover Gaussians and things that have Gaussian-type tails. Okay, some of what I'm going to say is. Okay, some of what I'm going to say is pretty straightforward when the domain is compact, but then obviously you cannot even cover a Gaussian. Okay, so I know some of you have seen this geometric approach for the weak stability results. So I'll go relatively quick over this and mostly mention the result. So here what we try to do is we try to work along the lines of optimal transport that are probably familiar to many people in the Familiar to many people in the audience. So, we want to mimic the techniques around C-cyclical monotonicity. So, we like to make this definition here, which should have a similar flavor. So, what it is, is that take a coupling, take K points from your space, finitely many points, and look at the product of the coupling along those points. And now compare this number with what you get when you permute the destination points. The destination points in the other density. Okay, so I'm not going to look at the support because the support is basically always the same as the one of the product measure. There's no information. Instead, we look at the density. And what we want to see is that this very specific relation holds. Now, it looks different from the usual cyclical monotonicity, but the intuition is the same. Okay, this is like a first order condition of optimality for those frozen points. So if you were to solve the problem in a discrete way, To solve the problem in a discrete version where you have empirical measures and the supports are the xi's and the yi's, okay, then essentially this is a calculus problem. You think you should find the first order condition for the optimality, right? And then, you know, after a bit of math, this is the equation you would get. Okay, so in that sense, it's the same intuition as an optimal transport. Okay, this says that if you permute the destinations, you will not improve the cost, the discrete cost of your optimal transport. Discrete cost of your optimal transport. Okay. Another view of this condition is you can rewrite it with this auxiliary measure R into this form here, and then this turns out to be equivalent to the fact that the density of the coupling happens to be a product, has a factorization into two functions, which are functions of x and y alone, the marginal variables alone. Okay, and there this connects to some results. Disconnects to some results that are classical in the literature on Schrodinger bridges. So it's known that this relates to optimality. What I think is new is that we try to really use this equation as a tool in the same spirit as C-cyclical monotony is used in optimal transport. And that's evidence maybe on this slide. So it's relatively easy to see that when the problem is finite, so there exists a coupling with finite. So, there exists a coupling with finite entropy cost, then cyclical invariance, the thing we just defined, is equivalent to pi being the optimizer, the unique optimizer. Okay. We can push it a little bit further to a case where the problem maybe isn't finite, so there may not be a coupling with finite cost. In optimal transport, it is known that even in such a situation where basically cost minimum Where basically cost minimization doesn't make sense, the geometric notion of optimality is still meaningful and may single out a unique optimal coupling, which in McCann's work, that's basically a generalization of the Bernier map. Somehow to find the Bernier map, even if the OT problem is infinite. And something very similar succeeds here. So it turns out that the uniqueness of the cyclical invariant coupling. Cyclical invariant coupling is always true, even if the problem is infinite. And for existence, I want to go through a stability result, which will be of independent interest. And again, this, I think, is motivated from how you work. I don't know if you follow some introductory book on optimal transport. Probably the existence of cyclically invariant sets, that sort of stuff, the way it will be argued is you start with a discrete approximation. In the discrete case, you do things by hand. Case, you do things by hand and then you pass to the limit. Okay, so we do the exact same thing on this slide. So here I imagine that in addition to mu nu, I have a sequence of marginals converging to those mu nu in the sense of V convergence, so relative to bounded continuous functions. And maybe we can ignore these things. Let's just say there is a fixed cost C and a fixed epsilon, which you can also take to be one. Okay, so what we want to say here for stability is that if the marginals converge, If the marginals converge, then also the solutions converge. And instead of just talking about the minimizers, I will phrase it in this geometric language of cyclical invariance. So then the desired statement is that if the coupling is, if pi n's are cyclically invariant couplings relative to their data, so these are given, then necessarily a weak limit of pi n exists. I call it pi here. I call it pi here, and this guy will be cyclically invariant with respect to the limiting data μν. So, if we know that all the problems are finite, this simply says that the unique optimizers converge weakly, but it is meaningful even beyond that. So, in the case of an infinite problem, what we can clearly do is take any approximation that you like, for instance, IID samples. So, then mu n, mu, n. So then μn would be empirical measures with finite support converging to μ. Of course, for the empirical measures, the problem is finite because basically on those points, the cost function is just a bounded function. And then the theorem would guarantee the existence of a limit pi, which is cyclically invariant. And according to the previous slide, that would be also the unique cyclically invariant coupling. Okay, so stability implies existence in this. implies existence in this context. Okay, nevertheless, for later on, I'll mostly care about the stability result and not so much about this infinite value function case. There is a deep convenience here though about the geometric argument that we don't have to worry about integrability properties. Okay, this is a purely local approach which completely circumvents even talking about the integrability or the total cost of things. Okay, so. Okay, so more or less in my mind, this weak stability always holds. Technically, there is a caveat here. It says for Euclidean spaces, in reality, the paper works in any setting where you can differentiate measures in the usual sense of Lebesgue's theorem. So these are actually more general. I think more or less, this generally works in finite dimensional settings, reasonable manifolds, and so on and so forth. It may not work in infinite. It may not work in infinite dimensional settings. So that's just a technical limitation of how we do these things. And I'm not going to talk more about where this really comes from. Okay, so more or less, if marginals converge weekly, solutions converge weekly, okay, without any quantification of the speed in that result. Okay, so now we want to say something in Walserstein sense, something more quantitative, and hopefully also get a convergence rate. Okay, so for that, the setting and Okay, so for that the setting and the techniques would be kind of different. Before I head there, are there any questions so far? Great. Okay, so here the paper slightly different settings with n marginals, capital N marginals. You can just assume we have two marginals, which are now called X1 and X2 instead of X and Y. Okay, so here this is going to be a Wasterstein type setting. So there'll be Wasterstein type setting, so there'll be growth assumptions on the cost, growth of some order p, p between one and infinity. The marginals now are assumed to have finite p moments. What we try to say is that if the marginals are closed, then value functions and optimizer are closed. So we have to say how exactly we measure the distance between the marginals. There is more than one way to do this, just in the same way how when you have a norm on R, there is more than one way to. R, there is more than one way to form a norm on R D, right? So, here, just for notational convenience, we choose this L P type norm. This is only, this matters only for the constants. Okay, so essentially, this is just p-Wasterstein distance between the marginals, two sets of marginals that are called mu i and mu i tilde, respectively. Okay, so we want to have a result. So, here things are stated in the case where epsilon is one. Where epsilon is one, that's not really a restriction as long as we're not sending epsilon to zero, because basically by just dividing the original problem with epsilon, we get rid of it. Okay, if you want to reintroduce the epsilon, you just multiply epsilon goes here, epsilon goes here, epsilon goes here, right? You just, that's just a new cost function. So that doesn't really make a difference. All right, so what are the results? So here, this is the qualitative. Is the qualitative result? What it says is that if the marginals converge in Wasserstein sense, then the value functions converge and the optimal couplings converge in Wasserstein sense. So it's sort of the expected result, but it's not necessarily an obvious result. And in this one is proved with a completely different type of argument than the previous stuff. Then on the quantitative side, so here Then, on the quantitative side, so here is the first part of it. This says that the value functions are Lipschitz continuous relative to the marginals in Wasserstein sense under a condition on the cost function. Okay, there's a condition on the next slide called al which also determines the Lipschitz constant. Okay, so what's the condition? The condition is this one here. So, for the two given sets of marginals, Sets of marginals, this inequality should hold whenever pi is a coupling of one set of marginals and pi tilde is a coupling of the other set of marginals. So this inequality is clearly satisfied when C is Lipschitz with constant L. Then this is actually satisfied for any measure pi and pi tilde. Our issue is that we are most interested in the quadratic cost function, which of course is not a Lipschitz function. Of course, is not a Lipschitz function on Rd, and then it's not immediate how to do this thing. Okay, so the basic idea here is that because the marginals are fixed or at least restricted, we may be able to get this inequality by somehow pushing stuff into the marginals and allowing functions which are non-libs. Okay, so indeed this works out for the quadratic cost as is exemplified here when p is 2. So here the constant is something Constant is something which involves the second moment of the marginals. Okay, but then we can move away from the Lipschitz setting and still have a Lipschitz type property here, which indeed translates to a Lipschitz property of the value function. So for us, this is a pretty good success relative to the usual Lipschitz setting. Okay, and so the AL condition holds more generally when costs are essentially products of two Lipschitz. Are essentially products of two Lipschitz functions. I won't dwell too much here. So I'm just emphasizing that it works for quadratic cost. And I think the updated version, the next revision of the paper will have a more general result than what is on this slide anyway. Okay, let me instead move on to the stability of the optimizers. That will be important later. So this is the quantitative result for the stability of the optimal couplings. What we're trying to say is that if the marginals converge in last Is that if the marginals converge in Wasserstein at some rate, then hopefully the optimizers also converge at some rate, and this is even well, this is a global inequality here. Okay, so delta is the distance between two sets of marginals in Wasserstein p sense, and we try to quantify the Wasserstein distance between two the corresponding optimizers. Okay, so here they are in Q-Wasserstein, but one example would be just take. But one example would be just take q equals p for the moment. It's just something a little bit more general. Okay, so what the result says is that under some conditions, there's an estimate with three terms. Okay, and you can take this is the number of marginals. So if you want, that's just the number, two, for instance. So there's a linear term, one over q and one over two q term. So q maybe the most interesting case is q equals two. So basically, this is going to look like a one over four Holder type result. One over four holder type result when you restrict it to a bounded set. The constant L is the same constant from before, but now there is an additional assumption on the marginals, which is something like a transport inequality. Are there questions about this result? It's a bit hard to read, but yeah. Hi, myself. Is it what's the dependence on the regularization parameter of the constants how they with the here epsilon is one okay and the constant let me talk about the constant a bit later maybe okay essentially is linear in the regularization so i'm thinking of the constant as being linear you know depending on c divided by epsilon in a linear way Epsilon in a linear way. So, this constant here comes directly from the assumption. So, the assumption is that at least one of the two sets of marginals satisfies a transport inequality of this type. So, we go through entropy calculations. So we need to control Wasterstein distances in terms of entropies. And the assumption here is that for one set of marginals, for any two couplings, there would be. Couplings, there would be such an inequality. So, Wasserstein distance between couplings can be controlled by essentially some power of the relative entropies. And so here we're just using or more or less directly applying an existing result of Bollet and Villany, that indeed this is satisfied as soon as the marginals have good integrability. So there's no specific form. This is not an inequality. Specific form. This is not an inequality which requires a Gaussian type something or anything like that. Okay, so the lemma here says that take the marginals mui, as soon as they have an exponential moment of this type, then necessarily this is satisfied and you know you basically know the constant. Okay, so we are mostly interested in quadratic cost and sub-Gaussian marginals. So then you want to play this with q equals 2. With q equals 2. And you see that the Gaussian or a Gaussian tail indeed satisfies this for sufficiently small alpha. So it's crucial here that this says for some alpha, which can be close to zero if necessary. Okay. And so in that context, this would be like c squared. So this would have the one over epsilon, which is in the exponent. So that's bad. But then there is a log which brings it down. There is a log which brings it down from the exponent, so that's why I'm saying things are kind of linear in epsilon. Marcel, in this TQ the entropy is the same, in one over Q and one over two Q, right? The first and the second term are they differ just by the power. Correct. Yeah, I mean, this is a global inequality, right? It's not just when the entropy is small or something like that. It holds for small and large, which is part of why. Which is part of why this is also a global inequality. It holds when delta is small, but it also holds when delta is large. For the local kind of regularity, we are probably most interested in the small delta. More questions? Okay, so bottom line is we have some results which tell us that the solution. Which tells us that the solution and the value are stable with respect to the marginals, and there's some quantification of that which is reasonably robust. It works with unbounded supports and quadratic costs and also various other costs. Okay, so now let me talk about synchronous algorithm. Synchron's algorithm can be stated in two ways. One is a primal way, which talks about couplings, and one is a dual way, which talks about potentials. Which talks about potentials, which I will introduce a bit later. So I'll start on the primal side, just because we have been talking about couplings so far. Okay, so now we're back to two marginals. Here they're called mu1, mu2. Later, they might be called mu mu. So what's synchordon's algorithm in this primal formulation? We start at r. Remember, dr is simply, well, or up to a constant. Well, or up to a constant is e to the minus c when epsilon is one, say the product measure of the two marginals. So that's sort of known, and that's the only point where the cost function is going to enter the game. And once we define this as pi naught, okay, we define a recursion, namely the rather nicodem density between or relative. Between or relative to the previous iteration. So here x, this is x1, x2. There are two variables. It is given by a function actually only of one variable. So for instance, when n is one or odd, we take the Radharanikodim derivative between the correct marginal that we want to match, the mu1, and the first marginal of the previous iteration. Okay, then for the next n, we repeat. Okay, then for the next n, we repeat the game, but here we're going to take the second marginal instead of the first marginal, and then we go back to this and to this. So, what that creates is a sequence Pn where alternatingly, so for odd indices, the first marginal is the correct marginal that we want to have, but actually the other one is not going to be the correct one. Whereas for even n, the second marginal is fitted, but the first Marginal is fitted, but the first marginal is not. So it's like an alternating projection type of thing. Indeed, one can check this is just the explicit solution of an optimization problem, namely pi n minimizes entropy relative to the last iterate over the set of all measures which have the correct first marginal but arbitrary second marginal. So that's when n is out and when n is even. n is odd and when n is even similarly this would be a star and mu2 okay so alternatingly you make one marginal correct but the the other may be not correct it's just from this definition it is apparent that the density of pi n relative to r is a product of you know on the one hand side functions of x1 on the other hand functions of x2 so that the whole thing will have a factorization function of x1 function of x2 which x1 function of x2, which actually implies that pi n is cyclically invariant with respect to its own marginals. So that's one way to see that one could interpret pi n as a solution of an EOT problem with the exact same cost function, but with different marginals, and namely just whatever are the marginals of pi n itself. Okay, so this maybe surprisingly to me hasn't really been highlighted in the literature, but it clearly gives us an avenue. But it clearly gives us an avenue to talk about convergence, which is that if we knew that the marginals of pi n converge to the correct marginals, μ1, μ2, then if we have a stability result, it will necessarily tell us that pi n will converge to the true coupling that we're trying to find, the true optimizer. Is that thought clear? I'll take it as a yes. Okay. As a yes, okay. So, for the synchron mark, so for the marginals of pi n, which here I call synchron marginals, um, the convergence actually is long known and very robust, very straightforward. Okay, so here the only assumption is that the cost is finite, the total cost. And so we know that an optimal coupling exists. It's called pi star here. And then, in even in the old papers from the 50s or something, you can find this very nice identity. Find this very nice identity, which is based on a very simple entropy calculation. So, on the left-hand side, it has the entropy of the true optimizer relative to the nth iterate. And it says that that's equal to the entropy of the true optimizer, okay, some number, it's a constant for us, minus this sum of entropies between the iterates and the previous iterates. Okay, now one fact about entropy is that Entropy is that h is a greater or equal to zero no matter what. Okay, so basically, this says that a positive quantity is a fixed constant minus this. So clearly, it cannot happen that this sum converges to infinity as n goes to infinity. It just has to be finite because otherwise this entropy would be a negative number, which doesn't make any sense. Okay, so simply from this identity, one can observe that this skies being summed up, they have to Skies being summed up, they have to converge to zero. Okay, so this converges to zero. Then, okay, there is another simple fact about entropy, which is that when you project to marginals, the entropy always decreases. That's called the data processing inequality. Okay, so when you take the marginals here of these two couplings, something peculiar happens. For instance, when n is even, okay, now take the first marginal here, i is one, then for the second guy, we know it has the Then for the second guy, we know it has the correct first marginal, so that's the mu1. So this actually gives us that this entropy here converges to zero. And the same is true the other way around for I equals two. Okay, so there is this sequence where one marginal is already the good one, but the other marginal is wrong. However, the entropy of the wrong marginal relative to the correct one converges to zero. Okay, and it's known that when entropy is wrong, That when entropy converges to zero, it implies convergence in total variation. Okay, so in particular, this implies that the marginals, the synchron marginals, converge in total variation to the correct marginals. Okay, that's a summary of what I just said in lengthy words. And this holds true, like just without any condition, the cost could even be infinite, discontinuous, whatever you want. Okay, so that was long known. Qualitatively, now, surprisingly, recently Now, surprisingly recently, Sir Flavien observed this result that actually, maybe you can sort of guess it from here, from the fact that the sum actually remains finite, that there should be a convergence rate. And he showed that the one over n rate is always true with this explicit constant in front. Okay, so to summarize, the pi n's are optimizers of certain EOT problems with wrong marginals, but now we know that the wrong marginals actually That the wrong marginals actually converge to the true marginals, which sorry, they're now they're called mu nu again instead of mu1, mu2. Okay, and so we can think of the conversions of this as a particular application of stability results, which is okay, so now I want to throw the stability results at the Synchorn problem. Let's see what we get, okay? Maybe some words about what was known on Synchorn Converges. On Synchorn converges. Firstly, let me mention that the convergence was well known when the cost function is bounded. So, for quadratic cost, that's the case of compact support, basically. And in fact, it's even known that the algorithm converges linearly. So that means exponentially, actually. Okay, so exponentially fast convergence when the cost function is uniformly bounded with actually no other condition. This boundedness is slightly improved in a paper of. Slightly improved in a paper of Russendorf. And I have to say that until I really looked at it, I just assumed that what's done there is like very general. But it turns out that the unboundedness that is allowed there is really very limited. So it will definitely not cover quadratic cost with two Gaussians, say, or something like that. Okay, essentially, you can only allow for a little bit of unboundedness in one of the two variables. Okay, so we try to cover quadratic cost here. To cover quadratic cost here and Gaussian-like marginals and many other marginals, too. Okay, so we want to use our general results. It's always true, as we've seen, that the synchorn marginals converge in total variation, in particular weakly. So we can use our general weak convergence result to obtain something which hasn't basically no integrability conditions or anything in it, namely that as soon as the cost is continuous and the total cost is finite, we know that. Finite, we know that the weak convergence of Synchron's algorithm must hold just because we had this geometric result, which didn't really assume anything on the marginals. Okay, here there is a convenience. So it turns out that those marginals produced by the algorithm, okay, you can write an explicit formula for them, but it's not very easy to access its integrability properties unless you put pretty strong conditions on the original data. The original data. So here it's very convenient because we don't even have to worry whether this nth problem here is finite. We can use this geometric notion, which is sort of always applicable. Okay, so of course, for an algorithm, it would be good to have a convergence rate, not just a qualitative result. So we want to apply our Wasserstein setting here. So here we make conditions so that our previous results apply. Our previous results apply. Namely, we assume that the marginals have some exponential moment for arbitrarily small alpha. So, this would again, this would cover the sub-Gaussian marginals and quadratic cost. So, in general, for pre-general C, without any particular form, we know the convergence of the entropic cost, which is this thing here, and we have the convergence in Wasserstein's sense of the marginals. Sense of the marginals, sorry, of the solutions, but without a rate. Now, in the second one, maybe let me specialize to the case of quadratic cost. So let's just assume that C is quadratic cost and maybe Q equals P equals 2. Okay, then with a known constant C, which comes from the transport inequality and this AL that we've seen before, we have a convergence rate on the entropic cost and we have a convergence rate on the control. Convergence rate on the optimizers themselves in Wasserstein's end. So, again, here the constant C is sort of linear in the cost and one over epsilon, the regularization parameter. Okay, that matters because in practice, the epsilon is pretty small. And that's so C, even if C is bounded, C divided by epsilon is going to be a pretty seriously large number. A pretty seriously large number. And I think where it matters is that, so that's going to go into this here. So previously I said it was well known, sorry, that we have linear convergence for bounded cost. And while that's true, the constants contained in that literature, they are of the form e to the c over epsilon, which even in nice cases is going to be more than e to the 1000. So that's really more than us. So, that's really more than astronomically large, and sort of at finite accuracy, that's not actually a useful estimate. Okay, so the fast convergence observed in practice is not fully captured by any of those results, I think, also not in the bounded case. Marcel, in this result of the previous slide, where did you use that C was continuous? Did you need C continuous? Yeah, C is continuous. Yeah, case continuous. I wrote that in the very beginning. That's for all of my talk. Okay, sorry. Most things would fail rather directly when C is discontinuous in a significant way. So lower semi-continuous or things like that is not so much? No. I don't know, I can comment on that later. I mean, in some cases, you would need also the upper semi-continuity. So, essentially, you ask for continuity. Okay, so now I've talked about the primal formulation of the synchron algorithm. Likely, those of you who have seen the algorithm have seen the dual formulation. So let me talk a bit about the dual problem of EOT. Okay, there is a convex dual problem as usual. One way to write it is this thing here. So we're going to maximize over certain Mice over certain controls. The controls are two functions. They are functions of the marginal variables alone. So f of x, g of y, say. And then what are we maximizing? This is the integral of f with respect to its marginal. Similarly for g, and then it's just minus e to the f plus g minus c integrated with the product of the marginals plus one. Or another way to write it is to put the log here and not put the plus one there. Okay, so this is the dual problem. That okay, so this is the dual problem. Just like in mathematical finance, entropy minimization over you know martingale measures and so on, there would be a dual, which is the maximization of exponential utility, which is exactly what's happening here. So, F and G are like the portfolio variables in that language. Okay, so this is again, this is a strictly concave problem now. So, we'll have a unique solution. It's unique up to the fact that you can always kind of move a concept. That you can always kind of move a constant from f into g, and that will be the only non-uniqueness here. Okay, so this fg are called potentials or Schrodinger potentials or EOT potentials, and knowing them is equivalent to knowing the optimal coupling. So namely, the optimal coupling or its density always has this form with the F and the G. So knowing the potential is kind of the same as knowing the coupling. Okay, so to get rid of Okay, so to get rid of this constant that one can move around, we can normalize the potentials. One way to do it is that the means are the same. So here I'm assuming that the cost is integrable from now on. And in that case, it turns out that the potentials, they're also always integrable. So this makes sense. Okay, so now we want to have a stability result for the potentials themselves. And here I'm going to state only sort of a special case or a corollary. Only sort of a special case or a corollary from a recent paper with Johannes, who is in the room there. What is a bit tricky formulation-wise is that, okay, you want in the general case, maybe to have a result for weak convergence of the marginals. So these marginals, they could be not measured theoretically equivalent or reasonably comparable. For instance, they could be discrete converging to continuous measures. Here, the potentials in general are kind of elementary. In general, they are kind of elements of L1 spaces, so it's not straightforward to compare L1 spaces over measures which have kind of completely different support. One is continuous, one is discrete. So the statement of the theorem is a bit longish, and I want to avoid it here. So here I'm going to talk about the case that matters most for Syncorn, which is equivalent marginals, or at least absolutely continuous marginals. Okay, so in this result, Marginals. So in this result, the marginals converge weakly, and there is this top-line assumption, which, if you think of it as a stability result, is probably reasonable. So you're thinking you have some marginals at hand. You need to check that the integral of the cost converges. So that's basically an integrability condition on mu and mu n because they converge weakly and c is continuous. So if c is sufficiently integrable, this should work out. Okay, so here in the absolutely continuous case, there is. In the absolutely continuous case, there is a condition which is some sort of boundedness in probability of the reverse densities. Under these conditions, the potentials converge in probability, L0 means in probability, of the respective marginals. That's the result stated here. And one application of this is when the marginals converge in total variation, so then in particular they converge. So, then in particular, they converge weakly, of course, and this boundedness condition is satisfied. So, then, if you can check this condition on the integrated cost, it follows that the potentials converge in probability. And if you also want to go back to the optimal couplings, this implies that the couplings converge in total variation. So, as a corollary, we have stability in total variation of the optimal coupling. Okay, so if the marginals converge in total variation, Marginals converge in total variation, the couplings converge in total variation, too. That comes out of this analysis, and not sure that comes out of the other analysis at all. So in a way, when we do the very general weak convergence result, in weak convergence, it's very easy to have compactness. So along a sub-sequence, convergence is kind of immediate. And the difficulty is to identify the limit because the convergence is weak. You somehow, you know, it's harder to put your head. You somehow, you know, it's harder to put your hand on what the limit looks like. In total variation, convergence is very strong, so it's relatively easy to identify the limit. But in the space of measures, it's very hard to have compactness for total variation. So here, by taking the detour through the potentials, that's basically why we managed to succeed, because instead of looking at the space of measures, we look at the much smaller space of measures that are sort of determined by functions. sort of determined by functions f plus g. At least that's how I think about it at a high level. Okay, so we want to apply this to Synchorn, see what we get there. And I'll use the opportunity to give the dual formulation of Synchorn, which is probably the more famous one. So here it's going to be an iteration giving us functions g n and fn, which we initialize simply at zero. And then the iteration starts here. So integral. starts here. So integrate e to the g minus, sorry, e to the g minus c with respect to one marginal. That gives you e to the minus f. Then you take that f and you plug it. You do the similar thing in the other variable with the first marginal. That gives you e to the minus g. Then you take the g, you plug it here, and you go forth and back and forth and back. And in the discrete world, if you think about it, this is basically a vector e to the This is basically a vector. E to the minus C is a matrix. So this integral becomes a matrix vector multiplication, which is why I said in the beginning that to do synchron, essentially, you need to be able to do matrix vector multiplications fast. Okay, so that's the iteration. How does this link to the primal formulation? So let's say whenever you have functions fg, you can define a measure like this. Okay, so e to the f plus g, this symbol here. f plus g this this symbol here is e to the f of x plus g of y minus c of x y take this as a density okay to create a measure pi so then if you take this fn g n the corresponding pi 2n is exactly the 2nth sincorn iterative that we have in the primal formulation and similarly if you know one is odd and one is even then that gives you the pi 2n minus 1 so that gives you the example Pi 2n minus 1. So that gives you the exact same sequence as before, and is some algebra to just verify that they're indeed the same. Okay, so this is another way to write synchorn. And so the synchorn iterates are very closely related to the potentials of the pi n. Essentially, they're the same up to some index shift. Okay, so in the context of SYNCORN, we know that the marginals always converge in total variation. So now Total variation. So now, by going through the stability result, if we know, if we can check that the integrated costs converge, then we immediately obtain that the iterates F and Gn converge in probability to the true potentials Fg, and the primal iterates, the pi's, converge in total variation to the correct optimal guy. Okay, so now it's a slightly different situation where the mu and mu n are. Where the mu and are given to us from the algorithm. And as I said initially, it's not super easy to check the integrability properties. Okay, so checking this is not straightforward. And we don't know if we have conditions that are sort of close to optimal at all. One condition that does work and that is good for us to cover quadratic costs of Gaussian marginals is this one here. So this condition is satisfied as soon as As soon as C has an exponential moment with respect to the product of the original marginal. So that's the primal data. Okay, and again, here alpha can be small. We really want, so even if you think that your variance is not going to be, you know, very concentrated, you still, so you have to be aware that when you do this with the original epsilon, basically this C, you should think that this is C divided. You should think that this is c divided by epsilon because you went through this transformation, and epsilon is probably going to be quite small. So, you really want to have this alpha to be able to kill the epsilon. It is very easy to get a convergence result under a condition here where alpha is greater than one or something like that. That's a result that would also be in my lecture notes. But it doesn't really cover the relevant regime that we're interested in here for small epsilon. Okay. Small epsilon. Okay, so anyway, so we kind of succeeded here to cover the case of quadratic cost and subgauss and marginals that we're looking for. In the dual side, we don't have a convergence rate. I think that's still one of the things that are completely open. All right, I think I'm out of time and I'll finish here. Okay, thanks a lot for the great talk. That's a great talk. Are there any questions or comments? Can I go back to my question about continuity of cost and uh why does it sorry can you speak up a little? It's actually Speak up, please. So I'm looking at the ceiling now. The continuity of cost, can you comment very briefly about why the one-sided continuity is not enough? Well, maybe it depends on what exactly one is trying to do. For the synchron convergence, continuity is probably not crucial, or at least, let's say, when the cost is bounded, then this would work out. Bounded, then this would work out even for just measurable cost. For the continuity, for the stability result, you cannot expect general stability when the cost is not continuous, right? Because you would be able to sort of manufacture marginals which dip into the discontinuity of the cost function. And at that point, the continuity of the value would break down. Do you see what I mean? Yes, yes, yes, yes. One could say that, of course, I can allow certain types of continuity, discontinuities, and focus on marginals where these discontinuities are null sets, and then one may hope for a result. The limit epsilon to zero that I didn't talk about, that one. I didn't talk about that one also fails in the discontinuous case. I mean, lower semi-continuity is not good enough. That's kind of because the entropic problem sees only things which have positive measure under the product of the marginals, whereas the limiting optimal transport problem, of course, the optimal guy may be concentrated on a graph or something. So that can create a complete disconnect between the two. Disconnect between the two problems if the cost function is discontinuous. Yes, all right, thank you. Thank you. Are there other questions? Okay, so if there are no more questions, so let us thank the speaker again and And there is a a break a coffee break now.