Yeah, sorry. Zoom one on one mistakes shouldn't happen anymore at this point, but still happens. Okay, so assume now you can hear me. So thanks everybody for getting up. At least if you're in the time zone where you have to get up at this time. So what I'm going to talk about is this work, which is actually still work in progress. So that's why I put like a small warning down here. That's a collaboration with Florian, who is in the audience as well, and with Raini Henrien, who many of you know, who is at the Weielschaft Institute in Berlin. And this is our attempt to study a little bit of theory and also some numerics of PD control with joint chance constraint. And I'll try to explain what these are and what aspects come in when you do PD control. When you do PD control, when you deal with infinite-dimensional problems, all right. Okay, so the abstract problem statement is the following. So it's the classic optimal control. So it's an optimization state framework where this U is always going to be my control and the Y is the state variable. Okay. And I have a governing equation. Think of an elliptic. Equation: It's think of an elliptic PDE, so we're mostly going at linear, linear PDEs, linear elliptic, and also like bilinear PDEs. So, I write it in this abstract form. And into this equation, A of U is a PD operator. So, if it's linear, obviously, it doesn't depend on U, but it could depend on you. U is a deterministic control. So, U is just a function in a function just defined over space. So, it's a deterministic function. But this partial differential equation involves an uncertainty here. Involves an uncertainty here that's c that's c of omega. So that's an uncertain initial condition, boundary condition right-hand side. And because of this uncertainty, also the state becomes uncertain, right? So even for a deterministic control U, the uncertainty here on the right-hand side, which actually enters linearly, and I'll talk about, I'll be very honest about what we can do and what we cannot do. This uncertainty in the equation makes the state also uncertainty, and so that's why the distribution. Also, uncertainty, and so that's why the distribution of this C gets trans that's why Y also satisfies a random direction. And okay, so this is the standard approach in PD's constraint operation under uncertainty. And now, additionally, we have we consider what's called joint chance constraints on the state variable. So, the state is the solution of this PDE, okay, and we wanted the state. Of this PDE, okay, and we want that the state is between two bounds pointwise for all x or for almost all x, right? And that the probability of this happening is larger or equal than p. And p is a number that's typically close to one. So p could be 0.9, 0.95, 0.99. So with high probability, you want every realization of the state variable to be pointwise at every point, in between these bounds. In between these bounds. Okay, so this is when you have bounds on both sides. So that's the formulation. So this is the main challenge in this formulation. This is the main problem here. And I'll explain, I'll talk a lot more about what this means. So to give you an idea, so this is really a constraint on the paths. So it's a constraint on the individual realizations of the state. So each path, if it's in one, it would be a path, but each of them. it's in one it would be a path but each of them has to point wisely in between uh between be between these these two bounds if it's supposed to count in this probability and um so what what does this mean this is an example that shows this basically here this here there's only an upper bound here and that's this green dotted line okay this is actually a two-dimensional the state variables are here two-dimensional variables that are plotted here these are these random samples from the state but then i i look at them from from kind of this direction so i kind of From kind of this direction. So I kind of get rid of one dimension. And then these paths look like this, right? And then all the black ones, the ones that are not a single point below the bound, they count to the probability, right? And the ones, the blue ones, these two ones, they don't satisfy it. So that's why they don't satisfy this constraint. So just to illustrate again that this constraint in a... Just to illustrate again that it's a constraint for each realization, and that's a pointwise constraint that makes them join. So, again, here I just copied this formulation here again. That's the formulation. And so if I plotted, and I know many of you have seen this, but I just want to, because I was initially very confused about these things, so I'm trying to illustrate them a little bit here. So if you had two random variables, okay, if this y would be a two-dimensional random variable, and this would be the two random variables, okay, and if that Random variables, okay, and if that distribution of the y's would have these contour lines, that's the blue-dotted lines, okay. And then I randomly draw samples from that distribution, and that's the blue dots here, right? Then the ones that satisfy that constraint are the ones that are in this red area here, okay? That means that both of their components, their X component is less or equal than this line, and their Y component is less or equal than this line. Okay, so they only count it in if both of they only counted in if both of they satisfy both both of these constraints and if you have this uh point wise here then principle they only allow count in they're only allowed right if they satisfy this constraint for all for infinite for a continuum of points x okay so here i plotted it for for two dimensions so in some sense infinite number of constraints and uh yeah this is basically just the to the illustration that i want to point out right that this is this is quite different Out right that this is this is quite different from this one. The only thing that's actually changed here is I've switched around these two statements, okay? But that makes them quite different, right? So, this one means joint, that means only the points are in that satisfy this at every point, almost every every point. This formulation would be different. This formulation would say that you are interested in the points that satisfy at every x that satisfy this with the probability of. This is with the probability of P or larger. So, this would be something that you have to check at every point in physical space X. So, it kind of falls about. It separates into many, many kind of constraints, but the easiest to satisfy, versus this is a constraint that you have it the other way around, right? So, you can only check it when you know it's not a point-wise thing in space. not a pointwise thing in space i guess that's that's the main thing i'm trying to i'm trying to see here and something like the lower one is um maybe has not sure what kind of applications this would have but it's it would be easier actually to realize than these joint these these type of constraints and uh obviously this is very closely connected to risk averse optimization with var with var constraints so c var is is a kind of a somewhat a little bit nicer version of var and dealing with these chance constraint is related to var optimization These chance constraint is related to viral optimization. Okay. And let me kind of point out: Drew did this nicely in his talk yesterday already. So let me just point out: so, what difference does it make that there is PDEs involved here versus just doing it in finite dimensions? So Strew already pointed out yesterday. So these problems are compute intense, right? So each time you compute one sample, one realization of the state, right, that depends on the random variable, you have to solve a PDE, okay? Variable, you have to solve a PDE, okay? So that's why they're expensive, right? If you need kind of gradients, which we usually need in optimization, right? We want to kind of find the control u that minimizes something. If you need gradients, then you have to solve at least one outer PD. That's the adjoint equation, right? And if you need Hessian, so Hessian applies, you need to solve more PDEs, right? So that's very computationally quite expensive. You have this infinite-dimensional bound, right? These bound constraints have. Right, these bound constraints have to be satisfied for almost all points in D. So, D is by the way, D, just to point it out. So, D is the physical space, right? So, this is a unit square, something R1, R2, R3, and omega is the random space. Okay, so infinite-dimensional bounds that have to be satisfied for almost all points X and V. These problems typically have a high-dimensional. Have a high-dimensional uncertainty. So, this uncertainty that comes in has a you need a lot of Konen-Love Kaiel modes, right? If you want to approximate, so it's yeah, so it's a high-dimensional uncertainty space, which is obviously always hard to approximate. And if you do this, and I'm only going to show numeric results here, but if you do an in the future, it's interesting, there is several sources of errors, right? So, we have to discretize our physical domain, right? So, if you use finite Our physical domain, right? So, if you use finite difference, finite elements, whatever, whatever you want to use, you have to discretize the random domain, right, using Konan Love expansion or any kind of these approaches. And then on top of it, we'll have to use sampling ideas. So you also get errors from sampling, right? So if you want to do an optimal way to solve these problems, you have to balance errors coming from different sources. All right, and then this is the honest slide. So, what are we actually doing, and what can we do, and what can we do? And what can we do, and what can we not do at the moment? So, we are looking at linear and bilinear optimal control problems. Bilinear, I'll show what this means. That means that the state and the control are multiplied with each other with these type of chance constraints. We'll argue existence and uniqueness for the linear problem of these optimal control problems with these kind of constraints. Of constraints. We'll discuss how to compute these probabilities, these chance constraint probabilities, and its gradient without using regularization. So we don't want to use any smoothing or any kind of regularization. We'll show how to do this. And I'll discuss how to use quasi-Monte Carlo methods to get a better approximation of these probabilities. That's kind of what we do. What we don't do is Of what we do, what we don't do is currently we require that the map from the uncertain parameter from C to the state has to be linear. Okay, we the PDE does not have to be linear, right? The control and the state don't have to be linear, but we need that the map for fixed control that the map from the uncertain parameter to the state, Y, to the realization of the state, that has to be linear. We currently require that the uncertain parameters are multivariate high-dimensional Girl Gaussians. Carkausians. And we have a finite noise assumptions. That means we cannot do it in infinite dimensions, infinite dimensions in random space, but I guess that's usually not the case anyways. So we require Gaussianity, but we believe some of these things can actually be extended to something that is Gaussian-like, to something that is a little bit, that is more general than Gaussians. And this is a technical detail. So when we compute the gradients of these probabilities, we actually require discretization. Require discretization of these bounds. So I say formulate infinite, an infinite number of bounds, but we actually have to make them finite-dimensional to avoid certain degenerate cases where when you compute the gradients, I'll try to point it out later. Okay, so that's the thing we do. And so that's the honest slide to show that what we do and what we don't do. All right, so basically that's for my general introduction. Let's just look at examples that fall into this class of problems. Class of problems. So, this is a classic linear quadratic tracking type optimal control problem. Okay, so that is the desired state. You want the states to be close to Z. The control is deterministic, so that's why you have this control cost here, although it's positive. And because the state is uncertain, you have integration over the random domain. So, this is the standard risk-neutral formulation. The standard risk-neutral formulation of an optimal control problem under uncertainty. And the PDE is also linear here. So it's an elliptic PDE. And the randomness here enters as Neumann data on the right-hand side of this PDE. So that's the classic linear quadratic problem with tracking type. And additionally, we consider this chance constraint, which means that all realizations of the state have to be between two bounds with a certain probability. With a certain probability point-wise, okay. So that's one example. That's the linear problem. For this problem, we can actually show uniqueness and existence of a solution. That's number one. And then the number two problem where I'm going to show some results is the following. So the main difference is here now that this governing PD equation is not linear anymore, right? U is the control and it multiplies the state here. Okay, so that's what's called a bilinear structure, and that comes from applications in quantum quantum. In quantum control, and some other applications, where you have this structure, that's where you have this product between the control and the state variable. And the objective here is even simpler, right? So here we actually don't have the tracking term for the state. We only have the control cost here. So if you think u0, that's a given function. If you think that's zero, then this would really be just the L2 squared control cost. But we additionally have chances. But we additionally have chance constraints. Now they're just unilateral constraints, but it could be bilateral, that doesn't really matter. It's just for in our application, we just have unilateral constraints for this problem. Okay. And the notes here are the control objective only involves the control cost. We could involve the state as well. It doesn't, it's a little bit simple like this, so that's why we use this. And the mapping from the control to the state variable is non-linear, right? So if I if I Non-linear, right? So, if I pick a U, have to solve this, that's a non-linear map. But the mapping from the uncertainty to the state map for fixed control is linear, and we kind of need that for the tricks we currently employ. Okay, so these are the two examples. I'm going to show you shots later. What can we say about the theory of these problems? The theory is only for this linear problem. for linear for this linear problem for this for this for the for the first problem i showed you for this one we can actually say uh something about rigors about theory so um again we we introduce this definition this phi of u will always be the probability uh at a given control u okay so to compute this phi of u we need to we take u we have to solve this uh this stochastic uh pd which gives us a distribution for the y's right that's this distribution and then this this Distribution and then this probability is computed by checking every realization of y, right? Or I mean, there's infinitely many, but checking realizations of the y and checking if they're in the bounds, and that's the probability. So that's this quantity I need to be able to compute to make sure these chance constraints are satisfied. Okay, so that's phi of u, and I'll keep that notation throughout my talk. And then for a given probability p, which is also an input parameter. probability p which is also an input parameter right you want to have 99 let's say of of this probability to be in between those bounds right for this given p I define the set the set of controls such that controls optimal controls u right or physical controls u such that the corresponding states satisfy this condition right satisfy that this probability is greater or equal than p this is the feasible set this is the set of controls Set, this is the set of controls that could be empty, right? But it's the set of controls that leads to states that satisfy the chance constraint. And the first thing we can say is that this probability function phi is well defined. Okay, so this is really measurable and it's well defined. This set here is always convex and weakly closed. Okay, that's useful for proving existence, obviously. And in particular, if for every In particular, if for every probability, because that's true for every probability, this function phi is weakly upper semi-continuous. Okay, so it's a function, yeah, it's weakly upper semi-continuous. And the arguments here are somewhat standard arguments. So, one crucial thing is that one needs is that Gaussians have large concave densities, that's important, and the rest is basically. And the rest is basic function analysis kind of arguments to prove these kind of things. And one has to be a little bit careful because we prove these bounds. We only said these bounds have to be satisfied almost everywhere, which is the right thing for sober-left space functions. So we have to be a little bit careful with sets of measure zero and see if they matter or if they don't matter. Okay, that's one thing. And the second thing is And the second thing is, yeah, so this was just the well-definedness of this probability function. The second thing is, what can we see about the existence of solutions and the uniqueness of solutions for this linear case of this actually of the optimal control problem? We can basically get the nice and kind of somewhat expected result that the reduced objective, right, that's the one where you eliminate the state. If that's coercive, which is usually the case when it has this quadratic. Which is usually the case when it has these quadratic terms, of course, and strictly convex and lower semi-continuous, right? Which is always satisfied for these nice things we look at. And if the set of controls that are admissible with respect to the control constraints and that are feasible, meaning that there is a control U such that the corresponding state satisfies the chance constraint if that's not empty. If that's the case, then the optimum control problem that I showed you before admits a unique solution. Showed you before admits a unique solution. Okay, and the proofs are basically just exploiting convexity and the fact that this is non-empty and weakly closed. So you use the usual sequence arguments to show that this thing has a solution. Okay. Okay. Now we're going to talk about estimating this probability. Okay. So this is my same sketch here again. So I have to estimate this probability. probability okay um how can i do this i could so one one thing i can do is right you is it i for this is for a fixed u okay for a fixed control u i have many um states y right because there's the randomness enters into y so what i could do is i fix u i uh i get a distribution i take random samples from y right these are these blue dots here in my two-dimensional sketch right um and then i i'm interested in the And then I'm interested in the probability to be in this red rectangle, right? Because these are the ones that are inside, all the other ones are outside. And so the naive way to do this, obviously, right, is just doing Monte Carlo sampling. And then I just look which points or which realizations, right, which fall outside the feasible domain and which ones fall inside it. I can do this also in high dimensions. So that's in principle not a problem. That would give me a Monte Carlo estimator of this kind, right? So basically, just do. kind right so basically just do an average this should be this should be an n here so that so n is always the number of monte carlo uh steps monte carlo samples i'm using and uh it will be given it will give me this estimator right so and that converges at the usual one over square root of n Monte Carlo convergence but there are a few problems with this the main problem is that doesn't give me access to any gradient right I'm interested what do I need right I want to solve an optimum control problem so I need to minimize over Problem. So I need to minimize over u over the controls. Okay, so that's why I need the derivative of this probability with respect to the control u. And if I just, and by the way, how does my control influence this picture here, right? The control influences the distribution of the samples, right? The control influences the distribution of the states. States realizations are these points here, right? So if I change. Here, right, so if I change the control a little bit, right, then these samples or the underlying density, which is sketched here with contours, right, will change. But the problem is because I've already sampled, so I've already kind of discretized this in this way, right? So the problem is if I change the distribution slightly, then it's going to be non-differentiable, right? Because if one of those realizations is on the red side or on the white side, right, is a 0-1 decision. white side right is a zero one decision it's a binary decision right that's why it's there's no there's no hope to to actually make this this concept to make this differentiable right so there's no differentiability so this thing will not this will make this will be very hard to to to compute right uh one could of course make make introduce regularization right make this boundary between feasible non-feasible make it blurry and tricks like this right and that then you can recover differentiability okay but we're not gonna do that we have this other Going to do that. We have this other, we're following this other way to compute these probabilities, and that's called the spherical radial decomposition. That's something that actually learned from, we learned from Rene Henry. He's been doing this in different contexts. And I would say the stoastic optimization community has been doing this in some contexts for these kinds of things for a while. So, what's the idea? And what's the idea about this? The basic idea is: first, we transfer, let's say, First, we transfer, let's say, we have a Gaussian and it's somewhat slight, I mean, it can be deformed, right? Non-IID, but we transfer it with a map to an IID Gaussian. Okay, so this is what I'm sketching here. So nice circles. And then we change this integration over the random space where we integrate here over the state variables, right? And over this characteristic set for the state variables. And that's basically what we did here. You count on which side you're on, that's the characteristic function. On that's the characteristic function. We change this integration here, which we change the coordinates basically. So we change this integration here as an integration over the unit sphere. Okay, that's this thing. And direction. And the unit sphere gives me directions. And in each direction, the unit sphere, and this is a one-dimensional integration in it on each ray here. Over a fixed direction eta. That's this eta. So I basically can write this integral over the integral over the unit sphere that gives me directions in which I want to go. And then in the direction, I integrate from zero where I start up to this point rho. And this point rho is the point when I go along this ray where I'm going to leave the feasible domain. Okay, and what's the function integrate in 1D here? I integrate the chi distribution along that, the one the chi distribution along that ray. What does the chi distribution have to do with anything? So we know that if you have an IID Gaussian, then if you just go along a ray, the probability follows that chi distribution. So that's the requirement I need. It works for probabilities when I Works for probabilities when I know analytically the one-dimensional probability distribution along each of these rays. And for an IID Gaussian, along all the rays, it's this chi-distribution. It's the PDF of the k-dimensional chi-distribution. The dimension enters in this distribution, but that's what it is, right? So the trick is split it into unit sphere, look at all these unit spheres in possibly in high dimensions, right? And look at one of those rays. I know the exact analytical PDF along that ray, and that's the same for every ray, right? And that's how I'm going to split my integral. And why is this better, right? Now it's nicer because now this 1D integral, I can now compute analytically, okay? Because it's the chi-distribution here, and they go from zero to a number rho, which is this point I still need to compute, right? But if I have this thing, I can just replace this thing. I have this thing, I can just replace this thing here by this thing here, which is and f here is just the CDF of the k-dimensional chi-distribution. So I value just the CDF at this point row. And I've basically done this one-dimensional integration analytically. So I now have integrals of the unit sphere, and we're going to use Modical Assembly for that. And then I'm going to get this in this one dimension I can do exactly once I found this row. Okay, so that's the whole. Okay, so that's the whole idea about this. And if I could do it for one ray, right, I can do it for many rays, right? So for each ray, I have to compute this intersection, this point here, which I'm going to argue, show you how to do this basically. But that's relatively easy to do. And then I just replace this integration over the unit sphere with the Monte Carlo approximation. I take random draws on the unit sphere, and that allows me to estimate this using Monte Carlo. Okay, so I still do Monte Carlo, but only on the unit sphere in these one dimensions, I'm doing it exactly. So it requires computation of this point, and this is, but this is for this context. It's actually something we can do. It basically means we decide, we figure, find a direction, a random direction in random space, and then we add it to the mean of the distribution until at some point it starts to, we don't satisfy the constraint anymore. And for linear problems, you can basically just write down a solution. You can basically just write down a solution, you don't have to compute anything. But we think that actually can be extended for certain non-linear problems as well. So it requires computing of this point, but for certain problems, you get that more or less almost for free. And so why is this nicer, right? So the nice thing is now every direction, so every sample you have actually provides some input, right? Because even if it's very most of these rays, some rays will never leave the red area. That means you compute the CDF here. You compute the CDF here up to infinity, and that means this is just the number one, this is just the value one, right? But most of them will actually hit some kind of boundary at some point, and they all provide some information, right? So each kind of sample kind of gives you some kind of information. And which is a little bit different from Monte Carlo, right? In Monte Carlo, you only have the information that the vanilla version, right? You only have the information if you're inside the red or outside the red area. Okay, but even Okay, but the even nicer thing is that this gives us immediately gradients without doing any regression. And why is that intuitively? It's a gradient with respect to u, right? So if I change u, I change my underlying distribution. That's the blue stuff here, okay? But if I perturb this a little bit, right, that means that this distribution is only perturbed a little bit. And that also means that these red row points, right, they're also going to change a little bit, right? So if I consider it with respect to these row points, right, this. These row points, right? These are gonna, the location of these row points will change, will be differentiable with respect to perturbations of the underlying distributions, which are caused by changing the control U. So that's the main thing why this is actually quite nice, because it gives you a pretty much straightforward way to compute gradients. And that's here on the next slide. So basically, this was the formula we found to compute this probability. Okay, so this is just Monte Carlo on the unit sphere, and this is the CD. Sphere and this is the CDF of the chi distribution. I value it at these rows, right? But now this row for a fixed direction at the this at the i's are the directions, for fixed direction at the i, right? This row, this function is actually differentiable almost everywhere as a function of u. So in order to kind of compute the gradient of this, all I basically need to do is I just use chain rule. So basically, this CDF, when I differentiate with respect to this inner thing, becomes the To this inner thing becomes the PDF of the chi function, right? And then I have an inner derivative here that multiplies with it. Okay, so this idea gives me a very straightforward way to actually get gradients. And there's no regularization, no smoothing or anything involved here. Okay, so it's a cheap way to compute the gradient. There is one adjunct solve that you have to do here for the gradient. And there's no regulation, no smoothing, nothing like this necessary. nothing go nothing like this necessary here even though it is a kind of a it is a kind of a quite a non-smooth problem and it only works for for this only mostly works for Gaussian like distributions at somewhat simple bounds right I'm not claiming this works for everything but I my claim is that it's our claim is that works for a useful class of problems okay so I get gradients of feed that's nice okay so so so that's nice and now how we still need to How we still need to sample the unit sphere. It's a uniform distribution on the unit sphere, right? Because we split the integration over uniform of unit square and then going along this rays and doing it along this rays exactly. So, okay, so we need, we do spherical way that decomposition integrates exactly over the race, but we still need to sample on the unit sphere and of dimension k minus one. What, yeah, of the unit sphere? And I'll talk about k in a bit. So, what's the dimension of k? It. So, what's the dimension of k? So, in the problems I'm interested in, k is the effective dimension of the uncertainty. So, k is the number of kl modes that you retain in your decomposition. Okay, so that's kind of how many, yeah, that's a measure, right? How many important directions you have in random space. And I think one interesting aspect is here that we have our random variable here, is c, right? C, right? And C might have, let's say, C is the K L expansion doesn't decay very rapidly. So you might need a lot, many K L expansions, and then you have to do uniform sampling in high dimensions of K here. But the nice thing is, look at the equation here. This is the PDE, okay? So the randomness enters here into xi. B is just a linear operator, that doesn't do much, right? But our Is too much, right? But our constraint, the thing we need actually that shows up in our chance constraint is not C directly, right? It is this, this, the state, Y, right? And what's the connection between this psi, let's say the right-hand side forcing or anything, and the state? They are connected in this case linearly, okay? But what connects them is this PD type operator, right? And this is, and so basically, when you move. And so basically, when you move an inverse, it has been invertible, right? You move it to the other side, right? To this side, that means that this is smoothing because it's a PD solve. Think of it as a Laplace solve or something like that, right? And it's smoothing not only in space, it's also smoothing in random space. So if this random parameter, right, it has a KL expansion that only decays to something very small after 50 modes, right? After you have multiplied it with eight. After you have multiplied it with A inverse, after you've done a PE solve with it, right? Then the decay for the corresponding states will be much, will be in general much faster, right? Because there is an inherent smoothing going on here, okay? And what we need, because we have this chance constraint on the state, is exactly we need this K is the effective dimension of the states. So the PDE helps you here to kind of reduce the dimension that you need to work in because this thing ends up smoothing. thing smooth this ends up smoothing the states compared to whatever random input you have and because you that often gives you moderately small dimensions you can actually uh instead of monte carlo right you can actually use uh quasi monte carlo sampling okay quasi monte carlo is uh as i'm sure many of you know is is is quasi you don't randomly take samples you have like some deterministic uh method to take samples and the idea is that these are more better distributed more nicely distributed and if Distributed, more nicely distributed, and in small and moderate dimensions, this really gives you quite a benefit. So we know that Monte Carlo converges at a rate of one over squared of m, quasi-Monte Carlo converges faster, sometimes even one over n, but that's that's hard to get. But in moderate dimensions, quasi-Monte Carlo is actually good. Okay, so this dimension reduction here reduces the effective dimension k, and that's why it might be beneficial to use quasi-montecal. And I'll show you that this really makes sense. So this is just, this is just. So, this is just a slide to say, okay, so we have PDEs, right? And we are interested in computing this probability phi. And instead of, in principle, every sample we take requires to solve a PDE, but we don't want to do this because we might want to take 10,000 or a million of samples and then it becomes very expensive. So that's why we use low-rank-based reduced order modeling of the PDE, which we do up front, right? And then that requires a certain number of PDE solves, but then we always work with. Number of PDE solves, but then we always work with the reduced order model. And this is basically for the linear problem. This is the standard thing that's either called, you know, has all these names, right? Sometimes called POD or it's SVD. So it's basically just this standard projection-based reduced order modeling. That means we have an upfront cost, right? And then after we have our reduced model, we don't need to go to the PDE anymore to solve. We can just call the reduced model, which is usually much cheaper. And for the linear problem, you do this one. And for the linear problem, you do this once and then you can use the same reduced order model for all iterations. For the bilinear problem, you have to update this reduced order model whenever you change your control U. So in every optimization step of the control problem, you need to recompute or update your reduced order model. That's a difference. But basically, we do this, and that means that when we take these samples on the unit sphere because of this decomposition, we don't always have to solve PDEs. We can just call this. Just always, we can just call this cheaper model to make it faster. So, that means that the number of samples doesn't taking more samples doesn't mean you take more PD solves, because you've done these upfront and then you just use a linear algebra model, basically. Okay, and then this is my example. So, this is a linear PDE. I'm going to show you just how accurate this works. It's linear PD and this is what I showed you before. It's a two-dimensional equation, two-dimensional PD. Equation, two-dimensional P on the unit square. The randomness enters on the right-hand side here. Yeah, and I'm basically just going to compare you. I'm interested in the convergence of this. How well can I approximate this, right? And it depends on several factors. It depends, obviously, the number of Monte Carlo samples or quasi-Monte Carlo samples. It depends on the number of KL modes K I use. So these are these K L modes that are being used for Y, right? And there is this effective dimension. There is this effective reduction of dimension coming in because y is smoothed once with the Laplace solve compared to xi, which is where my original uncertainty. This k and we'll see that for this problem, you can keep k quite small around 10. And there's the mesh distributor in the physical space that matters. That's these things. And basically, this is the result. So what am I doing here is I show you the error. So I computed in brute force the true. computed in brute force the true probability, okay, and I compare it with my Monte Carlo approximation of that one. And I just plot the absolute value difference between the two. On the y-axis here, I have the number of Monte Carlo samples. And then I do the standard Monte Carlo method. This is the one where we just take points that we check if they're inside this red box or outside. And then you get this one over typical one over squared in convergence here. Okay. Then I do this spherical radial Monte Carlo method. So I split this, find these row points where these rays hit the boundary, okay, and then use that. And that gives me the blue line here. So you see it's better by a constant. So the difference between, if you ask for the same accuracy, the difference between the green line and the blue line is about a factor of three to four in the number of samples. So this spherical radial decomposition gives you, makes it faster by. Gives you, makes it faster by a factor of three or four. But then when I combine this spherical decomposition with, instead of just Monte Carlo with quasi-Monte Carlo on the unit sphere, right, then I get this red line. Okay, so you see you get actually quite a bit larger convergence. If you compare the naive Monte Carlo or the standard vanilla Monte Carlo with the red line, you usually save a factor of 50-ish, right? And this gap actually grows down here. So basically, it means that you need a factor of 50. So basically, it means that you need a factor of 50 fewer Monte Carlo samples when you use this spirit-rated decomposition plus quasi-Monte Carlo and so Monte Carlo to approximate these probabilities. And that's not mentioning that these two ones basically almost for free give you also gradients, right? With this standard way, it's hard to get gradients, but these are better, right, in terms of multi-color symptoms you need, but also they give you gradients very easily. So that's kind of one of the main ideas here. The main one of the main ideas here. This is just studying this as a function of this, how to approximate again, it's the same error here as a function of different spatial discretizations. N is the number of grid points in one dimension. So this is computed on a mesh of 128 times 128 points. So this is the usual behavior that you need a sufficiently fine mesh to resolve the physics, right? That's what this shows. And this is the sensitivity with respect to K. The sensitivity with respect to k, k is the number of kl modes, right? So these are the kl modes that I use for the for the state variable. The state variable is random. And you can see if you only use five, that's not enough, right? That means that if you increase the number of samples, the error goes down nicely, but at some point it just levels off, right? Because it doesn't matter how many more samples you take, the error is dominated by KL truncation error. If you need 10, you can get much further, but at some point it still levels off. Get much further, but at some point it still levels off. Okay, but if you need 15, you get pretty far in terms of accuracy. So this just shows this trade-off between different sources of error. Okay, that's mostly what I want to say there. And yeah, so this is kind of, this is also interesting. So this is saying the following. So if you are interested in probabilities that are close to one, okay, so if your p. Okay, so if your P is let's say 0.99, right? If you're interested in these probabilities, which means this red box I had in my sketches is fairly large, right? Most points are inside and only a few points are outside. And this is a comparison between, this is just standard Monte Carlo method. And this is so here is the probability here on the axis here goes to, you plot one over p here. Okay, so here the probability goes to one. One, if you the probably how I the notation I used before, just to one. And what you see here on the on the y-axis, that's kind of basically the error. It's this error, it's the standard deviation divided by the variance. And what you can see is if you use just standard Monte Carlo here, so this is all just standard Monte Carlo. If you do standard Monte Carlo, this error grows, becomes really bad when you go to small probabilities. You go to small probabilities when you go to 0.99 or 0.995. Okay, the error, this is the error here, is very large. If you use this spherical radial decomposition with a certain number of K, these are again the KL modes, you get the error becomes way smaller. And so basically the smaller probabilities you're interested in here, the larger this gap between standard Monte Carlo and spherical radial decomposition of Monte Carlo gets. And the reason is intuitively, right, that The reason is intuitively that in this limit that you're interested in very small or very high probability, p is close to one, right? This red box is quite large, right? Most of the points are inside. So if you use the vanilla Monte Carlo, most points are inside, right? If you're interested in P is equal to 999, one out of a thousand samples will only be outside that box. So you need a lot of samples to actually get a decent approximation of that. To actually get a decent approximation of that probability. With this decomposition, right, you will each ray will give you some useful information. And so that's why it's a much better use of samples if you're interested in probabilities that are close to one, particularly. That's what this one plot shows. Okay. Okay. So far, I've only talked about estimating this probability. I haven't shown you that the gradients, but the gradients you get from following this. From following this, they are basically discreetly exact, right? You get the exact gradients. The only error in, yeah, so you get, if you can live with the Monte Carlo approximation, then you get the exact corresponding gradient if you do this spherical radial decomposition ideas. Okay, so now you can use these things to actually solve control problems, and that's what we used here. So, this is just one result for this bilinear control problem. So, we minimize. So, we're minimizing this function here. This is a bilingual equation. You can see that the control u multiplies the state y. And this is the one-sided joint chance state constraint. And that's it. We can compute, with these tricks, we can compute the probability, we can compute gradients of the probability with respect to u, the control, right? And then we basically just use standard optimization methods. Optimization methods, SQP type methods to actually solve this control problem. And let me show you the results first. So, if there is no uncertainty, if the uncertainty is just at the mean, right, then we know that the exact solution can be shown to be constant one. This is very close to constant one. And then when you actually are interested in different probabilities, you see that the optimal control develops these kind of two spikes. Develops these kind of two spikes, and the reason for these spikes is that in these two locations, there is a high chance that you not satisfy that these pointwise bound constraints for the state. So, what the controls are trying to do in this case is the controls are trying to make sure that at these two points where the highest chances that you don't satisfy those constraints, what these peaks here are doing is they will push the state down at these two points to make sure that the random realization. make sure that the random realizations of the state don't don't um uh still will satisfy with a very high probability the the the bound constraint you you set and this is just a plot that shows um the relationship between the probability and the the cost right so this gives you some idea you want more you want higher probability to be inside right that goes in this direction and this is basically the cost you pay right this is the objective function here you see and this kind of gives you some idea if you want high You summarize if you want higher guarantees, right? It's more expensive, and this is what this is what is what is you can do this kind of plots. Okay, so main takeaways, this is for linear and bilinear optimum control problems with joint chance state constraints. We have been trying to develop these methods. This decomposition is for probability estimation, it allows to compute the gradients without using any regulatation. Without using any regularization, so that's nice. And the PDE operators that show up there result in some smoothing property that also reduces the effective dimension, right? The effective dimension of the uncertainty because there's this, yeah, because you have a certain parameter and the state, and the state should have lower effective dimension because there is smoothing going on. And that's why you can actually formulate this estimation of the probability. This estimation of the probabilities in fairly low dimensions. So you saw it usually done in 10 dimensions. And in these low dimensions, you can actually improve over standard Monte Carlo by using quasi-Monte Carlo. And we've shown that this actually, you can get some benefit from that here as well. Okay, that's it. Thanks. Okay. Thank you very much for the nice talk, Georg. A lot of interesting new ideas. Does anyone have any questions? Does anyone have any questions? Yeah, I have a few questions. Yeah. The first question is: what kind of quasi-Monte Color points you use here? What sequence? So there is no standard way to do, as far as I'm, I'm not the quasi-Monte Colour expert, but there's no standard way to do quasi-Montecolo on a sphere. So what we're doing is actually we take quasi-montecolar. Quasi-monte color points in the space, in the space in the RRK, right? And then just project them onto the sphere. And that gives you something like a QMC distribution, QMC sampling of the uniform distribution on the sphere. And the sequence we use, I forgot actually, it's the SOBL. It's one of the standard. I've tried two, I think, and it didn't make a difference. But the idea is basically, because you need those on the sphere, right? Because you need those on the sphere, right? The idea is just to take them in the space and then just kind of cut the length to one, and then you're on the unit sphere. Yeah, so well, here you have a quite nice structure which is either used like SVD and ROM and setting or like you know the Karbuna Law expansion for the state variable because the state variable essentially is a Gaussian random field if you have a linear dependence on their random variable and then for this Gaussian random field you do the projection via you know Kaufman allow of expansion Kalbuna law of expansion, you have a certain decay in eigenvalues. And depending on the eigenvalues, you can design some suitable quasi-Monte Carlo where you can achieve higher convergence rate. Like for instance, instead of n to the power minus one, you get n to the power minus two, three, depending on how fast your eigenvalues decay. But but there is, yeah, but there is, yeah, I mean, I will definitely talk about this in detail, but right, I mean, but we, we, yeah, you can do the new. Yeah, you can do the unit sphere, right? But there is actually a little bit of non-smoothness going on, right? Because we take these random samples in the unit sphere, right? QMC, and then how much they contribute to the sum here, right, depends on how long these rays are, right? Because it's not completely smooth, right? So there is some, you get some roughness from this, and that's why I think in QMC, we also don't observe this, it's not perfectly smooth, this problem, right? Because of this. Problem, right? Because of this factor that you get here. And that changes. So basically, you evaluate the CDF of the chi distribution in every of these directions that you chose on the unit sphere. So there is, it's not infinitely smooth, right? Yeah, I have a question, if I may. I missed at the beginning when you displayed the model whether these random elements. Random elements are processes. Do they change in time or is the same? So, right, so there's no time dependence for this here, right? So the psi and so on, that's a change in time is a stochastic process or it's just a random vector, just random? I think of it as a Gaussian random field. Think of it as a Gaussian random field, right? So it's let's say it's a normal distribution, maybe an infinite as a function, right, with mean zero, and then some covariance matrix that could, for instance, be something like an inverse Laplacian or something. Yeah. So it's a Gaussian random field. Yeah. So it does not change in time. That's what I mean. There's no time in anywhere here in this formulation, right? This only space. There's only physical space and random space. Yeah. But do you plan to incorporate some dynamic? Want to incorporate some dynamic in it? I haven't thought about it, to be honest. I don't like, I mean, these control problems are usually you could do time-dependent things, but I haven't, to be honest, I haven't thought about it. Yeah, that would be one order of magnitude to different. At least. Yeah, I haven't thought about it. I don't know. I mean, yeah, I can't say much about much. I can say much about much many hard to say something smart about it because I really haven't thought about it. Yeah. Right. So any other questions? I mean, I had some comments and maybe we can just go into the one of those breakout rooms and I can talk with you and people can then take a quick break before next talk. In 10 minutes, we come back to the Zoom room. Georg, would you and Florian have a few minutes to talk in one of these rooms? Okay. If you tell us where. These rooms, okay. If you tell us where to go, which one hold on a second, let me find the picture first. I guess uh, breakout room number one. I think I have to double-click on it and then okay, I'd like to join you too. Please, everybody back in breakout room number one. Okay, so we're gonna have to leave here probably. Let's see.    