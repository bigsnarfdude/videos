So they were wanting to adjust the volume. We seem to be doing this. Are you guys ready now? Yes. Jane, can you hear me? Yes, I can hear you. You can start, okay? Please. Okay, thank you very much. Please, okay. Thank you very much for giving me the opportunity to talk here about parallelism in algebraic geometry and in structures in algebraic geometry. This talk is picking up parts of our discussion of yesterday, so I restructured it a bit to stress the point that I had made yesterday. So briefly, why should we consider parallel computation? We consider parallel computation. We said it yesterday already. Today, we get an improvement in speed not by the speed of a single core, but by using multi-core computers and even high-performance clusters. So, the increase in performance is in parallelization and not in the performance of a single computing node. This has been exploited in mathematics in many ways. In many ways, not so much in symbolic computation, but numerical analysis has 30 to 40 years' experience with massively parallel computations and simulations of flows and turbulence have faced problems similar to ours in certain situations when they do parallel computations. That will come up later on in the talk when we borrow methods from them. But there's one big practical But there's one big practical drawback to massive parallel computations. That's you have to manage your resources, you have to manage your data, you have losses by communication of data between your cores, you have the problem of what to send to what node, and so on. You have to make sure that you kill processes when you need to kill them, restart them. All this needs to be done. All this needs to be done, and these are tasks that we need to think about when we want to do parallel computations in algebraic geometry as well. The ideal speedup of parallel computations is linear in the number of cores, but this is ideal. There's always something which impedes you to attain this speed up. So, first of all, of course, you can only get as much out of it as. Get as much out of it as you can do in your parallelization. If you have a large non-parallelizable part, a critical chain, then you won't gain so much from parallelization because there is still this chunk that you cannot break down to smaller pieces that you can parallelize on multiple cores. And I already mentioned it, you have communication overhead. If you have to communicate, If you have to communicate very often very small amounts of data with very small computations, then you might spend most of your time in communicating and not in computing. And last but not least, you shouldn't write to the same set of data from two nodes. So you need to make sure that you are locking data, but then another node has to wait. So there are limiting factors in reality. And let us And let us look at different kinds of parallelism. We have fine-grained parallelism. So that is where you synchronize and communicate very, very often in very short intervals. Typically, you find this in the arithmetic of computer algebra systems. They do that very deep down so that the user doesn't see it. You have coarse-grained parallelism, where you occasionally synchronize and communicate. Synchronize and communicate data, but not as frequently as in the fine-grained synchronization upstairs in the first item. So you can think of it like one being as fine-grained as, say, flour, and the other thing is as rice, corns, or whatever. That's the idea that's behind it. You have larger chunks that are being computed before you need to communicate again. And of course, we heard it yesterday. And of course, we heard it yesterday as well. There is embarrassing parallelism, where you don't need to communicate between the different parallel computations at all because, well, there's nothing to communicate. They are independent. Now, let's come to what was mentioned yesterday already: common belief about computing and algebraic geometry. We know that among the main workhorses is Gribna bases and in In the history of it, Buchberger's algorithm, we can do better now. F4, there are many things that have been improved, but still the main workhorse are Gruppner bases, and they don't profit as much from parallelization as other things. Typical properties of Gribner bases, they are doubly exponential in the number of variables. They don't have an obvious parallelization. An obvious parallelization. We heard that yesterday also from John, who said, Well, it's a wizard who parallelized that, and it was quite difficult. And still, it doesn't scale really well to many nodes. So, yes, there is no obvious parallelization, but you can do something. And of course, Gribner bases by themselves are very unpredictable in the practical computing time. In the practical computing time, if at some point you get one from reducing an S-poly, you're done. But if you don't, then it can go on for ages, days, weeks, months. And the result can be very unpredictable in size as well. Well, depending on what you get, even just changing one sign in the input, you might make the difference between having the ideal generated by one. Having the ideal generated by one or having an ideal generated by two gigabytes of data as a Gribna basis. So, in summary, it's not obvious at least for parallelizing computations in algebraic geometry. It's not a no-brainer. So, there has been some effort into parallelism in algebraic geometry already over the last decades, and there has been success, of course. Of course, there's parallelization on the level of arithmetic, as in any of the modern computer algebra systems. That's what you expect in a computer algebra system nowadays, that it makes use of the resources that are there by parallelization, also the low-level routines. There, we are talking about fine-grained parallelization, we are talking about shared memory, and of course, this is limited to the machine you're on. We have embarrassing parallelism, which has, for example, been exploited for zero-dimensional objects. Think of the normalization of curves, where you can just determine where the singular points are and then work at each of the singular points separately to get all the information you need. Or you can think of periodic integrals where you can decompose into separate. Where you can decompose into several integrals. All those things are instances of embarrassing parallelism. This, of course, scales and it scales also to large numbers if you have a large number of single instances of those objects of embarrassing parallelism. And then algebraic geometry also borrowed modular methods from number theory mainly. From number theory mainly, and does computations over fields of positive characteristic, be it via hands lifting, be it via Chinese remindering, to get results also in characteristic zero. This is available, this has been done, but looking at this, one and three are not genuine, massively parallel algebra. Parallel algebraic geometry algorithms. They are either borrowed from arithmetic or borrowed from number theory. And two, of course, is just limited by the number of objects that are independent. So up to now, we didn't look at anything that is really genuinely massive parallelism. But there have been other things in algebraic geometry that have been treated that give you. Treated that give you parallelism. So one thing can be summed up under the name of fan traversal. Those fans appear in different contexts, Gripna fan, GIT fan, looking at tropical varieties. And for example, for the Maury-Chambered decomposition of the cone of M06 bar, parallel computations have been made. Have been made, and using parallel computations, this could be cut down from eight hours already with the very efficient implementation, sequential implementation of Boom, Decker, REN, and Kaijer. This could be cut down to 21 minutes on a cluster, so we could win a lot here. But I But I don't want to talk about those single applications here. I want to talk about the structure, the structure of a fan traversal. How do you do a fan traversal? Well, you go someplace in your fan, you look around, you do the computation you have to do there, you look around, you check what neighbors you have, you add the neighbors you haven't seen before into the to-do list, and then And then you pass to the next item in the to-do list to again remove it from the to-do list, do the computation for that item, look for its neighbors, and so on. So that's a rather simple thing to do, and you can do it in parallel as well, in particular if you have a lot to compute in each of the items, and then a small step. And then a small step for passing to the neighbors. And here is the whole thing again in a picture. Here you see we start on the left side where you have the dot and then the box start. All rectangular boxes here are transitions. So those are the places in our diagram where something happens. Whereas the round or Round or elliptic boxes are places. That is, where things are waiting to be computed or put from the transitions when they have been computed. So we start with a dot in our place on the left side. It goes into start. That's where something happens. That the computation actually that we need in each of those items and then. And then we put the neighbors of it into the place that is labeled neighbors here. And we want to insert it to our storage, to our to-do list. And of course, inserting there needs to be enabled and disabled, depending on whether something is already reading from it or writing to it. That's what is on the right side with this enabled double circle up and down. Double circle up and down, and then on top you have the transition get frontier node that computes all the neighbors, all the frontier nodes. Then for each frontier node, it passes to neighbors, and then of course they are output to the next one, and so on. So we have a whole circle here. That's a very, very coarse diagram of what's happening. But when you compute native, When you compute neighbors, you do not just get one neighbor, you get many items. You put all those items into neighbors, and then the worker insert is picking one after the other and working on it, and then passing it on to enabled, where it then waits to be picked up into get frontier node, and so on. So you can think of So, you can think of those boxes always spilling out one or several balls that then wait to be picked up by an arrow originating at the place, and those are then used in the next transitions, and so on. That's on one hand, the general idea is a very general idea of a fan traversal. A fan traversal, and on this fan traversal, I also explained a way to look at such a parallel problem from an abstract point of view, from the point of view of a Petri net. That is, the items to be computed are just dots or balls that sit around in those round or Those round or elliptic places, and each of the boxes which can compute can pick something from there. And of course, if you have more than one core, if you do that in parallel, then several of them can pick something from places at random, more or less, whenever they are free to do something. Something and that means, in contrast to writing this down as a sequential algorithm, here parallelism is built into the net into the Petri net, and we do not do any heuristics on what to pick first or what to do first. It's just picked from this set of waiting items at random. So that is. So that is what I wanted to explain on a very general fan traversal. Now you might say, okay, a fan traversal. Whenever something is a fan, then you can do a fan traversal. That's something parallel. Okay, we spotted one thing. But is there more in algebraic geometry that is parallel? And for that, I want to give you another example, an example where we have to look hard to see the parallelism, but once you have spotted, Parallelism, but once you have spotted it, you say, oh, yes, of course, and that carries over to many other situations. So let us look at a completely different kind of example, at a more mathematical example. Let us look at the question whether a given variety is singular. Of course, in this audience, I don't tell anyone anything new. I just wanted to show that picture here. If I say the singular locus of hypersurface. Of a hypersurface is just the place where the Jacobian matrix has rank smaller than one. Or more generally, of course, the Jacobian criterion looks at the n minus d minors of the Jacobian matrix of the given set of generators. And then the singular locus is just the locus where x is singular. And this can be determined as the vanishing locus of the original generators and certain minors of the Jacobian matrix. I think at that point, you are all still following the mathematics here. That's how you would do it by hand on a small example. But let us look at a real-life example. And that example originates from the research work of Isabel Stenger, who constructs Who constructed Godot surfaces, and for some of her constructed surfaces, she just wanted to check naively, directly, whether they are non-singular, because that is one of the prescribed properties of a Godot surface. But her surface was described in a P12, so by 13 variables, it's a surface of dimension 2, and the ideal is generated. And the ideal is generated by 22 polynomials. So writing down the Jacobian matrix is already quite a bit of work, but then taking the minus of the approximate size, you are just killed by the number of miners that you see. It's 55 million miners approximately. And the Jacobian criterion has to test whether the singular locus is empty. Is empty. What does that mean? That means we need to check whether one is an element of i plus the ideal of the n minus d minus of j. So this is a Gruppner basis computation, or it's a radical membership computation, which is in its heart a Grubner basis computation. So all minors, and of course your original generators, which isn't important here. Which isn't important here, considering the number of minors, need to be given to the computation as input. And if you do Gribna basis, just think of the pair set. How many pairs do you get from this amount of minors? So you'll say there's no way this will go through naively, just without doing anything. Even if you wait for ages, then it will at some point just end because you run out of. And because you run out of memory. And the worst thing, you get a certificate of smoothness or singularity only at the end of all computations. It's not that you can say, okay, I have one singular point here, so the surface is singular and fine. No, it's really that you have to go through this Gribner basis computation to the end, to the very end, before you can say anything. Before you can say anything. So this tells us there has to be a better way. And for the better way, let me just fix the usual notation. We are over an algebraically closed field. We have some polynomials defining the ideal of our variety x. And we want to test whether x is singular without computing. The singular without computing the singular locus because checking whether something is singular by computing the singular locus is computing a lot more than you are asking for. But how could you approach this? Who thought about singularities from a different perspective? Well, Hirunaka did when he wanted to resolve singularities. He proved resolution of singularities in characteristic zero in 1964 by a finite sequence. By a finite sequence of blow-ups in sufficiently well-chosen centers. And he has a termination criterion for his algorithm, which is not the Jacobian criterion. So we have to look at that termination criterion more closely. So, what does he measure? He looks at the orders of certain elements. Certain elements G1 to Gs at the point P. What are those elements? Well, G1 to Gs are the elements of a minimal standard basis of the localization of our ideal at P with respect to a local degree ordering and sorted by increasing order, that is, sorted by increasing. Increasing the degree of the lowest term. And what else do we have? Yeah, the order is just the degree of the lowest appearing term, as I just said. So does that help us at first glance? No, because we would have to check it at all points. But what's Hironaka's criterion, first of all? What's Hironaka's criterion, first of all? It says x is singular at a point p if this invariant that we formulated, new star of xp, exceeds lexicographically a tuple of n minus the dimension of x times a one. So if there is one generator in the standard basis which has order Which has order at least two, then we know that the thing is singular. But still, we would have to compute that at each point. That's not nice because, in general, we have infinitely many points to test. So, we have to find a better way to do that. We have to find a way to compute in larger, let's say. Compute in larger, let's say, chunks or larger open sets. So we want to cover X in a suitable way. That is something to keep in the back of your head for the next things we look at. So what is the first entry? The first entry is the order of the Gribner basis, the standard basis element of lowest order. So it's at the same time the order. At the same time, the order of the ideal itself, the minimum of all orders of elements of I. By the way, if there is a question, I have my slides on full screen, so you have to yell if there is a question. And then, of course, I'm happy to answer questions, but I don't see anyone raising his hand or so because it's on full screen. So we know the order of I is one. The order of i is one if there is at least one element in i which is of order one. In other words, in that case, the first entry is one and in a suitable neighborhood of p we find a smooth hypersurface containing x. H is an element of i and it's of order one locally at p, so also of order one. So also of order one, locally, of order at most one, locally in a sufficiently small open neighborhood of P. And in that open neighborhood, then the hypersurface defined by H needs to contain X. That is the simplest instance of Hironaka's famous hypersurfaces of maximal contact. The locus of The locus of order at least 2 can be computed by looking at the original generators and their first derivatives by the variables. So we can check whether the locus of order at least 2 of x is 1, sorry, is empty. So whether the ideal that we have in So, the ideal that we have in here contains the one. And this is, of course, again a global computation and not a local computation at a point P. So we can check whether there is any point where the first entry of our tuple new star is one. Now, let us continue. We have seen we have such a hypersurface, and this hypersurface could. And this hypersurface could be used as a new ambient space. It's an ambient space which is one dimension lower than the previous one. But unfortunately, on that open set around P that we chose as neighborhood, this age usually is not just some a n minus one, but it's just something smooth which is isomorphic to it. So we know we have a regular system of We know we have a regular system of n minus one parameters there, but we have to determine it. So we are no longer working in our polynomial ring, but at least locally, we are working in a ring which is a power series ring in y1 to yn by the cone structure theorem. And we can even transfer this to the polynomial ring modulo h that describes our hypersurface. That describes our hypersurface, and then localized at some polynomial g, that is, we have to define a certain other hypersurface from which we have to stay away because their h is just not smooth. This G needs to describe a hypersurface containing all the singularities of H. And with this, we are We are on a set V of H intersected D of G, and we can do that at all points. So we can cover X by it. We can even cover X by finitely many choices in that way. And then on each of these V of H intersected D of G, we can find a global system inducing a local regular system of parameters at each point. System of parameters at each point of this intersection and these we can use as the variables with respect to which we need to take the derivatives. We can again check for the locus of order at least two, check whether it's empty or not, and so on. So, by means of Yoranaka's hypersurfaces of maximal contact and Contact and by means of covering X, we have a different method for checking our variety whether it's singular or not. The number of charts, of course, significantly increases each time we plug in such a hypersurface of maximal contact, but the codimension of X on that open set inside our That open set inside our ambient space drops because we chose an ambient space of one dimension lower. Is that a question? Okay, so if we reach a Well, if we reach a sufficiently small codimension, of course, then the Jacobian criterion becomes feasible again. And then, of course, we should switch over to the Jacobian criterion. But then we do the Jacobian criterion in a much smaller context, but many, many more times. But this huge Griebler basis computation that we had with 50 million of items we have to put in has been broken down to many, many smaller problems. Down to many, many smaller problems that we can compute independently. And this is the hybrid approach that I just described. So what are the challenges? First of all, of course, each time when we choose such neighborhoods, we want to choose them as large as possible so that we don't end up with an enormous number of charts, but just with about roughly as many as we need. Roughly as many as we need. We want to keep the number of charts low, but also want to do charts that are easily computable. There might be charts that are really, really hard, and there might be other charts which go through like this. So you have to be very careful and choose what charts to do first or which ones not to do first. Not to do first if you are in a sequential setting. If you are in a parallel setting, you'll do it differently. And of course, you have to be careful to do a consistent choice of the local system of parameters on each chart, which you can do in well-chosen charts in the sense you pick a regular system of parameters at one point and use the ones induced by translation. Translation for all the other points of this chart. So you see where parallelism is heading here. It's heading towards the charts that we need to look at. So the first challenge, choose large neighborhoods for new ambient space, means, first of all, we want large neighborhoods, so we try to quickly cover all of it. To quickly cover all of X with only a few neighborhoods. So we need a method to check whether everything's covered. That is, again, of course, a radical membership test for the sets that we had used to localize. And then we want to keep the number of charts low and we want to do easily computable charts. If we have many, many cores, then we can give. Then we can give all cores something to do, and we wait until those computations come back. And the fastest ones come in first. And as soon as we see that everything's covered, we can say, okay, we are ready. Everyone else, just stop computing, throw away your intermediate results. We don't need you anymore. So we let the fastest ones win and And the big bottleneck here is the expensive computation of derivatives with respect to a system of parameters, not with respect to the original variables. So if we look at this parallelizing problem, then we know we won't do that by hand with just a few nodes. We want good scheduling, we want good resource management, and Management. And most of all, we don't want to get our hands dirty unless we absolutely have to. So we started looking for a parallelization framework that does all the dirty work close to the operating system for us and without too much technical overhead for us as users. We found it. We found one that is also. We found one that is also robust against huge variations in computing time and size. That's something that is inherent to this problem, but that's something not all computing environments for parallelization can handle. Because if you use something ready-made, it needs to come from a world where the same thing can come up. And there is a world where parallelization is really everywhere in hyper. Really, everywhere in high-performance computing, and where you have those huge variations in computing time, and that is where you simulate flow and turbulence. Because where you have laminar flow, you can use large boxes, but if you are close to some surface in your flow, then you get turbulence and you have to compute with very, very small boxes. You have to get very, very fine. You have to get very, very fine, and you have very hard computations there. So, you have huge variations in computing time and size in these problems as well. That makes a parallelization framework from that world usable for our problem. The one we chose is GPI space of frown over ITWM. And as computing back ends, we used singular. Now, you might ask, what about Oscar as a back end? Ask what about Oscar as a back end? Now, here's the drawback: Singular is pre-compiled as lip singular, for example, and it is ready to be used as a back end. Whereas Oscar is just in time compiled. And if you want to pre-compile it, it gets huge. And therefore, it doesn't fit properly in there as a back-end. In there as a back end. But anyway, singular is used as a back end in Oscar. So we can go straight to the back end here. Now, what are the components of GPI space? First of all, we have a robust, distributed, scalable runtime system that does the resource management and that has a good scheduler, which even manages to kill processes which have been forked by. Which have been forked by processes that you actually want to kill. And its description is based on Petri nets. That is what I explained in example one to just show you how to think about parallel problems. So, PetriNets use by their structure the parallelism of the computing environment without any further, let's say, The let's say manual choices because this random choice of the items from the places is built into them. GPI Space also has a virtual memory management, but this one doesn't get along well with the internal memory management of singulars. So with the singular back end, we chose to switch that one off. What's the biggest advantage? Parallelizing in that way. In that way, by using GPI space, we didn't have to change the backend at all. We could use lib singular on the compute nodes and use GPI space because GPI space behaves transparent and can handle whatever back-end you give to it. It just manages the parallelization for you. And here you're And here you have the Petri net for the algorithm I just described. You start again on the left side, you go to the initialization and you get one or n charts. In each of those charts, you do a trivial check. You check whether you are. If you know that you don't have any data in there that you need, you can also discard. Now, on the bottom, more or less in the middle, you have this place. It says it's not smooth as soon as you have found at least one point where the order of one element of your new star is more than one. Then you have found a witness that it's not smooth. You can say, Hoyka, I found it. Say, hi, Erica. I found it and return false, and that's it. Otherwise, you have to compute a hypersurface of maximum contact and go into that loop on the left side. Until you reach the codimension, do the Jacobian check. And if it's not smooth, again, you do a hureker. Otherwise, you throw away the tokens where you know it's locally smooth. At some point, you'll run out of tokens. Point, you'll run out of tokens. And if you run out of tokens, you can return true, and that's it. So that's the general idea behind this algorithm now presented as a Patriot. Now, let us look at some practical aspects. So first of all, please only cover X. Please only cover x. Don't cover the original ambient space because you're only interested in points on x. As soon as x is covered, stop all the other computations and do the next step. That is, decide that it is already smooth. The algorithm will find the best covering because the fastest charts. Because the fastest charts will come back first. And as soon as all those fastest charts suffice to cover X, you know that you have a good covering. This covering, by the way, has further nice properties if X is smooth. On each of those charts, you have X as a complete intersection. So you have realized. So, you have realized the covering showing that x, your smooth x, is locally a complete intersection. And of course, then you can use algorithms that work with complete intersections and not just with general varieties. Usually, if you reach two or three minors for the Jacobian criterion, then you should pass to the Jacobian criterion instead of going further down because then the Jacobian criteria. Further down because then the Jacobian criterion is more efficient. So instead of the Godot surface, which I had at the beginning, we used another surface with prescribed properties, a Campedalis surface for doing the timings. Because for the Godot surface, timings always took a quarter of an hour, half an hour for each run. And for the Campedale surface, we can do 500. We can do 500 repetitions within a few minutes. So, for just doing more repetitions and get a better picture of what's happening on each number of cores, we used the Campidali surface. And here you simply see how the computing time goes down with choosing more cores until, of course, we run out of. We run out of charts in our cover. So at 256, we just don't have anything else. We cannot improve anymore. That diagram doesn't tell you so much, but if you look at the next one, here you have the speed up versus the number of cores. And the red line is the linear speed up that I presented as the ideal case at the beginning. You see, At the beginning, you see, at the beginning with few cores, we are almost linear, and at the end, of course, we draw it below this line as just a problem. This line as just a problem of measuring. 64 and 128 nodes, even better than linear. What's happening here? Sure, yeah. There was an interruption of a few seconds. Okay, when was the interruption? A few seconds ago. You started saying the first at the beginning it was linear, and then that's the last thing we heard. Okay, at the beginning it was linear, and it It was linear, and if you look to the very right, of course, it drops below the linear, that's fine. But if you look at the middle, somewhere between 64 and 128, then you see our dots above this line. Well, above the ideal speed up. That seems to be hard to believe. But if you see that you are ignoring rather Rather slow and computationally really complex charts by choosing good charts, the fastest ones who win, then this effect is really manifesting itself here around 100 charts, where you get a speed up compared to the sequential computation because the sequential computation passes through computation. Passes through computations that are actually not necessary, but come up somewhere on the way and are an obstacle to getting to the result fast just because you've chosen the wrong chart at the wrong moment. And with sufficiently many cores, that doesn't happen to you. Instead, you get the best pass through all your charts you have. Have. So that's the reason why we are above that line. Now, other possible settings. Well, whatever should or could best done be done in charts. That would be the first and most generic answer to what other parallel settings we have. And I want to give you, for the end of this talk, just one example. One example, and that's designularization. And for desingularization, again, I brought you a Petri net. We are starting at the left-hand side with the box in it, the transition in it. That gives us a number of charts. For each of these charts, we pass to find center. This gives us a pair. This gives us a pair XAF, that's our chart, and CAF, that's the center in that chart. Find center follows the tradition of Hiranaka and chooses the center as the locus where the governing invariant of the whole algorithm has maximal value. If the value is constant, then CAF equals XAF because all points have... Because all points have the same invariant value. We'll come to that later. But as long as the chosen center is still smaller than x, than our chart, we do a blow up and put something in C. After the blow-up, this will be split up in some way to new problems by just passing to the standard charts of Of projective space, just because each blow-up introduces a number of new variables, and of course, you want to pass the charts so that your number of variables doesn't explode on the way. Then, when you have those charts, you do on each chart a fine center and so on. And when you have a center equaling to your X on that chart, then you finish up. Then you finish up by saying done and glue. Done and glue means take all those charts that are finished and put them in D. Glue them together while putting them in D. If X is not fully covered, then you have to consume this again into dunent glue and glue it with the next one coming until X is covered or X is covered, or nothing else is coming from above, but usually until X is covered. And then when X is covered, you say, okay, we are fine. We have found an embedded resolution of singularities of X in an ambient space Z with some exceptional devices B in our input. And that is all I wanted to tell you. That is all I wanted to tell you, and I think I'm more or less in time. What's this? Hi, did you say something? I don't know, we're asking I was asking people they have questions. People, if they have questions. So, you talked about not being able to use Oscar because it had just-in-time compiling. Is the constraint for using the particular parallelization manager you chose at a VSC library? The constraint for that parallelization manager is that we need sufficiently small computing. Small computing backends. And if you look at Oscar, then Oscar in a pre-compiled way gets huge. So it didn't fit in the limits of that parallelization framework. You could think of other parallelization frameworks where this might not be a hindrance. You could think of clusters where the nodes have sufficient memory, are sufficiently large. Memory are sufficiently large to use it with Oscar. It's not a hard structural limit in general. It's just that this parallelization manager doesn't fit in with OSCA so well. But of course, you can serialize data out of OSCA and push it over by some Julia functions into the parallelization manager. More questions you didn't say, I think. So are you in the process of implementing resolution of singular parties or is it or Done. All right. So we are in the process of implementing resolution of singularities in Oscar. Yes. There was resolution of singularities in Singular. And something I didn't talk about, also because Matthias will talk about the scheme framework in OSCA, is that we do a complete re-implementation of resolution of singularities in OSCA. Of singularities in OSCA, because this time, in contrast to singular, we have the scheme framework underneath with which we can compute the resolution of singularities in a much clearer and more efficient way because we don't have to bother with a lot of technicalities that are already done in the scheme framework. You will see in first prototype for simple resolutions of singularities. More complicated ones are to follow. So currently available is Lippmann's resolution of singularities for surfaces and of course the naive ones for curves. So curves, embedded curves and surfaces by Lippmann's algorithm are already in the master and the others are not yet in the master. And the others are not yet in the master. That's the state currently. I was really asking about the parallel resolution of singularities. There is a parallel resolution of singularities for surfaces following the Kosar-Jansen CITO approach that was implemented. That was implemented by Lucas Rista within the scope of his thesis. He defended his thesis shortly before the pandemic. Yeah, a few words about the encoding. So she mentioned this messaging. And this messaging could be an overhead. Do you have? Is it using thread? Does it multiprocess? And you have to. What is the encoding? Can you describe how you're sending messages between your machines? You mean within GPI space? So to GPI space, you specify the Patriot in its control language and you specify the individual transactions. The individual transitions as individual, say, program code in a certain way. And GPI space then transmits the things. It's not multi-threaded. It is doing its own business. It's keeping all the computational tasks separate because it's a parallelization system. Parallelization system for large clusters. We are not talking about shared memory environments here. Does that answer your question? Yes, I did. Okay, let's take out the day. Thank you. Thank you. So, I think so. Except that again.