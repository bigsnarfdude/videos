Now you won't hear it, that's right, of course. Okay, that'll do. Right, so we all get pretty excited as data scientists about what's in the matrix and how to model the data matrix. And typically in genomics, I guess we've got features in the rows and different samples or cells in the columns. So in my lab, we've Columns. So, in my lab, we have a lot of different software tools for sort of filling out this matrix perhaps from the raw gene expression sequencing data in the case of single-cell data. So SC pipe and flames is about making this feature matrix for short read or long read data sets. Also, different packages for different features, maybe introns are of interest, or maybe you want to visualize the output of your data analysis. So, there's packages like Glimmer for that, and then perhaps the matrix actually has methylate. Perhaps the matrix actually has methylation or other interesting data types in there that you can get from long-winded sequencing. So we've got packages for that. But today I'm going to talk about some old work that's sort of new again in the context of single-cell data analysis. So the motivation here was a study of COVID-19 patients. So we have immune cells from these patients, PDMC samples, and we've got this sort of multi-group and then multi-sample within group setup of the experiment. So we've got healthy controls, severe samples. Controls, severe samples, moderate, and some asymptomatic samples. And the question is: you know, what is different about the immune cell profiles in these patients? And so you think, well, how would you analyze this big data matrix where basically it's very, very wide because you've got thousands of cells from all these different patients that sort of stretches this matrix out? So, fortunately, other people have looked into this for us. So, you can imagine modeling this in a big mixed model. So, there's one solution. So, you basically keep the big matrix. Solution, so you basically keep the big matrix as it is, or another solution that was mentioned by other speakers of what it is about the pseudo-bulk approach, where we basically collapse all of those repeated measures that we get within a patient that have some correlation structure and will be more similar perhaps than between patients, and then model the pseudobulk there. So, that's input for our analysis today. And so, other papers, including Mark Robinson's group, have shown that pseudobulk analysis performs just as well as fitting a mixed model to the As well as fitting a mixed model to a huge matrix, and it's actually obviously quicker to do as well. So, perhaps it's already solved, but I guess for this data set, we kind of look at it. And so, here's an MDS plot, and you can kind of see that some groups of patients are sort of more similar to each other than others. And so, that kind of raises, and of course, when we look through all these different dimensions, we try and see which ones are correlating with different covariates that we have. We put all those covariates in the mean model. We put all those convariants in the mean model of the data, but the wonder here is: well, maybe actually there's some effect on the variability in some groups that are systematically more variable. So perhaps we could model the variances differently to get around this. So indeed, if you sort of summarize the variability here, you can see the healthy controls have a lower sort of average variation, so about 0.15, compared to the most variable group of samples there, that are the green ones, which is about sort of in the Which is about sort of in the order of nearly almost, not quite, but anyway, quite a bit more variation in the least variable group compared to the most variable group. So yeah, our question here is how can we tweak existing methods that have already been shown to be quite good on this sort of data to model this heteroscedasticity in the data? So if you're familiar with how you deal with this sort of data in Lima, so we do a log transform to get the data, assume normality, we can use. The data, assume normality, we can use regular regression approaches for this. And so, typically, we do as input for this: you've got your design matrix where you put in all the effects of interest that you've noticed by making some diagnostic plots and saying, right, we want to put in definitely the different patient sample group, and maybe there's other effects in there that we put in as well. But what was noticed for this sort of data is there's sort of heteroscedasticity here. There's sort of a mean variance trend that we really want to take account of. That we really want to take account of if we're going to be able to use normally the assumption of normality in our analysis. So, this is the method called VOOM, where basically you've got this mean variance trend and you get sample observation specific weights that go into your regression. So, you do weighted least squares and that incorporates this home of heteroscedasticity. So, what we thought to mix it up a little is well, perhaps there's different trends for these different groups. So, maybe we could go through and estimate these trends separately. So, we basically These trends separately. So we basically split the data up, have lots of little design matrices that we run, and then we can get lots of different distinct mean variance trends that again allow us to derive weights that we use in our weighted least squares regression. So this is the first approach. It's called group by group. The second is actually adapted from something, very old work, work that I did as a PhD student, and we never really investigated it too much further. In terms of, well, if you model the variance. Model the variances here. So, okay, we've got two things. We've still got our mean variance trend, we want to take out account of that, but then we also want to fit a sort of a model to the variances where we say, well, perhaps different columns are sharing a different variance term. Say they, you know, there's a variance term for the healthy controls and for the moderate samples and for the asymptomatic and so on and so forth. And you can estimate this from the data. So essentially, you've got this extra term, this gamma here that you can. This gamma here that you estimate distinctly for the different groups, and that has the effect of moving this mean variance trend up and down a little. So if it's further up or further down, you're basically downweighting or upweighting the samples, sorry, the observations and the samples in those different groups to sort of account for that differences in variation. And so this gives you a sense of sort of sample specific, but of course they're shared across the different samples from different groups. So you have this block, so we call this. So you have this block, so we call this Vermont quality weight block. So you've got this block design for your variance model that you fit to the data, and you've got these gammas that you can then put in and they update this matrix of weights that sort of takes into account that mean variance trend and then that group specific variation as well. So of course we want to test this out. So we start with some simulated data and then move on to our COVID-19 data. And of course inspect this across lots of data sets as well. Of data sets as well. So, although I've showed you a COVID-19 example, we could see this sort of group-specific variation, lots of other data sets you could look at. And then compare that to regular bulk RNA-seq methods, which have already been shown to perform quite well on this sort of data sets. So, those are our sort of reference, you know, bare minimum, not taking account of the group-specific variation. So, we did many different simulations. This is just to kind of show you that the simulation parameters we said. With the simulation parameters we're setting here, which are detailed there on the right-hand side, you can actually get very distinct group-specific variations, and they have this effect of shifting this foom trend up and down. So we're basically saying we've got a couple of groups where we simulate to have higher group-specific variation, and a couple of groups to have lower group-specific variation. And then we simulate DE for 50 genes in each group. So we've got a set of things that should read out and a certain fold change and a set of things. Certain fold change and a set of things that are all, if they're discovered, false discoveries. So, for this setup, so if we compare the high versus high group, so the first three bars here are the standard methods. So, some edge our options and a Voon, just assuming that there's just one grand mean variance trend. And then the last two methods are the new ones that I've mentioned. So, this looks a bit disappointing because you go, well, you're actually losing a bit of power there, so a 0.05 cutoff. You make fewer discoveries. Cut off, you make fewer discoveries. But if you think of what you're doing here, you're actually less confident because you know these groups are more variable. So rather than using that global trend, we actually discover fewer things. But I guess what's important is we've made fewer errors at doing that. If we compare the low versus low group, we sort of have the opposite effect. This is where we sort of get a bit more power because, again, the global trend is sort of higher than it should be. So you're overestimating the actual variation that's in those data. So there you can discover more things. And again, there are. You can discover more things, and again, the error control across all of these simulations is quite good. So, you can just basically hold your size for these newer methods, whereas for the other methods, they actually make a few more errors than you might expect. So, we go through and do lots of different simulations here. So, in the interest of time, I've deleted a few slides. But the settings are generally some null settings. So, let's just have all of the groups have consistent group-specific variation. So, there's nothing to see there. So, then basically, it doesn't matter which configuration you have here. It doesn't matter which configuration you have here. All these methods recover roughly equivalent amounts of genes. Another setting is if you increase the technical variation there by, say, increasing the number of cells in the different groups. So this is effectively increasing or decreasing the library sizes. So it's technical variation that you're loading on top of these and again things sort of read out as you might expect. So if we go back to our COVID-19 data, we can find compared. We can find compared to standard methods, EdR, Vloom, we do find a good corpus of, say, 356 genes that are common between all of them. But our newer methods also find some additional genes. And so I guess the question here is always, well, is this just noise that you're plumbing the depths of in your data, or is there any signal there? So if we test some of these genes and do some gene-enrichment analysis of these, you can find some interesting pathways there. Interestingly, in the There. Interestingly, in the original book, before we had our method, they made some statement about increased activity of interferon gamma genes in these particular cell types, these NK cells. And they could notice that in a subset of the asymptomatic patients, there was increased expression, but then some of them, not all. So if you actually incorporate the increased variation in your model, you can find, and in this case, it's if we use the boot by group approach, we find an extra hundred and something genes. We find an extra hundred and something genes, and indeed they read out this particular phenotype for different gamma signal. And that's been reported in other papers as well. So the idea of looking for this was not just pick a random pathway. It's like, well, other people have found this as well. Why is it in our data? It's like, well, it is in our data, but it's kind of maybe suddenly masked by this increase in variation. So to summarize, if you kind of make these sorts of pictures of your data and you notice there might be some increased You notice there might be some increased variation in one group versus another. There's a couple of options you have here. So, move by group on the far right is essentially this specific group-specific meat variance trend. Some caveats here is that if you have a really complicated linear model, this doesn't really work because you imagine if you subset different parts of the data, you can't actually estimate all the different parameters that are in there appropriately. So, you kind of be missing something. Appropriately, so you kind of be missing something, so maybe in that case, it doesn't help. But in the middle, there you have a sort of more general purpose, you've got a more stiffer mean variance trend, but you can move it up or down, and that still does a pretty good job, and that can sort of deal with any arbitrarily complex experimental design. So here's the updated slide for Ellis, which is our paper was published. So this is work from Yu. Ew, who's a PhD student in the lab, has just finished, submitted her PhD. So there's code there if anyone wants. PhD, so there's code there if anyone wants to try it. And we were celebrating US paper at lunch recently. So thanks to everyone in the team for that. And I guess I just leave you with some papers related to this work in case you're interested in chatting about other topics in this sort of suite of tools that we create in the lab over the break or have any questions now. Very nice work. So how much of that group system variance is due to differences in the portion of express genes? So how much of it is actually due to variation in the expression levels? And is it worthwhile to split it into these two? Yeah, I guess we're kind of just bundling all of this variation in the we don't really know it's too hard basket. Whereas I guess as from Augusta's talk, you can potentially try and model it as sort of a mean effect rather than just bundling it. Of a mean effect rather than just bundling it, and we don't really know, just putting the variance part of the model terms. So I don't haven't specifically looked trying to tease apart those things. I mean, do you think that if you knew about that, you could put that in as extra covariance, maybe? So, like, I guess if they're quantitative measures that you have for the different samples, then maybe they're useful things that you would actually say, well, actually, I can calculate that from the data, and maybe it's a useful covariant to put in the mean model. So, yeah. But I guess wherever possible, whatever the collaborators tell us, we try and put as many of those main effects in the model to soak up as much as the kind of known variations as possible in the model. And obviously, a lot of those covariates, they want to read out and find the differentially expressed genes for those with different cell types. So, yeah, we have to put those in a list. It's the idea of more like pseudo-bug formats also. Bug from my own signal cell sort of push the data for like a non log normal distributor data so that you can tell. Yeah, it does help. So I guess if you're going to collapse cells together, then the sparsity, the data will be less sparse necessarily. So there's one good helping point. I guess the other thing is that your mean variance trend looks a bit more like what you'd see in a bulb experiment too. You'd see a bulk experiment too. So you're kind of going, well, by doing this, you're kind of making it. The assumptions are kind of seem reasonable. You don't have super weird meat variance trends, so it seems to be okay. Compared to what would be in a single cell, if the trend is a component direction for some things. Yeah. So yeah, the trends look less weird once you pseudo-bulk them. So they look more like things you would have seen before if you've dealt with RNA, bulk RNA secant in the past. Governance has taken in the past. Fantastic. Well, if anyone has any further questions, please hit Slack for the next lunch or coffee break. And thanks again, Matt.