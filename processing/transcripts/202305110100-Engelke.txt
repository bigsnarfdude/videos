Thank you very much. Thanks for having us in the presentation. It's a nice event. And it's a pleasure to present some of our recent work here on extra model models. And since this is a kind of broad audience of people from spatial statistics, extremes, machine learning, and client, I'd like to rather give an overview of a high level. Higher level than going very much in detail. And yeah, this actually worked a lot. Of course, I will mention one on the way to the different places that we wrote recently. So to start off, two examples of where we might be interested in modeling the extremes of regional economies. The first one is a paper that we wrote with Bai Man Asali and Anthony a while ago on river flows. And there, of course, you're interested. And there, of course, you're interested in understanding the floods at these different 31 populations. This is the new basin in southern Germany. And you might ask questions about interpretations. So, you want to understand what is the graphical structure between these interpretations, how do floods actually interact with each other, and what you might hope to find is not just the physical flow connection structure, but the additional connections. Structure connections tell you something about other properties of the basin or spatial properties of the rainfall patterns. I was also interested in finding the best statistical model that explains in the best way the extremal dependence on this data set. That means we like to have a range of models with different complexities that you can regularize and then attune to have the right amount of door for sparsity. And that's another data set on Switzerland, temperature data, this was 2022, which was actually the hottest summer in Europe. And here are some stations in Switzerland, but you can really think of this as stations around Europe or the whole world. We have many, many stations. Here I show actually the largest average temperature. We're during that summer in these different locations. Again, think of this as a high-dimensional problem where many stations will now go. Probably where you have many stations around the globe, and we're not really interested here in the graphical structure between these points. What we really care about is to have a good statistical model that explains the extremal dependence, that models the extremal dependence, that might possibly be non-stationary and like this animation, but also simulations and so on. And what comes to mind, especially with this audience, is an SPD-type approach for this data that would allow for faster estimation. Would allow for faster estimation of contents, faster conversations. I'll come back to this to make some as well. So, this is just to show you a motivation why we could be interested in general terms sparsity. It's obviously many benefits. One would be that we have interest validated better on lower dimensional structures, like we have mocked networks and sparses. We might be able to characterize densities from higher dimensions to lower dimensions. From higher dimensions to lower dimensions, which makes the inference much more efficient. We can think about high dimensions where the samples can be smaller than the dimension of our problem. The discussion is pretty big, you know. Usually we only use the largest observation of our sample, which all of the number of sequences, which is much, much less than the whole number of sensors. And that means we should really embrace any additional infrastructure that we can find in our metadata. In our metadata. And people in the same space develop all these data work from different approaches and data approaches to the text. And what's interesting about farmers in mixed models. It's a different type of farm. Think of what types of variables can be extreme together. Competent extremes. You can think about graph modeling approaches or custom CPC extremes. And people in this room and in the community have a lot about these approaches. And a few years ago, we got to rapidly evolving field just to take a snapshot of what existed at that time and try to categorize those approaches into different groups. And I also want to talk about one slide about technicality, which is the wish for that method. With some bushman masters and geographical models. But before I do this, like this slide I mentioned just yesterday, I wanted to say something about this general model. We have heard a lot of different ways in this first part. I just want to highlight page and all this topic. There will be other talks after this also with these models too. Just to highlight quickly. Okay, so that's the first one that we use to set up this left chapter with different shows. Depending on the data you have, as we talk about the and other asymptotic image points. In both of them, we can think about looking at giving maximum or just maximum or threshold diseases. These are kind of very organized approaches, just with different distributions that we see. For example, as I think, we can have for point-wise, we can process everything. Professor HATO processes, but also for our HATO processes. Yeah, these two things are very related. Okay, so these two big different approaches you can now in both asymmetric dependent and independent models you often have most of the performance R plus some basic W by R. And you can think of this as an increase in digital dependence to this. And to this run to the W and then playing with the tail heaviness of this common scale and then the tail heaviness in the spatial field, we can change the dependence structures. And I also mentioned some more models of this form. With Jenny and Thomas, we actually analyzed models of this type in general and tried to characterize what dependence structures you would get with different tail havings these different parts. And I mean, I'm mentioning this because the price of asymptotic components would have an exponential variable, and then you do a lot of lifetime. You can essentially see this as an RFS. There's a connection to the Aussie process. Essentially, these two are the same. One is the spatial formula, the other one is the finite dimensional of this process with this dimensional detail, and this is the Gaussian distribution. And this is why we can see those two things that have the analogs of Gaussian distributions of Gaussian processes. Distributions that govern some processes in asset projectly dependent exchange. And we'll see some analog results in this talk and in my other talks later. So, this is really just a slide to be on the same page and hopefully just collect the different types of analysis. We will work this work with asymptotic EV kind of models and later on concentrate on these rather than size distributions for reasons that are very simple. For reasons that I will explain. Why we do this, often these isoprologically independent models might not be suitable for all types of data, but they have a very nice structure that we can well understand. It's often a starting point to actually understand new concepts in this level. And if then we have data that exists asymptotic independence, we can still generalize this from the understanding that we gain from these models of market value. These models of multiple processes. Okay. Okay, now I'm coming back to the sparsity, right? That we want to have interpretation and so on. Until recently, basically, these two fields of graph modeling almost separated, we're not able to use the very powerful tools that people have developed in conditional tenants modeling and graph modeling together with our classic extreme volume models. And this is a And this was kind of the starting point for the staff to think about a way of combining these two things. So one problem is that if you think about maxing processes, this is something I've never mentioned, that you could actually resolve by your lemma, that then for maximum processes can actually respect thresholds. And we thought we were able to find something interesting. And this is exactly what we tried. I tried to combine these two fields in total and introduce the notion of sparse traditional increases and graphical models for these threshold extreme models in extremes. And this was a discussion basically, so there were a lot of means with this approach. And there were actually more questions after this before writing this paper relating to how can we do efficiency and inference in these models, how can we give a graphical structure take a different way, how can we have construction models, and when we think about asymptotic independence and graphical analysis at the same time. I want to just talk to the meeting on some advanced information on these questions. Okay, so as I said, max distribution is alone, whereas maximum approach and maximum distribution does not seem to do for the optional. So we were rather looking at threshold experiences. In the workshop, this is also a kind of a perspective that is become much more popular in extremes than this external approach. So let me quickly define what this is. So here, to be simpler, normalize the model distributions to standard operator. Standard operator, we are really just looking at the dependent structure between the components of the three-dimensional vector x in the experience. So, what we do in this threshold approach is we define threshold view and we look at all the existing points that exceed this large threshold u. And here we say that we look at observations where at least one of the components exceeds the threshold q. We can also define other exceedances where we take other risk functions in the maximum. Where we take other risk functions and the maximum. This is then called R value for processes, the limits of this. So then you find a proper limit distribution. And then the proper normalization distribution converges with a limit. And then this limit is called a multiple distribution. So this is really limiting a distribution of all the extreme observations. That we're interested in and that we're trying to model. And by consumption, now that you normalize it, is this distribution system in this space where we have all this small points, essentially, and we have the large points. We control the space around really large distributions, non-parametric distributions, and The question is: how can you add any practice on this type of distributions? The reason is not direct because it's not from the space, so it's not directly. Not directly any author or linearized of the conditional linear because this really maintains. Okay, so but first let me introduce what conditional independence and graphical controls are. We can somehow read that. So conditional independence we can define as an absolute which means this was our two G, so we have to have a new background. You would say that traditionally independent KC with the We don't know anything new about A, we know B, and we sort of say so. If we know C already, we have C already, we don't know anything new about A if I know also C B. So A and B are conditional independence. And we can link this condition independence to graphical models by this marketplace. This is a microcopy, which says: if I have a graph like this, and if there's no edge between two nodes of the graph, then we should have a position between these two nodes. You're given the remaining nodes. For example, one linear condition, you're given two and four. Sometimes I'm better remote as we also model the difference between. Um, classically meaning this is just a function of factorization densities. Um, and of course, for property, that actually can be very easily distributed. So that's very nice. I like to have something like this. So it's a most picture of processes of distributions are defined on most. So we have to reduce the spaces to products because to be able to define the position of the analysis. And we do this by saying, okay, let's just restrict this distribution to this displacement vector y. We conditioned y, we're conditioned on the vector that the first verb was larger than the one. And that will give us a condition on. Look at the two recognition of the flag that the second components are. We have another big vector. And now we say that in the notion of extremal conditional independence, all of these auxiliary vectors, we have conditional independence in the usual sense. So this extremal conditional dependence is really defined as the usual conditional independence, but on all of these product spaces. And we can actually show that enough to look at one of these product spaces. That enough to look at one of these product spaces that will already be sufficient to define this conditional independence to hold actually for all of these conditionings. And well, extremal graphical models are then defined really just in the usual way, but using this new notion of conditional independence that is Taylor made for these types of distributions. All right. So, now to make this useful statistics, we want to use. Statistics we want to use would like to look at certain types of parametric models. And I mentioned before this class of Hisler-Reiss distributions and Brown-Resnick processes. This is really just the parametric class of multiple Pareto distributions of processes. And as I argued before, you can think about this as the analog of Gaussian distributions of Gaussian processes in extremes. But differently than Gaussian distributions, in fact, these Laris distributions are not. In fact, these Larais distributions are not characterized by a covariance matrix, but they're characterized by a variogram gamma. So this is something like the covariance matrix, but for intrinsic Gaussian fields. And one can actually define, this is what we did with my PhD student, Marnel Henschel and John Ziggers, one can define a related Hislagreis precision matrix theta, which essentially is a pseudo-inverse of this paragram gamma. Okay, and with Verogram gun. Okay, and with this matrix theta, we actually get this nice property that we can read of conditional independence in the extremal sense now by simply looking at the zeros of this matrix theta. There's also a link to these intrinsic Gaussian fields that people know probably from geostatistics, and Peter will talk more about this in his talk. But just as a remark, it's not as nice as in Gaussian processes where we would. Processes where we would have just a normal inverse, but here actually we have a theta that is a pseudo-inverse and that is not invertible since one vector here is in its kernel. But anyways, we have this matrix that allows us to identify extremal graphical structures simply in this parametric way. Now, once we have this, we can actually ask how do we do inference. And let's first assume that the graphical structure is known, someone gives us the graphical structure. Someone gives us the graphical structure and would like to estimate from data a model on this given graphical structure. So, if we estimate the parameter matrix of the Sysler-Rice distribution just empirically and then compute the corresponding precision, we'll see that this matrix is dense and doesn't have any graphical structure. This is just by this variation, stochastic variation that will never estimate exactly zero on the precision matrix. And so, the goal is really to find a graphical. To find a graphical estimator from this that has zeros at the correct locations of the graph. So, for example, if we have this graph here, we'd like to have a zero in the precision matrix at location 1, 4 and also 1, 5, because there's no edge between 1 and 4 or 5. So we'd like to have this pattern of the precision matrix, but at the same time, we'd like to have certain values on our estimated variable matrix. And the question is, can we actually solve this matrix? Is can we actually solve this matrix completion problem where we start with our empirically estimated parameter matrix but also want to have this to be graphicable on that given graph? And yeah, we could actually show that this completion always exists and is unique for any type of graph. And the completion is much easier if you have a simpler type of graph, if you have a very complex graph, a non-decomposable graph, then these completions are more complicated. Then these completions are more complicated, and you can still compute them, but it's kind of less direct to compute them. It takes longer to compute them, but they exist and they are unique. So we can do actually inference on given graphical structures with this result. Now, in any practical set, that's not true, probably, but in most practical applications, you don't know the graphical structure, you just know your notes, and you would like to understand. Your notes, and you would like to understand or learn in a data-driven way what graphic structure underlies your data. And well, there are different approaches to this. One thing that we tried is to push positivity constraints on this precision metric theta. The goal here was not really to estimate consistently the graph structure, but to introduce a sparse model for statistical inference. And this means that while this is a good statistical model, it does not have this guarantee of It does not have this guarantee of consistent graph recovery. If you want to have consistent graph recovery, there are two different approaches. Either you restrict yourself to a simple class of graphs, for example trees, and on trees you can use something called the minimum spanning tree or the extremal version of this. And we can show that in a non-parametric way here, we can actually recover the true underlying tree structure. And yeah, if you want to know the details of this, you can look at This you can look at this paper with together with Standislav Burg. Now, often the graph structure will not be just a tree but more complex graph. And so for general graphs, we adapt something that in the Gaussian case has been very well studied, namely estimation by L1 penalties, so graphical Lasoux type approaches. That means we want to put a penalty while estimating the parameter but the precision matches, we need to put a penalty, L1 penalty on the entries of this. Lung penalty on the entries of this precision matrix, which will make sure that you estimate zeros at some of these locations. Here it's different than a classical graphical LASU because the CPA is not invertible. So you have to think about pseudo-determinants. And also this Lighthill problem looks slightly different. It's optimizing all the semi-definite matrices. But still, if you analyze this problem, you can show that this is actually convex and you can show consistent graph recovery of any general graph, even in high dimensions. Of any general graph, even in high dimensions, meaning when the dimension grows and is larger than the sample size. This is in this last paper. Okay, so that means we can estimate graphs in given graph structures, but we can also estimate graphs in a data-driven way from the extremal observations of our data set. So, let me come back to the two examples that I showed. Back to the two examples that I showed in the beginning. So, the first one was this river example where I wanted to understand the network structure, the probabilistic graphical structure that is behind this data set. And yeah. So, what we can do, we've implemented these graph learning methods in a package called graphical extremes. It's very easy also to load this data. It's included in this package. We can either fit an extreme. We can either fit an extreme or minimal spanning tree or this eGlearn algorithm for general graphs. Okay, let's start with the general graphs. So, here I show the sparsity matrix and the estimated graph. We have a tuning parameter. I should have mentioned this in this estimation of the graph. We have a tuning parameter that allows us to interpret or to choose the complexity of the graph. So, this large row, small row values actually mean that we have a very dense graph, which we can see here. Very dense graph, which we can see here. If we include, if you increase this row, we get a sparser and sparser graph. Okay, so you can see we get less and less edges in that graph. And we can also compare this to this extremal tree estimate, which is forced to be a tree structure. And we can see, interestingly, if this row becomes larger, we're actually approaching this tree structure without really imposing this. And this is kind of intuitive here because physical flow connections on the tree will have a very strong influence. Tree will have a very strong influence on the dependence, of course, between fluids. Now, what is the best tree structure? Well, we can look at this for by cross-validation or by looking at these PIC values and as a function of this row tuning parameter. And we can see that, well, we find a certain value for this row. We can look at the corresponding graph and we see it resembles the tree, but it has additional edges. These additional edges probably These additional addresses probably allow to explain us more of the extremal dependence that not only comes from the physical connections of the river, but also from spatial precipitation fields that might include additional dependence, even between stations that are not directly flow connected. So this would be the best statistical model in this class of graphical extreme LA models. Okay, coming back to this other example of heat waves. Heat waves. So, here, really, we were not really interested in the graphical structure, but rather in a spatial model. And, well, spatial extreme value models, we can look at Brown-Resnick Random fields, Brown-Resnick processes. In this case, this barogram matrix gamma that I mentioned before and the Hislav-Weis distributions now is a function gamma, small gamma, of simply the distances between two locations in its simplest form. And there are popular variables. And there are popular variam models that Anthony also has in his review article. For example, of that fractal form here, which essentially is h, which is distance to the power of alpha and to scale kilometer. And there are also other variant models. But the problem with all of these popular varygram models is if you look at the finite dimensional distributions of this and the corresponding precision matrix, then this will have. Precision metrics, then this will have a sparse structure. So we will have a dense structure. So there's no sparsity. And that means if the number of locations in Switzerland or Europe or the world is very large, then we have very slow computations of order d cubed. And this is something that we would like to avoid if we have these large-scale applications. So there is this nice paper by Finn and co-authors on this SPD approach to Gaussian modeling, where they In modeling, where they essentially say, Well, we may be not interested in the graph structure, but we can actually create a graph structure triangulation on these points and then try to construct a good approximation, good graphical approximation to that spatial data. And they show that this can actually be done for covariance functions that can be represented as SPDE models. And then we can actually have this finite element approximation. These finite element approximations that give rise to sparse approximations for these fields. Now, the natural question that comes to mind now: if we talk about ground-rasenc processes, we no longer have this covariance function that determines our model, but we have actually this variogram function that explains our model. And can we actually find a new class of intrinsically stationary SVEs of Gaussian fields so that we get a very rum? A variogram that can be approximated in a sparse way. And that would then give rise to a brown-rested process that has a sparse approximation in this extremal graphical model sense, and that allows for efficient computations of a much better speed than in the dense case. So, on this topic, I won't say any more because Peter will talk about this in more detail. To conclude, let me just summarize the Conclude, let me just summarize the progress we have made. So, in terms of statistical inference, we did some statistical inference in the original paper, but now we showed actually that we can, on general graph structures, estimate graphical models. We can also estimate the graphical structure. It's unknown, either on trees or on general graphs. Can we have sparse spatial models? Well, Peter will tell us. Models, well, Peter will tell us if yes or no. And I didn't talk about asymptotic independence here. Our models first, what I present here, is for asymptotic dependence, but we can use kind of the same principles or ideas to construct asymptotic independent models for graphical models for extremes. And this is what we did in this paper with the Yevgeny and Pierstein. And there, we actually showed that. And there, we actually show that this notion of extremal conditional independence is actually much more general than we thought, and it can be applied to other kinds of processes of distributions. For example, Levy processes. And this also gives a very natural notion of conditional independence and graphical models for such leading processes. But this is a bit of a different topic. That's why I didn't talk about this here later. Okay, with this, I would like to. Okay, with this, I would like to first say that you can find the code on GitHub or in this graphical extremes package if you want to try out these methods. And thank you very much.