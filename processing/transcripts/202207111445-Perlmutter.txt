Hello, everybody. Just for confirmation, can you hear me? I sometimes have been having some microphone difficulty lately. Loud and clear. Yep. Okay, perfect. Yeah, so thank you very much for having me here today. And I do want to echo the sentiment that I am definitely jealous that I can't be with all of you. It looks like it's amazing. But in any case, without further ado, I know I'm the person keeping you from dinner. So I'm going to be talking about the scatter. So, I'm going to be talking about the scattering transform for data with geometric structure. And so, I'm going to begin with a review of the Euclidean scattering transform and then talk about different versions of it for graph or manifold value data. And then, in the last part of my talk, I'm going to talk about modifications of these architectures that incorporate more learning and trainability. So, the scattering transform originally emerged as a theoretical model of convolutional neural networks, where it differs from other neural network-like architectures by using predefined wavelet filters. And the idea of this was that when you had these predefined filters, you could analyze it rigorously and prove that it had desirable stability and invariance properties. And in addition to being a theoretical object, it was also used to. Theoretical object. It was also used to get very good numerical results in certain situations, and in particular, it requires less training data. So when I say it's a model of convolutional neural networks, what I mean by this, we should have in mind the example task of image classification. And when CNNs perform image classification, what they're implicitly doing is first learning a hidden representation in a high-dimensional space. Representation in a high-dimensional space of each input signal. And then they are applying a classifier which actually makes the decisions. And so when I say scattering is a model of CNNs, what I really mean is it's a model of the hidden representation produced by CNNs. Then, once you do the scattering transform, you can then use whatever your favorite classifier is if your task of interest is classification. So, what scattering does is it produces a hidden representation of an input signal based on a multi-scale wavelet transform. Wavelet transforms are something which emerged in the 80s for image processing primarily, where what they do is they take a mother wavelet psi, they dilate it at a scale of two to the j to get a filter called psi j, and then they convolve the input signal. The input signal with this dilated filter. And so you can picture these at different scales of resolution, where we've assumed that the mother wavelet psi has mean zero. And we can also think about it in the Fourier domain under the heuristic that the frequency support of psi j is going to be an interval described by 2 to the minus j. So different values of j are either looking at an image through different scales. Looking at an image through different scales of resolution or capturing different frequency bands. One of the primary applications of these was for image compression because the wavelets will tend to have the effect of sparsifying natural images, where initially we had a big photo of a castle. And what we did is we downsample to get a low resolution version of the castle, which takes up less space. And then we have these high-frequency filters. High frequency filters, which are capturing the fine-grained detail. And the nice thing about this is these high-frequency, high-resolution wavelet transforms are mostly black, mostly zero, and so they're efficient to store inside of the computer. The scattering transform is a representation of the input signal that's based off of a multi-scale iterated wavelet transform. So in particular, what you first do... So, in particular, what you first do is you take the input signal, you convolve it with the wavelets, and then you apply some nonlinear activation function. It could be RELU, but in the scattering transform literature, it's more common to choose the modulus or the absolute value. And then you repeat this process as many times as you like, where you'll do an alternating sequence of wavelet nonlinearity, wavelet nonlinearity, wavelet nonlinearity. wavelet nonlinearity. And then you extract either fully invariant or partially invariant measurements from these in one of two different ways. The second way is that you just take the L1 norm of these signals. Or the other way is that you can convolve it with a low-pass filter, which you could think of as a very high Very high variance Gaussian. And this was originally introduced with the goal of increasing our understanding of how CNNs work. And in particular, it was designed to have the properties that you would want a good representation of an input signal to have. Namely, it's going to be stable to additive noise on L2. It's going to be invariant to translations and possibly other. Translations and possibly other group actions such as rotations. And we also want it to be sufficiently descriptive. And what this helps us understand is why is a multi-layer non-linear architecture better than a very wide linear architecture with the same number of parameters. And the reason is that a linear network can be invariant or descriptive, but not both. For example, Both. For example, if you just average a function over the real line, you get something which is Foley translation invariant, but you're throwing away tons and tons of information. And in particular, if you think about this in the Fourier domain, this is the zeroth order Fourier coefficient. So you can think of this averaging as discarding any useful high-frequency information. On the other hand, filters which focus in on high-frequency information tend to be unstable. So the idea is that the filter is a very important thing So, the idea of the scattering transform is that the wavelet can capture useful high-frequency information, and then the modulus will push this information down to lower frequencies. And what Millah was able to show in 2012 was that the scattering transform has these desired stability and invariance properties. Now, this was mostly meant as a theoretical object, at least originally, but Object, at least originally, but it's also quite practical in a couple of different situations. One is if you don't have that much training data, you're not able to train the filters, but this means that the scattering transform is going to be a lot less data hungry because you don't need to train the filters. And an illustration of this is a data set of 79 paintings, 64 of which are real Van Gogh's and 15 of which are fake. And scattering is able to tell. And scattering is able to tell the real Van Gogh from the fake Van Gogh's with 96% accuracy. The other thing about having designed filters is that if there's physics you want to build into the problem, you can design the filters in such a way that they're capturing these physics. So, this was an example from some of my old lab members at Michigan State, where they were interested in a material science problem related to the formation energy of Related to the formation energy of lithium silicon molecules and the underlying physics of the Schrödinger equation. And so, what they did was they took wavelets which were closely modeled after the eigenfunctions of the Schrödinger operator. Now, the descriptive power of scattering compared to, say, a linear transformation can be illustrated by the following example, where we're going to consider different. Where we're going to consider different textures. And I'm going to make the bold claim: the claim that this texture right here looks absolutely nothing like this texture right here. The problem with this claim is that if you do traditional methods, such as taking the power spectrum of these two signals, these look nearly identical. And even if you take first-order wavelet coefficients, these will again look nearly identical. Identical. But the second-order scattering coefficients look very much different. And this shows that the scattering transform is able to capture what is visible to the naked eye, namely that this picture right here looks absolutely nothing like there. So this is kind of a fun way of illustrating the power of a deep nonlinear multi-layer architecture. The other thing I want to mention is. The other thing I want to mention is that the scattering transform, we talk about it as a model of neural networks. We often talk about it in the context of classification. But fundamentally, it's an embedding. It's a transformation, which means you can use it for whatever task you're interested in. And beyond classification, a task which I think is particularly exciting is a synthesis. So here we've got a data set of textures. And the goal of the synthesis problem is to come up with new textures which kind of look like the original. Textures which kind of look like the original textures. So here's some work by Malah and Bruna, where they synthesize new textures through a variety of methods. And what you can see is on the far right, the scattering-based results look the most similar to the original method. So that was it for the background portion of my talk. And I should mention everything I've said so far. Everything I've said so far is a background that was done by other people. Most of the stuff from the second portion of the talk is going to be projects that I've been involved in. But the second half of the talk, I'm going to talk about adapting the scattering transform to graphs and manifolds. And the key challenge is coming up with new wavelets for these more general domains. And once wavelets are defined, the scattering transform has the same definition as before. Definition as before. It's simply going to be an alternating sequence of wavelets and non-linearities. I also do want to acknowledge there's been a couple of different versions of the graph scattering transform introduced by some various good papers, including Fernando, who you just heard from a little bit ago. So, coming up with a notion of wavelet convolution on graphs or manifolds. On graphs or manifolds. First thing you'll want to do is you'll want to come up with a notion of convolution on graphs and manifolds. And so what we're going to do is we're going to take some sort of Laplacian type operator. So if we're on a graph, this could be the graph Laplacian, either the combinatorial or the normalized. And if we run a manifold, we could take the Laplace-Beltrami operator. And for either of these Laplacians, we then take it, we take a basis of eigenfunctions or eigenvectors. Of eigenfunctions or eigenvectors, phi k associated with eigenvalue lambda k. And you can choose these to form an orthonormal basis, which means that you can expand a function in this orthonormal basis. And then we define convolution as multiplication inside of this expansion. And this is exactly the notion of convolution that's used in a lot of popular graph neural networks, such as a ChevNet or a Cayley net. or a Kayleigh net. And right here, for a sake of generality, we're just letting hk be any sequence of numbers, but often we're going to choose them to be what's called a spectral filter, which means that hk is going to be a function of lambda k. And the reason why we want to make that assumption is that when you would take the filter to be a spectral filter, then this filtering will Then, this filtering will commute with either isometries of a manifold shown by this rotation here, or permutations of a graph. In other words, if you take a signal, filter it, and then rotate it, you get exactly the same thing as if you first rotate it and then filter it. So it has the natural invariance or equivariance properties that you would want. Now, to construct our wavelets, we're going to focus in on a special class of convolution. A special class of convolution operators which corresponds to the heat semi-group. So the heat semi-group is a family of operators PT such that Ptf of X, which we're going to call U of X T, is the solution to a heat equation on either the graph or the manifold with initial value condition equal to f. And what's well known and can actually be verified by Actually, be verified by directly differentiating under the summation sign is that if we choose g of lambda equal to e to the minus lambda, that the heat semi-group has this spectral representation. In other words, it's a convolution operator of the type that we are talking about. And the reason why we're interested in this is it's well known that heat will diffuse differently over different shapes. And so by studying heat diffusion, And so by studying heat diffusion, we can learn a lot about the shape of a graph or a manifold. Additionally, especially in the manifold case, it's got a very nice probabilistic interpretation where ptf of x will be given to be the expected value of, oh, I am missing an f here. This should be the expected value of f of xt, where x of 0 is equal to x and x of t is a Brownian motion. Apologies for the typo on there. That should be f. For the type O on there, that should be F of X T. So, given this heat semi-group, we can define a couple of different families of wavelets. The first one is taken by taking the heat semi-group at time 2 to the j plus 1, minus the heat semi-group at time 2 to the j, and then taking the square root, where the square root is defined in the spectral domain, meaning we write this out in terms of its In terms of its Fourier series expansion. And then we take the square root inside of the sum. And since this is an orthonormal basis, you can check that this is a valid notion of square rooting. And the nice thing about this choice of frames is that it will be an isometry, meaning that the total energy of the wavelet transform will be the same as the total energy of the initial signal. And like classical wavelets, you can view this as a This is a capturing information about the manifold at different scales of resolution. So, over here, we have kind of a zoomed in filtering. And then, as we go from left to right, what we're going to see is that a wavelets get more and more spread out across the manifold. But the downside of this is that there's no way to compute it without explicitly computing all of the eigenvectors and Of the eigenvectors and eigenvalues. And sometimes that can be computationally expensive, particularly if you say on a very large graph. So instead, what we also do is we also introduce another notion of wavelets on the manifold, which is exactly the same, but without the square root. I put this to the one power here in this equation, just really to emphasize that it's exactly the same equation, except I deleted that divided by. I deleted that divided by two over there. And the downside of this approach is this is no longer a tight frame. It's still a non-expansive frame. But the advantage is this is now this is just literally the heat semi-group at two different scales subtracted from each other. So if you have a way of simulating the heat semi-group without computing eigenfunctions, you can just do that directly. In the context of graphs, this is where we want to go back to the probabilistic interpretation. And we can recall that the heat semi-group was the transition probabilities of a Brownian motion. So the natural analog of this on a graph is a lazy random walk. And so on graphs, we typically define diffusion wavelengths just by taking a lazy random walk matrix and raising it to two different powers and Two different powers and subtracting them. And initially, we chose dyadic scales because this is kind of the traditional thing to do in wavelet constructions. However, it's really not necessary. And subsequent work with Alex Tong and many other people has shown that this is not necessary. And you can actually just use any sequence of increasing scales. Moreover, you can actually learn the optimal choice of scales through data in a backpropagationable. In a backpropagationable network. So, given any of these wavelet transforms, you can then define the scattering transform to be an alternating sequence of convolutions and nonlinearities. And then given this new generalized scattering transform, you can then show that the generalized scattering transforms, both on graphs and manifolds, have similar theoretical guarantees. Have similar theoretical guarantees to the original construction, namely that they're going to be non-expansive on L2, and they're going to be invariant to isometries and stable to deformations which are close to being an isometry. Now, to test this out numerically with the manifold scattering transform, we first considered some simple tests. Considered some simple test cases. So we had the MNIST digit set, but projected onto a sphere, where we're able to get 95% accuracy with a scattering-based approach. And then we also looked at a data set of only 100 different surfaces where you got 10 different people standing in 10 different poses. And the two tasks we considered were to try to tell which person is which or what. Person is which, or what pose is a given person in. And so those were our initial results for the manifold scattering transform. But a potential limitation of this was that these were defined on 2D surfaces where you knew the entire manifold and you had a globally defined mesh. And so that's all well and good, but that's not really realistic for many interesting real-world applications. Many interesting real-world applications. Another scenario, which comes up a lot in biomedical data, is all you have is you have some high-dimensional point cloud, and you just model this as lying upon some unknown, hopefully lower-dimensional Romanian manifolds. So, what we've introduced in recent work with Joyce Chu, Holly Steach, Siddharth Viswani, Deanna Nidal, Smith Krishnaswamy, Matthew Hearn, and Hao Tengwu. Matthew Hearn and Hao Ting Wu is: We've developed a diffusion map style algorithm for implementing the manifold scattering transform. When you do not know the manifold, you do not have any sort of predefined mesh. All you have is just a point cloud. And so to show that this worked, was we first redid all of our experiments on spherical lemnist and faust and showed that we could still get good results even when we didn't have a mesh. And then we also applied this to some single. We also applied this to some single-cell data sets related to predicting whether or not people would respond well to immunotherapy or whether or not people would survive hospitalization after COVID. And so I said this was a diffusion map style of results. So the obvious question is, do we have any theory showing that this is actually going to converge if you have enough sample points? And the answer is yes, and those will hopefully be on the archive. And those will hopefully be on the archive in the next couple of weeks. So, pivoting slightly, the graph scattering transform is fundamentally a bit different from other graph convolutional networks. And it's different in two different ways. One is that it's a pre-designed architecture that's not learning from the data, but the other one is that the But the other one is that the filtering that is done in graph scattering is a different, fundamentally different type of filtering than is what is done in GCNs. GCN style filters typically will take averages over local neighborhoods to try to promote smoothness, which can be seen as trying to be a low-pass filter. Wavelets, on the other hand, are bandpass filters focusing in on a different frequency range. In on different frequency ranges, they capture long-range interactions, and they try to detect changes at different time scales. So, for example, if we look at the graph wavelet of P4 minus P2, we could think about this as addressing the question of how is my four-step neighborhood different than my two-step neighborhood? And so, these two differences, the different style of filtering and the designed versus train, these are you can. These are, you can kind of mix and match, which I'll get to later. But in order to further compare these two different types of filtering, we focus in on discriminative power and we ask the question of when can a network tell two nodes apart? And a necessary condition is that the hidden representation of two nodes in a graph would have to be different. And there's a lot of work. There's a lot of work, some of it done by people in the audience, on the analogous problem for graph classification, which it shows is that GCN is about the same power as WL kernels, a little bit less depending on how the GCN is set up. But there's not as much work done in the context of node classification. And so in recent work with Frederick Wenkel, Yaming Min, Matt Hearn, and Guy. Matt Hearn and Guy Wolf. We did an analysis of this and we characterized a class of situations where GCN can provably not discriminate two nodes if their local neighborhoods have the same structure. But we show that the graph scattering transform can discriminate some of these nodes. And therefore, if we construct a hybrid network that uses elements of both graph scattering and GCN, this hybrid network. This hybrid network will have more discriminative power than a pure GCN network. So, the idea is that we have this nice theoretical model of learning on graphs via scattering. So, now let's try to incorporate learning. Like I said, there's two differences between graph scattering and traditional GNNs. One is it's a different type of filter, and the other one is it's pre-designed. So, let's take the insights we've Let's take the insights we've got from these pre-designed networks and incorporate them into trained networks. And so that's what we did: we developed a scattering channels for a GNN where we've got wavelet filterings, and then we're multiplying on the right as you would by a weight matrix, as you would in a GCN. And then we build up a hybrid network that has both GCN style channels and scattering channels where they focus. Channels where they focus either in on low frequency information or a higher frequency information. And then we build on that further by incorporating an attention mechanism, which automatically balances relationships between these two types of channels. So notably, this type of attention is actually a bit different than a graph attention network, which is made famous by Peter Valachevich. So those types of graph attention networks. So, those types of graph attention networks, which people might be more familiar with, try to adaptively balance the importance of different nodes within the neighborhoods of a given central node. Here, our notion of attention is trying to balance the relative importance of different channels within the network. So, here we have some results which show that both the scattering attention network and scattering gene. Attention network and scattering GCN hybrid network without the attention mechanism perform well on a variety of baselines. In particular, they perform quite well on data sets with a low hemiphole. And perhaps more interestingly, we can look at the way the attention mechanism acts on different nodes. And we can see that there's a non-uniform distribution. Ununiform distribution of the relative importance of the different types of channels over different nodes in the network. And I mean, with the chameleon data set, we see there's actually two different modes within the distribution, where at some vertices, the band pass filters are more important, and at other vertices, the low pass filters are more important. The other way of incorporating learning into Incorporating learning into the scattering network is to replace dyadic scales with scales which are learned from data. And in order to do this, we implement an RNN style implementation where we take the signal and we just repeatedly multiply it by the lazy random walk matrix P. And then we use a neural network to pick out which scales will go into the filters. Now, this construction is good. Now, this construction is good for two different reasons. One, it makes a more flexible network that can adapt to a given data set. The other thing is that you can implement this without ever actually storing large powers of p. And this means that if the network is very large, you never have to store a large dense matrix. So this implementation is significantly more memory efficient rather than. Memory efficients rather than more naive implementations of the graph scattering transform. And therefore, it considerably increases its scalability. Last thing I want to briefly mention is that another thing you can do with graph scattering is you can use it as part of an encoder decoder network, where here in this paper, we were interested in molecule generation, where we use scattering as the front end of an auto-encoder. As the front end of an autoencoder that we penalize by various chemical properties and then train a GAN on. And you can also use it for a lot of other tasks, which I think Frederick is going to be talking about more in his talk on Wednesday. So to wrap up, because I see I am almost out of time, the scattering transform is a model of CNNs with provable stability and invariance guarantees, and it uses pre-designed built. And it uses pre-designed filters, which makes it useful for low data environments. Here we're proposing geometrics versions for graphs and manifolds that have similar theoretical guarantees to the Euclidean scattering transform. And these wavelets can be either constructed spatially or spectrally and can also be incorporated into hybrid scattering GCN style networks. And that is where I'm going to stop. Thank you very much for listening.