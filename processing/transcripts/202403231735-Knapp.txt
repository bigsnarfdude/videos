Working with Gova, and he's going to talk about exponential relations and allow to break integer conjugates. All right, thanks, Seda. Thanks, Seda, for organizing, as well as Felix. And thank you to all of you for being here and sticking it out. I know it's been a long day, so I'll try to keep this engaging. And we will try to get through this. Try not to keep you over too long either at the end. Alright, so yeah, so I am going to talk a little bit. I am going to talk a little bit about some exponential relations among algebraic integer conjugates. Before I get into that, I would like to acknowledge the land on which I live at work and also the people who were there before me. So the University of Calgary is located in the heart of southern Alberta, and I'd like to acknowledge and pay tribute to the traditional territories of the peoples of 37, including the Blackfoot Confederacy comprised of the Sitsika, Pekani, and Kaidai First Nations, Sutana First Nation, and Estonian Nakota, including the Chenakee, Barracks, and the National. Dakota, including the Chenike, Bears Pois, and Minnesota First Nations. The city of Calgary is also home to the Meti Nation of Alberta Districts 5 and AS6. Alright, before we get into the math, I'm going to present sort of how we came into this particular problem that we're going to be looking at. And I'm going to do so by way of a motivating theorem. But before I say what the motivating theorem is, I want to note that it is not actually important to understand what this says in order to understand it. What this says in order to understand the talk. So, I'm not going to go too far into actually explaining what all of the adjectives in this theorem mean. But if we start with c being an irrational algebraic number, degree at least 3, degree d, and epsilon some positive number, maybe we also have a non-degenerate linear recurrence sequence of rational integers lying around. Maybe it's also not a polynomial sequence. Then the theorem that Bujaud and Then the theorem that Gujaud and Wynne showed in 2023 is that there are only finitely many of these uns for which there exists some rational integer vn, for which vn over un is kind of close to c in some sort of meaningful sense. So again, this, I guess what I want to provide here is an interpretation of this result that is just sort of like the briefest possible summary. And the idea here is that these u n. The idea here is that these u n's that we're starting with end up serving as the denominators for these rational numbers which are theoretically close to x. And what we're saying, since we're saying that there are only finitely many of these u n's for which we get a good approximation, we're saying that certain sequences actually can't really serve as denominators for these good rational approximations. And that's sort of the interpretation that we want to take away from this. The important kind of question that kind of question that Coa asked at the beginning of fall is can this exponent this 1 over d minus 1 here can we improve that at all and still sort of attain this same finiteness result and so this is sort of the the question that we're going to look at and the important sort of thing that we're going to look at is where does this exponent come from what's the fundamental fact producing this exponent and then can we manipulate that fact well so Fact as well. So the exponent is going to come from the following fact. So we're going to take f of x, an integer coefficient polynomial. We're going to suppose that it's monic, irreducible of degree d. And we're going to write down its roots. Now, before, you might notice that there's maybe an odd indexing here. We're indexing starting from 0 rather than indexing starting from 1. There is a reason for this that we're going to get into, but for now I'll just note that it's a little odd perhaps. Just note that it's a little odd, perhaps. We're also going to suppose that we've written down the roots in descending order of absolute value. Non-strictly, since roots may be complex conjugates and maybe have the same absolute value. But in any case, we're going to write our roots like this. And throughout the talk, we should always assume that anytime I'm writing down some alpha i's, they're going to be in descending order of absolute value. That's going to be severe. All right, I haven't told you what this fact is yet. What this fact is yet. We've just written down the roots. The fact is that alpha 0 times alpha 1 to the d minus 1 should be at least 1 in absolute value. And this is a fact that has a fairly short proof, so I'll go ahead and present it. If we look at this quantity that we're in, alpha 0 times alpha 1 to the d minus 1, we're just going to sort of replace, or we're going to write this alpha 1 to the d minus 1 as, well, alpha 1 to the 1 times alpha 1 to the d minus 1. 1 to the 1 times alpha 1 to the d minus 2. Yes, assuming d is greater than or equal to 2, of course. If you have d equal to 2, then I think, well, we're going to get there. We're going to get there. Yes. Still true, I suppose. Yes. So then what we could do is we could say, well, alpha 1 greater than or equal to alpha 2, and we're assuming we've written this in descending order. And then we're going to continue applying this process until Continue applying this process until we get to the very end when we've sort of enumerated all of our roots here. Of course, this is the product of the roots in absolute value, so that's the constant term of our polynomial. Our polynomial has integer coefficients and is irreducible, so that constant term had better be at least equal to 1. So we're sort of able to look at this power product here, this product of powers, and we're able to say, okay, that's at least greater than or equal to 1. And this is where. 1. And this is where that exponent of 1 over d minus 1 comes from, from sort of some expression that you manipulated, and the d minus 1 here turns into the 1 over d minus 1 in that other expression. So, okay, so if we wish to improve the exponent in that theorem, then we should ask what other numbers could we replace d minus 1 by and still have this same sort of fact to be true. And so we're going to look at the set of numbers for which this fact is true. So we're going to Back is true. So we're going to look at the set of all c for which, for every monic irreducible f of x with integer coefficients of degree d, roots in descending order, alpha 0 times alpha 1 of the c is at this point. All right, so we can start by answering this and saying, well, if c is less than or equal to d minus 1, the above property holds. So we're going to start by taking an appropriate f of x. If the second largest root is at least 1, Largest root is at least one, then, well, of course, the largest times the second largest to some power had better equal one. This sort of case is generally going to tell us nothing. So the interesting case is, well, when alpha 1 is less than 1, then alpha, the quantity we're interested in, alpha 0, alpha 1, c, is going to be at least alpha 0, alpha 1, and the d minus 1, because we're supposing we chose some c value smaller than d minus 1, and from there we already saw it. And from there, we already saw that that quantity of the greater hand sign had better be at least one. So, okay, so now we know that we can definitely look at c values less than or equal to d minus one. The question that we want to answer then is, can we choose a larger value than d minus one? And the answer is no. So it's going to turn out that if we have something like if we can guarantee that alpha zero, alpha one, and c is equal to one, that c is going to be less than or equal to minus one. To be less than or equal to 2 minus 1. So, where does this come from? It's going to come from a certain family of polynomials. So, here we've got a fixed d, and we're going to allow h to vary over a lot of integers, and we're looking at this family of polynomials, x to the d minus hx to the d minus 1 minus 1. We're going to apply this property to the roots of this family of polynomials and sort of get an interesting relation. Let's see. So, for these particular So, for these particular polynomials, it's going to turn out that for infinitely many integers h, this f dh is irreducible over z of x. This is coming from the Hilbert irreducibility theorem. If that's unfamiliar, I'm not going to go into that. You can just sort of trust me on that. It's going to turn out that we've sort of constructed this family of polynomials to have one large root. The largest root of this family of polynomials is going to be about the Of polynomials is going to be about the size, about the same size as H. The other roots, the other d minus 1 roots that we have, they're all going to be small, and in fact, they're all going to be about the same size. They're going to be around the size of h to the minus 1 over d minus 1. In particular, we can sort of see that if you multiply all of the roots together, you should get 1, and so you can sort of check the sizes of these roots. One root of size h to the 1, d minus 1 roots. h to the 1, d minus 1 roots of size, h to the minus 1 would be d minus 1, so things are checking out. So, okay, so now that we have sort of analyzed the root sizes of this family of polynomials, we're going to claim that we're going to apply this property and we're going to see that I c value has to be no bigger than t minus 1. So let's go ahead and do that. We know that for each of these polynomials, 1 is less than or equal to 1. Polynomials, one is less than or equal to the product of these roots. But we know exactly how large alpha 0 and alpha 1 individually are. Alpha 0 is h to the 1, alpha 1 is h to the minus 1 over d minus 1. And so now what we're seeing is that this, for infinitely many integers h, this particular quantity is larger than some, maybe not exactly one, but larger than some absolute constant. And if that exponent is negative, then Is negative, then as h grows, as we sort of vary over this infinite family of h, that right-hand side goes to zero. We can't have that, so that exponent, in fact, must be non-negative, but that's exactly the same thing as saying that c must be less than or equal to 2 minus 1. Great, so we've learned sort of in recap that a real number c, non-negative real number c, has the property that for every irreducible monic. That for every irreducible monic f of x with integer coefficients, that alpha 0 to the alpha times alpha 1 to the c is at least one at absolute value, if and only if c lies in this interval here. And as a corollary, we find that that exponent in sort of the motivating theorem is optimal. We're not going to be able to produce a better exponent for that. But this raises a couple of follow-up questions. Namely, why did we sort of only look at the first root? Can we sort of extend this and look at not just we sort of extend this and look at not just alpha 1, but also alpha 2, alpha 3, up through alpha k, for instance. So what do sort of those higher dimensional variants look like? And then a sort of second question that we can ask is, well, if we pick a C in this sort of nice set that we've got, can we guarantee not just greater than or equal to, but actual strict inequality there? In general, we might expect that equality would be sort of a very specific, special phenomenon, and that strict inequality would be equal to equal to equal And that strict inequality is the more general thing that you would expect. And if we can show that there's strict inequality there, can we quantify by how much larger is actually that product than one? So we'll split sort of the rest of the talk into these two components. We'll first look at the higher dimensional problem statement. So if we pick a d greater than or equal to 2 and a k that's strictly less than d, so k here is going to be Than d. So k here is going to be the dimension of the problem that we're looking at. We're going to let e k d be a subset of r k. It's going to be the set of all tuples c1 through ck, where each coordinate is non-negative and such that for every irreducible monic f of x of degree d, this product of powers of absolute values of the roots is at least one. So very much in analog to the previous thing that we looked at. And here we can see sort of why we've indexed it the way we have. Sort of why we've indexed it the way we have. We're looking at alpha 1 for alpha k, so we can look at this exponent and see what's going to see. Alright, so here's the question that we have. What does this set look like? What should we expect? And the theorem that we've recently proven is that Ek d is the set of all points in Rk which satisfy a certain set of 2k linear inequalities. So each of the coordinates being non- Each of the coordinates being non-negative is not a surprising one. But perhaps the surprising one is sort of this set here. Looks kind of complicated. We'll do a couple examples here just to sort of see what these inequalities look like. So E1D, this is the set that we saw before. In this case, K is 1, so there's one inequality in sort of each group. The first one is x greater than or equal to 0. The second one, this first sum here, is empty because we must take i. Here is empty because we must take i equal to 1. So we're just looking at x less than or equal to d minus 1 over 1. Great, this is the exact same set that we've recovered before. However, we can now step up to k equals 2. So we'll have two inequalities of the form x greater than or equal to 0, y greater than or equal to 0. And we'll get two inequalities here as well. So if we look at i equals 1, that's going to be, again, this first sum will be empty, so we'll have x plus y less than or equal. x plus y less than or equal to d minus 1. But if we choose i equal to 2, this first sum is no longer empty. And so we see minus d minus 2 over 2 times x plus y less than or equal to 2 i is 2 over 2. These xi's are the ci's and the exponents. We're just calling them like variables x and y right now, so c1 is certainly. Yes. Okay, so here is some sort of abstract set of inequalities. We might wonder, okay, what does this look like? Might wonder, okay, what does this look like? So here is a picture. You can actually sort of draw it out very nicely. This is E2D. We can sort of write down our four vertices here at some nice quadral angle. However, we can sort of continue, and we have descriptions of EKD for general K. And so we can even take a look at E3D. So we're going to get some, in general, in RK, we're going to get some positive. General in Rk, we're going to get some polytope, and that polytope is going to, we can write down exactly what the vertices are. The construction is a little complicated, so I left it out of the talk. But we can, in general, we'll have two to the k vertices. It will be some nice polytope, and we can describe exactly sort of what this set looks like. All right, we might also ask: well, okay, it's great to be able to. Well, okay, it's great to be able to write this thing down, but where did these weird inequalities come from? So recall that family of polynomials that we were looking at earlier that looked like x to the d minus hx to the d minus 1 minus 1. It turns out that if you generalize this and don't just look at the middle term being d minus 1, but you allow yourself to look at different values of that exponent there, that's going to produce for you the sort of exact set of inequalities that sort of Of inequalities that sort of arise in this particular example. Yeah, five minutes. Thank you. So you just use x to the k? Minus k or something? Yes, yeah, exactly. So for each i between 1 and k, you're going to end up with one inequality for a particular family of polygons. Yep. So you get k inequalities, which are exactly this k. The nice sort of property of these The nice sort of property of these polynomials is that they have some group of large roots and some group of small roots, and all of the roots have a predictable absolute value. All right, so that sort of concludes our brief investigation into the higher-dimensional variant. We'll now sort of look at the strict inequality question. So we might want to ask, is it possible to get equality? And so we'll step back to the one-dimensional. So we'll step back to the one-dimensional version, and then we'll sort of expand our focus into the k-dimensional version again. In this case, we might want to ask, is it possible to get equality? Of course, there's sort of a trivial answer, which is, well, yeah, if all of your roots have absolute value one, then this is kind of boring. So we're going to maybe not consider that to be an answer. If we have non-cyclatomic, we can actually reduce it to the case where c equals d minus one. Only possible when c equals d minus one. Possible when c equals d minus 1. This is not obvious, but not terrible to write down. So the answer, there are some non-trivial answers here. So for instance, here's a quadratic. It turns out you could more or less pick your favorite irreducible quadratic because d equals 2. So this product that we're interested in is the product of the roots, which is the constant term. And in this case, I guess I should say irreducible quadrant. In this case, I guess I should say irreducible quadratic with constant term plus or minus one, because this will always work out nicely. For cubics, it turns out that we can actually also get equality here. So if we have something monic and irreducible, constant term 1 again, and if the two smaller roots are a complex conjugate pair, then we sort of reduce from d minus 1 to 2 because we're cubic. And then this is our two smaller roots have the same. Our two smaller roots have the same absolute value, and again, constant term, multiple, and so on. So here's an example of such a polynomial that you can come up with your favorite as well. However, the interesting thing is that as soon as degree is strictly greater than 3, we actually are going to be able to get strict inequality there. We're not going to see any instances of equality. In general, we've been able to show that if the degree is sufficiently large, Is sufficiently large, then again, we're never going to be able to get equality here. We're always going to get strict inequality. I should note that the lower bound on D is suboptimal right now. So we've said if D is strictly greater than 3, K plus 1. In the K equals 1 case, we just saw that actually we can do better. We can prove it for D greater than 3. And we've done the K equals 2 case as well. You can say something even a little more subtle than just a Little more subtle than just a strict inequality, but I won't get into that here. The final sort of question that we want to ask, and this is sort of our future direction for this project, is can we actually get a lower bound of the difference between this product of powers and one? So something with some kind of linear forms and logarithms and sort of being able to say, sort of leverage the fact that these all are conjugates and be able to say, well, maybe we can expect Say, well, maybe we can expect some sort of error term in this difference. All right, that's all I have to say. Thank you so much for your attention. Are there any questions? Yeah, very nice time. But I thought you would go back to the diasporic approximation question at the end. I mean, because it started from that, you know? Yes. So shouldn't we bad to reapply it to Shouldn't be bad to reapply it to that? I mean, you're just on those fall numbers, or do you want to go back to that? Yeah, so what we found was from sort of that very original question about approximating irrationals with these certain sequences of denominators, that exponent that showed up in that very first period. And then you never know about it again. Oh, there was a brief comment. We'll see if we can. We'll see if we can work our way back to that. That exponent is optimal. That was. Oh, did I go past it? Yes. When I said the exponent in our motivating theorem is optimal, that was referring to that very first theorem there. But maybe then you can, you can make your question so that they will be finite maybe or some zero density. Maybe you can define that. Yes, exactly. You can do things like that. Yes, exactly, exactly. Yeah, so there could be some sort of further refinement. Yeah, yeah, it's already optimal to have zero, but a finite number is what you're asking now.