This notion of two seagull that we just saw in the last talk was developed under that name by, but it was developed independently by the people who are here. So I mean, Ama and Andy. And then they have a decomposition space. So this talk should be viewed as complementary. So this is sort of introducing it from that point of view. From that point. So we're happy to have a little packing to get our next group of people. Well, thank you. Thanks for having me and asking me to do this. This is nice material. And a big thanks to Walker for the nice talk. I'm going to kind of build on what he's done. Kind of build on what he's done, setting everything up for us. So for the start of this story of decomposition spaces, I want to start just talking about delta. So I want to give a factorization system on delta. Delta. And since I'll be talking about delta a lot, let's just say its objects are bracket n consisting of n plus 1 elements, ordered. And what's this factorization system? So, first of all, I'll write I'll write A in delta for the active mass. So, what are these? These are endpoint preserving order preserving. So these are alpha from n to m, so that alpha 0 is 0 and alpha n is I'm Andy. So, this is one half of this factorization system. The other half of this factorization system, well, before I do that, I'll say that most of the generating maps we're used to in Delta are of this form. So, these generators are the ones, either the co-face maps that skip a particular element, or the codegeneracy maps that hit an element twice. So all of these delta i's except for i equals 0 and i equals n are examples of active maps. And then the other class is Sikai. These are injective, surjective, and hits high place. These are always active. Okay, so the other two kinds that I haven't talked about are this delta 0 and delta n. Those fall into the second class. I'm going to call these inert. So these are the alpha from n to n so that these preserve distance. And of course, these And of course, these last two, delta zero and delta n, satisfy this. So none of these preserve distance in this way. So these are basically sub-interval inclusions. Okay, so those are the two classes of maps that are going to form this factorization system that we're talking about. This isn't the only factorization system on Delta. The one that's surjective maps and injective maps, this is clearly a different one. But let's maybe just explain why this is a factorization system and exactly what that means. System and exactly what that means. And it means that every map in delta factors uniquely as an active map followed by an inner map. I'm also going to decorate my arrows. I'll put a bar on the head of my arrow if it's active, and I'll put a tail. And I'll put a tail on my arrow if it's inert. Hopefully, I'll do that consistently throughout. Maybe I'll swap between them if I'm feeling. I don't want to mess around. But if I give you an arbitrary map, I'm claiming that this factors uniquely as first an active map with a P in the middle here followed by an inner app and And what does this look like? So I know that, well, maybe I'll just write the answer, and then you can all look at it and say, oh yeah, there's no other way that you could possibly have done this. So this act map, what it's going to do, it's going to act like this. And I haven't said what P is. Is and this map is going to add up as zero. So the original map may not send zero to zero. I have to fix that, and the way I fix that is by saying, well, instead of sending zero to alpha of zero, I send zero to alpha of zero minus alpha of zero, which is zero. This also tells me It also tells me what p should be because n is supposed to go to p, so I should have alpha of n minus alpha of 0 equals p. So I guess that tells me what p is. And then these inner x, these are interval occlusions, they're just given by adding some colors. Once I've made this condition that I'm going to bracket P, I mean, the only way to fix this to get alpha pack is to just add alpha simple. Is that okay? So this is sort of manifestly unique. So this is a unique factorization. Good. Now, one nice thing about this factorization system. Factorization system is that if I look in Del Terra, if I'm ever in this situation where I have two of these with the same source, I want to get my numbers the same letters. Okay, suppose that I have a span like this, where one leg is active, the other leg is inert. You have the leg is inert, which I've just labeled as plus C because that's what inert maps do, they just add some constant. We can always complete this, and the result is actually a push out in delta. So the map on the right, the inner map, is again a plus C. And the map on the left is sort of, I mean, it's determined by alpha. It's what it has to be. Maybe I won't write a formula for that, but what it's going to do is start off on the interval k. So this is like here. And then inside here I have Inside here, I have C up through C plus N, I guess, sitting inside of there. And what I'm going to do is I'm going to act on 0 up through C minus 1, just sending them to themselves. And then I'm going to act on things bigger than C plus N by moving it around a little bit. And this will give me this active map on the block. Another way to say this is that this K is looks something like this. This is like the ordered connections of these things where I identify the endpoints. And because I started with an active map alpha, this P is also going to be at the same time. So it's sort of the identity wedge with alpha, wedge with the identity account. And you can check this as much. All right. So I think that's all the facts I want to know about active and inert maps in Delta. So what's a decomposition space? So these are simplicial objects. Um where for now I'm going more generally than what Walker did. I'm saying spaces. Here I mean the infinity category of spaces. We could just take sets. Okay, so everything I'm going to do today, we may as well be in the exact same setting Walker is in, right? Set. But I'll keep calling them spaces, so I may as well write spaces down. And this is a decomposition space if it sends every active inert push-up squared. Delta are to a pullback. So, if I start with what I have over there, that particular one, then I'm looking at Then I'm looking at, I get a diagram like this of spaces. And we just ask that this be a pullback. So I give you a concrete diagram, like this just comes from the simplicial. Like this just comes from this implicit little structure. And to avoid being too technical, if we're working in sets, I really mean pullback in the usual sense. If we're working in spaces, I definitely mean homotopy pullback. But let's keep this calm today. Nothing is going to change in what I write, whether I write spaces. Um right so let me go back to Go back to. I want to say more about this. So, first, I think the main thing I want to do is I want to give a sort of smaller set of conditions and really connect with what Walker has done this morning. So, for this, let's give another definition for let's say x is an upper. X is an upper two-seagle if the diagrams look like this Archobex. Okay, so here I'm going to write the, I'm going to use this bot symbol for the bottom face map. Base map. And I'll do that pretty consistently. So in this case, it's just D0. And this should be for all 0 less than I less than N. In order to have any such I, I guess N has to be at least 2. Otherwise, I don't have any such I. Um otherwise I don't have any such i. And you might recognize this square in the case n equals 2. So this is one of the squares that Walker did write down in his presentation. It's his first one saying what is associativity. But it wasn't just this one, it was also another one. Associativity required something else. So likewise, x is lower 2c. Uh likewise, x is lower 2 sql if the squares now I'm going to do the same kind of thing, but I'm going to use bottom, top base maps. All right, these are pullbacks. And again, the unequals. And again, the n equals 2 case of this, where i equals 1 is the only option, that is a square that was in the previous talk. But beyond that, it looks a little bit different. Maybe a couple comments. So the first comment is. So, the first comment is that definitely decomposition spaces are both upper and lower 2-sequent. Alright, this is just about what I said earlier about the generating maps in Delta and whether they're active or hard. That's it. Another small comment is that X is upper to Siegel if and only if XOP is lower to Siegel. So maybe it's not so important for us to worry too much about one or the other. I mean, we can take the opposite simplicial set or simplicial space and get a deal. So these are just immediate questions. A theorem I'm going to tell you this is the feller et al. The last part at least is that upper two seconds plus lower two seconds implies decomposition space. So if you satisfy both of these conditions for all n, then actually your decomposition space to be counting. I'd like to say a little bit about how you can establish this fact. How you can establish this fact. Some of this predates, you know, some of the facts that come into this predate this paper and are in the GKT paper. But let's give a little bit of indication of why this is true. So answer the question first. Exon is not a simplicial state. Not a simplicial space, it's a co-sublicious space. So here, if I X op, so if I start with delta op to spaces, by X up, I mean take the functor delta to delta that sends That sends that's the identity on objects and it maxed F alpha from N to M This goes to T maps to M minus minus alpha of n minus t precomposed with this, that's what I mean by x hot. So this has the effect of essentially just replacing di by dn minus i and si by sn minus i, I think. So that's what I mean by opposites and pushes. Is that okay? Is that okay? Yeah, that's not what I thought. Yeah, yeah, sorry, sorry. So this is XOP replaces DI with DI minus I and SI and SI minus I. So in particular, if I take the nerve of the category is X, then Is x, then x op is the nerve of the opposite of the Okay, so clearly we have many many more pullbacks that are required for decomposition spaces than we're getting in upper or lower TCQ. But maybe the most important ones are these pictures. These pictures where I'm trying to compare one of these outer face maps with a degeneracy map. There's no degeneracies at all here. So I'm going to write main ingredient one, but this is a main ingredient for thinking about decomposition spaces generally. I'm sorry, is there an example of an upper two SQL space which is not lower two SQL? An example of an upper two SQL space, which is not. Yes. So I'm trying to think of an easy example. So the ones I know are. Of an easy example. So, the ones I know are a little complicated. I mean, you can take something like the estop construction of a Walthausen category where you only have basically monomorphisms but no epimorphisms, and so we can use the argument in one direction. That's probably something. I'm not sure if it's an easy example, it's an actual example based on. Yeah, so Hannah is going to tell us more about this, I think, tomorrow afternoon. Yeah, the examples I have are like about unary operatic categories, and I don't want to get into that. But anyway, there are real examples that just satisfy one of the other. So the main ingredient one is the pasting offer pullbacks. One is the pasting offer pullbacks. And what this says is if I have some rectangular diagram like this. Let's say of sets, but basically anywhere. And we assume that the right-hand square is a pullback square. Well, I have two other squares. I have the left-hand square and I have the big square, this composite square. Welcome is a part. If and only if, did I say the big square or big rectangle? Let's say big square. I guess it's not really a square. Word topologists. That's a very good point. Okay. Okay. So let's say a little bit about what this will get us. So here's a proposition. We can erase lower two sequel. We can excise it from our minds. It doesn't exist. It from our minds. Doesn't exist. So, proposition. The following are equivalent. First axis and upper two single space. Two, let's modify this just a little bit. So instead of looking at all of these pullbacks for each n, I just take one of them. I'm going to label this as This has heart because heart is a provective sum. 0 or less than I less than. So I just need to pick one of them. This is really what I want the proposition to be, but to sort of illustrate the pasting law, I think it's better to add one more. I think it's better to add one more condition for each n bigger than equal to two, which will just go down farther. So I apply d2n minus 1 times, and I get 2x2. Otherwise, I can do the bottom. I can do v dot, go to x n and v1 n minus 1 times and go to x1 and v5 again that this is a pull. Is this an instance of the square used defining two SQL? Is this an instance of one where you're defining two SQL squares? Because it could be X1s there. So yes, because when I was talking about n-fold and M-fold, this is composed of the two-fold multiplication with. I think it's a special case. By the way, what I'm going to Way, what I'm going to do here, you could use as an exercise to compare with the definition Walker gave. It's a really good exercise. And using this, and you can also view this as one of the polygonal decompositions, right? You just cut off one triangle from your n plus two column. Right, yeah. So for some there is like no restriction. There is like no restriction for the it's whatever one you want for each end you pick at least one could have more than one just some so I'm getting rid of this for all oh I thought you were being like imprecise and kind of it's literally just I just need one at least one okay so I guess one applies to immediate Right? Because if I have it for all of these, then I definitely have it for what? Just take i equals 1, and then you put. And for 2, it applies 3, and 3 implies 1. Um I'll write down this n so we need to use some induction in one of these directions.  All right, so what's the argument? I can take these squares that I have, put them together, this composite on top, n d2 to the n. D2 to the n. This composite is D1 to the N. So if 3 holds, then the right square and the outer square are both equal vects in this diagram. And so the left-hand square holds. That's 3 implies 1. I've got this. Is I've got this, and I've got the big one, so I have the left one. And I guess the n equals 2 case of 3 is the bottom case of the pullback. And for 2 implies 3, that means I have this as a pullback. Pullback and inductively, I assume this is a pullback, and so the whole thing is a pullback. Induction plus this pacing over pullbacks. And similarly, there's a version of this, but lower decomposition spaces. Now, this is maybe a warm-up, but a But a version of this square there because that's the end of the argument. It says that if I have an upper decomposition space, proposition, a lemma That's his upper G Sequel. And if n is bigger than 0 and X from I between 0 and N, then this Yes, so I won't. So I won't prove this one, but the proof is almost, you know, it just uses this asymmetrical pullbacks and the fact that, you know, PISI is the identity. And that's identical. Now, this lemma misses one case, but this would be a collection of those types of inert pullbacks that we're sort of missing at this stage. Among the basic ones. So We're missing x1 we're missing this pullback. We're missing this pullback. We're missing the n equals zero case. That's the only case that we're missing. N equals zero, I better be zero as well. And I don't yet know this one. Okay, so our main ingredient two I'm presenting these because these are also both true for higher dimensional cubes, is that these pullbacks are closed under retracts. So by this I mean there's some category of squares of spaces, commutative squares of spaces. And if I have a retract in that category, then I'm good. And this guy is a retract of one that appeared here. So llama. So lambda prime the stars at the back when x is upper two sequels. And this is just a retract argument, so proof. Track argument, so proof. If I take I think I want to use S1, S1 on the top, S0, S0 on the bottom. That includes into the square X2S2. X2S2 and then I think here I use D1, D1, D0, D0. This is the identity. So this sort of matrix denotation is just saying which maps I use on each of those corners. And the previous lemma without the prime says that this middle guy is a pullback. Now, of course, for both of these, there's lower two SQL versions that I'm omitting. But what this amounts to is if I look at the, I mean, what have we done? So if I start with these upper or lower two SQL spaces, Upper or lower two SQL spaces, and I only look at, let's say I'm trying to show that these active inert maps, pushouts are all sensitive pullbacks. I break up my maps into these basic maps, EI, SI, and so on. I do it one by one and glue them all together. Now the composition The composition space axioms verified at least for the basic generating maps of delta. So the rest, the rest comes back. Right, and you know, there's a lot of other, you know, if you have a decomposition space or a two-seagle space, there's a lot of other squares that are also pullbacks that come for free. Lots of lots of active-active pullbacks potentially. Pullbacks we have potentially. But maybe I'll leave this part at that. And maybe I'll say a little something about Siegel spaces, one SQL space. So, I'm going to give a definition. Seemingly different than what Walker gave. This is SQL. If I look at xn and I take its two outer faces. And I take its two outer faces. These have them default. If this is a pullback for every n, I guess n. For every n, I guess n better be bigger than or equal to 2 for this to make sense. You can compare this to the usual notion. Maybe you inductively prove that this is the case. What happens is these outer face maps become projections, projecting away a factor, and then it's sort of an exercise just playing with some induction. And pasting lemma, yeah. So definitely you need the pasting lemma over and over and over again, frequently. Maybe you can get away with off these things. Maybe. Right, so here's a proposition SQL spaces are becoming Are decomposition spaces? I don't want any new idea. I don't have a new idea. I have the same idea that we just saw. I only have one. Okay, I had two ideas, but really I have one idea. And. And proof, let's show seagull implies upper two seagull. I really only have the one ingredient, so let's try to use that ingredient. So, remember, it's enough to show it would be enough to show this is a pullback where here I put d n minus 1. n minus 1 and here I put dn right I only have to show those are pullbacks for one choice of I pretty much the whole reason I wrote that proposition down so let's I'm going to put a question mark because this is what I want. Could extend this up. I mean, the question is, if it's a pullback. Yeah, the question is, is that a pullback? I want that to be a pullback. If it is, then I'm upper GC. It always kills. So we extend it out. I assumption this left or this right square is a pullback. That's what the Sequo condition says. So I'd like the whole thing to be a pullback. But this sits in part of a cube. Alright, better be taking notes, but is it actually that upper two siegel means bottom instead of the top? Upper two seagull is using the bottom face map. So upper two seagull is like maps that preserve the top element. It's, I mean, I have to look it up every time. Okay, so just using simplicial identities, this composite of an outer face map with this inner face map actually can be written as two outer face maps. And so both of these are pullbacks. And this gives us that the outer thing is a pullback, the right thing is a Outer thing is a pullback, the right thing was a pullback. I guess I'll put a square. So again, these things are all purple x, but this time you use a cube, which is a good trick. Now maybe I have time for one last thing and this wasn't mentioned in the previous talk, so it's a great place for it. And that's the path space criteria. And that's the path space criteria. And what does this criterion say? I'm going to phrase it just in terms of upper. This is upper two siegel if and only if it's upper decalize. Is Siegel. This is also the final path statement. So, what this is saying is, I can detect. What this is saying is, I can detect upper to sequality using some construction and the usual Siegel condition. And there's a corresponding thing for lower to Siegel that I won't write down, but it's exactly analogous. I think the hardest part of discussing this is just I have to say what the upper decollage is, which isn't so. Isn't so hard. Actually, I'm using less ideas in this proof than I used in the previous proof. So, what is this deck top X? So, I start with a simplicial space X. I give you a new simplicial space where everything is shifted. So in degree n, this is xn plus 1 on my original space. And all the face maps are exactly the same index as they were before. So equality on the left is the maybe D top I for I face map on the left and D I for the one on the right. So I've actually thrown away a couple. So I've thrown away the top face and top degeneracy maps. I don't want to have too many. I sort of have this extra ones floating around that I'm going to throw in. And of course, there's also a And of course there's also a there's a lower decoulage or initial path space that's defined by the same kind of thing except maybe there's a little shift here. The way the bottom case of the jerseys is so let's prove the path-based criterion. Well, if I look at The relevant siegel square so this is the top, which is DM When you write E top, do you mean the original top or the E top? Yeah, I mean in this case it doesn't maybe I'll write half half. So here I mean D top for deck. Right? In the deck a lot. So it's not the old top. It's not the old top. And that's exactly the point. So I have this square and the vertical ones, the D0. Yeah, so both of these are D0 in whichever means I mean them. But I'll I'll put tops, I'll decorate everything with tops. Tops, I'll decorate everything with tops. You'd actually want to do that if you're doing the other one. This is exactly xn plus 1. Now this is still dn, which is an interface. This is exactly what this square is. Right? And we saw that x is upper 2 sigma if and only if this square is always a pullback. And of course, that can. And of course, that top x is equal if and only if the square on the left is a pullback, and that's everything. One is a pullback if and only if the other ends, and that's all. Okay, I think that's a good place to start. Thanks.