Sufficiently rigorous, but they will be rigorous enough. Say that. Okay, so I maybe don't need to have this slide. I think it's interesting that Jeremiah spoke this morning. And if you Google image MRI, this is the first picture that comes up, this middle one right here. And he had the same picture. And he had the same picture in his slides. So I don't know what that says about us. But, anyways, or Google. Yes. So I'm really interested. I'm interested in a very particular type of inverse problem, which is really motivated by what in data science now is kind of a very ubiquitous problem, which is low-rankedness. And a lot of my PhD. And a lot of my PhD work was on low-rank signal plus noise models. And now I'm sort of moving beyond that to a more generalized sensing framework. And on their own, low-rank models have kind of a, are useful in their own right, both for all of some, many of these are topics I myself have worked on. So this, maybe not. So this first guy right here. This first guy right here, that's spectral clustering. And if you form a version of a population kind of matrix, that's got a nice low-rank structure in expectation. This bottom corner right here is PCA, in which the underlying covariance matrix has this nice low-rank structure. And maybe this one right here, this is tensor data, which has a nice low-rank structure as well. So, kind of these low-rank structures are ubiquitous, and we want to kind of come up with a, well, there are. To kind of come up with a well, there are already models out there, but we want to try to understand a sensing problem from using this low-rank structure. Okay, so the model that I am interested in is this matrix sensing problem. So, unlike what we might be familiar with in classical compressed sensing, in which you have vector-valued observations, here you have matrix-valued observations. So, in particular, So, in particular, each yi, which are your observations, are drawn from an inner product of xi with m, where m is a low-rank matrix. I'll talk about that in a second. This inner product is a Frobenius price inner product on matrices. And I'm going to be assuming that all of the matrices you see here are symmetric, and in the case of M, positive semi-definite. Okay, and the scaling by square root of n, don't worry. Don't worry about that right now. That's actually just for that, it turns out to be the right scaling to use in this case. Okay, so this kind of encodes the idea that instead of just having, if you were to look at linear regression, you would observe inner products of vectors with another vector. Here, observing inner products of matrices with another matrix, instead of having sparsity in the coefficients, you have low rankedness in the coefficient. Okay, so this is a nice matrix analog of the kind of sparse linear regression problem. Regression problem. And what I'm particularly interested in is not just the optimization point of view, but more so the statistical point of view of this model. I'm trying to understand what is the relationship between N, which is the sample size, D, which is the D by D matrices that are appearing here, R, which is the rank, and sigma, which I haven't told you about yet, but that's the parameter governing the standard deviation of the noise. Okay, so just to encode it a little bit more concretely, you can view your sensing matrices as some kind of sensing operator. And so that might be an MRI machine, that might be your phone camera, that might be a radar. And all of those are capturing somehow some low-rank matrix M in a noisy fashion. And that's exactly what this is showing right here. Okay, so I'm particularly interested also in the high-dimensional. interested also in the high dimensional regime where d squared which is essentially your ambient dimension of your data right you have the observations x i living in d by d so it's dimension d squared well d choose two and so i'm particularly interested in when d squared is much larger than n which is your sample size but n is still somehow larger than your effective uh dimension which is d times r because if you have a low rank matrix you can always factorize it into two in a product factorize it into two in a product, an outer product of two matrices, and that has D times R many entries. Okay? All right. Yeah, so just as a detour, let's talk about sparse regression. I call it sparse regression. You might also call it compressed sensing. There's lots of different flavors of this out there. But suppose we're given n observations in RP such that y is equal to x beta. Such that y is equal to x beta naught plus some epsilon i. That epsilon should not have an i next to it, it should just be epsilon. And we're assuming that the L0 norm of beta is equal to s, which is the sparsity level, which is much smaller than p. So just like I said, s here is playing the role of r for rank, s for sparsity, r for rank. And n here, I haven't specified, but that's the number of samples. So n is the underlying dimension. Is the underlying sample size, and here the dimension P is kind of the high dimensionality. So, typically, when you study this, you study settings in which X is a tall and fat matrix, right? So in particular, you're interested in settings when your dimension is much larger than your sample size. And we're going to come back to this model in a little bit. But if I said to anybody here, most likely, I said, how might you go about solving this problem? You would all say, okay, let it. We do. Okay, well, nobody said the same thing. Yeah, we do the lasso. Yeah. Right, so the lasso minimizes a L2 error with an L1 penalization, right? You have this L1 penalty on the coefficients, and that's to promote sparsity. This is kind of the natural convex relaxation. There could be greedy approaches, and actually a case could be made for that. And in fact, what I'll be talking about is that. And in fact, what I'll be talking about today is kind of a case like that. Very hand-wavy. Okay, so just like you might have this lasso, which is a L1 penalization, which penalizes the magnitude of the entries of your vector that you're trying to estimate, we have a matrix lasso. And this attempts to solve the problem where you have a least squares objective and you add to it a regularizer. And you add to it a regularizer, but now instead of regularizing the L1 norm, you're going to regularize the L1 norm of the singular values of the matrix, which is equally known as this nuclear norm. Okay, so you had a nuclear norm regularization. This is kind of a classical now approach to solving this problem. And by the way, because I assume M is M, which is the sensing matrix of interest, that is a positive semi-definite matrix, we are going to optimize this over positive semi-definite matrices. However, this Matrices. However, this retains a nice property that it is convex, just like it does in the parse regression case. So that's my convex function right there. Now, something about this is if you were to apply, say, a proximal gradient method to this, you would realize that at each iteration, you need to compute a full d by d dimensional singular value decomposition. That's because the proximal operator for the nuclear norm. The proximal operator for the nuclear norm is going to be the function that takes all of the singular values, subtracts lambda from them, and takes whether or not they're positive, right? It's the shrinkage of the singular values. So in order to shrink the singular values, you need to know the singular values. In order to do that, you need to calculate the SVD of the matrix, or at least all its singular values. So in particular, at each iteration of this algorithm, you're computing a full D by D S V D. And so when D is moderately large, Is moderately large, then you start to run into issues. And I was trying to get, one second, I was trying to get simulations for this, and I realized I'm having trouble getting good results for D bigger than 200 because of issues related to SVDs. Yeah. If I had to add it to I would Yeah, so that's a good question. But essentially, what you're doing is you're kind of using a truncated SVD in that setting, right? So I would say instead of doing a truncated SVD at each iteration, which still requires doing a truncated SVD, there's a lot SVD, there's also this approach. So, thank you for that tia. So, instead of just assuming, instead of running this on the full matrix space, which also has a requires saving that matrix at each iteration, why not just operate on the factors of the matrix? So this non-convex procedure essentially factorizes what the objective variable. So, instead of having z, which is positive semi-definite, we know that if z is rank r, it can be factored as a matrix. is rank R, it can be factored as a matrix UU transpose. And so instead, let's just optimize over UU transpose directly. And this actually has a hard constraint that the output of this procedure will be rank R, at most R. It could be less than R, but at most R, where R is the number of columns of this matrix U. Okay, now the problem with this is it's now non-convex, but you've alleviated the computational burden. So you've introduced non-convexity, but alleviated the computational burden. Yeah. But alleviated the complication balloon. Yeah. Yeah. So something I'm going to be. So, okay. Great question. So I started this problem thinking about neural networks, particularly two-layer neural networks. And typically in a lot of the way that kind of conventional wisdom in neural networks go is let's take a very big dimension R, where R is kind of very large. Very large. And I realized if I want to get nice results about this, we need to understand this matrix sensing problem fairly well. And I realized there wasn't the kind of preciseness and crispness of the results that I wanted, even when R is known. So for this talk, I'm going to be assuming that R is known and correctly chosen. That is a good point. You do need to know R, and the message of this will be some. Of this will be somehow be related to the fact that knowing R is giving you much more information than not knowing R, despite introducing non-convexity. Yeah. Good. So this is an asymmetric version of the problem where I'm not assuming that my matrix is symmetric anymore. So you can factorize it as u times v transpose and you try to minimize this. I don't care how you minimize it for now. Just assume that this is some minimizer. Okay, so equals argmin is meant to mean any element of the set of minimizer. Meant to mean any element of the set of minimizers. And again, you're typically for neural networks, you're typically interested in the regime where R is very large. So you have kind of large weight matrices, or at least comparable to the dimension D. And then, so yeah, what I said is to understand this problem, we need to understand the case where it's underparametrized, where there's essentially a unique solution of some kind. Okay, and so we also, the asymmetric version was also fairly difficult. The asymmetric version was also fairly difficult, so let's understand the symmetric case first. So, this is kind of a very simplified version of what turns out to be a fairly complicated problem. There's already a lot of nuance for this fairly simple version. Was there a question? Yeah. Yeah, yeah, this is just real. If your observations were non-symmetric, then you might assume the matrices are non-symmetric. Yeah, this is kind of the more natural problem that shows up in practice, right? You would not necessarily. Yes. Right, you would not necessarily need symmetric observations. Okay, but I'm also arguing that matrix sensing is an important problem in its own right and is a problem worthy of study. Okay. So we have a lot of previous theory for both of these procedures. And both of these results that I'll be talking about, they kind of mirror each other in some sense. So we have this convex estimator, which minimizes. Convex estimator, which minimizes this nuclear norm penalty with appropriate regularization parameter lambda. So Z lambda is the minimizer of this convex relaxed nuclear norm penalized, et cetera, et cetera. And then this non-convex procedure, which essentially, typically it's gradient descent, but there are variants where you can do sort of preconditioners and things as well, and accelerating it, et cetera. But all of these are essentially trying to find the minima of that second non-convex problem. Okay, so these are statements about the. These are statements about the minimizers of these objective functions. So, the convex case, the minimizer can always be found convex. However, again, it requires typically computing full D-dimensional SVDs. The non-convex procedure might be trapped at local minima. However, that's kind of maybe not necessarily the case because, in the noiseless case, in the noiseless setting, it's been shown that the landscape is. That the landscape is somewhat benign, meaning that all local minima are global minima, otherwise, they're strict saddles. That kind of result is out there for the noiseless case. So if you use a saddle avoiding procedure, then you will converge to a global minimum. In the noisy setting, the similar results aren't quite out there, but similar enough results are out there. Yeah. That's convex. That's not strongly convex. Yes, it's not strongly convex. Yes, some minimizer. Yes, any minimizer. Yeah, so Z lambda requires tuning lambda because there's some tuning parameter associated to it. U0 requires tuning R. Okay, so it depends on how many columns you want to do. So the interesting thing that I thought was with correct That I thought was with correct choice of lambda when n is larger than some constant times d times r, where dr really is your intrinsic dimension of m, remember because it's a low-rank matrix, d times r. Your estimator attains an error rate in Frobenius' norm, scaling like some constant times sigma times the square root of d times r. And actually, with the correct choice of r, your u0 attains the same error rate. So something on we, something. So, something kind of smells weird about that to me, because with the correct choice of R, you somehow know more than in the case of Z lambda, where you don't know R. And this is the correct tuning of Lambda, but Lambda actually doesn't depend on R for this tuning, which, I mean, it depends on the noise value sigma, it depends on the dimension D, but it doesn't depend on R. So something kind of smells weird about this, these types of results to me. They shouldn't be the same. To me. They shouldn't be the same, in my opinion. Maybe you might agree, disagree with that. So the idea is that somehow, remarkably, both procedures attain the same optimal error rate. And there's actually lower bounds in some sense saying if n is smaller than d times r, you can't get an error like this. So this is at the optimal rate up to that constant. This is the best possible rate up to constants that you can get for this error. And somehow both procedures attain. And somehow, both procedures attain the same error rate in this high-dimensional regime that we're interested in. So, again, something smells funny here, and I want to see what it is. So, what I'm going to be focusing on is what is the precise statistical behavior of each of these estimators. So, on the one hand, we have this convex z lambda, and on the other hand, we have this non-convex u0. Any questions so far? All right. So here's our statistical model. So again, this hasn't changed from several slides ago. But we will be assuming that the epsilon i's, which are a noise, are a Gaussian table. Yes. You would get flat or right matrix without knowing the right or some range of lambda. So, in particular, recommended result was something like one or two. Yes. So, that was also another result where you could infer the rank without knowing. So, why given given that why this Given that, why PCA coefficient PCA? Yeah, so because in the robust PCA setting, you're typically observing values of the matrix itself. You're observing the entries plus sparse corruptions. Here you're observing through an operator, and that operator is Gaussian. So it's like really scrambling it up. It's not just giving you the exact values. And typically, when we think of, for example, linear regression or sparse regression, we don't necessarily Parse regression, we don't necessarily think that we can do exact reconstruction unless you have kind of, unless you're in the noiseless space. And so what was weird to me was the fact that the noise dependence was the same. Yes, there are those kinds of results, but they did not, I think, really, I think the fact you can have exactly. I think the fact you can have exact recovery, right? And exact recovery, I think either you have it or you don't. And I think the fact that you have noise here is really what's interesting because here you need to determine your tolerance to the noise. And I think the difference here is that somehow the tolerance to the noise is the same for both estimators. And that's what's confusing to me. Yes, good question. So each XI is drawn from the. Sorry, did you have a question? is drawn from the sorry did you have a question for sensing setting if you look at um product for recovery of lustro versus say greedy approach um whenever and in one case um whenever the greedy approach works is the same regime where the lambda works you get Yeah, I disagree with that. Yeah. And I can quantify exactly, and it's actually a very similar result to what we're saying here. Yeah. Yeah. The regime in which they do not agree is the exact same as the regime in which essentially S is the sparsity, so the mini-max rate is S log P over N. It's where S log P over N does not converge to zero. And they actually don't necessarily agree in that case with noise. And there's a very precise. And there's a very precise sense in which that can be made the case. Yeah. That's right. Yes, that's right. And I have two ways to estimate M. M is a low-rank matrix, so I can pre-factorize it, or I cannot. Yeah. All right. Good. So my model here is that each xi is drawn from the Gaussian orthogonal ensemble. And this is somehow the natural generalization. somehow the natural generalization of a standard Gaussian to matrices. So what does this mean? It means that the diagonals are drawn from a normal 0, 1 and the off diagonals are drawn from a normal 0, 1 half subject to the symmetry constraint. And I'm going to talk about this in just a moment. But I'm also going to be assuming that m is of this form where it can be factorized as square root of d. That's just because scaling tends to be more useful. Of v lambda v transpose, v are the is the m by, or sorry, d by r. Is the d by R matrix of eigenvectors, lambda is the diagonal matrix of eigenvalues, and I'm going to be assuming that the smallest eigenvalue divided by sigma is larger than some constant. And that's basically to assume that m is kind of not essentially lower rank than it is. Because if that were not the case, then we could pretty well approximate m by just taking a smaller rank. So I want to assume that r is somehow correct. Okay, so why the Gaussian orthogonal ensemble? Because maybe you haven't seen this. If you're from random matrix theory, you might say, well, yeah, of course you would use that. But if you're not from random matrix theory, you might say, I don't know why you're using that. And I think the idea is it retains the key properties of Gaussian, standard Gaussian random factors. So we take x, which is a matrix which is drawn from a Gaussian orthogonal ensemble, and we consider a lowercase x, which is just a standard Gaussian. A lowercase x, which is just a standard Gaussian in p dimensions. So the GOE has the property that RxR transpose is equal in distribution to x for any orthogonal matrix R. Similarly, R times X, where X is a vector, now is equal in distribution to X, right? That's the rotational invariance of the Gaussian. We also know from random matrix theory that the spectral norm of X is roughly order square root of D with high probability. And that's from chapter. And that's from chapter three of Vershinen. Similarly, if you take a standard Gaussian vector, its norm will be order square root of p with high probability. This is with exponentially high probability and d or p, respectively. And finally, we have this nice kind of, this is actually a product of the rotational invariance, essentially. But if you inner product x with any matrix m, it's a Gaussian random variable with variance Ferminius norm of m squared. And if you interproduct lower. And if you interproduct lowercase x with any lowercase m, it's a Gaussian random variable with variance norm m squared, Euclidean norm. Yeah. Yeah, so it turns out that this rotational invariance property is really key to our results. Yeah. And the reason actually a lot of the similar type results for the lasso are holding is because of this rotational invariance. No, it does not. ID Gaussian asymmetric. It does satisfy this, yeah. But you have to, you can change R here, so it's not the same R. Yeah, I want just, I want the same R on both sides there. But if you were to put a different R and then just have IID Gaussian, then it would be the case that that's true. Yeah. And the reason we want the same R here is because M is positive semi-definite, essentially. Yeah. Okay. Okay. All right. Let's go back to our two estimators, just to remind you. We've got our convex sky and our non-convex guy. Convex guy, Z lambda, non-convex, u0. Z lambda minimizes what I call g lambda of z, which is just this nuclear norm regularized squared loss. And u zero is any minimizer of this f of u. Minimizer of this f of u, which is just the squared loss where you've parameterize z with u u transpose. Okay. All right. So here's here's the first, this is actually a corollary, but I'm calling it a theorem because it's a little bit easier than the theorem. So for any sequence of matrices M satisfying our main assumptions on M, namely that a smallest eigenvalue is larger than Eigenvalue is larger than some value lambda min, which lambda min over sigma is bigger than some constant. As long as the minimum of n and d tends to infinity, such that sigma squared r squared d over n tends to zero, with overwhelming probability, it holds that the Frobenius norm of z lambda minus m squared divided by the Frobenius norm of U0 U0 transpose minus m is bigger than or equal to 1. One with strict inequality holding in settings that aren't essentially trivial settings. There's strict inequality in nice, many, a large swath of M's. And I'm assuming here lambda is appropriately tuned. So lambda is somehow taken to be the optimal lambda that achieves that error rate that we saw before. Okay, so what does this tell us? This is telling us something. If R is fixed and known, the convex estimator is in fact inadmissible. And by the way, this, each of these Frobenius norms, you can think of those as actually your test error given your data. That's a random variable where the randomness just comes from your data that you have. This is actually your test error for your. This is actually your test error for your z lambda and u zero. Yes, if you can compute it. Yeah, so I said this, I don't think it is a strong assumption because of nice results about the geometry. So that there are gradient descent, roughly speaking, will converge. There's a lot of. There's a lot of kind of caveats to that, but the gradient descent, roughly speaking, will converge to a local minimum, and roughly speaking, all local minima are global minima. So, yeah. So, why is this true? No, so lambda is lambda is. I'm just saying it's an appropriately tuned lambda, that is the correct choice of lambda to attain the upper bound that we saw on the previous slide. No, no, no, it's not for all it's it's for that particular choice of lambda, that's right. What yeah, I've chosen the best one. Yeah. Oh, yeah, okay. Yeah, you have to be a little careful because that's a uniform statement on lambda. And this is a random variable that changes with lambda. So you have to be a little bit, no, you can't do that because it's like, you have to take like an argument to make that the case, yeah. The case, yeah. Yeah, you could do that though, probably. I think you'd lose, like, this overwhelming probability would be slightly less overwhelming. Okay, so why is this true? So, in order to understand this, we need to understand the matrix denoising counterpart. So let's consider a simple matrix denoising problem where we observe M, this is the same M plus sigma times H, where H is a Gaussian orthogonal ensemble matrix. Orthogonal ensemble matrix. Again, this is a low-rank matrix plus noise. I spent the majority of my PhD working on problems that are exactly of this form, so we know nice results about this. So we've also got a convex estimator in this setting and a non-convex estimator. The convex estimator is just the soft thresholding function. It's the prox of the nuclear norm with respect to where you observe m plus sigma h. All this does, decompose. All this does, decomposes. Well, okay, you also have a hard thresholding estimator. So the soft thresholding estimator takes your singular value, or your singular value decomposition of m plus sigma h, eigenvalue decomposition, and takes w hat times the soft thresholding of the eigenvalues times w hat transpose. And so this minimizes exactly the same loss function without having that pesky x sitting inside there that we had before. There, that we had before. Similarly, your hard thresholding procedure just takes the leading R components of your singular values. So it just hard truncates past R. This also minimizes the corresponding loss function if you observe m plus sigma h. So now we have a much more general statement here. So we're going to let phi So we're going to let phi be any one Lipschitz function with respect to the Frobenius norm. Okay? And this eats matrices and outputs numbers, this phi. So fix your favorite phi. I'm going to be fixing phi as the Frobenius norm in order to get my previous result, but you can choose any one Lipschitz function. So in the same regime as before, as long as sigma squared, r squared d over n tends to zero, and we're going to fix any epsilon. To fix any epsilon, we're going to look at so ignore the scaling by this one over square root of dr. That's just because the norm of u0 u0 transpose in Frobenius norm is order square root of dr. So normalizing it gets it to have a norm that's order one Okay, so we're gonna take phi of u0 u0 transpose Okay, so that's a number, right? But that's a random variable, right? We wanna study its behavior It's actually going to concentrate It's actually going to concentrate. The statement of this result is saying it concentrates around the expected value of the corresponding hard thresholding in the matrix denoising case. The beauty of this, the hard thresholding and the soft thresholding have no x living inside them. They're just, if you observe m plus sigma times h. Right? So somehow we've eliminated the x. So if I were to say this result without with the x inside the statement of this expectation. The statement of this expectation, that's not as strong of a result, right? Because having the x in there is maybe not as big of a deal. But because we have this effect, that's almost, well, not trivially true, but with some work true. But the neatness of the result is that what's inside the expectation doesn't depend on x. This is saying that, and we have the same result for z lambda. Z lambda concentrates around the soft thresholding matrix denoising counterpart. No, Z is a hard thresholded version of M plus sigma H. This is a new random variable, sigma H. Yeah. You observe M directly, just a noisy problem, and we know our operators. That's right. And now you've got. Yep. And now you've got, so that's a random variable. These are random variables that have different spaces, but you take an expectation, so it's just a number now. Right, so the probability that either of those quantities are larger than epsilon is a little O of 1. Yeah. No, no, because this is it. This is all that it is. This is the statement of the result. So, what I'm going to do is I'm going to show. So, what I'm going to do is I'm going to show that Frobenius norm, I should have written this proof in here, I don't have the proof, but I can take phi to be the Frobenius norm and Frobenius norm minus m. So that's my numerator, and I take phi to be the denominator. They concentrate around Z, the corresponding Frobenius norm of Z hard threshold and Z soft threshold. Turns out Z hard threshold dominates Z soft threshold for matrix denoising. Why would the computer have a configuration? Because this expectation here is something that we can actually compute. We can't necessarily compute it in the case where x is just a general operator. We actually understand the behavior of matrix denoising estimators. Right, so actually the reason I stated in terms of the, which are one Lipschitz functions, is a sort of well-known result in probability is that if for any one Lipschitz function, you're the Your empirical distribution of one, or all one Lipschitz function of your empirical observations converge to the one Lipschitz function of some other observation. That means that they converge in distribution. So this is actually telling us that in some sense, a slightly stronger sense than what I stated, because it's not just converging to zero, it's a little bit stronger. But here the U0, U0 transpose behaves, roughly speaking, equal in distribution to its hard thresholding matrix denoising counterpart. And Z lambda behaves roughly in distribution. Lambda behaves roughly in distribution to its matrix denoising soft thresholding counterpart. Okay, so with just a little bit of time, I'm going to talk a little bit about the lasso and hopefully answer your question here. So suppose we're given this, again, the lasso setting where you have n observations and you're assuming the sparsity is S sparse and it's living in P dimension. So again, we have this lasso estimator which has L1 penalization. Lasso estimator, which has L1 penalization. And we're going to assume, I'm actually assuming the consistency regime here, but the results actually can be extended, not this regime. So the consistency regime means that the minimax error rate of beta hat with scales like this quantity right here, and I'm assuming that this quantity right here goes to zero. Okay, so p can grow however you want as long as this quantity goes to zero. So we're going to define a Gaussian. So, we're going to define a Gaussian denoising model, which I changed it to Gaussian denoising, but sometimes they call it the Gaussian sequence model. So, I'm going to superscript seek for sequence. So here, just observe beta. Oh, there's a typo. That should not, okay, there should be, yeah, it should be just sigma times g. Yeah, there's no fraction there. I don't know how that appeared. Yeah. Yeah, that could lead to problems. Okay, so that's sigma times g. times G. So yeah, Gaucho denoising. And we've got the corresponding soft thresholding estimator, which is just your usual soft thresholding element wise. So it's actually been shown that for any one Lipschitz function phi, again, kind of the same setting as before for any epsilon, the probability that phi of beta hat, which is your lasso estimator, scaled by square root of p, that's just for scaling issues, concentrates around your soft thresholding of your Thresholding of your sequential corresponding Gaussian sequence model, you take that expectation, but it concentrates essentially in the same kind of statement as before in this regime. And what's nice now is the sense that I made that statement that I disagreed with you is this is a soft thresholding of the sequence operator, but you could also define the hard thresholding, and you can have a similar result for the minimum, global minimum of a greedy, like if you were to try to minimize it globally. Like, if you were to try to minimize it globally. And that's why, well, that's out there in the literature. This is not proved by me. Yeah. There's actually, this is a very general line of work in statistics showing this principle. That's the Lasso estimator with the X. Right, so again, beta hat, which is our Lasso estimator, roughly looks in distribution like the soft thresholding of a gas. Like the soft thresholding of a Gaussian sequence of the corresponding Gaussian sequence model or Gaussian denoising model. Yeah, yeah. Except now the sigma has to be inflated. Yeah, it's really complicated. So I won't go into the details. But yeah, sigma is not just sigma anymore. It's sigma plus something else that depends on the high dimensionality. So what did we show? We showed that the Showed that the convex estimator behaves like the matrix soft thresholding. This is showing that the convex lasso for linear regression behaves like the vector soft thresholding. But we also show that the non-convex behaves like hard thresholding. And that's really the novel, the novel statement. I mean, the first result wasn't out there, so we had to prove that too. Yes, and non-convex uniformly dominates. And non-convex uniformly dominates convex. In particular, if r is known. So I set out to do this when r is not known. But yeah. I think they might be. Yeah. They might be. Well, I think it'll be different. It'll be a very different flavor. Yeah. I think there'll be some space regimes where one is better and yeah. Space regimes where one is better and yeah, one, yeah. Yeah, you can, but I'm not doing that. Yes. Yeah, because knowing R is different than finding the minimum over all R. Yeah. Yes, that's right. Okay, so again. Okay, so again, what was the main question? What is the precise statistical behavior of each of these estimators? I looked at my watch. I realized I have no idea how I'm doing on time. So, minus 15. All right, right on time. So, it turns out they match each estimator in the corresponding matrix denoising model. And in particular, the non-convex with known R beats the convex. And so, my sniff was actually right. Was actually right early on. All right, and that is it. Thank you. Remember who said last one and the first slide were wrong? Yeah. Well, yeah, so again, it's global optima, so greedy. I think I wouldn't even have it very well. Well, there's this one where you can look it. It turns out that the non-implement is only when you have noise or like hard or soft. I see. Because it's a one-zero problem when you have no noise. It's either you cover or you don't. Wait, wait, wait, wait. I never have to develop your fire title. Yeah, I'm not with her. Well, I think the difference, though, I really figured out why they got it here, but the difference is that you have a border. And there's a detail here. Actually, so I hit this in there, but I assumed that sigma was strictly non-negative or strictly positive. If I want to make wave in my in the segment in order for that, in polyteam.             Okay, come on to the extra web.         Yeah, I think Oh. Yeah, I thought I was trying. Yeah, I thought I could have a minute. You were in the same plane that I was, right? Going down, back up? Yeah. It didn't fall, but it did not land. For a couple of minutes, I was thinking about believing in God. I've been there. No, it was pretty scary because, yeah. It was, yeah, he said it was too windy to land the first time. Can we begin, Rene? All right, so there's 10 of us, you don't need to introduce yourself. So, you don't need to introduce me. Alejandro, just a few moments ago. So, Alejandro Will Norden, professor at the University of Pennsylvania, and he will talk about constraints, report.