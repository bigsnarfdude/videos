Very nice workshop and for the invitation. So, yeah, I'm Elia Zadik, and so what I'm going to talk about today is joint work with Jonathan Elswidth from NYU. Okay, so this talk will be about the ononatic phenomenon, and specifically the case of sparse network PCH, a specific inference model. And, you know, if you're not familiar with it, just like the general context, it's like sub-phase transitions in the context of statistics, in the context of inference. So you may, you know. Concept of inference. So you may, you know, judging on the conference, like, you know, you may have heard about smart space transitions in, you know, various different random structures. Like, this is kind of trying to introduce it in the concept of inference. Okay, so let's see. So, well, what is the main topic? The main topic is phase transition, as I said. So that the fact that Heidegger's statistical models actually also undergo phase transitions, sometimes very sharp ones. Phase transition is a term for physics. It corresponds to the idea that small changes in the parameters of the model can really change structural. Parameters of the model can read the same structure with the model itself. And you know, what could be like a good phase transition to keep in mind, what's a good, a good example of what type of phase transition we're referring to is like what is known as the PCA, for example, phase transition. So the phase transition with respect to the principal component analysis. That's also known as the BP phase transition. You may have heard it in this spectrum of radomatrix context. So what's the idea here? The idea is that you may have a spike that is just a uniform vector. Form vector on the sphere in P dimensions. So you can imagine that you have like just here the sphere in p dimensions, and you choose like a beta, which would be your signal that you do not know somewhere like on the sphere. And you have a W, which a matrix with IID standard Gaussian entries, and you have Y, which is your observation matrix. And it's just given by the addition of W plus the Rank one matrix produced by beta times square root some constant times the dimension. And then the goal is from Y, you want And then the goal is from why you want to be able to infer the rank on space. You want to denoise these observations. And you can ask, of course, how you can do this with respect to the MMSC, the mean, moment, square error, of, you know, like of which essentially the mean of the L2 square between the rank-1 spike and the posterior mean, which is the best estimator in the L2 sense. And you detect the limit. So that's the total camera. And some interesting phase tradition happens: is that if lambda is less than one, so you know, you can see that if lambda grows here, the See that if lambda grows here, the larger lambda is like the easier should thing, that we speak. Somehow, if lambda is less than one, the interesting thing is that symptomically this is one, which is actually completely trivial. So why is this trivial? It's because this is really the performance of the zero estimator that just doesn't use at all the observations. Y, and this is because you know it has unit log, the spike. On the other hand, if lambda is bigger than one, something suddenly happens, and somehow you get an MMSC which is strictly less than one. Somehow, like this phase transition, you can see. Somehow, like this phase transition, you can see it like this: like if this is the limiting MMSC and this is lambda, all the way to one, it's trivial and suddenly becomes down like an invariable. And that's like, you know, the limiting MSC phase transition or the BBP phase transition. So you may be familiar with this or in other contexts as well, like for example, in stochastic low modeling, and you can be saying, okay, it's like the classic, this is the normal thing, that somehow the limiting emissive has like the smooth curve as the dimension grows to infinity and there's a narrows. The actual answer of this is no. So there are examples of statistical impress models that this is not the case, and this is what the OLAT phenomenon is about. So to introduce this phenomenon, let's consider the previous model under a sparsity assumption. So imagine this model of PCA under an assumption that the signal is actually K-sparse. So under an assumption that the signal is K-sparse, where the sparsity is case sublinear to dimension. And this is a model initially introduced and analyzed in this context of... Used and analyzed in this context of Little Nothing by John, Nicola Matri, and Cynthia Ras, and also later this year, last year with me and John. So, what's the idea? The idea is: imagine that beta now lies on the sphere, but also its sparse. So it's k-spars, it means that exactly k of the ends are non-zero, and also that it's binary for simplicity. So it takes value zero or one over square root k. So this gives, you know, like just like this finitely many values that beta is chosen from. And then it's the same model: y, noise, beta, beta transpose, square root some constant. Transpose squared with some constant, and well, now you don't put the dimension because it's another point where the tension happens. Hopefully, at the end of the talk, you will understand why it disappears here. And then, what happens with Lumium MSC is that if lambda is less than one, it's trivial, but suddenly if lambda is bigger than one, this becomes vanished. So this becomes like, you know, if you go from nothing to all, suddenly, like the transition, it's a step transition, but you know, you go all the way to one and suddenly you drop down to zero, which is a very quantitative different picture. And the question is: you know, why this is happening? And, you know, this is really in sad contrast with regular PCA, like the moment you add this assumption, that somehow there's a different point where this really is a jump in the limited MSC from trivial to validate. And, you know, a little bit of a literature about these phenomena when they have been first observed. So they have been first observed in different setting in what is known as parse linear regression, where the spike is sublinear in the square root of the dimension. Spike is sublinear in the square root of the dimension. So it has been recently established for the MLE and conjectured generally by a paper of mine, David Gamani, 2017. And two years later, it has been proven by Caloon's Jamixu and myself. And after this, actually the last year, we observed in our community quite some work, actually realizing that there is such a surf-based addition going on in many other contexts. And a lot of people are participating in this conference. For example, where some, if you want, natural sparsity parameter or sub-linear sparsity, I mean, it's natural. Parameter or sublinear sparse, I mean, it's natural due to small television. For example, in the Bernoulli group testing world, like we have seen, like by some work of true altitude and scarlet, like last year, that when the data line sparse is called logarithmic, like the same surface essentially is happening, sparse PCA, when case sublinear in P, but bigger than P to some power, Zeroes linear models, same situation and a little bit different setting on the radon graph mass inflation. So the main question is what is causing this surfaceization? Like what makes this being so different? What makes this being so different? And also, like, there is the question of why there are these restrictions on the sparsity. Like, why there are these restrictions on the sparsity? Are they fundamental or not? So, what I'm going to describe to you is, as I told, the context of sparsness of PCA. So, this is a model that includes the sparse PCA model I told you before. It's a little bit more general. Let me tell you what this is. So, it's like, again, you see why, your observation is why, but just the tension now, which is given by gas. The tension now, which is given by Gaussian IID elements per entry, and then you just take the spike, the vector, and you take the t-tension power of it. So you just add, you know, you scale a little bit, and then you add Gaussian noise. And the question is whether you can infer this random tensor. And again, the sparse becomes, you know, you assume that the signal is actually sparse. Of course, when t equals 2, like this is the sparse we see any more. So, and you know, of course, you can imagine t larger than 2. What I'm going to describe is a result that's What I'm going to describe is a result that says that, so initially, like Barbia, Macris and Ross proved last year that actually when t equals 2 and k is sublinear in p, but also bigger than p to the 12 of 13, some constant actually the online phenomenon appears. Actually, we establish a result that this online phenomenon appears for all sublinear sparses in this setting and for any t and for any k which have been any p, which is interesting. But the reason I'm choosing actually to say to talk about this model is because it's a clean enough model that we can kind of Clean enough model that we can kind of completely characterize about an even larger family when the transition is for such signal plus noise models, if you want. And when the transition is when all or nothing appears. And the reason is that we can prove that the surface transition is actually equivalent with a simply probabilistic condition for these models. So we can prove that the surface transition actually boils down equivalently to a simple relative entropy condition or KL condition between two laws. And you know, this is why I prefer to talk about. And this is why I prefer to talk about this, for example, as compared to sparse linear regression, where this phenomenon has emerged, like as a first example, is because it becomes much cleaner. Okay, so let's jump into it. If there are no questions, to see like, you know, what exactly I'm talking about and what exactly is this sub-phase transition and why this model is of interest to study. Okay, so the little bit more general class, we're going to start to study the sparse entropy. Class, we're going to start to study the sparse natural PCA. It's what is known as Gauss-Anatic model. So it's a very simple model, actually, if you have never encountered a very simple infra setting. So you have again on the sphere, let's say on capital N dimensions, and you have S, which is just a finite or constant, not constant, a finite size subset of the sphere. So let's say S here. And you choose beta, your para, not uniform over the whole sphere, but uniform over this subset, which could be arbitrary. Okay? Be arbitrary. Okay, so your signal is just a vector which chosen out of this finite subset of the sphere. Then you choose noise, which is a vector with ideal Gaussian elements, and you're observing y, which is a square root lambda, the prior you chose plus noise. So no matrix, you won't know anything, just the vector plus noise model. And just, you know, there is this S, this prior information. Okay, so why this model is interesting, why the Gaussian model is interesting, besides its hopeless implicity, is because you can call. It's hopeful simplicity is because it encompasses actually the PCA models. So it encompasses the PCA model I told you in the first slide with a slight twist that's actually rather much here on the spike. So it's rather marked PCA that also actually has a BP phase transition. So it's also like not sharp, the MMS phase transition. Why is that? So in this model, recall that it was of the form square root lambda V V transpose plus W and V was a uniform spike. So you can just vectorize everything. If you can just vectorize everything and you have vectorize of y, it's just Everything and you have vectorized of y, it's a square of lambda, the vectorized form of vv transpose plus the vectorized form of taffy. So, if you do this, then you know if v is like plus minus one, okay, a property scale to be on the sphere, and you you can just take the vectorized, you know, take all the random matrix that you can obtain and take the vectorized form in p square dimension, and this will be your s. And then you really have an one-to-one mapping between whether the matrix is here and the oscillative model. So, this has not the olon phenomenon, but then you can introduce sparsity here. But then you can introduce sparsity here. So you can introduce sparsity here and assume that like it takes values minus of n squared squared k, plus or n squared k, and zero. And then again, if p squared measure, and you have actually the sparse p C m on the last in the second slide. And also you can introduce a tensor power. Why not? Like just say the vectorized form of the tensor power of A and go to the P to the T dimension. And with this fast tensor PCA model, like I formally now introduce. And you know, what's interesting is that like this has BPP, so that has not a sharp phase transition. So, that has not a sharp phase transition, but this has a sub-phase transition, as I told you, and this half, as I will tell you, actually, and this has. So, surprisingly, you know, it's a simple enough family that actually brings the dichotomy. So, it shows the dichotomy between SAT and also for this limiting number. Okay, so cool. So, hopefully, now you have some interest in this Gaussian active model. And now, let's see what the phenomenon is about. So, a nice thing about the Gaussian active model is you can really The Gaussian models, you can really very cleanly say what is a loneliness phenomenon for it. It means that if you consider the limit in OMSC, there is some lambda critical that it jumps from one to zero. So there is some lambda critical that, you know, you can think of it what it depends, so what the model is defined on, right? So it defines just the support of the prior and the dimension of the model. There is some lambda critical that for any epsilon positive, let's say 0.01, if the lambda is less than 0.99 lambda critical, it's trivial. Critical, it's trivial, but if lambda is bigger than 1.001 lambda critical, it's valuable. Somehow, like all of the number happens, there is such a sharp phase transition essentially for the limiting embers at some point. Okay, so this is the definition of what only phenomenon is for a Gaussian active model. Let's see now the main result. So, let's say the main result that I told you that's actually equivalent. See the main result that I told you that's actually equivalent, it boils down to a probabilistic condition. So, for this, I need three small notations. So, let's take S, this hidden subset, and this hidden subset is support of the prior, and I take M to be the cardinality of S. So, just take the possible values of the prior container. Let's denote by M, you know, just as a thing, for example, the entropy now of beta, because it's a uniform over m points, it's just log n. Okay, now define PC. Okay, now define P sub lambda to be the law of the observations you see at signalization lambda. So to be essentially the mixture law, if you want, of what you see here. So this is a notation. And Q define essentially P sub zero, the observations you have if you didn't have any signal flux ratio. So the Y sub zero because you can see that this is, you know, just W here. So this is standard Gaussian law, if you want, in the dimension. So if you do this, actually, the main result is as follows. You can prove that. You can prove that actually, for any Gaussian attitude model, under a small non-degenerate assumption that I'm skipping, the all-on-atting phenomenon happens at some lambda critical for this Gaussian model if and only if all two conditions hold. First, the phase transition should take place at twice the entropy of the prior. There is no other point whatsoever that the transition can take place. Such a subphase transition took place. It needs to be at twice log the cardinality of the prior, of the of the of the support of the prior, up to one plus e. Prior up to one plus equal to so, like the first implication is that there is no other point where the transition can happen. And the second point is for the transition to happen, if you take the law of your observations at criticality, so that's P2 log m, and you compare it in KL distance with the law at zero SNR, so like what your observations look like at complete noise, then the KL distance between the two distributions divided by the entropy of the prior value of M should vanish to zero. Value game should vanish to zero in the large dimension. Essentially, it doesn't need to nulliate, and that's a nice thing, I think, about this theorem. So, the only light phenomenon happens at some lambda critical if an only lambda critical is too log n, and the Kl distance at criticality and at zero signal ratio vanishes in the limit at a fast enough rate. It doesn't vanish in the limit, vanishes compared to the entropy of the drive. And that's an if and only if, and that's kind of the nice equivalence. So, one good thing about this is that now if you have this equivalence and you care about the specific. This equivalence, and you care about the specific model like fastest of PCA that has been studied in the literature, you just have a probabilistic and calculus question in front of you. Just go out to ice log n and prove that the K L vanish is converted to log n. Like in the like and just, yeah, so it gives you a tool. And that's actually what we did. So to generalize the result in the literature about sparse SPCA, what we said is, okay, so let's go here, consider S to be the k-sparse binary, let's say, vector. So that's the beta to be 0, 1 over square root k. 0, 1 over square root k the p that are you know k sparse okay what is m here m is the cardinality of s and that's just p twos k because it's a one-to-one correspondence between beta and the support of beta so that's the m so okay so that should be the criticality that's why there is was before the two log m there the two log p to sk so and then for any t which is the tensor power and for any sublinear you know sparsity in p you can consider y to be constant times two log m square log m times the t tensor power of beta Square glue of m times the test of the power of beta plus rods, and that's the sparse of PCM model that has been studied. And you just need to prove this K L condition, and we actually prove this. That's by a small condition intrigue, we establish that this K L condition holds at two log M at two log V to K everywhere, and therefore the limiting MMSA obtains the transition indeed as lambda equals one, as predicted for any K little log P and for any P bigger than two. And that's the result about. And that's the result about sponsorship C, which is just an application of the main result. And okay, so let's say again the main result. So the main result is of this form. Solonite phenomenon happens at some lambda critical if and only if lambda critical is true again and there is a scale condition. Okay, so at the following minutes, I would like to dive into the technical aspect of cycle time to see why this equivalence holds and what's hiding behind it. What's hiding behind it, and why, like, an MMSC phase transition, like, you know, like actually boys down to KL. Okay, so let's see what's going on. Okay, so let's define P sub lambda to be the law observation, right, that we had before, and Q to be the standard Gaussian law of y sub zero. Okay, and what we need to show. Okay, and what we need to show is that all of that phenomenon happens at some lambda critical if and only if lambda critical is two log m, and the kl between the two logs divided by the entropy of the prior is equal to zero. Okay, let's try to demystify this connection here. So the main demystifying tool for this connection is what is known as the IMMSC relation. So what's the IMSC relation originally proved by Gauss and May Verdu? It's a very beautiful relation that is actually folding exactly, just follows by, and essentially connects. And essentially, it connects the KL distance between P sub lambda and Q with the MMSC in a very exact way. So there is nothing hiding here. I'm not hiding anything. There's a really exact way to connect these two objects. And it's a type of relation that I can give to you. And after an hour of integration by parts, Gaussian will prove it. So it essentially says that, like, if you have the, if you take the KL distance between P sub lambda and Q, and you take the derivative with respect to the SNR, this is really just half minus half the MMSC. Half minus half the MMSA for any for any standard, for any gauss-latitude model. This is just true. And this is a beautiful relation because why it's important for us is because, you know, in the all-on-adding phenomenon, this is equal to asymptotically admittedly asymptotes to infinity to trivial if lambda is less than lambda critical and zero if lambda is bigger than lambda critical. So in particular, the derivative here is equal to half minus half, zero if lambda is less than lambda critical, and half minus zero, half if lambda is bigger than lambda critical. If lambda is bigger than lambda theta. So essentially, the Olamath phenomenon equivalently, that's a totally taking some limits here, but equivalently boils down to a phase transition in the K L in the derivative of the K L diverse with respect to the S and R. So that's the first connection. That's actually where the big connection is happening to the K L. So why this is interesting, right? So imagine now the K L. So if you take the K L here, at lambda equals zero, because Q is P of zero, right? So this is the Q is the standard log Y, it starts at zero. It starts at zero, and you know that under the Donald phenomenon, its derivative is constant all the way, it's zero all the way to lambda critical, so it's a constant function. So, for Donald phenomenon, it happens all the way to lambda critical, this should be zero, and then it's just half the slope, so it should be half lambda minus lambda c. So, essentially, geometrically, the K L should behave in a very particular way for the oral light phenomenon to happen, essentially, in the two different states, like it's constant and then half. This flow, and this is actually, you know, we can make this one because I'm hiding some limits here. So as I could take the correct order of the limits, you can prove that the only phenomenon happens at some lambda critical if and only for any constant alpha, the limiting KL at the observations constant lambda critical, Q divided by Lambda critical, this pointwise limit of this object here, is actually half alpha minus and positive. What am I saying here is that you take this function here, and you take the pointwise limit of it, and what happens is with respect to alpha, It and what happens is we expect to alpha because you scale things from zero to one, it should be zero, and then it's just alpha nine to alpha. And you know, why now suddenly there is a lambda critical in the denominator is because by chain rules, if you differentiate this with respect to alpha, you get essentially this because that's what it is. So that's why you get the extra entropy in the denominator. Okay, so that's the connection between now you have all my phenomenon transferring to an infinite family of KL conditions. Family of K L conditions. So the question now is: how do you boil it down to one KL condition? And also why the lamatritical should be always twice the entropy. Okay, so let's see. So let's see a little bit high level, like the two connections, the final two steps of how to do the two implications. So let's see how this is true. Like let's say you have this KL condition and you're interested in the critical tattoo log n. So you can just consider this function, right? So you need to prove equivalently from here that the pointwise limit of this takes this forward. You can just consider Takes this form. You can just consider this function, this sequence of functions here, so at alpha two log m. And you know, you can, you know, of course, continuously, but for alpha equals zero, it is zero, this is trivial. And it's by just analytic properties, it's not decreasing at happier since analytic properties of the KL. Now, this condition says that you got it at alpha equal to one. So the condition is true at alpha equal one that you want. So because this is true, and at one it's fixed to be zero, because it's not decreasing, it needs to be zero all the way here. And because it's Way here. And because it half ellipses, it's actually upper bounded always by this line that you wish actually to be on. You know that it's upper bound. And then by just expanding the KL, actually the other direction is the easy one. So you can just, it's a fine step to like a page of calculations to prove the other good one. So that boils down to this implication. The other question is: okay, what happens in the other direction? Why follow that phenomenon happens equivalently if this infinite family of KL conditions take place? Infinite family of KL conditions take place while the lambda critical is twice the entropy necessarily, and also this one K L condition. So actually, if you prove that Lambda critical is two log M, then you know you can just come here and actually get the desired K L condition here by just plugging alpha equal to one. Why? Because you know this object here for Lambda critical two log m is this object exactly up to a factor of two in the denominator and this is after you zero. So essentially it all boils down to for you know to get from Points down to for you know to get from the infinite family of K gal conditions the one desired KL condition to essentially prove this Lambda Cali that like the Lambda Cali should be twice the entropy that there is no other point and this is really just an algebra I mean this of course it's like for the for the expert here but like this is really a this is really like an algebra calculation so you can prove that the K L condition like it's it's an easy calculation that for any Gaussian model the KL actually decomposes with respect to the mutual information between beta and y sub lambda Between beta and y sub lambda, and you know, for alpha bigger than one, so this is this calculation here, and for alpha bigger than one, actually, you know that the neutral formation between the signal and observations is actually the entropy of the signal, right? Because the entropy, the neutral formation is by definition, the entropy minus the conditional entropy, and this is of lower order because recovery is possible. So, this is really like lambda over two minus minus log m. So, essentially, by just decomposing, you know that this function, you know the point-wise limit, this alpha over two minus. We know the point-wise limit is alpha over two minus log m over lambatritical, but of course it should be half alpha minus half. So, by equating these two, we get that like the lambda critical should be two log m. So, okay, that was a very technical slide, just for the experts in case they are interested of like how do you make this equivalence. And let me wrap this up. So, you know, this is the spastes of PCA model I'm talking about. So, I told you a way to get the low-line phenomenon, you know, to improve it. It was known only partially in the literature for all cases, applianity. In the literature, for all ks, applianity p, and for all t. And actually, I think the nice thing about this result is actually boils down to a really just like making the whole life phenomenon a simple real event of the completion, and then just essentially apply it. And okay, so let me let me say some final thoughts. So, okay, in case you haven't heard of this, because you know it's quite recent, it's only the last four years going on, like this a new SAR MMSC phase transition going on in inference. It will be very nice to see if we can go. It will be very nice to see if we can go beyond the MMSC, right? Because MMSC is not, for example, computational. Like, if we have, like, for example, is approximate message passing in sparse models exhibiting the same phase transition. We have some reasons to believe that, you know, yes, but like very, very partial results in regression and sparse PCA. There is also another, you know, this gives a clear understanding of when the transition is happening for Gaussian Addictive models. It would be very nice to go beyond. I mean, we have some partial recent work with John on noiseless discrete channels involving the group 10. Noiseless discrete channels involving the group testing, for example, model that people have worked on. It would be nice to go beyond this. I mean, there are some, you know, more minor questions like is sublinear sparsier discrete really crucial here. And just a final thought and ambition a little bit. You know, we really would, it would be very nice to have a complete characterization of like, you know, this SARS versus not sharp transitions in inference. And it's really kind of reminiscent of like this breakthrough work in the early 2000s by Frank Gutenberg. In the early 2000s, by Frigut and Bougain, that tried to characterize exactly in random graph properties of which transition is actually SARP or not. So it would be extremely nice if we're able to build such a cute criterion, at least in certain inference models, of when the MMS transition is happening. And with this idea, thank you very much.