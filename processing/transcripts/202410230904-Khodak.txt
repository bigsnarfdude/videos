And thank you all for coming. Today I will be talking about some work on scientific computing that I did with Shao at Georgia Tech and my former NAO advisor is Ina and Amit at CMU. So an emerging application of machine learning is in this area of scientific computing and especially in trying to do physical simulation much faster than can be done with classical methods. And the kind of target applications here are things that are currently running slow. Things that are currently running slowly, and we might want to, we have a lot of data for them, maybe, and we want to accelerate them using some type of neural networks. So, some extrude, maybe a highest profile application of this is real-time weather prediction. So, there's these classical approaches that can solve PDEs and do weather prediction, but you can also, you have a lot of weather data, we can try and use some type of fancy neural network to do much faster prediction with maybe less compute. Say something that takes Any less compute, say something that takes one GPU, which used to take a whole CPU kind of server system. Another popular thing to do is inverse solving. So if we want to simulate a PD at a variety of different kind of input conditions and geometries and see how well we can design a device, we can run a lot of classical simulations, gather a lot of data, and maybe train a neural network to try to predict. To predict, if we vary the parameters of, say, the fluid or the geometry, what kind of properties we get from the device, maybe maximize whatever we need to maximize. And this is in some ways bad because you have to run classical solvers to get the data. But then once you actually get a good neural model, maybe you can differentiate through it, but you can run it a lot faster using just forward passes and so on. And a last kind of thing you can do with neural networks that you couldn't do with classical solvers is if you. That you couldn't do with classical solvers is if you have these exponential scaling issues. So, for example, in many particle systems, the PDEs people usually solve for those are scale exponentially in the number of particles. We often have many particles in our systems. And so, people have been looking at how can you use things, actually in this case it was a diffusion model, to kind of substitute for this very difficult, otherwise, numerical simulation. And kind of a big default approach here has been to replace classical solvers, so something like OpenFoam, which will solve a partial differential equation for some fluid, just directly with the neural network. And this has some advantages, like it will, once you actually train this thing, you can actually do things very quickly with just forward passes, but it also has disadvantages. So you usually don't have correctness guarantees on your output, you can't really do high precision. You can't really do high-precision applications. So you can do things like epsilon negative 2 error, which is fine for maybe weather prediction, but you're not going to be able to drive things to things that you really need kind of linear convergence for, like epsilon 10 to the negative 8 error. And there's been very recent work this actually came out in September and made some noise, at least in scientific computing ML Twitter, which is that some person went to a really large amount of scientific machine learning. Of scientific machine learning kind of papers that were doing machine learning for fluid-related PDEs, and they found various things that they were just comparing to really weak classical baselines. So just using the wrong solver to compare to, and then once you actually use the right solver for the same problem they're running, it turns out that the improvements were not as big as plenty more. And so the approach I'm going to be talking about mostly today is a somewhat more integrationist approach, in which instead of just completely In which, instead of just completely replacing the numerical solver, we can try to use machine learning to integrate directly into the classical solvers. So we can still take in the data, but we'll also consider the original algorithm. And the hope here is that you might get some type of classical precision and guarantees, but also with data-driven acceleration. And this type of approach is very closely related to this area of theoretical computer science called learning augmented. Theoretical computer science called learning augmented algorithms or algorithms with predictions, which has itself had a lot of success in some other areas of computing, including database management, job scales alike, and energy systems. So the plan for the talk is to learn how to solve linear systems, which is a very important subroutine in scientific computing. The way we're going to use this is to use past linear system instances to configure classical solvers as a learning augmented algorithm. Classical solvers as learning augmented algorithms. Before I get to that, I'm going to do a little sidetrack into learning augmented algorithms to discuss whether we actually know how to do the learning in these types of systems. I use past instances to try to incorporate or learn something useful to run those algorithms with. And feel free to stop me at any time to ask any questions. I listened to Chai's advice and my slides are only 22 slides, so hopefully that won't go over. Yeah. Yeah. So the first question I'm going to pose is: can we actually learn to do this learning augment to use augmentation in algorithm? And the thing we want to do here is we want to use some past instances of running some algorithm to do something better in the future. And we want to do this theoretically in a couple of ways that kind of reflect classical considerations in learning theory and also theory. Considerations in learning theory and also theory of computing. So, we want to do this efficiently in terms of both the number of samples we want to use, so the number of past instances of running this algorithm that we see, and also in terms of the compute. So, can we actually learn this configuration that we're going to use efficiently? And we also want to do this configuration in a way that is going to adapt to the instances we see. So, we don't want to just learn a single fixed parameter for the entire distribution of instances we might want. The instances we Of instances we might want. The instances we see are going to vary over time, and we want to learn, say, a function that takes maybe some features about the instance and outputs a customized configuration of the algorithm for that instance. And there's been a lot of deep past work, so Ellen has done some amazing work on this, as well as many others, that have shown a lot of sample complexity bounds. But there have been kind of two areas where some of these bounds have fallen short in these two considerations. These two considerations. So you can do things efficiently in terms of the number of samples, but oftentimes a learning is going to be computationally inefficient because it's doing some type of ERM on a very not nice function. And the other thing we often don't get is instance-adaptive configuration. So we often don't know how to take features about our instance and output a configuration for our algorithm that's going to be adapted to the instance we care about. And the challenge here is that performance of the cost of running the algorithm. Like the cost of running the algorithm, say the number of iterations it takes, or memory, or whatever cost we're trying to optimize, is often a hard-to-optimize function of the parameters of the solver or the algorithm we want to run, and of the instance. And so the idea that has driven a lot of my work here, and will relate to the work on linear solvers as well, is this idea that we can usually approximate the cost we care about by some type of nice surrogate that can give us the Type of nice surrogate that can give us basically the same guarantees that we wanted in the first place, but is much easier to optimize. And so we have this kind of general purpose of the solution of use the feedback from the algorithms we're running and the instances to construct some type of surrogate function for the algorithms and forwards and optimize that instead using some type of off-the-shelf methods. And the idea here is basically the same one that we know from classical learning where we never optimize the 0, 1 function when we want to do classification. Function when we want to do classification because that's a nasty function to optimize. We always optimize some type of story on top, and it's basically the same idea, except now lifted to algorithm classifications. So I'm going to start with an extremely simple example from bipartite matching. And here, you don't need to understand basically the details of the algorithm, but as many of you probably know, you can do bipartite matching in the dual space by running a primal dual algorithm, and the duals that you find. And the duals that you find in this primal dual algorithm will correspond to kind of the optimal maxweight matching of your solution. And so, in NARF 2021, some people kind of came up with an algorithm that can use predictions about what the kind of optimal duals are for your bipartite matching. These predictions are kind of integers. And for an end-node graph, they can construct an algorithm that initializes using these duals and will run. Initializes using these duals, and we'll run at the time that's kind of n, number of nodes, times square root n, times the L1 distance between the predicted duals, w and the true kind of optimal duals of the matching for that instance, wt star. And t here is going to index the instance. No, or w? W is, for now, just given endogenously. So you get some w. It could be a function of features of the graph, could not be. It's just a vector. The graph could not be. It's just a vector that's the same number, it has n entries where n is the number of nodes. And they also came up, they gave a learning guarantee for now we have this objective that we might want to optimize, which is how fast we want to, or we know that the L1 distance kind of governs how fast we run this matching algorithm. And they came up with a sample complexity guarantee that said. With a sample complexity guarantee that says that if we run ERM on this thing, then we can use m cubed over epsilon squared samples to get an epsilon suboptimal solution. Now, as many of you might have noticed, this is like these are integers. So while this is a convex function, this fact that you kind of need the duals, and the original paper you really did need the duals to be integers, kind of makes it difficult to take advantage of this convexity. advantage of this convexity to do something much simpler than do this ERM on kind of a space, like an exponentially large space of integer value vectors. It turns out that you can do some fairly simple rounding schemes to relax this thing into now real duals in W to the Rn, and W in the reals, in the real space. You can get the same runtime guarantee from this, but now you can apply basically online gradient. Apply basically an online gradient descent plus online to dash conversion to get to do this efficiently and an actually better sample complexity. So n squared of right to plant squared. Although no, I'm not claiming that this original thing is tight or that this is tight. I mean this is clearly not tight. I'm guessing you can probably do better than ERM here, but it's much harder to do, apparently. When W goes from integers to reads, doesn't it increase the capacity and The capacity and required most ample size? Well, maybe. But we don't know. We don't know what the lower bound here is. Because it's convex, you can also get kind of easy extensions of, say, you want to have some features about the graph and you want to learn a linear function from those features to the predictor. Yes. Just to remind you of what is the best algorithm that doesn't use learning for microphones. The best, so that's a good question. In fact, the best-known algorithm, you sometimes Algorithm, you sometimes can do better than m times square n times kind of the worst case you get here. So, because the worst case here is going to be n, right, because it's at one distance. The reasoning given at least in this original paper for why we care about this is because, in practice, according to them, and I've had pushback on this, everybody runs Hungarian, which is kind of this kernel-dual algorithm, in practice. But definitely, I've gotten pushback on this from very. I've gotten pushback on this from various. Like, I think you go to OR and they say, we've worked on this for a long time and we can do better than this. And so, it's a good question. And I don't, yeah, I don't think that's the best you can do. Another fun aspect of our approach is that you can do the proof in Mult. Okay, so where else can we use this kind of surrogate approach? So, in the same paper, we looked at a bunch of different graph tasks, we looked at online. Different graph tasks. We looked at online algorithms, so things like job scheduling and page migration. You can also do this thing in differentially private statistics. So, if you want to incorporate some type of external information into releasing statistics, such as differentially prior quantiles, differentially prior data release, or covariance estimation. So, basically, like trying to improve the error you get from your statistic while maintaining the same privacy level, you can also do this learning there. So, we came up with, I guess, both the algorithms in this sequence of papers and also. Both the algorithms in the sequence of papers and also the learning guarantees for those algorithms. But now we can get back to linear systems, which is what I started off my talk with, and see whether something similar can be done there. So hopefully I don't have to convince you that linear systems, solving linear systems is an important task in computing. So here you have matrix A, which you can usually view to be very large and very sparse, and a target vector. Very sparse, and a target vector B, and you want to just find an X that is having AX equals D. And so, this is very important. I'm sure you've all used this at some point in your lives. The question is why we want to do learning here. And the answer is that many applications involve solving many of these linear systems. So, for example, often we want to solve sequences of related systems. So, for example, say we want to simulate heat diffusion, which is a partial differential equation. So, we want to start off with what is the heat on. Start off with what is the heat on some two-dimensional space and how it dissipates over time. And the way people do this simulation is actually at each step of this visualization, what you're going to be doing is solving a linear system to kind of get the next x. So x here is going to be what is the state of your heat over your 2D space, and A and B are going to be kind of derived from, say, finite difference approximations to your 2D. To your PD, and then you're going to solve ax equals to b, you're going to get your next x, and continue. And the runtime of certainly heat simulation and many other cases is really dominated by time it takes you to solve your linear system. So here, your u is your state of the system, right? The heat equation. Yeah. So those x's do not correspond, right? The x in the x. The X in the linear system. Yeah, the X is a discretization. These X's in the linear system are U's, basically. Yes, they're yeah, discretization of U's, yes. And there's many other applications in which you want to solve sequences of linear systems. So, for example, if you run a Levinberg-Markhardt to solve a non-linear system, basically internally, what you're going to be doing is solving linear systems in there. You also have In there. You also have applications in computer graphics where you say you want to smooth some object, and so on. And the solvers that, the classical solvers that people use for this, have parameters that really strongly affect their performance. So the question that we're going to be addressing in this talk is whether we can learn those parameters. So the basic goal that we looked at is whether we can use feedback from solving the past linear system instances we saw. So we see instances A1x equals B1. A1x equals V1 through ATX equals VT to get set a better solver parameter at step t plus 1. So, more formally, can we online learn to set a sequence of parameters in some parameters we still define later? To set the parameters in some solver for our linear systems, such that the average cost we occur over time approaches the cost of using the optimal parameter in this parameter space. Parameter in this parameter space plus a term that goes to zero as t goes to infinity. So you guys probably recognize this as just an expression for regret. And the cost here is going to be the number of iterations that our classical solver will take. And we looked at a very specific solver called successive over relaxation or SOR. And this solver is a linear solver, so these days it's used mostly as a preconditioner or a smoother in modern solvers, as I just preloved methods. Solvers, just pry log methods, or multi-group. But it has kind of a very well-established theory. And the cost of running the solver depends very strongly on a relaxation parameter, which is basically just a scalar between 1 and 2. And so, for example, if you get the right scalar here, you can run this solver in, say, 40 iterations instead of, say, 150. The challenge of The challenge, of course, is that we have this function, we kind of sort of understand, even from classical theory, how this function might behave. But there are kind of two issues. One is that to actually know exactly what the optimal thing to do is for any given matrix, you have to eigen decompose the matrix, which is harder than actually solving a linear system. And the other issue is that these functions can be not fun to optimize. So, for example, the function I plotted here is the cost function for a digital. Here is the cost function for a degenerate B. So basically, I picked a specific target vector b, which was specifically targeted to make omega 1.4 have this extremely sharp drop at 1.4. And this is kind of a classical issue in bandit tasks where it's very hard to pick this needle out of a haystack of plus 26. But I could have done this for many other omegas, and so then you have this issue of where, in hindsight, what is the alpha omega that you use? But if you talk to any scientific computing expert in practice, what they'll tell you is that the kind of solver the performance of your solver, so the number of iterations that something SOR will take, depends mostly on the spectrum of the matrix A and not really on any properties of B. So here I've kind of picked a degenerate B, but this doesn't reflect anything that happens in practice. In practice, the solver, like for practical B's, will depend basically on just what are the properties of A. Depend basically on just what are the properties of A, how well are the kind of the eigenvalues of A clustered, and so on. And so, our approach will be to model typical systems with a distributional assumption on BT while making no additional assumptions on AT. The distributional assumption is kind of reasonable. It's basically just a Gaussian. And what we're going to show is that in expectation over this b, you get a much nicer function. You get a much nicer function or a much nicer dependence of the function you're trying to optimize on your omega. And this does reflect what happens in practice, because again, in practice, the performance of your solver depends on the spectral properties of A and not really on the target vector B. Corresponds to what happening in practice? I wouldn't say it corresponds to what happens in practice. In practice, your B's are never stochastic. I think it's more of a kind of a reasonable assumption. Of a kind of a reasonable assumption in the sense that B usually doesn't affect your target performance unless you really pick it to be nasty. So it's like an anti-degeneracy assumption. But in practice, no, we don't usually sample these from a stochastic distribution. Yeah. And I'll go through a light amount of the proof here to show that we can actually use this distributional assumption to show that the expected cost is going to be lifted in this parameter. Going to be lifts in this parameter omega, and that will allow us to apply classical bandit tools. So, kind of the key lemma here is that we want to show that the expected cost in terms of sampling B is going to be Lipschitz continuous in omega with a Lipschitz constant that depends on N, and also the precision we want. So, how close we want the norm difference we want for Ax to be different than B when we solve our linear system. And the key idea here is to. And the key idea here is to look at the termination criterion. So our solver is going to terminate once it splits out an x at a variation k. That is going such that the a times xk minus b, the norm of that, is less than epsilon. And what the randomness of b gets us is two things, or one thing really. It gets us anti-concentration, which is that the probability that this quantity lands close to epsilon is small. Is small. So the probability that lands very close to some to the actual threshold that specifies our termination is quite small. And then if you combine this with... Is there some smoothing? It's basically some smoothing, yes. And then the probability that, or and then we can combine this anti-concentration result with just showing basically Lipschitzness of the criterion itself to show that. Criterion itself to show that it's going to land far enough away from the threshold such that if you change this parameter omega prime slightly, or omega to omega prime, the probability or the likelihood that it's going to change it enough to go back over the threshold or to go under the threshold is going to be small. And this basically allows us to show that the expectation is reasonably smooth. So basically a small perturbation to omega is unlikely to change radar. To omega is unlikely to change greater. And the importance of this is that we can, with continuity, we can do less discretization and get better average cost as we run, say, a bandit algorithm on this task. And it also is going to enable us to learn an instance-dependent policy that'll get into the end of method. And so, the theorem that this lip shifts allows us to show, we take this lip shiftness, we plug in your favorite bandit algorithm, and Vanda algorithm, and you can basically get this regret. The regret here is going to be t to the two-thirds, which is kind of the classic rate for Lichitz Bethlehem. Now, as I mentioned at the beginning of the talk, we really often care about not just doing as well as the best fixed omega in some parameter space, but trying to do instance adaptive learning. So, try to pick a good omega for the actual matrix we see in practice. So, can we actually do that in this setup? And the thing we look at is kind of a structural assumption here, where we consider a sequence of linear systems whose matrices follow a simple structure where there is some fixed global matrix A, and all the other matrices are scalar offsets of that A. So there's some CT times the identity where you just add it to A, and that's how you get your AT at every time step. So this is a kind of simple structural assumption, but it arises in. But it arises in useful cases. So, for example, if you're solving a heap equation with a time-varying diffusion coefficient, then the matrices that result from that that you need to solve over time are going to have this form. And our approach to solving this question is to reduce this contextual bandit problem. So, the context here is going to be CET, to online regression. So, this is kind of a result of a sequence of work. I guess it was started by. Sequence of work. I was started by Foster and Arklin in ICML 2020, but there's been a variety of work that can reduce contextual bandits to just online regression. And you can then show that this cause is not only Lipschitz in omega, but it's also Lipschitz in the context C, and then basically do this online regression by fitting kind of a Chevyshev series to this expectation. So trying to predict the expectation from or trying. The expectation from, or trying to predict the cost from CT. And so, what this gets you is that as you see more and more instances of your linear systems, over time, the cost, so the average cost you incur, is going to approach the cost of the instance optimal omega. So, the best omega you can use in your algorithms, in your SOR algorithm that we're looking at. So, over time, when this kind of instance optimal omega, it's going to be omega. This kind of instance half Olnega, it's going to be Dolnegar just type. And also, I guess I want to highlight this type of thing you can actually just deploy in practice. So this is wall clock improvement of using SOR actually to precondition the conjugate gradient, where if you use kind of a default decent choice of omega, you can do like twice as bad as kind of the vet competing with the vexed learned omega. And then if you do this type of regression-based approach, you can. Type of regression-based approach, you can learn and adapt evolved. You can do basically three times better than your digital approach. And here, there's no pre-training, there's no nothing. I'm just showing you pro-Walklock improvements, just starting from a heat simulation, basically. Yes? Can you give me a second? What term is you're approximating with this temperature? The term you're approximating is the expectation of your cost, yeah, the expected cost at time t. So for your specific matrix T, which depends on CT. Which depends on Ct, you're trying to basically learn a Chebyshev series of, as a function of Ct that's going to approximate your expectation in squared month or a squared error. Okay, and that basically wraps up my talk. What we've done so far is studied kind of a simple but practical way you might integrate machine learning into scientific computing, and we showed this kind of strong guarantee under strong assumptions, but I think in terms of like domain-specific. Assumptions, but I think in terms of like domain specificity, these assumptions make sense in the sense that these sets of structural assumptions that you can make arise in practice in actual simulations you might run or kind of reflect reality in terms of solver performance. And overall, I think looking at kind of science and algorithms as machine learning theories is kind of an exciting area because you can find these types of interesting structures that maybe you can adapt your algorithms to and learn how to adapt your algorithms to those structures. And then you have these opportunities to integrate provable numerical. You have these opportunities to integrate provable numerical techniques into or integrate ml into these actual numerical techniques that we have guarantees for. And lastly, there's a lot of unsolved problems, I think, at this intersection that are worth it. So thank you. Can you say more about the experiment in the computer? So what was like the data set, I guess? And like did it was it like how this non-degeneracy assumption sort of there this was not so this data set that B's were not sampled from that distribution these B's resulted from the this was this was basically solving a heat equation So in two dimensions and the A's and B's were the A's did follow this kind of A plus C T structure but the B's A CT structure, but the B's resulted from the simulation itself and not from they weren't sample brands. Okay. And then a second question about that is: I guess, like, as we probably know, sometimes to get over these non-degeneracy assumptions, we would make this assumption that the bees are just sampled from like this, like a nice distribute, not, I mean, obviously, not in this context, but like, sure, yeah, and a nice distribution. Sure, yeah. Yeah, and a nice distribution of like, yeah, that don't have point masses. And could that be used as a relaxation of this, or do you need nice caps? That's a good question. So I definitely was thinking of this version when I, especially when you started looking at like the probabilities of shifting over the termination criteria, it's kind of very relevant. I wasn't sure if it would be possible to extend to say bounded density distributions. say bounded density distributions because you kind of needed you did need randomness to go in pretty many directions. Because SOR is basically an iteration, it's an iterative procedure where you just keep throwing a single matrix at it. So you keep powering this matrix and eventually you just get one eigenvalue that's really high and a bunch that are really low and then you you can't afford it to be low in any direction. And so you needed some type of isotropy. So I think it might have been difficult to apply. So I think it might have been difficult to play that from Naive, but can you talk a little bit about how SOROX and depends on all of that? Is it just worth the inclusion of this one? And then my second question is: so sometimes for systems where you're sort of updating A like this, you would do Is there any relationship between those methods or like kind of classical ways that things used to work? So the way SOR works is, indeed, I don't know if I would call it factorization necessarily, but you basically take your matrix, it's a sparse matrix, you take its diagonal, upper triangular, lower triangular component, you multiply some of those components by omega in the right way, kind of, and you construct a kind of and you construct an upper triangular matrix that you can then invert quickly because it's upper triangular and then you just you keep applying it to B you keep applying it to D by that kind of what's called an iteration matrix and that's why you get this kind of power ring where you just take a matrix and you just keep applying its inverse to D but you can do that inverse quickly because it's triangular and that matrix depends on omega your second question was about I guess symbolic I guess I haven't heard of this approach so I guess Approach, so I guess I even put it into a lot of e-faces. I'm trying to just wonder if there's any I see. No, there's nothing like that going on here. I didn't realize, I guess, you can use Trileski for different sparsity structures. I know you can do it if you have like one matrix and you want to use the same thing. But the different sparsity structure is something I would have to look into. Or sorry, same sparsity structure. Yeah. But interior point solvers are probably the next part. I see, I see, yeah. Certainly for like this assumption with A plus C T, your sparsity structure kind of stays the same. So something like that will definitely work there, and I should look into that. I guess in general, our results hold for the kind of results that don't have that assumption hold for kind of broadcast submatrices that can change. I only know that virtual recipe, which is not working with us. Sure, yeah, but it's still kind of. Sure, yeah, but it's still kind of a baseline that you could consider applying. So it's a great point, yeah. Thanks. Hey, very good. Thank you. Thank you. Make sure that we do it even. We have to turn it off and over.