Shall we go? I think we can continue, right? Anyway, the next speaker is Tim Roy from the Technical University of Berlin. The talk is going to be about a Bregman learning framework for sparse neural networks. Should use this? I mean, okay. Yeah, thanks for inviting me here. I'm actually not from Berlin. I'm from doesn't matter. Uh, doesn't matter. Um, yes, so this talk is mainly going to be an optimization talk. Um, I try to stretch myself a little bit to find the connection to medical imaging in the end. First, I want to mention the collaborators on this project. It's Leon Bungert from the TU Berlin, Daniel Tembrink from FAU, and Martin Bugeer, who's now at DAISY and University of Hamburg, which I'm actually currently a PhD student of. Which I'm actually currently a PhD student of. Okay, so what's the plan? I'll try to be very strict on time because we have a photo shoot. First, introducing the algorithm that we're using called BRECN iterations, also known as mirror descent. Then I'm motivating or the main motivation was training sparse neural networks. I'm going to show how to do this. Then the convergence analysis. And here, then a little bit I tried to find. Here, then a little bit I try to find the connection to medical imaging, which is a bit funny and a bit made up, but we will see. Okay, so first of all, what are Breckman iterations? This algorithm is, I don't know, usually when I talk about it, it's not so well known. That's why I wanted to motivate it. Our basic task is we have a loss function L here from some space theta to R, and we want to minimize it. But additionally, we have given some regularization function of J, which we want to minimize. Regularization function of j, which we want to minimize. And I mean, the basic approach, very simple here, this regularization approach where you just add these two functions together with some regularizing parameter here. Very incomplete list, of course. This is like the most classical thing you can do. But instead of considering this problem in the context of Breckner iterations, we would rather consider something like this. Would rather consider something like this. This has also been in the talk of Giovanni, actually, where, let's say your loss function is some kind of linear operator, and then you have some, let's say, L2 norm here on the outside. And then you rather want to find the minimal element with respect to your J functional, which could be, for example, the L1 norm, sparsity promoting, under all vectors or parameters that fulfill data losses. That fulfill that the loss is equal to zero. This is the classical pursuit that we're considering here. And what is the approach? It's some iterative scheme. It's some, yeah, minimizing, I like to interpret it as a minimizing movement scheme. So in each step, we say the following: we want to minimize our loss function here with some time-step parameter tor, but we only allow ourselves a certain distance from the last iterate, right? This is. Last iterate, right? This is implicit EULA or minimizing movement. And now the question comes in: how could we incorporate information of our functional j? And because the idea is now in each step, we want to only go a little distance with respect to our functional j. And how can you do that? Well, the basic idea is to replace the L2 norm here with the Breckman distance of the Of the current or of the iterate that you want to obtain to your previous iterate. Okay, what is the Bregman distance? It's basically defined here. We have some functional j and we assume it only to be convex. And then we have these two values and two parameters that we want to compare. And we basically what we do if we say that this. If we say that this j would be differentiable, then we kind of measure the distance of the functional j with respect to its linearization around some point. Okay, we say this is our theta here. We linearize around theta. This is with the gradient here. It's basically a first-order Taylor expansion. And then we measure the distance of J theta bar, this is now the point you want to compare to, with the linearization. To with the linearization. This is the picture you can keep in mind. If this green thing would be our actual functional, and this blue point is theta, and this is the point we want to compare it to, then the pink line or this pink thing here is what the Bregman distance measures. It measures the distance between the linearization and the actual functional. Okay. Right. And to come back, this is now the idea that we use. Idea that we use this distance here to replace the L2 distance in this minimizing movement scheme. Some notices, some remarks on the Bregman distance. It's not symmetric, as you can obviously see. So it's not really a distance, but it at least is positive and fulfills some kind of triangle inequality. And the most important example is if your J. If your j is L2 norm squared, then you actually recover the normal L2 distance that you have. This is like the most important thing, and we're going to use this a lot in the following. So the brackman iteration then looks as follows. We have here this minimization step from the minimizing movement scheme. And of course, I noticed this before, you always have to have the sub-gradient at which you want to evaluate your. At which you want to evaluate your Breckman distance. If it's differentiable, that's not a problem, but we're in the context of only convex, not differentiable. So you always have to find the subgradient here. And this is basically done in the second step where you update iteratively also your subgradient. Yeah, which is the following. This just ensures that pk plus one is also a subgradient of theta k plus one. Okay, this is the Breckman iteration. Breckman iteration, some intuition, how it looks like. You have monotonic decrease of the iterates in your loss. So it actually does the thing that we want. It decreases the loss. The kind of particular property of Breckman iteration is that you converge to your noisy data in the limit, which means that this is the basis pursuit that I wrote down. We actually converge. Pursuit that I wrote down, we actually converge to a minimum of our loss function here, unregularized loss function. So, if you think about image denoising, I will show an example. What will happen is in the limit, you will converge again to the noisy image, right? This is something you maybe don't want to have, and that's what I written down here. You can only approximate it, your true solution up to some noise level, so that's why you usually employ. Noise level, so that's why you usually employ early stopping. You really have to stop earlier if you want some regularization effect on your image. Otherwise, you will always converge again to the noisy image. Right, this is classical theory from OSHA 2005. And what I also wanted to mention, because I introduced with this minimizing movement scheme, you always think about gradient flows. What do you obtain if you send your time step? If you send your time step parameter to zero, you obtain this so-called inverse k-space flow, which looks like the following. You have here, this time derivative, and you always have to ensure that your time derivative is then in the sub-differential of j. And again, here, if now j would be the Euclidean norm squared, then the sub-differential is the identity, and we actually obtained a normal gradient flow. So it's a proper generalization. Why is it called inverse scale? Why is it called inverse space flow? That's why I have a short example here, and I took this picture before I left the city, which is probably the most influential for this workshop here, Genoa. And I don't know if you can see it. I can see it very well here. This is a noisy image here. And what I'm doing is I just do some TV regularization. And I want to start in this inverse. To start in this inverse flavor with something which has a very low TV norm, so to say, and the best thing is actually a TV norm zero. So I start with something that is a completely constant image. So how do the iterations then look? After five iterations, I introduce some features. And now they, if I think in TV, they kind of introduce features on a larger scale. So I started on the largest scale, which is the whole concept. The largest scale, which is the whole constant image, and then I do a little bit of iterations and I start to introduce more smaller scale features. Then I do this for some iterations. And I don't know where I stopped. Maybe k equals 200. It's okay. It's not the best denoising. What you can do here, of course, it's just a motivating example. This is somehow TV denoised version of this. And if I do it. Version of this. And if I do it, if I would do it to k to infinity, then I would recover this noisy image again. Okay. So this is the basic behavior of Breckman iterations. Of course, this step with this arg minimization here of the loss function can be very expensive. So what do you do? Well, you linearize. You just linearize your loss, which means you also do a Taylor approximation around the last iterate. You know that this linear. You know that this linearization only works well if you know that your iterate is in a is close to your previous iterate in some L2 distance. So that's why you just add it as an extra regularizer. So you add this L2 distance to this problem here. And this just means that you replace your J by an elastic net regularization here. This is the steps that you have to do. So in order to linearize, Do. So, in order to linearize, you linearize your loss functions, your loss function, and then to keep kind of close to your last iterate, you add this L2 term here. This means you replace J by the elastic net functional here. And this then gives linearized Bracknell iterations. I rewrote them a little bit now. I don't want to do the full computation, but basically, how it looks like here now, you have this is again your subgradient update. There didn't change. Update, there didn't change much, but your update for your prime, for your main variable, theta here, is now just a proximal operator on J, which in the cases that I want to consider, L1 norm, can be evaluated very easily. Okay, and the connection that I always want to highlight to a more well-known algorithm, namely mirror descent, is that. Descent is that it's basically the same under certain assumptions. Mirror descent is very well known in machine learning applications. And basically, what you have here, you assume a strongly convex and differentiable function h, and then you do the following update. You have here the gradient applied to theta. Then you do this step, which is basically the subgradient update here. And then the gradient here, the grain of H is referred to as mirror. The grain of H is referred to as mirror map, and then you go back to your actual space with the gradient of your convex conjugate. And what you can show very easily in like a few lines of calculations is that if your J, if our J in this context is differentiable, then the Merdescent update is applied to J delta, to the elastic net regularization, is MERDESENT. But this is mostly the takeaway when I But this is mostly the takeaway when I give this talk for machine learning people. If you want to do mirror descent, but you are worried that maybe you cannot define a mirror map via some differentiable function, you can just instead use linearized pregnancy iterations. Works very well. It's just the same. Okay. So much for the introduction on that. Now I want to come to the application, what this project was, namely training sparse neural networks. What is the idea? This is kind of relates to this compressibility that Giovanni talked about just in another setting. Namely, we have some dense neural network with a lot of weights here. And instead, we want to obtain a sparse neural network, which means we only have certain weights active. Right? You can represent these weights in a weight matrix, and then you can just think about the sparsity of your weight matrix. About sparsity of your weight matrix. That's the basic setting. And why do you want to do that? Well, the major point is that they have lower computational complexity, better to evaluate also on smaller devices, maybe. There's the second point, which is a bit of a bit slippery, namely that sparse neural networks are less likely to overfit the training data. This per se is true, but if you want them to infer that they also generalize better, so this is. Generalize better. So, this is, yeah. No, I don't want to talk about this too much, but I would say the main good thing is that they have a lower computational complexity. Okay, so how do you obtain sparse neural networks? There's a rich theory on that. There's either the classical thing, which is called pruning. One of the first papers there was called optimal brain damage to think that you have a brain and you cut away connections. This is called Connections. This is called pruning, but then you can also just do L1 regularization, of course. I mean, you have a weight matrix, you put the L1 on there, then you be sparse. This is the thing that you already have a dense neural network, and then you make it sparse. We are more in the regime of sparse to sparse training, which means we are never really dense. We want to be sparse throughout the whole training process. And there's been some heuristics called evolutionary training. Called evolutionary training, and we now want to do it with Breckner iterations, right? If you think about the nice picture of Genoa, you start now not with the TV norm, but with the L1 norm. You start with basically something like zero weights. This doesn't work. I will talk about this later, but something like that, and only add useful connections throughout your iterations. This is the idea. This has been done for machine learning, for example, by Martin Benning for nuclear norm stuff, but for the L1. Nuclear norm stuff, but for the L1 norm with Breckman iterations, we are somehow same. We did this. This is the idea that I already said. And yeah, just a quick recap over the neural network setting. So we are giving some final training data here, and we want to train a feed-forward neural network, very basic, which just means that each of these, we have a layer structure, and each of these layers is. Structure and each of these layers is parameterized by we have some activation function, a weight matrix here, and some bias vector. There might be some residual connection. I didn't wrote this down here, but that's the situation. Very basic. Our parameters then are here, weights and biases. And now the functional j you could think of just as the L1 norm of all the weights. You can do more fancy stuff, maybe in the More fancy stuff, maybe in the results I can talk about this. So, either you can just do each of the weight entries separately with L1 norm, or you do some group L1 norm. For example, if you have a convolution on your network, you can try to regularize filters of convolution on your networks. Right. Now, the more interesting thing about the loss function is that you usually have the empirical risk, which is Empirical risk, which is looks like that. You just sum up over all your training data with some small loss function here. There should probably be a normalization factor for the analysis that I'm going to do. But of course, you cannot evaluate the full gradient, or actually, you don't want to evaluate the full gradient. This is the two things. First of all, it's computational, not feasible to do it, but also if you do. But also, if you do these mini-batch updates that I've written here, it's probably also better from a stochastic point of view. So, that's also why we use this mini-batch update, which basically just means instead of your whole training data in each step, you use some small portion of your data and compute the gradient of this thing then. This is then basically yields something like stochastic gradient. Basically, yields something like stochastic gradient, stochastic bracketerations, which is the equivalent to stochastic gradient descent. This is interesting from an analysis point of view because the existing analysis only works in a deterministic setting. Now we are in the setting where you have this g as a gradient estimator, which is the setting of normal gradient descent, which just means you have some function which gives you an estimator of your gradient. Gradient. Could, for example, be a batch gradient, as I showed before, could be something else. This is now what I'm going to do from the analysis side. Right? We have to have some assumptions on a gradient estimate that the first thing is unbiasedness, just means that the expected value of a gradient is actually the gradient. Say, this is not so bad. This is very typical. The next thing is that we assume bounded variance. This is already a little. Bounded variance. This is already a little bit more restrictive. I want to mention that there's a more restrictive assumption used in literature sometimes, namely that you do not have this, but you have only that the gradient, the norm of the gradient squared is bounded. And there's also another, there's a paper which shows that this together with this cannot work together with a convexity assumption. Convexity assumption, but this here is a little bit weaker. So I just wanted to mention that there are even weaker assumptions where you just have some affine bound here, but we use bounded variance in our analysis. I would say compared to the convexity assumptions that I'm going to introduce next, this is still mild. What do we have to assume on the loss function? First of all, Lipschitz continues. All Lipschitz continuity of the gradient. This is basically written down here, just in a fancy form. I mean, I could have just written down Lipschitz continuity, but this is the actual estimate that you used. And in the analysis, that's why I've written down this. This is okay. I would still say this is okay. And using only Lipschitz continuity of the grain of your loss function, you can. Of the grain of your loss function, you can already show convergence, not convergence, you can show loss decay. But this is written down here. There's a lot of extra terms, but basically you have the loss at iterate k plus one, and here you have the loss at iterate k. And which is what is really nice, we have theta sigma here, which comes from the bounded gradient assumption. And actually, if you assume that your gradient is a perfect estimator, Gradient is a perfect estimator, so we have zero here, then this recovers standard results from literature for non-convex Regno iterations. So this result is pretty nice, I would say. Assumptions are somewhat mild. If we now want to have convergence of the iterates, now it's going to be from a practical side, it's going to be dodgy because we need to assume strong convexity. And of course, as you all know, And of course, as you all know, this is just not fulfilled in the machine learning context. But it's also not so easy to show the convergence of Bregman iterates without some sort of convexity assumption, because mostly you have a coupling of stochastic dependent variables that you would not have, for example, for stochastic ray and decent. For stochastic ray and decent, you do not need to assume this at all. You don't need convexity. You don't need convexity, you don't need even this bounded variance assumption. I'm not saying that our analysis is sharp, but at least the way we did it now, we need to assume it. So please don't beat me up about it. I know that it's not too realistic, but it's a nice assumption to do analysis with. Basically, what we need to assume is that we need to have convexity, and here the Breckman distance appears again, because if we want to have Want to have convergence of the iterates in the L2 norm, then this here is also the L2 difference, so we need convexity in L2. If we want to have convergence of the iterates in the Bregman distance, then we have the Bregman distance of our regularizing functional here. So you always have to have some lower bound with respect to the norm or the distance that you want to show convergence in. Okay, and basically, if you assume this, everything is nice. If you assume this, everything is nice, we can show convergence, and we have a standard convergence rate. This is the result in the paper. There is a lot of work, of course, on stochastic mirror descent and this types of stuff, who also work on the mild assumptions, but they always assume differentiability of this functional j. And we just don't want to do this because we want to work with the L1 norm. So yeah, this is the convergence. The convergence result that we have. The wish list is, of course, replaced the convexity assumption, something like Kodika Yoroshevitz. This is something you could look into. Martin Benning has done something with this. So this could work. And the next thing is maybe you could weaken the bounded variance assumption. This is just a wish list, what we can do, what we would like to do. Okay, now I have some little time left. Just want to show you some numerical examples. Very easy. Numerical examples, very easy training a multi-layer perceptron on MNIST data set. This is not interesting. This is the validation accuracy, everything's the same. The interesting thing is what happens here because now we initialize a network close to zero. You can't initialize it really as zero because then you have some, I don't know, it's always called symmetry where you have some, which you can't break out of the neural network. Basically, if you train it in a standard way and initialize. You train it in a standard way and initialize with zero, it doesn't work. So we initialize it close to zero. And what I compared here in black is what stochastic gradient descent would do with the sparsity here. So of course, SGD would destroy the sparsity in one step. This is what you see here. The yellow line is what proximal gradient descent would do. Proximal gradient descent doesn't really want to start sparse. So what it does also, it shoots up a little bit and then comes from above. And what the Bracknell iteration actually does now in green is Iteration actually does now in green is it if you remember this minimizing movement scheme, it just does little steps in the L1 norm in each iteration. And from a computational side, this is nice because then already during your iterations, your net is rather sparse. So the forward pass has a lower complexity. Computing the sparse gradient is another topic I'm not going to talk about. This is more difficult. Okay, you can do the same with convolutional filters with a group norm. So what I do here is So, what I do here is I have L1 norm on whole convolutional filters, which I want to regularize away. So, I start with very few active convolutional filters, and then during the iterations, some of them pop up that are relevant to the network, and they also get trained simultaneously. This is kind of the effect that it has. Yeah, this is just some numbers. Basically, it works the same as proximal gradient descent. Basically, it works the same as proximal gradient descent. It's not so interesting. What I want to talk about in the last two minutes that I have is now neural architecture search. Namely, what we noticed is if you have like a big superstructure of a net and you do like a L1 regularization, this also optimizes the architecture of your network that you have. And we just use this on a very simple denoising, deblurring task, so also toy examples. But what was really nice is that if we do this with the Brechm iterations, and we have a simple multi-layer perceptron, I'm going to explain this graphic now. So we have a multi-layer perceptron when each layer we allow ourselves around 800 neurons. And we initialize with like this very white thing here, namely around 1% of neurons are active. And here, what we do, we And here, what we do, we regularize over whole lines of the weight matrix so that we can actually activate or deactivate neurons. This is also kind of group sparsity, what you can do. And then if you use the same algorithm, then you converge to something like this dark bluish line. And this is the auto-encoder structure, which was really nice to see that using this Breckenridge iterations with L1 sparsity, you can automatically. You can automatically kind of obtain this auto-encoder structure. I tried the same with just proximal gradient D's, and it does not work so nice. You get something else. So, this seems to be really an effect of this small steps in the L1 norm. And so the question, of course, would be, can we do something similar for a unit, which is now the connection to medical imaging? And the answer is that we had some ideas, and I neither And I neither have time in my talk, and I also didn't have time to try it out. But this would be the next project to kind of use these insights from this auto-encode example to train something more interesting like a unit. So yeah, that's everything. And we have code online, and this is the two papers. Thank you very much. Any comments, questions? I would be interested practically what's the difference of your brackman iterations to adding the L1 as a regularizer and maybe have a varying weight, first a strong weight and then gradually decreasing it. Do you think it makes a big difference in practice? Conceptually, of course, I know the difference, but do you mean like actually doing it with proximal gradient descent or? It with proximal gradient descent or with just different so the difference to just differentiating it is that you have true zeros. So what people do in practice, if you just add L1 norm on your loss function and do autograd on it, then you don't have true zeros. You don't have true sparsity. You could have, of course, threshold it in every step, but the nice property of doing it with like the proximal operation. The proximal operator that we have true zeros, and we can actually exploit this in the forward path. Yeah, I mean, using the proximal operator, of course. I mean, okay, with the proximal operator, this is the example that I showed here. The main difference is that the proximal grade decent comes from above. So if you kind of integrate over this line, this gives you your complexity. And then with the Bregman iterations, you're lower. You have a lower evaluation of weights. But from the But from the actual result, this was this that I skipped. It's basically the same. What I mean is that in the Breckman iteration, the weight of the regularization of the sparsity is automatically adjusted, right? The weight, you mean the parameter of the... The weight of the sparsity, how strong you want to impose sparsity. All right, that's a good point. So actually, from the theoretical result that I showed, it should not matter because for the BRECM iterations. For the BRECM iterations, but as we are in this non-convex setting, you still have to tune it a little bit. The parameter, this is not so nice that it actually matters. Yeah, that's a good point. But in a theory, in a convex setting, it does not matter your parameter because you have this unbiased property, but here it kind of does. Yeah, I meant something slightly different. I meant in the brackman iteration, when you start with a very sparse solution, then it With a very sparse solution, then it becomes less sparse all the time, right? So it's like first having a strong sparsity prior and then reducing the sparsity prior. My question is: if you would do this with a standard sparsity prior and do a proximal gradient descent and have a schedule on how strong they are, how similar the results would be. Right. Okay, this is, I don't know. This is a good trial. Okay. Okay. Maybe we can chat. Yeah, yeah. Okay, thank you. Thank you for the interesting talk. I have a somewhat similar question. So usually, if you want to have a sparse network and you do pruning, you start out with a dense network that you trained and then you do sparsity. And here you're basically taking the opposite approach. And the first training densely and then Training densely and then making it sparse is maybe one could argue more like the human brain works because you have lots of connections and then they get pruned away over time. Do you have any thoughts on that? Because, I mean, those are two very interesting, valid approaches, but they seem to be, yeah. I mean, I'm not an expert on the biological mechanism, but there's this thing, um, five. But there's this thing fire together, wire together from a, and this is more like if you have neurons that kind of activate a lot together, then they form new connections. So from this point of view, this is actually more natural, right? I mean, if you think in terms of pruning that you have, you have a big network and then you throw away unnecessary connections, this is something else. But what this actually is, if you learn, so you don't know anything at first, and then you learn. You don't know anything at first, and then you learn new things. I think, from a okay, I'm really not an expert on the neuroscience, but I think that it's also not so unnatural to assume that new connections form during a learning process. I think, but this is very vague. Yeah, it's yeah, I think it's very interesting. I think no one really understands it, and it's really cool to see both directions. Thank you. Thank you. Okay, uh, you want also, yeah, okay. So, what activation function did you use for your multilayer perceptron? Because you've assumed smoothness here, right? So, did you actually use a smooth one or did you use ReLU? Uh, well, we used ReLU. So, no, actually, no, we have one experiment where we use JLU, the smooth approximation with Gaussian distribution function. It kind of works the same, but from a practical point of view, it doesn't matter so much. I don't know. And then we need not discuss strong convexity here, right? Then with the Releue and whatever in there, right? So there's a gap between the theory and the I mean this is definitely so the convergence of the iterates is definitely not applicable to this neural network setting. This is more from an abstract right setting. Abstract setting. Yeah. Then could you show this auto encode result again? I think, you know, I think of an MLP as having kind of one-dimensional. I realize you're applying it two-dimensional images. Could you kind of explain? You got two axes here. So what do you mean by one row? Right. So these are the layers. Yeah. Right. Okay. I understand. And then with each layer, you have rectangles, right? So what is going on? This rectangles is just, this doesn't matter. This is just to show it. This is this basically just shows the number of neurons active in. The number of neurons active in each layer. I could have made it just a line. I don't know. I used a block. So, this is the number of neurons active in each weight matrix. And how we did it is we take a whole row or column of your weight matrix, put a L2 norm on this. And then over all the rows that you have, you put the L1 norm. Ah, okay. So this means that if one of your instance in this group thing is now zero, that means that the whole row is zero, which means that a Is zero, which means that a neuron is not active. Okay, so it doesn't you would never do that on the input layer, so that's why you have those. Exactly, I cannot do this on the input layer. You can do it with features once you've got okay, that helps me understand. Thanks very much. Okay, yeah, so it's a very nice talk. So it's related to the question that he should ask. So, but it's a little bit different. So, when you start with, like, for example, for the first With, like, for example, for the first layer, right? Right? Yeah, so do you start with all of them from in the iteration zero, all of them with a lot, with a little number of neurons actually active? Yes, together? Yes. At the first iteration, I see, I see. Yes, what we don't do here, we do not do something smart that we assure that there's a path, a forward path or something. We really just initialize the weights. The weights, they are scaled differently to so that the weight is scaled approximately to the sparsity parameter, and then we just do a Bernoulli mask. Basically, say this amount of parameters gets randomly thrown away. Yeah, so what I don't understand when you make a comment that, you know, because if your forward pass is already spar and little like this, right, then the back propagation should be spar in this manner as well. This manner, as well. What your comments said, you know, it's not clear how to do sparse gradients, it is not so clear because even if your forward pass is sparse, you might always get information if you do the back propagation. Even if you don't self are zero, you might get some information in the gradient and the backward path. Actually, that's where I'm not clear because if you have a certain connection to from the first layer to the end, right, when you do back propagation, it's only. The end, right? When you do back propagation, it's only go through the same thing. That's right. Um, so you said that even the forward pass, some connection is not there, but from the backward, there some of them actually non-zero. Yes. It's not clear to me. Okay. I can talk to you later. Okay. Okay. One more thing. Can you go to the results for the CIFAR? Yes, this one. Yes. Yeah. So I think you're Yes, yeah, yeah. So, I think your method there is actually not accurate compared to the other one, right? Especially testing accuracy. Well, so basically, this is vanilla grade decent this has no sparsity and you drop some accuracy. I know, I know, yeah. So, my point is that, you know, even get a little more accuracy is actually a big win in the community, right? So, so you're sparser, but you lose a lot of accuracy in terms of one or two percent. That's that's actually significant numbers, right? I think it's not so significant. Right. I think it's not so significant here because it's just CIFA and they are very much not optimized. I would not think too much. Okay, this is not stupid to say, but if I would cheat here a little bit or do something a little bit different, then maybe I could also get some point here. We optimized more this lesso, our competitor. So we spend a lot more time optimizing this because. I'm optimizing this because I don't know. I think it's nicer to okay. What I want to say is that I would not take these numbers as so super super important because it's just CIFA and I'm also not the best on optimizing these examples. I think from a I don't really know, but I think everything that works with proximal gradient descent should. That works with proximal gradient descent should work in this sense some similar fashion from accuracy like spread iterations. I think. No, no, I'm not curious. I'm just clear comment, right? So there's a big competition there, getting a little more accuracy is a big deal. So you lose some accuracy. Just shut it down. Yeah. I mean, I know, I know, but I know, but on CIFAM, yeah. But still, I mean, these results are also not competitive with state-of-the-art. You can, if you do some, you Art. If you do some heuristic pruning and technique, you get much better results. So that's why I'm not prowl about these numbers. Yeah. Just shows that it works in theory. Yeah. So we thank you, the speaker again. Please remember.