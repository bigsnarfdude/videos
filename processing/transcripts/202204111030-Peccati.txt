So, we have the pleasure now of hearing from Giovanni Piccati, who is going to give a virtual talk from Luxembourg, and will speak to us about an introduction to the Mali-Einstein method. Okay, thank you, Larry. Thank you, Larry, for proposing to give this talk and also for the invitation. I'm sorry not to be in Banfin with all of you. It seems to be like a very great opportunity to meet several people from different communities. So, I will follow your lead, Larry. So, I will follow your lead, Larry, and therefore I prepare the talk, which is an introduction to a method which is a combination of Stein's method with another collection of probabilistic analytical tools that are known as the Maliovin calculus of variation. So, this is a talk really about the basics, the foundations of this kind of connection between Maliovin's calculus and the Stein's method, and it contains a relatively small number of new results. Relatively small number of new results, and most of it is contained in previous contributions from some years ago. So, now the starting point. So, the starting point is the year 2009, where with my friend Ivan Rudan, we find a way of combining, as I was saying, so two collections or two, let's say, two streams of research in probability theory. One is the well-known Stein's method for probabilistic approximation. So, all of you are experts in these Steins methods. All of you are experts in this Stein's method, so my way of seeing it is a collection of probabilistic techniques that allow one to compare probability distributions by using differential operators via the use and introduction of ordinary and partial differential equations. On the other hand, the Maliavin calculus of variation, so here you see this is Charles Stein and this is Paul Maliavin, is a collection of is fundamentally an infinite dimensional differential calculus. Infinite dimensional differential calculus. And the operators of this differential calculus act on random variables that are functionals of an underlying Gaussian noise, like for instance a Brownian motion, an infinite-dimensional Gaussian noise in principle. And so the Mayavin calculus evolution was introduced by Paul Malia-Bann during the 70s. So 78 is the year of the paper that is usually regarded as the starting point of this theory. And the initial motivation of the Malayvin. And the initial motivation of the Maleavin calculus of variation was the study of the regularity of solutions to cast differential equations. But then the flexibility and the scope of application of this, of the operators of Maliavin calculus has been growing during the years, during the decades, and now there are a number of great diverse applications, among which are limit theorems and probabilistic approximation via the connection with Stein's method. And so, as we will see from my presentation, As we will see from my presentation, the concept that seems to connect the two theories, Stein's method and Maliovin calculus, is the idea of using integration by parts formula. So we already seen in the previous talk, and even the most basic declination of Stein's method is based on an implicit use of one dimension, of finite dimension, integration by parts. And behind the scenes of the Malayalam calculus of variation, one of the pivotal objects in the Malayalam calculus of variation is the use of infinite dimensional integration by parseform. Infinite dimensional integration by parts formula. And so, what we discovered in 2009 with my friend Ivan was that these two kinds of different types of integration by parts formula combine very well together in such a way that one can achieve quite a number of new and remarkable applications. So, actually, our initial aim, our initial motivation for developing, for combining this theory, was to study quantitatively the fluctuations of functionals. Fluctuations of functionals, so in particular non-linear functionals of infinite-dimensional Gaussian fields. So our fields, Gaussian fields of reference for us at the time were actually stochastic processes, Gaussian stochastic processes, like for instance the Brownian motion or the fractional Brownian motion, that is, a generalization of the Brownian motion. And so, and the kind of results we are trying to study quantitatively were results of the kind one calls the Brouam-Meyer central limit theorem. Bruhen-Meyer central limit theorem. So, this is a collection of papers starting more or less from the 80s by these two researchers, Bruhen and Meyer. And what one calls a Brouen-Meyer type limit theorem is a limit theorem that involves random variables that are partial sums of deterministic functions whose argument is a Gaussian sequence with a non-trivial. So, this is a Gaussian sequence with a non-trivial covariance structure. So, it turns out that in order to start So, it turns out that in order to study objects of this type in the large n limit, then one cannot really use marthingl techniques and not mixed in techniques that are important counterexamples that in some sense prevent you to do that. So, in order to obtain quantitatively in theorems related to this kind of quantities, we were therefore led to combine these two techniques, Maleav and Kalcrus, Einstein's theory. So, typical examples of quantities that we were able to study since the beginning were what one calls power variations. What one calls power variations. So, if X t denotes, for instance, your Brownian motion or your fraction of Brownian motion, power variation is just the sum over a finite sum of increments of this stochastic process raised in absolute value raised to some power p. And one typically studies the limit of this as n goes to infinity, and then of course the time grid involved here becomes finer and finer, for instance. And so, another quantity that we were interested in was central empirical moments, which are obtained by looking at the Brownian motion. By looking at the Brownian motion or the fraction of Brownian motion as a random function, and then by studying the empirical moment. So, here, probably, I should have written one over t to normalize to a probability measure. So, in the limit, t goes to infinity. So, these are all complex objects. There are non-linear transformations of our Gaussian field. It is not possible to write down explicitly the distribution of this random variable and this random variable, but however, these random objects are known to satisfy central in theorems. Central limit theorems in the large n or large t limits, and therefore we were interested in studying how good was this normal approximation for a fixed n or for a fixed t. Now, this is a theory that started quite some time ago, so it's more than 10 years ago, and so it started on this initial motivation, so the study of non-linear functional stochastic processes like the fractional Brownian motion, several applications and several generalizations have been achieved. So, some of them. Have been achieved. So, some of them might be of interest to this community. So, I will just put here a slide where I will describe some of them. So, for instance, in the last years, I guess some of the most spectacular or remarkable application of the theory has been to the study of random fields defined of manifolds. So, in particular, there is a stream of research that uses techniques of this type in order to study a very specific object that appears in the physical literature, which is what I brought. In the physical literature, which is what I wrote here as the cosmic microwave background radiation. And so the statistical study of this cosmic background radiation implies in particular that this is a realization of some Gaussian random field manifolds. And there are several ways of applying asymptotic statistical procedures to this kind of object that might involve the use of Stein's method combined with the variational techniques I'm going to describe. Then in a paper with Larry and Evie. In a paper with Larry and Yvonne from some years ago, we studied sparse recovery problems. So here you see a depiction of a phase transition. And then, as I will describe towards the end of my presentation, starting on the year 2010, there has been a large number of new results where this theory, this Miles-Alibin theory, has been extended from or generalized in the sense that it is applied to the functionals of one known point measure, like random post-measures, and in particular with the Measures and in particular with the idea of establishing limit theorems or quantitative limit theorems for functionals that are associated with random geometric graphs, random variables that are associated with random geometric graphs. So here is very small. Probably if I zoom a bit, you see that this is a realization of a random, of a typical random geometric graph. One is typically interested in studying geometric quantities like the length of the graph, the number of connected components of the graph. It turns out that the study of this kind of quantities, whenever the random point configuration comes from Whenever the random point configuration comes from a Poisson-type random measures, it is possible by properly extending the kind of results I'm going to describe in the next slides. Okay, so this is just an overview, a quick overview of what it's all about. So what I will do in this presentation is to focus on normal approximations. So I will show you some of the basic, the fundamental results connected with the theory. Fundamental results connected with the theory, but in connection with limit theorems or quantitative limit theorems, where the target distribution is a Gaussian distribution. So, in order to do that, I will adopt the standard notation. So, calligraphic n of mu sigma square will indicate a distribution of the one-dimensional Gaussian random variable between mu and variant sigma square. And whenever I attach a little d to the symbol calligraphic n, I will mean that this is a d-dimensional Gaussian random vector. D-dimensional Gaussian random vector with mean vector bold face A and covariance function, covariance matrix capital C. So actually, in my presentation, for most of my presentation, as I was anticipating somehow, the Gaussian elements will appear both at the limit, but also as a source of one-knownness of the model. And I will basically focus in my discussion. And I will basically focus in my discussion on two types of Gaussian sources of randomness, finite-dimensional ones and infinite-dimensional ones. So, my symbol for denoting finite-dimensional Gaussian sources of randomness is this bold-faced G of M. So, this denotes a vector, M-dimensional vector, G1GM, which is composed of IID N01 random variables. Okay, and this is for a generic M. On the other hand, as it was an On the other hand, as I was anticipating, the interest of the theory is that one is able to deal with infinite-dimensional sources of noises and in order to fix one of them. So the specific form of the Gaussian source of noise is not particularly important for the theory. It basically sufficient that this Gaussian noise is separable, that is, that it can be generated by using countably many IID Gaussian random variables. So, which is the case for the one. Variables. So, which is the case for the one I'm choosing for the rest of my presentation. But I just do that in order to fix the notation and to fix the ideas and to make the discussion easier. But you can really replace what appears in this slide, which is Brownian motion with a Brownian sheet, for instance, or a random field of a sphere or whatever. So now, back to the slide. So I will denote by capital W a Brownian motion on 0, 1, right? So this is a typical realization of a Brownian motion on 0, 1. And so Motion of 0, 1. And so remember that this is a Gaussian process which is started from zero, which is a center Gaussian process. The covariance of the process is given, so the covariance between Ws and Wt, of course, this should be T, so is the minimum between S and T. And we also require the W as continuous paths with probability one. So the only thing that I'm going to use about Brownian. The only thing that I'm going to use about Brownian motion is that one knows how to integrate functions, how to define what we call a stochastic integral with respect to Brownian motion. And so the most typical example of a stochastic integral with respect to Brownian motion is the one where the integrand is deterministic. So in particular, I would like you to recall the basic fact that if you consider a function h, which is a deterministic function, which is square integrable on the interval 0, 1. integrable on the interval 0, 1 with res to the Lebesgue measure, then the random variable w of h, which is given by the stochastic integral of h with res to Brownian motion over the interval 0, 1, is a Gaussian random variable with mean 0 and variance the L2 norm of h squared. And actually, if you consider a vector of random variables of this type, then this defines a Gaussian vector. These random variables of this form are jointly Gaussian, and they actually constitute what one calls the Gaussian space. Constitute what one calls the Gaussian space, which is generated, the L2 closed Gaussian space, which is generated, which is associated with Roundian motion. Okay, so once again, the only thing which is necessary, which is important in order to make the theory work, is that Brownian motion can be generated by using countably many IID Gaussian-Random variables. This is, for instance, the typical Etonisio construction or the Levi-Chesielski construction of Bramian motion, and then construction of Brownian motion and then when you have that all the theory can be can be developed uh can be developed independently of the of the specific properties of the Gaussian field you're interested in okay so you can of course ask questions if you think the notation is not clear enough or you need more clarification on the on the concepts I'm introducing okay now our problem is a typical problem for a probability theory for probability For probability theory, for probabilist, and it can be described as follows. So we start by considering a square integral of a random variable, capital F. And so this random variable will be assumed in the following to be a function of the underlying Brownian motion. And so we're assuming that the expectation of the random variable is zero and that the variance is one. And so we are renormalizing everything in order to make the formula easier to read. But of course, Easier to read, but of course, one can consider random variables with the generic variance and then renormalize things in a proper way in such a way that the variance will appear in the bounds. So in our bound variances are not appearing because we are renormalized everything at the beginning. And so our goal is to compare the distribution of capital F with the distribution of the standard Gaussian N01 random. Okay, so it was a typical probabilistic task. Task and our way of quantifying. So, we want to compare this, meaning that we would like to obtain quantitative statements that somehow measure, allow one to measure the distance between the distribution of capital F and the distribution of capital Z. And in order to do that, we're going to choose one among many probabilistic distances. This is what we call the one Vasterstein distance. So, the one Vasterstein distance within the distribution of Buses and distance within the distribution of f and the distribution of z can be defined in two ways. So, one is as an lymphimum over all couplings of random variables A and B, such that A is distributed like F and B is distributed like Z, of the expectation of the absolute value of the difference between A and B, of A and B. So there is an equivalent definition of this one-based and this and which is actually the one that is used in Stein's method. Is actually the one that is used in Stein's method as a supremum, right? So we take the supremum over all test functions that are Lipschitz with Lipschitz constant which is less or equal to one. And what we are taking is the expectation at the absolute value of the difference between the expectation of h of f and the expectation of h of z. Okay, once again, the supremum runs over all one Lipschitz functions from R to R. So this is. Giovanni, there's a question here. One moment. Yep. a question here one more yeah yes so just to make sure i understand so the the so the function the function f it goes from the from the linear space to r right exactly uh you can take f to be for instance a mapping that goes from the space of continuous function on zero one into r and uh and uh so the expect so the expectation is uh the expectation with respect to the to the With respect to the distribution of the Randyan motion. Right, absolutely. Yes, yes, okay. You can, yeah, exactly. So the expectation of H of F is the integral. Okay, one should probably justify a bit better, but is this with, so this is calligraphic view, and then here you can take the law of Brownian motion at dw. Okay, so it is really, so the noise. It is really so the noise, the noise source of randomness is the underlying Brownian motion. And therefore, here the expectation is taken with respect to this source of noise, which is the distribution of Brownian motion. It's okay? It's okay? Yes. Yes. Okay. Then of course, then of course this function takes, this function capital F takes an infinite Capital F takes an infinite-dimensional object which is a Bramian motion, so that takes values in the space of continuous functions and it transforms into a number. So this becomes a one-dimensional random variable with its own distribution. And so this is the expectation with respect to the distribution of F, but the distribution of F can be expressed by the change of variable formula in terms of the distribution of the Brownian motion. Okay. Okay, so I'm choosing this distance, the one vastest. Distance, the one vastest and distance, because the analysis is particularly nice as far as the application of Stein's method is concerned. But during the years, we have been able actually almost immediately, one sees that most of the theory with some technical details to be clarified extends to other distances, like the Kolmovoro distance, the total variation distance, the bounded buses, and distance. Bounded vast and distance, what one calls in the French circles the Ferté Mourier distance, which is just like the vastest and distance here, but also one adds that the function h is bounded, etc. So the one vastest same distance is a distance between distributions. And actually, the topology which is induced is stronger than the topology of conversion distribution, meaning that if you have a sequence of random variables, for instance, Fn, such that W of Such that W of FnZ converges to zero. Then this means that Fn converges in distribution to Z, but there are examples of sequences of random variables such that Fn converges in distribution to Z without having that W one distance converges to zero. The reason being that convergence in W one distance always implies necessarily convergence of the expectation. And we know that this is not necessary for convergence in distribution. Distribution. Okay, so our goal is therefore to estimate things, random variables, so distances within the distribution of random variables of this type. And of course, since I'm making the very strong assumption that the source of randomness in our model comes from the Brownian motion, then I have to exploit, I will exploit this feature quite heavily in the future. And see, this is where the Maliabin calculus of variation comes into the picture. Okay, so let me go. Okay, so let me go on with this distribution. So, as people in this conference know, one way of controlling distances like one vastest and distancy, especially when Gaussian approximations, normal approximation, central limit theorems are concerned, is the use of characterizing operators. And so, the way I'm going to use it in my slides is. Use it in my slides is the following. So, I consider a smooth function g, which is a function from r to r, and then I'm introducing an operator which for the moment I'm going to denote with calligraphic t g of x, which is obtained by multiplying the function g by x and then by subtracting the first derivative, I mean the first derivative of g at the point x. Okay, so this is a characterizing operator for the Gaussian distribution in the following sense. In the following sense, so a random variable, so this is what we call Stein's lemma. So, a random variable is such that the expectation of Tg of Z is equal to zero, let's say, for every function g is smooth, if and only if Z is distributed according to the N01 distribution. And this operator is actually quite a remarkable one because one can prove by applying integration by part formula that it is actually the adjoint operator of the derivative. The adjoint operator of the derivative operator in the space L2R with respect to the standard Gaussian distribution with mean zero and variance one. So now what Stein's method tells us is that it is possible to bound this quantity, that is W1F of Z, in terms of what one calls nowadays a discrepancy. And the bound goes as follows. So the distance, one buses time distance between this. One masses and distance between the distribution of f and the distribution of z is bounded by this quantity that was calligraphic s of f, calligraphic t, and g, which is defined as the supremum over all functions g that belong to a class calligraphic g that I'm going to describe later on, of the absolute value of the expectation of the operator calligraphic t applied to g at the point f. So we define this function, so calligraphic t of g of big f. Of G of big F, we take the expectation in absolute value and we take the supremum over all function in the class calligraphy G that is defined as a collection of all mappings from R to R that are of class C1 with a derivative, first derivative which is bounded by the square root of two over pi, and such that G prime is a Lipschitz function with a Lipschitz constant which is bounded by two. So this is what I mean by this. I mean that G prime is Lipschitz with a Lipschitz constant I mean that G prime is Lipschitz with a Lipschitz constant which is bounded by two. Okay, so the way of achieving this kind of bound is by solving actually an ordinary differential equation as I was anticipating. And so now there are several remarks that can be done that are typical whenever one presents Stein's method. It is that. So we started with an expression like this one, where we were considering test functions that are Lipschin's one function. Test function that are Lipschin's one functions and where two random variables appear. So the random variable capital F and our target random variable capital Z. And then the user stance method allowed us to move to another expression where we are actually considering function g that have an additional degree of regularity because they are of class C1 with a Lipschitz first derivative. And also on the right-hand side of this expression, the random variable capital Z has disappeared in the sense that the role of In the sense that the role of capital Z be completely replaced by the presence of this characterizing operator. Okay, so this is a typical collection of phenomena that are associated with the use of Stein's method. And now, our goal is now, therefore, if you want to effectively being able to assess this quantity, W1 of Fz, our goal is therefore to find a way of bounding this quantity, in particular this kind of absolute values of expectations, in a way that is. In a way that is uniform in G. And then, spoiler, the way, I mean, the fact that this is possible is that the regularity of the function belonging to the class calligraphic G is expressed in terms of constants that do not depend on a function in G. So it is uniform over the class calligraphic G. Okay, so now the problem is the following, therefore, so now fix a function g in this class, calligraphic G, a function that are Class, calligraphic G of functions that are regular in this sense. We would like to understand how to, therefore, control, how to uniformly bound the quantity expectation of calligraphic T of G of F that I now write explicitly. So this is the expectation of F times G of F minus the expectation of G prime of F. Now, let me remark parenthetically that it was a good idea to set the expectation of F equal to zero. Equal to zero because the class calligraphic G I'm considering here is invariant, is stable with respect to translation. So if actually the expectation of f was not zero, then this supremum would be infinite. But it is not because we are taking a random variable f with zero expectation. Now, as I was anticipating, the way we devised in order to control this kind of quantity. To control this kind of quantities, whenever the random variable f is therefore directly, can be directly written as a function of a Gaussian source of noise in our framework. It is the function of Brownian motion, is to assume that this function f belongs to the domain of some Malayalam type operators, of operators that appear in the Malayalam calculus of variation as applied in our case to the specific framework of boundary. The specific framework of Brownian motion. And so, and this is where integration by paths formula and an infinite dimensional setting play a role. And this is what I would like to describe in the form. So now there is a series of maybe five slides, which are a bit technical, but I need to introduce this formalism in order to make appear the objects that are fundamental, that make really for the strength of the theory and of applicability of the theory. Theory and of applicability of the theory. Okay, so other questions? Yes, so could you repeat why we want to bound to uniformly bound this term? We want to bound this uniformly. Okay, the fact that we want to bound it uniformly is because we want to bound the quantity w1f of z, which is expressed as a supremum already in itself. And then via the use of Stein's method that I'm not going to detail here, this quantity. Detail here. This quantity w1 can be bounded by another supremum. So we're taking the supremum over all function in the class calligraphic G of this quantity. So in order to bound this w1f of z, that is the left-hand side of our inequality, we need to find a bound on this expectation, which is independent of the choice of the function g in the class calligraphic g. Does that make sense? Yes? Okay. Okay, great. Yes, okay, okay, great. Okay, good. So, and the task seems to be reasonable already, even if you didn't see this kind of computations before, because we are already working with a class of functions that are regular with constants of regularity that are independent from the function of the function that are just uniformly given over the class calligraphic G. Okay, so now how one can Now, how one can do this kind of thing? So, there is one fundamental object that can be used in order to introduce everything. And this object is known as the Ornstein-Ullenbeck semigroup. So, it is not clear from the beginning that this is actually a semi-group, it verifies a semi-group property, but in a couple of slides it will become like self-evident. And so, and how can one be And so, and how can one build this orthanulaic semi-group? So, it goes as follows. So, you fix a t, so a point in time which is t bigger than zero, and then you consider a function f. Once again, it is a function of our Brownian motion, which is integrable. So, not to define it, we just need this function to be integrable. We are actually interested in squared integrable functionals, but you just need integrability to define this. And then one defines a new random variable, pt of f. So, this new random variable. So, this lunar variable is actually a function of the original Brownian motion. But in order to define this function, one needs to introduce an independent copy of our Brownian motion, which is this W prime red in red, in my description here, which is therefore an independent copy of our Brownian motion. Now, the way of building the Orstein-Nullen-Max semi-group is the following: you take the function capital F once again. We can see this as a function that Can see this as a function that takes values, actually, which is defined on the space of continuous mappings on 0, 1. And instead of taking as an argument of capital F the original Brownian motion, we take a linear Brownian motion of W and W prime. And for x fixed t, we obtain this linear combination by multiplying w by the exponential of minus t. minus t and w prime by the square root of the exponential of minus 2t one minus the exponential of minus 2t now if you make a quick computation and and if you use the properties and the self-similarity properties of the Gaussian distribution, it is actually easy to see that for every t, this guy is still a Brownian motion. So it is a Brownian motion which is expressed in terms of w and w prime, so that is a bit dependent on w and a bit dependent on w prime. And of course, the closer t is to zero, the closer this object here is to the original Brownian motion w, and the closer little t is to infinity, the closer this new object is to the independent Brownian motion w prime. Okay, so you take f, therefore, you take as an argument of f this linear. Take as an argument of f this linear combination of two independent Brownian motions, and then you condition. So you take the conditional expectation of this guy with respect to the original Brownian motion. Now, by definition of this, of the conditional expectation, this P T of F, as I was anticipating, is a function of W and it actually turns out that if you let T move, you obtain a semi-group. So this actually verifies the semi-group property in the semi- Actually, it verifies the semiconductor property in the sense that P T composed with P S in this construction is equal actually to P T plus S. And this representation of the Orstein-Unlimblen-Maxime group is known as the Meller representation of the Meller form of the Orstein-Unlimb-Semi group. And then, as I was somehow implicitly telling you in my previous discussion, if you consider P0 of F, that is, if you specialize this definition to the case where T Specialize this definition to the case with t equals zero, then this simply gives you the original random variable, capital F. And if you let t go to infinity, then it is, one can prove that at least in this in L2, whenever f is in L2, then this P T of F converges to, let's say, P T of F is going to converge to the expectation of F. So one can see, therefore, the semi-group P T of F as an interpolation between the random variable F and Between the random variable f and its expectation, uh, just a quick question. Yep, uh, so here you could have probably gotten these limiting properties of t going to zero and t going to infinity by simply even looking at the connects combination. So, that square root of one minus e to the negative two t is the one that gives you the that is key to the semi-group property. Is it right? Yeah, yes, yes, yes. I mean, yeah, this one, yeah, yeah, yeah. Yeah, yeah, yeah. I mean, every constant is well calibrated in order to give you the doesn't work if it is not that. Yeah, okay, because I can also still take one minus e to the negative t and you can still get these limiting properties, but the semi-group property may not hold. I'm sorry? So I can take the convex combination of w and w prime. Yeah, yeah, yeah, yeah, yeah. But but you are saying the semi-group property won't hold. Yeah, yeah, yeah, exactly, exactly. Okay, okay, thank you. Okay, uh, now, uh Now, what is interesting is to understand some spectral properties of this semigroup, Pt. And the spectral properties mean basically that we are going to characterize the eigenspaces of the operator Pt of F for a fixed T. And in order to do that, one has to introduce quantity, which is an example of a stochastic integral with respect to Brownian motion. integral with respect to Brownian motion. And the definition goes as follows. So for n bigger than one, we consider a function little f which is symmetric and a square integrable with respect to the Lebesgue measure, so the product of Lebesgue measure on the square 0, 1 to the power n. Now, whenever you have a function of this type, then it is a classical result and one can define multiple stochastic integrals. That is iterated integrals with respect to Brownian. integrals with respect to Brownian motion of the function little f. So these integrals are defined as follows. So i n of f is given by n factorial and then multiply. So what we are going to do here, we are going to take n times the integral of f separately for each variable with respect to Brownian motion. So we are first integrating from 0 to t n minus 1 with respect to the tn variable. We obtain here a function that depends on tn minus 1. We integrate it again One, we integrate it against with again with respect to this Brownian motion here, and then we repeat n times this operation. So we integrate iteratively, in iterative manner, the function f n times with respect to the same Brownian motion for the initial Brownian motion. And we multiply this by the factor n factorial in order to simplify the formula. So this is simply a multiplicative factor that allows one to obtain nice. Allows one to obtain a nice formula. Okay, now it turns out that this collection of multiple integrals describes perfectly the eigenspaces of the Ornstein-Ulenbeck semigroup of the operators in the Orstein-Ulen-Beck semi-group. And it goes as follows, actually. So if you fix a t bigger than zero, and if you look at P of T, that is this Orstein-Ullemic operator, as an operator that takes square integrable random variables and transforms it into square integrable random variables. Into square integral random variables. We can define the space Cn for the moment. So the eigen spaces are given by the space Cn, and these spaces are defined in the following way. So C0 is equal to R to the real line. And then Cn is just the collection of all multiple integrals of order n with respect to a square integrable symmetric function on the square 0, 1 to the power n. Okay, so this object, so this. So, this object, so this Cn, so this spaces Cn, which are therefore the eigenspaces of the Orstein-Unlimited semi-group, are called the Wiener chaoses that are associated with W. And so for every N, Cn is the nth-Wiener chaos associated with W. And one should really think about these guys as infinite-dimensional counterparts of Hermit polynomials of degree n. So, this is a strong assertion in the sense that you can actually represent explicitly. You can actually represent explicitly each one of the random variable as an infinite series of multivariate Hermit polynomials of degree n. And for specific choices of the kernel f, you can actually represent this i n of f as a single one variable, one variable, Hermit polynomial in one variable. Okay, now remember the definition of your Austin-Lunan Max semigrow, that is given by this condition expectation. That is given by this condition expectation. And now imagine to apply this orthanulian semigroup to a multiple integral. So, this tells you, so the definition that I give here requires you that you replace in each one of the iteration of your integration, operation of stochastic integration, the exponential of minu t dw plus the square root of one minus the exponential of minus to t dw prime, which is our independent copy. Prime, which is our independent copy, and then that you take the conditional expectation with respect to w. Now, if you do this operation, that you have to replace each one of these integrators with this linear combination of integrators, then you can apply the binomial formula and write this as a sum of 2 to the power n integrals, where each integral will have some dw prime integrator except for 1, where all the integrators are equal to w. And so if you condition to w. And so, if you condition to W, the properties of centering of stochastic integrals will make everything disappear except for the n times integrators that only contain w. And if you, it's a bit of fun waving, this is the explanation that tells you why these are the eigenvalues, the eigenspaces. So, indeed, the action goes as follows. So, for every n, the action of pt, so of the semi-group pt on the nth multiple integral with respect to f. Multiple integral with respect to f is actually given by the exponential of minus nt of i n of f. Okay. Yes? I have a question. So can you go back to the slide 11, please? Yes, please. Just for the multiple stochastic integral. Yeah. So what is okay. So if I take just a just say Say n equal to one. So a simple stochastic simple Vener integral. Right, right, right, right, right. So if I take just n equal to one, then the wiener integral is just a Gaussian random variable, right? Exactly, exactly. Absolutely. Yes, yes. Now I don't, what I don't understand is that, so the i n of f, so it's what object is that it's a function? It's a function. What's that? It's an integral of Gaussian random variable, of a Gaussian stochastic process, as you say, with respect to the Brownian motion, to other stochastic process. Probably the most simple example of a double linear integral is obtained by taking the integral from 0 to 1 of Brownian motion integrated with respect to itself. Okay, so this is a double. Okay. This is a double integral. This is a double integral, right? And then it a formula tells us that this is w squared minus one, right? And so, and this is actually the Hermit polynomial of degree two computed in W1. Okay. And okay, so there are a lot of relations of this type. So you can really think that by properly applying I n of f the definition here and by expanding calligraph little f into a proper basis, then you can represent each of these guys. Then you can represent each of these guys as a linear combination of Hermit polynomials of degree n in many variables. Okay, so i n of f is just a Gaussian random variable. Oh no, it's a facilitating Gaussian. No, it's Gaussian. No, no, it's not Gaussian. No, it's not Gaussian, it's not Gaussian, it's Gaussian only random variable which is defined on the probability space. In the linear space. Exactly. Exactly. It's a random. It's a random variable. There is no other so in the definition of i and of f, there is no other source of randomness than the Brownian motion, right? Because the function little f that you are integrating is deterministic, it's just some function which has no algorithm. And so this is the only source of randomness. And it is, so it is like this. Okay? Okay, then we apply PT to that. Okay, I apply PT to this little waving from my pot, but then it gives you that. This is the action. So the action of PT is given by this guy. Now, okay, there are several things that one can say about this vinyl kiosis, and one that we use all the time is actually the fact that they can be used in order to decompose any random variable. So this means that if Random variable. So this means that if you take a random variable f, which is a square integrable functional of your Brownian motion, then this function can be always represented as an infinite series of its expectation, and then a sum, an infinite series of multiple integrals of increasing order L, where the convergence of the series takes place in L2. Right? So this can be seen as a counterpart of the one-dimensional fact that the hermit That the Hermit polynomials are a complete system of orthogonal polynomials for the one-dimensional Gaussian distribution. Now, if you use this representation of a specific random variable and then you apply to it this description of the action of the Orstein-Nullen-Baxaming group, you see that Pt of F is equal, actually, it can also be rewritten in a form which is not the medium form, as the sum of the expectation and the sum of the exponential of minus. Of the exponential of minus nt multiplied by i n of fn. So you start with a random variable of this type, and then the Orston-Williams semigroup is obtained by multiplying each one of these elements in the decomposition by the factor exponential of minus nt. Okay, so this is actually an operator that reduces the fluctuations of high-degree polynomial components in each given random variable. And the bigger is t, the bigger. The bigger is the reduction of this fluctuation of high-order component of this one. And so, by using eto isometry, so this is a computation I'm not going to make directly, one can actually prove that the variance of f can be written as the series from n going from 1 to infinity, n factorial, multiplied by the L2 norm. This is the L2 norm in the space L201 to the power n. 01 to the power n. So you can decompose and represent the variance in this way. Okay, now to resume, I've given you now a definition, I mean, an equivalent characterization of the action of the Orstein-Ulenbeck semigroup on any square integral random variable by means of its chaos decomposition, that is, the decomposition into a series of multiple integrals. Now, starting from that, we can give a full description of several objects that are attached to. Description of several objects that are attached to the semi-group. So, for instance, if you take the derivative of this guy in t equal to zero, you obtain the generator of L. So, the generator of the Orstein-Ulamic semigroup, which is denoted by L, is the operator L of F that transforms a random variable with this form into a series, which is given by minus n i n of f n. So, it's obtained by taking every element of the chaos decomposition and by multiplying. chaos decomposition and by multiplying it by minus n. It is simply done by taking the derivative here. And saying that f belongs to the domain of L means that we are requiring that this series is converging in L2. You can also take the pseudo-inverse of L, which is denoted by L minus 1. And this is simply obtained by taking a random variable, which has this serial representation as a series i n of fn and by multiplying f n and by multiplying each element by minus one over n. And this guy is a pseudo-inverse because if you apply L and L minus one to the generic squared integral one variable f, which is in the domain of the right operators, then L L minus one of f is equal to L minus one L of F, and it is equal to the difference between F and X expectation. So it's a true inverse only when Inverse or only when it applied to it is applied to centered random articles. Okay. So, okay, so this is these are the operators. So we have the semi-group with this description. We have the generator of the semi-group and the pseudonyms of it. Now, in order to at least achieve one of the results that I would like to describe, one has to introduce another concept, which is the last formal concept. And this is the notion of the Maliavin derivative. And this is the notion of the Maliavin derivative of f. So the Maliavin derivative can be actually interpreted as a directional derivative, but I can give you a direct definition for it. So one starts by considering random variables that are called cylindrical random variables. You take some function f, which is smooth, and which is actually only a function of finitely many points of the Brownian motion. So wt1, wtd. One can define the Maliabin derivative of f as a function. of f as a function actually so it is a function that takes a point x and transforms it into the number dx of f, which is a random variable, which is obtained by taking the sum for I go from 1 to d. So d is the number of arguments in the function f, the partial derivative of f in the direction xi, multiplied by the indicator of 0 ti of x. Okay, so the indicator of 0 ti, where ti is the ith coordinate with Where Ti is the ith coordinate which is contained here. Then one has also to observe that WTI can be also written as a stochastic integral of the indicator 0 Ti. Okay, now this is a random element, so it's a random function actually. So this random variable dx, so this mapping x into dx of f that I'm going to denote by d of f is actually a random element. It takes values. actually a random element it takes values in the space of square integrable random variables random functions square integral functions on the interval 0 1 so and it turns out that this operator so this operator that takes a simple random variable cylindr random variable of this type and transforms into a random function of this type can be extended to a large domain which is typically denoted by d of one two and this large domain And this larger domain is typically referred to as the domain of the derivative. And in this case, one can give an explicit representation of the derivative in terms of the multipolitical representation. So we are not really going to use this in the following. The only notion that should be kept from this discussion is that in the case of simple random variable, the Malayvin derivative can be. The Malayvin derivative can be described in this omega by omega form. And then that one can extend by clausability and density such a definition to a larger domain, which is actually Sobol space that we are going to denote by D1 of 2. This is the maximal domain for which the derivative is well defined. Now, the crucial point for our discussion is that this operator D, even when extended to this maximal space D1, 2, Maximal space D12 is actually a derivation in the sense that it satisfies a chain rule. So if you take a transformation, smooth transformation phi of F1, Fm random variables, and you take the derivative of this guy, then this can be written as the sum over all partial derivatives of phi in the direction xi multiplied by the derivative of the ith random variable. And there is another point is that, okay, now the derivative takes. Okay, now the derivative takes random variables and transforms it into random functions belonging to the space L201. And this operator also admits an adjoint, which is going to be denoted by delta. And so this is the adjoint of the derivative. It's therefore an operator that takes random functions and transforms into random variables. And this operator verifies what we call. So this is the example of this infinite-dimensional integration by path formula. By path formula. And it tells you that if you take the expectation of f, any f in the domain of the derivative, and you multiply it by delta of u, then this can be is equal actually to the expectation of the integral over 0, 1 of the integrand with respect to delta multiplied by the Malevin derivative. And I'm going to denote this integral of u times d integrated over 0, 1 as the inner product. As the inner product of the f and u. And now the key relation finally that links all these objects together is that if you now take a random variable f that belongs to the domain of the generator of the Orstein-Ulemic semigroup, then f so f belongs to the domain of the Orstein-Ulemic semi-group, if and only if, so this is an if and only if, f belongs to the domain of the derivation, df belongs to the domain of the score of code integral. Of the score of the integral, and at the end of the day, we have that L of F is equal to minus delta D of F. Okay, now I will skip this because I want to go to the, since I have just a few minutes, I want to go to the crucial computation. So, why is everything useful? How can one use all these kinds of relations? So, let me, however, make a remark that is that now, if instead of taking our source of randomness to be an Taking our source of randomness to be an infinite-dimensional object like a Brownian motion. I take it to be just a Gaussian vector, like the one I introduced before, composed of centered Gaussian-independent random variables with variance one. Then all these objects that I've defined take a familiar form. So for instance, the Maybean derivative is simply the gradient of the function f computed at the point gm. The score of an integral or the adjoint can be represented in terms which is very much similar. Which is very much similar to my description of the operator calligraphic T before. And so, and then the generator of the Orsteil-Lunar Mech semigroup, which is minus delta of the gradient, by applying this formula, turns out to be a second-order differential operator. And if you specialize everything to the case m equal to one, one single Gaussian random variable, then the operator delta is simply the operator, the determining operator calligraphic T that I introduced before. Now, Now, how this is used, so I want to make just three slides. It is used as follows. So, remember that we have a function f of our Brownian motion with mean zero and variance one. And our original goal was to control the absolute value of the expectation of the operator calligraphic T as applied to G computed in F. So, this was given by the difference between the expectation of F, G of F, and the expectation of of f, g of f, and the expectation of g prime of f. Now, the name of the game is to transform this quantity in a way which is comparable to this quantity, and to do this by using our integration by pass formula and by exploiting the fact that g prime is bounded in absolute value by the square root of 2 pi. So we can do this as follows. So assume that the random variable f is so in the domain of the derivative operator, then we can write the expectation of Then we can write the expectation of fg of f in the following way. So, first of all, since f is centered, f will be equal to the composition of l n minus 1 of f multiplied by g of f. Now, I know that L of F is minus delta D of F. Okay, so I can replace L L minus 1 of F by minus delta and then D L minus 1 of F multiplied by Gf. Now I know. Now, I know that there is an integration by Fart's formula which is available that allows me to go from expectational products of this type to expectation of inner products of this type. And therefore, I can transform this guy into the expectation of the inner product on L201 with derivative of gf and the derivative of L minus 1 of f. Now, I also know that there is a chain rule, and therefore that this derivative here is actually equal, so d g of f. d g of f is actually equal to g prime of f times d of f. And applying this, I just can write this as g prime of f multiplied by the inner product between d of f and minus dl minus 1 of f. Now at the end of the day, I can rewrite this quantity exactly as the expectation of g prime of f, like here, multiplied by this inner product between the derivative of f and minus dl minus 1 of f. And minus dl minus 1 of f. And if I plug everything together, and if I use this bound of g prime, I can actually obtain that the square root of pi over 2 multiplied by the quantity that we would like to control is actually bounded in terms of the expectation of one single random variable. This is one minus the expectation of h of f. And by Cauchy-Schwartz, you can prove that actually the expectation of h of f is one, and so this is bounded by the square root. And so, this is bounded by the square root of the variance of h of f. Okay, two more slides, and then I will finish my presentation. So, we have therefore proved this relation, that if you take a random variable f, which is smooth enough in order to be in the domain of the derivative operator, with variance zero and expectation zero and variance one, then the vastest and distance between f and z is bounded by some universal constant, and then the variance of some specific random variable. Of some specific random variable that is written in the form of an inner product. And it turns out, so now this is a bound, which is interesting or not, depending on your application. So it turns out that this bound is extremely powerful. So in particular, one can prove a result of this type. So one can prove that if you consider a random variable Z in N01, and then you fix a random variable F that belongs to the Q scales. So this means that IQ of F Q of f is a multiple integral of order Q with respect to Brownian motion. So a multiple integral of order Q with respect to Brownian motion. And you ask yourself, what is the distance between the distribution of f and the distribution of z? So remember that by our discussion, it is clear that random variables in the qth chaos are never Gaussian. So it turns out that this quantity is bounded by some universal constant depending only on q, which is 2q minus qqq. only on q which is 2q minus 2 divided by 3 over pi q multiplied by the difference between the fourth moment of the random variable f and the fourth moment of the standard Gaussian distribution which is given in particular by the number three okay and so this result was proven in a qualitative form in 2005 and it is and it is and so and this result tells you basically that it also holds quantitatively and Holds quantitatively. And it also tells you that if you take a, therefore, a collection of random variables that belong to a Vinner chaos that are normalized in such a way that the variance is one or convergent to one, then this sequence is converging to the Gaussian distribution in the sense of the one vastest and distance, if and all if the fourth moment of f is converging to three, which is the fourth moment of the Gaussian distribution. And this also holds in more general distances, like a total. In more general distances, like a total variation distance or the Kolnogorov distance, etc. So, let me just tell you that there are further results that are useful whenever one is not able to deal with this L minus one operator. So, this can be quite tricky to deal with. And there are a collection of results that are known as second-order Poincaré inequalities that allow one to deal with this kind of results whenever L minus one is not available for analysis. And everything. And everything that I was telling you also holds in a multi-dimensional setting. That is, you can obtain multi-dimensional bounds in one basis and distance in the convex distance, also with respect to relative entropies, there were application to functional inequalities, and further characterization of linear chaos. So, the second part of my talk was about discrete models, which I will skip. I will just tell you that there are books that describe in more detail, of course, the theory that I describe. More detail, of course, the theory that I described very quickly in my presentation, and there is also a web page that is dedicated to this Malewinstein approach. So, I took probably five minutes more of my allotted time, so I apologize for this. And I would like to thank you. Thank you, Giovanna, for a very nice talk on the validated style method. And I'll throw it out now to. And throw that out for any questions? I just have one quick question. Yes. So, in all your analysis, you're looking at L2 of 0, 1 to the n. So, all these results would carry forward if you're looking at L2 of 0, infinity. I'm sorry, I'm sorry, I don't understand. I'm sorry. So, your L2 spaces, they're all defined on 0, 1. Yes, yes, yes, yes. There is no. There is no, I could have done exactly the same thing if I consider the Brownian motion on zero infinity. Once again, you don't even need a Brownian motion. You just need that you have a noise which can be generated in a separable way by a countable collection of independent Gaussian random variables. Okay, okay, okay. Thank you. Okay, are there any other questions? Actually, I've got a question. Yep. Okay. Brilliant talk. And I should probably know the answer, Giovanni. Can we use an RKHS, a Silverbot space for doing all these Mayawan calculus? Yes, absolutely. Yes, yes, yes. We can. We can. It is done. Can, we can. It is done. If you have a, I mean, the typical situation that we have is the one where we're given a covariance function of a given Gaussian process, and then starting on the covariance function, we built in Hilbert space. And the way, I mean, it is not described in this way, if you look even in my book, but it is exactly this. So it is the right way of going. And so it can be, so let's say, So let's see what we need to have is actually not even okay. So we need to have a collection of random variables that are indexed by separable Hilbert space. And so this can be obtained, for instance, with this reproducing kernel Hilbert space. It is a classical way of doing it. And it is actually the right way because, for instance, when dealing with stochastic processes of a Gaussian type, one can very often represent them. Uh, one can very often represent them explicitly as integral with respect to Brownian motion to the standard Brownian motion. But this can be inefficient for several reasons. And the right way to go is actually to represent everything in terms of the reproducing kernel hybrid space. So the answer is yes. As far as separability is preserved, then everything goes through properly. Okay, and then the follow-up question is: can we say anything depending on which kernel we choose for the RKHS? kernel we choose for the RKHS? So I would say that all these objects are independent of the choice of the kernel. All these definitions should be independent of the choice of the kernel. So I should check this, but it is probably true. Okay, so it doesn't buy us anything to take particular kernels? No, no, no. Okay. Thanks. I have a quick question. So, in the presentation, it was like mapping to R. And in your like just a moment ago, you told you can do the mind. If your reference is mapping to sound, infinite dimensionals. I'm sorry, I know I can't hear you very well. I hear just one word every two words. Can you hear me now? Can you hear me now? A bit better, probably. Yeah. Okay, so in your presentation, the F was mapping to R. And you also told that you can have F to be like a multivariate function or mapping to some multi-dimensional space. Absolutely. So I was just wondering if you can do, if F is mapping to an infinite dimensional space itself, will this thing spoil? So you will. So you you would um so can we go to an infinite dimensional space as a an image space? So the answer is yes. It has been done. It is uh the the people that are doing this are uh actually um simon campes probably is in the audience and solène burgan so they did it first uh for a for a for hilbert space-valued mappings okay but now i think they can do banach space valued mappings which is Banach space value mappings, which is very remarkable, I think. And so all these formalities can be lifted to an infinite dimensional space. So going to the finite dimensional version is, I mean, it's subtle, but it was clear from the beginning that it could be done. Going to an infinite dimensional space, especially one without a Hilbert space structure, is much more tricky. And I think that what they did is remarkable. So it is, yeah, indeed. So the answer, the short answer is yes, it can be done. Short answer is yes, it can be done. Thank you. Okay, are there any other questions? Thank you, Jovani. Thank you again for a very nice talk. Thank you very much. At this point, I want to turn the meeting over to Lester. Lester is going to illustrate for us a very nice virtual tool. Nice virtual tool, a virtual chair that we can use with the virtual participants, kind of for meeting in this other universe where we can, you know, go to have offices and have private conversations and write on whiteboards. So, Lester, are you there ready to show us virtual chair? I'm here. I'm ready. Okay, great. All right, everyone. Thanks for coming. I'm glad to see so many people here, both in person and online. Yes, as Larry said, my task is to introduce our. Yes, as Lara said, my task is to introduce our virtual venue to you. So I just pasted a link in the chat. If you haven't joined already, I encourage you to actually just click the link right now. You can see what I'm talking about as I go through it. If you click on this link, this virtual chair venue link, it will take you to a screen that looks a bit like this. Let's see. Shannon. Okay. You'll find yourself in a world like this. Can you all see my screen? Can someone? Can you all see my screen? Can someone see me? Oh, there's Murad. Hey, Murad. This is me. Great. This is me. This is Leslie.