Today I'll talk about against predictive optimization on the legitimacy of decision-making algorithms that optimize predictive accuracy with Silas there and solar and our end. And so I'll start off by talking about a few examples about cases where these predictive systems have failed in the real world. So in the Netherlands, a welfare fraud prediction system ended up misclassifying thousands of individuals and taking away the welfare that they shouldn't be getting. And then EPIC is a healthcare And then Epic is a healthcare production tool that was trying to predict sexists, but in the real world, the performance was far worse than what the developers were originally reporting. And then in Oregon, there was an artificial intelligence algorithm developed to predict child maltreatment, but this was dropped before it was even deployed because of concerns about racial bias. And so in this work, we're asking why do these automated decision-making systems keep failing? Is there something shared across all of these, or is there something distinctive in each of the cases that have caused them to fail? Fail. And so what we argue is that there are shared problems which are manifesting across these application domains. And so automated decision-making systems can mean a number of different things. There's applications like spam filtering in our emails, granting loans like an upstart the application discussed earlier, and automating existing welfare rules that humans have developed, just adding them into an automation system, or even things like forecasted weather. And these all have different characteristics. So, for instance, spam filtering and loan granting, these use machine learning. These use machine learning to learn these sort of predictive rules. For automating existing welfare rules and also loan granting, these are making decisions about individuals. And for things like weather forecasting and loan granting, these are predicting future sort of outcomes. And so in this work, we point out those particular category of decision-making systems that we call predictive optimization that satisfies these three characteristics and say that they suffer from a number of recurring critiques and flaws that make it so that they will not work as they purport to. Not work as they purport to. And so predictive optimization, as we've defined it, has these three characteristics. It uses machine learning, it's predicting future outcomes, and then it's using these predictions to make decisions about individuals. And because of these three characteristics, there are particular sorts of flaws that are inherent to allocations of predictive optimization. So I'll give a few examples. I'll use the example of loan granting from Upstart, where they use historical observational data to try and predict whether someone will pay back a loan in the future to decide whether they should be granted a loan now. To decide whether they should be granted to a loan now. And so, as the previous talks I've also talked about, by using machine learning, specifically supervised machine learning, we have to have a sort of concrete target. This is unlikely to be like the sort of concept we're trying to measure, so creditworthiness. We're only able to measure something like whether someone has paid back a loan within, say, two years. And this will miss people that pay back loans after that, or working hard to pay back loans and other sorts of things. And by predicting future outcomes about individuals, these systems are going to suffer from limits to predictions. These systems are going to suffer from limits to prediction. While we may be able to predict sort of aggregate population-level outcomes, it's very hard to predict individual outcomes. Someone's ability to pay back a loan will depend on a number of things, like maybe the current economic climate, different sorts of family emergencies, or someone could even win the lottery. So there's lots of these things that kind of put us up or down on how well we can curate. And then, as a result of all of these characteristics, these applications are also susceptible to Goodhart's Law. So, individuals might exercise strategic behavior. Might exercise strategic behavior. So, for example, in an effort to increase credit scores, people try not to apply to credit cards. That doesn't really change their ability to pay back a loan, but it will lower their credit score. There's even sorts of predatory credit cards that say if you apply for them, it will help increase your credit score. And so, individuals are trying to sort of exercise strategic behavior to change how well they might be judged by these systems. And so, any sort of application of predictive optimization will fall susceptible to these critiques. And so, the argument we kind of have is that we identify a recurring We kind of have is that we identify a recurring set of seven flaws, which I've just discussed three, that apply broadly to predictive optimization. These are hard to fix technologically and they give the sort of claimed benefits of these applications, which is that they are accurate, fair, and efficient. And so there's another roof I keep will here. I'm going to also talk about sort of the way that these systems don't work as they purport to actually work. And so for our empirical findings, we sample eight real-world applications. This includes things like hire review for job hiring, AAB Navigate, which is predicted to school drop. AB Navigate, which is predicted to the school dropout, and we find that all seven of the flaws we pointed out apply to each of these eight applications. And so, because of this, we offer a call to action, which is that any application of predictive optimization should be considered legitimate by default unless the developers are able to demonstrate how it's going to avoid these sort of failures. And so instead, we alter sort of alternatives, which include changes to address individual critiques, but addressing each of these individual critiques will bring the application further away from what predictive optimization actually is. For example, Optimization actually is, for example, by using less machine learning and choosing more simpler rules. We're institutional changes to address issues at their root. We're incorporating categorical parameters, which is kind of sort of like more complex versions of angraphic rules, and also things like pressure lotteries, which will be a preview to a sci-fi style once again. One question. So I wanted to ask about the mismatch, the target mismatch. Your example of the two years default was kind of interesting because I would have thought it's a policy decision made by the lender, whether what they care about primarily is for payment within two years.