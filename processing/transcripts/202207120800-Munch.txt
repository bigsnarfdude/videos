All right, so hopefully everybody can hear me. And I have no idea where the video is. So, hi, online people. I'm hoping you can see me. Right. So, let's see. So, first off, thank you to the organizers for having me. This is lovely. And I always enjoy having real in-person conferences because, man, it makes a difference. Sorry, online people. Hi. Right. So it was actually really helpful yesterday having Bastian give a version of talking about stuff related to topological data analysis because. Stuff related to topological data analysis because kind of gave me a sense of a lot of the stuff I wanted to talk about. Because I think there's a lot of interesting things that these communities can do that overlap and have some really good conversations. And so my plan here is to basically give you sort of a sense of several projects we've been working on and different ways of taking input data and getting it to a point that you can start using machine learning and deep learning and all these different things on it, taking a sort of topological view of your data. Taking a sort of topological view of your data. So, to that end, this is sort of the general TDA mantra, which is thinking about data in terms of shape. And there's multiple ways of interpreting this, which is either your data itself is shape. So you're interested in something like understanding shape and structure in the input data points. So in my group, we do a lot of collaborations involving plant biology and trying to understand things related to quantifying the shape. Quantifying the shape and structure of plants. I'll talk about one project in a little bit. Sometimes you're interested in taking the data points themselves and trying to understand the shape and structure of the data points. Sometimes there's cases, so for instance, like the time series example that we have down here, there's cases where you have the time series, which doesn't necessarily look like it's the kind of thing where shape could be something that's useful. But it turns out that analyzing shape and structure in those cases has a lot of interesting applications as well. Has a lot of interesting applications as well. So in my group, a lot of the things we do sort of boil down to the following pipeline. So there's a lot of versions of somebody hands you a data set, right? And one of the things that I'm really big on is if you're going to work with a data set, collaborating with somebody who really understands the application where this data set is coming from. And so once you have that collaboration established, what you can do is you can start trying to figure out. Can do is you can start trying to figure out the questions that you're interested in asking and then how you can encode information about whatever that data set is in some sort of topological structure, right? And so, we've got a lot of these topological signatures that give you all sorts of interesting ways of taking your data and trying to distill it down to some hopefully useful, important essence of the shape. And then, once you have these topological signatures, the next game is trying to figure out how you can get those things into some sort of machine learning anything, right? The topological signatures. Right. The topological signatures themselves tend to be kind of these really weird mathematical objects, persistence diagrams, barcodes, mapper graphs, read graphs, if any of those words mean anything to you. And so then trying to figure out how to get from there into some sort of statistics and machine learning framework to do analysis in a way that preserves the known structure of what you started with is a non-trivial task. So what we're going to do is sort of talk about these topological These topological signatures, in terms of well, okay. So, here's how I tend to think about what we want from topological signatures. How about that? So, when we come up with some sort of topological representation of your data, there's several things that I'm usually looking for when I'm trying to understand whether or not it's doing a good job. So, the first one is quantifiable, which basically means I can make a computer make it for me, which, you know, I really don't want to do these things by hand, if possible. Concise, meaning there's some simplification of the representation, right? Simplification of the representation, right? So there were questions yesterday about like whether or not these are lossy processes, and they absolutely are, but the goal is: can you distill down your information to the useful essence as opposed to losing a bunch of information you needed? Comparable. So we spend a lot of time trying to figure out metrics for these objects and how you come up with a distance between these sort of very strange structures. And once you understand how distances work, trying to make sure that anything that you're coming up with is robust. So if you give me a noise. So, if you give me a noisy version of whatever your true underlying manifold is, hopefully, the representation that I gave you isn't that far off from the actual ground truth. If it is, then you're not really doing a really good job here. So there's sort of two classes of things that show up in this work. There are tools that come out of here that tend to be more on the algebraic side. So Bastian talked a little bit about persistence diagrams. Talked a little bit about persistence diagrams and things like that, since those come from homology. Barcodes are just another version of representing this information. Euler characteristics and Euler characteristic curves, there's a lot of these algebraic sort of underlying structures that come out. And then there's another class of tools that we use that are more graph-based. And again, that's probably going to be of interest to this community, where the sort of output representation of our data is a graph. It's basically sometimes a skeletonization or some. Sometimes a skeletonization or some other sort of underlying representation of your data, but that comes in a graph form with some additional structure. All right, I'm going to try to talk about at least several of these. But overall, so this is sort of the main things I look for when I'm trying to get all of this data into some sort of machine learning pipeline, which is you first off want to find some sort of topological signature that actually does a good job of representing whatever it is you care about your data, right? And that usually involves really talking to the people that made the Usually involves really talking to the people that made the data and understand the data to get there. And then the second part is: once you have these things, how do you get from there to some sort of machine learning, deep learning architecture in a way that preserves the original structure, right? And the whole game is trying to do this in a way that respects the topological information. Okay, so time permitting, we're all going to find out together. I'm going to try to talk about three different versions of this. Versions of this. So the first one is sort of the more plant biology-based representations where we're using Euler characteristic curves for understanding shape and structure in barley. There's a bunch of stuff that we've been doing that involves understanding shape and structure in terms of time series, and in particular, graph-based representations of time series. And this is the one that we may or may not have time for, but talking about the more graph-based representations of data. Graph-based representations of data, and how we can try to start figuring out passing these things to machine learning architectures that respect the known underlying structure. Okay. Oh, yeah. What I'm not talking about. Okay. So I'm not actually going to talk about the directly getting graph input data into your machine learning architecture. Sarah's going to talk about that this afternoon, I think. Yeah. Wave, say hi. There we go. Okay. So I'm going to avoid that. And so she's going to give a lovely talk later on some of those. And so she's going to give a lovely talk later on some of those other aspects. Okay, so let's talk about Euler characteristic curves. Euler characteristics. So if you're ever in the position to start doing some sort of like outreach activity with like kids and you want them to like get excited about math and stuff, Euler characteristic is a really great place to start, right? So you might have seen this, you might have ended up on the Wikipedia page. I'm on there far more than I should admit. But the Euler characteristic is this really interesting simple quantity where if you give me any sort of Me, any sort of platonic solid, and you count the number of vertices minus the number of edges plus the number of faces. For all these platonic solids, it turns out that you always get two, right? Which is weird. Now, why Euler decided to do this, I will never understand. But so here's three examples with a tetrahedron, a cube, and an octahedron. You can do this with basically anything, any other sort of structure that looks approximately similar. So it turns out that this thing, which is usually written as It turns out that this thing, which is usually written as chi of whatever your space is, it turns out that this is a topological invariant. So, for topologists, we really just care about whether or not two things are the same up to like squishing and stretching and sort of moving stuff around without tearing and gluing. And so, in my brain, those are all basically the same object because they're all essentially a sphere. And so, the Euler characteristic says that because they're all essentially a sphere, the Euler characteristic is always two. Right. And it turns out that. Right. And it turns out that if you do this on other sorts of spaces, however you break up the space, however you decide to come up with vertices and edges and faces and split them all up, you end up with an Euler characteristic that's the same for that particular space. So everything we just looked at was a sphere. And so the Euler characteristic for anything that's topologically a sphere is two. If you do this on anything that looks like a disk, you get one. If you do it on anything that looks like a circle, you get zero. If you do it on anything, it looks like a circle, you get zero, et cetera. This is non-trivial that this works at all. This is, you know, several hundred years of mathematics trying to figure this out. Well, maybe like 100. Okay. But the whole idea here is that this is a sort of first representation of a topological invariant, because if you give me two spaces that are the same, I can promise you that the Euler characteristic is the same. And for our purposes, in terms of using it as a representation of shape and data, same doesn't necessarily help me because if you hand me two. Doesn't necessarily help me because if you hand me two spaces that have the same Euler characteristic, that doesn't actually promise me that I have the same spaces. For instance, a torus and a Klein bottle and a circle all have zero, right? But what I can promise you is that if you give me two spaces that are different, that def or sorry, hold on, two spaces that have different Euler characteristic, that definitely means that the spaces were different. Now, this is also a little bit overly simplified because my data doesn't usually come in these nice little topological spaces like this. So what are we going to do? So, what are we going to do with that? So, what we're going to do with this is sort of add another layer on top of it where we can use this Euler characteristic in a way that turns out to be incredibly powerful for shape analysis. So, the directional transform is this idea where I'm going to be interested in regions of RD. So, domains in RD. I probably want them to be connected to make some other things play nice, but that doesn't necessarily have to be the case. And what I can do. And what I can do is, I can choose a direction. So if I'm living in, oh, what is my notation says Rn. So if I'm living in Rn, I can pick a direction in Rn, which is actually a location on a n minus one sphere, right? And based on that direction, I can get a function defined on the points of the space. Okay. Turns out this is really just an inner product, if you like thinking about it that way. In this case, I've got my blue arrow going that-ish way, and the smaller values are at the top. And the smaller values are at the top. And if I were to draw actual contour lines, it would be perpendicular to where that arrow is going. Right. So, what this has done is this has taken my region of space that I'm interested in and stuck a function on it. And the reason we like functions is because we can mess around with sublevel sets. Okay, so here's what I can do. I've picked a direction. So right now my omega up there, that's my direction in my n minus one sphere. I should probably do this with actual numbers. So if I'm in the, if n is. is with actual numbers. So if I'm in the if n is two, oh wait, my pictures are now in n equals three. Okay, my pictures are n equals three. So that's my two sphere, which is a normal ball. So what I can do is I can look at sub-level sets of my function, which basically means pick a threshold. I think I got t up here, right? Look at the set of points in the region that have function value less than or equal to t, and look at the Euler characteristic of that thing, right? And then what I can do is I can mess around with t, right? is I can mess around with t, right? So the idea here is that for different choices of t, I get different Euler characteristics, and I can draw those, I can plot those in this Euler characteristic curve that you can see on the right, okay? So so far, there's no reason for you to believe that this is a good representation of my data because I picked a direction and I don't know why that's useful. And now I've got this sort of time series out, but it turns out that if you do this in all directions, if you can somehow mathematically give If you can somehow mathematically give me every possible or the characteristic curve for every possible direction you could come up with, this is a complete representation of the domain that you were interested in in the original RN, right? So this is a theorem by Kate Turner et al., where the Euler characteristic transform, which is this idea of taking, in this case, we're going to work with simplicial complexes in R3 and turning it into this weird collection of data that this is actually a complete representation. That this is actually a complete representation of that simplicial complex because this thing is injective. Injective means that if I spit out the same thing, they must have been the same before. Yeah. Okay, so how are we going to use that for data? Okay, one aside, and then we'll talk about how we're going to actually use it. Okay, so here's the data set that we got on this particular project. And this is always fascinating to me because the time scale for biology is just so much different. scale for biology is just so much different than math or like especially like CS conferences, right? So this experiment was started in 1929, right, in California. So Harry Harlan and Mary Martini went out around the world and picked up different samples of barley from different regions. They ended up with, I think it's 28 founders, and you can see all the different locations where they got these barley from on the left. And what they did was they The left. And what they did was they ran this artificial evolution experiment where they started by crossing every pair of founders. And so they got whatever 379-ish different starting genetic crosses. And they took all these things and they put them out in a field. And then they basically just let these things cross-pollinate. And I'm sorry for the biologists who are listening, cross-pollinate and make more baby barley and do this for like, but they did this and then they collected all the seeds. Did this and then they collected all the seeds from that, planted them out in a field, and did it again. And they did this for like 50 years. Okay, could you imagine publishing something after 50 years? So they did this for 50 years. There's a series of people who have maintained this experiment, starting with Harry Harlan and Mary Martini in the top and ending with our, well, ending with currently our collaborator, Dan Koenig, is at the bottom. And so what they did was they maintained all of this like dried specimens of all of these different barley. Different barley grains and all sorts of stuff. We've got a crazy amount of data. And so, what Dan Koenig did was mailed us, oh, I didn't write down numbers, hundreds of barley panicles. And my collaborator, Dan Chitwood, put it in his giant X-ray CT scanner. And what results is these X-ray CT scans of all these barley panicles, like that, the picture with the, this over here, this picture right here. All right. So now I get to do something because I have a Because I have a notoriously black thumb. Nobody lets me touch the plants ever. And so, what we did was take these x-ray CTs and break them up into pieces. So, everything I'm talking about is Eric Amezkita's work, so he's the person down there. And so what he did was he took all of these scans, he individually separated out all of the seeds from these panicles. So, by code, we didn't have to do that by hand, and we ended up with about 38,000 seeds. And so, what we were able to do is that we can do that. Thousand seeds. And so, what we were able to do was to take these seeds and use the Euler characteristic transform as a representation of the seed shape. Okay, how's that work? So, what we're going to do, I can't do mathematically every possible direction, so I'm going to do the next best thing, which is to just pick a lot. So, we took 158 directions on this, on each of these barley seeds, got an Euler characteristic curve for each of them, basically concatenate all of them, you get this giant, giant, giant vector. As you all know, you do. As you all know, you do not want to pass that many input variables without doing something careful. So, what we did was we used, I think this is KPCA for these results, some lower dimensional projections to be able to work with sort of fewer input data, or sorry, input variables. And so then what we can do is we can then start trying to do things with predicting founders based on just looking at the seed shape. So, in this particular, these particular results, all we're doing is looking at the founder line, not the stuff that came later. Founder line, not the stuff that came later. But what we can do is we can start trying to predict what founder your seed came from based on nothing but having this Euler characteristic transform information. Now, it turns out that, and I think I might have copied old versions of the results. I think we've got some better ones in the paper. Somebody can yell at me later. But what ends up happening is that we started by putting in just traditional descriptors into the model. So nothing but like length, width, height. But, like length, width, height, there were like 11 of them. I don't remember what the other ones were. And we were getting things, classification accuracy in the 50%. And again, for those of you that hang out around NIRIPS, biologists get really excited when you get numbers anywhere close to this, right? So these are not going to look as nice as the biologists got excited about. What we next did was we tried to just use the topological information and we did better. Nope, sorry. This is old versions of results. But in this case, we did not do better. But if we combine the topological Do better. But if we combine the topological description with the traditional descriptors, we actually did a considerably better job in terms of our classification accuracy. So the idea here, and I think the sort of take-home message of this is a couple of things. So first off, I don't think that TDA is some magical silver bullet for representing everything, but it is a helpful contribution to other sorts of things you might be working with. And if you can find ways to take your data and Ways to take your data and use topological descriptors in a way that's sort of relevant to the problem at hand, it's potentially very helpful. All right. I'm going to take a detour to the next guy. Questions on that before I keep going? And of course, feel free to stop me. I do have a tendency to talk fast. Yes. The function itself, this guy? The function itself, this guy. Yep. So, all this is, is an inner product with the direction vector. And it turns out that that just gives you a function that goes increasing in the direction of your vector. So what we do is we take sublevel sets. So here's a 3D picture of one of those barley seeds, actually, and we cut off the chunk of pieces at Chunk of pieces at multiple stopping points. So every time we stop. Or they're just saying it's a little loud in the back. I do tend to be loud. So what ends up happening is that you cut off at a certain threshold, you look at the sublevel set of that portion, you take the Euler characteristic of that, and you do it at multiple stopping points, right? So every time I stop this particular procedure, I get a different Euler characteristic, and that's what you're seeing here, right? So for instance, And that's what you're seeing here, right? So, for instance, if I stop at T2, then I have an Euler characteristic of apparently 10-ish. And if I stop at T4, I end up with an Euler characteristic of, I can't read graphs like this, one, maybe? Right. Yeah. Okay. Yeah. So, for this particular paper, what we did was we picked a lot, and a lot was 158, and we tried. And a lot was 158, and we tried a bunch of different values of that. Now, what's also fun, I unfortunately did not put the pictures in here. We did spend a bunch of time figuring out which directions are the most useful. So we were able to go back and take the machine learning output and figure out the, I think was the coefficients that were the highest and decide which directions were used the most. And unsurprisingly, so your barley seeds are kind of like a football with like a thumbprint in it, right? And it was mostly focused around the directions that sort of measured the thumbprint. So you don't, you know, in theory, you don't. Don't, you know, in theory, you don't need to use all of them, but you don't know that a priori, and so you have to kind of be a little bit more careful after the fact. Good question, yeah. Yeah, absolutely, yeah. Um, so and I know some statisticians were working on this. The last I had heard was like getting some more theoretical justification for you pick a bunch of random directions and see how it goes. Um, so yeah, absolutely. We decided to do just a bunch of. Absolutely. We decided to do just a bunch of sort of uniform directions. And even then, I think the way we chose it isn't quite uniform on the sphere, but yeah, absolutely. Yeah. Yeah, I'm also curious on that. Yeah, which would be super cool. Absolutely. There's some work, the mathematician types are doing a lot of things related to like, can I prove that I need finitely many, right? But the finitely many results tend to be like, I already know what my simplicial complex is, and so I'm going to know exactly. Complexes. And so I'm going to know exactly which directions I need in order to get it, right? But you can, I can promise you finite. It's exponential and something, but I can still promise you finite. But yeah, I think there's a lot of nice things to be done with figuring out the right number of directions and things. Okay. I'm going to talk about another project. Cool? Cool. Okay. So that was the easy topology. Now we have to go do the hard topology. All right. So persistent homology. And so, so this. Persistent homology. And so, so this is also again helpful because Bastian sort of primed you on this. I'm going to have a bunch of slides that are going to look disturbingly like Bastian's too. But let's start with where our data is coming from. So I'm going to start with data that's basically a bunch of time series. And so what's really sort of this really nice magical procedure is that a way to get shape out of time series is to do what's called either Tokens embedding or delay coordinate embedding, kind of depending on who you ask. But the idea is as follows. But the idea is as follows. So I have a time series. And again, my brain does shape. And so I really want this to be some sort of representation of the underlying state space. So what I can do is I can take my initial time series. So my time series starts as t values and x values. I'm going to pick two things. I'm going to pick a delay parameter and I'm going to pick a dimension. So my delay parameter is tau, and tau is essentially sort of how far off I'm going to look. Far off, I'm going to look from one of these input data points to the next. And M is my dimension, is how many of them I'm going to look at. And so, what happens is I can take, essentially, I can think of it as having like a comb of length M, right? So, M is three for the moment. If I look at the X coordinate values that are sitting on top of equally spaced T coordinate values for T, T plus tau, and T plus 2 tau, I get three numbers or M. I get three numbers or M numbers, depending on what dimension you're in. And what I can do is I can forget the fact this came from a time series and just treat it as a coordinate in space. So the three blue points that you can see over here on the left that are highlighted correspond to the red point over there that's in three space. All right. And so what ends up happening, and unfortunately, this is a stagnant PDF instead of a video, but if you slide those points along and look at a whole bunch of different locations for this, you end up seeing this red point go around this circle. End up seeing this red point go around this circle, right? So, what ends up happening here is periodicity in my contrived sine curvy looking thing turns into periodicity in a circular structure, right? And so, what's really nice is if, major caveat, you did a good job of picking M and tau. What I have is I have some sort of representation of what's going on with the underlying topological or what's going on with the underlying attractor, right? So, what we're going to do is we're going to use Is we're going to use this to do things like try to decide when parameter or when behavior in time series changes based on, say, changing parameter values or other sorts of things. So here's one example of looking at that. So this is the example of the sort of standard Lorenz butterfly. And so what I can do is I can fix a few of the parameters. So the Lorenz system is defined by these differential equations on the left. And I'm going to fix, ah, good, I wrote it. fix ah good i've wrote it down sigma to be 10 and beta to be 8 over 3 and i'm going to mess around with the row parameter right and it turns out that if you mess around with this row parameter you can get very very different behaviors in terms of your um in terms of your system right so in this case it goes from being a chaotic butterfly looking thing uh to being a periodic sort of circular looking thing and back and forth and all sorts of different stuff uh and the picture if for anybody who knows about this stuff the bottom picture is the lyapunov exponents um The Lyapunov exponents and the top is the local extreme information. Okay, so what we're going to do for a bunch of this stuff is basically just trying to mess around with understanding how I can represent the shape of the attractor and use that as some sort of input to a machine learning model where I can mess around with trying to predict these parameters or the behaviors. All right. So to do that, I'm going to do a So, to do that, I'm going to do a little mini crash course in persistent homology. So, this is your review from Bastion's talk. So, homology, the idea with homology is that if you give me an input topological space, it gives me back for nice enough assumptions a vector space. A lot of times, what we care about is just sort of the dimension of the vector space that tells us lots of things. But sometimes you want to have the full vector space information. And so, different dimensions of homology give you. And so, different dimensions of homology give you different information about the underlying structure, right? So, H0, so the zero-dimensional homology has something to do with connected components. One-dimensional homology has something to do with loops. Two-dimensional homology has something to do with voids. Three-dimensional homology, I lose my ability to give you any sort of like mental picture intuition, but it still exists and you can do it as high as you want. All right, so for different spaces, so in these particular examples, everything is connected. And so, I end up with a rank one, zero to. With a rank one zero-dimensional homology. But for instance, my circle and my torus and my Klein bottle have different numbers of loops, and that's shown by having different numbers of or different ranks of my first dimensional homology. My point doesn't have any loops, so it's got nothing. Right. And so what we can do is we can take input topological space, output vector space, and sort of see what happens with that. Now, the downside of this is that I give you a single space. Is that I give you a single space and I hand you back a vector space and you're supposed to do something with it. The sort of mantra of persistent homology is that we, pardon me, we don't necessarily care about a single stagnant space. I'm really interested in how a space changes over time. And that changing over time should be something that's, again, useful for whatever your data is. But here's my sort of oversimplified example, right? So what we can do is we can do things that are called filtrations of simplicial complex. That are called filtrations of simplicial complexes, which is where I take a simplicial complex and I'm just going to put in more and more simplices at every time step. And what I can do is I can measure homology at each of those time steps and sort of see how it changes. And again, if you've never seen this before, this is not going to make any sense, but there are ways to kind of keep track of all the algebraic information as you go, right? So it's not necessarily just that I saw two things and one thing and then three things. There's actually a whole bunch of relational information that's going on in the back. Relational information that's going on in the back. Everything I just said, it turns out is linear algebra, right? Go back and learn linear algebra again. I need to learn linear algebra again. All right, so if you can keep track of this information, it turns out that you can actually break this up uniquely into some sort of representation where you can see how the different things or the different structures persisted over time. So in this particular case, if I'm looking at connected components, I start At connected components, I started with two connected components because I got two individual vertices. Those two vertices merge into one, and then I see a new connected component that shows up at the second step. Then those merge into one and I get a loop structure, and then that fills in. So in this particular case, I saw two connected components that merged together into one, which means there was a connected component that appeared at one and merged in at two. And so I end up with a point in my persistence diagram, which is at one, two. So one Which is at one, two. So one means whenever that particular structure showed up, and two is whenever it filled in, right? I liked Fascia, you had creation and destruction instead of birth and death. It was much less violent. Right. Okay. So the idea here is that we can take input filtration information and get out this always 2D point cloud mass persistence diagram as some representation of my underlying structure. Representation of my underlying structure. All right, so let's talk about how you can get there from point clouds. Again, this is basically the same thing you saw on the slides yesterday. But if you give me some sort of input data that looks like a point cloud, and maybe I got it as a bunch of points that are embedded in Rn, and maybe I got it as just some sort of discrete metric space. So a bunch of data points where you have any notion of similarity. I can build what's called a RIPS complex or a Viatoris RIPS complex. Viatorus Rips complex. So, the idea is that one way of getting at this is you can think about it in terms of growing Euclidean balls around each of your vertices. So, whenever two of these balls overlap, then I get the corresponding simple, or sorry, the corresponding simplex between the vertices. So, for any fixed epsilon, so my epsilon is currently the diameter of those circles. So, for instance, this vertex and this vertex represents those two points. Represents those two points, the corresponding balls overlap, and therefore I get this edge in the middle. And then anytime I have the ability to put in higher dimensional simplicities, I do. So because pairwise distances between these three vertices are all less than epsilon, then I get the triangle in the middle. Again, you can do this with Euclidean space, but you can sort of generalize this to all sorts of weird things. Let me show you the picture with point cloud version first. First, right? So everybody starts with these sorts of pictures that look like point clouds that kind of sort of look like a circle. But the idea is that for different choices of this epsilon radius parameter, you get sort of a larger and larger simplicial complex. And again, what we're going to do with all this is start watching how the homology changes as you increase that parameter, right? So in this particular case, because I see an overarching circular structure that starts, oh, somewhere around that picture over there, which is the big circle, and then it kind of persists to the end over here. Kind of persists to the end over here. I've got some point that's far from the diagonal, which means that I have some sort of circular structure that persisted through these pictures for a long time in terms of the radius parameters. Now, again, I don't necessarily want to have to do this on point clouds. I can do it on other things. So here's my graph data version. So, for instance, if you hand me a graph, this is supposed to be an unweighted graph. What I can do is I can start thinking about these things in terms of short. Start thinking about these things in terms of shortest path distances between vertices. So, now for any two vertices, I've got a distance between them that's defined by how many hops to get from one to the other. I can come up with some sort of distance matrix. And it turns out you can pass this to the same sort of thing we were talking about before and still get persistence diagrams out. So, in this particular case, I've got two different loops that show up. Both of them show up at this step, which I didn't label. I think that's distance one, and then persist. One of them persists until you get here in this bottom. Until you get here, and this bottom guy fills in, and the other one persists until here, where the other one gets filled in. And so that shows up as two points in my persistence diagram, both of which start at one, and one fills in at four and the other at five, apparently, right? Okay. All right, so the other things we like to do with these, I mentioned from the beginning that we really like being able to come up with distances between the representations of our structures. So we have things like bottlenecks. So, we have things like bottleneck distance. For the sake of time, I'm going to skip most of this. But the whole idea is that you can come up with some way of essentially solving an optimal transport problem between your two persistence diagrams you're interested in and seeing how similar they are. And the other thing that we really care about is once you do that, you want to sort of make sure that things are stable. For the sake of time, let me move on from that because I don't think we need to do that. All right. The last thing, though, that I do want to mention is that there's been a lot of work. There's been a lot of work in terms of trying to figure out how to take persistence diagrams and spit out some sort of vector representation because, again, machine learning algorithms really like vector representations. And so there's a lot of different ways to do this at this point. If you're interested in sort of how all these different things work, feel free to come talk to me later. There's a lot of interesting stuff going on in this direction. But you can take your persistence diagrams and turn them into either functions or 2D functions or all sorts of other representations. The game with all of these is that they're all trying to make sure you preserve the underlying metric structure property, right? So if I'm going to have two persistence diagrams that are similar and pass it through your vectorization, you should give me back two vectors that are similar. Okay, so let's get to actual applications. Okay, so here's the idea. What I'm going to do is I'm going to take input time series and run them through. Time series and run them through this pipeline to be able to start trying to say anything about predicting what their behavior is, right? So you can take your time series, you can do Tachins embedding, you take the RIPS complex for a whole bunch of different epsilon parameters, that gets you some sort of, you can think about it as a distance matrix on a simplicial complex that spits out a persistence diagram, and then you can do whatever your favorite vectorization procedure is to get some sort of representation, right? So, this is sort of what happens at the end of that is that Of what happens at the end of that is that you can start thinking about sort of input time series that have very different sorts of behavior. So, in this case, this is an example from the Rossler system, which is another sort of standard dynamical system that people tend to use. I've got a time series that's from a sort of periodic regime and from a chaotic regime. And similar to the Lorentz system, what we can do is we can mess around with one of these parameters and see how stuff changes. All right, so how we decide what sort of behavior we're actually looking at is. We're actually looking at is this picture on the left, which is you can decide things about periodicity by looking at what's called the Lyapunov exponent. And so, what we did was we basically highlighted the regions of this alpha parameter that have, I think the highlighted bit is periodic bits of alpha. And so, what we can do is we can run this whole system through this pipeline. You end up just passing your machine learning algorithm some sort of vectorization of your persistence diagrams, and then predict on the output whether or not you're seeing something that's periodic. Whether or not you're seeing something that's periodic or chaotic from the start, right? So the misclassifications, it's a little bit misleading. The locations where the system was misclassified are a bunch of alpha values, and then we just highlighted all of the vertical strip of it. So there's basically like one, two, three, four, five, six, seven-ish locations where we got things misclassified. And anything you don't see was classified properly. Was classified properly. And so, again, what you're seeing is that stuff that's near boundaries that we know that there's something changing in the dynamical system is where we tend to misclassify stuff. No surprise. So, the downside of this, okay, upside first. The upside is that persistence does a really good job of measuring and encoding the overarching shape of the attractor. The downside is that point cloud persistence is just notoriously computationally slow. Just notoriously computationally slow, right? You have to do things like sub-sample and do all sorts of other things to try to get around it. And so, one of the things we've been thinking about is how do you mess with your input in order to simplify it so that the persistence isn't so bad. All right, so here's how we're going to do that. There's another way of getting at a representation of your time series, which is coming from more of a graph-based model. And to do that, we're going to look at this thing that's called an ordinal partition. All right, so the same trick with the Tawkins embedding from before, where I basically Tawkins embedding from before, where I basically had a delay parameter and a dimension parameter, and I look at sort of three points on my time series and I let that kind of move around and I try to see what happens, right? In this case, the only information though that I'm going to store is relative function values. And so that's basically the permutation of whatever it is you're seeing. So in the case of having a dimension of three, there are six different permutations of three numbers. And so the only options for what the shape of this thing can look like. For what the shape of this thing can look like are the six things that show up on the right. Order doesn't really matter, we just labeled them in pi one through pi six. All right, so the idea is that for this particular data point, if I stop there, I see pi five in our numbering system, and that gives me some representation of that particular data point. So, how do you get a graph out of that? You get a graph out of that by basically making a node for each of your permutations and including an edge when you And including an edge whenever sliding your window along moves from one of those permutations to another. And what you can do is you can then keep track of this graph. Now, again, if you kind of unfold the graph that you're seeing on the right, what you end up with is actually a circular structure. And hey, that's a good thing because I started with a sine curve and periodicity this way should turn into periodicity this way. So what we're doing is we're encoding all of our information in this graph structure and then using that as an input to all of our machine learning algorithms to do basically the same task. To do basically the same task. So, time series turns into this permutation sequence, turns into the ordinal partition network, turns into persistence diagrams and then vectorization. Right, so here's an example where we were doing this with, okay, this is again the Rossler system. This is the same system you saw before, where again, we're trying to figure out if the representation that's coming out of this persistence diagram-based. Persistence diagram-based representation of your graphs is actually getting you any useful information? And the answer is mostly yes, right? So the top row is the information about the local extreme of your system. The next one down is the Lyapunov exponent. So that's basically what we're using for ground truth. And then the next three, in this case, are three different vectorization single point representations of our persistence diagrams that we had. And for the most part, sort of the this middle one is getting the closest, but what we're kind of looking for is whether or not change. Whether or not changes in the Lyapunov exponent and therefore changes in the behavior result in changes in the persistence diagram vectorization parameter. And so for the most part, the answer is yes, although there's no clear split. What is nice is that all of this stuff is relatively resilient to noise. So this is the results of the differences between the sort of function values you get with SNR. So increasing noises that way, right? And so the more separated you can keep these things. And so, the more separated you can keep these things, the better a job you're doing of being able to differentiate between different sorts of periodic versus chaotic regimes. But there's always a but there's actually a lot of information we're dropping in this particular situation. So in this case, we dropped two pieces of information that were important. When you build this ordinal partition network, first off, you have a directionality that's inherent because you're always passing. That's inherent because you're always passing from one node to the next. And so you always know that there's a direction on your edge. And second, you've got information about how often you use the edge. So there's a weighting information that we were also dropping when we were just keeping track of the original stuff. So in the follow-up work, what we started doing was trying to see if you could mess around with different choices of graph distances and then passing those to your different persistence diagrams to try to see what's going on. So this is sort of the standard example that can break everything. So if I give you Break everything. So, if I give you a graph that's just a cycle and a graph that's a cycle with a single cut through the middle, and in this case, my outside edges are very heavily weighted and the inside edge is just one cut across. What ends up happening is that if we use our original representation of our data and we look at our persistence diagrams, no surprise, my top persistence diagram sees that one big loop because there's one point that's far from the diagonal, but my other one has two, which is basically because it sees that. Which is basically because it sees that sort of middle cut through as being important. So, what we've been doing is messing around with changing your distances on your graphs to see what happens. We use a bunch of different options. The one that I like the most is using diffusion distance based on information with the weighting, right? So you just hop based on relative probability at that particular node. And then that almost immediately gets rid of that second unnecessary loop. Right, so you can do other fun games with this. You can do things like this is: we took all of these representative persistence diagrams, we took all of their pairwise bottleneck distances, and we did MDS projections on the resulting diagrams to see how well they split up, right? And again, this is doing a good job, which good job in the sense of the things that are periodic and the things that are chaotic are relatively separated in. Are relatively separated in persistence diagram space, which at least tells me that there's something useful for me to do. The downside here is that this is MDS, which means I can't take a new persistence diagram and decide where it should be in this particular projection, but at least it's sort of a justification for persistence diagram space being something that's useful in terms of separating things out. Other things we're doing in this direction. Yeah, so. Yeah, so yeah, I'll at least say a few things. So, we've started trying to incorporate more of the information from these graph-based representations. So, for instance, everything I just said did not include the directionality, but it did include the weighting. And there's other sorts of variations of persistence that you can use that do encode other information that you might want. So, zigzag persistence has something to do with having arrows that go in opposite directions. We can do bifurcation analysis. We can do bifurcation analysis using something called Crocker plots, using other sorts of graph-based representations of data. There's a lot of interesting things that are happening in this direction. What I do want to advertise to this group, so we have a Python package with basically everything I just said and will generate a pile of data for you. So if you want some sort of like nice graph output model that you can use on your particular system, this will generate a bazillion graphs for you with some nice labelings. Nice labelings. And as with all academic code, there's always bugs. So if you find something, please feel free to let us know. All right. Let me take questions on that part because I'm not sure I'm going to have enough time to talk about the last bit. Yeah. So I'm wondering, Griffin, so do you reason from varying factor type between my pub gaps and from the so from the closest? So from a political perspective, I guess like there's no way you can distinguish points between like a sign is on frequency and a sign is like two signs of frequency not with the rigs we have yeah yeah so that so the question is what what kinds of things can we capture from the time series with these methods as opposed to other things so yeah so things like period doubling don't tend to show up right if you run around your super Right. If you run around your circle faster than not, it does sort of show up, and I haven't figured out exactly how to measure this yet, but it does sort of show up in terms of the zero-dimensional persistence based on density. Because if you go around a loop faster, it's not going to be as dense as if you go around it slower. And so sometimes the zero-dimensional stuff has been useful for that direction, but we're not entirely clear on how exactly those things work out. So, actually, Ismail's paper where he was messing around with this with cracker plots has some stuff about that. Crocker plots has some stuff about that. The bottom right thing. So, yeah, so a lot of what persistence in general is good at, and most TDA methods is sort of measuring global structure, right? So, if it's something that I can measure in terms of looking at a shape and saying something about its overarching properties, there are ways to mess with these things that you can do things that are more local. Although we haven't tried doing anything like that at the moment. So, there's things like local persistent homology and some stuff. Like local persistent homology and some stuff that can get you sort of more fine-grained stuff. But the easier things to get at are sort of like the big overarching structure information, right? So, if there's a way to rig your systems that you can see period doubling somehow shows up as that, but I haven't figured out how to do that yet. Yeah. Are there other questions? Yeah, yeah, go ahead. Possibly. Well, so it's encoded in some ways. So the graph-based representation I just showed you, the ordinal partition network, one way of thinking about it is that it takes your Network: One way of thinking about it is that it takes your state space and it breaks it up with xi less than or equal to xj hyperplanes. So it's basically just like you take sort of the middle axis and you just cut it up into a bunch of pieces. I don't have that picture up here either, but I can show you that later. And so what ends up happening is that you're just kind of moving from one region of state space to the next. And the closer you are to that diagonal, sort of the weirder stuff gets and the more noise gets in the way. There's other representations, graph-based representations. So one of them is the coarse-grained state. So, one of them is the coarse-grained state space, which is one of those things down there. And so, that does basically a better job of breaking up your state space into more useful pieces, right? It's harder because you have to have some sort of like data-driven approach to start, but at the same time, you can probably keep more of that information about proximity and stuff like that. And so, I think there's some interesting places to go with that. Yeah. Yeah. Um so starting about intentional. So the way like how you look at our time each of these snapshots shows like a different team. So like this this picture sort of relied on a lot of work on the transferability side on graphing the calculation and people like people are going to think that problems actually lie in the same Actually, like you would see into this snow space, but then you would basically like sort of like it emerges with like you didn't have to meet you with us the same thing so that they can transfer from as far as to eventually yeah. So can you comment on sort of what the other you say what we have on this different event or snapshot, like putting everything back to you? Yeah, so a lot of the sort of big picture of what we do tends to be looking at sort of one parameter varying families of things. So if you've got anything like that in your machine learning algorithm or whatever, like the sort of general viewpoint is that rather than looking at any one snapshot, you can sort of keep track of things as they pass along. Right. And so with the homology, one of the reasons. Right. And so, with the homology, one of the reasons homology is nice is that there's a bunch of math underlying what I'm saying, but you can keep track of which thing, which structure showed up over here and is persisting. And you can tell the difference between that and a bunch of things that are just flashing at you repeatedly, right? And so the thing though is that you don't necessarily have to use homology. There's a lot of different ways that you can use that viewpoint, right? So for instance, you can do the Euler characteristic version, which is where now I'm kind of watching something change and I'm measuring it based on Euler characteristic. Oil characteristic, but you can also do things where stuff is sort of coming in and out. There's a zigzag version, there's a lot of different ways that you can sort of play with this as watching some sort of interesting representation of something changing, right? And so the sort of viewpoint that I think is helpful, and it all depends on what your application is, is sort of, can you find a way to watch something change where either we tend to not look for like a single parameter value of like the right parameter value, but instead you can look at sort of overarching structural changes. Look at sort of overarching structural changes and sort of see what's passing along for the entire continuum. I'm not sure if that helps answer your question or not. Yeah. Yeah. Whereas I was just going to I was sort of working before that safe for you to have the keys for seeing the method safe. Yeah, so there's a lot of viewpoints that kind of generally go that way. And part of this is because so a lot of the simplified things are just inclusions from one to the next. But if you have any sort of association from one step to the next, there's a lot of information that you can that you can keep from there to there, right? So like, so like. You can keep from there to there, right? So, like, so like Sarah's talk later is going to be talking about sort of ways that you do a pooling layer that sort of keeps track of some of this stuff, right? And so, as long as you can kind of have some association from one step to the next about who's associated, there's some interesting topological things that you can keep track of. Yeah, so this might be a longer conversation later. We got the answer. Yeah. Ah, more questions on that before I yeah. Yeah, you absolutely could. So the trade-off is, and what I love about the other characteristic thing is that you never have to compute persistence, right? So the trick here is that I could absolutely do all of the same trick with the directional transform and come up with the persistence diagram. Transform and come up with the persistence diagram in that particular direction and then use that as my descriptor. And maybe that gets you more information. But then, A, I have to compute persistence in a bunch of different directions, which I absolutely do not want to do. And second, the Kate's theorem basically promises me that the ECT still has just as much information as the persistent homology transform, which is still blows my mind, right? But I should be able to get away with the shorter computation time without ever having to get persistence. In theory, Ever having to get persistence in theory, it'll still get you stuff. I, yeah, you gotta, it's the trade-off is: is it gonna get me more information based on the additional computation? And I haven't, yeah, these are relatively large scans. I don't want to do that. Yeah. Yeah. Oh yeah. Oh, yeah. Yeah, that's what I was thinking. Was your shape stuff yesterday? I think the directional transform would be a really interesting way of going about it. But yeah, you can absolutely take this directional transform idea and stick your favorite descriptor on top of it, right? So actually, okay, I'm going to do the three second version of the other thing I wanted to talk about just because I love these pictures. So we've been messing around with some embedded graph stuff. Okay, sorry, I'm going to do a bad job of slides because I'm going to skip all of the like lead up slides to the stuff I was going to talk about. Lead up slides to the stuff I was going to talk about. But one thing that you can do is you can come up with something that's called a merge tree. So a merge tree is essentially looking at a connected component information of sublevel sets. If you've seen dendrograms, it's a similar sort of structure. And what we can do is we can represent embedded graphs using these things. This is my merge tree directional transform on an embedded cat, right? And so what you can do is you can take different directions of your cat and look at the changing merge tree structure and use that as some sort of signature. Structure and use that as some sort of signature of your data and try to find out if there's like more interesting stuff going on. This was from an REU project. They really did not like the idea of changing the direction of the function. They really wanted to rotate the cat. So this is the cat in multiple locations, right? But what you can do then is you can take embedded graph information, represent it via, in this case, we were doing it as, so we had the merge tree for every direction. We have a definition of distances on merge trees. Distances on merge trees, you pick a bunch of directions, you can integrate over that and get a distance between pairs of embedded graphs. And so that thing over there is MDS on different embedded letters, right? So all the blue things are Zs and my MDS put them nicely far away and isn't that kind of it. But yeah, you can, you can, the point of this is that you can swap out that directional transform with a whole bunch of things on the other side to see what happens. So yeah. Well, I'm probably running into, oh, sorry. Well, I'm probably running into. Oh, sorry, was that a question? Okay. So let me just make sure. We already basically talked about this. So it's basically just a bunch of different ways of doing this. And, you know, the whole take-home message is just, you got to be careful when you're actually using topological representations of what's going on. But what I do want to do is make sure that I thank all the people that did work on this. So the undergraduate, the group members that were participating in stuff are highlighted on the left. And that's a picture of my group from this past year. From this past year. And all of my, thank you to all of my funding people and all of you guys for listening. So thanks.