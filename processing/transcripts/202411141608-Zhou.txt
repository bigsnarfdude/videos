Talk about gains on graphs. And this talk is actually based on our archive preprint: finite agent stochastic differential gains on large graphs. And this is a joint work with Raymond Hu and G. Hodo. So as a brief outline for today's talk, I'm first going to introduce our research motivation and the model setup. Then I'm going to talk about two theoretical results we have proved under our model. But the first one is the convergence of fictitious play. The second one is the semi-explicit construction of. Semi-explicit construction of the Nash equilibrium. So, as a very brief review for the framework of stochastic differential game we're talking about, here we focus on a non-cooperative game with n players on a finite time horizon. So each player has a state dynamics given here as an SDE with a given initial condition, while the coefficients Bi and sigma i are actually controlled by the strategy process alpha. And each player minimizes her expected cost as an Her expected cost as an expectation of the integral of the running cost Fi plus the terminal cost GI. And in this paper, we'll be focusing only on the Markovian strategy, which means that the strategy of a player i at time t is a deterministic feedback function phi i acting on the current time t and the current state vector xt of all the players. So, before getting into the model setup, let me briefly introduce. Setup, let me briefly introduce some related literatures. That there has been an abundance of finite agent gains and meanfield gains on complete graphs, which is kind of the most natural case to think about. One typical example is the systemic risk model proposed by Kamona, Fuk, and Sun. So after that, people start to think about integrating graph structures into the models. And this gives birth to the graph form infield games proposed by Keynes and Huang. Huang. So, I mean, normally when we think about mean field games, like we first have a sequence of finite player games, and then we take the number of players to infinity to do the mean field approximation. While Grapho mean field game takes exactly the converse idea that they first define the game in the asymptotic regime and then they project it back to finite player games to prove the propagation of chaos type result. So, in this case, this graph phone is actually not a graph. Graphon is actually not a graph to be rigorous, but actually the limit of a sequence of graphs. So, the graphon is something that exists in the asymptotic regime. So, here the player index set is actually this 0, 1 unit interval. So, there actually exists a continuum number of players. And this W is called the bounded graphome, which maps a tuple of player indices to a number in 0, 1. So, it can be understood as the weight of the edge. As the weight of the edge between two players. Okay, so the Graphon Minfield game, however, has a drawback, which is that this limit network is actually dense. So it's not capable of modeling the gains on relatively sparse graphs. So motivated by this, there's a group of researchers in Germany, like by Fabian, Trey, and Koppel. They propose the LP graph form infield gains. And the only difference is to change this bounded graph to an unbounded graph. Bounded graphme to an unbounded grapho and adding this LP condition here. And it turns out that they're able to model the game on relatively sparser graphs, but still not on an arbitrarily sparse graph. More recently, the same group of people proposed the graph x mean field games. So here, this w is called a graph x, and they changed the domain such that it's not i squared anymore, but it's 0 to plus infinity squared. is 0 to plus infinity square. So the graph fact is actually nothing but actually a stretched bounded graph form in the sense that now the domain becomes the player index stretched by the sparsity measure of the graph. So in this case they're able to model the game on an arbitrarily sparse graph. But of course you have a cost to pay which is that here this graph X mean field game becomes a mixed mean field game in the sense that there are two Field game in the sense that there are two types of players. One type of player is in the high-degree core and the other is in the low-degree periphery. So it complicates this mean field game a lot. So besides this sequence of development of graphon-type meanfield games, there has also been some investigation on the finite agent linear quadratic games proposed by Laban and Sorred. And this is also going to be one of the main references for our research. But their model. But their model is restricted on vertex transitive grass, while we want to develop something that works on general groups. So here comes our motivation. The first motivation is that we hope to model the heterogeneous player interaction by a graph. So this is kind of consistent with what we have seen in the previous literature in the sense that we hope to understand the graph vertices as players. So if two players have an edge, then they can directly interact with each other. Otherwise, they have to make indirect. Other, otherwise, they have to make indirect interaction through other players. On the other hand, we noticed that there's a prevalence of sparse graphs in real life, for example, social networks. And that's why the Graph-Home time info game cannot model the games of sparse graphs in a very good sense. So that's why this leads us to the investigation of finite agent games or general graphs. So let me start with our model setup that we have. Our model setup: that we have a simple undirected kinetic graph G with n vertices. So, I would denote the degree of each vertex as D, and each player has the state dynamics, running cost, and terminal cost given here. So, actually, this model very much resembles the systemic risk model in the literature. And the only difference is in this red term. So, in this red term, I have some square root degree here, and this summation is for player J, where For player J, where VJ has an edge with Vi, right? So actually, this red term can be understood as the mean reversion type behavior within the graph neighborhood of player I. So a natural question to ask here is like, why do we need these square root degrees? That's something I would explain in the next two slides a little bit later. But what we can observe for now is that when this G is actually the complete graph, then this model just reduces to Then this model just reduces to the classical systemic risk model. And actually, when this model parameters A, Q, and Epsilon are all taken as zero, then this model is exactly the model proposed by Lepper and Sorin in their paper. So this model actually contains the models in the previous literature as special cases. Okay, let me interpret these square root weights a little bit. So the first interpretation comes from the compact representation with the graph. From the compact representation with the graph Laplacian, which is one of the central objects to investigate in spectrograph theory. So, this graph Laplacian is actually a matrix defined as having the diagonal entries as one, and for the off-diagonal entries, it's non-zero if and only if this i has an edge with j, and you will put the minus one over the product of the square root of degrees. So that's where we see the square root of the degrees appearing here. And it turns out that we can. And it turns out that we can rewrite this red term using the notation of the graph Laplace but just the minus EI transpose L X T. So it has a very compact representation in the connection with what we have in spectrograph theory. Well the second interpretation comes from the perspective of a graph random wall, which is which is actually a procedure of substance transmission among a certain network. Among a certain network. So, here we want to guarantee the symmetry of this transmission along each undirected edge. So, which means if you check an edge between player I and player J, if you view it from one side and the other side, that should be symmetric. So, in particular, let me explain what this means, is that we can rewrite this red term. I just multiply the square root of dvi both on the numerator and the denominator here, so it changes nothing. But now this But now this becomes an exact average, and I'm assigning weight, the square root weight, to this player J in the mean reversion level of player I. So this means I can understand that the contribution of player J to the mean reversion level of player I is actually this PJI defined as the square root of the ratio of the degrees. Well, now if you check like conversely that the contribution of a player I state Player I state to the mean reversion level of player J, you will find that it's exactly square root of dvj over dvi, which is pij for this exactly same p. So this symmetry should be understood in this sense that if you view from the side of player i, then the contribution is pij, and you view it from the side of player j, then it's pji for exactly the same function p. So as a last remark for this model, So, as a last remark for this model, is that when we consider a regular graph, which means all the vertices share the same degree, then this red term just becomes the arithmetic average. So this is just the exact mean reversion behavior. Okay, that's all for our model setup. And now let me talk about the two theoretical results we have proved. And yes, describe what is the automatic strategy for each career. Oh, it's actually the Markovian strategy. It's actually the Markovian strategy. But then you don't have any restriction on the policies? No. No restriction, just Markovian. You can take feedback over all states. Everyone can take feedback over all states. Yes, yes, yes, yes. So it's a complete information game. Yes, each player can observe the state of all the other players, right? So there's no incomplete information here. So let me continue that the first result we have proved is for the convergence of fictitious play. So the fictitious play. Of fictitious play. So the fictitious play is actually a well-known iterative numerical method solving for the Nash equilibrium. So inside the fictitious play, each player would just optimize herself while assuming all other players use fixed historical strategies. And we would expect the convergence to the natural equilibrium after a sufficient number of rounds. So let me clarify a little bit here that by meaning this fictitious play, we are just actually referring to the fixed point iteration. So we are not taking average of all historical spaces. Of all historical strategies. And although this fictitious play is a very nice numerical technique solving for a match equilibrium, the convergence is not even guaranteed for tabular games. So that's why it's worth our investigation. And this is like the result we have proved in the paper, that if we add some mild conditions on the initial feedback functions fictitious play starts from, if we define a graph-dependent constant CL in this sense, it's a function of the graph lab. Is a function of the graph Laplacian and also the model parameters. If this Cl is less or equal to one, then we can prove that the steam produced by fictitious play converges to the Markovian match equilibrium. And the remark here is that this Cl is actually a graph-dependent constant, and the dependence of C L on the graph Laplacian is completely through the two-matrix norm of L. So, yes. That means you also have a reason that there is. Some reason that tells you that there exists a unique Markovian Nash equilibrium in the game, remote. Yes, yes. Why do you have a uniqueness? I think the uniqueness is from the linear quadratic game, right? Because this is a linear quadratic game. I think there's some general conclusion in the literature. And according to our proof, we first prove the convergence of Victoria's play and then we check the limit, we identify the limit, and then we prove that the limit actually corresponds to Markovian natural equilibrium. Okay, so let's continue to see some uh numerical experiments we have made for fictitious play. So in this plot, this is a game with uh fifty players on different graphs fixing all the other model parameters. Fixing all the other model parameters. On the axis, I plotted the fictitious play stage index, and on the y-axis, it's actually the log error for fictitious play. So, here all the curves are kind of like straight lines. So, that's why here we say we observe a contraction mapping-like structure for the error propagation of fictitious plate. And more interestingly, you can observe that these straight lines have different slopes on different graphs. So, this exhibit a graph-dependent rate of convergence. Graph-dependent rate of convergence. So, as we can see, like for the complete graph, the two-matrix norm of the graph Laplacian attains the minimum possible value. So, that's why the fictitious play converges the fastest on complete graphs. So, that makes sense, actually. However, what is interesting is that if we check this complete bipartity graph, then it's actually a bipartisan graph. And we know that if a graph is bipartisan, then the matrix norm of the graph Laplacian attains the maximum possible value. The maximum possible value. So, but in this numerical experiment, on complete bipartisan graph, fictitious display converges still faster compared to these graphs, right? So, there's something happening here, which means that the two-matrix norm of the graph-lapt fashion is actually not the only graph-related quantity that determines the convergence rate of intuitions. So, of course, we expect that there's some other underlying graph-related quantities that work. In this case, the unfortunate. That works in this case, but unfortunately, we are not able to find it and we'll leave it to some future studies. So, as a very brief sketch of the proof for this convergence, we just write down the value function of each player in each fictitious play round as this Vik. And since it's a linear quadratic game, we just put up the quadratic answers for the value function. And we make some simplification in this case that we assume a linear feedback function. A linear feedback function for each of the players, and by a simple induction argument, we are able to argue that the phi ik, the feedback function for each player in each fictitious play round, is always linear in x. And then we can change, plug this onset into the HJB equation to get this ODE. So, in this ODE, what I want to talk about is just that you can see that if all these FK in the iteration K is given, then this is actually an ODE with a then this is actually an ODE with respect to f k plus 1 with a given terminal condition. So if we know all the f's in the kth iteration, then we can solve for the f in the k plus 1 iteration. So that's how it propagates with respect to the round of fictitious plane. And then we prove the convergence of this fik t as k goes to infinity. Lastly, we identify the limit and check that the limit actually corresponds to the Markovian mesh equilibrium. So, then let me proceed to the second result we have proved, which is actually the semi-explicit construction of the Nash equilibrium. So, actually, this idea is very simple in the sense that if this graph presents a nice enough symmetry, then you would expect that you would have some easier ways to solve for the Nash equilibrium, right? And here we are focusing on this class of the graph called the vertex transitive graphs. So, a vertex transitive graph is a graph where the graph Is a graph where the graph automorphism group, the group action of this graph automorphism group on the vertex set is actually a transitive group action. So you can understand that in vertex transitive graph, all the graph vertices can kind of be understood as having this equal status. So that's where the symmetry comes from. This theorem that we've proved in this paper, we can check that if this graph is a simple connected but also vertex transitive. But also vertex transitive. Now, here it's necessary to add this extra assumption on the model parameter that this q squared has to be equal to epsilon. Okay, so this model parameter restriction cannot be removed because we have to make things decoupled in our derivation. But under this assumption, then firstly, the first conclusion we can have is that we check this ODE, which is a matrix value ODE with respect to R with a given initial condition. Condition. So this matrix value ODE has a global existence and uniqueness result, where this Q here maps a matrix X into the determinant of some function of X to the power of 1 over N. And it turns out that if we solve this magical ODE, then for this R, then if we get this R, then we can easily explicitly construct the Markovian equilibrium based on this R. On this R. So the explicit construction is given here. And the point is that if R is norm, then we can plug in this R and R prime here to get P. If P is norm, we plug in P here to get FIT for each player I. And finally, this FIT gives us the alpha hat I, which is the Markovian ash equilibrium strategy for each player. So that's an explicit construction. So two remarks here. So, two remarks here. Like, first, why do we care about this semi-explicit construction of Nash equilibrium, right? Because it provides an efficient construction of the numerical baselines, especially when n is large. So, if we check this table of the running time comparison here, so this baseline construction refers to the construction that's numerically solving all the Riccati equations. So, let me remind you here that there are n coupled Riccati equations here, and each Riccati equation. equations here and each Riccati equation is in dimension n by n. But if we use the semi-explicit construction we only have to solve one matrix value of ODE in dimension m by n. So we save the factor of n in this sense. As you can see when there are 128 players, the semi-explicit construction only uses the time to construct all the numerical baselines. While the second remark is that this result is actually generalizable to the mean reversion dynamics within the more broadly Reversion dynamics within a more broadly defined neighborhood. For example, the L neighborhood. So the L neighborhood I'm referring to here is a collection of vertices that are exactly L steps away from the vertex VL. So in this case, I'm replacing the red term with the term here, where I'm summing with respect to this J that Vj is in the L neighborhood, and I'm replacing the weights here with the number of paths from Vj to Vi of length, exactly length L over the size of the L neighborhood. L over the size of the L neighbor. So in this case, you can do some calculation and you can show that this is nothing but having the same form as what we've seen before. Except the only difference is that we replace the graph lab fashion L with the polynomial of L. But in this sense, all the derivations and proofs still works. So in order to generalize our result, you only have to replace the graph Laplace with this polynomial of element, or the conclusion still holds. Holds. Let's look at still some numerical experiments on this seemingly explicit equilibrium. So this is a game with 50 players on a complete graph. So on a complete graph, our model is nothing but just a systemic risk model. And on the left-hand side, I'm plotting out the equilibrium state process, and on the right-hand side, the equilibrium strategy process. So if it's a systemic risk model, then we know that there exists a closed-home solution. So we plotted the Closed home solution. So we plotted the baseline construction, closed-home solution, seeming explicit construction, and we see that they perfectly align with each other. But for a more general case, we change the complete graph to a cycle graph. The cycle graph is also well known to be vertex transitive. Well, in this case, we don't have the closed-form solution anymore, but we still have the baseline construction, semi-explicit construction. Well, they still align with each other perfectly in this set. So these serves as. Is set. So these serve as numerical evidence that our semi-explicit construction is able to provide the correct numerical baseline. Okay, lastly, let me talk very briefly about how, what's the idea of this semi-explicit construction. So similar to what we've done before, we write down the value function of player i, put up a quadratic answer since it's a linear quadratic game, and turn it to the Riccarti equations. So there's Equations. So there are some interesting things I'm doing here for this Riccati equation: I'm collecting all the interaction terms between player I and other players into this definition of P. So I'm going to do something with this P a little bit later. But I just write down the Ricardi equation with a terminal condition and I summarize some of these parts in this P. So the reason I'm doing this is because one of the main technical tools we're using for the heuristic construction is the fixed. Using for the heuristic construction is the fixed point iteration for this p. So this means that when p is given, we can explicitly solve this Riccarti equation such that each f i t can be represented as a function of p. So we get f1t to fnt as functions of p, but recall that this p is defined as a function of f1t to fnt again. So I can plug these things into the definition of p to get back a p. So this provides Get that P. So, this provides a consistency condition for this P. So, that's one of the main techniques. And the other one is about how to explore symmetry. So, when it comes to symmetry, of course, the best tool in mathematics is algebra. So, that's why we make use of the regular representation of the graph automorphism group to exploit the symmetry of the group. So, from this regular representation, From this gradical representation, what we can prove is that we first find the key quantity, eta t, which is defined here. So if you look at the right-hand side, you will find that this quantity actually depends on the player index, i, right? But through the symmetry of the graph, what you can prove is that this actually doesn't depend on the player index. And this quantity is actually one shared by all the players. So we find something in common for all the players. In common for all the players. That's why it's called the key quantity. So, just for the purpose of completeness, I've written down the definition of sigma here: that sigma is some function of p tilde and p tilde is some function of p. And this p is exactly the same p. And the next step is that we would like to find a characterization for this key quantity eta. So, I'm representing this eta, so this is exactly by using the fixed point argument I've mentioned a little bit earlier. I've mentioned a little bit earlier. We can write this eta as some expression that only contains the model parameters and eta itself. And then what we are doing is that we are replacing this interval term that contains eta, relating it to be r, and then using some matrix calculus theorems, we can turn this equation exactly into the ODE. So that's why this ODE appears in our sini-explicit construction. Explicit construction. So that's the main idea for this construction of the Nash equilibrium. Okay, the last little part here is for how we prove the global existence and uniqueness for the ODE for R, right? So if you check the ODE here, you will find some Q' here. But keep in mind that this Q is actually some determinant to the power 1 over n. So this determinant can be understood as the Determinant can be understood as the product of eigenvalues for symmetric matrices. So, this Q should be understood as some geometric average. As a result, we would expect that the right-hand side will not be a global ellipsoid function. So, in order to prove the global existence and uniqueness, we first prove the local existence and uniqueness by the standard application of the Picard-Lindov theorem. And then we turn this ODE to investigate it on the spectrum of RT, and we prove the concatenation. Of Rt, and we prove the concavity of this function. The concavity actually provides a uniform a priori bound for Rt, and combining both parts, we get the global existence in. So as a summary for my talk today, we have actually proposed a finite agent linear quadratic game on general graphs. We proved the convergence of Victoria's play on general graphs, showing numerically a graph-dependent rate, and we talked about the seemingly explicit construction of. The seemingly explicit construction of the natural equilibrium of vertex transitive graph. And this allows us to efficiently construct numerical baselines. So these are the main references for my talk today. And I thank you for the attention. Any questions? Yes. So when you at the beginning, why you feel Uh when you sh uh at the beginning when you show the convergence of the petition, you have some smallness assumption, right? Yes. And then do you think that uh by doing so instead of just directly taking the previous uh policy and then doing fast response to the previous policy self-average over time will help you to relax this efficiency? Oh, uh yes, uh that's a nice question, but I would say uh if we only consider like the last iteration that's already Like the last iteration that's already very hard. Like, because it's a feedback control, so there's a lot of things coupled together in this sense. So, like, since in the proof we've considered here, like, only involving the last iteration things has already become very hard. So, that's possible, but I don't know whether we have enough, like whether we will meet some technical problems when dealing with it. Any other questions? Have you thought about extending to the common noise case? Oh yeah, the common noise case, of course, is possible, yes. But yeah, it's a nice, I mean, it's a nice future work for us to do. But here, like, just for simplicity, we have not introduced that. What do you think would be the challenges there? Or do you think it just easily boasts? Since this is like a finite agent game, I think it's not gonna be that hard to add a common noise here. I would expect it to be maybe there will be some extra coupledness here, but I would expect that that difficulty can be overcome since it's a finite digital game, not a mean field game. Because adding common noise for mean field game is another story. In the interest of time, the last question from this question. Mostly very nice talk. Yeah, same introspection question. Yeah, I'm just curious. So you've been thinking about, well, you mentioned thinking about how graph properties translate to your results. I was just curious to know your introduction. What about, especially since you're considering sparse graphs, what's your intuition about how expansion will play into your results? What expansion? Like graph expansion. You know, if you have an expander graph, it's very sparse, but somehow the information travels very quickly. So I was just wondering. So I was just wondering how, like, it's kind of the opposite of the graphic. I was wondering how that would affect the results. Like, you have a very give an expander graph, basically. Oh, expand your graph. Yeah, I see, I see. Oh, yeah, an expander graph, yes. Actually, we've tested that. So, yes, yes, that's a very good question because when I was doing this research, I expect that on expanded graph there might be something different because it's a well very Something different because it's a very well-known type of graphs that, although it's a sparse graph, it has a high connectivity. So, yes, here you can see I did this for the Ramanujan, which is kind of a well-known spectrograph, right? Yes, we tested that, but unfortunately, we found nothing special for that. Yes, yes. Unfortunately. Okay, thank you again. Okay, thank you again.