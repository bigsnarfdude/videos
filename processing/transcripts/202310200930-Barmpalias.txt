The topic is: how do you diagonalize against path random tree? A tree we are in the counter space is path random if all of the paths are random. Okay, let's see. There is a motivation behind this topic. What happened? It used to work. Okay. Okay. Right. Okay. So algorithmic randomness appears in different forms. But perhaps we're most familiar with the infinite bit sequences, the reals, when the real algorithm will be random. Real algorithm to be random, it doesn't have patterns, but you can increase the dimensions. You can look at a matrix, an array, a tree, a set of points, a structure. When is a set of points random? You have to define some probability measure. And there is a whole area called random closed sets where you define such measures based on a measure on the set of points. The general sort of topic is: how can you change from one form to another while preserving the randomness? For example, say you have a tree. Can you effectively transform any tree to a perfect tree while preserving? While preserving randomness of the paths, that sort of thing. The answer is generally not. But as a topic, other forms are considered distributions. How can you transform something that is random with respect to one distribution to something that is random with another with respect to another distribution? To another distribution, and there the Bernoulli sort of trick is known where you have a Bernoulli distribution of a known bias, and you can effectively transform something that is random relative to that into a uniformly random sequence. And there is a whole area in probability theory and information theory based on such variations. There is also work on effective aspects of this. Diane Venue and Monique have written a very nice paper on this that also involves non-computable distributions. How can you effectively transform something that is random to that to a uniform one? We focus on search of random points and close at some particular Close at some particular. So these are represented by trees. And we look at while requiring that all of the paths are random. We look at basically how fat the trees are, how often they branch, how many accumulation points they have. And with respect to these properties, whether it's How whether it's possible to transform one to another. I think it's not surprising that the difficulty is to go from something thin to something fat. So say if you have a tree which is just one path, a random reel, can you effectively produce a perfect tree from that? Can you sort of manipulate the bits? Then you sort of manipulate the bits so that the answer is no. So basically, branching is increasing the branching of something that is in conflict with preserving the random. Right. So, the motivation that were some questions of one way, he wanted. One way he wanted to study what happens between weaker lemma and weaker lemma, which are two well-known principles in reverse mathematics, basically compactness. So we can't dilemma says that every infinite tree has a path, and weak Wicker's lemma every positive tree that is a tree, a positive measure, a set of paths to the tree. A set of paths to the tree positive measure, and he wanted to do that by using trees of positive measure. For example, the principle P says that every positive tree has the perfect sub-tree. So you don't only require one path. On the other hand, you do require that the tree is positive. You can weaken this and say every positive. Weaken this and say every positive tree has an infinite countable family of paths. It's easier to find something like that than something perfect. And TLA says that every positive tree has a positive perfect subtree. So not only perfect, but also positive. And the project was to separate all these principles. It's not so hard to show that in the To show that in the recursive mathematics in RC naught, sigma-one induction, these implications fall. And the idea was to separate these. We have done everything except for minus and p. Okay, and I think we were nearly there. So Well, these are we build omega models for that. So I think induction doesn't matter. Yeah. So yeah, the idea for the separations is to build an omega model, which is basically an idea in the the Turing degrees. I I don't know. I don't know. I don't know. Right. So, if some basic terminology is a tree is pruned, if it doesn't have dead ends, that condition can be important. It's proper if it has infinitely many parts. It's proper if it has infinitely many paths, it's positive if the measure of its path is positive, it's uh path random if it's all of the paths are random. And a variation of that, a stronger version of that, is path incompressible when there is a fixed upper bound on the randomness deficiency of the paths. That's also sometimes important, but no, not always, I would point out. Not always, I would point out the case. So, again, the question is: with respect to this sort of branching density characteristic, can the randomness be effectively manipulated in the sense that from a tree you can get something more fat that increase the branching and still preserve randomness. So, the initial point in this project was a theorem of Ludovic Patek, who showed that there exists a perfect path random tree which does not compute any complete extension of the annual infinity. So, what does VA come from? Well, DA is associated with WKL. So, models of weakening slow. Uh, models of Wikiman's lemma are built by the eight degrees. Okay, so uh this concerns the first separation, or rather the separation more precisely, WGL with T, not T plus. So the first step was to improve this to separate WKL from T plus. And if you want to build a model, you must have something slightly stronger. One version you can use is that every positive container. Okay. So the first theorem was that there exists a positive perfect path random tree which does not compute any complete extension of VA. That was an improvement of the Mel that they are in that separate speed, and every port should contain. That was independently obtained by Greenmere, Miller, and Nick around the same time. So, these are the questions for this talk: Can every tree compute? So can every tree compute a proper tree? So a tree with one of many paths, maybe one path. Can it contain a proper tree within the path incomparable trees? Can every perfect tree contribute? So I'm increasing the intensity of branching as I go down. Can every perfect tree compute a positive tree? Can every proper tree contain a compute a perfect tree? And somewhat. Somewhat more generally, can every sparse perfect tree be effectively densified, transformed into a density tree? So we will see precisely how we can measure the frequency of dense. With respect to path incompressible and path random trees, in case you got confused, this is the case where it doesn't matter so much. Where it doesn't matter so much. So, Dennis Herschel should jocus, and we independently observed that every path, a perfect path random tree computes a perfect path inconversible tree. So, in fact, it has to have a tail. A tail means you go to some node and take the tree after that where the deficiency is. Where the deficiency is bounded. So, this is more or less for the same reason that if you have a Python class of Python class of randoms, then it contains the fail of every random real. But the condition. But the condition that it's perfect is important, because otherwise, if you try to follow the book without that condition, you may end up somewhere where you just have a single path. Of course, efficiency will be founded there, it's around the part. Okay, so just taking a step back, this question. Back, these questions can be seen as analogs of randomness extraction. So you have something somewhat random, and you want to extract something that is perhaps more random or random in a different way. Again, there are many papers on this topic. But the difference is that the increase now is not on the randomness, but it's on the structure. Randomness, but it's on the structural density, so the density of the branching while you try to preserve the randomness. So it looks a bit like that. So you have something thin, maybe you have only one branch, or maybe only one accumulation point, the mid point, or countably many, and you want something perfect, or even a positive measure. Or even a positive measure. So, as you increase that type of dimension, then what happens is a lot of times you lose the path randomness condition. Okay, path random trees, they sound like random trees, they're not random at all, okay? They're quite hard to produce as random elements. Hard to produce as random elements in a space with respect to some distribution. Why is that? Can you feel that? I mean, what happens is, okay, say you want to define what is a random set of points. One natural way to do that is following the smarting of definition, you say what? They avoid all small sets where you say you are in the handler space. We'd say you're in the hand of space. And the problem is that if you have something perfect with branches all the time, avoiding even a small set of wheels is hard. So if you follow this natural approach, and there are ways to do that, you end up that random trees are basically all have finitely many paths. That basically that, okay, we know randomness for finitely many paths. So they're kind of hard to produce generically. And there are precise restrictions that characterize how often can they branch. And if you're familiar with previous papers about random trees, About random trees, they are different. For example, if you get a tree that is random with respect to a branching process, these trees contain non-random paths. So this is joint. Okay, so the first theorem is a characterization of what type of Of what type of density, branching density they can have. So we say that, say, if you have an increasing sequence, we say that a tree is perfect if each node of length ln has at least two extensions, length ln plus one. And the theorem is that the following are equivalent. So there exists an L perfect path random tree if and only if If and only if there's this one that is path incompressible, if and only if this condition holds, which is basically a condition derived by the for L and the E lemmas. So what does it say? Well, it says that LN has to grow reasonably fast. Okay, so you cannot require that it always branches very soon. Now, this might be. Very soon. Now, this might be counterintuitive because you say, okay, well, I have a tree of positive measure. Of course, the good branch. But the difference is that this requires that every, every, every time, that's the difference. Okay. Okay. With regard to the question whether you can effectively densify back incompressible tree. The theorem is that as long as, say, we restrict the perfect tree, as long as you satisfy a condition along the lines of the previous theorem, you can do it with high probability. I don't know, probably not always, but. Probably not always, but I don't know. So any sparse, computably perfect path, incompressible, computably means some computable function can be effectively transformed into n squared n log n. This is the kind of growth indicated in the previous theorem. A perfect path in comparison to tree almost surely. What does this mean? Well, it means that you have a truth table. Well, it means that you have a truth table operator that once you fix L and M, that operator will change one, the L perfect tree, each L perfect tree to something that tries to be L perfect. And it always preserves the randomness. So the condition is that if what you start with is pattern compressible, so is Is but incompressible, so is what you end up with. And the high probability concerns the second that sometimes it may not be imperfect. So with very small probability, the probability is the uniform probability on this type of trees. So with small probability, it may fail to branch along some path. You may have a yeah on a tree. It should like to I'd like to go into the proof, but I won't. What happens in the proof, though, is quite indicative to the problems that we usually have on this topic. That analyzing, avoiding fast random treatments, the topic of the talk. And the difficulty is that, say, if you want phi, phi is what you construct. If you want phi to map every path random tree to Every path random tree to always a path random tree. This means that if one path of the image gets compressed, you randomize, you want to immediately be sure that every tree that maps to some tree containing that path that got compressed has a compressed path. Okay, so you see the damage done, then you see all of the trees. It's done, then you see all of the trees that might, especially if you have a TT function out, you can see immediately all of them, and you want for each of them to be able to compress. Okay, how easy is that? It could be you can only compress that many parts. Depending on what this inverse image looks like, it could be that you can't compress. So in that case, you would have a problem, right? Because you would have Right? Because you would have a tree which is not path incompressible and something path incompressible that maps to it. And the promise is that if what you start with is path random, then the image has to be, so you break your promise. So it seems that all of the theorems I'm going to present have this tension. And And we deal with it in various ways. I think we have not reached the bottom of these. So I think there is a theory where things are nicer and more uniform with respect to these results. So in this case, we commit to that requirement. So whenever something gets randomized, we make sure that, but we have We make sure that, but we have to be a bit more loose with the presenting the perfect property. Um this the kind of says uh something about the Something about the previous ones that if you allow isolated path, then it's kind of easier to preserve randomness. So this theorem says that if a real is random and computes or even enumerates a path incompressible tree of unbounded width, it's proper, then it's two-in-complete. Okay, so if you have only one random real. One random real it's difficult to just introduce branching and expect that all of the files are random. Hersky, Jokos, and Sup obtained a similar result for perfect trees and two random. So they show that if you're too random, just random and incomplete, if you're too random, then you don't compute. Random, then they don't compute a perfect path under chip. And they use some non-trivial results to obtain that. Our proof is direct. So what's the corollary? If you think about it, think how can I produce a bath random tree? Maybe I start with something random, then I split it in various ways. Random, then I split it in various ways. Well, this doesn't work usually unless you're complete. So if you have a random real and you split into columns, you know that the columns will be random by bugs and Estherium. However, the deficiency. Of course, just be careful. This may not be a tree may not be closed on the Or three, which may not be close on the units. So, this is what it looks like. If you have one column, then it's difficult to sort of spread it out and still preserve the unit. And the more general statements look more like that. If you have a few, maybe some isolated parts, and it's difficult to just introduce branching everywhere without increasing the deficiency. I don't know if there's some, it may remind, I mean, this has the previous result has to do with incomplete randoms, right? You know something about randomness. Incomplete randoms differ from complete randoms a lot. So for example, they don't compute PI degrees. There are some similar results in a paper by Lauren Bienvenu and Chris Porter called Deep Glasses, but their theorem is for computably bounded, that the branching is computably bounded. But they did do the finite analysis that we also. finite analysis that we also did but they did it first uh that uh if you if you split a random source into k even a finite string k many a random sources without a significant increase in the randomness efficiency uh you cannot uh this is about as hard as uh computing the k lit out problem and there is a similar characterization for uh guessing at completion for k-bit segment Of the J-bit segment of PA by 11. So, what this says is that these infinita statements have finite analog. Right. So, you can derive that from this, that the class of path-random trees is negligible in the sense of buging. In the sense of Bugin. Bugin has this notion that the class is negligible if you cannot support it with a computable measure. So if you cannot define a probability, so that with positive probability, this doesn't apply. And yeah, it's hard to find such a measure on the space of trees, but it's not only a computability issue, as I said in my question. As I said in my question a few days ago, it's actually difficult to construct such a measure. I tried, even without a computability condition. I believe that you can use neutral measures which are fixed points, but these are quite artificial. I get a mixture to support this class. Okay, Abba, I've already asked this question. The next theorem is the other separation in the first market table. There exists a perfect path random tree which does not compute any path random positive tree. Again, the same style. You have perfect, but now you want to densify it somewhat, make it positive and perfect. So, and what you need for the model is. What you need for the model is that every possible truth contains a perfect set of tree. Actually, the condition is stronger, but you cannot even enumerate. This has been simplified somewhat. At the beginning, it was forcing with sets of positive measures. Then one way One way simplified it, and now it's a reasonably forcing, reasonable forcing argument. However, he does use a lot of intuition from generalized Poisson point processes, which is basically what I described when I was saying, when do you call a set of points random? It avoids small sets of measure. The problem that it would. Measure. The problem that we cannot use that directly, or at least I can't, I don't know how to do that, is that if you take such a probability measure on sets of points, the random closed sets have finitely many paths. Okay, so it's not useful for this type of theorem. However, it turns out we do need this intuition in the forcing argument. So, see, this probability measure is defined on the FEL topology, which is also known as heat or missed topology, where basically you define the probability of hitting a set of points, a simple set of points, and then this extends to if you have some functional capacity functional, you are to the whole place. So there is this intuition that we don't actually use the measure for some measure. Unfortunately, my dream was to use that measure. I tried a lot. In the end, we had to do the forcing. So this is what I said. This forcing arguments. This forcing argument smells for some processes, but it doesn't actually use it. Okay. And finally, okay, that's the more recent sort of result. There exists a proper path in comparison to tree, which does not compute any perfect path random tree. So say you have something thin, we call this the Something thin, we call this trees skeletal because they only have one accumulation point. And everything else is isolated. So we know that if you only have one path, it's easy to diagonalize. However, if you have, so we want slightly to increase, make it proper, and still be able to diagonalize. Unfortunately, the former argument works with. The former argument works with measure, the back measure. Here, I cannot use measure because if I use Poisson measure, we get the problem I mentioned before. So you do need to diagramize directly. So I started doing, I was planning to give a technical talk, however, I stopped. Uh yeah, actually I shouldn't talk about this. Do you have any questions? I have a conjecture. So the conjecture is that, sure if it's true, but the conjecture is that a path incompressible tree, which is skeletal in the way I showed you before, if it computes a perfect path random tree, it also computes the path incorporate. Maybe it's too good to be true. I'm just trying to sort of emulate Sort of emulate the theorem about one path against inhuman paths to bring it to the case of skeletal versus a perfect tree. I feel that there is something behind that that I haven't uncovered completely. So I think we're really close to getting this model for the remaining inspiration. The remaining separation. I think if a conjecture like that, not exactly that, would make this very easy. And there is also the question about doing this using some kind of probability measure. About that later one, I'm not optimistic at all. May not be, I tried a lot. Thank you.