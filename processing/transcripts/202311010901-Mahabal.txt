Classical analysis of machine learning. I'm from Caltech and the Center for Data Driven Discovery. So machine learning in astronomy has been on the rise. It's an understatement. You all already know that. There were great intro talks on the first day by Paolin and Sunin, and that gave a very good setting for many of the things that we are seeing later on. And as the data sets get bigger and bigger, we need to have more responsible machine learning, which means that things should be interpretable, we should understand what we are doing and all. We should understand what we are doing and all that. Last year, we held an IU symposium in which my co-organizers included Jess and Gwen. And the proceedings should be coming out anytime. That was about the pitfalls and the promise of machine learning in astronomy. Many great talks there. And more of those are happening, so people are realizing what they should do. But with more complex methods, and especially as the libraries become easier to use, there is less of an onus of. Of an onus of responsible use because it's so easy to take a data set, run something on it, and get something from it, right? And that's why it's even more critical to do things. If you look at model interpretability, what you see is that if you use something like decision trees, that was mentioned on the first day, they're very interpretable. You can see there's a decision being made, you go either here or here. It's easy to understand. They may not be the best ones because there are complexities that not all decisions are so black and white, and you need to have various branches coming back. Have various branches coming back that need to be loops and so on. And even random forests, which are essentially made of lots and lots of decision trees, but taken not exactly at random, but in an algorithmic way with subsets of parameters and subsets of trees, it becomes less interpretable as you see from this diagram. And deep learning is even further out, very little explainable. But the thing is that these techniques generally have better accuracy, they have better performance, and so on. So that is something. Performance and so on. So, that is something that we'd like to have. And that is why the arrow that you see here, the expectation is that there is a Pareto surface that could be formed, and we should be going in that direction where we not only have more explainability, but continue to have the predictability, prediction accuracy that we have. And so, with interpretability and explainability, explainability is where you try to have built into the model whatever you want to be able to explain or get out of it, whereas interpretability is typically post-hoc. So, you run a method. Post hoc. So you run a method, get some results, and then try to say why you got those results by doing various things. And with machine learning methods, most methods really, if you put garbage in, you're going to get garbage out. But if you don't know that you have put garbage in, the problem with machine learning is that you always get an answer. And if you start believing those answers, then that's not always a good thing. We talked about label noise in training samples earlier. That's very crucial. And what we tend to do is that we do a lot of plugins. tend to do is that we do a lot of clustering when we don't have labels. And for instance, t-SNE and UMAP-like methods have been on the use has been on the rise. But there are many hyperparameters that are involved in there too. So even there one has to be really very careful when one tries to do that. Because again there when you run t-SNE out of the box, you get a diagram, you get a distribution. And that is something that may feel that it's fantastic. Let's use it. But again, one has to remember that there are things that you may want to get. You may want people. Outliers, we didn't talk much about outliers at this meeting at all. We should try to rectify that tomorrow because that's something that is everywhere, and those are the ones that help us push the boundaries of our understanding, both in-class outliers as well as out-of-class outliers, where you may be able to find entire families of objects that you have not seen before. And with deeper surveys, bigger surveys, more time domains, that's something that's maybe more important. And of course one has to keep on using the simplest method possible because there are many methods that one could use. Because there are many methods that one could use. Rather than throwing deep learning at everything, if random forest does the job, that's what one should be using. But while keeping that in mind, we should be moving towards foundation models. Foundation models are on the rise in the last few months. There has been forest. If you are not your SAM, you should give it a try. Even if you don't work in any domain, that's segment anything model and read the paper on SAM. It's a fantastic paper which goes into the degritties of so many things that go with foundation models. The foundation models. Foundation models. In the foundation models, what becomes possible is zero-shot learning. You can take something that has been trained on one set and you can move it to another set, and it's expected to work almost out of the box. And most of the times it does. So rather than transfer learning, which involves a few other things, retraining, you can also be able to use that. So one of the things that we should be going after is generating golden data sets. And by golden data sets, these are not only data sets which work fantastically with test cases, but also cases which are ambiguous. But also cases which are ambiguous. So, just like when you write tests, you also write tests that fail so that you know that when it fails, it has failed correctly. Similarly, with golden data sets, you should have ambiguous examples there, which you know where they should be going when your classifier runs on them. So, a couple of things that you can do right at the start when a project is starting. Data sheets for data sets, if you can start doing that, that'd be really good. It's like a calling card, the data set telling you what exactly is in it. You, what exactly is in it and where you can use it and where you cannot use it. For instance, careful reflection on the course of creating, distributing, maintaining data set, including any underlying assumptions, potential risks or harms. There's a risk or harm, it's a general thing coming from medical and other things, but even in astronomy, where you should not be using a particular data set, it will be quite useful. What are the implications of use? And then the data set consumers, they have the information that they need to know to make informed decisions as to whether they can use these big. To whether they can use these big error bars, whether the wavelength coverage is appropriate, or at the ages, things are going bad, and all those things. But 10 years back, we wrote a paper, Anita and Vinay were co-authors in 10 Simple Rules for the Care and Feeding of Scientific World. And I think that's still relevant. Being able to do some of the simple things is the best useful thing there. So, just like the data sheets, what one can do is use model cards. What one can do is use model cards. When you write models in machine learning, they come with, again, cards with them telling the consumers what they are capable of, where you can mix them. And if these are there, then it's quite easy to start putting things together. And this is a fairly complex diagram. It keeps on increasing, getting better and better again, more on explainability and interpretability, which method can be used, where, what are the additional things that you can be doing with those, etc. I won't talk about that, don't have much time, but we can. Talk about that, don't have much time, but we can talk more about that. So, some simple approaches that you can do in any of your methods is understand feature importance, which features are important for which. And again, when you are trying to do multi-class classifiers, it may not be one set of features that will be useful because for some class, some features are good, for some other class, some other features may be good. So, you have to keep that in mind rather than trying to get just one set of features for everything, especially for a large set. Do perturbations, activations, gradient visualization, surrogate models are very useful. So, you do. Are very useful. So, you do a deep learning model, it gives you great results, you don't understand it, then you try to come up with a set of decision trees that can reproduce results somewhat like that, and that will make you understand what the deep learning model may be doing. And probabilistic models, of course, they are great based in neural networks, uncertainty internet estimation, multi-council dropout, and so on. We should be doing more and more of that. One example that I mentioned is we have been doing some of this in the GW Skynet model, so medium Cabero and So, Miriam Cabaro and me, and Jess, we did not jump this off audience at all. So, Miriam and Jess and we wrote a paper on GW Sky and taking only public data. It was great, but we didn't exactly understand why it was great. We went on to use GW Sky Multi, which not only said yes or no, but also a black holes or not black holes or not also. And then the paper that's going to be released now is going to give the explainability in the Give the explainability in the diagram on the right shows that. So, I'll leave with this slide which talks about the bottlenecks and opportunities. So, I'll just go through that. So, we need more generative models, including adversarial examples. I think that's the critical part. We need to know what is what effectively combining diverse data and working with real data is very crucial and critical also. So, simulations are great, but real data come with real noise, and understanding those is very, very crucial. Then, exploiting power of archives, not just going after the latest shiny such. Just going after the latest shiny surveys that we have with all three that have been accumulated, we should forget that answering fabrication explained. And what I'm looking forward to is something that you see on the right: latent spaces, similarities in different training sets, combining of time series and spectral energy distributions, and quid basis blocks for VNS, etc. I'll stop with that and leave maybe a couple of minutes for questions. We've got time for one long or two short questions. I saw on one of your slides one-shot learning. Zero shot. Zero shot learning. And that's something I'm always wondering about with the zero shot or I assume one shot you're like