Catherine Rio from Sydney, who will talk about well the links between reconfiguration and Markov chains. And I'm very, very interested in that because often when I write an introduction to a reconfiguration paper, I hand wave that there might be some connections to. And I hope that I will be able to be more precise in the future. Well, let's see, you tell me at the end. Let's see. You tell me at the end if you think that makes sense. Thank you very much for the invitation to give a tutorial. Today I'm sending greetings from the other side of the Pacific and also from tomorrow because it's Wednesday morning here. I'm going to try to indicate some connections between Markov chains, particularly the mixing time side and reconfiguration. And I'm using my sort of teaching setup. So feel free to ask if you have questions. Feel free to ask if you have questions. I'm going to try and monitor the chat and we'll see how we go. But first, I thought I should see if I can tailor this to your interest. So my current understanding of what people in reconfiguration are looking at, and this is probably quite incomplete, but in particular, I read Naomi Nishimuri's very nice survey. So I believe you've got a finite set of solutions to some combinatorial problems. To some combinatorial problem. And you've got some operation that you perform on these solutions to get from one to the other. So this gives you an idea of adjacency: how many steps does it take to get from one solution to another? And so you can form a graph, which is the reconfiguration graph on this set with the notion of adjacency giving the edges. And because it's combinatorics, I imagine that usually the set omega is exponentially large as a function of the size of the problem instance. As a function of the size of the problem instance. Right? And it also appears to me that you have many interesting structural questions that you're looking at. Some of them are kind of local in the sense that you give an X and a Y and you want to know whether there's a path in the reconfiguration graph between those two solutions or what is the length of the shortest such path. And some of them are global versions of those where you want to know if the reconfiguration graph is connected and what its diameter might be. And what its diameter might be, and other questions, no doubt. And then there are algorithmic versions of those questions. And if you've got algorithmic questions, you've got computational complexity as well. So does that sound fair, like a fair kind of overview-ish? Yeah, people are nodding. Okay, good. So I'm going to be comparing the mark of chain stuff to that. And I noticed that, um, That. And I noticed that some of your problems are really hard. Even deciding reachability might be intractable for some problems, just discovering whether there's a path from X to Y. Whereas if you're doing Markov chain mixing, you make a lot of assumptions on your problems and everything is essentially as nice as it could be. So I'd say your problems might be harder than ours in that sense. Okay, so in Markov chains, the stuff that we do, I'll just talk about discrete time. I'll just talk about discrete time and finite but exponentially large state spaces. So that's normally a set of solutions to some combinatorial, an instance of a combinatorial problem. So it's basically the same as your omega. And there are some allowed transitions, which give you a kind of notion of directed adjacency initially, but there's also a probability distribution on those. So from a given state, you choose what to do next with some probability. Do next with some probability according to some rule. I'll make all this precise in a moment. And so that gives you a kind of directed graph underlying the Markov chain, which is storing the set of all the possible allowed transitions. And even though it's directed for the time being, soon we're going to make an assumption that means it won't be directed. And then it will really be the same as the reconfiguration graph. Yeah, and as I already said, omega is big. Omega is big, like really big. Okay. So, more formally, a Markov chain on a state-space omega is just a stochastic process that starts somewhere and then it walks around kind of randomly, which has this memory-lessness property, which means the probability that your next state is y, conditioned on the whole history up until time t, only really depends on. Only really depends on where you are right now at time t. And so you can start at a fixed or random position, and then after that, your xt will be the random state at time t. And because I'm using OneNote and sliding, I'm not animating, but you've got some example here of a random walk on a graph, a hypercube. So you start somewhere, and then at each time step, you randomly choose. Step, you randomly choose one of the three directions in this case and walk to the neighboring vertex. So, in n dimensions, you choose a direction with probability one on n each, and you take a walk. And if you keep doing that a lot of times, you'll end up at a random vertex. That's a simple example. Another example is a Markov chain for graph colourings. So, let's have at least three colours and an input graph G, and we can look at all Graph G, and we can look at all the proper k colourings of G. So, of course, you just don't want adjacent vertices to receive the same colour. And we'll call the set omega k of g, the set of all k colourings of G. Then the easiest way to walk around on this set is probably to recolour one vertex at a time. And this is called the global dynamics or the single site update chain. So, yeah, at some random colour. You add some random colouring X, you just choose a vertex uniformly at random, you choose a colour uniformly at random, and you try to recolour the chosen vertex with the chosen colour, and you move to that new colouring if it's a proper colouring, and otherwise you just reject these choices and stay where you are. So, in this example, if we propose to recolour vertex four with yellow, we have a look at vertex four and we see that would be fine. 4 and we see that would be fine. So we would go ahead and make that recoloring. But if we propose to recolor vertex 3 with the colour red, we see that we can't do it because 3 already has a red neighbor. Okay, you just repeat this many times and you will randomize the colouring. Cool. So we're starting in this, so for the purpose of this, we're starting at Purpose of this: We're starting at a proper coloring when you're analyzing. Sometimes we start at a non-proper coloring, and we know that after we've recolored each vertex, it'll stay proper forever. So you can expand it to non-proper colourings, and they're all transient states. Okay, yeah. Okay, so let's make some more nice assumptions about our Markov chains. The Markov chains are not a very important thing. The Markov chains that we just saw, those two examples, I didn't have to like check my watch before I decided what to do. The transitions are the same no matter what the time is, and so we call that time homogeneous. And I'll always be talking about time homogeneous chains. And for such a chain, you can store all the transition probabilities in a big matrix called the transition matrix P. You have to fix an ordering on the elements of omega, the state. The elements of omega, the states. So you just fix some arbitrary ordering and order the rows and columns according to that ordering. And then the xy entry of p is the probability that your next state is y, given that your current state is x. Okay. All the rows sum to one. But the matrix, because the state space is huge, the matrix is also huge, and you can't work with the matrix in polynomial time directly. And in particular, you can't calculate the eigenvalues. You can't calculate the eigenvalues in polynomial time. Oh, and I forgot to say why are we talking about Markov chains on Omega? This was supposed to be on my first Markov chain slide. The reason why we introduce a Markov chain on a set is we're interested in sampling from that set according to some distribution. So we want the stationary distribution, which I'll describe soon, to be what we want. And then we can use the Markov chain for sampling. So the aim. Sampling. So the aim is basically sampling, just in case. Or, you know, you just look at it because it's fun and you like analyzing Markov chains. One of the two. Okay. So because we have transitions stored in this matrix, you can look at the corresponding directed graph, which just has edges for every pair, an edge from x to y, if the probability p x y is positive. And as I say, currently. And as I say, currently this is a directed graph, but soon it won't be. We'll make another assumption. And you might have self-loops. So if there's a state put probability, then you might have a self-loop at a given state. For example, in the graph coloring chain, when we choose a vertex and a color, we might choose the color that already exists at that vertex, and then we definitely stay put. So these are similar ideas to adjacency and reconfiguration graph, except at the moment. And reconfiguration graph, except at the moment they're directed. Soon they won't be. And then a nice property that we want is for the state space to be connected under the moves of the Markov chain. And we say that the Markov chain is irreducible if this happens. So this is very similar to the connectivity question, except it's strong connectivity because the reconfiguration graph, the underlying graph at the moment is directed, but not for long. But not for long. And we do not usually even think about what's the shortest path between these two states, or at least I never do. So we're usually not at all thinking about diameter. And I guess it's because we're walking randomly. So we're not optimizing anything. Okay. The next nice property is aperiodicity. So for this, we want some notation for the distribution of The distribution of the state at time t conditioned on starting at a particular state x at time zero. So if your initial state is little x and you take t steps, then this p t x is the distribution of the state after t steps. And then we look at all the ways you can take a walk in this market chain and get back to where you started from and the number of steps you can do that in. And we take the greatest common divisor of that. Common divisor of that length, and if for every state that greatest common divisor is one of all the ways you can walk from x back to itself, then your Markov chain is called aperiodic. And we want to rule out any kind of oscillatory behavior of the chain by making this assumption. So, in this example, we saw before of the walk on the hypercube, we're walking on a bipartite graph. So, you can see that any walk that starts at x and comes back to x will have starts at x and comes back to x will have even length and the greatest common divisor will be two. But to break this it's enough just to make sure you've got one sort of self loop somewhere and then because we're assuming if it's irreducible then you can walk around you can get somehow to this self loop you can go around the self loop and make an odd path that goes from X to itself so just having one So, just having one self-loop in an irreducible chain is enough to get a periodicity, but usually, to keep things nice and symmetric, you would actually probably put a self-loop with the same probability at every vertex. Let me not draw them all on. Okay. So, we like irreducible aperiodic chains, and in fact, we give a chain a new name. If it is irreducible and aperiodic, we say it's ergodic. And aperiodic, we say it's ergodic. And we like ergodic distribution, ergodic Markov chains because they have unique stationary distributions, which are limiting distributions. So what is the stationary distribution of a Markov chain? It's essentially an eigenvector with eigenvalue one. It's a row eigenvector. We usually write it on the left. So it's a probability distribution. It has non-negative entries which add up to one. We ordered them. Up to one, we ordered them in the fixed ordering that we put on omega to make the transition matrix, right? And if you're in the stationary distribution at time t and you take one step with your Markov chain, then at time t plus one, you still have the stationary distribution. That's why we call it stationary. So you stay in the stationary distribution forever. Okay, so ergodic chains, this is sort of classical theory, they have a unique. Classical theory, they have a unique stationary distribution. Because the graph is irreducible, the stationary probability is positive everywhere. And because it's ergodic, it is a limiting distribution, which means no matter where you start from, after t steps, if you let t get bigger and bigger, the probability that you're in a particular state y after t steps just tends to pi of y. And in particular, you've forgotten. And in particular, you've forgotten everything about your starting state, x. So you've completely forgotten about that and you're just following the stationary distribution. And you can get a whole lot of this stuff out of the book, Markov Chains in Mixing Time by Levin, Paris and Wilmer, which is a really good resource. So I'll just pause there if there are any questions. It was really just so I could have a sip of water. It was really just so I could have a sip of water. Okay, so a question you can ask now, or was that a question? Sorry, no. You could say, all right, you've got a Markov chain. How do you find the stationary distribution of the chain? But actually, we do it the other way around. So we usually start with the stationary distribution because we want to sample from a particular stationary distribution and we design the chain so that it has the correct stationary distribution. It has the correct stationary distribution. And the tool that we use to design the Markov chain to have the right stationary distribution is called the detailed balance equations. So a Markov chain satisfies detailed balance with respect to some row vector psi, let's say, if the product psi of x p of xy is always equal to the product psi of y p of y x for all pairs of states x and y. Pairs of states X and Y. And if this holds for some psi, or especially some non-zero psi, we will say that the Markov chain is reversible or time-reversible with respect to psi. And the point is that if you have a ergodic Markov chain, which satisfies detailed balance with respect to some psi, then psi is essentially the stationary distribution of the chain. To get the stationary distribution, you just have to normalize. You just have to normalize psi. In particular, if m is ergodic, then you know the stationary distribution is positive everywhere and the p's are positive everywhere. So this, what am I trying to say first? If you have detailed balance with respect to psi, then all the entries of psi have the same sign. That's the first thing. So you can normalize to get a distribution. To get a distribution unless you have the zero vector. Secondly, because the stationary probability is positive everywhere, this will imply that if p of xy is positive, then p of yx must also be positive. So if you have detailed balance, then any time you can get from some state x to some state y with a transition, you can also get back. And this means you may as well treat the graph underlying. The graph underlying the Markov chain as an undirected graph. Okay, so by the time we have the detailed balance or the reversibility, the graph underlying the Markov chain really is essentially the same thing as the reconfiguration graph, I would say. So that's nice. And basically, I only look at chains which are, oh God, it can satisfy detail balance. Cool. So I think that's. Cool. So I think that's everything on that slide. So we're going to make those assumptions from now on. Let's see how this looks for the graph colourings example. So it can be shown that the graph colouring chain is irreducible if you have enough colours, at least the maximum degree of the graph plus two. And that allows you for any vertex to recolour the vertex with a different colour, different to the one it has and different to the one. Different to the one it has and different to its neighbours, and that gives you enough room to sort of walk around. Yeah, it's easy to see that the Glaba dynamics, as I described it to you, is aperiodic because you always stay put with probability at least one over k because you can choose the same colour that V already has. So that's straightforward. That means this is an ergodic Markov chain. And finally, if the two colours differ in exactly If the two colours differ in exactly one vertex, then to get from one to the other, you have to choose exactly the right vertex and exactly the right colour. And the probability of doing that is exactly 1 over kn. And that works whether you're going from x to y or from y back to x. And so we see here that, well, the transition matrix is symmetric because everything else is zero and there are some numbers on the diagonal. You can also say it satisfies details. You can also say it satisfies detailed balance with respect to the vector 11111, because if you put 1 times this here, it's obviously equal to 1 times that. And that tells you that the stationary distribution is going to be uniform, because we get the stationary distribution just by normalizing this vector, right? So anytime you want a uniform stationary distribution, you want to have a symmetric transition matrix. Which is what we say here. So, this is how we use the detailed balance equations in order to construct a Markov chain that has the stationary distribution that we want. And if you want a uniform distribution, then you want a symmetric transition matrix. But this assumes you've already got some way of proving irreducibility because this machinery works for ergodic Markov chains. However, aperiodicity. However, aperiodicity, as I've said, is usually just achieved by putting self-loops everywhere. Okay, so now we've put a lot of conditions on the market chain. It's very well behaved. We know it converges to its stationary distribution eventually. And the question is, how quickly does it converge? And how can we get a handle on that rate of convergence? We want to be able to say for some fixed small quantity epsilon. Small quantity epsilon, how many steps I need to get within epsilon of the total of the stationary distribution. So the user wants to be able to control that tolerance. And that means we need to talk about convergence, which means we need some idea of how far a distribution is away from another distribution. And we use total variation distance. So if sigma and mu are two distributions on omega, Distributions on omega, then the total variation distance between them is just a half of the one norm. So half of the sum of the absolute difference of the values across the state space. And we use this half because this is actually equal to the maximum overall possible events of the probability of that event. Actually, let me put sigma first. Probability under sigma of that event minus the probability of that event under mu. And you don't need the absolute value. And you don't need the absolute value signs here. So, this is a nice way that you can bound the total variation time using any event that you want, and that's why we introduced the factor of a half. Right, so then with this distance in mind, we can define the mixing time of the Markov chain, of a ergodic Markov chain. It is a worst case overall initial states, X. States X of the first time T at which this total variation distance between the stationary distribution on the one hand and the distribution of the state of the Markov chain at time t. So that was this notation we introduced earlier, ptx. If you start from initial state little x and you take t steps, you've got some distribution. How far away is that distribution from the stationary distribution? And we want that to be less than epsilon. And we want that to be less than epsilon. And if you take tau of epsilon steps, then you guarantee that no matter where you started from, you're within epsilon of total of the stationary distribution. And I'll just remark that the convergence is monotonic, the convergence to stationary, which means that if you're within epsilon at time, oh, I missed, I don't know. If you're within epsilon distance from stationary at time t, then you're going to. Distance from stationary at time t, then you're going to stay, you're going to get closer and closer to stationary after that. So that's why we can just look at the first time at which this happens. And it's monotonic for a ergodic Markov chain. Okay, so then we say that the Markov chain is rapidly mixing if this mixing time is bounded above by some polynomial in essentially the size of the instance, which we take to be the log of the size of the state space, and the logarithm. Space and the logarithm of the inverse of the tolerance epsilon that the user provides. So notice that tau is a function of epsilon up here. And sometimes you see it just as tau of one quarter. I mean, you can put any small number in there, but if you want, the user can control epsilon. So I'll just say that this rapid mixing condition is a pretty strong condition because we're saying that over an exponentially large Saying that over an exponentially large state space, we want to get exponentially close to uniform, to stationary distribution in polynomial time. So you'll only have explored a tiny fraction of the state space when you spit out your final answer. But because you're going in all possible directions, sort of you've spread out. Okay. Good. So that's mixing time. So that's mixing time and rapid mixing. Now, really, what's controlling everything is the eigenvalues of the transition matrix, which are also called the eigenvalues of the Markov chain. But as we said earlier, the state space is so huge that you can't actually calculate the eigenvalues directly in polynomial time. So you quite often just want to creep up on them. So we have methods to try to bound the mixing time of the Markov chain. Mixing time of the Markov chain. And a nice method is called coupling, and there's also a variation called path coupling, which helps you to define a coupling. But I won't be talking about that today because I think it's less related to reconfiguration graph. I will be talking about geometric ways to sneak up on the eigenvalues. Oh, I should say this coupling thing is a probabilistic type idea. Idea. And a third sort of approach is functional inequalities, which aim to get bounds on the eigenvalues quite directly. So these are pretty technical and difficult to pull off, but they often give you better mixing time bounds because they're more direct. All right. So the rest of my time, I'm going to talk about these geometric methods to try to bound. Methods to try to bound the mixing time. So, first, I'll ask you a question. There's two cartoons here of a state space, and which one do you think the chain is going to walk around fastest? Yeah, the left, obviously, even though I haven't really given you any info. But in the left, you imagine that there are lots of the transitions that can move freely around. It basically doesn't have any bottlenecks, whereas on the right, we have this bottleneck. Necks, whereas on the right we have this bottleneck where either there are very few transitions going from left to right or the probability of those transitions is very small, something like that. But these constrictions clearly would make it difficult for the chain to escape and you would expect exponential mixing time. And if you don't have those, you would expect good mixing time. And Jeremy Sinclair introduced something called conductance to try to make this precise. It's really a sort of It's really a sort of surface area to volume ratio. And then you can actually bound the mixing time in terms of the conductance both from above and from below. So you can use it to prove slow mixing. But then the question is, well, how do I estimate this quantity called conductance, which I haven't really defined for you? And this is what I want to talk about, a method that was introduced in order to get a hand along. In order to get a handle on conductance, ignore this too. That's not really there. And the method is called canonical paths. So people used to go from the canonical paths to the conductance to the mixing time, but now we can go directly from the canonical paths. Well, now since 1992-ish, we can bound the eigenvalues directly using the canonical paths. And I'll explain how that works. What you want to do for all pairs of states. Do for all pairs of states X and Y, you're going to define very clearly and specifically a path from X to Y called gamma X Y. And every step in the path is a step in the Markov chain. So it's a path in the reconfiguration graph. And you have to have clearly and well-defined, there should be no guesses when you figure out what this path is. So that's the canonical path from X to Y. That's the canonical path from x to y. Then you look at the set of all of them and you say, Is there any one transition in this state space which is carrying a really heavy load? Essentially, you can think of the number of paths, but as we'll see, it's also weighted by the stationary distribution and a little bit weighted by the length. And if you had a constriction in the state space, then it's going to be hard to avoid having lots of paths going through these. Paths going through these few transitions that get you from one side to the other. So the load will tell you if you've got some sort of constriction. And conversely, if there's no heavy load on any transition, then maybe you're in a nice oval shaped state space, as in these cartoons. But I was thinking about this for this talk. We don't look at the lengths too hard. We do have to get an upper bound on the lengths, as you'll see. Have to get an upper bound on the lengths, as you'll see, but they might not be the shortest paths. We're trying to do something else, we're trying to make sure not too many paths are using the same transition. And I wondered, it almost feels like these requirements are sort of at odds. If you're trying to avoid a heavy load on a transition, maybe you have to bypass that transition and take a detour. And maybe that makes your paths a bit longer than they would be if you wanted a shortest path. But I haven't, that's not anything. I haven't, that's not anything. I'm not saying anything mathematical there, just sort of thoughts, you know, feelings, talking about feelings. Okay. Right. So I want to tell you more about the canonical path method. And that means it's time to talk about eigenvalues. So we take our chain, which is nice, nice, nice. It's time homogeneous, ergodic, and reversible with respect to some stationary distribution. Distribution and temporarily I will let capital N denote the number of states. Then the classical theory tells us that the eigenvalues of the transition matrix are real. The largest one is one, and it has multiplicity one. And then all of the rest of them are strictly less than one and strictly bigger than minus one. This strictly bigger than minus one here is because we assumed it was aperiodic. So that keeps the smallest eigenvalue away from minus. The smallest eigenvalue away from minus one. And then, when you can think about diagonalizing matrices and so on, you'll see that the powers, how quickly the powers of p tend to the matrix with stationary distribution, essentially, in each of the rows, it's controlled by the eigenvalue other than one, which has the largest absolute value. So that's either going to be lambda one, if lambda one is close. Lambda 1, if lambda 1 is close to 1, or it could be the absolute value of lambda n minus 1, the last one, the smallest eigenvalue, if that's very, very negative and close to minus 1. So we have to sort of take both of those into account. And we call the biggest of those two quantities lambda max. And then we can get a upper bound on the mixing time, which has this factor one over one minus lambda max here. And then another. Here and then another factor which depends on the log of the inverse of the smallest stationary probability and the log of the inverse of the user defined tolerance epsilon. So that's where this dependence on epsilon comes in here. And you can see this discussed in a really nice paper of Alastair Sinclair that I'll be referring to a couple of times. This quantity out the front, the inverse of one minus. out the front the inverse of one minus lambda max is the relaxation time and it's either going to be equal to one over one minus lambda one or one over one minus the absolute value of lambda n minus one and we only need to worry about that if it's equal to one plus this negative number lambda minus one okay usually for most combinatorial chains lambda one is going to be the the thing that's actually in charge of Thing that's actually in charge of the convergence. And so people typically didn't want to bother with the smallest eigenvalue, lambda n minus one. And one way to just take that, to make it go away, is to make your chain lazy, which is, well, I think the main way that term is used is you take your transition matrix, you add the identity, and you divide it by two. identity and you divide it by two. And what this really does is you're saying with probability a half, do nothing at all and otherwise do the transition that P tells you to do. Which seems, it's always seemed to me a little bit extreme to just make your chain do nothing with probability half at every step. But, you know, it can help. Or there's a method that Diaconis and Salov Kost described sort of in the middle of a paragraph. Describe sort of in the middle of a paragraph, this particular one that I like, you can apply it directly to bound one plus the smallest eigenvalue inverse, and you don't need to worry about laziness at all. And so I think people should use this method. And in particular, if every state has a self-loop, then it's really easy to bound this quantity. You just take the smallest self-loop probability, you invert it, so you get the biggest inverse. You invert it, so you get the biggest inverse self-loop probability, and you divide it by two. And that's a provable upper bound on this quantity. And it's very likely for a combinatorial chain that this is polynomial, and it's very likely to be a small degree polynomial. And in particular, it's extremely likely to be much smaller an upper bound than the upper bound you're going to get when you think about lambda one. So you can make your chain lazy and make the smallest eigenvalue go away, or you can just spend a tiny bit of time thinking about. Just spend a tiny bit of time thinking about it, and you'll probably be able to deal with it. That's just something I like to mention. But overall, the point is we can focus on the second largest eigenvalue, lambda one. And this one minus lambda one is called the spectral gap. I don't often use that, but we want the spectral gap to be large so that when we invert it, we don't get a blow-up. Okay, so here is So here is the canonical path method from, as described in Sinclair, Sinclair's 1992 paper. So you take your very nice Markov chain, time homogeneous, ergodic, and reversible. It has a stationary distribution pi, and you have already defined a set of canonical paths for this Markov chain somehow. And then we define the congestion of these paths to be this expression here. So what's going on? So, what's going on? You take the worst case, the maximum overall transitions or edges of the underlying graph, edges of the reconfiguration graph, essentially. First, you divide by this quantity, which is the quantity that appears in the detailed balance equations for that transition. So, the transition ZW, you divide by pi of Z P of Z W. And this is some idea of the capacity of that transition to carry flow. To carry flow. So, this, we divide by the capacity. And then you, what's on the numerator is a sum over all canonical paths which happen to go through this particular transition that I'm just going to pretend I've got them all to go through. Okay, some transition here from Z to W. And we're not just adding them up, but we have to put the product of the stationary distributions of the The product of the stationary distributions of the endpoints and the length of the path in there. But essentially, if pi is uniform and if you just use an upper bound on the length for everyone, then you are just summing up the number of paths. So that's the congestion measure. And the result says that one over the spectral gap is bounded above by the congestion. So your best bound on the congestion will appear as a factor in your bound on. Here, as a factor in your bound on the mixing time. And that means we want an upper bound on the congestion, which is polynomial in the logarithm of the size of the state space. Okay, I'll have a pause here. Are there any questions about that? So, the reason why I thought I'd talk about canonical paths is it's really intrinsically using paths in this reconfiguration graph. Graph. Okay. Sorry? So I have a small question. Yeah. Why does the definition of rho doesn't depend on pi of w? Oh, good question. Remember we have detailed balance. So pi z p z w is equal to zero. This is the other way around. Yeah. So we often write it as q of z. It's as Q of ZW, but I just didn't want to introduce any new notation. But yeah, it is symmetric, so thank you for asking. Any other questions? So this is defined as maximum overall edges in the configuration space, right? Yeah. But how do you estimate that? Because you can't compute it, right? No, I'm going to go through an example which will give you. Through an example, which will give you some idea of how we do it. But basically, you define your canonical paths very carefully and then you do a trick. Yeah. Well, the trick is called encodings, and it's a very nice idea. But first, I wanted to show you quickly how this proof actually works. Okay. So how. Oh, sorry. If you could give me a quick reminder, canonical paths, there's really no restriction on them except from X to Y. Except from X to Y. They go from X to Y and they use transitions of the chain, and that's it. Yeah, that's okay. So I get to pick quite freely which ones I use first. Yes, absolutely. But you need to make smart choices. Yeah. Yeah. Okay. Yes, you are the design, you're the engineer of your canonical paths, and then your task is to prove that that was a good choice. So this firstly, just let's have a look how they work. So this is just a restatement of what. Work. So, this is just a restatement of what we're trying to prove. And then we use something called a variational characterization of an eigenvalue. So, we switch to the matrix L equals I minus P. I think this might be a Laplacian. The advantage of this matrix is instead of the eigenvalues of P going from minus one to one, now they go from zero to two. The eigenvalue one becomes eigenvalue mu naught equals zero. So we equals zero. So we sort of swap the relative order of all the eigenvalues. And lambda one is now the smallest, it becomes the smallest positive eigenvalue mu1. So mu1 is the smallest positive eigenvalue of L and all the eigenvalues of L are greater than or equal to zero. And I guess that's why this variational characterization thing works, eigenvalue of L. So you can write this beautiful expression. You can write this beautiful expression. It's the infemum over all non-constant function ψ from the state space to the reals of this big sum divided by another big sum. The big sum on the bottom is like a measure of a global measure of the variation of psi. So psi of x minus psi of y, you square it and you multiply by the stationary probabilities of x and y, just because that's what we're doing. And in the numerator, it's more of a local. It's more of a local measure of variation. How much can psi vary over transitions of the Markov chain? So it's the same psi of x minus psi of y, but now we put in this quantity we were discussing before, pi of x, p of x, y, which is symmetric in x and y because of detailed balance. And we want to know this ratio. So what we do with this, we're going to take this difference psi of x minus psi of y, and we're going to think about And we're going to think about a telescoping sum along your canonical path, psi xy. And you can orient all the edges from x to y, and then each edge has, I don't know, I always forget which is head and which is tail, but one of them will be the tail and one of them will be the head. And it doesn't really matter which way you label these and you just have to be consistent, right? So in the first So in the first line, all we do is we say that psi of x minus psi of y is the same as if I sum up these differences along the canonical path. That makes sense, right? Because all the vertices in the middle, those terms will cancel out. So this is just a telescoping sum along the canonical path, which is nice, canonical. Which is nice, canon Nicole Path. But what we want to see, oh, I should have said this is the denominator of that expression that we had on the previous slide. That's where we start with, the sort of global variation thing. We expand along the canonical path, but we want to see for the numerator, we want to get back to psi of something minus psi of something squared, and a way to turn the square of And a way to turn the square of a sum into the sum of the squares is to apply Cauchy-Schwartz. So we've got a sum of some quantity and to make it into a product, so you can apply Cauchy Schwartz to make it into an inner product, we're just going to multiply each term by one. And that's often how Cauchy Schwartz shows up in combinatorial arguments. So then the right-hand side of Cauchy Schwartz is the sum of the squares. So the sum of the squares of ones. Squares of ones is going to give you exactly the length of the path here. That's how that comes in. Okay, so we're halfway through and I decided to move to another slide because I ran out of room. But we're going to exchange the order of summation. And that'll mean we're first summing over transitions. So the same left-hand side that we had before, which is the denominator of the big expression. First, we sum over all transitions and Transitions and what we're summing still looks the same. It's just the difference across the endpoints of this transition squared. And then the sum which was over pairs x, y that we had out the front here becomes the sum over all canonical pairs that use that transition. Because those are the only ones where that transition is going to show up in the expansion that we had in the previous line. And what you see here is looking very much like our See, here is looking very much like our definition of congestion, except we haven't divided by the capacity of the transition. So, in the next line, we just multiply the capacity in so that we can divide by it, and then we get an expression which is in the definition of congestion, except the congestion is the worst case. As we discussed earlier, it's a worst case upper bound. So, you put in this upper bound, and here's your congestion, which can come out the front. Congestion, which can come out the front, and then when you look at what you've got, it's exactly the numerator in the variational characterization for mu1. Well, one minus lambda one or mu1 is going to be the infemum of all of those. And so, in particular, when you plug in the value of psi that gives you the lowest, or you, you, you know, if, yeah, I guess by Yeah. I guess by continuity. Anyway, you get this, and dividing by the spectral gap completes the proof. So that's how the paths come into the proof, which I think is quite nice. Make sense? Quick question. So in this, are you going to tell us how you actually pick these choices of psi? No, psi is a test function. And you don't, this is just a proof. I'm not, we don't ever have to worry about the. Proof, I'm not, we don't ever have to worry about them when we actually do the canonical path, but in the definition, psi, you have to take the smallest ratio over all possible psi. And so I guess for this last bit, if it was a finite set, I would just let psi be the one that gives you the smallest value here. But it's an in-themum, so I guess you probably have to worry about continuity if you were doing this properly. But as you'll see, when you define the canonical paths, we're not worrying about the test functions. This is just for the proof. This is just for the proof. Yeah, if you were doing the functional approach where you do something like a log sublef inequality or something, then you're working with these test functions all the time. I don't know if that answers your question. I think it should, but I quite understand it. Okay. I'll move on to actually work with the canonical path and then you'll see that we don't need to worry about these. Okay. So what I want to talk about is to work through. To talk about is to work through an example, the nicest example of this, which is the example for which canonical paths were introduced. So it's a Markov chain to sample perfect and near-perfect matchings. We start with a graph, and we know a matching is just a set of edges which are not incident. And if we have even number of vertices, then a perfect matching is a matching where every vertex is incident with an edge of the matching. We're going to be interested in those and also in near-perfect matchings. And also in near-perfect matchings where exactly two vertices are unmatched. Okay. So we'll let curly p be the set of perfect matchings of the given graph and curly n is going to be the set of near-perfect matchings of that graph. And Broder introduced a Markov chain to sample from this state space. So I'll just tell you the transition procedure. Starting from a perfect or near-perfect matching, with probability of half, we're going to do nothing. So this is Half, we're going to do nothing, so this is a lazy chain, okay. And then, if we decide we are going to act, we choose an edge of the graph uniformly at random. If our current matching is a perfect matching and E is sitting there as an edge of the matching, then we remove it. So, this takes us from a perfect to a near-perfect matching. If our current matching is a near-perfect matching, Is a near-perfect matching, and the edge we chose is exactly between the two vertices that are unmatched in the current matching, then we're going to add that edge in. And this is how we go from near-perfect matchings to perfect matchings. And finally, if you're in a near-perfect matching and you choose an edge of the graph and exactly one of those endpoints of the edges is matched in the current matching. In the current matching, we call the matching edge E prime. Then we're going to remove E prime and insert E. We'd like to insert E, we can't because E prime is blocking it. But if we only have to remove one edge, then we go ahead and do that. And that's a transition. And in all other cases, we don't do anything. So I guess why are we introducing in many instances, you're really interested in the perfect matchings, but perfect matchings are kind of brittle and hard. Matchings are kind of brittle and hard to adjust sometimes for some graphs, and maybe you need to make bigger changes to get from one perfect matching to another. So, instead, we expand the state space to include near-perfect matchings, and you can drop down from a perfect matching to a near-perfect matching and walk around much more freely and then go back up to perfect matchings. And there's also versions of this where you kind of do these same transitions for Markov, for matchings of any size, but we'll stick to perfect and near-perfect matchings. Stick to perfect and near-perfect matchings. Then we can prove that the Broder chain is irreducible. And we made it lazy, so it's definitely aperiodic. And we know that lambda max is equal to lambda one. So we only need to worry about the second largest eigenvalue, lambda one. And you can also check if you can go from one matching to another in one step, then the probability that you did that is just exactly one over two m, a half that you chose to act, and then one over m. act and then one over m if little m is the number of edges you have to choose the right edge of the graph and that's the whole thing so the state the transition matrix is symmetric and hence the stationary distribution is uniform now broder gave an argument for this chain to prove rapid mixing using coupling but unfortunately there was an error and actually coupling will never work really for this chain because there are too many bad cases so instead Bad cases. So instead, Jeremy Sinclair introduced the canonical path argument to show rapid mixing for this chain. So how do we define a canonical path from a matching M to a matching M prime, which are both either near or perfect matchings? And in my picture, they're both near perfect matchings. What you want to do is you want to be guided by the symmetric difference. So this is a common theme in these arguments. The symmetric difference. these arguments the symmetric difference defined appropriately kind of really measures how far you have to what how much work you have to do to get from m to m prime and in this nice setting um the the symmetric difference of two perfect or near perfect matchings is always going to be a disjoint union of mostly cycles and then up to two paths. So here's the disjoint union for this particular example. And to define the canonical path, we're going to all Canonical path, we're going to order the components of the symmetric difference canonically. Maybe you look for the least labeled vertex and you grab that component and call it the first component and then you continue in that way. So we are going to deal with each of these components in order. And as we deal with the component, we're going to lay down steps of the canonical path. So by the time you've dealt with all the components, you shrink the symmetric difference to the empty set and you've reached. Difference to the empty set, and you've reached your end destination. Okay, so that's what I've just said. We might label the start vertex of each cycle, which could be the smallest labeled vertex in that cycle. And we should choose the least labeled end vertex of a path. So I've indicated those with a little asterisk. And for the cycles, we might also want, I'm going to want to refer to the edge of the current matching M, which Edge of the current matching M, which is red, which is incident with the start vertex. So that highlights a particular edge. And yeah, I've already said that as we process each component in order, we're extending our canonical path. So I'm going to tell you how to process a cycle. And I think I want to assume that M is a perfect matching because the first thing I want to do is remove an edge. So this is just an example of what we do in this case. Just an example of what we do in this case. We have an eight cycle in the symmetric difference, and we want to replace the four red edges with the four blue edges. So, first, we're going to remove this highlighted edge E. And this gives us space to try to put in the first blue edge if we do one of those, the third type of transition I mentioned, where exactly one endpoint is matched. So, we remove this red edge so that we can insert the blue edge. So that we can insert the blue edge here. And then we keep going. You see that we're sort of working around this cycle in this order, turning red things, taking out one red thing and putting in a blue thing each time. We get down here. And by the time we reach this matching, the next edge we want to add is precisely between the two unmatched vertices. So we are allowed to add it. And we've gotten ourselves from M to M primed using steps of the Using steps of the Markov chain. And in the process, we've dealt with one component and we've laid out this canonical path. If this was the whole symmetric difference, then this would be our entire canonical path. Does that make sense? Yes. Yep. Great. So you have to spell it out as specifically as this. We would have to say how we choose which direction we're going in. And they make that choice carefully. They make that choice carefully. In fact, they say that they'll choose an opposite direction if they're processing a cycle compared to a path, and that becomes useful later. That's a technicality we won't get into. So paths are processed similarly, and this gives you eventually, when you've handled all the possible components, a way of getting from X to Y for any pair of states X and Y. And now you've done your creative bit, the designing of these paths. Designing of these paths. Now you need to analyze them. What is the load on each transition? And the key tool is you define something called an encoding of the transition. So this is still what we tend to do when we're applying this method today. You want an encoding, which is an object, such that if you know the current transition and you know this extra object called the encoding, then you can infer from that. So the current transition. So, the current transition in this is called T, not time, but transition. If you know the encoding, you have enough information to figure out the n vertices of this path, which means the number of paths through this given transition is at most the total number of encodings that are possible. So, you bound the number of paths by bounding the number of encodings. And the encodings used by Jeremy Sinclair in their work is. Their work is this thing. So if your transition goes from near-perfect to near-perfect, and we're processing a cycle, so exactly the situation we looked at in the example, then you start with, I mean, pretend you know X and Y first. So you start with the symmetric difference of the endpoints of the path, X and Y. Then you take the symmetric difference of that thing with the union of M and M prime, the end. And M prime, the end, the two matchings involved in the transition. And finally, you take away the special edge E for reasons that I'll explain in a second. And in all other cases, it's the same recipe, except you don't take away the special edge E. The point of doing this is, the reason why we remove E, is it makes sure that the encoding itself is also a perfect or near-perfect matching. And that means the maximum number of paths. The maximum number of paths through any transition is at most the size of the state space. And that's going to be our bound. Usually, we don't get exactly the state space back. Usually there are some small number of defects in an encoding that means it's slightly messed up. But because there's only a small number, usually it only expands the size. From the size of the state space, the number of encodings is at most polynomially larger. But in this example, they restrict to the state space. They restrict to the state space itself. And as a very quick example of how this works, imagine this is our transition from M to M primed, which was one of the ones we saw before. Then we know the symmetric difference of the N vertices was this eight cycle. And we want to take the symmetric difference of that with any edge that you see up here at the top, the union. So we're not going to keep the bottom edge because it's here. The bottom edge because it's here. We're not going to keep this edge, and we're not going to keep either of those. So that would give us this edge, this edge, this edge, and E, but the final command was don't include E. So we're left with this thing, which is an encoding for that transition of the endpoints X and Y. Okay. And so the trick is to come up with a good encoding, prove that you can unique. Prove that you can uniquely recover the n vertices if you know the transition and the encoding. Or sometimes maybe there's a polynomial number of options. So that can also happen. But you can recover X and Y sometimes with a polynomial number of guesses. In this case, we recover uniquely. And for us, the encoding was a perfect or near perfect matching. And that means that no transition lies on more than this many canonical. Relies on more than this many canonical paths. And we also have a bound on the length of the paths. So I think I'm pretty much out of time, but I'll just show you the final calculation where they actually bound the congestion. So here's our definition of congestion again. Pi is uniform, so you could just cancel a couple of pi's, but I've put one over pi is the size of the state space. P of ZW was one over two times the number of edges. One over two times the number of edges, the probability that you do the transition. The red one is the number of different paths that use the transition ZW. And we just showed that that is the number of encodings, which is at most the size of the state space again. And then we have the length of the path, which I said was at most 2n is the number of vertices in the graph. And we've got two more stationary. More stationary distribution probabilities, right? So you just carefully write all this stuff down. Obviously, those cancel, and you get this bound of four times the number of edges times the number of vertices in G, which is really lovely, very small. I mean, it's at most cubic in the number of vertices, polynomial upper bound. And so this polynomial just sits here out the front in the upper bound on the mixing time. And the only other thing you have to do is put the log of the stationary distribution. To do is put the log of the stationary distribution and the log of epsilon inverse. So that was an introduction to the parts of Markov chains and mixing time that I think are the most relevant for you guys. And in particular, the canonical path method because it so heavily uses paths in the reconfiguration graph. So that's all I was going to say. Thank you. So, thank you, Catherine. Are there any questions? Yeah, I have a couple. Thanks for this. This is very helpful. This is Dan Cranston. One, would you be willing to share these slides? Sure. I actually uploaded them to the place where we're meant to upload slides, so they should be already available. Oh, that's great. Without my scribbles, though. Without my scribbles, though. Yeah. If you want, I can email you for the scribbles or whatever. I could actually, I could upload them again. I'll upload it. Yeah, that would be nice. And then a couple other questions. One, I've heard about this notion of multi-commodity flows, and that's somehow related to canonical plants. Could you say a paragraph about that? Say a paragraph about that? I can, I can, I can. Here. So I wasn't sure how quickly I would go through this. So multi-commodity flow is essentially canonical path, except that you previously we were constrained to only choose one path between X and Y, right? But maybe if you could choose multiple and sort of average the flow that you're sending from X to Y, you can sort of spread. That you're sending from X to Y, you can sort of spread it out over multiple paths, then maybe you can get even less load across transitions. So you think of it as you have pi of X, pi of Y units of flow, and you just spread them usually evenly across multiple paths. And apart from that, the proof that it works and the analysis is very, very similar to canonical paths. Is there a polynomial relation between the bounds that you can get? Or can you do arbitrarily better with a Or can you do arbitrarily better with a multi-commodity flow than you can with canonical paths? Ah, that's a good question. I feel like it's more that people's, I think we use multi-commodity flow when we think that canonical paths just won't work. And an example, well, in the work I've done where I've used multi-commodity flow, it's all been to sample graphs with given degree sequence. And if you picture what the symmetric difference of two graphs with given sequence sequence, Symmetric difference of two graphs with given degree sequence is it's a mess. It's a mess because you no longer have disjoint cycles and paths. Instead, at any vertex, you've got some number, I couldn't reach my red one, so I'm just using purple and blue. Some number of edges from one graph and the same number of edges from the other graph. And you somehow have to match them up into things you can deal with. So we tend to pair them up, which means for if you come in on a person. Which means for if you come in on a purple, we're going to leave on a blue. And then we come back into this vertex on a purple and leave on a blue. And for each way that you pair up, you get walks. You can sort of you've dissolved the symmetric difference into a union of edge disjoint walks, and then you process those walks in some way. So you get circuits. But how could we possibly choose a good pairing in advance? We don't know how to choose a good pairing. We don't know how to choose a good pairing, so we just look at all possible pairings and then we call these psi, which is sort of overloading psi. Then you get a canonical path from x to y, which depends on the choice of pairing psi. So because we don't know how to pull the symmetric difference apart, we pull it apart in all possible ways, define a canonical path for each one, and cross our fingers. Yeah, yeah. So essentially, somehow you're hoping that you can do well on the average. Exactly. And then that forces you from not choosing. You just sort of pick something at random and it works on average. That's it. Yeah. And you could try to, you could maybe try to pick a good pairing for a canonical path, but it's just, it's too hard. You know. Sure, sure, sure. And then lastly, And then lastly, if I want to read more about this than what's in your slides, what is the good place to go for that? Right. The textbook of Levin, Paris and Wilmer that's mentioned on the slides has a huge amount of detail. The paper by Alastair Sinclair that I keep mentioning is good and has references back to some papers of Diaconus that it builds on. And I would also say there are several chapters that have been made into a little green book. Made into a little green book by Mark Jeram. So if you have a look at Mark Jeram's website on his publication page, you scroll down until you see like chapter one, sampling and counting or something, chapter two, blah, blah, blah. There's a whole bunch of chapters and one of them is on the canonical path type method. Thanks very much. This is very helpful. Oh, you're welcome. Very welcome. Are there other questions? Are there other questions? Oh, so if nobody else has a question, I have a very naive one. So basically, if we think about the matchings and let's say perfect matchings, I think you mentioned that, well, the way it is designed, the algorithm you showed, you want to go to new perfect matchings in order to have small changes. Yeah. So what's So, what's not working if I just look at the symmetric difference of two perfect matchings and rotate the alternating cycles? Well, I guess how we need to stay within the state space, I guess. So, if I start from If I start from here and I just, what would we try? Try fixing this area, then I'm going to get, what am I going to get? I'm going to get these three and a blue one, and it's not a perfect or near perfect matching anymore. You've got a different mark of the chain. Yeah, so would you, oh, do you want to fix the whole cycle in one go? Yeah. Cycle in one go. Yeah. Okay. So now you've got really big moves, right? Because these cycles could be linear in size. So we tend, and I believe your community also tends to focus on local moves. There are some exceptions in the Markov chain community where you make these massive sweeping changes, but we tend to focus on local moves because we have a hope of analyzing them. Whereas for the big global stuff, usually it's very difficult. Yeah, so I think you could define a chain, a chain. Actually, how would you do it? Maybe you could define a chain like that, but I think it would be hard to analyze. That's my feeling. Thank you. So, you get to go and have dinner now. Or you're late for dinner, I guess. Sorry about that. Hopefully, there's still some food when you get there. So, I'll say goodbye. Thank you. Bye.