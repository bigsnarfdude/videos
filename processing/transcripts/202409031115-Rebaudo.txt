So, let me share my slide. Can you see the slide? Yes, yes, we can. One second. No, okay. Okay, so today I'm going to talk about the multivariate speeches sampling process. And this is a joint ongoing work with Patricia Franzolini, Antonio Leoy, and Igor Pruster at Bokon University, all of them. So, what we want to do, as the title said, is to Title said is to extend to the multivariate framework speech sampling process, and in particular to study and understand these three discrete structure and random partition model in the framework of partial changeability. So to do that, first I want to start reviewing the changeable case that is infinite changeable random partition, a speech-sampien process. And so in order. And so, in order to set the notation, they record the result that we want to stand. So, a speech sampling process is a random probability denoted by P tilde if it can be represented in the following way, that is essentially a weighted sum of a discrete random part and a non-atomic probability P0. So, the most interesting part from probability. The most interesting part from a probabilistic perspective is the discrete one since it's random while P0 speaks. And the atoms theta h of the discrete part are IID from P0 itself and are independent from the weights pih that can fall on arbitrary distribution. Moreover, if the weights are on the same Are on the simplex, so they sum up to one, not just their sum is smaller or equal than one. We don't have the non-atomic part, so just the random discrete part, and we name it a proper species sampling process. So why species sampling processes are important in Bayesian statistics and in statistics? Because they are used for speech sampling problems that involve unknown discrete distribution. Involve a non-discrete distribution and prediction of new data arising from a discrete population, and are also using mixture model with a likelihood. So, the species MPM process will be the mixing measure, and then we can use, I don't know, Gaussian kernel or density kernel to perform density estimation and model-based clustering. And include almost used mixture model in Bayesian statistics, like finite mixture, mixture of finite mixture. Like finite mixture, mixture of finite mixture, and infinite mixture. Okay, so more importantly, for us, speech sampling processes are in one-to-one correspondence with the partition of infinite changeable objects. So there is a characterization that I want to describe. To do that, first I want to define. To do that, first I want to define infinite changeable objects and, in particular, a collection of random partition pi is a family of random, is a sequence of random partition pi n, where pi n is a partition of the integers from one to n for every integer n. And is a changeable if, okay, first pi n can be obtained by eliminating the n plus one element from the subsequent element in the sequence, so pi n plus one. Sequence of pi n plus one. And the changeability entails that for any permutation sigma of the n element, the probability of the probability mass of the partition is invariant to such permutation. So the order does not count. This is the magenta assumption. And in particular, we can characterize infinite changeable random partition via the PPF that essentially is the probability mass function. Essentially, this is the probability mass function that of the partition that has as a sufficient statistics the frequency of the partition, and this is possible thanks to changeability. Okay, so just to illustrate it via an example, we can see in the plot that we have the probability of a given realization of the partition, and it must be equal to a different realization where the difference is just obtained. Where the difference is just obtained by swapping some element, right? And this is due to a changeability. So, in which sense the speech sampling process characterize infinite changeable random partition? In the following. So, first, we can define the law of an infinite changeable sequence of a server x1, x, n via the projection. If you want the sample in the statistical interpretation, x1, xn for any sample. X1, Xn for any sample size n via the definite theorem, the representation, where we use now a speech sampling process as a definite measure. And this is equivalent, so in this sense, it characterizes speech sampling process, characterize infinite changeable random partition to the following mechanism. First, we sample a random partition of the label of the observation, like this one. Like this one, the one colored together, the same silica cluster together, and then we just assign labels to the different block. So we independently sample the unique value, we are denoted by x star one, x star k, so we have k unique value from an anatomic probability measure p0. Okay, and moreover, the last another Last another characterization of infinite changeable random partition is via the recursive predictive sequence. Indeed, this produce a law for the observable sequence called species sampling sequence that can be also characterized by, if you want, UNESCO 23 or a more sequentially predictive sequence via the predictive probability function. Okay, so this was. So this was a brief review of speech sampling process and their characterization related to infinite changeable random partition. And now what we want to do is to go beyond exchangeability. And indeed, if we have, I don't know, categorical covariate, or the data are recorded in the different groups or populations. In different groups or population. Let's say we have a treatment control on. We, of course, we want to use such information of the covariate, such structure, and in particular, we will focus on the framework of partial changeability. And under partial exchangeability, dependent discrete prior were developed and successfully applied and studied in statistics, but a unifying framework that characterized partial Framework that characterizes partially changeable partition, like the one of SPCSMP process was missing. And what we wanted to do is to define and study such unifying framework. Okay, so first let's set the notation and definition of partial exchangeability. So now we have, as before, an observable sequence X, so X1, X2, and so on. But now we have an additional information that is the group division. Information that is the group division that we want to preserve the apart exchangeability, and here we denote some such information with the sequence d, where di can take value from 1 to j, and 1 to j are the label of the different group. And essentially, di tells us the group from which i is observed. Okay, so now we can say that x is part of. Can say that x is partially changeable with respect to this group division provided by the sequence d. If for any integer n that can be for the sample size and any the invariant permutation sigma, so now we are considering a subgroup of the permutation of the integer. And what is an invariant permutation sigma? It's a permutation that preserves the information of the group. So essentially, we can permute within the group, but not. Mute within the group, but not across the group. We don't want to lose the information of the group assignment, so the structure that we have in the design of the matrix. And so, from a societial perspective, in the Bayesian infant. Okay, so what is partial changeability? If for any invariant permutation, the law is invariant to such permutation. Okay, so we still have the magenta assumption that allow us to perform inference of. Perform inference of the changeability within group, but we preserve the information of the group from a statistical perspective. And now, if the sequence of observation is extendable in each group, we have also a definite theorem that says that x is partially changeable with respect to d, if and only if xi can be thought as condition independent given a vector of random probability p tilde 1, p tilde j, so vector probability. P tilde j, so that random probability specific to the different group from the random probability of the group from which xi is recorded. And to conclude, we have to assign the distribution q to the vector of random probability measure, right? So q entails the information on both the marginal distribution of the group, but also the dependence across group, and thus, from a socio-perspective, what entails the information. perspective, what entails the volume information under Bayesian framework if you use it to perform inference. Okay, so in statistics, in Bayesian parametric, in Bayesian parametric, several dependent priors, especially the Benedisc prior, were introduced successfully to tackle statistical problem and application and also were well studied. So here it's just a list to provide some examples. A list to provide some examples that will belong to our unifying class, right? So we have dependent the speeches sampling process via additive structure, via hierarchical structure, via nested structure, composition of the previous classes that were also studied, and other single atom dependent process, normalized completely random vector, and so on. Okay, so from a statistical perspective, Okay, so from a statistical perspective, a lot of dependent prior were successfully introduced and applied. And what was missing is a unifying framework to characterize partially changeable random partition that they induce, and also to provide some structural property and understanding of this larger class. So for instance, one very popular, arguably the most popular measure of dependence across the previous is the correlation in the sense of the correlation. In the sense of the correlation of two random probabilities evaluated in a given set, A. And this does not depend on A in all the previous examples and thus is taken as a measure of overall dependence and so for of the borrowing of information in the abesion procedure. And so this is just these are just two examples for the archives process or another different model. It is the Nest Common Atom model. Common atom mode where the correlation depends just on the upper parameter and can be used thus for prior elicitation in terms of volume information but not on the set. So we would like to study and so to understand why in this general class that characterize partially changeable random partition and understand if correlation is a good measure of dependence and why. So first we want to characterize First, we want to characterize partially exchangeable random partitions. So, let me define what are partially exchangeable partitions. So, given a group division D, the one introduced before, a collection of random partition pi, the sequence of partition pi n, where pi n is a partition of the integer from 1 to n for every integer n, is partially changeable with respect to the group division provided by the sequence d. If first, pi n can be obtained by eliminating. First, pi n can be obtained by eliminating the n plus one element from the subsequent element in the sequence, so the partition of the n plus one integer. And partial exchangeability entails that for any d-invariant permutation of the n element, the probability must function the partition, so the law of the partition is invariant to such permutation. So, essentially, the order of the observation within group does not matter, but the information of the group. Matter, but the information of the group is preserved. And the law of such random partition will be characterized by the partial exchangeable partition probability function that were introduced in the literature for some specific example of dependent species sampling process like additive and the article one. And we will define in general to provide characterization, right? So here Here it's just an illustration of the probability mass function of a random partition that is partially changeable. And indeed, if we permute observation within group, like at the bottom of the slide, the probability mass functional variation must be equal. While if we permute a cross-group, it can be different as in this example. Okay? Them. Okay, so we want to characterize partially changeable random partition, and to do that, we start defining multivariate species and process implicitly. Okay, so given a partially exchangeable sequence with the definitive theorem here in the question one, we say that the definitive measure, so the random, the vector random probability is a multivariate speech sampling process. Is a multivariate speech sampling process if the law of the servile bubble is characterized by the following sampling mechanism. First, sample the random partition of the sequence from a partially changeable random partition, okay, that allow us to cluster potential observation within or across group, and then assign the label independently, so the unique value xi k, xi1 till xik. Xi1 till Xik ID from an atomic base measure P0. Okay, so now if we characterize more explicitly what is the law of such vector random probability, we are characterizing partially changeable partition exactly in the same way that was done for exchangeable random partition via species sampling process. Okay, so the first characterization I want to discuss is via the partial exchangeable probability partition probability function, PAPPF. And to do that, actually, we have to define it as a mathematical object like a PPF of some integers that then will be put as the frequency of the unique value recorded in the partition. Value recorded in a partition within a lacrosse group. Okay, so now we say that pi is an infinite partially exchangeable random partition with respect to a group assignment D, if and only if there exists a function f that will be the PPF such that the probability mass function of the partial exchangeable random partition in a given realization. random partition in a given realization is what equal to such function f evaluated in a matrix of integer. So n is a matrix where the column can be fought as the group, the observable group assigned via D. And the rows can be fought as the label of the unique value assigned to the different blocks of the partition. So njk So, njk can be thought as the number of subjects from group j that are assigned to the cave set of the partition, right? And to be a valid PAPPF that characterizes actually partially changeable random partitions, such function f must satisfy some property. So, first is define of this matrix of integer that can be fought as the frequency of the unique value of Unique value or the block to the partition within and across group, and of course, must go in 0, 1 because it's a probability mass, right? And then we have a condition similar to the one of the EPPF that entails closeness to marginalization or Komogoro consistency, right? And then, since the label of the observation or the order of locked partition does not matter, if we permute. Not matter if we permute the row, such function must be invariant, right? Okay, and now a function that respects this property is called partially exchangeable partition probability function, PAPPF. And actually, we showed that characterize the law of the partially exchangeable random partition. Okay, so not only can we derive it as a function of a given dependent problem. Dependent process like the economy archi√®re process, as was done in the literature, but now we start directly from it to define an arbitrary partial exchangeable random partition, right? Like it is possible via the EPPF. Okay, so now the question is: what is the form of the multivariate speech sampling process? So, multivariate speech swamping process, so this is the second characterization theory. Characterization here as the following form: is a multiple species sampling process if and only if P tilde J is also a species sampling process that is a weighted sum of an atomic measure P0 and a discrete part. And in the discrete part, the atoms are shared across. Atoms are shared across the groups. Okay? So the atoms theta H are already from P0 and are independent from the weights in all the different groups. Okay. And this is the second factorization theorem via, if you want, the random probability, the dependent random probability. And finally, we can characterize it also in terms of sequentially predictive distribution. So Distribution. So if P tilde one, P tilde J is a multivariate speech assembly process, character is a multivariate speech assembly process, if and only if the predictive distribution of the X, so multivariate speech assembly process is the definitive measure in the partial under partial changeability. First, we can sample the first observation X1 from P0 and then at the N plus one step, we can sample XN. We can sample xn plus 1 given the previous observation, and it can take a new value, sample from p0, or one of the previous value already recorded in x1, xn. And the probability of one of such events is controlled by the multivariate prediction probability function that essentially is a function of such matrix of integer. Of such matrix of integer, that also must satisfy some property to be a valid multivariate prediction probability function and characterize in this way partial exchange random partition and multivariate species sampling process. Such properties are similar to the one discussed for the PAPPF. And now, potentially, we can start defining a multivariate prediction probability function, checking if it satisfies some property. Satisfy some property and indirectly define a multi-ariser speech sampling process, right? So, this is another characterization. Okay. So, now that we have such framework that extend the one-of-speech sampling process and the infinite change-and-o-partition to partial changeability, thanks to multivariate speech-sampy possibilities we defined and characterized before. We would like also to study some structural property of such class. Some structural property of such class that are useful for Bayesian inference and in terms of providing analytical result to have a deeper understanding and also to develop algorithm. So what happens for instance in terms of the correlation? In all this class of multivariate species and P process that has contains all the previous examples listed in the slide, like the article nested. Like the article, nested, additive process, common atom model, and so on. The correlation between P tilde 1 and P tilde 2 evaluated in a set A, where P tilde 1 and P tilde 2 are multivariate species terminating process, actually is a simple marginalization functional of the PA PPF. So for simplicity, let's look at the second line. Let's look at the second line of the box where also the match and distribution are the same. And the correlation is actually just the probability of the ties across the group between two observations divided by the probability of the tie within group across two observations. So it's a very simple marginalization of the PAPPF, if you want. And of course, such ratio probability of the ties does not depend on the set A. Not depend on the set A. So rather than say typically it does not depend on the set A. Now we know that is true for the whole multivariate species and Boxes that characterize partially exchangeable random partition. And actually, it has a very simple interpretation directly in terms of observable. It's actually a function of the probability of sharing atoms, regardless of their specific value in this class. Okay, moreover, the correlation is greater or equal than C. Equal density, of course, because it's a ratio of probability. And also, the correlation between two observations is just the probability of the ties between them. So that they are clustered together and also is given to equality, of course. So the dependence boils down of sharing the underlying common atoms. And actually, a similar result is true also for Eiger moments, but entails the probability of sharing atoms or. Atoms or within a lacrosse group, considering more than just observation. Okay, so correlation now is can be used as, I mean, is justified as an index of dependence, so between zero and one, that can be, that is global, does not depend on the set, can be also interpreted in terms of meaningful observable quantities. But is it a good measure of dependence or not? Dependence or not, so we know that okay, independence implies correlation equals zero, but the visual verse in general is not true, right? Aside from specific examples like multivariate Gaussian, so on. So, in this class, multivariate species sampling process, is it true? And if not, is it true in some relevant classes? So, the question is the following: We would like an index that is, we would like the correlation to be a Relect the correlation to be a very good index of dependence if not only is between zero and one, but is zero if and only if we have independence, not just linear independence, and is one if and only if we have fully changeability at the extreme. So actually, under malcondition that we are not stating, hopefully, the draft will be available soon, but the malcondition are satisfied by all the previous examples listed in the slide. So, here are the process. In the slide. So, hierarchical process, additive process, common atom mode is not the entire class. So, from a probability perspective for characterization of particular partition, these might condition are too restrictive. But for Bayesian inference, it contains everything. So, for a statistical perspective, they are not. So, you have to have a process, nest a process, additive, and so on are all there. Actually, the correlation is a very good index on dependence in the sense that correlation equals one if and only if. Correlation equals one if and only if we have full exchangeability, and correlation equals zero if and only if we have independence, like I don't know, the multivariate. Okay, so this justifies further use of correlation as a good measure of dependence within the class of multivariate speech sampling process that entails essentially everything in under partial exchangeability using Bayesian dependent on parameter prior. Okay, discrete prior. Okay, so these were the So, these were the main results I want to discuss. I want to show also how such a result can be applied to provide algorithms. So, of course, now if we have a sequence, a partial exchangeable sequence with a multivariate speech sampling process, now if we have the PAPPF, and that way we can start now defining the sequence via the PAPPF because we have the condition. Because we have the condition to check to say if it's a valid PAPPF, and we are not losing anything because such conditions are the one that characterize partial change of random partition. We can provide predictive scheme just simply applying, at least in principle, the definition of conditional probability, right? And so provide multivariate change-destruct processes. So extension of the non-change-destruction process. When non-changer samples. The problem is that such a ratio of PPPF often, actually almost always, is not really tractable, right? Indeed, even in the changeable case where we have a ratio of EPPF, such a ratio boils down to a simple ratio in the Diricher process, that is the one of the Chenles-Sam process, or in other product partition models like in the Gibbs type. Like in the Gibbs type prior class, but not in general. So, in under patch changeability, it is even more complicated. But a common technique is to define directly a PAPPF, if you want, or to characterize a PAPPF on an existing model via composition of simple building blocks that are the EPPF, let's say, of a Gibbs type prior. KIPPS type prior like is done, for instance, for the ERCADUSHE process via the table augmentation. If you want, in this way, such augment the ratio in such augmented space still boils down to product of simple ratio, allowing us to derive tractable marginal algorithm. And this is true actually if we compose, I don't know, hierarchically speech-sampling process, so with the Chinese restaurant franchise. The Chinese restaurant franchise, if you want, metaphor for the augmented predictive scheme. This is true for nested structure and this is true for additive structure. But now, potentially, we can go even beyond and compose EPPF directly without starting from the discrete random probability and still have tractable marginal schemes and predictive distribution. And actually, if we start comparing. And actually, if we start composing in this way the PPF to define the P APPF, we will satisfy by default the condition that we have to check to be a valid PAPPF. Okay, so here the idea was just illustrated. We developed algorithm via this variable augmentation and composition of PPF to already existing more. Models in the literal, and we want to just illustrate a comparison. But before concluding, I want to first thank my co-author, so Beatrice Antonio and Diego. And these are the main references, and any comment, question are welcome. This is an ongoing work, but hopefully soon we will have a first draft to share. A first draft to share. So, thank you very much. Thank you very much, Giovanni. Maybe we have time for one or two questions. Okay, so I have one. Can you hear me? Yes, I can. Okay, excellent. So it seems to me that a lot of stuff. It seems to me that a lot of stuff that you have done in this project is like working on the representation theorems that you had, that Pittman had on speed sampling models for these vectors of a speed sampling process. And sorry? Yes, I couldn't hear well, but now I can. Okay, okay. And so, do you have anything? And so do you have anything like on the limit of the proportion of elements that it's in each partition? Because in one dimension you have that those are like size by if you sorry, can you hear? Yes, okay. Okay, so for the exchangeable partitions, you have that the proportion of elements on each of the That the proportion of elements on each of the blocks, long-run proportion, it's a size-by-sweight, right? And my question is: do you have anything on those limits or some intuition of what might happen for this case? Okay, no, we don't have any theorem now related to that, but I think that at least a representation like the At least a representation like the one of King Man by paint box theorem, right, will be available also here actually we're working on it so that the convergence essentially of the random frequency to the weight, okay, so it should be true, right? So we just had to prove at least I hope that, but at the moment, you know, we don't have We don't have any formal theorem related to what you ask, but yeah, it's good. I mean, intuitively, there should be some analogous now. So it's something we would like to study and prove formally, right? Thank you. So let's thank Giovanni again. Thank you. Thank you. Our next talk it's not aligned it