Hello and thank you for inviting me. I'm going to tell you something about a new project which is in some sense a work in progress. Not in the sense that the theorems are not in their final form, they are, but in the sense that there are many questions around it that one can still think about and we didn't yet. So because I want to talk about functional surface area measures, I should probably start by reminding you what is the surface area measure of a convex body. Is the surface area measure of a convex body. So, okay, this is some basic definitions that we all saw. I don't think I should repeat them. So, when I say a convex body, I mean something that is compact and has non-empty interior. I denote the volume in the usual way, the support function in the usual way, the Minkowski sum in the usual way. And I will just remind you that the Minkowski sum can also be defined by just adding the support function. By just adding the support functions. And then, when you have all of these basic definitions, what is the surface area measure? So, you can define it using the following theorem. So, for everybody k, I can associate some measure on the unit sphere that I denote by SK, oops, that has this property. If I differentiate the volume at the point k in the direction L, In the direction L, the derivative is the integral of the support of L with respect to some measure that depends on K, and I call this measure the surface area measure of K. And this theorem is the way I choose to define surface area measure. Of course, you can write explicitly what this measure is. And this is written here. You take the uniform measure on the boundary of k, the housed of n minus one dimension. Hausdorff n minus one-dimensional measure restricted to the boundary. You push it forward to the sphere under the Gauss map, under the normal map. This gives you some measure on the sphere which is exactly this sk. But I'm actually not going to use this definition anywhere in this talk. This is for me the real definition of surface measure. That's okay? By the way, at any point, if you have questions, please stop me. And now I want to do the same for fun. And now I want to do the same for functions. So, what kind of functions? So, I will use log concave functions, which already appeared here. We all know what they are. Here is the definition. It just means that log f is concave. Now, we don't have a word for it. For bodies, we have convex sets, and we have convex bodies, which usually mean a set which is also compact and has non-empty interior. And I don't have a word for functions that are, in some sense, bodies. Functions that are in some sense bodies, in the sense that they are upper semi-continuous. This is like being closed and have positive integrals, that like having non-empty interior, and have finite integrals, that like being bounded. I don't know how to call them, so I'll just call them local k functions and we will always assume that they also have these properties. The usual examples just take a convex body and take the indicator, this works, but there are of course many many other. But there are, of course, many, many other examples, like the Gaussian is such an example. And what I want to do today is I want to think about those functions as generalized convex bodies in some sense and talk about the surface area measure of the function f. Now, originally, I had some slides here about why we should do this, but I think that in this audience, you've all seen such things before, you know. You've all seen such things before, you know that this can be extremely useful, even if in the end of the day I only want to prove results about convex bodies and I don't care about functions, still such functional techniques are often extremely useful. So I don't think I should explain more about what we want to do, just about how we do it. And well, if we want to mimic the thing from before, we need to know what is the volume of f, which you know, there is only one thing that makes sense, take the integral, we should know what is the support. We should know what is the support function of f and what is the addition of two local k functions. So, again, you might think about all kinds of definitions, but if you want to have some basic properties, there is essentially only one definition. So, there are all kinds of theorems that say this. Maybe the first one was by Artwinett Millman. Here is some variant that I proved a few years ago, which I personally like because I proved it. Which says that. Uh which says that let's assume that I associate to every local k function a convex function hf, which I want to think about as the support function of f. And I only assume three things: one, that bigger functions have bigger support functions, two, that, well, for indicators of convex bodies you really get the usual support function, and three, that the support of the sum is the sum of the support for. Is the sum of the support for some sum of convex body of local k functions? I don't assume anything about this circular plus. There is some sum such that this thing is additive. Then there is only one choice. The sum has to be the soup convolution and the support function has to be take minus log f, take this e to minus phi, so take this phi, and take the Legendre transform. There is no other option. So this will be my support. Option. So, this will be my support function, and this will be my sum. And I also, okay, well, once you have the support function, you also know what is the homotopy, what is the dilation. If I multiply the support function of f by t, what does it do to f? Well, write it down. It does that. So, you also know what is the homotopy operation. Okay, so now we have all these operations, and I want to define, to look at some variations, so let me just Look at some variations, so let me jump to the end in some sense and define the most important definition of this talk. So, I want to define the surface area measure of a local k function. So, I have a function f, which is e to minus phi. What do I do? I take the measure with density phi, f dx. I take the gradient of phi, and I push forward this measure with respect to the gradient of phi. This is some measure. This is some measure, a Borel measure on Rn, which I choose to call SF. Notice that I did not assume any smoothness assumptions, but I don't need them, right? Because phi is a convex function, so it's differentiable almost everywhere on the set where it is finite. So, you know, this grad phi is defined almost everywhere with respect to this measure. So the push forward is well defined always. I don't need any differentiability assumptions. So this is the measure. So, this is the measure. Let's do some definitions, some examples, just to make sure we understand what it is. So, if I take a Gaussian, if I take this f, then phi is just x square over 2, the gradient is the identity. So, sf will just be f dx. Okay, fine, it will be a Gaussian measure. But it can be things, very different things, right? If I take this function f, I take a function which is piecewise linear, then what will happen? Well, the gate. Well, the gradient of this function is always either v1 or v2 or up to vm. So the push forward will be supported on a finite set. It will be a sum of delta functions. Or you can write down which weight you give to each of them. And the last example, just so you will see that something here is a bit strange, is that if f is the indicator of a convex body, this is not the surface area measure of k, right? Surface area measure of k, right? Because the gradient is always zero. So I push forward all of the mass to the point zero, and I just get the volume of k times the delta at zero. So it's a bit strange, but that's what it is. And in particular, there is something strange here that I want to claim, and I will claim in a moment that this is a surface area measure, but the total measure is the integral. The total measure is the integral of f, right? Because I push forward this measure, it doesn't change the total mass. So the total mass is just the integral of f, which is not surface area, it's volume in some sense. So it's a bit strange that I call this thing surface area measure. You can ask why do I give it this name? Because why is the gradient always zero? I mean, it's not smooth. I mean, it's approximate. The gradient? I thought approximately. I don't approximate anything. I take this function, I take the indicator, which is, well, there is a set where it's not defined, where the gradient is not defined, but this set is of measure zero. I mean, this definition, you are right that probably I don't want to apply this definition to this case. But the way I hold the definition, I mean, if I want it to actually be a measure and not some discrepancy, I mean, that's what I get. That's what I get. So, why do we call this strange tilt surface area measure if it's not surface area, if it's volume? Because, well, sometimes at least, we have the same theorem as that we had in the this in the case of convex parties. So, sometimes if I take the integral of f and I differentiate it at the point f in the direction t. Point F in the direction T, I get the integral of the support of G with respect to the surface area of F. So, what is this sometimes? We know it cannot be always, like Galina said, it's not true if F is an indicator of a convex body, for example, because we are missing some distributional thing that happens on the boundary. So, it cannot always be true, but sometimes it's true. And there is a theorem of Colesanti and Fragal. Theorem of Colesanti and Fregala that says exactly that says give some conditions for when this is true, and those conditions are fairly technical and not sharp in any way. But if things are finite and smooth and grow fast enough, and the differences are still convex, if you have enough technical assumptions, then this is true. But those assumptions are very, very far from being sharp. And more or less at the same time as this paper, Same time as this paper, I wrote a paper about the mean width of local k functions, which was actually the very first paper I ever wrote, that basically showed that this is true for this specific f with no assumptions whatsoever on g. So sometimes this is actually true. But we want to understand when this is true. What does it say of f that this is true? Those conditions are not the real conditions, they are not sharp. So if you want to So if we want to understand what's really going on, we have to go to a paper of a Cordero and Clartag about moment measures. So I will try to explain what they proved. It's a bit technical. It's not so important for us. The important thing is the condition. So what did they do? So here is what they. Let me try to explain the result in the following way. I am going to define two functions on Two functions on convex functions. So those are functionals. This f is, you take a convex function psi, take the Legendre transform, take e to minus, take integral, take log, take minus. Okay, fine. What is this thing? So if you write down the Pre-Copalendre inequality, it just means that this function is convex. Right? Because regular addition is mapped to inner convolution under the Legendre sum, you have to write it down. The sum, you have to write it down, but that's basically what Precopen just says. It's completely equivalent. And then this is just a linear function, because it's the integral with respect to a measure. And this statement, the one we are trying to prove, is basically a computation of the gradient of this F. This is what this thing is. And basically what Cordow and Clark did is they said, well, we don't know to compute. Well, we don't know to compute the gradient of f, this may be difficult, but f is a convex function by Precopal Edler, so it has a sub-gradient, all of the linear lines below the function. So let's at least try to check if this thing that we expect will be the gradient, maybe it's not the gradient, maybe it at least belongs to the subgradient, it's a bit weaker than this statement. And then we were able to prove exactly when this is true. Exactly when this is true, with completely sharp conditions, if and only if. What were they able to prove? They were able to prove that this is true if and only if the function f is essentially continuous. So what does essentially continuous mean? This might be the second most important slide of this talk after the one defining the surface area measure. So I take my function f, I ask what are the points F, I ask what are the points where it is not continuous, and I want this set to have zero measure, but not zero Lebesgue measure. This is always true. I want it to have zero measure with respect to the n minus one dimensional Hausdorff measure. Why is n minus one the correct thing? Well, because a convex function is always continuous on the interior of the support. And outside of the interior of the support, I'm just zero. And outside of the interior of the support, I'm just zero. So of course I'm continuous. So the only interesting points are the boundary of the support of k. And the boundary of the support of k is an n minus one dimensional thing. So I want to be continuous almost everywhere on this boundary, but with respect to the natural measure on the boundary, the n minus one dimensional Hausdorff measure. Is this okay? Okay. Okay, so it is because of this expression, it just says, you know, I use the word continuity, but it just says that the function f is zero almost everywhere on the boundary of the support. That's really what it means. So, for example, indicators of convex bodies are really not essentially continuous. And turns out that this is exactly the condition needed to prove this thing. To prove this thing. And this is what Cordova and Schlatak showed. They didn't use the word surface area measure. They called this measure SF the moment measure of a convex function. But it's exactly the same definition. It's exactly the same measure. And this is what they proved. So this raises the obvious question, right? We have two theorems that are not comparable. We have this one of Colesante and Fragala that shows you how to differentiate. That shows you how to differentiate it under some technical conditions. And then you have this theorem of Cordero and Clertag that shows you, okay, my ABC is bad, sorry about the order. That shows you exactly what is the correct condition, but it shows you something a little bit weaker. It only shows you something about a sub-differential. And you want to know: is this gap real? This gap is real? I mean, can you prove the stronger thing under the weaker assumption? And the answer is: yes, you can. If f is essentially continuous, with no assumptions on g whatsoever, you can compute this derivative. And actually, this being true, even for one specific function, the indicator of the ball, is actually equivalent to f being essentially continuous. So this is exactly the correct conclusion. So this is exactly the correct condition to make this look. So why do I like this theorem? So for two reasons. Okay, one, it's the correct thing, it's the stronger thing that, you know, implies both of the other theorems. But I also think that once I will try to show you a little bit of the proof, and I think it shows in a very nice way why essential continuity is important. It's very transparent where Very transparent, where does essential continuity pop up, and what can you do without it? So, let me try to say a few words. So, the proof is naturally a bit technical, because we're trying to get rid of technical assumptions, right? So, what can you do? It's going to be technical, but I want to skip the parts that are only technical. So, I want to prove this thing. When you unravel all the notation, what is surface area, what is subconvergence? What is surface area? What is subconvolution? When you unravel all of the notation, this is what you have to prove: this derivative. And then you first show that this is true pointwise. And well, you just have to show it. And once you show this, all that remains is to prove that you can differentiate under the integral. Which sounds tot totally technical, right? Of course you can differentiate o under the integral. You can differentiate under the integral. This is something we always say: okay, there will be some dominant convergence, something. Of course, you can do it. But in some sense, in this proof, this is where the geometry enters the proof. Because this stage did not use essential continuity, it did not use anything. It doesn't even use convexity. The part that the g the only part where the geometry enters the picture is the part where you want to exchange this derivative and this integral. Derivative and this integral. And then what happens? So you need to show it, you need to show this for every function alpha, and you do some clever reductions, and you reduce to one very specific function alpha. First, you say I bound it, and then okay, you should you red you basically show that to prove this for all alpha, it's enough to prove it only for this one very specific function alpha, which is basically. Alpha, which is basically the Euclidean norm of x. Okay, so for this one very specific function, you can just compute this side and this side and check if they are equal or not. So you compute. So here is the computation. Again, you can read it if you want. The right-hand side is very easy to compute, it's just the chain rule. The left-hand side, you have to be a bit careful. You have to be a bit careful. I use layer cake decomposition. I use somewhere hidden here. I use decomposition to mixed volume to convince myself that something is monotone. There is some non-trivial geometry in computing this, but you just compute and you get the answer. And after you compute everything, let me get to the punchline. So proving the full theorem reduced to just checking this case and computing this case. And computing this case gave you this equality. And this equality, you know, this equality is just a coare formula. So for which functions is the theorem true? Exactly for the functions f that satisfy the coare formula. Okay, and now you check which functions satisfy the coare formula. So all local k functions satisfy a generalized coare formula that Formula. That's not very hard to prove if you are willing to use some big guns from geometric measure theory. You use the divergence theorem for Lipschitz domains, you use the Coriolis formula for general functions of bounded variations, so you use some big canons, and you immediately get that for every local k function, this is true. But that's not what we want. We want the same without this term. Okay, so when will it be true? If and only if this term is zero. If and only if this term is zero, when is this term zero? If and only if f is zero almost everywhere on the boundary of the support. So if and only if f is essentially continuous. So that's basically the full proof. Modulo, all the lot of things that I didn't tell you. And I think it shows in a nice way why essential continuity is important. And it also gives you some idea for how the general theorem looks like, right? It's still a work in progress, like I said. It's still a work in progress, like I said. But if f is not essentially continuous, you will get a more general result that will take this term into account. Colesanti and Fragala has such a more general theorem, but again, under very strong technical conditions that are definitely not optimal. So it's not clear what is true there. Okay, so since I have like six minutes, right, I will tell you very quickly. I will tell you very quickly something related that I didn't think I will have time for, so I will say it quickly. So, once you have this self-asserial measure, you can try to ask all kinds of natural questions people usually ask about the case of convex bodies to check if you have the correct definition. For example, you can talk about a Minkowski existence theorem for such regions. So, let me remind you what I want to do. What I want to do. So here is the definition of a Alexandrov body or wolf shapes. This already appeared in Galina's talk. I take the biggest convex body with support function is less than some psi. And I can do the same for functions. Given a function psi, I can take the biggest local k function f with support less than psi. This is some local k function, which I will call the Alexandro function of psi. Function of psi. And by the way, the support of f is actually equal to psi almost everywhere with respect to this measure, which is like the case of convex bodies, so it's a good way to check that all the definitions make sense. But I don't need this. The thing I really want is I want to talk about p-means that Galena already defined for bodies. Right, how do I take the p-means of KML? I raise the support functions to the power of p, add them, take it to power 1 over p. This is not a question. 1 over p, this is not a convex function, but I take the Alexandrov body. And I can do the same for functions. Take the support, if the support is positive at least, raised to power p, add 1 over p, take the Alexandrov function. And then if you try to differentiate with respect to this addition, I don't know what are the minimal conditions for this thing, I did not check them, but at least if everything is nice enough and smooth enough, you know. And smooth enough, you know what you will get. You will get exactly the same formula as you get for convex bodies. So, because of these formulas, this one and this one, these measures are often called the p-surface area measures, the L P surface area measures. So, the L P surface area measure of K just has density hk to the 1 minus P. So, the L P surface area measure of F just has density H F to the one minus P. The hf to the 1 minus p. And then you can ask, I give you a measure, I want to know, and the value of p, can you find f that has this surface area measure? And what is known? So the case of bodies is, of course, well known. I believe that this formulation is due to Lutwak, but it's a small variation of the case p equals 1, which was known to Minkowski. Which says that if I have an even measure which is not supported on any hyperplane, then it is always true that I can find a body whose p-audial measure is proportional to mu. And well, if p is not equal to n, I can also make it exactly mu by some homogeneity, but that's not very interesting. And here is a two-line sketch of the proof. You know, it's a caricature, of course, you have to check a lot of. You know, it's a connected true, of course, you have to check a lot of things. But the full proof basically is just define a functional, here it is, minimize it, write what it means that k is a minimizer, write the first variation equals zero, and you get the condition you want. That's it. For this thing, if I take an even measure, are you sure? Yeah, it's a not driving a little bit. No, this is the kind of things you have to check here, but you say it's Minkowski, it's not. Sorry? For P driver. Oh, for P.1 it's not. Yeah, for P l of course. Because for P less than one, this derivative involves the Alexandrov lemma. That's the main. Right. To compute what it means that the gradient of I is zero, you need Alexandrov's lemma. This is the non-trivial part here. But it is true even for P less than. But but I think as well as kilo, but in general for p equals zero as well. Resistance, I think, as well as kilo forward well for p equals zero, which is harder, there is some condition there. For p bigger than zero, I think it already. Okay, but okay, we will we will check later. Maybe I will. So this is for bodies and for a For a for local k functions, the case p equals one was proved in this paper of Cordero and Clartag. And the proof is essentially the same, conceptually the same. You define a functional which looks very similar, you minimize it, you don't have Blaschke selection theorem, so you have to work to show that the minimizer exists. But you show that the minimizer exists, you write what it means that the gradient is zero and you get exactly what you want. And you get exactly what you want. So at least on a very, very high level, the proof is the same. And then you can ask, okay, what will I do for other values of p and I concentrate on p between 0 and 1? For the theorem is still true. This is my theorem. If I give you an even finite Borel measure, finite first moment, not supported on any hyperplane, then for every p, I can find a local k function such that the p area measure is proportional to mu. The P area measure is proportional to mu. And if you try to mimic exactly the same proof, you have a problem that this SFP is not, has, has no invariance properties. Right here in the previous theorems, we were able to write this as an unconstrained minimization because multiplying f by a constant doesn't matter, or multiplying k by a constant doesn't matter. By a constant doesn't matter. He didn't read it. I have like 20 seconds. Right? So we have this homogeneity property. And we don't have it in our case. So we cannot write the problem as just minimize some functional. So what we do instead, we minimize this functional under some constraint. We cannot rewrite it as an unconstrained optimization problem. So we just don't. So we just don't. We just solve the constraint optimization problem, and then we basically use Lagrange multipliers to show that if I have a minimizer under some constraint, I have some condition. So let me write it just last slide with a bit more details. I have some domain of all even local k functions with positive support. I have these two functionals. I need to show that I attain the minimum under this constraint. I need to Constraint, I need to show that actually this minimum is in the interior of D. Because if it's on the boundary, you cannot say that the gradient is zero. It needs to be in the interior, so you also need to show this. Then you need to show that, well, by Lagrange multiplier, the gradient of i is proportional to the gradient of j at the minimal point. You don't really have a Lagrange multiplier theorem, right, on infinite-dimensional spaces like this. But here you can make it work and then find. It works. And then finally, you have to compute this gradient and check that you actually get the correct. So there are a lot of things I didn't tell you, but that's basically the big scheme of ideas. And I think I will stop here. So thank you. Questions? So how do we get the position?