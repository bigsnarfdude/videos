So thanks for the introduction. So my talk, of course, is related to the documents. So I'm going to say, of course, many things that all of you know, but just to be sure. So today we are just going to be looking in some sense of the most basic variational problem. So we want to minimize. Variational problem, right? So we want to minimize an integral functional where the energy density only depends on the gradient of the map. So u is a map from a domain omega in Rn to Rm, and today everything is vectorial. So m and n are at least two flows. And as you all know, in vectorial problems, a crucial feature is that the energy density f is often uncomplex. And of course, this is a source of spray difficulties. So in particular, in a linear elasticity, In particular, in non-linear elasticity, you can think that f is the stored energy function of your elastic material, which has a reference configuration on media, and then you deform it according to Maps U. And so, of course, the precise choice of F depends on the precise material that you have. And in linear elasticity, the energy is essentially never convex. So, let me give you a super classical example. So, you take an elastic rod, and you prescribe this place. And you prescribe displacement by the conditions which compress it. If you are in a model without gravity, it's clear that the robot can either bend downwards or upwards. If you are in three dimensions, it can also bend sideways. Clearly, none of these solutions is preferable to all of the others. And this kind of behavior of, this kind of non-uniqueness of critical points tells you that the energy just cannot be complex. So even in some sense trivial problems, in elasticity, the energy is not complex. In elasticity, the energy is not convex. So, convexity is just not the correct condition. So, let me give you an example of a class of energies that you can consider. So, this is just an example. So, it's a kind of new model where the energy splits into two terms. So, there's a first term which is a function of this quantity. This quantity here will appear throughout the talk. It measures the distortion of angles, okay? So, this is the operating one. So, this is the operator none, and it measures the distortion of angles. And then there's another term which measures the distortion of volumes. Okay, so you know, it's not a fundamental law by any means, it's just an example. It's a classical example, it goes at least back to Florey in the 60s. And it's sometimes called the additive isochoric volumetric split, because the first term is the isochoric term, it measures distortion of angles, and the second term is, of course, the volumetric term, which measures distortion of volume. Distortion of all. So you see again very clearly that this kind of model just cannot be convex because, for instance, the driving it is not convex, and this quantity here is also not convex. So again, you know, these energies are never convex, essentially. So what is the correct condition as a Dolphin-Chinese off? The correct condition in the factorial comes up of variations is quasi-convexity. So here's again the definition of quasar convexity. It means that the It means that the volume of omega times of f of A is less or equal than the interval over omega of F of Eu, and this must hold for all maps U which are smooth and are equal to A on the boundary. So what does this mean? It means that's what yeah sure. I of X yes. Uh yeah, yeah, of course. Um so this means that linear maps are So this means that linear maps are. Otherwise for me to be u of x is I. So I'm definitely so this just means that linear maps are minimized, right? On the right you have a competitor for A because you have to see. Have a competitor for A because you have the same linear boundary conditions. So, quasi-complexity just means that linear masses are minimizers for energy. Of course, this is very reasonable from a physical perspective, and it's also the correct mathematical condition. So let me, again, in connection with the Lofos talk, rewrite this condition in a slightly different hope, in a slightly different way. So you can rewrite this as this kind of inequality, right? So because u is equal to y on the boundary, the average of du over omega is just a. Of du over omega is just a. So you can rewrite this inequality as this kind of Jensen inequality. So you see, quasi-convexity is really a Jensen inequality for gradients. The Jensen inequality tells you that if f is convex, you can pull it inside an integral with a less or equal sign. Okay, quasi-convexity tells you you can do the same, provided you have a gradient with minimum conditions. So quasi-convexity is just a Yens inequality for gradients. And as you probably know, if you assume As you probably know, if you assume standard growth conditions, then classic complexity is basically equivalent to the existence of minimizers in W and P. And this is an extremely classical result. It goes essentially back to Moore in the 50s. And there have been contributions by many people. I just listed here some names. There is, however, a problem, which is, of course, in the coupled variations we'll have to assume growth conditions, but unfortunately these growth conditions never hold in hellasticity. Growth conditions never hold in elasticity. And so, you know, it's basically an open problem in the field, at least since the 80s, to prove the existence of minimizers for quasi-convex energies in inelasticity. So inelasticity, the energies typically become unbounded in the region of small Jacobins. And again, you can see in these examples that I wrote before in the book. Like, for instance, this term will blow up as the Jacobian goes to zero. And the reason this high. And the reason this happens is pretty intuitive from a physical perspective. What does it mean that Jacobian goes to zero? It means that your map is mapping sets of positive measure to null sets. So we are having interpenetration of matter, and it's essentially the regime where elasticity breaks down. Elasticity is about reversibility. If you have this kind of behavior, it stops being reversible. So you expect strange things to happen in your energy. And so, as I said, it's not clear how to use quasi-backs. It's not clear how to use quasi-convexity in all linearisticity since there are basically no growth conditions. These functions become unbounded. Okay, so let me give you the main example of a quasi-convex function. The main example, as you probably all know, is the determinant, right? So the determinant actually satisfies the quasi-convexity inequality with an equality. So the volume of omega times the determinant of A is equal to the integral over omega to the determinant of u. I mean, in the unlikely case, it's never seen before. In the unlikely case, I've never seen this before. This is basically because the determinant of the u can be written as a divergent, so you apply the divergent theorem, you just pick up a boundary term. So this is sometimes called the nullagring property of the determinant. So the determinant is our main example of a quasi-convex, non-convex function. It's clearly not convex because there are directions along which it decreases very fast. So indeed, quasi-convexity is strictly weaker than convexity. Than convexity. And positive convexity implies another condition which is much older, in fact, and it's called random convexity. So random convexity just means that you have convexity along ranking segments, right? So if you take two matrices B and A, which are random connected, then you have convexity along the segments A and B. That's just what rank on convexity is. So if you have a smooth function, this just means that your Lagrange system is elliptic in the sense of Legendre Admars. In the sense of the gender adamant. So it's really, in some sense, a regularity condition for critical bodies, right? So it's a much older condition, and okay, it's natural from the regularity theory, but for us it's very natural because it's a necessary condition for quasi-convexity. So of course, whenever you see an implication, you like to know whether it can be reversed. And this is what Maury asked in the 50s. So Maury asked, does Rankine convexity imply quasi-convexity? And you know, at this point, And you know, at this point, you have to realize that if the answer is in any way yes, this is one of the most remarkable theorems in mathematics. And the reason is the following. You see, these are notions from completely different worlds. Rankpond convexity is a condition which in some sense is trivial to verify. It's a calculus condition. How do you verify if something is rank-punk convex? You take two derivatives along a certain direction. It's positive or not. It's something that you can do numerically, you can do analytically. It's very easy in principle to verify. To verify. Quasi-convexity is completely different. I mean, to verify if something is quasi-convex, you have to solve an infinite family of variational problems, right? That's what it's saying that linear maps are minimized. So you see, if franklin convexity implies quasi-convexity, this means that you can solve variational problems for free just by taking derivatives. You take some derivatives along certain directions. If it's positive, then you have solved the variational problem. 100%. Right, one convexity implies. Convexity implies ellipticity for Euler Lagrange or degenerate ellipticity? It depends what you call ellipticity, right? It is equivalent to ellipticity in the sense of Legendre Animal. It's not strong ellipticity, some people refer to. It's really equivalent. I mean, let's put it like this. If you have um if you have a quadratic form, if f is a quadratic form, I mean it's it you know, it says ni it says nicely. You would call this a german truly. You would call this a german to this? Yeah, because it's it's positive semi-definite. Yeah, it's a semi-definite. Ah, okay, this is what you mean, right? It's not uniformly, it's not uniformly recognized. Yes, of course. Sure, sure, of course, of course. Sure, sure. I understand, I understand. Yeah, you're right, you're right. Exactly. I mean, in order to prove regularity, you have to make all of these conditions uniform, you know, to make them exactly uniform. Yeah. Okay, so of course. Okay, so of course, you know, as you probably know, the answer is unfortunately no, in higher dimensions. So in 1992, Schwarak showed that if the target is three or higher dimensional, then rank and convexity does not imply quantum convexity. And there's a more recent example by Grabowski, which takes even higher dimensional targets. Okay, and the reason I mention these two, you know, also Grabowski's example is that these are the only examples we have. There are essentially no other examples of the failure of this implementation. So, if you want, this is what I love with saying that no one understands quasi-convexity. So, of course, you know, as you see, in particular, the case of two-dimensional targets is open. So, what happens for two-dimensional targets? Does random convexity imply quasi-convexity? No one knows. And there's work by many people on this problem, which suggests that the answer is yes. So, let me in particular highlight the results of secular theory with Farakman Kirheim, which in my opinion are Which, in my opinion, are the deepest results in this direction. So they prove very general results, which in my opinion suggests that the answer should be yes. So of course, you know, today I'm only going to talk about the case of two-dimensional targets. So let me tell you now our main theorem. So here's just a little bit of notation. So R2Y2 plus, as usual, is the set of 2x2 matrices with positive determinants. Determinant and Ka is again the distortion of the matrix A. So it's the distortion of angles. It's the operator norm squared over the determinant. So here's our main theorem. This is joint work with Astava, Faraku, Koski, and Christensen. So let's take an integrand F as in this new Hooking model that I showed you. So it splits additively as a term of the destruction of angles plus a term of the destruction of volumes. And let's assume that the term of destruction of volume That the term of the structural volumes is convex, then for this class of energies, we can actually solve Moni's problem. So, for this class of energies, rapid convexity implies quasi-convexity. And moreover, we can show that quasi-convexity implies with lower semi-continuity. Again, this is not completely obvious because these functionals do not satisfy any kind of growth conditions, in fact. So, what do I mean by with lower semi-continuity? I mean the following. Give me a very nice bound. I mean, the following: give me a very nice boundary data, give me a sequence of maps with this boundary data which converge weakly in W12 and their distortions are suitably bounded, then I can pass, you know, I have the limit inequality. So the limit of the energies is greater or equal than the average of the surface. So this is our main theorem. What's the assumption of G? There's no assumption. But you know, of course the theorem is only non-vacuous for rank and convex functions. So in particular G has to be leap shifts, for instance. You know, in particular, you have to be leap sheets, for instance, for it in order to apply. And you know, more generally, there are Baker-Erickson inequalities, separate convexity inequalities, whenever you want to apply it. But a priori, you make no assumption. You just say, give me a function like this which is rank on convex. I can tell that it's quite a convex, and I can tell you that it's actually with lower semi-continuous. From the point of view of the minimization problem, do minimizing sequences satisfy the compactness? This condition. So, this is a question. This condition. So this is a coercivity issue, what you're asking. I mean, it's also not clear that any minimum you see there's no direct return in this energy. So there's no reason why the minimizing sequence should be bounded in W12, for instance. So what you have to do is you have to make it coercive in a suitable sense by adding theoretical terms. So this is just a weak uh if you want to prove existence you need coercivity and weak lower semiconductor. This is just a weak lower semiconductor. No, I mean, but as I said, you know. This only applies when f is ranking convex. Yeah, but what are the GCF requirements from which are convex functions? I will give you an example in a moment, but let me tell you that there are non-polyconvex functions in this class right away. Is there are there more questions? Please uh ask uh and the condition on uh the boundary condition is rigid, right? condition is rigid, right? You cannot touch that, right? You mean if I replace G with another with a sequence or anything? I mean then you can worry about concentrations on the boundary, for instance. Okay, the condition ensures, for instance, you know, I want some condition which ensures that my sequence is composed of homomorphisms, for instance. So, you know, I want this, you know, the fact that you, in fact, you know, once G is a homomorphism, it follows from the It follows from the greed theory that they are homomorphisms as well, because they have some bounds on the distortion. So they will become, for instance, so I need something like that. I mean, I just say it as a diffeomorphism. I'm not disclaiming. I mean, of course it's not automorized. It's just one can take. I think one can take, you know, as long as G has a nice extension, which is like essentially as long as G satisfies, is in W12 and the distortion satisfy the same boundary. Right, so what is Kuj? Let me just make this clear, right? So Ku for me is du squared over the determinant of du. Yeah, yeah, it's like in for the linear map. I noticed that I understand. Sorry for what I need to do for the picture. I understand. Sorry for the Sorry for the confusion. Yes. Are there more questions? No? Okay. So, okay, so in the time that I have left, the goal of my talk is to kind of give you an outline of the proof and try to explain the main ideas that we need to introduce in order to prove this result. So let me just first give you some buzzwords. Just first, give you some buzzwords. So, actually, there are two statements, right? The first statement is just there are two arrows, right? So, we are going to prove each arrow separately. Let's just first show that rampant convexity implies quiet convexity. And then let's show that quadrant convexity implies with lower semi-continuity. So, for the first part, there are going to be two ingredients. Again, these are just buzzwords at this point. The first is the idea of external integrance and the broken function. And for the second part, we are going to need Jensen inequalities for principal mouse and the stall for. So, okay, let's get to it. So, okay, let's get to it. So, let's talk about the first part, which is Morris' problem in this class. So, let's just first recall something from convex analysis, right, which is a query-millimeter theorem. So, the query-millimeter theorem tells you that if you have a polygon, for instance, which is convex, then this set is the convex whole of its vertices, right? So the vertices are the extreme points. And this is a general fact about compact, convex sets in manuscripts. Compact convex sets in banana spaces. In my first paper, I proved a version of this theorem when k is the set of random convex integrands, essentially. So you need to, you know, it's more complicated because it's certainly non-compact, it's a cone, and moreover it contains lines. But okay, there is a notion of extreme integrand, which is somehow the correct notion, which allows you to prove again that the set is the convex hole of its extreme points, okay? Of its extreme points. So, what does this mean? It means that looking at extreme points, you know, what is the point of looking at extreme points? It gives you a very efficient representation of your set, right? Instead of describing this pentagon by a bunch of equations, I can describe it with five points. So, extreme points are a very efficient way of codefying convex sets. Certainly, a set of random convex integrants is convex. So, let's just look at extreme integrants, essentially. Look at extreme integrands essentially. This is the main idea. If you can solve Morris' problem for extreme integrands, you have solved it. So, in principle, it should be easier. So, the nice thing is that in our class, the extremal structure is very simple. So, this is a nice observation by Gloss, Markov Lib and Neff. So, they show the following. Let's take f as in the theorem, which is rank and convex. Then I can write it as the sum of two functions. So, the first function, g, is I call a convex function. Function G is a polyconvex function. The second function, so this is also to answer Irene's question: what kind of functions you have in mind, this is actually the main function that I have in mind. It's this function w, which is completely explicit. So it's distortion of a minus log of the distortion plus log of the determinant. So this function is not polyconvex. So let me just recall, a polyconvex function, again, is a function which is a convex function of A and of the determinant. So it's, so of course, quasi-convex in. So, of course, quasi-convex in particular, it's very close to being convex. And W is not polyconvex. So, why is that? I mean, if you compute the limit of W of t times the identity as T goes to 0, so let's just do that. The distortion of the identity is 1. So, this just becomes 1 minus 0. Log of T times the identity is T squared. So, it's 1 plus log T squared. And as T goes to 0, this goes to minus infinity. And this never happens for polyconvex functions. If you have a polyconvex function, this limit is always function. Function, this limit is always finite. So w is certainly not polyconvex. And you see, this is the strength of extremal integrance. You start with a huge class of functions, and you can decompose it as a polyconvex function plus a multiple of W. By the way, this does not mean that w is the only function in the class which is not polyconvex. If you perturb w, for instance, it will still not be polyconvex. It just means that it's the only extreme point in the class. Extreme point in the class, which is not polycomplex. So, you know, instead of looking at the full class of families, we just have to look at one function, which is W. So that's the power of looking at extreme points. Do you have a similar theorem for higher dimensions? No. So, okay, let me say this right now. I mean, everything that I'm going to talk about today is purely two-dimensional, and I have no idea how to do any of it in higher dimensions. Almost all of, I mean, actually, all of the steps that I will talk about. Even your 2018 current is only? No, actually, this one, yes. Tinker is only a bit more. No, actually, this one, yes, but this is kind of pretty abstract. So it's, yeah. How can you understand what an extreme point of a code is? It seems like it's kind of... Yeah, I mean you have to fix a base of the it's it's complicated in fact because the code has lines. But if it does not have lines, right, you can just look at the base. Right, so you have a counting and then you look at the base uh th i and then and then you look at the two points of the base. It's more complicated than these because there are actually four lines in the code. Actually, full lines in the code. So you essentially have to model these lines. I mean, you know, there's actually this notion of extra malatable is introduced by Sherra. So there's a correct notion. But it's not obvious what it should be. I mean, this is more like a heuristic. So this is the first thing, right? So we just have to understand a single function in order to prove our thing. Okay? So, okay, what is this function that looks completely? function that looks completely crazy. I mean this function is actually in some sense completely canonical. Okay and I will try to convince you of this. So for this I have to talk about the second ingredient which is the Berkholder function. So you will have to go with me on a slight detour. So what is the Berkholder function? So the Berkholder function is a function that was found by Berkholder in 1984 in a completely different context. He was proving sharp Martingale inequalities. And it's completely explicit. And it's completely explicit. So in our notation, it reads like this: it's p over 2 minus 1, norm a squared, minus p over 2 determinant of i times norm of a to the p minus 2. Okay, what does this mean? Let me help you digest a little bit. The way you should think about this guy is as an L P version of the determinant. So why do I say that? So first of all, at the identity, it's always minus one, because this is one and this is one as well, so the p over to cancel. And at p equals two, it's just the determinant, the first terminal is, right? The first terminal is, right? So that's why I mean it's an LP version already. At equals to its determinant. More generally, it's P homogeneous, right? So the determinant is to homogeneous, the norm is to homogeneous, this is P minus to homogeneous. It's still isotropic. So if I replace A with QAR, where Q and R are rotations, I get the same. Again, like the determinant. And of course, very important for us, it's Reign convex. Okay, so of course when you see a Rankman convex function, When you see a Rankine convex function in R2 by 2, you ask immediately, is it quasi-convex? And this is what Ivanich did in the 1990s. So it's a long-standing conjecture that the Berkeley function is quasi-convex. I don't want to get into this, but let me just say, you know, forget about Maurice's problem in this whole generality. Even if you could prove that this completely explicit Reynolds convex tension is quasi-convex, this would have enormous repercussions in harmonic complex synonyms. So even these. So, even these particular functions proving its quasi-convexity is already extremely hard. So, we cannot prove quasi-convexity of the particular function, but we can prove some kind of partial quasi-convexity. So, here's what we can prove. Give me a map u which is linear on the boundary, and moreover, assume that B of the U does not change signs, so it's either non-positive or non-negative. So, this is, again, trying to work with Jan Christensen, and also again with Ascala Farako and Kowski. Farako and Koski. And so, under these conditions, we can prove the quasi-convexity evaluative for the Berkholder function. So, in some sense, we have quasi-convexity F2 assignment. So, I don't want to get into the proof of the theorem. I don't have time. Let me just mention that there were earlier results by Anstali Vanischpraut and Saxman between 2012 and 2015. And that our proof combines essentially their beautiful complex interpolation argument with the kind of extremality argument for workload. Extremality argument for the vertical function. So, okay, what is this workload? So, how does this relate at all with this W that we saw? Okay, what is the connection? So, let me explain to you. Inelasticity, it's not what we consider an involution hat, which essentially looks at the energy of the inverse map. So, f hat of A is, by definition, F of A inverse times the terminal. And the reason we define it like that, as I said, is that if I have a map U with inverse P, and the F energy of U And the F energy of U is just the F hat energy of U. Right? This is just the change of variables. I mean, here you have the inverse, and then you have the volume term that you need to change variables. So this is just the change of variables. So a nice thing about this involution is that it preserves all of the semi-convexity notions in the counter regulations. Although it does not preserve convexity, right? So if I take f equals 1, which is manifestly convex, f inverse is a determinant, which is not. Is a determinant, which is not. So again, in elasticity, convexity is just not a natural condition. Okay, so this is just an operation that I can do. And now here's the connection with Bertolder. So I have now this one-parameter family of functions vp. So let me differentiate it at p equals 2. So remember, b2 is minus the determinant, so the derivative at p equals 2 looks like this. Ah, but this is the sum of two functions which are, let's say, quasi-convex, or partially quasi-convex, right? Or partially quasi-convex, right? So, in the limit, so certainly the sum, it's still quasi-convex. So, in the limit, it will be quasi-convex as well. You can calculate the limit explicitly. It's this kind of function. So, it's a norm squared minus a term with log of the norm squared times the term. Okay. And now I look at the inverse time, at the inverse energy, and I get exactly W. So, W, so how do you get W? You have BP, you differentiate it at P equals 2, and then you look at inverse energy. So, I'm sorry. Yes, please. I'm sorry. It's in 2D, right? Yes, everyone. So in 2D the inverse is is really just some shifting of the of the of the columns up to the electronic part, right? So so I'm a little bit no, so this F hat is expressible explicitly in terms of F of course, yeah. In a very simple way in that, you don't In a very simple way, like that, you don't need to talk about a minus one, right? Sure. I can write it. I mean, I can write it like transpose or cofactors. No, the cofactor is just switching the cofactor. Do you like that more? I mean, this is not cool because it tells you where it comes from, right? It's just the energy of the inverse line. No, no, it's fine, it's fine. It's completely explicit, right? It's easy to prove. It's easy to prove. It's easy to do. It's completely. I mean, you know, you can calculate this in two minutes. I just. I'm not going to do it for you because, you know, there's no point. I mean, I'm just telling this is the. Okay, so what... So you see, so F is quasi-convex, or partial quasi-convex, and then so is W. So here's the corollary, right? So F and W are partial quasi-convex because we have our result for recorder. Okay, let me just say, maybe you were asking, didn't you have some kind of sign condition? Yeah, but here I'm assuming that the map is. Yeah, but here I'm assuming that the map is a smooth diffeomorphism, right? So I have a sign condition on the G code. So that's why it's in. So let me now just pause for a moment. I mean, okay, these inequalities, what are they? I mean, I want to convince you that these are actually beautiful inequalities. So let's just look at inequality for F. So as you all know, in the 1990s, Stefan Muller showed that if you give me a map in W12 with the Jacobian not positive, then the Jacobian is in L or Yellow. So the first inequality is actually a sharp global version of Java. Actually, a sharp global version of this. So let me show you. So, okay, let's just look at f again, right? It has a norm term and it has a determinant times log norm squared. So if I just rewrite, let me just rewrite this person for you. It looks like this. The determinant times log of the norm squared is bounded by the w1 to norm plus a boundary term in chain of the variables. And you know, this is a constant one, and this constant is sharp. So this is attained, equality is attained here for an infinite-dimensional family of mass, if you will. Dimensional family of maps, if you want. So if you want, you know, this is the L-log L integrability of the Jacobian with precise constant in the plane. And of course, in a polyfor node, you have a similar interpretation in terms of the inverse. So in that case, there's a result of Pascal in there, which essentially corresponds to this estimate. So that's just what I want to say. So again, that's why quasi-convexity is difficult. If you can prove it, you are essentially getting sharp integral estimates, with the sharp constant. With the sharp constant. Okay, so this proves the first part of the theorem, which is that Reckoner convexity implies quasi-convexity in our class. So let's just talk a little bit. I think I have like 10 minutes kind of about the second part, which in some sense is a bit more robust. So, how do we prove weak lower semi-continuity for energies which do not have growth conditions? Okay, so for these, I So, for this, I need to talk about principal maps for a second. So, bear with me. So, because I'm looking at maps from the plane to the plane, let me just use complex analysis. So, I'm going to say that a map is principal, if it's a map which is holomorphic outside the unities, and essentially it's the identity at infinity. So, it's a term which is z plus b1 over z plus b2 over z squared, and so on. So, it's essentially a holomorphic map outside of this, which is the identity at infinity. Which is the identity at infinity, and then it has this kind of Lohan expansion. So, why do I care about these maps? So, the reason I care about these maps is that this essentially generalizes maps which are linear on the circle. So, give me a map which is linear on the circle. So, in complex notation, it looks like the... I mean, I'm just saying that the conformal factor is one for simplicity. So, it's a map which is z plus b1 z bar. I can write essentially any linear map type to multiplication, right? To do multiplication, right, like this. And now I'm on the circle, right? So on the circle, z-bar is also 1 over z. So this means that a linear map on the circle extends as a holomorphic map outside the circle. And it's a principal map, in fact. And it's a principal map where all of the coefficients are zero, starting from the second coefficient. So principal maps are a larger class, in some sense, than linear maps, than maps which are linear. Than linear maps, than maps which are linear, with linear boundary conditions, right? So these are like maps with holomorphic boundary conditions. In general, of course, they come from linear maps if and only if all of the coefficients after the first one are zero. That's pretty clear. So this is a more general class of maps than linear maps. So here's actually what we can prove for this W. We cannot just prove the quasi-convexity inequality I showed you, but we can prove a Jensen recall that quasi-convexity is like a Jensen inequality for gradients. A Jensen inequality for gradients, provided you have linear boundary conditions. Okay, so we can prove a Jensen inequality for gradients without linear boundary conditions. Instead, you know, the Mach U is just assumed to be principal. So it's like a Jensen inequality for gradients with holomorphic boundary conditions, if you like. So we were very surprised by this. Uh I mean how can it be that you can have a Jensen inequality if you do not have boundary integrated, if you don't have linear boundary conditions? If you don't have linear boundary conditions, how can this be? So let me explain to you how can it be. So let's just take for simplicity p1 equals 0. Okay, so the reason I want to take p1 equals 0 is that the first term is 1 over z, so it decays very slowly. So let me just say that it doesn't exist. If it doesn't exist, you can compute this integral very easily, and it's just going to be the identity. So this is what we want to show. We want to show b1 is 0, and we want to show that this Jansen interpolate for a map which is This Yansen equal for a map which is holomorphic. So, again, here the explicit form of W is going to be important. So, this is what I want to show. And remember, U is a principal map. So, this means that it's the identity at infinity, right? So, okay, quasi-convexity likes linear boundary conditions. So, certainly, since you have a linear boundary condition at infinity, you have quasi-convexity over the whole plane, right? Because you have to do some micro-boxing. You have to do some kind of approximation argument to get this, but it's not difficult. So you have linear boundary conditions at infinity. So this is just a classic convex inequality over the full plane. So in order to prove our desired inequality, we have to see what happens outside the unit distance. And so let's just see what happens. So let's call C the integrand. So C is W of the U minus W of the identity. And let's just compute. So remember, outside the unit disk, the map is holomorphic. So the holomorphic Is holomorphic. So the distortion of the holomorphic map is 1. So 1 minus 0. And it's holomorphic. So the Jacobian is just a norm of u prime squared. So you compute exactly, and w of the u minus w of the identity, you just get this term, which is two times the log of u prime mod u prime. Ah, but you see, if u is holomorphic, then this function is harmonic. And this means that the integral outside is actually zero. That the integral outside is actually zero. So, why to that? So, you can think that you can apply the mean-value theorem at infinity. So, at infinity, I mean, remember, u prime, u is like z plus blah, blah, blah. So, at infinity, u prime is like 1. So, log of 1 is 0. So, psi at infinity is 0. So, psi at infinity is 0. And now just think that you are taking spheres centered at, you know, think about a Riemann sphere, you take disks centered at. You take disk centered at infinity and you make them larger. This gives you exactly the integral over the complement of the disk as you make these poles larger, right? I mean, if you want, you can invert everything at zero, but this gives us the same. So this is just a mean-value property. And of course, psi is our integral. So actually, there's no energy seen from holomorphic maps with this kind of object energy. So if you want, this is a general fact which says that the component of the unit disk is a null quadrature domain. The unit disk is a null quadrature domain. So, harmonic functions integrate to zero on this domain. So, that's how you get this inequality. It's essentially because of the special structure of W, there's no energy seen outside the energy system. Okay, so I've now given you this kind of crazy Jensen inequality for principal maps. So, let me just conclude my talk by. So, let me just conclude my talk by explaining to you why this is essential to prove a lower semi-continuity statement. So, this is what we want to prove for Delta. Again, we have a sequence which converges weakly in W12, bounds on the distortion, and you want to prove weak, lower, semi-continuity along this sequence. How do you do this? You use the theory of young measures. I'm not going to get into this. I'd also mention it briefly. Let me just say that if you don't like gang measures, forget about it. It's just a machinery that essentially allows you to reduce to the. That essentially allows you to reduce to the case where the sequence converges weekly to a linear map. Okay? So that's what n measure allow you to do. It allows you to assume that the limit q is actually cleaning. So let's just do that. Now you want to replace this sequence with a new sequence to which you can apply quasi-complexities. So because we have maps with the distortion, bounded distortion in L1, there's a beautiful result by Banich and Furach that say that That says that you have a stove of arguing functions. So this means that you can write these maps as a holomorphic function composed with a principal map. So principal maps, again, remember, are maps which are holomorphic outside unit disks. So this is a general fact about maps with these bounds on the distortion. So you can factor them as a holomorphic function composed with a principal map. Now, it's not difficult to imagine that the sequence is bound. That the sequence is bounded in W12. These guys are holomorphic. If you have any bound on a holomorphic function, it converges in any topology. And moreover, our sequence is converging weakly to the identity. So it's pretty easy to imagine that the holomorphic factors will converge to the identity in any norm that you could carry. So what can we do? Essentially, we can ignore the holomorphic factors, right? So we have our energies, W of duj, and now we can just replace And now we can just replace them with w of dfj because it's the same f dual term which converges super nicely. Ah, but you see, these guys are principal maps, and we just proved a Jensen inequality for principal maps, which is this one. So this is exactly the weak lower semi-continuity inequality that you would like, right? It's exactly this. That's why you need to. So you see, there's just no way once you do that, once you do this. That, once you do this, that you will get linear boundary conditions on the FJs. There's just no way. It will never happen, essentially. So it's really crucial that you have an inequality, a Jensen inequality, with more general boundary conditions in order to be able to get a lower magnitude. Okay, so this is kind of the end of my outline. So let me just leave you with an outline of the proof. We have, let me just give you a quick summary. So we saw that the quantum numeral theorem essentially shows Theorem essentially shows that we only have to understand a single function, w. And this function w comes by taking derivatives f equals to one percent of the function and then passing to the inverse function. So this is how we got that W square convex. So this gives us the solution of Morris problem plus. And then we saw that using the theory of non-pointer domains, this W actually has a kind of super Yens inequalities. And then by using the stall factorization and some young matter theory, we actually get this sequential result. Against this sequential leak lower semiconductor. So, this is kind of the output of the rule. So, let me stop here. Thanks very much. So, having compelling examples where the condition on KU being LQ sense. Right, so again, this is a matter of it's what I was saying before. So, it's a matter of coercivity. Again, you know, this assumption at the level of the Ethereum is At the level of the Ethereum is not completely natural because you like weaken vergence, but it doesn't come from the energy, right? So you would basically, of course, it's natural, you know, if you have your W and you add, so if you want to prove an existence theorem, you need to add certainly theoretically term in order to get bounds, right? And then you would add a term like then you have bounds, you know. And this is a term. It's not very good colour. I cannot debate that. So, you know, I think the point here, I'm not completely, I'm not so obsessed about what is W exactly. I think, you know, what I'm just trying to emphasize is that. Can I try again? Sure. Do you have examples of sequences where dev u goes to zero, but for? Oh, for sure. Yeah, yeah. I mean, you can find maps. W actually, you can find. W actually, you can find like radial maps which have for singularity at zero, you know, the inverse is not super nice at zero. And they have finite W energy, and you know, the distortions are, you know, this is a fairly weak condition, right? I mean, I'm not sure what you're... Are you asking what kind of to what kind of sequences can you apply the theorem? This is what you're saying. I'm just wondering about sort of worst case sequences, which all of the things that look like what you're trying to When you're trying to yeah, cautions cannot occur in this kind of thing. I mean, it's more like certainly a sequence of quasi-conformal maps, right, with uniformly bounded distortion. I mean, this is a sequence, you know, these maps can be pretty bad. They can have zero-Jacobian on fairly large sets. I mean, no sets, but still, you know, with a certain household dimension. So that certainly allows my differing. I mean, they could have zero Jacobian on many points, let's put it, like Latus. Um yeah, I mean I don't know what to tell it feels like uh when you so linear maps are not are not are not are not are not formal. Uh so like you know like uh so Yeah, I'm not saying you see this is yeah, exactly, but this is more you know, this essentially allows it to reflect the known the non-all part of a linear map, right? So this is very special about unities. You cannot put other domains here, but very special. Yes. To come back to the additional terms that you have had here in your functional loading, is uh KQ lower than T. This term is fully complex. It's fully complex, okay. Yes, yes. So just to go back, I mean, in the end, if you want to prove an existent theory, you need to have perceived the conditions, right? So you need to modify your function appropriately. I mean, this is the case already for polycomplex functions. If I give you... I mean, I guess it's my fault because I forgot the first part of your talk where you introduced the little examples of hyper-elastic energy. Yeah, I mean, right. Yeah, I mean, right. I mean, this energies model like some slightly compressible materials. But, you know, one can discuss how physical this is in any way, right? I mean, these additives play this. The point for me is that, you know, this W is kind of canonical object which comes from detection theory file. 