I'm sitting here with a sick kid who I have distracted with a movie. So, hopefully, everything will be good. All right, so thanks again for the invite to talk today. I know that everyone's been seeing a lot of interesting and different content. What I'm going to talk about is safe learning in reinforcement learning and mHealth. And as a motivating application, we're talking about this project HIDRA. This is a joint. This is a joint project with Duke, UNC, NNC State, and it's focused on promoting safe weight loss in persons with type 1 diabetes. And so the idea is that you help people plan exercise and diet changes in a way that is efficient and pleasant. So there's been several mentions of diabetes, both type 1 and type 2. And with persons with type 1 and diabetes, they try to change their diet. Type 1 diabetes is they try to change their diet and exercise behaviors, they can be quite a lot of negative feedback because their blood glucose goes out of range and that's very unpleasant. So you kind of get this negative feedback for trying to sort of make your lifestyle more healthy. So the idea is that through this app, they can say, oh, I'm going to exercise at three o'clock today, and it can give recommendations for how you should adjust your diet and insulin. So you can see here, it allows you to, these are screenshots from actually a couple of versions. A couple of versions ago, the app, but it allows you to set what your goals are: so, whether it's an exercise-related goal, performance-related goal, weight-related goal, or something else, set your starting blood sugar range, and then it provides informational feedback to just help you understand your own responses. So, connects to your blood glucose monitor. And then here you can see it's giving a recommendation. So, it says it's almost time, your next plant exercise is in 30 minutes. You should take a snap. Uh, you should take a snack, you should snack it as 15 grams of carbs and adjust your insulin as follows. Okay, so the idea is: how do we build something like this that learns the unique biology of each person? And I showed this diagram on the first day, which I'm sure no one remembers given the amount of content between then and now. The only difference between this diagram and the one I showed in the review is that now I've put health words instead of just generic words. So it's a reinforcement learning problem where you observe the health state of the. Observe the health state of the patient and then you select an intervention. So, this could be a diet, something a snack or a meal, and the macronutrients in that meal, or it could be an insulin adjustment. And then you give them the intervention, and then you observe the response. In this case, it might be, did they successfully complete the exercise? And was their blood glucose level in a safe range? And this is an online learning problem, so we don't know what the optimal recommendations are for each person. Don't know what the optimal recommendations are for each person when we start, and so we have to learn from information as it accumulates. But a key difference between this and other reinforcement learning problems, say in advertising, where I want to show what's the best ad for you based on what you searched in Google, is that we care a lot about doing this safely, right? So there's a lot of risk here if you're making bad recommendations, right? So it's potentially quite unsafe. And so we want to make sure that we are learning, but we're doing it in a way that doesn't expose the patients to risk. So that's the goal. To risk, so that's the goal. So, there's a lot of work in this area, or at least areas sort of closely related to this. In the status literature, and I won't go through all these references because this is a short talk, but there's been work on a constrained regime. So you maximize efficacy subject to a constraint on risk, but all this work has been done in the batch setting. So not in the online learning setting. It's more they run a trial. After the trial is over, maybe they look at side effects, the trade-off between side effects and efficacy, but not in an online setting. In the CS world, In the CS world, engineering world, there's been a lot more work on online safe exploration. And I listed a bunch of different kinds of criteria that are used. But the key difference here is that they assume that there's some nice composite measure, at least in the work cited here, that captures the utility associated with both risk and reward or risk and utility, which is not always easy to do in a problem like diet and exercise recommend and insulin recommendation. Diet and exercise recommend and insulin recommendation and type 1 diabetes. So, what we want to do is we want to learn in a way that guarantees or controls the risk of each individual patient at every possible time. So, to sort of formalize things, we're going to assume that the data across, let's say, end patients. So, this was run initially. So, we ran a pilot where we had an expert strategy, and then we are now looking at an adaptive trial that learns as. Trial that learns as data is accumulating. But the data are going to be of the form SARU, S-A-R-U, and so on. So you get an S-A-R-U tuple for each time point for each patient. So you have N patients in the trial. That's N. And so I index is patient. And then you have, say, T time points. And so here, ST is the patient state at time T, A T is the intervention. RT is the risk. So lower is better. Is the risk, so lower is better, and UT is utility, higher is better. For the RL people in the crowd, calling R risk instead of reward is probably like nails on a chalkboard. So I apologize. Also, lower values are better, lower risk. So just keep that in mind. And we're going to model this as a contextual bandit so that the states, these S's, are drawn IID, and that the risk and utility depend only on the risk and utility, or sorry, only on the current state and action. But the Current state in action, but the extension to MDPs is more or less straightforward, although the notation is more complicated. So, we're going to assume that there's a set of feasible treatment psi that maps states to a set of allowable actions. So, the idea is there's some things we know are unsafe and there's no need to try to learn that, right? So, if there are things that the experts or the clinicians tell us are not feasible treatments, we're obviously not going to try those to learn that they're unsafe. So, there's always this set. They're unsafe. So there's always this set that tells us in state S, what things can we try? What things are at least acceptable to try? Maybe we will learn they're not safe or they're not effective, but they're at least there's some equipoise or at least some possibility that it would be okay to try them. And in this case, a regime or a policy is a map, say pi, from states to actions so that if you were following pi when a patient presents with state s, you would assign them treatment or intervention pi of s. them treatment or intervention pi of s. Okay, so that's the setup. So we need to characterize the optimal regime, but then also the optimal safe regime. So to do that, we're going to use this sort of regression-based representation. So we'll define the q functions qr and qu. So qr is the expected risk given state and action, and qu is the expected utility given state and action. So u script u of pi is the expectation of qu s pi of s. Of QU S PI of S. That's the mean outcome if you assigned, or the mean utility if you assign treatments according to pi. And we're going to assume when we do our estimation and we define safety that there's some baseline safe policy pi zero. And we're going to define the set of safe actions in state S under tolerance tau to be the set of A that are feasible or allowable, that have risk, expected risk, no more than tau greater than the base. Tau greater than the base policy. So, in our case, this base policy is an expert-derived policy. That's the one that we piloted in our implementation study. So basically, we're going to say we are not allowed to choose or an action is not safe if it has excess risk of tau or more. Tau could be set to zero, in which case you're saying you don't want, you're not allowing any policy that's less safe than the baseline. And we say that a regime pi is tau safe if the actions it select. Tau safe if the actions it selects are in this set of safe actions for all s. And the optimal tau safe regime, say pi tau opt, gives you at least as much utility as any other tau safe regime. So the goal is how do we maximize utility subject to this constraint on safety? Okay. So that's the safe policy, but how do we learn this? We need to talk about safely learning. So we don't want to be unsafe while we're trying to estimate the safe policy. So now we need to talk about safe learning algorithms. So the idea here is. So, the idea here is we want the estimation procedure to remain safe at every point in time. So, let H T denote the information available before the teeth round of treatment assignments is made, and that script P of A denote the space of distributions over actions or interventions. And then a learning algorithm, pi hat t, is a map from history to a distribution over actions so that a patient who has state s t equals little s and around t would be assigned action or treatment a. assigned action or treatment A under this algorithm with probability pi hat t of s. Okay, so this algorithm you can think about you know something like UCB or epsilon greedy or Thompson sampling and so on. But we're going to say that the algorithm is alpha tau safe if for any state the total probability mass on unsafe actions. So this sum is over A in script A minus the set of safe actions is less than or equal to tau plus a term that's negligible. And so the idea is we don't want mass more than And so the idea is we don't want mass more than alpha on unsafe actions at any state. And we'll say that a regime is consistent if it's both alpha tau safe and the utility of that algorithm converges to the utility of the optimal policy or the optimal regime as n and t go to infinity. So that's the goal. How do we construct an algorithm like this? So to make things concrete, we're going to assume linear models here. This applies for basically smooth parametric models in our current framework. Models in our current framework. So the model for the expected utility is going to be psi of USA transpose, say, beta u star. So here this psi u is a, or phi u is a feature vector and a similar model for risk. So then the optimal tau safe regime, if you express it in terms of these models, just solves this simple linear program, right? It's a max over A, or sorry, it's not even, it's just a, it's a simple. Uh, it's just a simple if A is discrete, for example, uh, it's even easier optimization problem. So you just maximize this objective subject to this constraint, that's it, right? So maximize utility subject to a constraint on risk. Uh, so naturally, you might think, well, we'll take the data and we'll just estimate these two parameters we don't know, and then that's it. Uh, of course, if you do that, the plug-in estimator doesn't account for uncertainty in the estimates themselves, which doesn't guarantee safety, and it doesn't explore or learn, right? That would be something like a certainty. Explore or learn, right? That would be something like a certainty equivalence control. And so you're not exploring, which means the algorithm can get stuck in a suboptimal solution and won't improve. So the next thing to think about is to say, well, why don't we just do some other abandoned algorithm that solves this? Maybe I'll do Thompson sampling. So maybe I'll put a prior over these betas. Then at every point in time, I'll draw a beta, solve the system, and then take actions that way. But again, this actually doesn't ensure the safety constraint that we want because it doesn't account for. That we want because it doesn't account for uncertainty in the parameters. So, we're going to do the next simplest thing, which is the split proposed test algorithm or SPOT. Originally, the student who proposed this, who I should give all the credit to ElksCloud, he called this SPIT, which I thought was a pretty terrible name. So I changed it to SPOT. But the idea is very simple. You use sample splitting to control the error rate of a test, and you test to make things sure. rate of a test and you and you test to make things sure to make sure things are safe. So the idea is you randomly split the subjects into two groups, say D1 and D2. So that's all the patients in the two groups. Then you use half the data to propose a candidate treatment for a given subject. And then you use the other half to test the safety of the recommendation. And if you accept, then you assign the proposed treatment. Otherwise, you follow, you fall back to the sort of base safe strategy. And as long as you do a good job choosing the test so that you ensure that it has the right operating characteristics. Ensure that it has the right operating characteristics, then the algorithm will be safe. And if you do a good job proposing candidate treatments, then you'll ensure that the algorithm is consistent and efficient. Okay. So here's the idea of the test. So we're going to assume that at any time T, given state S, we propose some action that's at least allowable. And then we just test this null that it's not safe. So the null is that the action is not is unsafe. So that's the null. And if you reject the On safe, so that's the null. And if you reject the null, then the idea is that it is safe. So, this is the null we're going to test, and this is the test statistic we use. It looks a lot like a t-statistic. I'll note that even though we did sample splitting and we used an estimator that's fitted only half the data, the data are still dependent because you have this online learning algorithm. So, just because you split the subjects, the subjects themselves are still dependent within each subject and across subjects because the algorithm is learning and using. Learning and using everyone's historical data. So deriving the distribution of this thing is not trivial. But you can pretend like it is because it'll give you this, it sort of boils down to the same procedure. But yeah, this is not sort of a standard T statistic, but you can still get its limiting distribution. And it turns out to be normal. And so if this t statistic is below some critical threshold, you accept. Otherwise, you reject. And one critical part. And one critical part is that you don't reject with probability one, you rather reject most of the time. And you have this additional little random piece where you let the outcome of this test be random with probability alph over two. And the reason you do that is that you guarantee that you continue to explore and to learn. Okay, so if we reject and we find that the action is safe, we'll take it. But if we fail, The action is safe, we'll take it. But if we fail to reject, then we will still take the action with some small probability to ensure that we are continuing to explore. And then, in terms of the proposal, so the idea here is that you want to propose actions that we think are pretty good, but are also safe. So, the way that we operationalize that is use this idea of expected improvement, which is an idea that exists in the reinforced learning literature already, but maybe not in this form. So, let rho hat T1 denote the. Hat T1 denote the estimated probability that if you propose action A in state S, it will pass the safety test. And you estimate this using only half the data, use D1. And then you estimate the improved utility or the expected utility of that action in that state, again, using only half the data. And you define the expected improvement, say J hat 1 T S A, to be the probability it's going to pass the safety test, which is conducted on the other half of the data. Safety test, which is conducted on the other half of the data, and the expected improvement utility. So, the product of these two things will be big if you either have a high probability of passing the test or it gives a big expected improvement and it'll be biggest when both are big, right? So, you want to propose actions that give you a lot of utility, but are also likely to be found safe by your test. And then, the way that we actually propose an action is we maximize the bootstrap analog of this quantity here, this expected improvement. Improvement. So you take a proposal action A hat TP in state S to be the argmax of the bootstrap analog of the inspected improvement. And the reason you do this bootstrapping is to, again, add some additional exploration and to guarantee the algorithm learns. And so this is like a poor man's version of Thompson sampling, right? We're using the posterior, we're using the bootstrap sampling distribution as the surrogate for posterior. So that's the whole idea. And it And it's sort of, you know, a bunch of steps. So it's worth checking that you can prove something about it. And so, given the length of the talk, I'll just give kind of a high-level summary of what you can show. So the kind of conditions you need, you need moment conditions to ensure all those expectations exist and you can estimate them. We have a margin condition, which just basically controls the distribution of the gap in risk. This is similar for those who are familiar with these kinds of margin conditions and classification. kind of margin conditions and classification it has a similar flavor we have an eigenvalue condition which is common in adaptively collected data and then we also require the proposal step induces sufficient exploration so the bootstrap version of Thompson sampling that we showed will do that epsilon greedy will do that information directed sampling will do that but you need some additional exploration there and then the result is that given any alpha and zero one and any tau greater than zero if pi hat is the spot algorithm then Is the spot algorithm then under these regularity conditions above? Pi hat is in fact alpha tau safe and it's consistent. So the utility converges to the utility of the optimal regime and it's safe at every state. I think I'm running a bit low on time. So let me just skip ahead and just show one quick example. So the idea here is we're going to look at average utility and safety. And safety of this algorithm spot. And then another sort of natural idea that this is published in maybe 2019. The idea is you test all the actions jointly and control the family-wise error rate. So you get a group of actions you know are safe, and then you apply some learning algorithm to that. So for example, Hopson sampling or UCB or whatever it is that you want to use. That's guaranteed to be safe, but it's not very efficient because of the multiple testing. And then we'll also run an unsafe version of Thompson sampling. Unsafe version of Thompson sampling that just doesn't account for safety at all. And this is an example where you have safe, where you have dosing. And the higher the dose, the higher the risk of an adverse event. So that's a pretty common setup. And of course, the higher the dose, the more effective the treatment is. So we're going to set our tolerance to be 0.1. The baseline is give zero dose, so it's always safe. We're going to set alpha to be 0.1. Look at 50 patients, and the results are based on 3,000 Monte Carlo studies. Are based on 3,000 Monte Carlo studies. So, what I'm showing you here are the results of these three algorithms. Blue is unsafe Thompson sampling. So, here, if you look at the mean effectiveness, it's perfect. But if you look at the safety, it killed all the patients. So, the idea is, you know, if you don't account for rewards in this safety in this example, it's not, it doesn't do very well. And then, if we look at the two sort of reasonable candidates, the family-wise error rate pre-test Thompson sampling versus the Pre-test Thompson sampling versus the proposed spot. You can see that spot learns faster and attains higher reward by the end of the study, while both are above the safety threshold shown by this dotted line. One interesting thing here is that if you look at the agreement with the baseline policy, SPOT actually agrees more often than the family-wise error rate. And the reason is that if you look at the trajectory of the algorithm, it proposes things too aggressively initially and they all get rejected. And it sort of Get rejected. And it sort of slowly figures out what the safe dose is. And that suggests that you should tune this expected value, this expected improvement, you know, looking maybe upweighting the probability of passing the test in examples like this, but even still it generated higher utility. So just to sort of wrap things up here in the last minute, so SPOT learned quickly and safely. The family-wise error rate control alternative was safe, but it was. Control alternative was safe, but it was conservative. We saw that SPOT agreed with the baseline policy more frequently because it was proposing sort of too aggressive of treatments, which were then rejected. Spot also performs well across a range of settings that we didn't have time to look at, including plain, that's plain multi-armed bandits, contextual bandits, continuous treatments, Markov decision processes, and others. But I think this is just sort of one in a very large class of algorithms. And so there's Very large class of algorithms. And so, there's, in terms of open work, there's a lot of, there's a lot of interesting problems that I think still need addressing. One is optimality or efficiency. So, consistency is a weak requirement. So, we showed it was consistent, but we didn't show it was optimal, right? Most powerful, for example, or that it learned fastest in this class of tau optimal algorithms. We need to extend to non-parametric risk utility models. Right now, we can do smooth parametric, but not necessarily non-parametric. Looking at different types of risk. Looking at different types of risk. So, not just one type of risk, but if you have an entire risk profile and so on. So, I think there's a lot of open problems here that have a lot of practical implications. And so, I'd like the chance to discuss them with you if you're interested. So, thanks. Talk a question, Eric. Do you hear me? Yes. Yep. So, early on, you mentioned that this is happening online and it's kind of a sequential nature. And you also mentioned that the states are IID, but there was some kind of assumption on that. I was a little bit confused about, I guess, how to apply the algorithm online and why we would think of the state as the idea of this kind of like a sequential time series that might be like a correlation. Time series that might be correlation or something like that. Yeah, so the kind of two questions in there. So I think the first is: you know, is it reasonable to model mHealth data as a contextual bandit as opposed to, say, an MDP or a POMDP or something, right? Because it is time series data. So a lot of times these contextual bandits tend to be a nice middle ground that doesn't, because the signals are often so weak that going to a full-blown MDP, the price being paid for the addition. The price being paid for the additional modeling complexity doesn't often pay dividends in terms of the actual obtained reward because we know it's not really IID data, right? So, the idea is you do enough feature construction, you might have bases and times or random effects and so on to try to capture some amount of correlation in the modeling and still treat it as though it's this sort of IID data, or at least that the action isn't affecting the next state. But you can do all the same ideas with an MDP, but least square is because. MDP, but least squares become solving a Bellman equation and so on. But the ideas still apply. So I think this is maybe just the clean. One, this is actually how we implement it, just because of this trade-off between modeling complexity and performance. But the theory goes through for an MDP. So you can do that without much modification. And then in terms of the application, I think there's one of the questions was, I think, is, you know, how do you actually apply this? You have end patients and use data splitting. So you actually, yeah, you split the data for each patient. Actually, yeah, you split the data for each patient. So you kind of repeat this procedure with every patient in the order in which they are sort of needing treatment. So there's some kind of practical issues about the timing of when these things are updated and how they're run on the server versus on the phone and all this other kind of stuff that are interesting that I kind of just swept under the rug. Hope that answers your question. Yeah, thanks. I had just a quick question. I had just a quick question. Omeli, you could also somehow model the whole safety thing through the reward because you could kind of tie the reward to be kind of in a wall around the reward you would receive for the safe policy. If a negative penalization, or a very high factor, as soon as you're outside of the range around the reward, you're going to proceed to save. I think I was having a little bit of trouble hearing you, so I'm going to repeat the question and hope and you'll just make sure I got it. I hope to smell it. So I think the question was, why not fold the safety constraint into the reward? Yes. Okay. And then, and or just do some kind of constrained learning that forces the policy to stay near the baseline policy. I think those are the two questions, the two parts of the question. There's two parts to the question. So, the first part is: it's really not clear how we would build the correct sort of composite outcome. So, there's lots of settings where that is reasonable. You know, Peter Thal has this work in cancer where he has clinicians label parts of the sort of toxicity tumor reduction space, and they build a composite outcome that includes both efficacy and side effects directly. And he just optimizes that thing, and that's the end of it. Thing and that's the end of it. Here, in some of these examples, it's really not obvious to us what that composite would look like. And it really is an example where they run contrary to each other in a way that is not easily built into a single composite. Yeah. Can you go? Yeah, but we should. But excellent. That was very helpful. Thanks. Yeah, we're stop and then I guess we've.