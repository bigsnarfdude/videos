So, hi everyone. Before we get started with the lecture, I'd just like to give you a quick overview of what's happening in the next few weeks. So, concerning OOPS, next week is going to be free. There's an event organized by the One World Probability Seminar next week, if you're interested. Next week, if you're interested, in two weeks, there is the Clay School on integrable probability. And oops, we'll start back again at the beginning of August. So from on August 3, 4 and 6, we'll have lectures by Nina Holden on Schramm-Leubner evolution and imaginary geometry. The week after, on August 10, 11, and 13, we'll have lectures by Tom Hutchcroft. By Tom Hutchcroft on uniform spelling trees in high dimensions. Other than that, I just remind you how things work here. So everything is going to be recorded and live stream on YouTube. So we ask you to ask your questions in the chat and we can relate them to the speaker if necessary. We might have a break at midtime if you have questions and then towards the end of the lecture we'll Lecture will switch off all the recordings so you can ask questions directly with your mic. And all this said now, so I'm happy to reintroduce Shiershendu Gengulif, who's going to give his third and final lecture, his mini course on large deviations for random networks. Thanks, Alex. Okay, so let me start by sharing my screen. Sharing my screen. Okay, um, good. All right, so All right, so good. So, yeah, so today's agenda would be to sort of complete the discussion from yesterday. So how to prove the validity of the variational. Of the variational problem for LDP, I'll sort of talk about methods and then also sort of see how to study large deviation. Large deviation property is for spectral statistics. Okay, so just to recall, so we saw phi H N P delta, which was the solution to this variational problem. Which was the solution to this variational problem, optimal entropy cost among inhomogeneous random graphs. We saw this as symptotics, so it had to depend on whether H was regular or irregular. regular or irregular. So we saw that phi h n p delta divided by n square p to the bigger delta log one by p where I recall that delta was the max degree of h so we saw that the limit of this was one of two things depending on the limits of the limit Was one of two things, depending on whether H is regular or not. Theta and mean theta and one half delta to the two over size of H. Now, and I mentioned that theta was the solution. I think I'll call it I. But this was the independence polynomial. Polynomial of H restricted to max degree vortices. So somehow what and also what I mentioned was the two cases, like so in the irregular case, there is only one candidate, but for when H is regular, you have this. For when H is regular, you have this sort of one of the two would sort of work out to be the optimal one, and they came from two constructions. If you think of this as the justice matrix of the graph, one was constructing a small set of vertices of size depending on delta, of course. Depending on delta, of course, times np sorry, and in the triangle case it is, but otherwise this. And the other possibility was you take some small set of vertices and you connect them to everything else. So it's so basically in your graph, you take You take some small set of vertices and you connect them to everything else. Now, and this was, this is the asymptotics for p much, much bigger than n to the minus one over delta, but delta is a max degree. So actually, So actually, below this threshold, this construction actually ceases to work because the size of this red set of vortices is actually up to constants like n times p to the delta. So you see that if p is much smaller than n to the minus one of a delta, this is smaller than one, and of course you need at least one vortex. So this thing sort of starts making sense beyond p being. Beyond P being this small, this and below that for regular graphs, the solution to the variational problem still can be found, and it turns out to be just governed by whatever makes sense, which is the click. And okay, so I don't want to spend too much time, but just broadly, the strategy of proof. The strategy of proof, okay. Why does sparsity help? Like, we could not sort of say anything such precise. So, recall for dense graphs we could not say such structural. Structural things. And the reason is it's an entropy minimization problem, right? So it's a so the it is an entropy minimization problem. Now more precisely, this is you sort of you have a bunch of quantities. You have a bunch of quantities IP of Qij, and then you sum over all ij, and then this qij is the weighted matrix q, which satisfies some conditions. So we know it's a convex function as a relative entropy, but it turns out that when p goes to zero, it admits some nice polynomial approximations. So the key word here is when So, the key word here is when p goes to zero, IP of p plus x admits nice polynomial approximations. In particular, one particular like to give you a flavor of the kind of statements that turn out to be. Give you a flavor of the kind of statements that turn out to be true. So, IP of P plus X, if P is going to zero, you have this lower bound that it's bigger than, I'm like, there is a hidden constant here, but the constant tends to one as p goes to zero. So, you sort of reduce your entropy immunization problem to a reasonably tractable. A reasonably tractable polynomial minimization problem or a polynomial or an optimization problem involving polynomials. And then the strategy, broad strategy in solving to solve, sorry, to find asymptotics of the enterprise. Of the entropy functional, you right. So, we want to find the minimum. So, this is the minimization problem. So, this is the minimum over all the infimum over all weighted graphs. So, to find So, to find upper bound comes from construction. So, if I want to find the best possible weighted graph to get an upper bound on the infimum, you can just provide a construction that works, and the entropy of that construction will be an upper bound. And so, the click and the anti-click and the anti-click or And the anticlick or the hub will sort of do the job. So, whatever I said before, the structures that turn out to be the dominant mechanism, if you so you can barehand construct them and compute what their entropy cost is, and that will give you the upper bound. The lower bound is the challenging part and at a high level. At a high level separate or divide the graph into high degree and low degree vertices. So, I give you some arbitrary graph which lets Which let's say you know has high subgraph density. So you want to show that the entropy cost of this graph is at least five or whatever is coming from these two constructions. So you want to, so you have these two constructions at hand. You want to show that they're the best possible that you can do. So you have to show that any other graph which can be a suitable candidate, which can be a plausible candidate, must have at least as much cost as the better of the click or the anti-click construction. Click or the anti-click construction. And so to do that, you take any graph and sort of this is a sort of a delicate thing to do, but you sort of find the right threshold and you sort of separate out vortices whose degree is higher than some particular threshold that you sort of come up with. So you take the graph and you remove the and you sort of look at the high degree vertices and the low degree vertices. And sort of consider the contribution to the subgroup density from the two parts. And it turns out that if H is connected, if H, the subgroup that you can If H, the subgroup that you care about, is connected, then it's more optimal to just have one part. So So, you take the graph, you a priori can have some contributions. So, you know a priori that, of course, this is all on top of the base graph P. So, you have an education P that you start with. You know that you have a high subgraph density. So, you know that the edge densities cannot be smaller than P, sort of not, does not make sense to actually make edges have smaller density than it typically is. So, you have some base graph, which is like an edge graph, and on top of that, you have some additional reinforcements. Reinforcements. The additional reinforcement you separate into two parts: the high-degree part and the low-degree part. Whatever additional contribution to the subgraph density you desire, and you know that this graph actually admits, has to come from this reinforcement. And the reinforcement, so some of it comes from the high-degree part, and some of it comes from the low-degree part. It turns out that if H is connected, it actually is not optimal to actually have both of them coexisting. So, either the high-degree part exists or the So either the high degree part exists or the or the low degree part. Dominance. And this is what corresponds to the hub. And this is what exactly makes this solution: that you have a bunch of high-degree vortices and they account for the additional density. Similarly, the low density part will be this. So you have some small set of Some small set of part of the graph, and you put in every possible edge there. So, you plant a small click. So, because the size of the graph that you plant is small, the degree is not going to be high, but they're so compactly arranged that you will have a huge boost in the density of subgraphs that you want. So, this is corresponding to the sort of the low degree, and this is the high degree. This is at a very high level how the proof works. How the proof works. Like, okay, of course, there's a lot of delicate steps, including what exactly do I mean by high degree, but I'll not go into that. But the connectedness is important, and if H is actually not connected, if H is disconnected, then one can have mixtures. Where you have the coexistence of both the click and the anti-click with suitable sizes so that the eventual thing is the most optimal that you can do. Okay. Any questions? Okay, so this is why sparsity helps, and most of we don't have such structural theorems. In the dense case, where the only proof technique is still again some polynomial approximation by Helder, but that's sort of at a sort of crude level that you cannot sort of say anything meaningful other than maybe in the case of regular graphs, where exactly the constant function is optimal solution as we sort of talked about yesterday. So, of course, this indicates, and this is actually also known, but this does not immediately imply that this indicates. Indicates the following geometric fact is true conditioned on high density. So, if you condition on your graph G having Your graph G having a high density of edges, it is likely that G has a click or an anti-click close to the sizes mentioned. Now, okay, so conditional properties are delicate, and all of this is happening at an exponential scale. So, you really need to have a very good understanding of what the conditional measure is to be able to make a statement like that. And so, this sort of entropic approach does not quite yield this strong statement, but the work that I mentioned yesterday. But the verm that I mentioned yesterday by Harl, Mossad, and Samotic, who take a more cometary approach to proving latitude, which I'll also lead to soon, they indeed prove a statement of the following kind. They prove a similar statement for For um and then uh following their work, uh, so these people um sort of had sharp results for flicks, uh, but then following um their work, uh, like I mentioned yesterday, Basak and Basu. how can also settle the problem for all regular age up to the optimal threshold and and it turns out that for irregular graphs it's actually significantly delicate already one one just to sort of give you a flavor of the kind of things that can happen Flavor of the kind of things that can happen for very sparse P. So just look at four cycles. So for sparse P, homomorphisms and isomorphisms behave differently. So, for example, if you take a four cycle and you look at copies of the four cycle inside your graph, so roughly you expect it to be of order n to the four. So, you choose four guys, n to the four, and each of them opposite probability p. So it's Up first is probability p, so it's p to the 4. So these are the rough order of number of copies of the 4 cycle in a graph. But if you look at homomorphisms instead, you could actually just get by just having a structure like this because then you can actually identify this vortex and this vortex. Vortex and this vertex. So if these two vortices map to this vertex, then you get a homomorphism of this graph into the bigger graph. And the number of such structures is n cube three vortices p squared. Now you see that when p That when p is super small, n cube p square is much much bigger than n to the 4 p to the 4. So when so this basically means p square is much much smaller than 1 by n or the same as saying p is much smaller than 1 by root n. So you see that as So you see that as P becomes sparser, you have to be really careful because subgraph counts, homomorphisms, these are all different notions and they sort of have different asymptotics. And so this work of Basak and Basso were actually looking at the first kind, where you really look at the number of isomorphisms of the regular graph into your big graph. But just to sort of give you a flavor of how things can sort of go wrong when P is super small, and it actually turns out that the even sort of That the even sort of leading order behavior for the property is not very well known for irregular graphs for a very small speed. Okay, so any questions? Okay, so this was all about the original problem and how to solve it. But like I said at the beginning, how does one prove its validity? Like how does one prove that it is indeed what Like, how does one prove that it is indeed what governs the large deviation? And so, recall that in the dense case, we approximated any graph by a block graph with crucial. With crucially, with number of blocks not growing with n. I'm like, at the end, the proof that we actually gave actually allowed a various sort of slow growth, like a polylog or something. But just generally, the view was: I'm like, if there are too many blocks, then the approach would fail. Used coin tossing probabilities to bound the probability of looking like a given block graph followed by union bond. Followed by union bond. And this is the crucial part, which will fail unless the number of blocks or the number of things that you approximate to your graph by is not too huge. And the bounce that we got, it did indeed blow up very quickly, preventing this sort of naive approach to warp when P goes to zero. approach to work when p goes to zero at a polynomial rate and so i'm like but the but the general uh principles should stay the same so then the strategy should be maybe to come up with more efficient covers of the space So, recall, I mean, like the original approach of approximating any graph by a block graph actually was an overkill because it's a really, well, it was actually attempting to prove the full large deviation of the edge-free measure on the space of all graphs, whereas the actual thing that we care about is only the large deviation properties of a particular random variable, in this particular case, subgraph density. Subgraph density. And so one might be able to sort of come up with more efficient notions of covers just tailored for that particular application. So I'll actually give a quick flavor of a particular argument of Nick Cook and Amit Dembo, which uses spectral properties of the graph. But before that, I'll just sort of want to still sort of talk a bit about how to maybe generally approach the problem. So, okay, so just sort of a possible general approach, and this is indeed made to work in sort of a really breakthrough work of Chatterjee and Dembo 2014, maybe, and then that. And then that was refined, and various other notions are sort of put forward by Eldan and Ajiri. Maybe around, don't quite remember the exact time, but maybe around 2018. Okay, so what is the approach? The approach is to go back to the Gibbs measure framework. So, recall that one particular which is the same as basically saying compute exponential moments. Right, so if we can compute exponential moments sharply, then you can use Markov, or there are other ways to actually. Markup, or there are other ways to actually use sort of precise understanding of exponential moments to get large deviation bounds. So, in particular, recall what I mean by this. So, you have some random variable z. Let's not call that z. Let's maybe call that f. Let's say from the hypercube to the reals. So, I'm working with the hypercube of size n, whereas, actually, in the application, if you want to work on graphs, In the application, if you want to work on graphs, you should think of graphs as elements of the hypercube of size n just two. But I'll not do that. So, for the moment, we sort of work in an abstract setting where I just have a general function from the hypercube of size dimension n to the reals. And let's say I want to understand, so of course this induces a measure on the hypercube, where the measure of a particular configuration is proportional to e power fx. And so, of course, And so of course to make it a probability measure, you divide it by the partition function. So z is nothing but summation e power fx. Now this is exactly the setting that we saw yesterday in the exponential random, when we were talking about exponential random graphs, f was again some subgraph counting type function. But there, because we're talking about the dense dense setting. Dense setting, there was already an a priori proven Large division theory, and then we use the Large Devils theory to actually compute Z. Whereas here, the point is to go the other way. The point is to actually come up with a technique to compute Z to then understand large deviations. Okay. So, maybe actually, sort of, just to make sure that this is like an expectation, let me also. Let me also normalize it by 2 to the n. So, this is basically an expectation of e power fx under the uniform measure on the hypercube. Let me define z to be like that. So, of course, then this is not quite an equality, but then I'll just sort of think this would be easier. So, I'll just write proportional. Okay, so goal. So goal is to compute Z. Or approximate Z. Now, as I've already told, once you have exponential moments, you can actually use that to understand large deviations. But in principle, in particular, this is literally the probability of a particular set. So if Of a particular set. So if, if, if, I mean, this is a drastic example, but this is not what we will eventually use. But so, probability of A, let's say under the uniform measure, 1 over 2 to the n, is z if f is 0 on a and minus infinity of a. Right? So I mean like, of course, f has no smoothness and whatnot, but if Course, f has no smoothness and whatnot, but if f was actually zero on a set A of the hyper on a subset of the hypercube and minus infinity off of it, then e power f would of course be one on the set A and zero off of it. And so then this is nothing but the property of A under the uniform measure. And F is not smooth, but maybe you can sort of mollify F and then sort of try to play with the strategy once you have a basic framework to compute such things. Basic framework to compute such things. Okay, so Gibbs variational principle. So there is actually a general recipe to compute this. So log of Z is actually a super optimization problem. So you take soup over all mu, I will say what mu is. say what nu is expectation of f under new minus the relative entropy of nu with respect to mu mu is let's say the uniform measure mu is the uniform measure basically which is putting which is putting mass one over two to the n on every vortex of the hypercube and so what is new And so, what is new? New is all measure, so the supreme new, any measure on the hypercube. So, log of z is you take any measure on the hypercube, call it nu, look at the expectation of f under nu, and then look at the cost of. And then look at the cost of the entropy cost of ν with respect to μ. Now, this is actually not very hard to show. And so this is an exercise, exercise would be prove it. And the optimization measure, so and so hint would be the optimum. Optimal solution is actually new equal to nu proportional to e power f. So I'm like, so it turns out that this supremum is attended when nu is actually the measure pi, but for every other measure, this is actually a lower bound. Bond. Okay. And it just follows from sort of convexity of relative entropy. And it's basically, this is not unrelated to something that I've already said in one of the earlier lectures that the log moment generating function and entropy are sort of duals of each other. Okay, but this is a sort of nasty optimization problem because. Nasty optimization problem because you're looking at all possible measures on the hypercube. And so the mean field approximation is when restrict mu to product measures, which basically means every Every coordinate is independent. So, this is a supremum over all possible measures. A lower bond is taking the supremum over all possible product measures. And so, another exercise, again, quite easy to prove. Uh, prove that I mean, like, this is actually obvious from what I just said. So, if f is linear, then the optimal solution is a product measure. And that's actually obvious from what I said here. So, I said that the optimal solution for any f. Solution for any f is has density proportional to e power f. So if f is summation ei x i, then of course exponential of a linear term factorizes and all the coordinates behave independently. Okay, so the mean field approximation is tight when f is affine. Now the whole point Now, the whole point of this nonlinear logic deviation theory is to sort of understand what classes of f actually admit a reasonable nice mean field approximation. So, of course, it's not going to be tight if f is non-linear, but maybe if f is approximately linear in some suitable sense, then maybe if you just approximate by product measures, you are not doing too bad a job, and maybe you are doing sort of a reasonable approximation. And so under what conditions is mean field is a mean field strategy reasonable? And so this was so in Chadijin Dempo. So, in Chattajin Temple, this was the view that was put forward, and then they came up with some conditions. So, two conditions: one is low gradient complexity. Basically, this means if you look at the gradient of f at x for all x in the hypercube. Cube. So, think of actually f was initially supported on the hypercube, but let's pretend that f is actually a smooth enough function so that it can be actually extended on the full continuous hypercube. And so, otherwise, you can talk about discrete gradients, but let's for the moment just think of f as a smooth function. So, then if you look at the gradient of f at every vertex of the hypercube, so you have a bunch of two to that many vectors. If f was actually linear, then this would be just one vector. So, low-grained. So, low-grain complexity condition says basically that this set has a small metric entropy or a small covering number. So, this is the notion of approximate linearity. So, when the set of gradients is not too big, and then there was an additional second-order condition, second-order smoothness, which was basically looking at Which was basically looking at sort of double derivatives of fij and saying that they are not too big. So the L infinity norm of second derivatives is also uniformly small. So which also basically sort of points to the function being close to some linear function at least piecewise. So these were the two notions under which Chatterjee and Denver proved that if the function f satisfied those two conditions, of course, there's a huge number of quantifiers involved, which I'm not getting into. Huge number of quantifiers involved, which I'm not getting into, then this can be actually approximated well by a product measure. And this was then used for functions which were smoothenings of this to prove the large deviation principle, saying that this variational problem involving entropy is indeed what actually governs the large deviation. So, when basically μ is a product measure, this the relative entropy of μ with respect to μ. Of mu with respect to mu. So is of course a product measure because it's uniform, and if it's also product measure, then by tensorization of relative entropy, you essentially get the optimization problem that we had before. Minimum over all possible sums of entropy. So the sums of entropies are sums over all the indices. Now, because of tensorization, this is what you get actually. So it's not super important, but the key is that that problem is essentially the same as. Problem is essentially the same as saying that this problem has a nice mean field approximation. So, this is what Chadijan Dembo did, followed by Earl Dan, who actually improved the dependence and also sort of there was no second order condition, no second order condition. So, got rid of that. And the first one, instead of having a small covering number, Instead of having a small covering number, he replaced it by small mean Gaussian width. So, small covering number means that you can have a small set of vectors which forms an epsilon net on this set. Small Gaussian width of a set. So, Gaussian width of a set K is the maximum projection that it has on a random direction. Projection that it has on a random direction. So it's soap over x and k. So where g is a standard Gaussian vector. And it's actually well known that these two notions are related. One is actually usually a refinement of the other. You can actually, if you have a bound on the entropy, If you have a bound of the entropy number, then you can use chaining to bound the Gaussian width. But it's generally an easier thing to compute the Gaussian width. So, this was what Eldan did. And then Panier Jerry subsequently used mostly arguments of convex analysis to You get further improvements. And in particular, for the case of cycles, brought down the sparsity threshold to this one over root n that I was mentioning. So I'll not go into that, but these were basically the three sort of works that tried to make this approach precise. Make this approach precise of computing the partition function and then using that to get large deviations. And the computation is possible if the function is approximately linear in some suitable sense. And the approximate notion is that the set of gradients is not too big. But you could also sort of take a slightly more direct approach, which is closer to what we did for the dense graph. So just approximately. So here, Approximately so here you're trying to compute the partition function and then you want to sort of work with a function which looks like an indicator on a set. But you could try to do the more direct approach of approximating graphs by block graphs, but in a slightly more efficient way. And this is what was done by Nick Cook and Dembo took a more direct approach. Um to cover graphs by block graphs so using spectral properties So I'll quickly try to at least give you a flavor of what their argument is. So the starting point is this observation. So let's sort of define some notation. So let's say P P is the product Bonali measure, Bonally P measure on the hypercube, which is our base measure because we've been working with edition P graphs. And so this is the key starting point for any closed Convex set K Rn actually actually we can of course our measure is only supported on the unit hyper cube but okay so you can work with this then you get a an upper An upper bound where I p of k is the best possible inf y in k i p of y. So essentially this is going back to the original sort of additional things that we had. So it turns out that for any closed convex set, by convex duality you can actually get a bound like that without any sort of protection term. Like that, without any sort of correction term. So, the probability of any convex set under this product measure is you take the best possible vector y in k, look at its relative entropy with respect to the product measure, and that's basically an upper bound. And also, we can do some tilting, I guess. Okay, maybe not clear because the infimum might be a continuous point, and you're yeah, anyhow, so this is an upper bone, and of course, there are already strategies to get. Of course, there are already strategies to get lower bonds by tilting, but for the upper bound, I mean, this is what will sort of suffice for our purposes. Okay, and so now, so recall, always proving the upper bound on the probability is the challenge because that's where the union bound is. The lower bound is typically tilting or some very barehands construction. So, the strategy is to cover The space up to maybe an exceptional set by closed convex sets, like above, with the additional property. That the random variable of interest, let's say x, which in our case is a subgraph count, does not oscillate too much on k. Or, I'm like, just to be completely sort of certainty more comfortable, so you have a family, let's say, of Right, so in our original application of this regularity lemma, we took the space of all graphs and then we covered them by block graphs or balls around block graphs and said that because of this cut norm and the subgraph count being a continuous function, in the small balls, the subgraphs don't ask, the subgraph counts do not change too much. It's exactly the same principle, but here you are being more flexible with the cover that you come up with. Flexible with the cover that you come up with. It turns out that because of this apiary upper bound, it sort of suffices to come up with any closed convex sets such that it's not too much, so that it actually allows union bound. And then again, you want your random variable to not oscillate too much on each of the individual elements of this cover. And so, and using spectral Properties of adjacency matrices cover in the operator norm suffices for triangles at least so like if I have I have a bunch of matrices all it is is a matrices if I connect Matrices, all adjacents of matrices, if I can actually come up with a net, which actually is a net in the operator norm, then it turns out that that will also have the property that on each of these norm balls, the subcraft count will not change too much, or the triangle count will not change too much. And this is obtained essentially by a spectral projection argument. By a spectral projection argument. By which I mean you take your matrix and you project under the largest few eigenvalues. That gives you a low rank approximation, and then you sort of construct a net on that space. So I'm sort of, I don't have too much time, so I want to go into some spectral properties, which was the original plan, maybe. And then one more comment, though. One more comment though, for arithmetic progressions, let's say for k equal to length 3, which I was talking about earlier, one can construct covers using connections to Fourier analysis. So it turns out that there is a very beautiful connection between APs of length 3. Connection between APs of length three and Fourier analysis on ZModen. And using that, actually, you can construct efficient covers and for higher length of arithmetic progressions, you actually need to do more. The knife 4E analysis, actually, the standard 4E analysis actually does not work anymore, and you have to do some random sampling to actually get reasonable covers. But so, this work of Hurrell. This work of Harrel Samotic, Mossett and Samotic, and then followed by also Basak and Basu, they actually take a more commitment approach. So, cover graphs according to presence of comaterial structures which they call seeds or cores. Or core. So these are basically structures that actually facilitate the large deviation event. So if you, I mean, this would be like the hub or the click. So if you have some, these are basically structures in a graph, such that if you condition on such presence of such a structure, that actually boosts the expected count of a particular subgraph that you care about. And so, so, again, so all of these approaches are basically at a very high level. Similar, you want to construct an efficient cover. It could be either spectral, it could be either comaterial, or Commaterial, or if you sort of go back to this partition function approach, then you actually covered the space of gradients. Anyhow, so I'll this is all I wanted to say about this. So quickly in the remaining time, I want to sort of talk about a different kind of observable rather than sub-curved densities, which are spectral statistics. So in particular, so let's say lambda one bigger than lambda two. Lambda one bigger than lambda two are the eigenvalues of AG where this is a juice matrix of G so it's known that lambda one typically when p is not too small Lambda one is typically like np and lambda two is typically like square root np. So you can ask about what is the property let's say And um so cook and dembo again um So Cook and Dembo again using covering by affine sets showed that the mean field improved validity of mean field approach and then we And then with Mashrapaticharya, using our previous understanding of cycles, so recall that lambda 1 bigger than 1 plus delta n to the p actually implies. So, because cycle counts are connected to traces of matrices, so if you look at a cycle of size S, then if the largest second value is big, that implies that the number of cycles of size S is also big, and we have already And we have already understood the variational problem solution for subgraphomorphisms, which was either a click or an anti-click. And actually, it turns out that these two events are roughly the same if s goes to infinity. So, certainly, the largest large eigenvalue implies large cycle count, but then these are approximately equivalent if s goes to infinity. So, using previous. Previous work, which is with Phattacharya and Lubeski and Sao, which I mentioned yesterday, about the solution to the variational problem, you actually get the following answer. So, log of the probability of this event, let's call this event A, maybe minus of that. Maybe minus of that divided by n square p square log 1 by p is again a minimum of two things so this again corresponds to a click construct a hop construction and this is a click and so this the click is And so, this the click is slight easier. For the hub construction, recall that the corresponding thing for the cycle, there was a click and hop, and the size of the hub was given by the independence polynomial of the cycle. And it turns out that for cycles of size S, the independence polynomial actually satisfies a very nice recursion, which is this. So, the independence polynomial of a cycle of size S satisfies a recursion. Which is close to Chebyshev type recursions, and you can actually use that to really exactly compute what this polynomial is, and compute what theta would be. Theta was the root of a particular polynomial equation, and then you want to take the limit as s goes to infinity. So, this is what actually goes into the proof. So, actually, in the Cook-Dembo result, they proved a general result about the largest eigenvalue of an eder. The largest eigenvalue of an eta-trinic graph, but transited by any deterministic matrix. So, lambda one of any deterministic matrix where B is deterministic and A is the adjacency matrix was actually treated. And so, by taking B equal to the constant matrix P, one can actually sharp large deviation tails for the second eigenvalue. Because typically the first eigenvalue of an identity graph, as I said before, is np and the eigenvector, the corresponding eigenvector is basically all ones. So essentially by subtracting of b you are Of B, you are essentially so A minus B, the largest eigenvalue of A minus B is typically the second largest eigenvalue of A. And so you can use that to also have a version of this result for lambda 2, which is also what we prove with Madacharya. And I will not start write out the full statement, but here the click is the only thing that click anti-clicks don't make sense. So hubs. So, hubs don't show up. But there are some caveats, and I don't want to get into technicality. So, we cannot actually pull a full large deviation because we only work in the regime where the largest eigenvalue is around p. And so that's actually needed for this proof. So, I'm already out of time, but just in the last minute, I want to say that when p is very small. Very small. Actually, when so to be precise, when p is less than maybe log n, sorry, np is less than root log n by log log n. So which basically means p is like some polylog by n. Then the Then the edge of the spectrum, meaning lambda 1, lambda 2, are governed by high-degree vortices leading to Localized eigenvectors. Meaning, if you have a matrix, typically for reasonable values of p, the largest eigenvalue is np, and this is because the total number of edges is n square p. So lambda 1 typically n p because Because total number of edges roughly n square p or n inches to p. However, when p is very small, when p is very small, let's say when p is c over n, when the average degree is constant, then Then Np is a constant. However, you actually have some very high degree vortices in the graph just because of fluctuation. And so these have values roughly log n by log log n. So there are high degree vortices of this size. And so of course, these are the ones that actually govern the largest eigenvalues. Because the largest eigenvalue of a particular Largest eigenvalue of a particular of just this structure, this graph of degree d, let's say, if you have a star graph of degree d, then lambda one is actually like root d. So in this case, actually, these are the ones that actually govern the spectral edge. And so, with recently, with the Bashar Bhattacharya, Shaw Mattichary. Show material. We proved a large deviation principle for the edge of the spectrum both upper and lower tails. Meaning, what is the probability that lambda 1, lambda 2, lambda k is bigger than their expected values by some multiplicative factor or smaller by a multiplicative factor by reducing to a large degree principle for the extremal degrees. So, basically, what we say is roughly. Basically, what we say is roughly at a very high level, if the largest eigenvalues are large, and the largest view degrees has to be large, and vice versa. Although there are many quantifiers, and I want to, I will not say the precise statement. The actual statement appears in the lecture notes that I've posted. But yeah, so I think I'm out of time, and so I'll stop here and I'm happy to answer questions. Okay, so I think everybody has. Everybody has the opportunity to unmute themselves. So I think we should give a big round of applause to thank Shoshandus for his three lectures. And uh