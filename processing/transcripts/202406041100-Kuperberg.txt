I will sit here. I'll interrupt you if there are questions in the room. But yeah, our next speaker is Greg Cooperberg. He'll be joining us from presumably California to talk about the Solve-Kateav algorithm. All right. Okay. Thanks for the invitation. Merci pour l'a invitation. Since it's Canada, I might as well be bilingual for a second. Pretty good French, but better than nothing. But better than nothing. So, so I'm gonna try to adapt this talk. Let me see. So, where, oh, okay, here we go. Okay, so I have to go control page down. Down. Oh, damn. I mean, it was working before. I don't know what the problem is now. Oh, okay. I can just scroll through the slides. Okay. So go into presenter mode or go into what mode? Like presenter mode, like movie view. Okay, so the say that I have stuff covering it up. See, I have stuff covering it up. I don't, I wanted to have the chat window open, so the presenter mode is not that great. So this is, this is, I was testing this before, and some, somehow, some, oh, okay, this is good. I was pressing the wrong button. Okay. Okay. So, so here we go. Okay, so this is, I think, a good talk presentation. Okay, so this is where the story of what I'm doing begins. Okay. Okay. The Seleve-Katayev theorem. And let me just try to carefully state the result. Suppose you have what is called a universal gate set, and I'm going to work in just a single qubit D. We'll call the gate set A. And in math speak, and to be honest, I'm in pure math. That's really what I am at heart. It's said to be a finite set that densely generates G. And here, And here, in the original theorem, which is the most important case, it's closed under inverses. Then, given a target little g and an epsilon greater than zero, the theorem says we can approximate g to within epsilon with a word of polylog length, and we can find it in p, in other words, in polylog time, in one over epsilon as well. Now, for this particular audience, I don't need to spend too much time on this point. Spend too much time on this point, but this theorem is, it matters for everything. You need it to make sense of fault tolerance to begin with, because that'll only give you a finite gate set if you're fault tolerant. It drives quantum compiling. So it's a textbook result, but it's not an easy result. It's just sort of there in the textbooks. And I don't know, you know, in a lot of curriculum. You know, in a lot of curricula, you would just state this without diving into it. But anyway, it's certainly very important. And now, since we all know why this theorem matters, let me discuss then more specifically what the existing proofs give you. In the period when this was developed, there was published in 2002 the version of Katayev Shan and Vyalyi. Ketayev, Shian, and Violi, which says specifically that the word length, which is of course what you get to care about, what you care about more than the time to compute it, is polylog with epsilon 3 plus delta. In other words, any exponent greater than 3. And there, for the original question, which is what I really want to talk about, for a general gate set, that is where things stood for a long time. Long time. And they were only working in SUD, but in fact, you get this result in other Lie groups as well. They have to be semi-simple in order to get to for these proof techniques to work. And there can be a macroscopic term if it's a non-compact Lie group and so on. But we're most concerned about SUD, of course. And then there was this fantastic result in 2019, which really drew my attention bigly in a big way. Bigly, in a big way, which is that if you have a gate set which is not closed in inverses, then you still get a Solovey-Katayev theorem with the same rules, except with the polylog exponent depending on the dimension D. It's interesting finding a uniform polylog exponent in the non-inverse closed case is still an open problem. So they solved an open problem that was posed in the Dawson-Nielson paper. Posed in the Dawson-Nielson paper and later posed by me as well. And this, since their exponents were not as good and they had a complicated technique involving specific group words, it got me to thinking about optimization of exponents. Now, before I get to my result, I will now talk about the status quo. First of all, is the elementary fact, just the volume lower bound. The volume lower bound that you cannot do better than a polylog exponent of one. You just don't have enough words to cover to make what's called an epsilon net, otherwise. Second, there's the big result, which many people in this audience may understand better than me. I've struggled lately to think who to credit to exactly because there was a sequence of dramatic sequence of papers that is. Sequence of papers that establish this, but I decided at the moment for now to call it the DMW results, Dalhousie, Microsoft, and Waterloo. What? Okay. Laughter. For special gate sets, for gate sets with a number of special properties, and Clifford plus T happens to be one of them, then in fact, you can find words of length optimal up to a constant factor. Length optimal up to a constant factor, in any case, with the optimal polylog exponent, optimal that way. Okay, and the algorithm for the DMW algorithm is very different and is number theoretic, whereas the general Seleve-Katayev is a multi-scale algorithm. And now, also for comparison, if you're not worried about an algorithm, then there's also this smashing result elsewhere in the mathematical literature that Mathematical literature that with a mild assumption on the gate set A, just simply that it's algebraic, then Baugan and Gamboard in the case of SUD, and this was generalized to other compact simple Lie groups, says that you again get a polylog exponent of one for simply existence of words. And not only do you get existence, you get statistical equidistribution at a scale of epsilon. So that's amazing. But that's not what I have to contribute to. I have to contribute to. So, with that intro, let's go back to here. This is what I was really concerned about after looking at Adam and Tudor's paper on the inverse 3 case. 3 plus delta. Can we improve that? Well, I can. And here is my new result. With the same hypotheses, inverse closed general General universal gate set in SUD, if you like. And in this language, I say, you know, semi-simple real Lie group because I want to give it more generality. Don't worry about that too much. You can just say SUD plus a macroscopic term when G is not compact. Then I get a polylog exponent of alpha for any alpha bigger than log two to the base. log 2 to the base the golden ratio. And that's known to be a transcendental number. And it's 1.44042 dot dot dot. Okay so that's it turns out that there were just missing ideas or other ideas that were just sort of there beyond the textbook result. Okay. So just like General Selevikatev is Just like general Selovicotev, as I already mentioned, it's a multi-scale algorithm. Okay. And I do use commutators in a sense, but there are now two new ideas for me to get to this improved exponent. One idea is that I re-engineer the multi-scale iteration to take the exponent from 3 plus delta to just flat out 2. Okay. And for this first bullet point, For this first bullet point, what I want to say about this is that there's an extra element, which is just using conjugation rather than just commutators. That's really where an option for a possibility for improvement comes from. The second new idea is that actually, for those of you who are students of the Seleve-Katayov theorem and algorithm, you have group commutators that are the engine that drives. Commutators that are the engine that drives everything. And as I say on the slide, but it turns out that, and this was known already in the literature, there are other more efficient higher commutators. They just get you shorter, they're just more efficient. And there's a known sequence of them where the asymptotic efficiency is log phi 2. So that's where I get the log phi 2 plus delta. log phi two plus delta. Okay so let me just review a tiny bit of geometry. I'll discuss the SU2 case because that's just easier. It's just easier to discuss the qubit case. So SU2, the Lie group, is a three-sphere and it has this angular metric that you can define by or can make use of the thing. Make use of the thing that I have in this app. You can use matrix trace if you want to, or you can use quaternions, and you can define a distance this way. Whatever the formula is, the distance has an important property. It's bi-invariant. Distances don't change under either left or right multiplication. And in particular, there's this corollary that the distance from the identity doesn't change. Distance from the identity doesn't change under conjugation. That turns out not to be essential in the long run, but it's convenient for describing the qubit algorithm and in other compact cases such as SUD. So there's this extra bit of geometry, which is what happens when you take a commutator. You can just, I don't have any clean picture to prove why this works. The picture just demonstrates what the question really is. Suppose you have G and H. Suppose you have G and H. And by the way, in the cubic case, you have this converse result that distance from the same, if you have two elements, they're conjugate if and only if their distance from the identity is equal. So if g and h are conjugate, if the distance from their identity is lambda and they make an angle of theta with each other, then there's this formula for the distance of their commutator for the identity. Their commutator for the identity. You've got an arc sign, you've got a sine squared inside there. Okay, so the important thing is that to leading order, let me see if I can type something. It's O of lambda squared. Okay, so it roughly squares the distance down. But in any case, the precise formula is also useful. Okay. Okay. Okay, so so here now is the algorithms in two parts. Part one is rough steps, okay? Roughly, roughly exponential steps. I want to make steps that a step SN, which I don't care what direction it points in, but it's distance from the But its distance from the identity should be roughly 2 to the minus n, give or take a constant factor. And I'd like it to have quadratic length. No control over the desired direction. The word length is favorable. And these rough steps you can make in sort of the same way as in the standard textbook proofs of Solovy Kateev. Okay, just with Okay, just with a recursion using commutators. To make the step SN, I take a step roughly halfway back, halfway back plus a little bit more. And then that's my step SM, M is in Mary. And then I conjugate the previous step SM by some U, which is just bounded length. By conjugating by a bounded length word, I can get an angle. I can get an angle between s and its conjugate s u of theta, which is precise to some bounded number of digits. That's it. But if you just pick apart the formula, and this does a better, on this slide does a better job than what I jotted down on the previous slide. Here's the leading term for the length of the new one. And if all we want is a bounded length window, All we want is a bounded length window for the length of the new step. Well, you can get this with just a bounded amount of control on theta, which just means then that the conjugator to rotate around the step can also just have a bounded amount of precision. It can be a bounded length word. Fine. Okay? So now it's certainly believable. I mean, each new thing is about four times the last one and is twice. The last one and is twice as far down the exponential scale. So, with this extra constant number of letters to conjugate, it's believable and it's also correct that you get a quadratic recursion so that you do get this quadratic length. All right. Now, that's just steps that don't even point anywhere in particular. There's no target mentioned. Okay, so now. Now, suppose you do have a target group element G, and you have these quadratic length steps that are at every exponential, all exponential scales, then I want to make a total word whose distance from g is within n digits of precision, two to the minus n, which is then set less to epsilon. And suppose I still want the whole world, whole word, to also just whole word to also just be quadratic. In a typical, not the only style of proof, but in a typical style of proof, the exponent here, and certainly this is what happens in KSV, it increases by one. If you do what you normally expected, which is just a step towards g using every exponential length scale, then because the sum of squares is theta of n cubed, you would Theta of n cubed, you would end up with a total cubic word. To get, to protect the exponent, to have it not increase, these words have to have doubly exponential convergence, or in any case, that would suffice. Okay, so that's what I'm after. In fact, the Dawson-Nielsen proof does have doubly exponential convergence, but in the way that that proof is written, but at high upfront cost, higher than an exponent of two, certainly. So the way that I'm going So, the way that I'm going to do it is to make to rely on conjugation again, but in a more complicated way. Think of the step SN as a golf stroke. And we're playing golf, and we can aim the golf stroke. We can't change the length of the golf stroke, but we can aim it with conjugation by V and W. Okay. And also, I'm And also, I'm willing to go back a bounded number of steps in the exponential scale to make the golf strokes too. So that's why I have an M as in Mary here. Okay. And for the sake of doubly exponential convergence, instead of gaining a constant amount of precision, I would like to multiply the number of digits of precision by five quarters. Okay, that's good enough. I mean, Newton's method is an example of wx. Newton's method is an example of doubly exponential convergence where you double the precision at each step, but it's still doubly exponential if you improve the precision by five quarters. Now, since the golf step, the golf stroke is a fixed distance and the conjugators are adjustable, well, what would you really do if you played a game of golf where you couldn't control how far the ball goes, but you control the direction? Well, you would make an Well, you would make an you could make a zigzag, an open jaw where you swing twice. Okay, so here's an action picture of the plan. Okay, golf with doubly exponential convergence. I have skipped the numbering on the steps, but here is at the first scale that gives us our w here. Here, I guess I can pencil in the numbering. Okay, okay, so that's that's w. Okay, so that's that's d w actually the the numbering doesn't doesn't completely make sense because it's numbered according to scale. Okay, so that I guess that's part of why I guess that's why I skipped it. So anyway, here's the first two steps at one scale, and then here are the next two steps at a later scale, and by that time we were gotten really close, and then at some much later scale, there's another pair of steps. So here, so this is the strategy: precision. Strategy precision zigzag golf. Okay. Now, how exactly are U and V, the A, going to be precise? And here, this is where I complete the first part of my algorithm. They are precise by borrowing from the entire algorithm. By borrowing from the entire algorithm itself. And I've got to be real careful to get this to work because it sounds circular. And it would be circular unless the parameters are done carefully. But here, this is why this is the game. It's not really circular. In order to, if say you wanted to go from. You want it to go from 80 to 100 bits of precision, then the angle only needs 20 digits of precision for the aim for those two steps. If you want to go from n bits to five quarters n bits, then the aim only needs n over four bits. So you only need the entire algorithm, a factor of four back. A factor of four back. And those words, if you have faith that you really will get a quadratic result, are 16 times cheaper. So the idea is that if you go back far enough in this doubly exponential regime, that in this hope-for doubly exponential regime, that actually the aim will be about the same price and word length as the Gallstroke. Word length as this as the gallstrokes themselves. Okay, so here's the rigorous way to do that. I can't spend too much time on this, but if you just add up the constants with the chosen five quarters, and then you make assumptions at the beginning, then you get when the dust settles that it works because I'm not seeing it at the moment. I'm not seeing it at the moment, but it has to do with 12 being less than 12 and a half. But really, the formula doesn't indicate it, but if you replace the one quarter extra in the five quarters by a small enough constant, and then just put in a big enough constant K, I don't have a K, there should have been a C. But if you put in a big enough constant C here, oh, I see, but you replace 12. Oh, this. Replace 12, oh, this 8 by a K, then it just would have had to work because the conjugators get cheaper and cheaper the further back you go. Okay, so that already is an improvement. That's an exponent of 2 rather than 3 plus delta. Now let me go to the other idea where you also get an improvement. Think about the cost to make what in my algorithm. Make what in my algorithm is a golf stroke. And in the classical algorithm, you have instead of precision golf, it's like a shotgun golf algorithm where you have a cloud of commutators. That's the way the Kateyev's version and Dawson-Nielsen work. But in any case, you get each word at twice the length scale at a cost of four times as much length by using commutators. So, in our case, there's this length recursion that looks like this. And if you idealize, if you simplify it to this recursion for a function, then it's satisfied by 4 to the n plus alpha with alpha equals 2. Why does alpha equals 2? Because it's log to the base 2 of 4. 4 times as much length, twice as far down the exponential scale. Now, observe this that happens if you just do this twice, but you just. Do this twice, but you just make a double commutator: commutator of A and B, commutator of B and C, and the commutator of those two. I'm going to change colors on my underlining here. Well, that's a word of length 16 because it's the commutator of two commutators. But here in the middle, there's a B inverse followed by a B, and those cancel. So it doesn't have length 16. It has length 14. Well, that's already better. Better. Then the efficiency of this recursion is log to the base 4 of 14, not log to the base of 16. And that's already less than 2, of course. So you already get below an exponent of 2 with this simple cancellation observation. But that's just the beginning. Can we do better than this? Okay. So what I showed you on the previous slide is an example of a higher commutator where you get log to the base 4 of 14. Get log to the base 4 of 14. But if you plug in that expression, ABA inverse instead of AB and C, well, then actually it's an even better word. It has cancellation degree 5 instead of cancellation degree 4. And then you get log, this is the cancellation thing, you get log to the base 5 of 14 instead of log to the base 4 of 14, and that's 1.63. So that is even better. So these remarks on So, these remarks on abstract cancellation properties of words, or in group theory speak, how far down the word lies in the lower central series of a free group, were explored by these algebraists, Al Casapi and Tom. And as I just said, they considered the matter in terms of what you could call Nelpon's degree or position in the lower central series. So, they found a series of words. So, they found a series of words whose efficiency, whose cancellation degree or nilpotence degree, is a Fibonacci number, and whose length is O of 2 to the n, or theta 2 to the n. So, therefore, as you take longer and longer words, the exponent alpha converges to log to the base phi of 2. And as long as alpha is bigger than 1, there's a zigzag golf machine that protects the whole exponent. That protects the whole exponent. The exponent doesn't increase for the entire word either. And that's how I got to my constant. Now, I am probably running long time. So I'll just lay this out. There's a series of things that you would do for more complicated cases. So there's various technicalities here. Maybe the one that I would like to emphasize. I would like to emphasize is what you would do in SUD rather than SU2. The way that conjugation works in SUD for D bigger than 2, you can't just say that two elements are conjugate if and only if they're the same distance from the identity. Only if is correct. If they're conjugate, it does imply that. But conjugation is more restricted than being able to point the group element in just any direction. Instead, Instead, well, in fact, you can solve it by a more complicated zigzag. Instead of a zigzag of length two, you need just some bounded length zigzag that depends on the dimension D. So there are other technicalities in the other cases, but for this audience particular, I'll gloss over that for the minute. But now in conclusion, Now, in conclusion, I don't think that this exponent was known for general gate sets, for transcendental gate sets, even just for existence. Well, for existence, people conjecture alpha equals one no matter what. And that conjecture is probably true, I think. But in any case, it pushes the envelope in that case. It fails miserably. I'm using conjugation in a lot of places. So you don't get an inverse-free result. So, you don't get an inverse-free result. And it's an interesting open question whether you can even have a uniform exponent at all in those cases. Now, the major competitor to everything I'm talking about is the DMW algorithm. That's an amazing and beautiful algorithm. But the thing that I found is at least theoretically significant and might sometimes be useful. And finally, the simplest version of what I sketched. The simplest version of what I sketched with an exponent of two, as far as I know, it would be as good as the traditional thing in textbooks and expositions, I think. I don't think it's that much more complicated, actually. Maybe not even more complicated. I mean, it's sort of phrased differently, so it's hard to tell. Anyway, that's my presentation. Maybe I should have paused a bit for questions, but maybe that's difficult to resume, but certainly I'll take questions now. I'll take questions now. We did start a bit late, so I think there's time for one or two questions. Peter? Is the log subpipe to thing optimal for the way that you're doing it? I mean, or is that just the sort of the best words that I've known for this? Are known for this? That's a very good question. So, this I took those words from the paper by El Kasapi and Tom, and they weren't working in a lead group at all. They weren't working in SUD, in SU2, or SUD. They were working in a free group, and they have an abstract version of the cancellation property, which I'm calling Nilpoten's degree. Nilpoten's degree. Now they can't prove that their words are optimal, but they conjecture it. Actually, El-Kasapi had a follow-up paper where he found these nice words, so I'm going to call them El-Kasapi words. The paper has a conjecture that they're asymptotically optimal. We've done some, my student and I have done some computer runs to get data, and in the limited range. And in the limited range where we can do the snowpone's degree is the infinite D limit, the limit of large cases. We lost your audio. Could you repeat maybe the last couple sentences, please? Right. My student and I have done some computer searches, and the El Qasapi words seem to be. The El-Khasapi words seem to be exactly optimal in Milpoten's degree, as far as we've looked, which is not too super far because the searches are hard. But as the final part of the answer to the question, the qubit case could be different because that's not the large D limit. There could be words that work in SU2 that don't work in SU20. Actually, those words exist, but what we don't know is whether the optimum is better than this log25. Optimum is better than this log 2 phi. But I'd like to think that in one way or another, it's going to turn into a question with interesting exponents like matrix, like what happened with matrix multiplication. Yes, thank you. Other maybe quick questions? Matt? Yes, I have one. So then this is just a question about what you mentioned at the end with using the alpha. Mentioned at the end with using the alpha is equal to two case as another way to explain solo 8 type. So when I need to remember how the proof actually works, I go to Wikipedia rather than the textbook. Would you consider adding the alphas, you could be the two case of zigzag gulf to the Wikipedia entry? Well, the I'll do that. And then that's a great. That's a great question. I really like that question. The problem is that Wikipedia has conflict of interest rules, and I was reluctant to violate them even in spirit. I don't know if they would actually veto me, but I wouldn't mind. I mean, it would be a great thing to do. It's just that you're not supposed to do your own edits in Wikipedia just to promote your own stuff.