So, it's my great pleasure to introduce our next speaker, Professor Chris Connell. He's a professor at Indiana University in Bloomington. And he will be telling us about DAI. What is it? Look at the IGAG. I'm sorry. Network embedding algorithm. And so the composition-based network embedding algorithms for prediction tasks on dynamic networks. Please, please. Networks, please, Chris. Well, thank you. Oh, yeah, no problem. Thank you, Minha. And I just want to say this is all joint work with Yang Wang, who's a PhD student here in computer science. I'm in the math department. I'm a geometer, topologist. So I'm new to machine learning. And, well, not actually new. I've been, I would do it on the side for a few years, but I'm new to the theory of it. And so I've been getting into. Theory of it. And so I've been getting into network embeddings in particular as my area of specialty in within machine learning. So I'm new. And so I'm learning a lot from some of the talks so far. And especially because these are more geometric, like some of the conferences you go to, it's really very, very practical. And this talk will be practical, by the way. So I am going to be focused more on practice. And actually, we have code and we actually run things and check it out. Things and check it out. So, but there's a few ideas here which I want to explore and at a more theoretical level. So, let's look at that. Okay, so the problem I'm interested in is, say, you have a parameterized family of manifolds, as is often the case. And Z is, say, the parameter set in some RK. And we look at some operators. We want to understand these manifolds, not just from a point of view of their shape, curvatures, et cetera. Curvatures, et cetera, reach your curvature and so forth, but rather from maybe some other operators. So we might consider Laplacian, heat flow on the manifold, Drac operators. All these are traditional things for Riemannian geometers like myself. We would study heat flow, for example, and understand the final distributions, given the starting distribution of how heat would evolve over time on the manifold and things like this. And so we would want. Things like this. And so we would want to understand this now for a parametrized family of manifolds. And we want to use this to do prediction tasks, though, on the manifolds, which might represent data. Well, typically it will, of course, represent data. And then I'll specialize. This is kind of at a higher level. I'll specialize to a real situation where we look at networks like social networks. For example, you could just think of Facebook accounts or something like this. So we'll see. Something like this. So we'll see how to apply this idea, this kind of more general idea. And so for each operator, we have some sort of characteristic properties, not always a spectra, because a spectra may mean it's not self-adjoint and you don't have an ordinary spectra in that sense, but it's not a real spectra. But you can look at characteristics, invariant functions, things like this. So this is the general setup. And the goal is to. And the goal is to take this data, these characteristic things, or spectral data, if you will. I'll call it spectral data in general. For Laplacian, it will be spectral data. Then we want to take that data and use it to build a really good representative embedding. And so the idea of the embedding is to really cut down on dimensions and provide a low-dimensional representation, which is very, very excellent for separating points and. Separating points and also doing further down the stream prediction tests. So, I think everyone's probably very familiar with that, much even more so than I am. So, I'm not going to belabor how all those tasks. We'll do some example tasks and see how this works in practice, though. So, I want to emphasize the practicality of what I'm doing. And so, we don't want D to be too large here, the dimension of that embedding. And otherwise, you don't want to overfit. You'd also don't want to be. You know, you don't want to overfit, you'd also don't want to be too small, you don't want to cram it into too small a place where you can't do separation. So, how do we represent these operators, or once we've discretized things, how do we represent the operators that describe as a manifold? Well, there's a few ways. You could take sort of direct sum over all the parameter space of the manifolds. Again, we have a parameter family of manifolds. So take the direct sum over that. That's pretty large because each dimension, the dimension of V here is the dimension of the discretization. Here is the dimension of the discretization, which already is kind of large, and now I'm looking at like v tensor z, uh, sorry, v direct sum z tensor v direct sum z star the dual. That's where this operator, say, is going to live with itself adjoint. And this is, this is a, you could think of this case would correspond to taking Laplacian matrix for each manifold and putting it in its own, in a one giant matrix, where I have the first manifold. Have the first manifold, second manifold, third, fourth, etc. Laplacian for the first one, Laplacian for the second, Laplacian for the third, sort of on block diagonal. And so this is sort of inefficient. People do this, though. This is actually studied. Sometimes, what's the name? It sometimes goes by the Grand Laplacian or something like this. There's some name they put for this direct sum decomposition. It's extremely large, though. Composition. It's extremely large, though. This dimension of the space is then huge, right? It's like m times 2m times the size of the parameter space. So that's rather excessive. Another thing you do, even worse, is take the tensor product of the whole thing and look at this. I don't know that this is ever done. That's even larger, so you don't want to have that kind of representation. But the point is, you don't want to lose data. You don't want to lose data about. You don't want to lose data about the changes that happen over time. So, you want to think of the z will especially look at the case where z, the parameter set, is really corresponds to time. And you want nearby manifolds should be, you know, this should be somehow approximated well enough that nearby manifolds look similar to each other. So, we want to have that use that fact that we're not jumping discontinuously in the parameter space generally. On the other hand, Generally. On the other hand, the manifolds are different. And so we want to capture this change in the parameter of the manifolds, right? And we want to change, we want to capture the spectral data of these operators on the manifolds so that we can do better approximations. So we want to take into account those changes to do approximations. And so typically we'll see that the Z is really like a timeline in many applications, just one dimensional. So K is one, the real dimension. It's one, the real dimension is of the parameter space, it's just one dimensional. And we maybe know past data and we want to predict future data. So we actually want to, you know, prioritize some of the slices closer to present time, for example. And yet we want to use that past data to predict future data. We don't want to discard it. We don't want to just use the last slice. Okay. Is everyone with me there? Hopefully, any questions so far? Hopefully, any questions so far? Okay. So, the last way we can approach this operate embedding maybe is to look at the approximate operators. When I say app, that means I've approximated the operators on a discretization, say. And I can look at them just parametrized by Z as well, not just the manifolds, but the operators, and sit them inside their natural space. Inside their natural space, V tensor, V star tensor, and then carry along the RL to the K if I'm assuming Z is like a square set. So Z has been discretized, and I assume it's rectangular, not square, but rectangular. And maybe K is one and it's just a sequence. And so then I can tensor that and get a higher order tensor out of this instead of maybe instead of just a matrix or two tensors. Just a matrix or two tensor, like this would represent a matrix instead of just the two tensor. I'm now looking at higher-order tensors for these. Okay, and that's the approach. This is controlled in dimension, so this is the approach that I want to take. And so we'll see how to how to handle this. Well, first we have to discretize the manifold. And so, given an operator like Laplacian, Laplace-Beltrami operator, then we want to be able to discretize this. To be able to discretize this so that the discretization actually reflects the spectral data as well. So, if I have a, like, for example, take a discretization, I want the surface graph for a surface, for example, or a higher dimensional manifold. I want the corresponding simplicial complex. So, I get a simplicial complex under the discretization. I want maybe the graph Laplacian to reflect the actual Laplacian and to have. The actual applaution and to have similar spectra and control the spectra at least on the low end, and maybe the very high frequency stuff I can't control so much because of its escape certain theory and the bile law and so forth. For example, you should be able to recover the volume of the manifold from the bile law, right? So if we had the asymptotics of the spectrum to be correct, then you could actually recover the volume, which the graph doesn't exactly see the volume, but maybe it's approximate enough you can actually get that. Approximate enough, you can actually get that information as well. So, there's a huge literature on this. Well, not huge, actually. There's just a actually hasn't been too long that it's been really come to the forefront, but there's a literature on this, especially for the Laplacian. Other operators I'm not as familiar with. Heat operator, there's been work because of, for example, you just think finite element analysis in airplanes. You want to see how heat affects under friction. See how heat affects under friction of airflow and things like that. So, heat operator, Laplacian. I don't know of, like, for example, I look at the Dirac operator. These are used, for example, format of theory for spin operators and things like that. So, spin topological variance, I don't know of as much work done in that area. But for heat, for Laplacian and the closely related heat equation, yes. So, these give you, there are good discretizations that actually work. So, you can't just willy-nilly discretize. Work. So you can't just willy-nilly discretize it. Just because you're small doesn't mean that the corresponding Laplacian, because you might be too small in some places and not small enough in other places, you have to take into account curvature and so forth. So there's a whole literature of this. I'm not going to talk about that. I just want to point that out. So we're going to assume we've already discretized well and that this has been done and that the corresponding operators have similar spectra to the actual real operator on the manifolds. Okay. Manifolds. Okay. So let's simplify this. So once we have that, we can, we then, if we're, if we're doing this for like a time parameter family of manifolds, then we would actually have something like a dynamic network where k is one, the parameter space is just one real dimension. And I'll call z equals t here since we think of it as time. So I'll call it t for time. And the z set I'll replace by this. All replaced by this calligraphic T here. That'll be my parameter space. Okay, so now I have reduced down to a family of networks with link. And maybe because we want these networks maybe to carry other data, I might have some link vertex data associated to each link. Okay, so some tags or labels. And I want to keep that information or use that for the embedding as well. Okay. And so I think of the And so I think of the whole space here, the V as the, say, the vertex set of each of the graphs, networks, you know, graphs, same thing. The computer scientists always use networks, so I tend to present it as networks, but it's graphs, I guess, because it's more mathematical audience. So finding the idea again, it's finding the embedding of this network. Well, I may not want to just do embedding of the entire network. Really, I want a Network. Really, I want a representative single slice. So I take those vertices and I can treat all the vertices here as the same for each graph. So what happens is, suppose I have the graph, the number of vertices differ. Well, I am assuming that nearby ones also, you can track those vertices so that a vertex in one time slice actually can be seen or more general in the higher dimension of parameters can be seen as a vertex in the neighborhood. Conversation can be seen as a vertex in the neighboring. And the way you do this, you just throw them all in, whatever vertices you have with data you have. If some of them might be orphaned, but they're still represented in the graph, but the label data will be that they're orphaned or you don't have an edge. In other words, you either, after some time, your edges stop and that it doesn't continue on, or it's birthed, and now you have an edge into the next time slice. If I think of my time slice as this one. Slice. If I think of my time slice as this way, then you go into the next one. If you birth, you have an edge going this way, but not an edge going that way. And you also have edges within the time slice. Does that make sense, the structure? I'll show a picture in a bit of that. But that's what you want to think. So the vertex set is really the entire vertex set. But what are the graph relations? Well, we want those graph relations to be kind of a new thing that's amalgamated for prediction, and we just want to. For prediction, and we just want to embed those vertices into Rd, some dimensional space, so that in such a way that the distance from other vertices reflects those edge connections. And so prediction can be done well using that embedding. Okay, and so this is the idea of network embedding. And our dynamic network embedding, we want to use that time-changing data to get a better prediction on these sort of tests. On these sort of tasks. Any questions so far? All right, please stop me if you have any questions. I'd like to answer questions. All right, well, I'll jump into spectrum methods. So spectral methods for just static graphs have been around for quite a while to do network embeddings for static graphs. And well, spectral methods for graphs, of course, have been around for decades and decades, or maybe well over 100 years. Or maybe over well over 100 years. But more recently for embedding the results for static graphs, that's been around for a while as well, a few decades. So what's the idea? Well, you take the adjacency and Laplacian matrices for the graph. I'll write the vertices of V, E for edges here. And Laplacian is just given by the diagonal D minus A, the adjacency matrix, where D is the D. D is the D is the matrix of the degrees of each vertex of the graph, and adjacency matrix is just one or zero if you're a neighbor or not. We want to look at weighted graphs, so we're going to, because we're taking this label data in, we want to weight these graphs, so we'll have directed and weighted, weighted directed graphs, and not just graphs. But there's a version for that. There's also a normalized Laplacian, which for certain reasons, when the eigenvalues you want them to be normalized in a certain range. Normalized in a certain range, and so this is the standard normalized Laplacian. You take the diagonal, the degree matrix, and you conjugate, well, not conjugate, but you take d square root of it, one over square root of it, in a sense, eight, one over square root. And here, this is the adjacency matrix. You can split L splits as for the incidence matrix, you can split it as M transpose M, and therefore you know that that's You know that it's self-adjoint, right? And so these real eigenvalues will have real eigenvalues and so forth. And that's the spectrum. Okay, and so the normalized version has real eigenvalues in, I think, minus two, two or something. So it's just a normalized spectrum, and you can recover one of these from the other. So the meaning is interesting, though. These eigenvalues carry lots of topological information. So the zero eigenvalues, for example, the number of connected components. That's easy to see. Components that's easy to see, and the eigenfunctions are the harmonic functions which are component-wise constant. Because on a you know, sort of the same result for the maxim principle, but on a compact piece, at least the eigenfunctions, sorry, any harmonic functions on the compact manifold are zero. And this holds actually for the components of the graph. And in general, the eigenfunctions tell you some sort of resonant information. Tell you some sort of resonant information about how neighbors talk to each other. And so it can actually be recovered also from random walk data. So random walks are on the graphs, which is another method of doing embeddings. They're telling you actually very similar information, but you can weight them differently, but you can also weight the Laplace differently, too. So that's kind of a similar ideology for using random walk embeddings versus spectral embeddings. Okay, but the small eigenvalues. Things. Okay, but the small eigenvalues also describe higher connectivity. You can actually tell information, sort of asymptotic information about number three cycles and things like this. But you can't usually like, given a question you have about the structure of the graph, you can't like easily write that in terms of the spectrum. But there's some information you can glean from it. There's a lot of theory on that. Okay. So the weighted indirection direct inversion looks like this. And this is the And this is the weighted Laplacian. You have this interesting prone-Frobenius operator here, which is Pij is just take the weights and divide by the sum and look at that matrix of weights. And then phi here is the, phi is this matrix. So you write it as phi minus one half, say phi p p transpose phi. Phi is this, there's a unique prone for venius eigenvector since this is positive or this. vector since this is positive for these positive weights, left left eigenvector, so phi dot p is equal to phi. And then if you take that matrix, they take that eigenfunction, which is now just a function on the vertices, i.e. it's a vector, and you diagonalize it, then that gives you this matrix phi says the diagonalizing that prone for Benius eigenfunction. And then you can write this matrix for the weighted thing this way. There are many competing Laplacians for Competing Laplacians for weighted director graphs, but actually, this is the one that we want to use and it works well because of the symmetry of it. Okay. And again, here we're just trying to encode that label data so that we can apply this to like social networks and various networks with labeled data. Okay, so the first embedding for static graphs that we want to consider is the effective resistance embedding. And this is a neat idea where Where you can think of the resistance, think of it as an electrical network. So the network is each of the edges is some resistor. And if there's no edge there, it's like infinite resistance, right? You can't go through something that's not there. So that would be infinite resistance. But otherwise, they have the weights give you some sort of resistance on the edges. And you want to know, given two vertices, you want to know how much electricity, what the resistance is from here to here. From here to here. And that resistance, of course, the electricity can go along any and every path. And so it's sort of like Brownie motion. You have to consider all the paths at once between here and here, but now there's actually only plenty of paths because we're discretizing. And it turns out this quantity, looking at the, if I take the Penrose more pseudo-inverse of L, remember, L has some, maybe has, if there's more components, there's at least a kernel to L. There are harmonic functions. So you have to take a pseudo-inverse and you take that dependent. pseudo inverse and you take that Penrose more pseudo inverse which in a sense ignores the the kernel and then that gives you that gives you if you look at LLX you look at the diagonal elements in the pseudo inverse of the Laplacian minus two times the xy here then oh this isn't the symmetric case but if you have a non-symmetric case there's a slightly similar formula then this this actually tells you the resistance so somehow the Laplacian is So, somehow the Laplacian is seeing the resistance right across all the nodes, all the edges, and all the paths together. So, it's somehow aggregating and averaging the paths, just like you would think of the Laplacian should do harmonic functions, you know, average on neighbors and so forth. So, this is reflected in the fact. So this is not too hard to prove. But the resistance embedding then says, well, you're using the pseudo-inverse here. So, what happens is the inverse of the eigenvalues is what's coming in. So, the resistance embedding. Coming in. So the resistance embedding is just take so from the vertices each vertex goes to, so draws, take its values, its component, the jth component, and you would take the jth component divided by, so the jth vertex goes to the jth component divided by the corresponding eigenvalue up to dimension up to d. So you use the d smallest. The smallest eigenvalues. And this is this gives you. So here, remember, the eigenvalues are, I think it was back here. I said, yeah. Sorry. This is the largest. So these are the largest of those. And this is on second. Yeah, it should be. No, no, no. That's these. Okay, that was for the other thing. This is for the, these are in the order. For the these are in the opposite order, so this is the smallest non-zero up to the up to the d smallest non-zero. So you take this embedding, and it turns out, so this gives you an embedding which reflects the resistance. So if they're close in the embedding, the resistance was small between them. If they're far in the embedding, the resistance was large. And so this gives you an idea that, okay, there's a lot of connections to nearby vertices in the vertices in the in the um in the embedding and in fact if you um if you write the the n by n rank d approximate to the pseudo inverse and write it as this way one over the eigenvalues again um up to the so these are the the largest ones of the pseudo inverse because those are smallest of l then this will be this has the property that in fact The property that, in fact, these resistance is actually equal dead on to the L2 distance. So, if we take the L2 norm squared between the vertices of the embedding, it's exactly equal to this resistance. Well, not quite the resistance, but the d approximation of the resistance. So if d was bigger than n, it would actually be dead on equal to the resistance. Okay, so this way you can take a smaller d and you're getting most of the resistance data captured. Data capture, but you're then cutting down the dimension as well. Any questions on that? Right, so the smallest eigenvalues, non-zero eigenvalues of Laplacian correspond to the largest of the pseudo-inverse figure. So that's really picking up the largest spectral approximate the biggest, the biggest components, if you will, of the pseudo-inverse. Will of the Superinverse of the Plus. All right. So there's, let me just kind of describe the adjacency minimized embedding. So this is another approach where you use the adjacency matrix instead of the Laplacian. And one reason is because the resistance embedding has one flaw, that is the small, you're using the small eigenvalues, which are hard to capture. There's a lot more error to when you're dividing by a small eigenvalue. To when you're dividing by a small eigenvalue, it's very big, right? And so that causes several problems. You can get them, by the way, just like you can, you know, you can pick off the biggest eigenvalues of matrix very efficiently by looking at approximate, take a vector, hit it, and look at the, it's just like the Lyapunov exponent. The Lyapunov exponent will grab it in the biggest direction and make it very close to the eigenvector in the largest direction very quickly. So it's exponential convergence, the biggest one. Biggest one. There's a trick using inverses too. You might think, oh, but the small one you can't get, right? Because small space, small Lyopanov spaces are lost to the big ones. But there's actually a trick, not just not inverting it. Without inverting it, you can actually use the matrix itself through an algebraic trick to also pick off the smallest ones too. So it actually turns out to be efficient to choose off the smallest ones, but numerically a mess when you're working with those small eigenvalues. So the adjacency minimizes. The adjacency minimized embedding is actually just easier to work with. It actually captures very similar data. Again, the adjacency matrix, you know, the Laplacian is built from the adjacency matrix, but they actually carry different information on the face value. But it's equivalent information, right? But if you want to look at a particular eigenvalue, it's telling you different information. And so what you can do is you can do an L2 minimization of the distance. So if I look at this, what I'm saying here. the what I'm what I'm saying here is I want these these to I want the bedding to reflect I want the bedding to reflect the smallest weights for the for the adjacency matrix and that ends up giving me at the end of the day I can change this to the to get a eigenvector this is just becomes the largest eigenvector for d inverse a and after linear change this becomes Linear change, this becomes, okay, so I won't go through all this. This ends up being largest eigenvalue for I minus the Laplacian. Okay, so really this is telling you this adjacency minimized embedding, the y star is telling you, actually we use, yeah, z, we use the change in coordinates to d minus one half of y, so d sorry, d one half y to get the z embedding. That's called the Z embedding. That's called the adjacency embedding if I do this minimization. So I solve for this minimization for y, and then I plug in and then I take d to the one half of that to get the z star elements. And that gives me an embedding. This is just for d equals one, so that gives me a one-dimensional embedding that minimizes the distance corresponding to the adjacency, the weights of the adjacency matrix here. All right, but that's, and then you can do this also. But that's, and then you can do this also in dimension D as well for any dimension you want and pick off something. This is just ends up being the largest. So there, the embedding is the eigenvector corresponding to the largest eigenvalue of I minus L, right? So notice the smaller the normalized Laplacian eigenvalues then corresponds to the bigger eigenvalue of the I minus. The I minus L right, so one minus one minus Lambda for Lambda is the eigenvalue of this object here. Okay, and so I can do the minimization over what I do is to do this previous minimization due to the general case, you just do it, you do it first in one eigenspace, and then you take the minimizer you get there, and then you then you take the minimizer. And then you take the minimizer in the orthocomplement to that, do that one, and then you just do dimension reduction this way. And so that gives you, you always just do it in the orthocomplement of the previous minimizers, and then just inductively build that. And that gives you, then you get the adjacency matrix embedding. And so somehow this adjacency minimized embedding reflects also similar data to the Laplacian. Data to the Laplacian embedding, except that now we're really working with the bigger eigenvalues. So you're actually picking off some of sort of the bigger eigenvalues of the Laplacian, or not quite, but something like that. And so it's telling you a little bit different information because of that reordering that you do. But again, it should be minimal that you're close if the adjacency matrix has a strong connection between two vertices. To vertices, then again, you should be close in the embedding. So, there's a bunch of generalizations of this idea that you can do. For example, you can put weights on the path. So, you can think of the adjacency matrix, you know, as counting the, well, if you look at the convolution, so take the adjacency matrix to the L power, that's telling you the number of paths. If I just took the straight adjacent matrix. Paths, if I just took the straight adjacent matrix, the number of paths between x and y, if I look at the xy element in the net corresponding matrix A to the L. So the powers of the adjacent matrix tell you the number of paths. So what you can do is you can weight this for a weighted adjacency matrix. You can take a, if I look at the set of paths between of length L, say between I and J, call that, call that. Um, call that call that matrix P L, take the weights and then sum weights on that, and then sum them up from one to infinity. That counts a that gives me a weighted, okay. So, first of all, if I had a weighted adjacency matrix, I'd get weighted paths, okay, the total number with weights on them, but I can then also weight longer paths, discount longer paths or shorter paths, whatever I want. I can proportionally weight the path lengths themselves. And that's what the cat's metric. themselves. And that's what the Katz metric does. So it says, for example, if I took some number less than one, and I look at this quantity, well, this exponentially decays. So the paths, these are all the paths of length L from I to J, that gets discounted exponentially. But I can also do something more general and have weights actually be matrices themselves and then get something more. And then get something more general as well. Okay, but this is the idea that I can weight parts of the graph off, or paths that enter in parts of the graph, or I could just longer paths, I can discount their weights in the effective Laplacian or in the new adjacency matrix. And you can actually realize this cats metric then. You can actually realize a corresponding Corresponding adjacency, weighted adjacency matrix that would actually be of a virtual graph that would correspond to the adjacency matrix. So, the adjacency matrix for that virtual graph, the weighted adjacency matrix, virtual graph, would actually be the cat's metric weighted adjacency matrix for the original graph. Okay, so that's the, it's just this. If you have the original adjacency matrix of A, you just take, if I'm, this is if I'm doing one weight, then it's I minus X. Then it's I minus omega a inverse minus i of one over omega. You write that as a power series in terms of the powers of the original JC matrix. Okay, so these are some extra ideas you can do for messing with if you want to control how you're how you're doing the embedding at the end for the JC matrix embedding. You can do this. Okay. And there's some convergence issues. So what do we? Some convergence issues. So, what do we want? Well, we want it again to use a tensorial representation of our dynamic network or our, you know, our represented manifolds, for example. We have the discretization, and then we discretize them in the graphs, and now we have a layering of graphs. That's how you would do it. So, and these graphs might represent, yeah, so the graphs represent the, now I'm doing it vertically, so the vertical slices, time is going in that direction. So, this is the one-dimensional case. This is the one-dimensional case. And I just make a tensor just by stacking, right? So that gives me a tensor. I've assumed all the vertices are in common. So if I do an adjacency matrix, for example, then I can stack the graphs this way, and then I can make a tensor from the adjacency matrix of each graph. Or the Laplacian matrix, whichever I want, or any other operator as well. So I do this. And then I want something, though. And then I want something though, but we want to capture something that's not just slice-wise spectral. So the static idea would be just take each spectrum of each slice graph and then study that and put those together. But I want to do this all at once. I want to be more intrinsic about it. And so we actually use our decomposition theorems for tensors that generalize the idea of the spectral theorem. However, they're far more complicated. So they oftentimes go under the name of canonical polyadic decompositions. Polyadic decompositions. And what's the idea? Well, CP decomposition for the rank of the tensor. So, unfortunately, the rank of the tensor is the minimum number of components where I can write that tensor. So, say I have a three tensor. If I'm looking at just time, it'll just be a three tensor since my parameter space is just one dimensional. The third dimension just will be the size of the number of time slices. So, if I look at this. So, if I look at this decomposed tensor, so I've got the adjacency matrices, or if I did it in the picture, it's this way. I've put the adjacency matrices in and with a common set of nodes. And now I'm looking over all the time slices. So the third dimension is just the size of the number of time slices. Then I can ask, well, how can I decompose this minimally into some numbers, times, and these should be unit vectors? Unit vectors. So simple tensors, right, or elementary tensors, meaning just vector, tensor, vector, tensor vector. So if you think of the eigen decomposition, right, that's actually a CP decomposition where I've written, where r is actually equal to n in general. And I've written it as vi, if it's a symmetric matrix, I could write it as sum of lambda i vi tensor vi, right? So that vi tensor vi, that's the eigen. Vi tensor Vi, that's the eigen spectral decomposition of a matrix of a symmetric matrix. In general, you'd have Vi tensor Vj for a KK decomposition, Cartan decomposition, and so forth. So the lambda would be the eigenvalue, and the Vi's I would run from one to N, Vi tensor, Vi would be, Vi would be the eigenvectors. Okay, so this is generalizing the notion, so you can think of these as the eigenvectors for the tensor, at least when R is minimal. At least when r is minimal. Now, unlike the case of matrices, even figuring out the rank is n p-hard. Okay, so given a matrix of with if the dimension, just even in n, even for a three-tensor in n, it's n p-hard. So as the dimension goes up, it's np-hard in n. And what's worse is that you can have like open sets, Zirsky open sets in the algebraic variety of the tensors in the same dimension. Tensors in the same dimension where the rank is actually different. So it's not like a matrix where you know the decay of the rank of matrix is on some lower dimensional algebraic set. That's not the case. So things are complicated to actually compute the minimal rank, but we don't actually care about the minimal rank. Turns out, you know, so you would have no hope of having these algorithms finish in any time if you wanted them to compute exact best R values and things like that. R values and things like that. But there are fast algorithms known for computing approximate canonical paleadic decompositions given specific size elements for the matrices. Okay. Oh boy, I'm running slow on time here. Let me move a little faster. So there are many canonical polyodic decomposes. There are many varieties. There's also things like Tucker decompositions. There's just a whole slew of options that go by a bunch of different names. That go by a bunch of different names, and some of them are equivalent, by the way, but others that just have different names. They're special cases of general canonical polyadic. We look at it, we actually use a few in our algorithms. So this is, so that's the decomposition. Again, we want to write it in terms of simple tensors and maybe only a few of them, and we only want to approximate Z. So orthogonal version of this, well, you think like for Well, you think like for a symmetric matrix, you can choose the eigenvectors. If I choose it as Vi tensor Vi, I goes from one to n, sum of lambda i times that, those simple tensors. Then I can choose the Vi all orthogonal to each other, right? That's by the spectral theorem. But here you can't do that. You can't choose all the AIs, all the BIs, and all the CIs all to be orthonormal sets when you have a three-tensor or a higher tensor. You can't do this. However, you can. So, you can't do this. However, you can almost do it. You can actually require either the AI and AJ are all of the sets are either orthonormal, the pairs are orthonormal, or some multiple of each other. So that's sort of surprising. And that turns out to almost be unique when you do that. So we call that a strongly orthogonal decomposition. It's not actually unique, unfortunately, but it's almost, but then you can pick some things to make it unique. Some things to make it unique. So we can, if we order, for example, we want to order those eigenvalues to get the decomposition in the right order. And then, and then if you do sort of a mini max operation, we can actually make it essentially unique. And that's what we're going to actually use is this SOCPD, the strongly orthogonal CPD composition. Generally, we also try it with other versions and try our algorithms with other versions as well. Sometimes, actually, one. Versions as well. Sometimes, actually, one is better than the other. It's not always that you use this SOCPD and get the best results in our algorithm. Okay, so the idea is that we've then approximated up to only dimension D, that's our betting dimension, the decomposition of the tensor. And you can use this alternating least squares method. This turns out to be fast, and probably the best method is alternating least squares. This is not actually, it's not implemented. not actually it's not implemented in the python libraries completely um for example we actually implemented we added to well we didn't add it to the uh library but like pytorch and stuff don't have this um the orthogonal thing they it's a sort of a to-do they have never not finished so we went ahead and did it for them but um this is in you can see this in the github repository that we use for this case so that's a benefit to society if you will to have the orthogonal versions and there's various orthogonal versions And there's various orthologies you can do as well. Okay, and so once we have that, we can also look at modifying the Frobenius norm. So, these, the ALS algorithm uses a sort of Frobenius norm iteration reduction approach, sort of like least squares fitting, you think of higher-dimensional least squares. And so, the Frobenius norm, we also can muck with that in our algorithm to produce kind of weighted results. To produce kind of weighted results where we want to control, like say there's some curvature issues or something like that on your proximate manifolds, you might want to weight the norm appropriately to reflect the Romanian norm tensor coming into that. So just for various reasons, we want that generalization in the algorithm. So we can also do that. And this, the Frobenius-Norman tensors is given. Well, here's the three example itself. You just take the product of the component. product of the of the component-wise Riemannian inner products okay and so we use that and now the CPD modes so those the lambdas are the the corresponding components those are the mode functions and in the matrix case those would correspond again to the eigenfunctions and if we have the um we'll have multiple now they might be not the same so the bi and the ci don't have to be the same like they would be for a symmetric matrix even if you have a somewhat matrix even if you have a somewhat like a symmetric in the first two components just because the the the components are symmetric see these modes are actually taking into account the neighboring relations as well and so that's part of the strength of this is that it's not just taking to count like each slice separately but we're actually creating something unified about the whole structure of the parametrized discretized manifolds and so this gives you And so this gives you these give you different components. So now, so we can understand them in a little different way, but you think of them in terms of the like SVD decomposition, what those represent. And so that's the same idea here for a more general matrix. And also, we can analyze this at the end. I'll hopefully have a little time to show you why the method works and what the relation between the modes and say the slice eigenvalues. modes and say the slice eigenvalues is we can get actually a control on that to understand a little bit about that but it's sort of a little more mysterious at the three tensor level and higher and higher tensor level what these modes exactly represent okay but it does work well i'll show you that in a sec okay so um in our yeah for time's sake i'm almost out of time so let me ignore we do some pre- and post processing using really recursions and stuff to to because of the way the To because of the way the way data in, say, a social network would be, you have to do some pre-processing to make it appropriate to apply this algorithm for, but that's very easy to do and well understood. We do also a final convolution in the time direction. Once we get that time, the third component, say, would be the time, or the higher components are the corresponding parameters. You do a convolution of the parameters to wait, like you don't want to maybe you want to consider more recent data versus older data. You can do that very easily. Data, you can do that very easily. So, that's one of the nice things about this tensor decomposition: you can understand that last component, what to do with it, you actually just convolve to weight past times or things like that. Okay, so that also gives you a nice, nice way of working with the resulting data for the embedding. And so finally, our embedding is to send in. So, if I use the adjacency matrix, for example, as my slices in the tensor, then I would just send my ith vertex into. ith vertex into uh take and for a certain reason we take the second component the b component you can also use the first but you have to change it a little bit um and then you do a one-half weighting it turns out it's the right thing just like the square root on the one over the square root for the resistance embedding uh so now when you're doing jc matrix it's inverted so it's actually the square root of the eigenvalue is the right thing to take and this is now the mode of the cp decomposition so it's not really an eigenvalue but it's a similar idea and then you can evolve with the And then you can volve with the last tensor. And so that's the time tensor direction. And that gives you your embedding, okay, into Rd. And so we just chose D, whatever you want to be, just picking off the biggest modes, the D largest modes of the adjacency tensor now for this time-varying graph. And so that gives us the embedding. There's also a Laplacian version where, again, you're going to divide by, turns out this time you want to divide by the actual lambda. Time you want to divide by the actual lambda because the dimensionality. Okay, and that's the algorithm. I'll skip that, just the statement of the algorithm. So we do this actually on a bunch of data sets. We have, we actually use Facebook, social patterns, primary schools. This is not, this is somewhat periodic data in time. It's interesting as opposed to Facebook, which is sort of continuously evolving. And then we have some email. Evolving. And then we have some email, Stanford email data sets that sort of random that's more discontinuous in time. And we look at what happens with it. And here are the number of nodes. Facebook at 6063. So these are not huge sets, but they're decent size. And then we compare to a bunch of baselines, like some of the, this probably has the, had up to now, probably one of the top performance. Up to now, probably one of the top performers. We can actually beat this in many categories. And then some other, most of these are auto-encoders or recurrent network and with a graph, neural, graph-convolutional neural network added on the end. Sometimes, sometimes you compose with the GCN. And there's just the graph infmax, which actually is great. The resistance, the standard static one on the last timeframe, if you just do the last timeframe, we also can. If you just do the last time frame, we also compare it to just the standard static resistance methods or adjacency weighted methods on the last time frame. They actually do quite well by themselves, oftentimes, like for Facebook. And basically, these are our algorithms here. You can see we actually do often perform the best and outperform it. And surprisingly, the time, if you look at the running times, VGRN is sort of one-off method. It doesn't scale with the dimension. method it doesn't scale with the dimension so so if I compare these are using the same time as the VGRN runs our our time as the dimension grows we actually grow linearly linearly it for small d anyway for a while in our algorithms and and we actually are matching it to so we actually use the parameters in the algorithm so that they're all running about the same time in these things these are for dimension 128. Okay and so Dimension 128. Okay, and so I'll skip these. And these are different norms used for separation norm at the end. And what is the task we're doing here? This is actually a link prediction. So we're just predicting the next stage, we knowing the next stage, we just predict the link stage from having the past data. Given the past data, what's the next stage of the graph going to be? And so that's the link prediction. And basically, our album does quite well in this. So this approach does really quite well. And why does it do quite Really, quite well. And why does it do quite well? Well, it's connected to the resistance embedding for the static version. And I think one of the things is we did an eigenvalue analysis of this. And so you can do if you have slowly changing things, a relative, in other words, the time steps are slow relative, very small relative to the whole scale of the time, then you can actually see that the z tau could be like if z tau Z tau could be like if Z tau is the last frame and you write Z t for the last frame plus increments, then you can do eigenvalue analysis for incremental eigenvalue analysis. Ips and Nadler have some results on that. It goes back to the 70s, but there's sort of improvements on this. And you can compare the time slice eigenvalues versus the last frame, and you get some perturbative analysis, which shows that they're not changing drastically. Shows that they're not changing drastically on the high ones for the adjacency matrix. They don't change too greatly. The eigenvectors have a greater change because you have to divide by the, there's a component where you divide by the differences in the eigenvalues, that gets a bigger change. So the eigenvectors have a more dramatic change than the eigenvalues themselves. But in fact, the Bauer-Flight theorem tells you that the change in the eigenvalues is controlled by the size of the error matrix, the L2 norm of the error matrix. The L2 norm of the error matrix. Okay, so that gives you some sort of perturbative analysis. And so, really, this is actually not too, and under that assumption, our decomposition, the modes are actually not too different. It turns out we can, we do our own analysis here and prove that there's some control on the size of, in terms of the discrete change matrix, that's controlled. If that's small relative to the number of steps, then there's actually a decent. Steps, then there's actually a decent control on those eigenvalues. So, actually, our modes in that case are somewhat close to the modes for the static slice, and which we want that to happen because you'd expect that that would be the last frame is going to tell you about the next frame very often, at least when it's a continuing state change. This is not true, for example, in the school data set where it's periodic. It's more periodic, but it still does well. More periodic, but it still does well, and what's happening there is something different because there it's not because of the periodicity, you don't have this assumption. So, this kind of explains the Facebook why Facebook did so well. Um, but it doesn't explain the other one that for that, we need another analysis which says that the modes actually pick up that periodicity and time as well, the CP depublishing modes, whereas the static modes don't pick that up, right? So, that's somehow the neat attribute of this. Uh, we want to investigate that further, we haven't completed the analysis. To investigate that further, we haven't completed the analysis on that case. Like, what happens in periodic? Why does it do so well? But periodic data as well. And, you know, it looks like we're kind of record holding right now. So in some situations anyway. So this is kind of neat. And we're looking at the perturbative analysis here. Okay, make a long story short since it's over time. This tells you that it's close to resistance eventing. The idea we're using using the CP modes. Using the CP modes gives you something that's like a resistance embedding. And so, therefore, at least for data like Facebook that's slowly changing over time, you expect that it's going to be at least as good. It actually does a little better than the single time slice in our experiments, but we also, and other auto-recurrent network methods as well. But it also does well in other cases. It doesn't do as well, but nothing does well in the case of like the computer. Nothing does well in the case of like the completely discontinuous time stuff like the email data set, the third data set. That's everyone does sort of poorly in there, but we still do decently well. I was surprised actually in that one as well, that somehow the modes are picking up something in the time variance of those components as well. Okay, well, let me say one other thing about large N. This is my last slide. Um, this is my last slide, I promise. So, um, if I look at the MIN jC matrix, you can actually look at uh random graphs and see how this does. And it turns out that the control on random graphs, you look at random matrices, matrix theory is something I'm actually into on the theoretical side, like Tau Bu type bounds. Um, for what is the spectrum, you have these, you have these uh Wigner laws and so forth that determine how the density of eigenvalues. How the density of eigenvalues should behave. And we actually only get for, if I take the adjacency matrix, only a small percentage of the large are actually, there's only a small percentage of really large eigenvalues. So that tells you how big D should be. So we can actually use this sort of law of large numbers, if you will, for eigenvalues. So this Wigner law type stuff to actually tell you how big the D should be. And it turns out. And it turns out, you know, you can look at the distribution of the eigenvalues. It looks like this in terms of the case that we're considering for random graphs. And you can actually figure out that the dimension will be a small fraction of n that will cover most of the big eigenvalues. And then the small ones, it'll just have diminishing returns. So you can just figure out kind of how big that d is in terms of it and much less than root n. So, all right, I'll stop there and say thank you. And say thank you. I'll leave that last slide up. Thanks. Thank you, Chris. Thank you, Chris, for the very nice talk. Are there any questions from the audience? Hi, Chris. Yeah, I do have a couple of questions. So thanks for your talk, but I was wondering for the discretization of the Laplacian, what are the theoretical guarantees that when you're taking the Laplacian of a discretized approximation of a manual? A discretized approximation of a manifold, then that thing actually approximates the Laplacian of the smooth Laplacian of the original. I think that that's only known for surfaces, isn't it? Yeah, actually, you're right. Sorry, I should have said that. Yeah, the ones, the results I know are only for surfaces for that, that are dead on. Although I believe it is conjectured, that should be doable. I think it's conjectured, but only for surfaces, it's actually right, right? Even in that case, it's already limited. Right, exactly. I know my one of my. Exactly. I know one of my grad students, Jenyu Lee, needed this for surfaces. In fact, she was doing some entropy rigidity stuff for surfaces and used that discretization to get approximations on the entropy for the geodesic flow on a negatively curved surface. And needed that approximation exactly because she had developed a method for getting various bounds, dynamical bounds, based on discrete ideas. Balance based on discretized Laplacian, but we wanted that to for graphs. She did it for graphs, and then we wanted to do it, apply it to that. And so she used those results. So I do know those results for surfaces exist because she used them. But the higher manifold case, I guess you're right, is still a bit of open territory. Yeah, even though it's kind of assumed and people use it. Maybe although theoretical guarantees are not. But I saw a talk. Wait a minute. I saw a talk about this though. Really? Recently, somewhere. I know it's work done. Maybe I don't know how far it. I know it's worked on. Maybe, I don't know how far that should comment. Yeah. And I guess another question was. But again, I'm not using that. I'm assuming that's already been done. Whatever your graph structure you have is being done. Yeah, no, that's fine. I mean, everyone works with the graph Laplacian. I'm just like, I wanted to know what the status of that was. And so when you also, when you're taking a graph of a discretization of a manifold, you're going to end up with sparse adjacency matrices and a sparse Laplacian because. Sees in a sparse Laplacian because you're only seeing like the nodes that are around a point. But when you're working with a network in like social media or in like other domains where it's much more highly connected, the graph is not going to be sparse, maybe. So like how, or like the matrices involved are not going to be sparse as respectively. So how does that affect like your the performance of some of your techniques? Because you are looking at the social networks, but I was just wondering if you. But I was just wondering if you like if you saw some change in that. Yeah, I mean, so let me look at the time, show you the time. Yeah, so this is in seconds, right? So our algorithm dealing with like the, let me see what's the biggest set here. It was, let's look at the call it set. That's eight, that's nearly 2,000, 2,000 nodes. So, you know, the JC matrix is 2,000 by 2,000. And then the time is, I think. And then the time is a, I think a few, was it like 100 or something? Oh, no, time steps is only 10 there for that one. Okay. But that's a pretty big, that's a pretty big, already that gets pretty big in the embedding, and it's not sparse, as you point out. But you can see that we can still do this in even when D is 120, the dimension is 120, it's still taking 600, 600. Oh, sorry, this is the colors. It takes longer, yeah. It takes 1400 seconds. It takes 1400 seconds for the complete algorithm to run. So that's pretty reasonable. And it looks like it's growing linear in the dimension. As you grow in the size, it's actually should be growing at like, it has to be growing at least by n to the 2.33, right, to get the spectral CP decompositions no faster than n to the 2.3. It's probably more like n to the 2.76, something like that. Like that. So, a final question, which I asked a little bit ago, but how does the number of eigenvalues change that you need in order to represent the, say, a surface of genus G depend on the genus? Oh, yeah. I have no idea. Well, okay, so for the representation, yeah, it depends on curvature. It depends on curvature, probably more like where the so you have say you have constant curvature uh surface. I'm not even worried about constant curvature. I mean, just thinking about it, like embedded in R3, but and you have like a discretization as a graph. And it's like whether it's not, so that embedding is see, it depends on the embedding and that embedding is not constant curvature. So it really does, the curvature comes in, right? And like you'll have the negative curvature in the inside of the genus, right? Inside of the genus, right in that embedding. Well, whatever embedding you use, you're always going to have some negative curvature and positive curvature both in that embedding because you can't embed negative curvature and just but even to just represent the genus you need a number of nodes to be able to like right graph it out yeah i don't know the minimum yeah the more precision you want you have more vertices where there's higher curvature less where there's lower curvature and then but you still need you know more and more to uh to approximate i don't know More to approximate. I don't know the numbers on that offhand, but I can tell you that given the graph, what D should be, and this analysis here gives you like only O of one over N. So the D can actually decay like something. Sorry, no, no, no. That's N minus it. So I didn't actually write the bound for DN here. What is it? It's something like. It's something like, um, yeah, you know, it is. That's right. Uh, so it's like little O of O, okay, I have to read, this is written for the density when it's between minus two and two. So this is for the normalized applaution. So when I multiply that out, it's like it's it's like a little bit bigger than O of one. So you're just growing, you only need like a Just growing, you only need like a fraction that's growing, like certainly way less than root n. D can be way less than root n in terms of the number of eigenvalues you need. I think it's actually maybe something like c plus log n or something to get the to get most of the mass you need for a good representation in our embedding algorithm. So, so Chris, does that mean we can we can really estimate like the embedding dimension D? We can kind of estimate it numerically. Kind of yes, made it in American, yeah. When you're gonna that's a very nice picture, actually. Well, okay, sorry, that's for random graphs, so that's like for the random graph. The problem is that your actual graph, you have to do the analysis on each one if you if you want to do that ahead of time. Because see, this is the this is the random ensemble graph theory for like random matrices. If you look at the adjacency matrix for a random graph, that gives you this a random matrix ensemble, and then you use random matrix spectrum. Then you use random matrix spectral theory of random matrix to figure out what the distribution is. And it looks like this for that. But with the database for all of the elements of the database, Chris, right? Second? If you have a database of like graphs, then you could do it for all of them and then get an embedding dimension where you could fit them all. Yeah, yeah, you could do the analysis across that. But again, it won't be a truly random, like this is assuming like the newly random variables like the GUE ensembles, if you're familiar with random A. Ensembles, if you're familiar with random matrices. So that's not going to be the same. This is actually applied to the ensemble that you get from adjacency matrices of the results of Ding it all are from adjacency matrices on random graphs. So you start with the random graphs by Bernoulli putting in edges or not, and then you look at adjacency matrices, and then you compute the distribution of the eigenvalues for large n. And that's the story there. That's the story there, but actual ones don't tend to exactly aren't going to necessarily follow that same GOE distribution. Still, I think it's a very nice result, actually, with this embedding dimension. I think it's a very nice result. Oh, thanks. I suppose, I don't know, I don't know whether we can just kind of use this result and then in practical situations, you kind of try to fit the somehow close to this result. Somehow close to this result, you use it as an initial, somehow. Does it make any sense? Yeah, yeah, it does. What actually do here's a guess, an initial guess, yeah, yeah. Here's what you do. You take one of your adjacency, you take one of your slice parametrized glass. So suppose you're, okay, it depends on your, what kind of crazy parametrized family of manifolds you have. Say you have your manifold, you discretize it, and you look at that, a slice. Suppose the slices are roughly the same, but you're not looking at like totally discontinuous. Looking at like totally discontinuous data, right? But your parameterization should be usually in a real practical situation, there's some continuity. So take your, so if I have a one-parameter family of dynamic networks, for example, just take the last one, the most recent one, that's the most important one. Then take that one, do a fairly quick analysis on the static adjacency, look at the eigenvalue distribution, and then there's a cutoff threshold once they're a certain size. That we do understand based on this previous. Do you understand based on this previous analysis on small eigenvalues here? Yeah, then you can cut that off and you'll see the distribution of the eigenvalues. You'll see when they suddenly drop out. And at that point, that's where you take the D. And then that D should work for your rest of your parametrization as well. Okay, the whole for the whole tensor. Does that make sense? So that's how we practically do it for these algorithms. The nice thing about coding. The nice thing about coding things up and actually having to make it work is: A, you have to really make it work, and B, every one of these actual practical questions gets answered, right, in order to get it to work. So that's the beauty of actually proving, putting the proof in the pudding, right? Of coding it up. And I know I'm imposing on Min. Thanks, Min, for your patience. No, no, no, no. It's fine. Go, go ahead, go ahead, go ahead. So, then, one last question, Chris. I mean, I'm really curious to know what would happen. I mean, I'm really curious to know what would happen with the Dirac operator because I think Alan Kohn has a result where I think he can kind of characterize Riemannian metrics using spectral properties of the Dirac operator and a couple of other operators, but that one features in there. But I've always kind of wondered if that's computationally feasible. And you did mention it at the beginning, so I think it's a fair question. I think it's a fair question. Maybe you have thought about it. Well, I've thought about lots about Dirac operators on Riemannian manifolds. That's sort of my area of thinking usually is Ramanian theory. So there, you know, you can think of like the Dirac as sort of like a square root of the Laplacian in a way, but it's not self-adjoint. So you have this, you have the issue, you know, you're now going to complex spectral theory, which is fine for this, by the way. So you can still work with, you can also work with singular values. With you can also work with singular values, either work with complexes or singular values, either way, singular real values, or do other decompositions, right? So, you have think of matrix decomposition, you have KK, which is sort of singular value, you have polar Ruhatitz, you know, all these different Lie algebra, well, Lie group decompositions for matrix groups. And so you can work with the spectrum there to understand it. However, the one problem is that the resistant, like the I. The one problem is that the resistant, like the idea, the intuition of the resistance embedding, which how you're going to use those, those, the spectra for an embedding is going to be different because the resistance embedding that we're using for the C B decomposition is based on the resistance embedding for static graphs, right? The well-known resistance embedding. So that's the electrical conductivity between nodes. And that's the fundamental basis for our embedding, why we. For our embedding, why we do that with the CP decomposition, and we have to figure out the right normalizations and things when we do that embedding, but we're able to do that. For Dirac operator spectra, I don't know if you weren't trying to do a betting, it would be a different interpretation. You have to go through the meaning of it and figure out something that would correspond to maybe electrical resistance, or in that case, actually more like quantum wave spectrum across the manifold. So there'd be a different interpretation. So it'd be a different interpretation, and that would reflect how your embedding for the tensor version would go, and it would be a different thing. So you wouldn't just willingly apply the same embedding algorithm to that spectra. Does that make sense? So I guess you'd need to have some sort of intuition about the wave equation traveling through the manifold in order to kind of use that in some meaningful way. Is that what you're saying? Yeah, well, for the wave operator and the Drak is something somehow like a Schrodinger version of what. A like a shorting derversion of wave, yeah. Um, that's okay, so that's something to think about, but yeah, beyond my yeah, I suppose that it probably will lead to some new algorithms because I have, I think I have not really seen right that much of the direct operators in the application. I mean, for use something new, yeah, like it comes up like in Cyber-Witten theory for four-manifold, smooth four-manifold invariance, and that's that's And that's the context I know info. So I haven't tried it with that, do anything with that, but it's an interesting thought. Thanks. Are there any other questions in the audience? Any questions of the participant comments?