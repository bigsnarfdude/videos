And yeah, thank you. Thank you for the introduction. Yes, we were office mates, but only virtually because you are never in the office. But yeah, so I'm going to talk about DSA a bit more. It might be a bit repetitive, but I think it's worth talking about some of the foundational issues or stuff that I think are issues. And yeah, so the title of my talk is The Unreasonable Flexibility of DSA. So I stole the title from a paper which has the title, The Unreasonable Effectiveness of Mathematics. And I think DSA has some similarity to what the paper is about, where they say why mathematics is so effective. I think DSA is also effective, in my opinion. And I'll try to convince you why I think that way. Why I think that's fair. Okay, so let's see why we all know what DSA is, but so yesterday after our discussions on the open problems and how we can prepare for the next pandemic, I was thinking a bit more. I think there was, well, we somehow saw that the mechanistic models have their place. They are very good. If you want to understand the underlying mechanism, the biology. Mechanism, the biology of the disease. And then we also saw that the somewhat agnostic statistical methodologies, they sometimes outperform when it comes to prediction and forecasting and things like that. So the statistical methods, they also have their place, but they are maybe a bit too data-driven at times. So may not be very satisfactory. So I think what we need is a class of models, mathematical models. Of models, mathematical models, that's doing both in a way. So that's faithful to the underlying biology as well as amenable to statistics or basically the way we collect data. So yeah, there are two important questions whenever we have an ongoing pandemic or an epidemic. What are the underlying mechanisms? I think that speaks to the underlying biology. I think that speaks to the underlying biology. We want to estimate lots of parameters that are basically production number and so on. And we want to do that based on data. So it's also important to know how, what's the collection procedure, what exactly are we observing? And I feel like some of the criticisms of the mechanistic models stem from the fact that they are agnostic of how data are collected. So if we can somehow If you can somehow merge these two things, maybe we'll have a slightly better class of models. And I was wondering then, what's in between these two types of models? So, one is this physics-inspired mechanistic models, and then we have statistics. And somewhere in between is probability theory that's basically that has access to both sides of the world. So, with that, this is basically where my journey with the My journey with the infectious disease epidemiology began. So, I think after I came back from the Marseille workshop, we were hit by the COVID-19 cases in the United States. So, and I was part of this response modeling team. And initially, we had some daily new cases type data, which looks like this. So, this is exactly, I think, in our first meeting when we. I think in our first meeting, when we had, I think that was March 19th or something like that. And this is what we had observed in Ohio at that point. So, some number of cases, very few. But of course, it completely belies the fact that, okay, later on, we're going to see more than 10,000 cases looking at this figure. But in general, when we are dealing with pandemics, I would say that the data is some sort of daily new. As some sort of daily new cases or cumulative cases and things like that. But we are not sure what these daily new cases are. Are the result of some testing, repeat testing mechanism? Or are we getting the true prevalence? Or if not, I mean, symptomatic, asymptomatic, lots of different things that we don't know. So if we get the data, whatever it is, and if we completely ignore how they are collected, then we might have a perfectly fitting model to it, but that may be. Model to it, but that may be, you know, may not still be good enough because we might still be committing some model error. So there are some typical problems that I think is prevalent in infectious disease epidemiology. So on one hand, if you have a mechanistic model, most often deterministic ones, that do not necessarily talk about how you can get a likelihood function. So because it's already Function. So, because it's already a mean field limit, a law of large numbers type. So, you get a deterministic equation. And then, when people try to fit those deterministic equations, they would pull out a likelihood function out of thin air, basically. So, which is not principled. Again, it might be good for fitting a curve, but it's not necessarily faithful to the underlying mechanisms. The other thing is because these are mean field equations, so we don't necessarily know. So, we don't necessarily know the number of susceptible peoples in the population. And sometimes it's just in an ad hoc fashion, it's fixed at the number of the population size of the whole city or the whole state or even the whole country. So, of course, statistics has some solution to this problem. You can treat it as a parameter and do some fitting, but that's often a problem. And then we often ignore identifiability issues. Ignore identifiability issues. So we have a lot of parameters and we try to identify all of them even when they are not identifiable. So maybe sometimes we just have to accept the fact that not all parameters will be identifiable and then find out solutions to what yeah, what we can do. And the last one is the Markov assumption, which is, of course, in the deterministic world, it says the ODE models. That's convenient. It's easy to fit. Convenient, it's easy to fit, and so on, but it's not always realistic, or maybe I would say that it's never realistic. I mean, so so and and the non-Markov versions are actually not that complicated. So maybe it's worth considering the truly non-Markov models when we can. And of course, when I say non-Markov, then again, there are technical details, like you can transform it into a Markov. You can transform it into a Markov model and things like that. But here, by Markov assumption, I mean that exponential solution terms. So you have a constant rate of infection, constant rate of recovery, and so on. So I think these are some of the problems. And of course, there are other problems, but I have highlighted this because I feel like the DSA method is somehow able to address these problems in particular. So, okay, to save you probably. Okay, to save you from repetition, if you are familiar with Python terminology or Python thin text, so of course, from Greg's slides, just import the self-eye construction and the basic DSA methodology. And also from Odo's talk, the renewal equation stuff, because we are also going to talk about the non-Markov versions. So if you have already done that, so we can now start with maybe a bit of DSA that we already talked about. DSA methodology. So, Greg already told you that DSA methodology takes as an input some ecological model, it's a population-level model, a mean field equation, kind of like the SIR equations, either in ODE or PDE form. And then it gives you an agent-based description. And not just an agent-based description or a model, but also a likelihood function. So you can do inference, you can do parameter inference and uncertainty quantification, and so on and so forth. And the mathematical underpinning of this approach is the self-reconstruction, as he talked about. But I thought maybe I can give some actually numerical examples how this would work in practice and what are the advantages of this change of perspective from the population counts to the time. So I thought maybe I can just highlight one more aspect of this whole DSA methodology. So just to begin with, let's have Just to begin with, let's say we have a standard SIR model in the stochastic setting, Markovian. So you still have exponential assumptions here. So beta is the infection rate and gamma is the rate of recovery. And these guys are XS is the number of susceptibles, number of infected, and number of removed individuals. So if you take a large population limit, they converge to this set of ordinary differential equations. Now, Equations. Now, this is well understood and everything. So let us create a Markov chain on the state space S I and R. So now I'm going to take this population level model and create an agent-based model. So I have a Markov chain on the state space S I and R, and I'm going to prescribe the marginal probabilities of this Markov chain. So I'm going to prescribe the law, probability law of this Markov chain, and I do it this way. Of chain, and I do it this way. So, I take what probability that at time t it will be in state s, it's just st divided by one plus two. So, that's the total total mass, and for bi, it will be just i by one plus row, and for r, for some reason it's cutting off there, but it's rt by this. If you care about technical details, if you can you can verify that this prescription is indeed well-defined in the sense that it satisfies the chapman-conbograph requirement. Satisfies the Chapman-Kolmogoro equation. So it's a well-defined Markov chain. Of course, this is the time in homogeneous one. So the rates are time dependent. So you get this Markov chain. What does it do? It gives you the individual trajectories. So if I simulate this Markov chain, it will tell me at what point somebody gets infected and what point somebody gets removed. So you'll get these symbols S, I, and R, and so on. So that's good. So that's good. But the benefit is that if I now want to write a likelihood function for any kind of data. So suppose I know at what time he got infected. I can write down the likelihood function. If I don't know the exact timing, I know that it was an interval. I can still write down the likelihood function. If it's any kind of, if I know that it was a result of a test, and the test has a specific sensitivity and specificity. Sensitivity and specificity, I can still write down a likelihood function. And I don't have to invent or bring in any kind of other probabilistic likelihood functions into the play. So this is completely faithful to the underlying mechanistic model that we had and still gives me a way to write down likelihood functions. So I think that's quite nice. And yeah, so I'll maybe give you an example, but before we do, so let's say, okay, so what about we don't know the record? So, what about we don't know the recovery times, which is often true. That's fine. That's also easy to handle. So, I say, okay, we can lump the states I and R. I don't know recovery times. And I have, I define this as S and this, I say, this is S complement or something like that. So now I construct a Markov chain that has only two states, S and S complement. And I can still write down a Markov chain on this. And this is the margin of probabilities. The marginal probabilities. You can check that this satisfies a Chapman-Call-McGrove equation, the time inhomogeneous one with this intensity matrix. And you can do basically whatever you want to do with this Markov chain. So now I'll actually go back and show you some numerical examples. So please. Examples. So please bear with me. Maybe I will offend almost all of you because there is not enough math. So the mathematician in you will be not satisfied. But maybe, yeah, I'll show you two basic stuff. And then the, you know, but I think it's important to actually see what this really means. So I have this ecological model here, the exact same OD equations that I talked about. You can rewrite this in terms of a single equation, so you can collapse this into a Of a single equation, so you can collapse this into a single equation in terms of the s. So I've done that in terms of s because that's the most important quantity I'm going to write down likelihood function based only on infection times. This is what I'm solving. I take some parameters, let's say 1.2, 0.4, and 0.01. So rho is the initial proportion of infected. So this will have an R0 of 3. It doesn't really matter. I solve these equations, I get these kinds of curves. And of course, Curves and of course the obvious mean field interpretation is that they are the proportions of individuals in different compartments. So if I take a vertical line at that point, know what proportion of people are in what compartment. That's good. I've just also solved the one-dimensional equation, the collapse single equation, and just to show you that, okay, it's exactly true. So because I didn't show you the proof of it. So now. So now the DSA perspective. So we take this population-level model, and then I'm claiming that, okay, I should get an individual description. So what do I do? So that means that I should be able to construct a one-dimensional continuous time, not continuous, continuous univariate distribution. So I just create a univariate distribution that has three parameters now, beta, gamma, and this. I write down the likelihood function and the corresponding sampling. Corresponding sampling scheme, which is basically the probability integral transformation. And then here's your agent-based description, just this probability distribution. So you took a population-level model, you got an individual-level probability distribution for individual trajectories. So that's good. I say, okay, so now what do I do? I get this one. If I run it, then I get whatever I sample from this distribution, I get infection. Sample from this distribution, I get infection times. Okay, so based on these parameters. So I can do this. Okay, so this is one single sample of infection time. Of course, this person is supposed to be embedded in an infinite population, but this is an agent-based model that came out of population-based model. I say, okay, so how about a thousand samples I do? I run this model and I get a thousand samples. These are infection times that you see here, and some of them are. See here, and some of them are infinity, and that's because some people will escape infection. So, for them, infection time is infinity, but others are some numbers, something like that. So, you can do that. If I now create a histogram of this, so that gives me daily new cases. So, you can also now go back from this individual description that you have created to the population-level models. So, you can just sample these numbers and then create a histogram of the. And then create a histogram of this, gives you this. If I just do the normal histogram, so there you see that this histogram, again, the probabilistic interpretation, not just as daily new cases, the probabilistic interpretation, this has a connection to the probability distribution, right? So this already tells you a lot about the parameters of the model. So you can write down likelihood function based on this. So what we did. So, what we did, we changed our perspective from count to time. But it's not just one way, you can go back and forth depending on what kind of data you have and depending on our understanding of how data are generated. So, let's give you some examples. So, suppose I know that somebody got infected at some time point, 6.95. We have very precise measurements. So, you have the app from Eben that you know at that time point you got infected. That time point you got infected and you submitted this data. Okay, so how do what's the likelihood contribution of that data? It's just this is the probability distribution. You take the log of that, that's the log likelihood contribution. You can do that. And then you say, okay, I know it's unrealistic to know the exact time points of infection. In general, they're calculated daily, or you can only get a window or something. You say, okay, fine. So suppose I know that somebody got infected between day three and day six. I think that's a realistic situation. I think that's a realistic situation. So, but we can still calculate the likelihood function because of the foundations in survival analysis. So, you just take the difference of these survival functions at these two time points, and you get a likelihood contribution for this data. And you say, okay, I have data, but for some people, it's maybe intervals, for some people, the precise times. That's fine because of this independence. All of these likelihood contributions are independent. So you can just multiply them. So, okay, so for instance, I have. Them. Say, okay, so for instance, I have a data set that has these random intervals. I don't know the precise times of infection, but only these intervals. Of course, I can just calculate the likelihood contribution here of this interval again because of the cereval function. I say, okay, so maybe knowing even the interval is not very realistic, but I just know that they were tested on a particular day and they tested positive. And they tested positive or negative. Both carry information about the distribution. But let's say somebody tested positive, what's the likely contribution of that? So here we have to make some assumption. So let's say we assume that you can test positive if you got infected in the last 14 days or something like that. So maybe this 14 is not known. So you can treat it as a parameter, but maybe you know that it has to be 14 days or 21 days or something like that. So even for that, then you can calculate the You can calculate the likelihood contribution because you can just take the difference of the survival functions at appropriate time points, and that will give you this. So the point I'm trying to say is that now, depending on how data is collected, so you say even people are repeatedly tested, so then you know these counts that you are getting, maybe it doesn't actually match the true epidemic trajectories that you'll be working with. You'll be working with, you can still write down a likelihood function that's faithful to both the data collection mechanism and the mechanistic model. So now let's go back to, yeah, so I thought I would create some fake testing data or, but I think I didn't do it. So, okay. So here's just, yeah, so some idea about the repeat testing of this. About the repeat testing. So, suppose you test somebody on two time points. So, what that means is that at these two time points, the person could be, of course, then the space is now R2. And depending on where you are on this orphan, you can either be SS, susceptible at both time points. You can be SI or SR. And because we have access to the probability law, Access to the probability law, you can calculate the probability of being in these different state space. So, what means we did a partition of the state space, and let's say we find the number of individuals in these different partitions. So, we say KSS is the number of people who were susceptible at both time points, and KSI is the number of people who were susceptible at the first time point and infected at the second time point. These time points could be arbitrary, but if you could get these. Be arbitrary, but if you could get these numbers, then the likelihood function is then very simple, it's just a multinomial because you have these boxes that are non-overlapping, and you know the probability of being in any of these boxes, and then you can just write yes and put it. So when you say count, you know the count of everybody. No, no, no. It's just if yeah, you decided to monitor 10 people and then you just checked on them at two different time points. It could be even once, actually. It could be even once, actually. If it's once, and if you do, let's say, for instance, for the prison study that we did, so there was a mass testing at a prison. So they tested all the prisoners and found that almost 80% of them were positive. So that's a simplified example of this. So you tested everyone there in the prison and found hardware, in which case this multinomial will collapse into a binomial distribution. But the point is, you don't have to cook up new likelihood functions, it's already embedded. Likelihood functions, it's already embedded in the mechanistic model that you're working with. So that's the idea. I'll actually skip the prison study and maybe talk about something that's more important. So, yeah, I think whenever you come up with a model and you publish papers, I mean, most models are good because they They fit the data well, or something like that. So I would like to highlight more on the foundational issues. So that's why I'm going to talk about this non-Markovian aspect. So Odo talked about the renewal equations, where he said that maybe it's maybe conceptually much more simpler and maybe has a stronger footing if you try to model the force of infection, the cumulative infection pressure. Infection, the cumulative infection pressure. And for that, the age of infection is a, I think, a good basis. And I would argue that not just the age of infection, I mean, times something. So many different covariates are important when you are trying to model the instantaneous rate at which you can get infected and maybe even at which you can get removed or recovered. So a simplistic approach. Approach would be to actually keep track of these covariates. So let's say we're just keeping track of age. And when I'm saying age, it doesn't have to mean the physical age. It could be time since anything. So it could be time since vaccination. That's also age in my opinion. It could be time since onset of infection. That's an age in my book. It could be anything, or it could actually be the physical age. If you are very old, then you may be more susceptible or more infectious. More susceptible or more infectious, or whatever. So, how do we do it? So, let's say for the susceptibles, we have this stochastic process, which is now a measured-valued process. This is a D-dock delta function. It's a point measure with atoms placed on the ages of these different individuals. So, SK is the age of the k susceptible people at time t. So, it's a function of time. We'll assume that it grows linearly with our usual. With our usual notion of time. And this is the corresponding age distribution of the infected individuals. So we keep track of the ages of these populations. And then how do we do the force of infection or the cumulative infection pressure? It's similar to the Selki construction that Greg talked about, but we have to modify it a little bit because now the Because now the force that an infected individual will exert on the susceptibles will depend on the age of infection of the infected individuals. It could also depend on the age of the susceptible individuals, but let's say we have just the age of infection here. So this is the integral of this measured valued process today of n inverse beta. Beta is the hazard function of the contact interval distributions. So contact interval distributions. Distributions. So, contact inter distributions from Evans model. So, you as soon as you got infected, and then from that time, the first time you contact somebody for making a potential infectious contact. That's the hazard function of that time. So, the hazard function of the distribution of that time. That's the beta thing. In the exponential case, this is just a constant, but here it will be a function, and we are free to. And we have free to specify this function. So, maybe for some disease, we have good data, and we can empirically estimate some infectiousness profile or something like that, and we can plug that in here. So, this is actually true to the underlying biology in that sense. So, we get this. We can, of course, do individual level simulations using an extended version of the selfie construction here. But the good thing is that. The good thing is that if we know that the population size is large, then we can do our usual law of large numbers kind of thing, which is a bit more complicated or requires a bit of mathematical sophistication to prove. But at the end of the day, you can show that this measured value process converges to some deterministic equations that are measured valued. If you write down their corresponding densities, then they satisfy some partial differential equations. Or if you write them in integral form, then you can write down. In integral form, then you can write see them as renewable equations. So you get these equations. Once we get that, we can do our usual DSA approach because DSA doesn't really care about whether it's an ODE or PDE. As soon as you can describe these probabilities, then you're good. So we, Greg showed this slide again. So the point that we are trying to make here is that if you have the standard ODE model where you're The E model, where you're, in a sense, assuming that the underlying distributions are exponential for your contact interval, which is the infection rate then, and recovery distribution, then you can do really bad. So here, we looked at data only for the first 20 days of the epidemic for put and mouth disease in the UK. And based on that, you are asked to predict. So, on the left side, we used Weibool and Gamma. Here are the distributions. And gamma, here are the distributions, the probability densities of these two corresponding distributions that turned out to be the best fitting one. And here you see that, okay, it generates a lot of trajectory. There is a lot of uncertainty, but that's because we're using only the first 20 days, so that's really nothing. But if you look at the overall trend of this thing, this is still doing a much better job than the corresponding Markov version. Where you have basically no clue where it's going. It's just claiming that it's taking off. And it may not be always the case that the non-Markov model will win because you may not have enough data to identify the parameters. But that's something that we had to try. Yes. Yeah, because this is real data. And only this part is taken so when you observe the first 20 days, the continuing difference will be dot confirmed. So the dots, this is the real data. So after that, so we are supposed to compare, let's say, this dot and maybe the blue curve, which is the average of all the different projections that are that we are forecasting, predicting. So here in the non-mark of the case, you see that at least. Of the case, you see that at least the average or if you want the median, I'm not sure the average of the median. But you see that this is still closer to what really happened afterwards, which the model had no clue about at that point. But here it's uh it's worse than how it line so it's still yeah, it's it's still going expensive. Yeah, so that's uh yes Yes, I mean they both do well, but here I think both models are subject to a situation where they're not supposed to perform well because the first 20 days they they basically know nothing about it. If you if you already observed If you already observe close to the deep, then I think both models could do fairly well. I think we're more interested in the situation where you are already not deep into the pandemic, the start of the pandemic, or when you are when you still can do something about these predictions. I mean, yeah. Yeah. So, I mean, you can do some So, I mean, you can do some analysis with that, but not in this picture at least. Yeah. Okay. So, that's for the foot and mouth. We did the same thing for COVID-19 in India. We only looked at one wave and we found out that the best fitting distributions were Dalma and Rebule or something like that. And then, of course, but this is not in the future, but this was already in the past. So, you can see how they fit in terms of. See how they fit in terms of the number of cases and the number of removals and so on. So they seem to be doing okay, but I think if it's already in the past, maybe it's not as meaningful as in the other case. Okay, so that's the non-Markov thing. I think that's one point that I really wanted to make. That sometimes it's actually not so difficult to go for the more general model. So we don't have to necessarily get married to. To necessarily get married to exponential and exponential pair. So that's the bottom line. Okay, now the next thing, which again might be offensive, but there are situations where the mass action models are okay when your population is, I don't know, there's a high level of contact, and maybe in some cases, and in some cases, we can even show that the That the corresponding, at least, the network model and the mass action models are equivalent in some sense. For instance, if you have a Markovian SIR model on configuration model type random graphs, and if you assume that the graph is sufficiently large, then this is actually equivalent to a version of a mass action model, but you have to reparameterize. So, you know, in that sense, but in general, these two have different flavors, and the network model might be more. The network model might be more realistic than the mass action models. So, yeah, of course, but again, as soon as you are moving towards more realistic models that will introduce new parameters, and then you might face issues of identifiability, which we have to acknowledge. So, not all network features will be identifiable, but we have to make do with what we can do in those situations. So, I just wanted to. I just wanted to highlight why I said that they're equivalent. So, if you have an SIR type epidemic, actually, SIR on the configuration model random graph. So, what's exactly a configuration model random graph? So, it's this. Suppose you have a graph and you are given a degree distributions, let's say either a distribution, so D1, D2, up to Dn. What you do is you assign those many. What you do is you assign those many half-edges to the vertices. Let's say here. So this is one, two, five, four, and let's say this is five again, something like that. Maybe one in one case. So you start with these many half edges and then you uniformly match them together. That's called the configuration model. It's an extension of the Erdos-Reni random graph where you pick a pair of vertices and say, okay, with probability P, there is an H between these pairs. P, there is an H between these square. So that's a very simple case. When the graph size is large, the Erdostrainy basically forces, or and our only degree distribution that you can get for large Erdostrani graphs is a Poisson distribution because Poisson is the limit of binomial. Configuration model is just an extension of, so slightly more realistic than Erdoshen. So, okay, so if you have this type of random graph, and if the dignity distribution is. And if the degree distribution is Poisson type, then you can again collapse this big system into a single equation for the susceptibles, which has some parameters. μ is supposed to be the mean degree, then we have beta tilde, gamma is the recovery rate, so beta plus gamma, gamma tilde, so that's a new parameter. And rho is theta tilde times rho. The rho was the initial portion of infection individuals, and you get this equation. There's one more parameter, kappa, which is the which is the density. Which is the density of edges. So if you have, let's say, a Poisson distribution, then kappa is one. If you have a binomial distribution, then kappa is, in which case, just n minus one by n, regardless of p. So that tells you something. So that tells you that if I want to, let's say, estimate the average degree of the network. Of the network based on DSA, we cannot because this equation doesn't capture any information about P. So it only captures the information about n, n minus one by n through this function kappa. So that's something that we have to know. Okay, I'll quickly just highlight some work that now tries to include both non-Markovian. Include both non-Markovian dynamics and the network structure into this model. So we have to specify a degree distribution, a probability distribution for the degrees, which we can do using, let's say, a probability generating function here. And then now we are moving from Markovian dynamics to non-Markovian dynamics. So we need to specify some hazard functions or some notion of hazard functions. Of course, if the rate instantaneous. If the instantaneous rate of infection depends on both the age of the infectious age of the infected individual as well as the age of susceptible individuals and maybe some other covariates, then this is not a hazard function in the classical sense of hazard functions in statistics, but this is just some function that describes the instantaneous rate of the contact interval distributions. We do the same for the infectious period. If you do that, and then we construct the graph as follows. We construct the graph as follows. So we start with vertices, specify their degrees, and then let's say, yeah, so these are the specified. We pick two edges at random, we pair them, and then that gives us the graph, basically, the configuration model. And then we play the epidemics on F. So of course, if there is a red viodex, which is infected, can infect one of its neighbors, it is not infected by them. So if it's blue, then it turns red. So, if it's blue, then it turns red if you're connected to a susceptible. So, if we do that, then we can do our law of larger numbers, but we have to specify what do we bookkeep. So, if we keep track of the degree information of the vertices and the compartment they're in, S, I, and R, and the corresponding covariates, let's say the age of infection or the age of the susceptible individuals or The age of the susceptible individuals or some other cofariats. If we do that, then this process is then a Markov process, even though the underlying mechanisms are not driven by exponential, exponential distributions. So this, yeah, it's a Markov process on a more abstract safe space. We can do law of large numbers. We can show that if we scale the system appropriately, then this converges to a deterministic system. A deterministic system of equations, measure-valued, and if they admit densities, we can write down PDEs, I think. And yeah. And once we do that, then we can, of course, do DSA once we get to that. Because again, DSA doesn't really care about how you got the equations. Once you've got the population level equations, then it can interpret it and give you an agent-based description, and then you can do your function. Description and then you can do your so I'll stop here. Yeah, we can, of course, include back.