And Jesse, for organizing this conference. It's been great so far. I'm very sorry to not have the chance to see all of you in Oaxaca. It's a beautiful place. So I'm going to be talking about genetic clustering, which is much less sophisticated than the phyloid dynamics method that I have been presenting. Method that have been presented in this section so far. And I'm really excited to hear about Vladimir's integration methods. That's really cool. So I'll have to read up more about that. But it does have, genetic clustering does have a role to play, particularly in public health applications of genomic epidemiology. So I'm going to start by just talking very generally about clusters. So it's an important tool for public health, particularly with outbreak. Health, particularly with outbreak detection. And normally we're dealing with clusters in time and space, right? So we're looking at a number of cases that occur at an elevated rate within a distinct time interval and that are located spatially, like proximate to one another. And so to illustrate this, I'm reusing this old chestnut of epidemiology. Chestnut of epidemiology, Jon Snow's map of color cases on Broad Street, where the number of cases are indicated by these dash marks on different areas. And the pump that famously John Zone took the handle off of to stop the outbreak is located somewhere around here. And so clustering has gone on to play an important role in epidemiology and public health. It's more challenging to use in the context of. In the context of infections that viral pathogens that established a persistent chronic infection with a long asymptomatic period. And that's because there's that delay between sampling the infection and the actual transmission event. Okay, so genetic clustering, as I alluded to before, is a simplistic method. Simplistic method, just barely qualifies as a phylodynamic method if you're using a phylogenetic framework. It is intuitive and it's a very popular method in genomic epidemiology. So there's lots of examples of it being used for whole genome sequencing, for mycobacterium tuberculosis. It's been genetic clustering, has been used in the US by the CDC for a lot of different states. For a lot of different states, for molecular surveillance of HIV clusters in their detection and response program. And there are similar initiatives in other countries around the world. The basic intuition to genetic clustering is that infections that remain genetically similar when you sample them are assumed to have some epidemiological relationship, that implies some sort of epidemiological linkage between the two. Epidemiological linkage between the two, not necessarily direct transmission, but that they're related by some recent transmission events. And so we're using genetic variation, mapping infections to some genetic space as a proxy for time and space in the context of clustering. The nice thing with genetic clustering is that it scales really well with sample size. And so when you're dealing with data sets that number in tens of thousands or even hundreds of thousands of infections, clustering remains a feasible. Is remains a feasible, computationally feasible tool, whereas phylodynamic methods are not necessarily going to get you there. Real quickly reviewing, there are a lot of different methods used for genetic clustering. I'm very crudely partitioning them into two categories, although there are others. So, pairwise clustering involves some sort of genetic distance comparison between Comparison between the sequences of two infections, and then we apply some threshold value to the resulting distance matrix. Okay, so if you can imagine some symmetric genetic distance matrix relating all your infections, those are all continuous valued, and you apply some cutoff value, and then you end up with some binary matrix, and that gives you your adjacency graph. And then we take that graph and look for connected components in the graph, and those become our clusters. For subtree clusters, Okay, for subtree clustering, we're building a phylogeny relating the infections, and then we want to apply one or more criteria to potential subtrees or subset trees. A subset tree would be the case where the group does not extend all the way to the tips of the subtree. So, in either case, we're applying some criteria in order to extract clusters from that phylogenetic tree. From that phylogenetic tree. So, this is a JavaScript animation that I threw together to illustrate pairwise clustering. Each one of these nodes represents an infection. The blue and orange don't mean anything really. I just like the colors. And as I move the slider, I'm changing the clustering threshold. So, at a very low threshold, every infection is its own cluster. And as I slide it over this way, everything gets placed in. Everything gets placed in one big mega cluster, okay, because my threshold is extremely lax. Okay, incidentally, if you want to steal this for like a teaching lecture or something, the source code is all online. I'd be happy to share the URL with you. It's also in the front of this talk. Okay, so one of the big problems with clustering is that there's a whole bunch of different methods, and they're largely ad hoc. Largely ad hoc. And the criteria that we use to define clusters, like that sliding threshold, there's not really a statistical framework for determining what threshold we should use. And in the past, that has largely been driven by historical precedents that we use 15% because that other paper used 15%. And so that's what we're going to use. And this has been a really serious. Been a really serious problem for a long time. I remember attending a panel at Croy, which is like a big North American HIV conference, right? And a whole bunch of public health people and doctors were really taking osophylogenetics people to task and think, well, what threshold are we supposed to be using? And they're like, there's all these different ones in the literature. We don't know. It's been an open problem for a while. So in this study, we're proposing that the optimal clustering criterion. Optimal clustering criterion is what confers the greatest accuracy in predicting the distribution of the next cases. And so, that we're trying to develop some sort of statistical framework that is going to allow us to enforce that criterion in tuning our clustering methods. And one of the things that we realize is that this is analogous to the modifiable aerial unit problem. And that's a problem articulated in spatial statistics and epidemiology. Spatial statistics and epidemiology, where there are any number of ways of partitioning up your geographic area. And often these can correspond to neighborhoods or jurisdictions of governance, for example. And the modifiable area unit problem or the MOP, which I should have put here, is an expression of the problem that The problem that the statistical results of a statistical test that you apply to this spatial distribution of cases depends on how you're carving up this area into units of observation. So here we have the same spatial distribution of points, and we have different areas. And the extent of what you would consider a hotspot based on some threshold number of cases in an area is highly contingent on how we partition up this area. We partition up this area. Okay, so we're then shoehorning this concept of the MOP to genetic space, where the clustering of cases, what we're going to refer to as known cases, is some partition applied to the known case database. And there are different ways of defining that partition. Okay, because genetic space is kind of weird. Genetic space is kind of weird. There are immediately a lot of statistical methods in that literature that we couldn't apply to this case, such as the odd-aggressive methods. However, by hitting the literature some more, we did find a paper from Tomoki Nakaya who formulated another approach to dealing with the modifiable area of unit problem or the MOP, where it's generally based on fitting a Generally, based on fitting a Poisson regression model, where the count outcome is the number of cases that occur in a given area for that particular partitioning of geographic space into areas. And so what we're going to do is we're going to separate our database into known and new cases, where new is defined by the cases that were sampled in the most recent time interval. And our time intervals are going to be years. Okay, so the number of new cases. So, the number of new cases that are adjacent to clusters of known cases. Okay, and I'm going to refer back to this here. So, here's one cluster, let's say this blue one here, and the count outcome is one new case that's adjacent to that cluster. That means that we're not necessarily going to have all new cases accounted for in our model. There are some new cases that are not going to be adjacent to any of our defined clusters. But we're going to take a Poisson regression model. But we're going to take a Poisson regression model approach to analyzing these outcomes. Okay, and what you find immediately is that if we apply some model selection method, we get a trivial solution that the most effective model prediction-wise is what collapses all the known cases into one big mega cluster. So, if you have this like massive cluster, it's really effective at predicting the occurrence of Occurrence of new cases because new cases have nowhere to go. If they're going to cluster with anything, they're going to cluster with that one big mega cluster. So that's not terribly interesting. And so we needed to have a model comparison framework instead. And so what we're doing is that we first propose a null model where the probability that a new case is adjacent to a cluster of known cases is only dependent on the size of that known case cluster. On the size of that known case cluster. That's the same as assuming that every known case has the same probability of being adjacent to a new case. Okay, and so that's that's our null model. And then we can express some alternate model. And the great thing with generalized linear models is that we can plug in any number of covariates that we happen to have. So I'm leaving this sort of unspecified, but there's like some linear model relationship of cluster level and individual. Cluster level and individual level covariates that are going to re-weight the probability for each individual, for each known case, re-weight the probability that they're adjacent to a new case. And so that becomes our alternate model. So before I go into the use of that model, I'm going to summarize a little bit about the data. So we drew from three study populations that had published HIV one pole sequence. One pole sequence data. These populations are based in Seattle, northern Alberta, and Tennessee. And this is just summarizing the distribution of those infections based on their sample collection dates across the number of years. So they're like fairly uniform sampling numbers over time. And then we can calculate the genetic distance. And then we can calculate the genetic distance for relating these infections, and we get this truncated distribution. So I'm throwing out all the distances that are above 0.05. And we can see that, like, on a global scope, they're fairly similar. Although if we zoom in on this lower tail, we can see that the northern Alberta cases tend to have more low distances more often. Okay, and so our problem then is where are we going to Our problem then is where are we going to place our cutoff for defining clusters? Okay, so we need to have some predictor variable to plug into our alternate model. And so what we decided to do was to take the difference in sample collection dates as a prediction. So we would assume that our hypothesis is that new cases are more likely to be adjacent to recently sampled known cases. All right, in order to our In order to articulate that, we're fitting a binomial regression to the known case database by taking basically extracting a bipartite graph of all the known cases in year I and the ones in year J, and looking at all the edges at the edges between them, and then fitting a model to that edge density. Okay, so there's the number of edges in that bipartite graph. We're reducing those edges down to until. Edges down to until there's just a maximum of one edge, incoming edge for the later date, and then we're normalizing that by the number of possible bipartite graphs we can pull out of the database. And so what we get when we fit that binomial regression model is that the edge density, the probability of an edge, decays with increasing difference in sample collection dates, right? Dates, right? So if the samples are separated by one year, then the probability is the greatest and that decays over time. And we got fairly similar results from the three study populations. For the Tennessee study population, we also had access to the dates of HIV diagnosis, not just a sample collection date, because the sequences tend to be sampled sometime. Sometime after an individual's infection is diagnosed, particularly because that sequence is obtained when they start treatment. And so that's their baseline sample. So that could be any number of years after diagnosis. And that's going to fuzz things up quite a bit in terms of trying to characterize the changes in transmission rates. Okay, so we fit a similar binomial regression model to the distribution of diagnosis dates. Distribution of diagnosis dates. There's variation in points here because we found that we had to adjust for variation in the diagnosis dates over time. Okay, and I'd be happy to elaborate about that afterwards. So now that we have this predictor variable, we have an alternate model. And so we fit the two plus R regression models to the known case clusters for a given cutoff. And so we're Clusters for a given cutoff. And so we're applying all these different distance cutoffs, breaking our known case database up into clusters, looking at how new cases map onto those clusters, and then fitting these different Poisson regression models to them. And then we're just taking the difference in the AICs. Okay, and so what we find is that when we have a very relaxed cutoff, everything is in one big mega cluster. The models aren't all that different. Similarly, when we have a very strict cutoff, we've blown up the database. So every individual is it. Database, so every individual is its own cluster. Um, everything both models are crappy, so they're similar into their AIC. And then we're looking for the greatest difference in the AIC between the models as we vary our distance cutoff. We find that that minimum exists and is located in similar spots for Seattle and for Tennessee, but not for Northern Alberta. Okay, and so this is. Okay, and so this is what we're proposing: is the I optimized distance threshold based on the distribution of new cases, our ability to predict the distribution of new cases on the known case database. This is also results that we obtained for sample collection dates in Tennessee and diagnosis dates in Tennessee. And we get similar outcomes, even though we're using different dates. Just to look at the data. Um, just to like look at the data, if we pick out clusters based on these optimized thresholds, we find that an example of a cluster where most of the cases, the known cases were sampled recently. Each point represents an infection. The size of the point corresponds to how recent the case is. And so, and the dark points are the new cases. Okay, so we have three new cases appearing in this cluster here. Here we have a cluster of a similar size, they're a lot older. Size, there are a lot older infections, and there's only one new case there. Of course, I'm like picking, and I'm, you know, I'm fishing a bit, okay? But you know, you can look and see that we can find similar examples for the Seattle and Northern Alberta database. All right, so in conclusion, this is a statistical framework for optimizing genetic clusters for public health applications. I could like, you know, do the old thing and say this is the first statistical framework, but I This is the first statistical framework, but I'm not going to do that. I mean, this is what it is. Hopefully, it doesn't suck. We do find that the optima, the robust subsampling of databases, they are fairly consistent over time with some caveats like if you deplete your database too much, then it kind of goes to heck. We do find evidence, preliminary evidence, of a lot of variation in the optimal thresholds from one country to another. Optimal thresholds from one country to another, from one study population to another, as we should expect. And these methods are now being extended by my student, Connor, to deal with substep tree clustering, because this is all pairwise clustering. So I want to acknowledge that all the heavy lifting was done by Connor Chato, my recently graduated master's student, who I think is on this call. And the work was published recently in Virus Evolution. Published recently in virus evolution. Okay, so I'll stop there. Thanks for a great talk. So I'll just quickly read a question from Leonid earlier, which maybe you want to add to. Would the threshold not need to be calibrated with respect to what you consider recent transmission? In other words, if your definition of recency changes, your threshold should change accordingly. Should change accordingly. I, yeah, that's a good question. So the threshold themselves aren't changing. It's like the way that we're weighting the clusters in terms of the expected number of cases that are going to join. Number of cases that are going to join that cluster is what is going to vary with how we weight them with respect to diagnosis dates versus sample collection dates. So it is possible, yeah, that if the sample collection, if the infections were sampled like way after diagnosis and they're like totally not informative about the actual recency, then yeah, we could expect the optima to be different, but I think largely based on sampling But I think largely based on sample variation or like lack of signal, I can't right now think of how they would systematically be different.