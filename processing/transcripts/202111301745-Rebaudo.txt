Making this very interesting workshop possible. So, today I'm going to talk about an ongoing joint work with Peter and I will start giving some model idea and motivation in an intuitive way before trying to sorry. Sorry. Okay, before trying to be more formal later. So what we want to do is model-based clustering align on a graph. So the idea is that we want to infer the cluster and align them on a graph, exploiting the information of the observation in between the main cluster. In between the main cluster. So the main idea that is that for us, clusters are homogeneous clouds of point in R D that has an elliptical shape. And jointly, we want to infer a graph where the cluster are the main vertices, and two vertices are connected by an edge if there are observations spread around the line connecting the vertex. The vertex. So, why we want to do that? The motivating data set are single-cell RNA data sets. And the idea here is that the clusters represent homogeneous group of cells with similar genetic expressions, so main phases, and the cells in between represent transition in between the main phases. In this way, the model wants to mimic some biological process. Biological process, like cell differentiation, when we start from stem cells and the cell transition in between phases till fully developed stage, or also more even more complicated process like the transition in between phases of the cells in a microenvironment of a tumor. So, the data that we have before pre-processing. Have before pre-processing are data from single-cell RNA sequence experiments that allow us to measure genetic expression at the single level. So we can represent it in a matrix where the value in this matrix are count that tell us how strongly a gene, so a column, a gene P is expressed in a cell I. And the nature of this data is such that Of this data is such that we have a lot of zero, actually, around 90%. So, extract meaningful signals is a fundamental question in bioinformatics for such kind of data. And we follow a recent pipeline where the first step is to discard low-quality cells and low-express genes. And after that, we want to reduce the dimensionality of the gene. Dimensionality of the gene via some kind of dimensionality reduction technique. What we use is essentially a zero-inflated negative binomial factor model described in a recent nature paper that take into account the data structure of single cells, so that are count data, we have zero inflation and over dispersion. Zero inflation and over dispersion. So the data that we actually have are a matrix. Now the matrix is denoted by Y and the row are the observation of the cells and the columns represent now some biomarker describing the genetic expression of the cell. So the scientific goal includes to identify group of cells with similar genetic expression. So for us cluster, the main cluster, the main phases. The main cluster, the main phases, but also to connect the main phases, exploiting information of the cell transitioning between the main phases, and thus align the cluster on a graph. And also from a scientific perspective, this should be used to identify the genes that are responsible for such transitioning between different phases. So, for instance, these are what the data can look like, real data after. Can look like real data after the pre-processing. Here we have a two-dimensional representation of a single-cell RNA sequence experiment on a horizontal basal cell from adult mouse olfatory epithelium. So I will start with some preliminary that also connect to what Igor talked about yesterday. So one way to perform probabilistic clustering is via mixture mode. Is the mixture model. And in particular, Gibbs-type process mixture model with Gaussian kernel were established by xenon parametric and parametric model to infer homogeneous group of observations via probabilistic clustering. Also, to connect to what was discussed this morning, of course, we have to be careful in the sense that on real data sometimes we can Sometimes we can overestimate the number of components, but here the goal is not to estimate the number of components. And also, in my experience, is that for finite sample, what we have to observe is that we don't, we have still parsimonious representation with this kind of model if we choose a meaningful loss function to minimize, to obtain a point estimate, and also to tune the upper parameter in the Gaussian kernel. In the Gaussian kernel, according to also the sample size we have. So, anyway, this kind of model can be represented, described in a hierarchical way, where the observation yi given kernel parameter theta i that here represents the pairs of the mean vector and the variance matrix came from a Gaussian kernel. And then on the individual parameter theta i. Parameter tai, we assume an exchangeable distribution that can be seen as such that the parameter theta are conditionally id from a discrete distribution, and we assume a Gibbs type process. So, Gibbs type process are a fundamental tractable subclass of species sampling process, proper species sampling process that include. Process that includes the most of the discrete prior studied in the literature. That is, we have non-parametric prior like the Diricher process, the Pit-Mayor process, and also the normalized generalized gamma process. But we can have also a finite number of atoms, so assuming a symmetric Dirichlet distribution on the weight, and also put a prior on the number of unit atoms. Anyway, since the Anyway, since the random probability G is discrete, we will have a tie between the parameter associated to the observation with positive probability. And this tie induce a random partition of the label of the observation. And we denote such random partition by Ïˆ n and such that the two observations are clustered together if and only if they share the random parameter. random parameter. So all this model can on essentially on a changeable random partition can be characterized in three different ways as was explained yesterday. And so the first one is via the discrete random probability. The second one that I really like if the goal is clustering because allow us to directly understand the assumption on the random partition is Random partition is via the distribution of the changeable random partition that can be characterized via the changeable partition probability function, the EPPF. So we say that a random partition psi n is a changeable if the law is invariant to the permutation of the label and we denote by C1CK the blocks of the partition. Of the partition. We can thus equivalently rewrite the mixture model as a random partition model that tells the assumption directly on the level of the partition, where we say that the random partition psi n can be sampled from an APPF of a Gibbsi prior, and the unique cluster value are IID sampled from a discrete Sampled from a discrete anatomic random probability P0, and then the observations that are clustered together share the same random parameter. If we restrict ourselves to the class of deep style prior, we have a nice product form of the PPF that is what is fundamental to obtain tractable analytical and computational results. So the last representation is in terms of polyogen scheme. Scheme and so we can characterize the random distribution also via the clustering assignment indicator ZI and under Gibbs type prior follow a generalized Cheney-Rastron process in the sense that the first observation has an cluster indicator Z1 equal to one and then the cluster indicator of observation Z n plus one given the Plus one given the previous can be equal to one of the previous observer value or a new value with an analytical expression that can be computed by the definition condition probability from the PPF and thanks to the product form of Gibbs type prior boils down to a simple analytical form that includes, for instance, the well-known Chinese restaurant process of the Dirich process. So anyway, speech Speeches sampling process mixture and also Gibbs type prior mixture entail independent cluster-specific values, not allowing us to infer the relationship between cluster. So we want to introduce a graph aligned random partition model. Why? Because we want to distinguish between the observation belonging to the main cluster, so the main phases, and the observation transitioning between two main phases. We want to Main phases. We want to also discard the observation that are transitioning between two main phases when estimating the cluster-specific value. So the cluster-specific gene expression of a main phases. And also, we want to exploit the information of the cell that I'm transitioning between two phases to inform about the biological relationship between two phases. So from a statistical perspective, we want to align the cluster on a graph. So, before defining formally our model, I have to set some notation. We denote by VI the vertex assignment indicator in the sense that an observation i is assigned to the vertex. So, a main phases, if and only if vi equal to one. And then we have the usual, if you want, cluster assignment indicator, the die, and we say. The die, and we say that the die is equal to k if and only if the observation i is assigned to a vertex cluster k. Why the die is equal with a note is equal to k prime if and only if the observation i is assigned to an edge cluster connecting to vertex k and k prime. And we denote by kv the number of vertex and by k e the number of edge and we And with n k and n k k prime, the frequency of the cluster of a vertex cluster or of an edge cluster, respectively. So the likelihood, now we can define our model. So at the level of the likelihood, we say that observation yi are conditionally independent from a Gaussian component with parameters that are shared within the same main phases, so the same cluster. Phases, so the same class vertex cluster, or transition phases, so the edge cluster. And to learn the vertex cluster parameter, we put a conjugate normal inverse swish prior. So how we can learn the edge component, and the idea here is to define the mean and the variance of the Gaussian edge. Of the Gaussian edge component as a function essentially of the location on the two vertex we are connecting. How? Such that the mean of a vertex component is actually centered between the two locations we are connecting. And with some simple linear algebra, we can also define the variance such that the control plot of the density of the edge component is essentially analytic. Component is essentially an ellipse or an ellipsoid in Rd that is stretch along the line connecting the two vertex. And we have two hyperparameters, R0 and R1, that tell us the standard deviation, if you want, of the Gaussian component projected on the line connecting the two vertex. Actually, this is R0 times the distance between the two locations, and R1. And R1 tells us the standard deviation on the other orthogonal component. So the idea is that here also allowing the standard deviation on the line connecting the two vertices to be proportional on the distance, we put less likelihood on edge that are far apart. Right. And to just And to just look understand maybe what this implies at the level of the observation, we can sample from the model. And here we can see that, for instance, the one color are observation sampled from an edge component. Okay, now we can focus on defining the graph align partition structure essentially. Partition structure essentially. And to do that, we start defining it at the level of the graph aligned random partition that essentially, so we have to specify the probability mass for the pairs of the vertex assignment and the cluster assignment, right? And we can do it via this composition essentially of PPF of the marginal like radiation multinomial that can be. Pinomial that can be somehow interpreted in a hierarchical way. So we can see that approximately, and I will be more clear soon, the vertex indicator assignment VI came from a Bernoulli with probability PV that control the proportion of cells assigned to the vertex. And we denote by NV the number of observations. Nv, the number of observations assigned to vertex. Given that, we can now cluster the observation assigned to vertex in different vertex via a partition characterized by the APPF of a Gibbs type prior. Here we just divide by Kv because the Z I are order, sorry, and the partition is not. And at the last step, given the previous, we Given the previous, we can sample now the assignment of the observation belonging to edges, to the different edges, via the marginal likelihood of a Dirichlet multinomial. Why here have some approximate, I've written approximately? Because actually, we have some constraint in the support. So, for instance, we don't want We don't want that. Sorry, yeah, which the two. So that an edge connect no vertex. So for instance, we cannot have in the support paralization such that all the cells are assigned to edges. So Nv is equal to zero, right? This constraint, what is imply? This constraint implies that we preserve the homogeneity assumption of H. The homogeneity assumption of exchangeability at the level of the sample, right? In the sense that if the joint law of the sample and also the graphal line random partition, so the pairs VI and ZI, is invariant to permutation of the label. So our probabilistic assessment and our learning is not affected the order in which we look at the observation. However, we lose infinity changeability. So we cannot see. So we cannot see our sample as a projection of an infinite changeable stochastic process that, of course, is not observable. But and so this doesn't bother us too much. What maybe can be an issue that I think it's worth to discuss is that infinite changeability is equivalent to finite changeability plus Kromogoro consistency. And here it means that we lose Kromogoro consistency. Here, it means that we lose Komogoro consistency. So, that if we look at the law of our sample, it cannot be seen as a marginalization of a law of a bigger sample. So, to connect also to the very interesting talk of Chris Holmes, here also we want to weaken the assumption of infinity changeability because I also believe that in a lot of Of an application like ours, the symmetry assumption of the changeability plus the infinite data assumption put a lot, a lot of restriction on the prior on the final sample. So for instance, if you think if we have two observations, we can judge them exchangeable with a negative correlation, right? Correlation, right? If we see them as a projection of an infinite exchange of stochastic process, they cannot have negative correlation. And this is just one example. And the recent is, especially for random partition model, more and more models that are finitely changeable, but not infinitely changeable were introduced. However, we still have to put some conditions such that Komogodo consists, even if does not hold, must hold approximately because we don't want a classy model. Don't want a crazy model that is, I don't know, a random Bernoulli for a sample size odd and a random Gaussian for sample size even. We want some kind of coherence at least in the limit or as the sample size increase. Why our model is not consistent? The intuition is that if we have one observation, it must belong to a vertex, right? Because there are no previous vertex now. There are no previous vertex now. So we have two observations, they must belong to two vertex. While if we have three observations, one of them can be an edge. That's why we lose Kolmogorov consistency and intuitive level. Of course, when the sample size is large, this should be less and less an issue. This intuition will try to be more formal. And so we can still have a three-characterization of our graph-aligned random partition model. Random partition model, one is our definition, so the probability mass of the graph aligned random partition structure, right? The second is at the level of the Polygon scheme, as we will see. And the last one can be seen somehow as a composition of the species sampling process. Almost, yeah. So, moreover, as said, we want to show that Komogolo consistency at infinite changeability. At the infinite changeability or the proximately, and asymptotically, we should be nice to study the definitive measure of the asymptotic infinitely changeable model. So one characterization is such that we can write our model, PR, for the graph-aligned random partition, so of the pairs VI and ZI, as a truncated version of a more tractable unconstrained model. Constraint model, PR tilde. And where the constraint is just the event, if we assign a cell to an edge, we must have two vertex. And so we can sample and also interpret, this is how I actually understand the model, from the unconstrained PR tilde in a simple hierarchical way, which is the following. Hierarchical way that is the following. First, sample the assignment of the observation to a vertex or an edge via an AD sample of the vertex assignment from a Bernoulli with the probability PV, then sample the clustering of the observation belong to the vertex given the previous. And this is controlled by the generalized Chinese Rastafarian process of Gibbs type. And finally, given the previous, sample. Given the previous sample, the clustering of observations belonging to edges and not just the clustering actually, but in which head. So, here the order matter via the polyagon scheme of a symmetric Dirichlet multinomial distribution, where the upper parameter of the Dirich symmetric Dirichlet is beta divided by the dimension of the C implex. This allows us to introduce a sparse graph. Graph. And just to connect again with why we don't have infinite exchangeability, is for two reasons. One is the constraint, clearly, that is at the levelization, but the second is also by the sequentiality, because here we condition on the, if you want, on the pivotalization of this part. And so we are not defining it at the level of definitive measure of the underlying. Measure of the underlying random probability. Okay, but why this is a good way to understand the two samples from the model? Because actually, we can see that under the constraint model, we can compute quite easily an analytical expression of the probability of the constraint, right? Relying on the result on Gibbs type prior and composition, essentially. And such probability of the constraint. Probability of the constraint, of course, the expression depends on the specific Gibbs type prior we choose, but goes to one quite fast as the sample size diverge for finite and infinite-dimensional Gibbs type prior. And the intuition is that the rate in which it goes is just control in the same sense as. sense as what is the probability of selling at least two clusters under this Gibbs type prior and for mixture of finite mixture it does not go to one but actually goes essentially to one minus the probability under the prior of a serving just one cluster that can be computed and controlled by some hyperparameter here the special case of a NUD process and finally And finally, we can also rewrite the law of the parameter under the unconstrained model as a sequential sampling from discrete random probability. This allows us to connect to Bayesian parametric model and also to derive conditional sampler. So given the vertex assignment VI, we can sample the parameter of the observation in the main phases. So the vertex parameter simply Vertex parameter simply from a Gibbs IID from a Gibbs type process. And given this realization, we can sample the parameter of the servation transitioning between two phases. So the edges, also conditionally IID from a discrete random probability whose weight are a symmetric sparse, if you want, Dirichlet distribution, and whose atoms now are not IID. Are not ID, so it's not a speech-samping process, but are the function of the atoms of the vertex they're connecting, as I described before. Okay, then I said our model is finitely changeable, but not infinitely changeable. However, we want some form of coherency, right? Even if we we can infinity changeability, we must have some other property. We must have some other property. So, one way we try to do it is to show that Kolmogorov consists in infinity changeability all the approximately. So, we have a preliminary result here for the case in which the Gibbs type prior is a Gibbs type prior with random weight that are asymmetric Dirichlet plus, let's say, if in such case there exists a random sample size bar n such that for any sample size greater. Such that for any sample size greater than such bar n, our proposal is Kolmogoro consistent and infinitely changeable. And we can find also the definitive measure of the changeable sequence of the cluster parameter. And what it sees intuitively is also a composition, a mixture of the Gibbs type. Of the Gibbs type prior controlling the clustering of the vertex and a discrete random probability controlling the clustering and the order clustering because now they're matter of the edges and the intuition that we arrive at such level. Why? Because after a sample size, we occupy all the possible vertex. So for the infinite Infinite so for the Tulinon parametric case of deep state prior, we don't have sorry already a theorem, but our idea is that we don't have a random sample size such that after that we actually are infinitely changeable from a gold consistence, but still the limit is infinitely changeable and can be characterized. Can be characterized by the definitive measure, and the intuition is that such distribution will goes to the stick breaking one. And so that should be the definitive measure of asymptotic model, but can be wrong. We still have to prove it. And okay, so from the previous description of our model and characterization. Of our model and characterization, we can also derive, of course, samples to perform inference in practice. One that I like is marginal sampler. And here it's pretty easy because we can rely on the interconnected composition of Gibbs-type pry-Erun scheme and the issue multinomial scheme to develop a marginal sample that essentially extends near 2000. We have to be careful in general because without Kolmogorov, In general, because without Kolmogorov consistency, it's not straightforward that we can apply NIL 2000 because the law of the marginal is not the law of a smaller sample when we find the generalized Chinese restaurant process, if you want. But the idea is that for us, inconsistency just holds via the constraints. So So just given that we delete one observation such that it does not have empty a vertex that is a singleton and has some edge, we have consistency. So we can still have a very simple marginal sampler. And then in practice, there are also other problems, of course. And for instance, what is a meaningful What is a meaningful point estimate for such graph-aligned random partition model? And I think it's a very relevant question. And we rely on a result based on the Cheesian theory for summary of partition for random partition model. So here, for instance, is a proposal. So first we assign the observation into vertex or edge. Let's say Or edge, let's say we assign to vertex if the frequency over the MCMC is such that they are assigned to vertex most of the time. And then we can compute a point estimate of the cluster of the n-at observation into vertices that minimize the variation information loss, as suggested by Wei and Garomani, and also as implemented in the recent SALSO package, R package. are packaged by Dahl, Johnson, and Mueller. And finally, given that, we can compute actually the probability of assigning the remaining observation into the different edges. So before concluding, I want to show some preliminary results that I have on our real data. And so here I want to show the inference on the graph-aligned random partition on single-cell RNA sequence experiment on horizontal basal cell. On horizontal basal cell from adult mausole fatal epithelium, and here we colored in not in black the main phases, the point estimate of the main phases, while the black cells allow us to infer the point estimate of the edge that I plotted. And on the right, we can see also the probability of clustering of the cell in the main phases. Of the cell in the main phases, that actually gave us at least an indicator, an indication that our summary is meaningful. And so future work, so in the near future, we want to study formally for the different Gibbs type, approximate infinite changeability under different Gibbs type prior, and of course, apply our graph-aligned random partition model also on other single-cell. Also, on other single-cell real-world data sets. And for model extension, we can exploit our random partition model representation to extend the model to take into account different convariants and combine different data sets in the analysis. And finally, also inspired by this workshop, I think it should be also nice to study in general: okay, finite exchangeability is a condition. Finite changeability is a condition that we want to a model. Infinite changeability sometimes is too much, but how can we define approximately equal model of consistency such that some notion of coherence should still hold. And these are the main references. And thank you for your attention. Are there any questions? On the clustering example, is it possible to kind of get any notion of the influence of individual datums on the structure? Yeah, so it's kind of an interesting is it that the points far away off the line sound particular like thinking of the application. Yeah, so if you look at that picture, is it possible to get a a sense of it's almost like the influence, like if I wiggle this particular data, does it have a strong effect on the overall kind of Effect on the overall kind of inference on number or the edges? Yes, it's a good question. So, yes, of course, how if you want an observation is influential, but this is not for the coherency part, no? It's okay, this, of course, depends which observation and an observation. And an observation that is not, for instance, if we relate an observation that belongs to already big main phases, this shouldn't affect so much. While, of course, if we have an observation that is very distant, probably we will be an isolated vertex. And this can introduce a positive probability of the other edge. So, yes, depend, of course, this where is the position in this. Where is the position in this space of the observation? And that's actually what we want. Some. I really like the model. I think it's really elegant. I got one question. Okay. I really don't understand how the model is going to choose between. Between a vertex and an edge, because if I can switch a little bit the black dots here in the graph, the second one can be easily confused to a vertex. So, how is the model going to differentiate between the vertex and the edges? Yes, that's Yes, that's, I think, also a very good question. My understanding, in the sense that, of course, one vertex Gaussian component, so potentially can be also a Gaussian component, right, from a vertex, right? Even if such precise parameter, even if such precise parameters If such precise parameter configuration of edge has probability zero. But still, why our model doesn't confuse an edge with a vertex, I think, is a very good question. And my intuition is that this is thanks to our prior, in the sense that, given that we have a vertex already. Given that we have a vertex already, right? We still have this mixture component for the edge. And in such a way, the model tends not to open a new vertex because there is already an edge component taken into account from that. Of course, I should define it more formally, but that's my intuition why it's happening like that. Okay. Yeah, because I can see that can easily be confused. Yes, yes, that's a good. Yes, yes, that's a good point. Is there any other question? How would you address it if you felt if you thought we'd point to a line on a manifold? No, I I didn't f think about it. I didn't think about it, to be honest. I mean, these all work on RD, of course, but on a mindful, we should change probably the definition of the kernel. And that's one way to do it. But then depend, yeah, I have to think about also which meaningfully belong. Just put like intermediate notes in between. Yeah, maybe project on the. Yeah, maybe project on the manifold, and yes, but I didn't think about it before, to be honest. Okay, well, thanks a picture again.