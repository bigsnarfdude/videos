So, I'll talk a little bit about some context of orbital free DFT. Hopefully, I can go quite quickly through that. I don't have to say much to this audience. I'll talk a little bit about variation principles, and in particular, why the grand canonical ensemble DFT is the way we think about doing orbital-free calculations. That'll lead me into optimization and a little bit about why, if we want to learn something about the kinetic energy functional, then it's About the kinetic energy functional, then it's absolutely necessary to do self-consistent optimization to test those. And then how we can start to correct semi-local functionals with Kato's nuclear cusp condition. So orbital-free DFT has been kind of a holy grail of DFT since even before DFT was formerly a theory, so in the 20s in the Thomas Fermi and von Weisacker models. There are lots of implementations. Apologies if I've missed your favorite program here, but there are relatively few. But there are relatively few for all-electron treatments and for finite or non-periodic systems. So we wanted to understand the density functional for the kinetic energy in this all-electron and finite system context. I converted this talk to a PDF this morning. I forgot that that has an animation. Don't worry about the black square there. It just shows that the program that we develop in our group, Quest, has been actively developed over the last almost 10 years now. So, almost 10 years now, and in that program, we've put this all-electron implementation. It's really designed as a test bed, so I emphasize this is not for large-scale calculations, unlike the previous talk, but it does give us access to all-electron Gaussian basis set calculations for orbital-free calculations. We can do a range of optimization methods, just a few of them here. So, I'll talk a little bit about these in the next few slides. So, we can either constrain the particle number or we can do an unconstrained optimization where the particle number varies during. Optimization where the particle number varies during the optimization. Of course, at the end, we get hopefully the right number of electrons in our system. And that unconstrained optimization, there are a couple of different methods for doing that. So we like to think about DFT in terms of Liebes formulation in my group. So Lieb's formulation of DFT relates the ground state energy here, E, as a functional of the external potential V to the universal density functional F of ρ in this manner, where they're mutual Legend differential conjugates. Legend differential conjugates. So, everything, all the information that's encoded in the energy is reflected in the universal density function, and vice versa. And there was a nice volume in honor of Lieb's 90th birthday recently, and we tried to write a pedagogical introduction to this theory, and you can find that at this reference. So, here we consider just the case where we fix the number of particles in the system, our number of electrons, n, but in orbital. But in orbital 3DFT, it might be more convenient to consider variations of the density, vary the density arbitrary during the optimization, and then the number of particles the density integrates to is not fixed. And the natural context for that is grand canonical ensemble DFT. So we think about then the energy is now a bifunctional of external potential and μ here, which is our chemical potential. And we can then take an infimum of ensemble density matrices with weights that. With weights that sum to 1 and are less than or equal to 1. So we looked at this formulation a little bit in this paper and how we could do optimizations in this context. And you can think of this as kind of a four-way correspondence. This is an old idea from the 60s where you can think about this as a bifunctional now of external potential and chemical potential. And that bifunctional is bivariate concave, so it has some nice properties. If you did the Legend of France, Properties. If you did the Legend differential transform of this in the same way as Lieb, then you would end up with a universal density functional that depends on the electron density ρ and the particle number n. Now, the slight catch with that is we don't usually know the chemical potential at the start of the calculation if you want to freely vary both the density and the chemical potential. So you can do a partial Legend differential transformation where we go from a bifunctional of external potential that we know and the particle number. And the particle number, or the target particle number, that we, of course, know at the start of our calculation. That then gives us a saddle functional that's concave-convex. It still has one stationary point. And if we take the Legend differential transform of this, then we get a universal density functional that depends on the electron density and the chemical potential. And we can freely optimize these variables. So this middle row here is where we would do our orbital free calculations. The price then is that these are saddle functional. The price then is that these are saddle functionals. So we could write the Humber-Kern variation principle in two different ways. So we could think about this as a constrained optimization where we fix the particle number and only vary over densities that have the right particle number. So this is constrained, but it's a minimization. Or we could do an unconstrained optimization, and we've seen this variation principle already, where we vary over chemical potential and density simultaneously. It turns out you can separate out the chemical potential part, so you can still use your pure density. Still use your pure density functional part for the universal density functionals. And the stationary point, or at the stationary point, the usual Euler equation is satisfied. I think Sam already mentioned that there are issues here as to whether these functional derivatives exist, whether you can take them for the exact functional. Of course, for local functionals that we use typically, then you can differentiate. And you can work around this. We've talked about this a little bit in this paper here if you want to consider the exact case as well. Exact case as well. So, what do we do in practice? Well, the implementation that we have uses Gaussian basis functions. We simply expand the square root of the density in terms of these Gaussian basis functions and optimize the coefficients defining the density using either these constrained or unconstrained methods. And as I say, we've implemented quite a few different techniques for this. Because it's a Gaussian basis code, most of the technology that you have for doing these calculations carries over. So, you can do things like density fitting, almost changing to speed up your Coulomb integrals. You can use Your Coulomb integrals, you can use standard quadrature, and so on and so forth. So you can recycle quite a lot of what you may already have if you have that type of program. In what follows, I'm going to look at perhaps one of the simplest cases of an orbital free density functional, the Thomas Fermi-Von-Wisaka density functionals or family of functionals. So I'm sure I hopefully again don't need to tell you too much about this for this audience, but here we can think about combining Thomas Fermi with the von Weiss like a functional. The von Weissacker functional with two adjustable weights, gamma and lambda, defining how much of each we have. We of course then add to those the standard Coulomb terms and an exchange correlation functional, typically a GGA type, and that defines our total energy expression. And of the types of optimization I mentioned, so the first one, constrained optimization where we keep the particle number fixed, we've already seen in the previous talk that you can write the Euler equation in the Schrödinger-type form. So you get a Schrodinger type form, so you get this kind of quasi-one orbital concham. This was done by Levi Perjo and Sarni a long time ago, and they said, Okay, this is easy, you can do it in any program you like, and it'll just work. That's not exactly our experience, but you can program this up. It's quick to implement. Unfortunately, standard convergence accelerators tend to fail with this, so you have to resort to quite heavy density matrix damping, and you get many, many iterations. When we couple it with different functionals, we find When we couple it with different functionals, we find it works for Thomas-Hermybon-Wisaka with weights of one on each component, but for little else. Lopez Acevedo, who will talk later today, noted that you can use this scaled equation instead. And then you can get this to work for the whole family of Thomas-Fermi-bonbeis like a functionals with all kinds of different weights. Again, you still have quite heavy damping to solve this equation and many iterations, but you can make it it work for a wider range of functionals. For a wider range of functionals. Unfortunately, the convergence is quite slow. So, here, this is just the convergence of the energy for an atom in a large, even-tempered basis set to try to get within one millihartry of the complete basis set limit. And for lambda equals one, so just Thomas Fermi plus one Weisaka, the approach of Levy, Pergen, and Sarni works okay. But the convergence here, you can see it takes 100 iterations. 100 iterations to get to something that is even approaching convergence. Now we have this long tail, and this long tail is here because I'm just showing the energy. I also insist that the eigenvalue, the chemical potential, is converged tightly as well. And we'll see in a moment why that is. If I change the weight of the von Weizacker term, then it's known that as you decrease this weight, you get slower convergence, and you can go to what would be a functional corresponding to something like the second-order gradient expansion. Like the second-order gradient expansion, and you get very slow convergence taking thousands of iterations if you insist on tight convergence of the chemical potential. So that made us wonder: okay, what if we do this unconstrained optimization? Then we simultaneously optimize the density and the chemical potential. We use this other variation principle or this other form of the Hohmurg-Cohen variation principle that I mentioned. And if you look in the literature, you find that Cohen Channon handed this quite a long time ago, and their practical scheme was to optimize the density. Scheme was to optimize the density for a fixed chemical potential, try to find chemical potentials that bracket the right particle number, and then bisect numerically for the right number of target particles in your system. So you can follow a scheme like this, and things do improve. So this was the convergence that we had before. Now the scale changes, so we get approximately an order of magnitude faster convergence, even for the most difficult functional here. You can see it has this quite bumpy convergence. You can see it has this quite bumpy convergence. This is the bracketing going on. So you're fixing the chemical potential, optimizing the density, adjusting the chemical potential, optimizing the density. You keep repeating that process, but eventually you can converge. The good news is you can couple it with more general functions than just Thomas Fermi von Weizaker. But it's still quite slow when you think about doing, say, a standard Cohen-Charm SCF. So it's a lot of energy and gradient evaluations. So, we've thought about doing this completely in one shot, where we've thought about trying to optimize the chemical potential and the electron density simultaneously rather than this nested iteration. So, we used what we call what's known as the trust region image approach. So, this was an idea suggested in the 90s for geometry optimization, where you think about the image function. That means you simply invert the mode corresponding to the chemical potential, invert its sign, and you switch from a Invert its sign, and you switch from a saddle search to something that is a minimization problem. And it's reasonably straightforward to implement, and you find that you can get much faster and tighter convergence in this all-electron setting. It's second order, so you get quadratic convergence both in the energy and in the chemical potential. And you get something that converges here for a simple case of just a helium atom in tens of iterations, much more like a normal Kohn-Sham calculation. Calculation. If you make the weight of the von Weisacoterm smaller, so you make the calculation more difficult, then you can see this kind of still holds up for the trim approach. So we can get tight, reliable convergence in this all-electron setting. So can we start to say something about the functionals? Well, according to the Liebes theory for the grand canonical ensemble context, then when we have these bivariate functionals that are, in this case, the universal density function. In this case, the universal density function will be concave-convex, then we should only have one unique solution. But it's known already if you take Thomas-Fomi plus one-Wiseka. It's not concave-convex, so we should expect multiple solutions. If you add an exchange correlation functional, even if you just add Dirac exchange, you break the convex concavity. And so, again, we should expect multiple solutions. But these well-behaved functionals, Thomas Fermi-von-Wiesaker, we find some multiple solutions, but not many, starting. Multiple solutions, but not many, starting from just 25 random initial guesses. So, this is not exhaustive, but at least you can see there are multiple solutions, but it's not so bad, and they tend to be well separated by energy. So, it's not much worse than doing a normal currench on where you might also find multiple solutions. You can take some GGA forms that are expected to be relatively stable by practical use, and again, you find that they only give a few unique solutions. Unique solutions. If you make your enhancement factor in the GGA much more complex, so people suggested using enhancement factors that are inspired by exchange correlation functionals, then these more complex forms can have many, many solutions. And here, for these so-called conjointness conjecture functionals that are based on these exchange correlation functionals, we can find almost as many solutions as we have starting guesses. So they can be really difficult. And these are close in energy and difficult. And these are close in energy and difficult to separate. So, why would we want to do this self-consistently? Well, if you look at the self-consistent solutions you get, people often in the literature test their functionals by saying, okay, take a Cohen-Sharm density or take a Hartree-Fock density, evaluate my kinetic energy functional. Does it give me a good energy? Therefore, is it good or not? The danger with doing that is the stationary density when you do the self-consistent optimization may be a long way away from your Khan-Sham or your Harpy-Fock density. Yep, Fock density. And that's what we're showing here, just for again atoms up to argon. So using a PBE density or using a self-consistent density, and you take various forms of the Thomas Fermi gonvis like a functional or various GGAs, and you can see in many cases the errors get much worse when you do this self-consistently because the density degrades significantly. But also, if you were to rank these in terms of what you think is a good functional or a bad functional, then you would get a different ordering. Then you would get a different ordering depending whether you do this self-consistently or not. So it's kind of essential to do this self-consistent optimization if you want to understand whether your functional is any good. One that stands out here is the work by Lopez Acevedo, where they actually did the self-consistent calculations. And in this case, you can see going from post-concharm or a PBE density plugged into the Thomas Fermi von Bisaka form with these weights, you get a much better result when you do the self-consistent calculation. When you do a self-consistent calculation, I'll skip over this slide in the interest of time and try to get onto the K2 cusp condition. But essentially, if you do this for molecular systems, you see a similar story. Self-consistency is still important. If you do the trim method here, this is just a median number of iterations to converge the calculations for this range of small molecules. And again, they converge reasonably quickly and with. They converge reasonably quickly and with tight quadratic convergence. So you can do molecular systems in this all-electron context. So, how do we go a little bit further? Well, of course, if you're doing soft-consistent calculations, you've immediately got access to the stationary electron density in this all-electron context, and it's well known that these densities are extremely poor. And Sam mentioned this morning the lack of atomic shell structure. So, we're going to consider the functional corresponding to this second order. Corresponding to the second-order gradient expansion. I'll put that on the next slide, but this is Thomas Fermi von Weissacker with a weight on the von Weissacker of 19. Here we've got the Kern-Schamp density for a neon atom, so we expect if we weight it by R squared to see shell structure. And if we do Thomas Hermy plus 191 Weisaker, so just replace our kinetic energy here. And for the exchange correlation part, we're just using PBE, but it could be any exchange correlation you like, and you get essentially the same results. Essentially, the same results, then we get this very, very poor green density. And you can see it's got no shell structure, and it also gets this tail of the density rather poorly. The radial weighted density kind of hides a large part of the problem. So if I look again at the Cohen-Sham density and put the actual just density directly plotted here as a function of the distance from the nucleus for this functional, then we can see this core collapse, as Sam called it. So the density just piles up at the nucleus. Up at the nucleus. So, really, you're looking for, you almost can't see the wood for the trees here. You're looking for a detail in the density to get the shell structure, but actually, you've got this huge error at the nucleus. So, how are we going to deal with that? And it really is a huge error you can compare with some bounds. So, there's a bound on the density at the nucleus you can check, and you can see the current SHAM density respects this bound, but it's massively violated for the second-order gradient expansion type functional here. Functional here. Okay, so here's the violation of this bound. So you can start to ask, and this is maybe a question for this workshop: what is the validity of the gradient expansion for kinetic energy functions? Should we even be using gradient expansion, GGA, meta-GGA type forms to try to describe the kinetic energy? Well, you can look at convergence criteria for the gradient expansion, and here are a couple of criteria that should be satisfied if you expect this expansion to converge. And you can plot. And you can plot them in the neighborhood of a reasonable density. So, here I just take the Kohn-Sham density for a neon atom, and I plot these quantities locally. And you can see that one of them is massively violated near the nucleus. Actually, for the rest of the range of the space here, it's not so bad. So, essentially, this tells us maybe we really need to correct near the nucleus for this failure. But perhaps elsewhere, we can use. But perhaps elsewhere we can use our local density function. So, how might we go about that? So, this was an idea that one of my postdocs, Michael Hutching, came up with. He said, well, if we look at what we need to know, then perhaps knowing the identity of the atom, the identity of the nuclei in the system, which is not so bad when we do a Born-Oppenheimer approximation calculation, is a useful starting point. And we could use Cato's nuclear cusp condition. So we could enforce this condition. Condition. So we could enforce this condition near the nucleus. Now, one problem with this is we're using Gaussian basis sets, so we're never going to get it right at the nucleus. But it turns out if you plot this ratio, divide 3 by this term, then it should be 1 where Cato's limit is satisfied. And it's actually close to 1 close to the nucleus. So this is the tail of density if you coherence calculation. It's pretty good here. And then as you get to the nucleus, it fails. And that failure is the failure of the Gaussian base. Failure is the failure of the Gaussian basis set limit. So we could enforce this condition in a small neighborhood around the nucleus as tightly as we can for our Gaussian basis, or you could use a different basis and perhaps you can enforce this even better. So we defined a correction term. So this correction term now, we have to know the identity of the nuclei. So we've broken universality slightly. We enforce it within an envelope, which is defined here, and it depends on the distance. And it depends on the distance from each nucleus in the system. But these terms are relatively cheap to compute, so it can be added fairly easily to any kinetic energy functional you like. Here's what we have from our second-order gradient expansion. You can see Kato's limit is not satisfied essentially anywhere, and we've got these ridiculously high densities. So let's apply it to this form. So we add this core constrained correction. Correction. So we enforce this Nuclear limit, and now this is the blue line, and you can see it's almost indistinguishable from a Kohn-Sham density. So I've got the Katos limit is slightly over-inforced by applying this correction, but it's pretty close to the correct density. We also respect the bound that I mentioned previously. So we get a total density, if I plot that, that looks pretty close to the construction density itself, and we get rid of this problem near the nucleus. And what surprises And what surprised at least me was that we get shell structure. Okay, so shell structure suddenly emerges. The blue line here is what we get. It's by no means perfect. And we still have this problem in the tail. And this exponential is essentially the envelope in which this constraint is applied. So just by applying this simple correction based on a physical constraint near the nucleus, we can get some shell structure. We get similar behavior for a wide range of atoms, and as we For a wide range of atoms, and as we do this, then the energies also improve. The energies are notoriously poor, with particularly this very simple kinetic energy functional, but they improve very strongly as well. But we get shell structure appearing. What we don't get is we can't fix things like the asymptotic behavior. This is a core correction, it only fixes the region near the nucleus. If you look at the tail of argon here, for example, it starts to go like the parent functional. The density starts to bleed out. The density starts to bleed out to long range. So, if we want to do better, then we have to go beyond the second-order gradient expansion. So, we could think about using a more complex kinetic energy density functional. And we can add this correction to any semi-local functional you like. So, again, here's the Kohn-Sham density. Again, I'm just showing this for neon atom as an example, but it's quite similar for this range of atoms. This is what we had with the second-order. The second order gradient expansion function without the core correction. With the core correction, we get some shell structure, but there's poor tail. We could go to fourth order, and it's hard to see anything happen there. The green line appeared. The density near the nucleus is better, but still terrible. And when we apply the core correction, we get essentially the same result. We don't really get much improvement. So, fourth order information is not sufficient on its own. One functional that I think we'll hear about this afternoon is a Purdue constant. Think we'll hear about this afternoon is the Purdue Constantine functional, and it uses a modified fourth-order correction and an interpolation. And this interpolation is essential, and it essentially interpolates between the von Weizacker and the fourth-order limits. And you would expect including the von Weissacker in this interpolation to make iso-orbital regions better, so we'd expect some improvement in the tail. If we just do a self-consistent solution to this functional, we get something that perhaps resembles shell structure but is relatively poor, and we still get this. Relatively poor, and we still get this poor tail. If we core correct it, then the density actually slightly overshoots in this case, and we get a strange behavior in this intermediate range, but we do get the tail back. And you can look into the form of this functional. This is its enhancement factor, and this is the interpolation function. And it turns out things are really dependent. All of this region, this behaviour can be traced to the interpolation function. So you can play with those parameters and you can make this region better. With those parameters, and you can make this region better or worse. And these parameters A and B were determined by Perdu and Constantine on reference hatrifon densities, I think. So it was based on a reference density rather than a self-consistent result. So one could perhaps hope to improve this interpolation to improve results. We already saw that when we went to the fourth-order expansion, the information was perhaps not so useful. So just as a quick test of this, we made a very simple, similar intelligence. We made a very simple, similar interpolation, but only using second-order information and the von Weizacker limit. If you do that self-consistently, then you get something like the blue curve. The von Weizacker limit fixes the tail, but it doesn't give you the shell structure in the atom. If you apply the core correction, then you get some shell structure back, it's improved, and you also get the tail improved as well. So, in other words, this is maybe a good baseline. The core corrected functional is a good baseline for trying to build improved density function. Build improved density functionals for the kinetic energy. Behavior in molecular systems. So here's just a simple lithium dimer. There's not too much to see here. The core correction behaves in basically the same way. It fixes, so here are these green spikes are the failure near the nuclei. The core correction is the blue line. It fixes those near-nuclear regions, but it's not going to fix the bonding region of the molecule. So if your functional, your underlying functional doesn't exhibit chemical. Underlying functional doesn't exhibit chemical binding, then that's not going to appear. But what it does do is quantities like the Laplacian that you may want to include in your functional become better behaved. So here, the green line is at the self-consistent density for the second-order gradient expansion. It's very ill-behaved near the nucleus. If you do a self-consistent calculation, that can cause you problems with trying to converge these things smoothly. If you call correct, then everything looks like the constraint. Looks like the Cohen-Cham Laplacian. So things become smoother and it becomes a little bit easier to do the calculations. So hopefully I haven't run too much over time. Just a summary then. So we think about orbital 3DFT in terms of this grand canonical ensemble context. And we talked a little bit about that this functional should be a concave convex functional and a four-way correspondence. We connected that to the optimization schemes that we use, and in particular the trim optimization method. Particularly the trim optimization method really does this saddle search directly, gives very tight second-order convergence. And I would say it's absolutely necessary to test the functional self-consistently, as well as post-concharm. And then we applied Kato's nuclear cusp condition to fix the region near the core, try to prevent the core collapse. And hopefully, these functionals give a good baseline for developing improved functionals in the future. I should thank the people that do all the work on the Quest Code, in particular Michael here, who's done most of the work on the Keto because. Done most of the work on the Kito condition information that I mentioned. Thank you for your attention.