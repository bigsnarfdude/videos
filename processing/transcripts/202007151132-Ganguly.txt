During the talk, you can ask them in the chat and we'll relay the questions if well, we'll try to answer if possible, and otherwise we'll relay the questions to Shirchandu directly. We should do a break after 30 minutes where you can ask some questions. And at the very end of the talk, we'll stop all recordings so that you can ask questions with your mic if you want. In the meantime, if you don't want to have your face on the internet, please check. Have your face on the internet? Please just turn off your camera and your mic. Just a reminder: this is going to be a mini course in three parts. So there's obviously today and then tomorrow and Friday, the classes will be at 12 p.m. Eastern Time, meaning one hour and 30 minutes earlier than now. So after this brief technical introduction, I'm very happy to introduce Shirley. I'm very happy to introduce Shushinda Genguly from Berkeley. And he's going to give us a mini course on large deviations for random networks and applications. Okay, so should I start? So, okay, let me try to sort of share my screen. Okay, so this is visible, right? Okay. Sometimes Zoom might freeze, and so I might be writing and things might not be updating. So if you think that it's been too long without sort of seeing any change, you should point that to me and then I should do something about it. It should be fine, but sometimes it does. Okay. Yeah, so thanks a lot to the organizers for the invitation. To the organizers for the invitation, and a great sequence of topics throughout the summer. It's a great way to keep everybody sort of excited about something amidst all these sort of grim circumstances. So, what I'll talk about is a topic that has seen a lot of activity recently, and it sort of has to do with large divisions for some canonical nonlinear functions of independent random variables. And so, I'll try to give a broad sort of overview of some of the progress, including some of my own work. Including some of my own work. But there's so many new results coming up every day. It's sort of hard to keep a tab on everything. So, if I sort of, if you think that there is something that I should be mentioning and I am missing somehow, so you should feel free to point that to me. I'm also sort of typing up some lecture notes. So, it might have some typos. It's been rather quick, but I'll sort of send them to the organizers after today's lecture, and they will contain a superset of what I'll sort of talk about today and over the next few lectures. Today and over the next few lectures. Okay, good. Okay, so large divisions for random networks and applications. So let's sort of start with the motivating sort of question. So let's start with an endoshi random graph. So this is a random graph. I'm pretty sure all of you are familiar with this, but just to sort of keep things concrete. On n-vertices, where every edge occurs independently with probability p. Okay, so you have some big random graph. And now let's fix some subgraph H. So some fixed graph H. Fixed graph H you can think of it as a triangle. So for the moment, consider just K3 or a triangle. This is a complete graph on three vertices. And the object of interest is, let's say, XH, the random variable, which is the number of copies. The number of copies of H in the random graph G. Okay, so G is a random graph, and I look at the number of triangles in G. So, I mean, it's pretty easy to compute what expectation of Xi G is. And that's just, I'm looking at, let's say, labeled Kriango. So, then it's just, I'm like, up to smaller terms, it's. Um, I'm like up to smaller terms, it's n cube, p cube. And so, the question of interest or the motivating question is the following. So, I just want to I want to understand atypical behavior of this random variable. So, in particular, I want to understand probability of events like the following. So, delta is some positive fixed, let's say, fixed number. So, XH is typically like expectation XH. So, people know a lot about it. So, there is, it sort of satisfies a central limit theorem and whatnot. And so, it's sort of really concentrated typically around expectation of XH. And I want to understand sort of atypical behavior. And so, this actually goes by the name. Goes by the name, and I husband sort of studied for a while now. Coined by Janssen and Rushinsky in zero two. Okay, so and And another sort of related question is the geometric one. So, this is something about computing the probability of a certain event. But I can also ask a more geometric question. What does, I mean like this is going to be slightly vague at this point. What does a random graph G look like? G look like given the event. Let's call this event for the moment as A given the event A. Like what is sort of a geometric manifestation of this conditioning on this event A? Do you sort of tend to see sort of more? So typically, if you have a graph on size N with edge density P, roughly you will have. Density P roughly will have n choose 2 times P many edges. But maybe if I tell you that the number of frankles is much larger than typical, maybe the number of edges actually become much larger. Or there could be some other sort of geometric consequences of this conditioning. So these are roughly the two sort of guiding questions that will dictate whatever I'm going to be saying over the next few lectures. And so before setting up, okay. So, before sort of okay, so and so an observation or just a fact, I'll be more precise, but like it's sort of not hard to see that XH is a polynomial of independent Bernale variables. So the edges of the graph are independent Bernoulli variables, and the number of triangles is in a graph, in this random graph, is roughly a polynomial of degree 3 in this random variable, in all this independent bits. Okay, so any questions so far? Okay, so any questions so far? Actually, let me just make sure that I am sort of the chat box is visible to me. Okay. Okay. Okay, so this is sort of the question, but let's see some sort of classical results about linear functions. So the hardness is that this is a polynomial, but let's just sort of recall some. Recall some classical concentration and large deviations result for linear functions. But if we will look at a Will look at a very basic setting. So let's first start with the Zuma Hofding inequality. So let's say are independent. Dependent mean zero random variables such that xi is less than so. Let's say I have a sequence of independent random variables such as that the ith random variable is between ai and bi for. Is between AI and VI for two constants AI and VI, almost surely. Okay. So then I want to, let's say, find some concentration estimate for the sum. So Sn let's say is the sum of text size. And I want to understand, let's say, probability. So Sn have, of course, has mean zero. Course, as mean zero, let's say I want to understand probability S n is bigger than t for some t. Okay, so this is a very classical thing, and everybody sort of is familiar with it, but let's sort of just review how to maybe prove such a concentration result. And the method is sort of super classical, but sort of it turns out that even generalization of that will help us understand the random graph question that I started with. Okay, so the strategy. Okay, so the strategy is to use. I mean, there are many strategies, so you can use Chebyshev's inequality or whatever, but if you want something stronger, is to compute exponential moments and then apply. apply Markov. Okay. So, like for example, like this thing is true that expectation of e power theta xi. So theta is some parameter that we will sort of optimize over. That thing is less than e power theta square bi square minus a i square sorry Sorry, that's not what I meant. Maybe by an 8. So if you have a random variable xi, which is means you're unsupported on between Ai and Bi, then you have this upper bound on the exponential moment. Okay? And so let's say, let's call this as maybe Ci. So this tells you that. So, this tells you that the expectation of e power theta s n is less than or equal to e power theta square summation ci square by 8. Okay, and now suppose I want to understand what is the property that Sn is bigger than t, then I can sort of just which means that this guy is bigger than e power. Which means that this guy is bigger than e power theta t. So this thing is by Markov less than e power theta square summation ci square by 8 minus theta t. And this bound holds for all theta. And now you can optimize about theta and then optimize over theta. Right, so the general strategy is you compute exponential moments and then apply Markov. And then apply Markov in sort of an optimal way. Okay, so this gives you just concentration. But now let's look at some sort of large deviation. So, again, so look at a special case where excisor kindas. So, let's say exciser all IID born only P and and And so, and now let's say I want to understand the probability that Sn is bigger than n times q, where q is bigger than p. So, sn the sum of x i's is typically like n p plus some fluctuation of order square root n. But now, let's say I want to understand what is the probability that is super large. Okay, I can sort of write do the same strategy. Do the same strategy. So let's say lambda theta is the log of this exponential moment jointing function, log of the moment jointing function. So in this case, it's going to be p times e power theta plus 1 minus p. So if I look at just one bit, e power theta xi is literally this. And so I'm just taking the love of that. That. So, if I follow the same strategy, the bound that I get is e power n lambda theta minus n times theta q. So, this thing is less equal to this by the same strategy. And okay, again, I can optimize over theta. So, so this is, so if I optimize, so I can like look at this variational problem, which is let's say theta q minus lambda theta. So, the theta that maximizes this, if I plug that in, that's going to give me the best possible bound here. And it turns out that the dual, so this is again. The dual, so this is again. So, this is a convex function, and this is the Legendre dual of this log moment-jentic function. It turns out that this is what is known as the relative entropy. So, this thing is actually the relative entropy of Bonoly Q with respect to P, which is nothing but this. Okay, so using that I get the probability Sn bigger than n q is less than e power minus n i p of q without actually any error. So this is sort of an error free in some sense. Okay, so that's an upper bound, but to get sharp results, you also need a lower bound. And so that's by a standard technique called tilting. So the strategy is to do a change of measure. Do a change of measure, which makes the atypical event typical. So, Sn bigger than NQ is atypical under the product measure where everything is a burn of the P, was actually pretty typical if everything was a burn to the Q. Q. So, so you can sort of you want to estimate the property of this event, you can sort of instead of measuring the probability of this event under the original measure, you can measure the probability of the event under a new measure, which makes it likely. And then to get back to the original measure, to get back to the original measure. To the original measure, one must estimate the Radal-Nicotin derivative between the two measures. Right, so I mean, like, just sort of abstractly, so suppose I want to measure the probability of an event A under. Uh, the probability of an event A under P, so that's the same as, um, so think of some other measure Q, and it's the same as saying e power log dp dq dq of A. I just did not do anything. So e power log dp dq is little dp dq times dq is dp. So this is indeed p of a, but dq puts a lot of mass on a. And if I can actually understand this. And if I can actually understand this thing reasonably sharply, then I can get a lower bound. Okay, so the upper bound is by this exponential moment argument, lower bound is by some change of measure and computing the radical integrative. Okay. Okay, so this is sort of you should sort of keep this strategy in mind, and this is something that will come up in our understanding of the Up in our understanding of the events in the graph setting as well. Okay, so again, back to this random variable XH. So just to sort of concretely XH recall, XH is the number of copies of H in some graph G. So just to sort of Some graph g. So, so just to sort of formalize the notation more generally, so for any graph h and g, let's call t h g as let's say so one over n to the size of the vortex. So, I'll explain what this is. So, G is let's say a graph on n vortices with adjacency matrix A and H has, let's say, K vortices. So, this is so basically. So this is so basically the this is the num um this is essentially the density of the number of copies of H inside G so I so I want to map H inside G so let's say the K vortices of H get mapped to the vortices I1, I2, IK in my big graph G and then what I want is for all the edges in H to map to some edge in G and and A I X I Y is the Iy is the Ixiyth entruth. This is the matrix of A, which encodes whether in G there is an edge there or not. So this basically says that what is the probability that what is sort of the average density of H inside G in principle, which means basically if you take a random bijection, what is the probability that it will end up being actually H? Okay. And so to answer the question, To answer the question that what does the let's see so recall the geometric question what does GNP look like given THG is large now in the quantizing example One thing I should have mentioned here in the coin testing example, everything is of density P typically, but if I tell you that the sum is actually much bigger than typical and it's NQ, then this essentially indicates, we also have a matching load bound by this trading strategy, that every coin basically roughly now looks like a bonoule Q. So, sort of in some sense, the optimal strategy for a sum of independent coins to have a large value is for each of them to have Coins to have a large value is for each of them to have a higher density in some sense. Now, from that sort of intuition, you can also sort of try to guess what the answer to this question is. And sort of a reasonable guess could be that GNP continues to look like a random graph. It doesn't show any type of graph, but with different densities. In principle, it can also happen that different edges occur with different probabilities. So it can look like an inhomogeneous random graph. Oh, there were some questions. Yeah, sorry. Yeah. So I'm like, I start with every edge being property P, but maybe some edges now have property P1, some other edges have property P2 in some sort of optimal way so that it makes it likely for the number of copies of H to be large. Okay. And so this is. And so, this is what we will try to establish over the course of this lecture. So, it will be convenient to right. So, when I'm saying that it looks like an Edoshini type graph or it looks like some graph, it's sort of to make sense of this, one has to define some notion of distance on graphs. Define some notion of distance on graphs. So, this is what we will do next. Define a metric on graphs and embed them in the same space. And so, this is a And so this is a definition. So let W be the set of all symmetric measurable functions from zero one square to zero one. So essentially you have a function So essentially, you have a function on the square, the unit square, which is symmetric and takes values in 0, 1. So you should think of this as some continuum limit of graphs. So these are what will be called as graphons. So note that any finite Any finite graph naturally embeds in W as a 0, 1 valued step function. Right, so I take a graph for any finite graph, there is an adjacency matrix A. adjacency matrix A, right? The entries of the matrix are, it's a symmetric matrix. The entries are 0 or 1 depending on whether a particular edge is present or not. So it's a graph, this matrix is a square of size n by n. Now I can just literally scale everything down to a size 0, 1 by 0, 1 square. And the entries here become small boxes. So a graph basically can be identified with a function which looks like this. So these are one by n squares, and these are zeros or ones, depending on whether the existence matrix has a zero or a one. So every graph naturally embeds into this space. Any questions about this? Oh, can you get some? Yeah, I was just going to mention there was a question from before the definition about the. About the intuition for in-home. Right, so it will turn out that it's sometimes actually not the most optimal thing to, as you will soon see, it's not the most optimal thing to actually raise the density of all the edges because that's very costly. You can actually only affect a small part of the graph to get the required boost. So you can sort of, let's say, put in a bunch of edges, but much smaller than the total number of edges in a very tiny part of the graph in a compact way so that it actually gets you the additional boost in the number of edge counts that you need. The number of edge counts that you need. Does that roughly sort of answer your question? Like, it's not probabilistically always optimal to increase the number of edges because it's very costly. Like, it's there are more optimal ways to actually pack smaller number of edges in a compact form to get the additional boost. Okay. Right. So, this is the space in which you will embed every graph. And now, let's define the metric. So, for F G in W so we'll define so let's consider we'll define let's define this distance between f and g as the following. So you take supremum over all subsets st sorry. Um sorry so you take subsets of zero one and now you basically you basically take so this so you should think of zero one as your set of vertices so these are all let's say continuum graphs on the So, these are all, let's say, continuum graphs on the unit interval. So, you take two sets S and T and you look at the difference in the number of edges going between S and T in the graph on F versus the graph on G, and you sort of maximize over all such charges of sets S and T. So, basically, this is known as the cut distance. So, essentially, what it is doing is it's sort of so two functions f and here close in this distance if they're all the cuts are roughly preserved. If they're all the cuts are roughly preserved, so the cuts meaning you have two sets and there is a cut between them, and you're looking at the number of edges going across them. And this is indeed measuring the difference in the number of edges going across the two sets. Is the definition clear? I guess it's hard to sort of make out, but okay, and Okay. And so it also turns out that, okay, so since we since we don't care about labels of the graphs The graphs we should identify graphs or graphons which are the same up to a relabeling. So, what does that mean? So, precisely, it means that let's say sigma is the measure preserving bijection between on zero one, and so then so we say f and g are equivalent if there exists if there exists sigma such that the distance between f and g composed with sigma is zero so so i can sort of so these are functions on zero one square but now i can sort of uh relabel them and change their labels around so i should sort of think of two functions at the same if they're the same up to some relabeling and so this is what Same up to some relabeling. And so this is what actually quantifies that. So this is what will, this is the quotient space. So we'll actually work with the quotient space. So we would work with the quotient space W tilde, where you identify this via this equivalence. Okay. Now, Now, okay, so I want to answer the question of what is the probability of a certain subgraph counting large. I know how to answer this for coin testing experiments. So, I want to sort of try to use that strategy to answer this question. So, for example, let's look at a slightly easier question. So, suppose I have a graph. So, suppose I have a graph. I will identify the graph with a square. And let's say I have, I divide it into block graphs, like blocks. So, there are four parts, let's say. Right, so I've divided my big graph into four parts. four parts. Now typically the density of so typically the density of edges between A1 and A2 is P right so all of them are roughly P so this basically if you zoom out I'm like the actual matrix is 0 1 valued but on an average everything looks roughly like P Right, so if you so on an average, everything looks like P, so it turns out that a graph, if you take a graph which is G and P, sample from G and P, that graph is actually pretty close to the constant function P. Right, so I so all my graphs live on the same space. So let's say I My graphs live on the same space. So let's say I sample some n-size graph from the measure G and P. The claim is because of what I just said, because on an average the density of edges across any two sets is P, the graph thought of as a graphon is actually very close to the constant function p. Okay, but now I can ask the question so a related question is the following. The following is a company. Let me just copy this. Okay, so instead of looking all p, I can actually now okay, so maybe this looks like p1, this looks like p2, maybe this looks like p3, this looks like p4, and so it's symmetric. It's symmetric. So I can actually sort of prescribe different values to different blocks, and I can ask what is the probability that the graph actually looks like this instead of everything being p. And notice that this is exactly the coin-tressing problem. Right? So, my graph is made up of coin tosses, which are all borderly P. Now, suppose I tell you that this looks like P1, which basically means that total number of edges in this is P1 times the total number of possible edges. And similarly, all the other things. So, everything was initially P, but now I've sort of pressed up different weights. But this is exactly the same bunch of coin tosses has a different sum than what its typical value is. Value is. Is that clear? If I tell you that what is the property that my original graph looks like, this block graph, it's essentially the same as the coin dusking problem. And so, actually, the property of this Using the same reasoning as the coin testing experiment, as the coin testing is the following: it's exponential of something like a one choose to IP of p1. P1 plus A1 A2 IP of P2 and things like that. So how many coins are here? It's size of A1 choose 2. The number of coins going between A1 and A2 is size of A1 times size of A2. And so exactly, and these are all independent. So by this contrasting. So, by this coin-tossing experiment, the probability of getting such a weighted block graph is exactly this. Okay? So, going back to our initial sort of guess that telling you that the number of triangles is large makes the graph look like a homogeneous random graph, it's sort of reasonable to try to. Reasonable to try to understand what is the best possible choice of these values such that this new set of values makes the atypical event typical. Okay, so this is so then so Okay, so so so this is sort of the predictional problem that I So, this is some of our so let's maybe Shrushandi, you had a question if you meant that all the terms in the experiential. Oh, yeah, yeah, yeah, yeah. So everything exactly. It's literally the client testing experiment, but repeated for every block. And their blocks are independent, so the properties sort of are multiplied, and each of them is exponential of the minus of the relative entropy. But okay, so I can choose, so I can try to choose. Um, so I can try to choose um different weights. So, Qij is such that, okay, so Um, that's a good idea. So, I have my original graph G, which looks like a constant matrix P, but now instead of for every argument tree. But now I sort of, for every ijth entry, I put in Qij. So Q matrix is a new weighted matrix, is a new weighted graph. Right, so this is the strategy that I'm sort of extending the previous strategy. So I basically choose all possible weightings of the graph, where the ijth entry has the new weight qij. The entropy cost is IP of Qij. IP of Qij and I want to I want to so the weighting should satisfy that under this weighting the the the the number of copies of H inside this new random graph is exactly what I want it to be which is bigger than one plus delta times the original expectation okay so this is I mean like by this strategy this is the best that you can do if you can if you think of everything if you think of this Think of everything, if you think of this inamogenous random graph as being the actual candidate, then this is the best possible candidate. The Q that minimizes this. Minima over that okay, because the property is going to be exponential of minus of this, so smaller this is, the higher the property is. So, any question? So, any questions about this? So, is the best probability one can get by this strategy of Okay, so this actually gives you a lower bound in some sense. This is again like a tilting type strategy where I sort of only look at a particular class of graphs. These are the thing homogeneous random graphs, and I look at the one which actually makes the property the most. look at the one which actually makes the property the most which makes it the most likely the one which has the least amount of entropy cost okay but how does one prove that this is optimal how to prove prove that this is indeed the optimal strategy And this is where, and so the key buzzword here is so, this is a key result from external graph theory, also sort of had many applications in ergodic theory. Many applications in a Guardic theory. So, this basically says that basically any graph can be approximated by a sort of a block random graph. So, I'll be slightly more precise, but sort of roughly, this says any graph. Can be approximated by a block random graph where the number of blocks is only a A function of the error and not the size of the graph. So think of every graph. So every graph lives in this graphon space, which is all of unit order. So suppose I take any graph and I want to sort of find. So this basically says that there is a block graph with the number of blocks, let's say only k. Of blocks, let's say only k, such that the distance in this cut norm that I defined, the distance between the graph that I started with, an arbitrary graph, and this block graph is going to be less than epsilon, where the number of blocks, which is k, is only a function of epsilon. So I'll set a weaker version of this, so the weak regulatory lemma by Fries and Kanan. There was a question before. I was wondering, someone was wondering why do we hope that the correct tilting measure should have independent measurements. This is exactly why, because it says basically that every graph looks like a block-independent graph. So, previously, the strategy was to sort of just make a guess, and then sort of that will give you a lower one of the property that's one of the possible candidates for the. The possible candidates for the new measure, but the regular dilemma tells you that it is actually will be optimal because actually any graph looks like an inhomogeneous random graph with the number of blocks, which is only a function of the error. Does that roughly answer the question? So let me sort of continue with this, and it might be sort of slightly more clear once I sort of state this. So essentially, it basically says the following: so, given Any graph there exists a partition of V into k classes A one, A two A K such that Such that if you look at this block graph where row ij, so where the ij entry of this block reference, let's say row ij. So row ij is actually So, I have my original graph G, right? It says basically you can partition the full vortex at V into K classes A1 through AK, such that if you now look at the block graph where it has K blocks on its side, where the ijth block has entry rho ij. Entry rho ij. Rho ij is this, is the number of edges going between Ai and Aj in my graph G divided by size of Ai times size of Aj. This essentially means what the edge density is in my graph G across AI and AJ. So it says that there exists a partition into k classes such that the distance between G, the original graph, thought of as a graph on maybe, and this new block graph. And this new block graph, let's call that G quotiented by, or maybe let's define that quotiented by this partition. So this is the partition, let's call this partition as P. So GP is by notation for this block graph. So it says that this error is no more than order one over root log k. So you see that the graph can be huge, but if I think of it as a function in 0, 1 and I want to function in 0, 1 and I want to approximate it by something which is like let's say epsilon then it suffices to actually partition the graph into number of blocks where k is such that one by root log k is epsilon. So the size of the graph is actually not very important the number of blocks only depends on only depends on the error. Okay? And one key thing, one crucial property of this distance is This distance of the cut distance is the following. That is what is known as counting lemma. Meaning that if two graphs are close in this distance, the number of triangles between the two graphs will also be the same. So also will be pretty close to each other. So actually, so for any graph H, so here is any graph H, so here is the statement: so fix H and graphons F and G. Then if you look at the density of H in F minus the density of H in G, that thing is order which is a function of it can depend on the cut distance between F and G. f and g so it says that if i am interested in the number of triangles let's say in my graph g or a function f and and there is another another there is another graph on g which is pretty close to my function f in the cut distance then the number of triangles in the two graphons are also very close to each other okay so this essentially sort of gives you that this strategy should be right so i i want to look at the problem of all graphs where the number of triangles is large All graphs where the number of triangles is large. The regular dilemma tells me that there is a block graph with not too many blocks such that this block graph is close to my graph in the cut distance, which basically means by the counting lemma that the number of triangles in this block graph is also pretty close to the original graph. Okay, and so basically it means that you can actually approximate so. That you can actually approximate, so plugging using the above. We got a couple of questions, people. So, rho is this even if i equals j. And the other one was other person was wondering if you want to use the contraction principle. Rho is this even if i equal to j. So, if i equal to j, then actually you count every h twice. Essentially, it's basically either you count every h twice or you here, you instead of looking at ai squared, you look at ai choose two. Looking at EI squared, you look at EI choose to whatever. Like, so basically, it should be the edge density. And the second question is: is it because you want to use contraction principle? At this point, actually, no, because I'm not proving a full large deviation. So, what I'm just doing is I want to just, so I already told you how to compute the property that the graph approximately looks like a block graph. And then the point is, I want to then union bound over all possible block graphs. Using the above Using the above, does that actually answer the two questions roughly? Yep, you're good. Using the above two facts. And we can compute the probability that G looks like a given block graph and then union bound over Over all possible choices of block graphs. Right, so the regulatory lemma tells me that it looks like some block graph. Neither do I know exactly what the parts are, nor do I know what exactly the edge densities are. So, what I can do is I can actually, because the number of parts is not too large, and the edge densities And the edge densities are all between 0 and 1. I can unit bound over all possible chests of block graphs, meaning basically consider all possible partitions of V into K blocks and all possible Edge densities rho ij up to an epsilon error. So, of course, there are infinitely many values of rho ij between 0 and 1, but up to an epsilon error. So, basically, you can take 0, 1 and divide it into a mesh of size epsilon. And you can look at all possible edge densities from this mesh value, all possible blocks which partition this full set, and then. This full set, and then for each of such block graphs, you can try to use this bound. The property that my graph looks like that particular block graph is this. And now I can union bound over all possible block graphs. And you will see if the union bound is over. Over a not too big set, the upper bound one gets is again e power minus whatever I wrote plus moderator term because remember This was the property of any particular block graph. And phi H n P delta was the best possible such block graph. So all these properties are at most e power minus this. Now if the union bound is of a smaller order term, then this will be still dominating the entire union bound even after union bounding over all possible blocks and edge densities. And so this is the claim here. So the point is this. This fails if P is going to zero with an faster than a polylog. Oh, sorry. And so the or okay, so the original, the full full LDP on graphons. Be on graphons for well, it's a there's a bit of a lag for a fixed V was proven by Chatterjee and Vardhan in twenty eleven and the argument above Which is more commentarial, similar but more commentarial was by Dubeski Emperor's I think in twenty fourteen, twenty fifteen, maybe. Yeah, so, but the point is: this union bound becomes too large, or the possible choices of densities and the blocks become too large if P is going to zero. Too large if p is going to zero. And this is because of this bound. So the error that I'm allowing here is one of our root log k. Yes, some of my iPad is hanging a little bit. Yeah, so the error that I'm allowed is one of the. Yeah, so the error that I'm allowing is one over root log k, and it turns out that if p is actually going to zero, then this error must also depend on p because you see that the number of triangles is like roughly p cube. And so because the error has to then depend on p, then k will become so large that the union bound will actually fail. So this strategy actually sort of is ideal for p constant, but it can be pushed to p going to zero with only logarithmically fast rate. Fast rate and it sort of completely fails when p is going to zero for with with as a polynomial in n. And so this is what actually we will sort of talk about tomorrow and the day after. But I'll also sort of start tomorrow's lecture with some discussion on what this quantity looks like. So, this is an abstract thing, which is a minimum possible. So, this is an abstract thing, which is the minimum possible thing, minimum possible entropy cost over all possible inhomogeneous random graphs. Now, as was asked before, why is it not everything becomes from P to some larger value? So, it indeed will be the case that for some values of delta and p, this thing is indeed, the solution is indeed that everything should be homogeneous with a different value than p. But then there are other values of p and delta for which this is not going to be the case. And actually the opposite. Which this is not going to be the case, and actually, the optimal result is not to change everything but change only some parts of the graph. And so, this is what we will start with tomorrow, and then we'll actually talk about how to sort of modify the arguments in order to be able to treat sparse graphs where P is really going to zero polynomial in the system size. And this naive union bond strategy will not work there. And one has to come up with slightly clever ways to cover your own space so that the total number of things that you are Total number of things that you are considering is not too big. So, I think I'm out of time. So, I'm happy to answer questions, but yeah, I think I'll just stop here. Okay, so first of all, let's give everyone the opportunity to unmute themselves and we'll thank Shushendu for this. So, I see a question here, and yeah, so basically, this result that I presented from Lubetsky Zhao, which uses this week. Which uses this weak regularity lemma. The bound, sorry, the whole thing is slightly slow, which is unfortunate. Yeah, so this bound here actually allows you to take p to be faster than, let's say, one over log into the one six or some sort of polylog. Yeah, so this approach, which is pretty similar to Chatterchain Varadhan, actually allows you to take p going to zero, but only very slowly.