So this is some work together with John Pearson and industrial collaborators at SurfX. This really came from this problem from our collaborators that then led me on an exciting journey through some maps I never thought I'd do, so that's quite nice. So the application comes from data assimilation, which is the main area I've worked in. I'm not going to talk about that now. If you're interested, we can talk offline. But basically, the people at Surfax are interested in forecasting. Forecasting weather for ocean. So, we're trying to include observations and improve the estimate of the state of the ocean, and this is one component of the system. And so, what we are interested in doing is applying a bunch of correlation matrices via this linear sequence of equations. And so, the reason here is we'd like to solve a problem that applies a kind of covariance matrix that has the form of a. Covariance matrix that has the form of A hat to the power L, but A hat one power has some nice structure. And so initially you could do this by kind of doing this sequentially because we have access to A hat as an operator. But the problem is these A hat matrix operators can be really large. What we'd like to do is exploit parallel structure and this is sequential, so that's not an ideal thing to do. So So, a key problem of working with industrial problems is your limited support your collaborators that have already implemented. So, I'm not going to talk too much about that in this talk, but really, this perspective is you want to try and design something that's going to be implemented in the existing code base. So that's one constraint that leads to some interesting problems. Okay, so I want to solve this problem, and ultimately, what I'm going to do is design some pre-conditioners, and I've got three steps. Steps. So, first, I'm going to reformulate my problem as a parallel and time or all-at-once problem, and then I'm going to identify some kind of ideal preconditioner. These are using existing techniques that have been pretty well studied. So, our new contribution is how do we actually try and apply a preconditioner in practice given the constraints of the system we'd like to use it for? Okay, so we can replace a R sequential problem by this all-at-once type approach. All-at-once type approach. So instead of solving L equations one after the other, we're going to solve a bigger system that has this structure here. Once we solve this, we'll get the same answer as before. So this is nice because we have kind of revealed some structure that maybe can be exploited. And the challenge now is I don't want to, I'm going to solve this iteratively, I want to design a new preconditioner for this system. So in this application, we're going to have relatively few We're going to have relatively few blocks of order at most 10, but the size of these blocks is going to be very large. Think of a discretized grid over the ocean. So I'm not actually going to have access to A as a matrix, but for the purposes of this talk, I'm going to pretend I do. So we have very large blocks and kind of few of them, which is a bit of a contrast to some of the all-at-once previous work. Okay, so originally we looked at this and we said, So originally, we looked at this and we said, well, this has a nice structure. Wouldn't it be even nicer if we had an additional identity in this top right-hand corner? And that's because then we have this nice block circulant structure. And if I have that, so this is going to be designed in my preconditioner. So this is almost my code A. And we can write this P in this chronicer form here, where I basically have the identity, we have copies of A appearing on the Have copies of A appearing on the diagonals with the identity, and then we have this C that copies the IN, where n is my big dimension, and C is going to be this circular matrix. So, this is nice because we have in kind of the first position of my two kind of quirks two circulant matrices. We can simultaneously diagonalize those and we can write P in this way that looks horrible, but it's actually nice. So, why is it nice? In my outit two brackets, I have an identity. That's pretty easy to deal with. Identity, that's pretty easy to deal with. My U are going to be my Fourier modes, so a circular matrix I can decompose by doing a Fourier transform. And then in the middle, I have something that looks, this is kind of not changed so much. And then we have that, instead of the C that we had before, we're going to have these lambdas. This is a diagonal matrix where on my diagonal I have roots of unity. So this is nice because what I want to do is apply the inverse as a preconditioner. As preconditioner and the outside brackets are trivial to invert using the properties of kinetic products and the properties of basically, I have nice things to invert. So I've replaced a hard problem with two easy ones and a slightly hard one in the middle. So I'm going to talk a bit more in a bit, but you can go one step further. But basically the the effort that we now need to kind of focus on is really going to be this middle bit here. This little bit here. So, this is a nice preconditioner. It's been proposed and studied before, and it's got some really nice properties. So, if we apply it kind of ideally, which is the exact precondition I had on the previous slide, we have that the precondition system is going to be a rank n perturbation of the identity. So, most of my eigenvalues are going to be one, and then there's going to be an equivalent of one block's worth of different. There's also an explicit characterization of what those eigenvalues are. Of what those eigenvalues are, which is really great. In this paper, they study a more general case where they have one matrix on the diagonal and the second matrix on the sub-diagonal can be general. So we're in an easier case because we have the identity. So that's really nice. And so we can basically explicitly characterise all of the eigenvalues of this system. And I'll explain why that's nice later. What is your mu meaning? So the mu's are going to be the eigenvalues of the a hat, the block. Of the A hat, the block. So I had an A hat block that repeated. If I have access to the spectrum. Yes. Yeah. So mu min is going to be the smaller. Yeah, sorry, that should be a mu bin. So mu is going to be eigenvalues. Sometimes lambda is going to be eigenvalues. Could there be negative buttons? Not in this case here. So in the general case, yes. Yes, so we have that a hat is symmetric and So I think in the McDonald's paper, they consider it a slightly more general case than we have here. But basically, our A hats are correlation matrices, so we've had quite a bit of choices in there. Okay, so then we spent some months working with that, and then we decided one. Sorry, one more question. So, how do you at least get assumption that the smallest CRFI basically would be at least one? So, we require it because. So we require it because um I'll talk a bit more about that later. Require it because of the old name of there, otherwise the conditions show. Yes, exactly. But the ECB are easy to use on the side. Yes, so for this case, we can approach one and things get worse as we get closer to one, which then motivates looking for this alpha circular thing later. So this was kind of an issue for a little bit. In practice, the spectrum of the AHAP that's used by our collaborators kind of That's used by our collaborators. I think it's really close to one as we discretize more finding, which could be problematic. So instead, what we can do is use an alpha circular preconditioner, which is where instead of just using an identity, we put an alpha parameter in front. So now we have a slightly different C. Before we had that C was circulant, so all of the entries were one. Now we've got one that's alpha in top right. And this is nice because as alpha goes. And this is nice because as alpha goes closer to zero, this is getting closer to the original matrix. So we can imagine that if I choose alpha as zero, then I'm going to have kind of a perfect preconditioner. Alpha is one is going to recover that. So pretty much a lot of the stuff I just said about the circular case holes here. So we have a slightly different formulation. We have the appearance of this gamma matrix, but that's diagonal. So including it in here, that doesn't mess up anything about the outer brackets being straightforward. Brackets being straightforward. And then we have in here that instead of lambda was a diagonal matrix of roots of unity, now they're just scaled by the alpha parameter. So this is also nice because we now, yes, we now have an alpha scaling appearing here instead. So if you're in a case where it's one, you can just choose alpha scale. So that's really nice. Okay, so again, it's a since we're going to focus on this for what follows, basically the difficult part is going to be this part of the Part is going to be this part of it. Again, the theory from before about the eigenvalues extends. So we still have a right-handed perturbation, we have an exact characterization of the eigenvalues, and we can see that an alpha appears here. We can see that we don't want these to be the same value, we want this value to be strictly larger, so that we have the ordering that we know what it is. But if that's an issue, because A hat is symmetric positive definite, we can just choose. And symmetric positive definite, we can just choose alpha smaller if that's going to be an achievement. Okay, so that's great. Problem solved, I can go home now. Apart from, I still have this challenging bit here. So, I've just written out what this matrix actually looks like. So, what we have, this kind of problematic part, which is what I'm going to focus on the rest of the talk, is you want to solve a block diagonal matrix, and each of the blocks is a shifted system. So we have A hat minus lambda one. minus lambda 1 times identity we want to apply the inverse of that to a vector. So here lambda i's are the alpha scaled roots of unity and I'm going to use mu to denote my values. And so I could do this exactly but that's really expensive because my problem really knowledge. So we are proposing replacing an exact solve with an inner iterative method. So we're going to look at solving these blocks iteratively. And one comment, often if we can decompose a half, If we can decompose A hat exactly like via a Fourier transform, we can actually make this inner bracket really easy too. So we do a similar trick. So we simultaneously diagonalize the second component and we have an outer bracket that's really easy to invert and then this middle bracket is just diagonal. So if you can do this, do this. Don't do the rest. But most of the literature of these block alpha seven. Most of the literature of these block alpha circuit approach preconditioners kind of just says, well, most of the time you can do this, so just do that. We're focusing really on cases where it's going to be very expensive or impossible to do this decomposition, which is the case for our application. So you can think even the numerical experiments I've been looking at, it will be very easy to do this, but for real cases, we have problems with like coslines being irregular and stuff, which make things a bit more tricky. So we're going to. So we're going to, in this setting, we can't do this, you can do this, do this. Okay. So when you are able to actually do this decomposition, this assumes you have access to our hat, otherwise farming will file. Yeah, exactly. So we can't do this. But for some, some, you might be able to. If you can do that, but we can't. Okay, so we're going to solve this problem iteratively, and our approach I'm going to present today is going to be. Proto-Method today is going to be using the same iterative method to implement this preconditioner as we're using on the outer problem. And the method that's being used by our collaborators is Chevy Chev semi-iteration. So I was not that familiar with this method. I was a quiet old gal. But this is used by our collaborators and it's got some nice properties for this particular application. So it's inner product-free, that's nice for parallelism. It's got some nice properties. So in the data assimilation application, So, in the data assimilation application, we often have very little budget. So, we have lots of processes, but we can only run a few iterations. That means we're very far from convergence. So, we can guarantee something in the convergence limit. That's not helpful to us. We really need these multiple properties in a few iterations, which CherryCherb allows us. But nothing is free, right? We're just moving the word. So, the difficulty with CherryChave is that you require estimates of the extreme eigenvalues with the The extreme eigenvalues of the system we're solving. So if we're solving A to equal B, you need estimates of those extreme eigenvalues. If those estimates are bad, your conversion is going to be bad. So if we're using a precondition system, we need estimates of the extreme eigenvalues of the precondition system. That's why I was really happy that we have these kind of exact, we have these results for the eigenvalues of the precondition system for this alpha side. So that's great. So that's great. So I will say we also assumed that in order to get our extreme estimates we needed good, we needed eye queries that they had the block, which we are assuming are available. They can be computed quite accurately in these systems. So in general you could use any method for Ax equal to B A A half x equal to B short-term recurrence like lunches. Recurrence like clunches, and then update the shift. I mean, this is available in the shift for the inner problem, you mean? Yeah, for all the shifts. This is your problem is A plus shift is equal to B. So with a varying shift. Yes. So this is a with A symmetric, this is not, it's available in the literature. Is available in the literature? In the literature, yes. In the code base of our in the code base that we're using, I don't think it is. Same iteration is just a possibility. Yes, exactly. And for which you need some spectral estimates. If you just use Launches or CG, you don't need any estimate. Yes. Yeah. So in our case, the outer problem is already solved using Chevy Chef. So the the assumption is the extreme wide votes are already available and like Are already available and like it has nice properties for this problem. I agree that you could do a different method. So we're using Chevy Chevroleti's inner problems because it's already been used outside. So all our assumptions are satisfied. But yeah, it would be interesting to see if that's the best way forward in general for this problem. Okay. Are we going to say what exactly the infrastructure should they have is I mean you're doing MPI, but is that what should be used? Uh no, because we don't have yet any but so it yeah. It's a work in progress. It's a work in progress, but yeah, also that's one of my global vision. Okay, so what I'm going to be doing is looking at these sub-problems where I'm solving A minus lambda I times I n. So lambda I is a root of unity. So I'm also interested in what happens if I choose a small number of iterations. So if I do a large number of iterations and my tolerance is very small, I can kind of numerically recover the exact group. Numerically recover the exact preconditioner, but I'm interested in what happens if I don't have very much budget to do this. And the kind of fun/slash challenge is that most of the roots of Unity are not real, turns out. So a longer theory is for a TV check that gives you nice results is for S P D matrices. And if you shift by a complex value, your matrix will no longer be S P D. So I spent a lot of time bashing my head against the wall, but we have something now which is exciting. But we have something now which is accepted. Okay, so the SPD case, so my plus and minus one, is really easy. And in our case, we're taking always an even number of blocks, so we always have a plus one and a minus one for each. Okay, so if we have an SPD matrix, I know my extreme eigenvalues, I want to solve mx equals b to a given tolerance, epsilon, then I can guarantee convergence to that tolerance in p star iterations. Well, so p star may not. So, p star may not be an integer, so just take the smallest integer by the p star and have this expression here. As you might expect, it depends on the tolerance I choose and my extreme eigenvalues of my matrix. So that's nice. It's nice, particularly in our case, because we have this plus one and this minus one. So, what you can do is say, if I solve a matrix problem where I shift by, here I've put in a more general case, but if I solve by n minus the identity. Solve by n minus the identity and m plus the identity, I can compute how many iterations would I need of both methods to reach the same tolerance. So when you do that, you get some results that you get a ratio that's related to the basically these expressions. So we have a condition number of both of the shifty systems, either a ratio of a minus one or a plus one. We can get two ratios. And the conclusion here is that this. Is that these values, we're going to need more iterations for the alpha plus one plus identity case than the A minus the identity case. So that means to reach the same tolerance. Or equivalently, if I use the same number of iterations, I'm going to solve the A plus the identity case to a worse tolerance than the A minus the identity. And we'll see some numerics later. So that's a bit confusing. I've got some numerics that might have descripted. To the minor description. Okay, so we know that the plus one, A plus the identity, is going to be hard. A minus the identity is going to be a bit easier in terms of iterations. What about all the other roots of the identity? Okay, so this is in the literature. Most of the time what you do to get these providers results of Chevy Chev is we say that we have to find an ellipse in the right-hand plane. So this is partly why we don't want to be close to one, because you really can't have this ellipse crossing. Really, you can't have this ellipse crossing the origin. So, we want an ellipse in the right-hand plane, symmetrical about the real axis, and then the foci of these ellipse, this ellipse, is going to replace the kind of eigenvalue, extreme or eigenvalues that we had in the SFT case. So, this ellipse is not unique, and kind of if you choose a bad ellipse, you get bad convergence. If you choose a good ellipse, you get better convergence. Choose an ellipse is hard. I don't do geometry. So, this looks quite challenging. And we found numerically that we get pretty Found numerically that we get pretty good convergence if we just choose this ellipse. So, this is a very skinny. It's an ellipse, it's just very skinny. In fact, it has no... There is an integer on this. Yeah, okay, I'll be very interested in it. In the 90s, there were papers on shifting with a complex shift asymmetric positive definite matrix using all sorts of things. So I'm sure you can. Yeah, okay. Yes, that would be great. So, yes, so we have done something, but if it already exists, that's not me. I'm not talking about my own work, I'm talking about literature. Perfect, yeah, I'd be really interested. So, what we can do is this is okay, so we're in a very special case where the shift is exactly the same. And so, basically, the argument for this ellipse method is that you you get an ellipse and then you map an ellipse to the the unit circle about the origin. Unit circle about the origin. And so, what we can do instead is because we have this very spectral ellipse, we can pick this straight line and we can find the mapping that maps this to the right thing in the end. And then we can use that to compute rates of convergence. So, we follow a similar argument, but we don't need, we're not restricted to this case where we have to have an illness that's a metric about the real axis. Okay, so what happens now? If I Happens now. If I have a shift that has a real part and a complex part, and I'm going to solve the problem a hat minus lambda in, what I do is I apply chevy chip with the points, yes, with the points that I showed on the previous slide, so they have a real part and a complex part. I can bound the convergence of Chevyshev applied to this approach with the complex endpoints by just considering the rate of convergence with the real. Considering the rate of convergence with the real part of the shift. And the real part, if I do a real part of the convergence with a real part of the shift, I can use all the SPD literature, all the results. So we can get an upper bound on the rate of convergence, the asymptotic convergence factor, that just depends. We pretend that we don't shift whether this converges are. And so what that means is that kind of as we go around the circle, like the The circle, like the plus one case, the plus one case is the worst convergence, the minus one case is the best. As we go round, we go from worst, we improve the rate of convergence to reach as we go round. And we also have that complex conjugate pairs have the same rate of convergence, like the same boundaries of convergence. Okay, so we have a result that's maybe a bit confusing. Let's look at some numerical results to show that bitmaking works in practice. That it maybe works in practice. Okay, so I'm now doing a very simple numerical experiment where I have a diffusion operator, we've got regular grids, so in practice I could just do a method of T of this and not save myself all the letter. But we're going to extend to some realistic experiments. So I'm going to apply Chevy Chevro to my outer problem and to build my preconditioner. I'm going to use the same inner and outer tolerance. That's a lie. I've used different types of health. Okay, so the first question. So, the first question is: what's the choice of alpha that's good? So, for some other alpha circular work, there's been some quite technical study of how to choose this alpha. Normally, it's interaction between, if you have multiple matrices, their spectra. Here, I think those results just say you should use the smallest alpha possible. So, you can see here that as we reduce alpha, we get kind of perfect conversion to model iteration from 10 to minus 5. 5. But this is using the exact preconditioner with no inner iterations. So I don't actually want to do that, and that's going to change to some plots for me. Okay. So if I do, I'm going to call this approach one. I'm going to say, let's just assign the same amount of effort to each of my sub-problems. So here I have six sub-problems, and I have varying levels of kind of itch total iterations per iteration available to me. So here's a high resource and a very low resource. High resource and a very low resource. If I assign it evenly, and my first case preconditioner, where I apply it kind of exactly, it only takes eight iterations. If I have unlimited budget, unsurprisingly, I can match that performance and get eight. But as I make things, as I reduce the amount of iterations, I basically get worse convergent. And then at the end, I don't actually reach the desired tolerance, even though it stops. And we can see that. And we can see that, so this is averaged over each of the outer iterations. We can see that in this top one, we're really reaching convergence for most of the sub-problems, and then maybe not for this one, but then we're not reaching convergence as we reduce the amount of available resource, we're kind of not reaching convertence in any specific case. So that looks a bit sad, really. But the idea is that we can use the theory we had before to say, well, we know this. To say, well, we know this one's really hard, and we know this one's pretty easy. So, why aren't we saying do the same amount of work for both of them? So, what we can do is use the theory and say, we'd like to solve each of the problems to about the same tolerance. And we know for the real cases, we know exactly what that ratio should be. And for the complex cases, we're going to use that rate of convergence bound in the computer before to design a way to allocate the resource. So this is kind of the rule that we need to do. The rule that we need to do. And so, what we have is we have our alpha, we have our rates of unity, and we have the extreme eigenvalues of the A-hat meters. That's access to those all-scalars. I compute each of these approximate rates of convergence, so they're exact for the plus and minus one case, and they're the bounds for the complex case. We have this thing, and then we can basically compute a ratio. So, if we have a hundred, if we have a total amount of resources we can use, we can say how much we're going to have a good deal. Can use, we can say how much we're going to have with each subproblem. And this just requires scalar computations, it's really cheap to do, you can do it ahead of time, and we're going to see how that happens now. Okay, so if I do the same experiment as before, instead of allocating one-sixth to each problem, we can see we allocate most of the effort to the plus one problem, not very much to the minus one, in between to the other, and the conjugate pairs have the same allocation. What we're doing here is we're basically solving. We're doing here is we're basically solving each sub-problem to the same tolerance rather than solving one really well and one really, really badly. And this means that as we reduce the resource level, we can maintain kind of ideal convergence for longer, and we can also, even in these very low resource levels, converge to the solution. So that one issue is, I said, I wanted to parallelize, I've just made one problem, have way more effort. But we know what these numbers are ahead of time. So you could perhaps group the expensive. You could perhaps group the expensive one and the cheap one together. Bam. Okay, so those were some very small test examples, and these are all with alpha equals one. But I already showed you that alpha was equal to one was like the worst. So what happens for these performance? So here on the left, we have a high toler uh high resource, so I have basically a limited budget. The black line was the ideal preconditioner, blue is if I just do the same amount, red is if I balance. Red as if I balance and kind of do uneven pre-scores. We can see that they can all quite well match the ideal preconditioner. We can't get down to one iteration, but we can get down to two. So that's my budget. But I'm not interested in this case. I'm interested in what happens if I have very limited budget, which is over here. And we can see that we get much worse performance than the kind of ideal case. And in particular, this alpha is water case is quite bad. We do see an improvement in a reduction in the number of iterations as we In a reduction in the number of iterations, as we decrease alpha, but only to a certain point, at some point we start getting worse performance. And we also see that there's more difference between the don't balance and do balance case gates, like similarly to we saw in the experiments before. So here we're going to use going forward, we're going to compare one, alpha is one, and alpha is ten to the minus two, because that kind of seems numerically to be for a bunch of different test cases, it seems almost properly. It'd be nice to be able to say something a bit more about. It'd be nice to be able to say something a bit more about that, but I haven't got there quite yet. Okay, so another problem is that I've improved the iterations, but that's by making my precondition more expensive. So is this going to save me effort overall? It depends, is the answer. So on the top, we've got the improvement to the residual with the number of iterations. And black here is if I use no preconditioner or a very basic preconditioner, the solid line is the best case. The solid line is the best case. So, at best, we can much more see it. Blue is my even for each sub-problem. Red is that I use my allocation rule. Solid is very low resource, and then dotted is the highest resource. So we can see that we do much, much better in terms of iterations. This is for alpha is equal to one on the left. But to do this, we require many, many matrix vector products. The matrix vector products are quite expensive. So, actually, although we have fewer. Actually, although we have fewer iterations, we require more matrix vector products than even just remote using a food conditioner. So that's not good. But the good news is that the alpha is equal to one case, which is really bad in terms of like we can do better by choosing a smaller value of alpha. So on this panel, we're looking at alpha as 10 to the minus two. We reduce the iterations quite significantly compared to the alpha's one case. And we can see here that for some choices of residual, we are beating the number of major expected products. Number of matrix vector products compared to the current choices of pre-positional. So this is going to be, I mean, yeah, so we can do a reduction, which is good. But it's going to be quite careful choosing your tolerances and stuff. Okay, some very horrible tables. Basically, the conclusion here, we're interested. I presented some really small block sizes, but I told you that my block sizes are so large I can't store them in memory. So what happens is we scale. So what happens is we scale. The top panel alpha alpha is one, the bottom is for alpha as not one. We can see that we get improvements that basically, and then these are the kind of budgets that we're choosing. So this is the left rightmost column is high budget. And in brackets, I've put the number of matrix fighter components. So we can have basically as we can get much better in terms of reduced iterations. Reduced iterations and matrix vector products by choosing a smaller value alpha and by doing this uneven balancing thing here. So we get the outer iterations look quite good and the matrix vector products tends to be quite high as we get bigger. Cool. My last result. These problems will do better as you have more blocks. That's not the situation we're in in our application, but basically you get good performance as we increase the number of blocks. But that's more for interest, maybe other applications. More for interest, maybe other applications. Okay, there we go. That's the conclusions. These alpha cycling preconditioners have been quite well studied. We're applying them here to a very particular application. We're interested in applying them with turbo shares because of the constraints of the operational implementation. We've got an approach that we think will work well. We get some reasonable scaling. These experimental results are quite preliminary, and we're hoping to do an implementation in our partner's operational framework to. Our partner's operational framework to see if it actually works in practice, which kills questions. So, thank you. Yeah, so I don't remember actually at the beginning you needed the smallest eigenvalue of a half to be larger than alpha. Than alpha. Then you say, well, we just choose alpha to be sufficiently small. But then I will be careful with that because the conditioning of your scale discrete Fourier plus Fourier matrix, be Eigen matrix or alpha circulate matrix blows up by using two small alphas. That's true. And that's an issue that is kind of like well known in the publishing time community where it would make problems for instance. So I would So, I will be careful with you too small values on cost. Yes, I agree with that. So, in practice, the matrix, the kind of problem that we're using for these practical applications, typically the smallest eigenvalue of a hat tends to the one as you refine that mesh, which is problematic if you're not using the alpha circulant because you get very, very close to one. So, I think you could pick alpha just not that much more than one. I just not that much more than one. But I think, yeah, and I think there's this difference in this practical implementation versus the ideal precondition, even for these small problems, right? So I think you wouldn't want to go really, really small. But yeah, that's the difference between final thanks to all of them. So this is So it's just a couple of things then just please be aware of. Okay, so lunch is quite early, so you can go to lunch. I'm going to lie.