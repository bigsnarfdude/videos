 Having me here and I sincerely thank the organizers. I thank the organizers for having me here and organizing this wonderful workshop. This is my first workshop, I think, in two years, after the pandemic in person. So I'm excited about it. And I'll be talking about Gaussian graphical models, massive dimensional Gaussian graphical models. Yeah. So we are interested in learning the graph of or the conditional dependence independence relationship between Gaussian distributed variables, which is characterized by the non-zero orthodiagonal entries of a position matrix. And in this paper, what we have done is we have We are proposing a factor model for the precision matrix, which used to be generally known for the covariance matrix. So, this is what I'm going to talk about. So, in many applications, learning the dependence structure between the observed variables are quite important. So, application can be in genomics, and we may in massive dimensional gene expressions. dimensional gene expressions we we the interest may be in learning how different genes are dependent and studying the dependency graph is often important in like immunology or like cardiomogenic disease processes also application can be in finance and all of us want to get rich i hope and we may be interested in seeing how different stock prices Stock prices are interrelated. So starting the graph between different stocks can also be can be important. And these are possible applications of studying the graph. So the correlation coefficient is a measure of linear association. However, in the presence of other variables, correlation coefficient can be misleading. So it Misleading, so it can be spurious. So, often we prefer the partial correlation for conditional dependence information. So, how is it defined? So, let suppose we observe d-dimensional random variables y1 to yd. And without loss of generality, let's assume that we are interested in studying the partial correlation of y1 and y2 conditional on the rest. So, the partial correlation coefficient between y1. Coefficient between y1 and y2, condition others is given by this formula. So it's the conditional covariance divided by the respective variances. So for Gaussian distributed data, this partial correlation of rho equal to zero implies conditional independence between y1 and y2. Now the conditional dependence information is actually encoded in the Information is actually encoded in the precision matrix omega. So if omega jj dash is the jjth element of omega, then this partial correlation can be written as this quantity. So where omega 12 is the one twelfth element in the prerequisite matrix, and these are the respective diagonals. So this step is not very immediate, but with some linear algebra tricks, we can. a linear algebra tricks we can come up to this so to study the uh conditional to study the conditional dependence independence uh relationship between a d-dimensional variable if we can observe their precision matrix and see which of diagonal elements are actually uh non-zero we can we we can we can actually study the graph now in high dimensional problems the precision matrix can be sparse so many very So many variables can be conditionally independent given the others. And we want to model that efficiently. Now there are neighborhood selection approaches and what they do, they regress each variable on the rest with penalty on high regression coefficients and they do it sequentially for each variable and that and they infer the graph. Now there are BJ. Now, there are a hierarchical prior on graph approach where first priorities assumed on the graph, that is, which conditional, which variables are actually of dependence and not. So, that is the graph. And a priority is assumed on the graph first. And conditional on the graph, then a prior on the precision matrix omega is assumed. Then, there are direct maleization approaches, and what frequentist people do is What frequentist people do is they often penalize the entire position matrix and aim to optimize the penalized likelihood and to get the point estimate. And what Bayesian people do, they have two approaches. So, one is they first assume shrinkage prior on the precision matrix. There are MCMC-based approaches, for example, in Banati and Goshal, where In Banati and Ghoshal, where they do MCMC to sample from the posterior distribution of posterior distribution of omega and then do posterior inference on that. And there are other recent approaches by Gannon and authors, Deshpande and authors, where they optimize the posterior distribution to get the Bayesian map estimate. Now, what are the problems in the neighborhood selection? are the problems in the neighborhood selection approach the estimated is the estimated precision matrix is often not positive definite that can be an issue now uh there now the hierarchical paradigm approach it is not really scalable to high dimensional problems and their application can be prohibitively uh restricted to problems only to problems limited to only a few tenths of dimension now the frequentist and vision map based approaches lack uncertainty quantification because we only get a point estimate with Because we only get a point estimate with those approaches. And the incensive Bayesian methods lack scalability because, while doing incensive, we need to ensure that we are sampling from the space of positive definite matrices. So that can be pretty challenging when the dimension is really high. Now, there is a substantial overlap between the literature of covariance matrix estimation and precision matrix estimation. precision metrics estimation so essentially the uh the like the penalization based approaches can be most of most of those can be slightly tweaked to estimate both predation matrix and covariance matrix and they share their own pros and cons however there exists the factor model approach for covariance matrix estimation so in factor model what we do in factor model what we do we assume that the covariance matrix sigma admits this kind of a representation Uh, admits this kind of a representation. So we assume that sigma equals lambda lambda transpose plus delta, where lambda is a d cross q order matrix and delta is typically diagonal. So this in principle, this presentation has full support because if we set q equal to d, then any positive definite matrix can be written in this form. However, in practice, empirically, we have seen that often a much smaller q compared to uh compared to d suffices to uh suffices to uh estimate uh sigma really well so this model is really attractive because uh we have this equivalent representation we can write y equal to lambda eta plus epsilon uh in this latent fact in this latent formulation where etas are the q d etas are q dimensional latent factors Dimensional latent factors following idiosyncratic Gaussians, and the epsilon can be interpreted as errors, Gaussian errors centered around zero and coherence matrix delta. Essentially, they're independent because we assume delta to be diagonal. Now, this latent variable construction allows for a scalable computation, be it frequentist or Bayesian, we can straightforwardly design an EM algorithm or a GIF sampler with that. And in this approach, if our goal is to If our goal is to estimate the marginal coherence matrix omega, then we do not need to impose any additional condition on lambda and delta. So while doing MCMC in the posterior search, while doing the posterior space search, we can just it's totally unconstrained. And if our interest is only estimating sigma, we do not. only estimating sigma we do not need to we do not need to bother ourselves for separate identifiability of lambda and delta so this allows scalable computation and if if there's reasons to believe that if there are reasons to believe that sigma is sparse sparsity can be straightforwardly induced by penalizing lambda and there are works on that and this also allows inherent dimension reduction Also allows inherent dimension reduction by setting q to be much smaller than d. However, such latent variable construction is not immediate even if we for precision matrices. So even if we assume that this precision matrix omega follows admits this kind of a representation, this latent variable construction does not hold. So it is not immediate how we can how we can use such tricks for position matrices. position matrices well one can say that uh one can say that uh i would uh uh i would estimate sigma using the position matrix using this factor model and then take its inverse as the estimate but it does not show it shows poor empirical performance and there are works on that and also sparsity in omega and sigma are not necessarily exchangeable so if if we know that sigma is sparse that does not imply that Sparse, that does not imply that if we know that omega is sparse, that does not imply sigma also has to be sparse, and we don't know how to model that in such case. So in this work, what we have done, we have actually using a latent factor representation for the precision matrix also. So here we assume that the data are coming from a Gaussian distribution with precision centered around zero. Precision centered around zero with precision matrix omega, and we assume that omega admits this latent factor representation with lambda lambda transpose plus delta where lambda is d cross key order and delta is a is a diagonal matrix. So here's the result. If y follows this, if the precious matrix can be written, admits this form, then y can be Then y can be expressed in terms of latent quantities ui and vi in using this identity, where ui is a q-dimensional, vi is a d-dimensional, and the joint distribution is given by this. So even if so for for a latent variable construction actually exists for the pre-scient matrix as well. So all the benefits we had for for for the For the classical factor model in covariance metrics setup, we can also have those in precision metrics. So, this is not immediate, but what we can see that using the Sharman-Nood body identity, the covariance of Y can be expressed in this form. So, this is not immediate, but once you see this, it is obvious. So, only one Vota Chajan others, they actually Others, they actually derived a factor analytic representation to sample from this distribution. They used a very similar latent factor construction. So if you remember his talk from the first day, he actually cited this paper and in his talk, he talked about a regression problem with 98,000 covariates. And he was running a full-blown MCMC on that. name simce on that and he was actually using i believe he was using this a similar construction but what they do they are actually uh using such a construction for sampling from this distribution for given the parameters what we are doing we are trying to learn these parameters given the why given the data that's what we are trying to do okay Okay. So now sigma can be sparse in practical applications, in many applications, sigma can be sparse. So how do we impose sparsity? Now we see that a sparse lambda actually favors a sparse omega also. So here we considered a toy example where each of the factor models are where we consider a sparse lambda and it actually Uh, lambda, and it actually uh, and the off-diagonal of omega is actually uh comes for entirely comes from the the from the cross product of lambda, and we can see that this is also sparse. The gray ones are zero, so a sparse lambda actually favors a sparse omega. So, sparsity can be induced in the precious matrix by penalizing this lambda. So, to get more insight, we considered the spike. More insight, we considered the spike cancel a prior on the elements of lambda. So we assume that given a that each a priority, each element of lambda is a zero with probability pi and some continuous distribution with probability pi. And typically, we assume a conjoid beta prior on pi. So for this pi-canceler prior, we can actually analytically derive the probability that an element, an off-diagonal element of omega to be zero. We can analytically get the probability. Analytically, get the probability. And here we plot the probability versus pi for different values of q that is the latent dimension. And we can see that by adjusting pi, we can have our desired level of sparsity also in the off-diagonal elements of the precision matrix. So if we choose a pattern pi, hopefully it can learn from the data and adapt such that the sparsity pattern in the precision matrix. Sparsity pattern in the precious matrix is learned. Now, to complete the specification, the model specification, we describe our choice of prior. So we consider the digital Laplace prior on the Lambda matrix. So on a D-dimensional vector, the Digital Laplace prior with parameters A and B is formula. Is formal is formalized as this. So each element theta j conditionally follows a normal distribution with variance psi j, phi j square, tau square. The psi j's are exponential half and the phi joint phi jointly follow a digital distribution and tau is the gamma distribution. So here this phi and psi and phi are local shrinkage parameters and the tau is the global shrinkage parameter. So the spike cancel prior Spike and slab prior is essentially a discrete mixture and is a mixture of two distributions. And while doing posture inference, we need to do stochastic search on the discrete space. That can get challenging in many when the dimension gets higher. So, due to that fact, the continuous Schengen priors appeared in the literature, and there's a very literature. There's a very literature on that, and which shows that the continuous linkage priors can have the desirable properties, and therefore we considered the continuous linkage prior here. So, for B equal to half, we get the exact definition by Vertica, only one Vatican and authors, their formulation. And so, they have considered B equal to half at the default choice, but what we have seen is by setting the value of. By setting the value of b appropriately, we can have better control on how much sparsity we want to impose on the model. And we let the we assume a D shellaplus prior on the vector is lambda. So we are we are a priority, we are assuming equal sparsity, uniform sparsity on each element of lambda. So as regards the prior on delta, that is the diagonal element, we consider a ditch-le-process prior with base measure alpha. Prior with base measure alpha with the concentration parameter alpha and base measure g naught, which is a gamma distribution here, and then again follow a gamma prior and alpha. So the reason behind considering a DHLA process is that in practice, many of the delta square can be quite similar. And we know that DHLA process allows for clustering. So using Clustering. So, using the DCL process prior and clustering, we can borrow information across different delta squares and can have improved inference. So, essentially, it allows for a data-adaptive additional parameter reduction. Okay, so now we need to do posterior computation. So, in any application, actually, Q would be unknown. The latent dimension Q would not be known. So, what we do so. So what we do, so we can choose a prior on Q and then in the posterior, we need to go for something like reversible jump MCMC to sample from this trans-dimensional distribution, to sample from to have these trans-dimensional proposals. And that can be quite daunting in many applications. So instead, we consider an empirical based type approach to estimate Q a priority and initialize lambda and delta. initialize lambda and delta by our by any method. So once we have this lambda and delta, we sample the latent u and v using this formula. So we generate u from this Gaussian distribution with mean zero and covariance matrix P given by this independently from Y and set V to V to with this. So for this step we require the Q we require The Q cross, we require the factorization and inversion of this P matrix, which is a Q cross Q order. So, Q is a Q the order of Q is small. And in this step, we can avoid any high-dimensional matrix inversion. So, remember that the data are D-dimensional. D is high, but Q is small. So, we are able to avoid any D-dimensional matrix inversion. Then, in step two, we need to sample the rows of lambda. Sample the rows of lambda so let lambda r be the rth row of lambda uh i am not going into the algebraic formulation but these are just uh we can these are some quantities we need to calculate and with that uh we can sample the the jth row of lambda the rows of lambda sequentially uh using using a Gaussian distribution so the covariance matrix these covariance matrix here This covariance matrix here is actually diagonal. So, even in this step, also, we do not need any high-dimensional matrix inversion. So, in both these steps, we can avoid any high-dimensional matrix inversion, which makes the algorithm really scalable. Now, in the third step, remember that we have a DP prior on the delta squares. And for what we have done, we have sampled the delta squares using the algorithm eight of Neil, but one. Using the algorithm age of Neil, but one can use anything. This is what we have used, and it worked really well for us. And to sample the Digital Laplace hyperparameters, we use the GIF sampler in the Vortecheri and authors paper. And it is also, it is a very, one reason we consider the Digital Laplace prior is because it is really scalable and fast. And it also does not require any matrix inversion. So for details, I will. Version. So for details, one can see this paper. So what's the so it is important to note that our approach or model has nothing to do with this particular digital Laplace prior. So one can use any shrinkage prior as long as it conditionally takes a conditional Gaussian distribution. Takes a conditional Gaussian distribution. So the examples include Spike and Slap prior, the Horseshoe prior, the Lasso prior, the Spiken's Lab Lasso prior. One can use anything. We just use the DCLAP as part. So it is straightforwardly adaptable to any syncage part that takes a conditional Ligausian form. So now we need to choose the graph. And how do we choose the graph? In my earlier slide, I said that the graph information is encoded in the The graph information is encoded in the zero and the zero non-zero elements of the precision matrix. But in continuous shrinkage priors, exact zeros are not observed. So typically we would get, typically after posterior sampling, we would observe something like this. So here, many of the op diagonals can be awfully small, but not exactly zero. So this is not something. This is not something related to our method. This is actually an artifact of continuous string catch prior. If we go for an MCMC-based approach with any continuous string catch prior, we will not observe exact zeros. So the question is, how do we infer about the graph from this systematically? So what we have we have to answer this question systematically, we consider this multiple hypothesis testing problem. So we assume that the null hypothesis we take for The null hypothesis: we take whether so here rho is the partial correlation derived from the precision matrix, and we assume whether that the absolute value of rho is small versus whether it is not. And we take the decision rule as this: as this, so if the posterior probability is exceeds some threshold, we reject H naught and say that that particular age is. That particular age is important. And we control the, and this decision rule also controls the posterior false discovery rate at level 1 minus beta. And in a 2004 JASA paper, Peter, Christian, Judith, and Giovanni Parmigiani, they showed that this decision rule also yields the lowest false non-discovery date. So this Decent rule has attractive theoretical properties. Now the Now, the so what we have here, so with this, we can get the graph, and our estimated graph is actually associated with an FTR value. So, our estimated graph is associated with an uncertainty, measure of uncertainty quantification. So, this graph selection procedure actually has nothing to is actually independent from our precision factor model, but as long as one has MCMC samples from From on the precision matrix, one can go for this. But since we are really scalable and we can efficiently sample from the posterior, we can get more samples or our estimate of these posterior probabilities in this approach, having these probabilities can be much more easier than other. Having these probabilities can be much more easier than other approaches and we can use this procedure so we study some theoretical properties of our of our approach. So here we assume that the data the precision matrix of the data admits this representation. Here we assume the diagonals are all equal and lambda naught and omega naught, the true precision matrix. Omega naught, the true precision matrix can be factorized in lambda naught plus lambda naught transpose. And the lambda naught is a q the number of columns in lambda naught is actually q naught n which is unknown. And here we need these technical conditions. So I would like to briefly describe what the conditions mean. So C1 and C2 imply the requisite sparsity condition. So we introduce this sequence Sn and increase. Increasing sequence of positive integers to infinity, which is of order log dn. So, here we attach this suffix n to d because we let the dimension of the data to also increase with n. So, we consider the high dimensional setup. And we with S n squared to be order log d n, we assume that in each column of lambda naught, the number of non-zero elements are Elements are the number of non-zero elements is Sn such that the singular value, the smallest singular value of lambda naught is positive, and the highest singular value or the spectral norm is of order of one so that it does not blow up. And yeah, and also we can let our from the conditions we can let our dn to increase in exponential order of n so we can. Exponential order of n, so we can actually recover really high-dimensional matrices. And C2 and C3 ensured that the marginal variance of the data lies in a compact set so that the signal to noise ratio is maintained and so that we can recover the unknown parameter, the pre-share matrix here. So, with this condition, we derived the posterior contraction rate. So, I would like to give some more intuition on the result. some more intuition on the result so uh here we assume that our uh our uh in our postulated model we set lambda uh we set our omega n to be lambda n lambda n transpose plus delta n square so here lambda n is a dn cross cubed order and we fix this dn so in practice also we fix dn using some empirical based type approach and here in for a theoretical results also we fix this uh sorry we fix this qn Sorry, we fix this qn and that is the latent dimension. We fix it, and as long as the qn satisfies this property, uh, our result holds. So, what this property says is that, so as long as qn is greater than the q0n, that is, we are, that is not, it is not very small, it is actually greater than the true q0n. However, at the same time, we need to ensure that it's not too large also. So, if these two conditions are ensured, our Two conditions are ensured, we can still recover the precision matrix. So, however, the rate is inversely proportional to Qn. So, if we the smaller the Qn is, the better rate we get. So, if Qn is larger, the rate worsens. So, the idea is since we shrink all the elements of Lambda. The elements of lambda uniformly using a shrinkage prior. So the prior actually shrinks the unnecessary elements and lets us learn omega as long as the model is not too mispecified. So with some, so if q further, if q naught and qn these are of order one, and if the dimension is of polynomial order of n, the beta can be greater than order of n, the beta can be greater than one so that the dimension can be larger than the sample size. And in that case, we have shown that the contraction rate is up to log n cube term of the minimum x-rate lower bound. So in this paper, we have also derived a lower bound of the minimum x-rate and we see that the contraction rate is log n cube of the minimum x-rate, which is generally considered to be a good rate for the entire posterior distribution. For the entire posterior distribution. So, now to see how our method does in practice, we do some simulation studies. We consider three scenarios. In the first scenario, we sample the data from an autoregressive process of order two. In the second scenario, we sample the data from a banded precision matrix. And in the third situation, we generate data from a precision matrix, randomly generated sparse precision. A randomly generated sparse precision matrix. And on the right, we have the corresponding graphs. These are the simulation truth. And these are the results obtained by our precision factor model. So they are not perfect, but the structure is mostly recovered both in the graph and the precision matrix. Now we need to compare it with other methods. So we compare it with a method by Gannon and authors. This is a recent JASA paper. is a recent JASA paper where they considered the posterior the direct analyzation approach with spike and slab lasso prior on the optiagonal elements and we are doing much better than them in terms of estimating the precision matrix as well as the graph then we compare it with the nevaro selection method by mayneson and bullman here also we are doing much better and in the third case we are And in the third case, we are comparing with the graphical lasso by Friedman, and here also we are doing better. So, these figures are from 50-dimensional prediction matrices. So, in 50-dimensional problem, it's easier to visualize. So, that's why I considered, but I began my talk saying that I'll be talking about massive pre-shan matrices. So, if I say that, so if I show you results from 50 dimension, it should not be. Results from 50 dimension, it should not be not fair. So, what we have done, we have ideally gone to higher dimension, we have considered 50, 100, 200, and 1000 dimensional problems. So, these are, so in these three cases, in this, we see how the Frovenius, what the Frovenius norm is from the true, from the estimated Precision matrix and the true Precian matrix. And we see that in the autoregressive case, in Aggressive case in lower dimension, we are doing comparable, but in thousand dimension, we are doing a bit, we are doing worse than them. In the bandit case, we are doing comparable results. We are having comparable results. And for the third case, where it was generated from a random sparse matrix, we are doing sure. So, what are the differences? what's the difference between the probene isn't the difference between noise estimator and the truth is that what you're showing yeah the provenius norm between the true precision matrix and the and the posterior estimate so we have considered the mean of the mcnc samples as the posterior estimate of the precision matrix yeah so for 50 100 and 200 the number of samples were 100 uh for Square 100 for thousand dimensional problem, the number of samples are thousands your conditions for your assembly results are not even validated because you had a D or you were pen or something like this. Yes. I'm not saying it's wrong, I'm just saying it's doing a good job despite the fact that I don't know okay, sir. So it's not the same. No, actually we have considered a large dimension, small and problem. So in the 200 dimension, our sample size is 100. So we are considering a situation where we have more data than the sample size. Data than the sample. Also, in the thousand dimension, the d and n are equal, but if you see how the number of unknown parameters, it is actually thousand times thousand by two, something like that. So the number of unknown parameters is much, much larger than the sample size, actually. Yeah. So now we have also considered, see, we also see what the sensitivity and specificity are. So all the methods exhibit similar. So, all the methods exhibit similar specificity, but our method shows much higher sensitivity in almost all the cases, except the AR case. So, in the AR case, we try to understand what is happening. So, remember that we generated data from autoregressive process of yeah. Focus on the resource. Factor analysis on a factor analysis on the precision method. Factor analysis on the precision matrix. So, now the question is: do you have to worry about identifiability? No. No. So, in fact, if you want to estimate the covariance matrix, you do not need separate identifiability of the elements, right? If your aim is to see the marginal covariance matrix, even in factor analysis, you do not have to worry about separate identifiability. If you are interested in what it is possible, the layers to track file, like for instance. To detract from it. Like, for instance, if you have a community detection track, there's overlooking clusters, so it's kind of related to what you're doing here. But the goal is to really identify the clusters and really decode what's going on. So I wonder whether we really can do some sort of formal interpretation of what you're finding here, if you're really interested in systematic or the situation matrix. Because in the plots you showed earlier, you show the graph itself, that's maybe the graph. So you can some sense care about that. But I wonder whether there could be some identifiability considerations going into that. Okay, just briefly. So Veronica's question is about identifiability of the lambda and delta, that is the fact the factor components. And she's asking, if I get her question correctly, she's asking whether that interpretation can lead to some graph decoding. lead to some graph decomposition and we can get some further insight. So we have not, that's a that's an excellent suggestion and we would pass on that but we have not done that yet. So we currently we are using we have used this formulation for to estimate the precision matrix and we are deriving the graph from that but uh we have to compute the production. Uh yeah, that that's an excellent suggestion, uh but yeah, we have not uh ventured that yet. But yeah, we have not ventured that yet. Thanks. Thanks for asking. So, secondly, we considered the sensitivity and specificity. And as I said, I was talking about the AR case, the autoregressive case, and why we are doing bad here. So, we are generating data from an autoregressive process of order two. So, in that case, in the thousand dimensional case, only two elements in each row. Two elements in each row would be non-zero and the rest would be zero. So the true Prevision matrix is awfully sparse. So what the other method, since the other methods showing less sensitivity compared to us, it clearly says that it indicates that the other methods are more conservative than us. So when the true precision matrix is that sparse, we are doing worse than them. We are doing worse than them, but in other cases, we are doing much better and much better in apprehending the true ages. So third, I would like to see what the execution times are. So I would like to jump to the 1000 event channel case, and we are doing a full MCMC. So we have done 6000 MCMC on MCMC iterations. MCMC iterations on each of the each of the setups. And for 6000 MC, the execution times are in seconds. So in 6000 iterations, the time was something like 35 to 40 minutes, something like that for a full posterior exploration. And it is actually faster than this Vegas method. So this Vegas method was a method by Gannon and authors, where they considered a Bayesian map estimate. consider a Bayesian map estimate and we are even even though we are doing a full so all of the other methods are all of the other methods are for map estimation so only in the in this compression only we are only we are doing the posterior exploration and still if still for we are doing much faster than this method and having better results than others so So, I would like. So, we have done one application in microarray data. So, we have 53 expressions from 554 genes from immune cells. So, this is a figure, the graph, and which shows that the true graph is really sparse. And here we show the precision matrix for the connected genes. And this is their dependency graph. Dependency graph. So here we show that there exist two blocks where within block the correlation is positive, but between block there's a negative correlation. That's what this graph, that's what this present heat map of the previous metric shows. So what Veronica suggested, perhaps something like that can be done by interpreting the parameters. And we also have validated our results. And we also validated our results by with thorough literature search in immunology and our findings actually consistent with the existing knowledge. So to conclude, what I have done is we have done factor models for physician matrices. It allows for straightforward induction of sparsity and computation, scalable computation is feasible via very easy to use GIP. By a very easy to use Gibb sampler, our approach enjoys near-minimum astronomical properties, and it is also adaptable to dynamic settings and many other settings. So, the classical factor model, it is not restricted to estimating the covariance matrix only. It has been further developed for covariance regression, for dynamic covariance matrix estimation. So, those can be possible with our approach in the graphical model setup. Model setup. So finally, I would like to thank my co-authors, my mentors at UT Austin, Dr. Robert Shorker, and Peter. And here are the references. Thank you. I think we need to speak to the microphone. Are there any questions? Any more questions in the audience? I think there's one. I think there's one. Let me try to get the thing. So I will do the turning around. So not to make you this. I can move another way. Thanks for the talk. Just a curiosity. So you have these two types of asymptotic results on the theoretical part. And you said, if I understood correctly, that in the case where you had the big go of one, then you had the minimax. Then you had the minimax nearly minimax rate. But in the other case, is the rate that you get minimax or so this is the rate for the question. So this is the general posterior contraction rate. So we were able to derive the minimax rate under this condition. X rate under this condition. So we were able to get a lower bound of the minimax rate. So we know that the minimum X rate cannot be lower than this. So we got the lower bound under this condition. So and we see that what is happening in that case. So for when we also let Q Q Q naught n to increase, we were not able to find the minimax rate in that setup actually. No. For this setup, there was no minimax rate in the literature. We did a No minimax rate in the literature. We derived the minimum X-ray lower bound for this kind of recent factor analysis models. I think there is another question. Michele? So it's related a bit with what Veronica was asking. So, and maybe clarification. So, you have these sparse inducing prior on the Prior right on the parameters lambda, right? So, in terms of identifiability, then of the precision space, so is it, I mean, connected with the question by Veronica, so is it that enough to get what you really want from the inference? So to estimate the precision matrix, we don't care. If we are to estimate the precision matrix, a sparse precision matrix, we don't care. But we have not followed on it yet, but we need to see whether if we impose some condition on. If we impose some condition on lambda, whether it gives some extra intuition, we have not pursued that yet, and we will look into that. But what we see that for estimating the precision matrix, we don't need to, and for to see the graph from the precision matrix, we do not need to worry about the separate identifiability. But yeah, something can be done. Well, for the second time, we move to the next speaker. And for the second time, we'll move to the next speaker. Let's thank again Norit for his excellent talk. So I'll stop sharing. So our next speaker is Daniele Durante. Daniela, are you around there? Yeah, can you hear me? Yeah, we can hear you perfectly. Yeah, we can hear you perfectly. So, he welcome, Daniele. He's going to speak about advances in basic inference for regression models with minor categories.