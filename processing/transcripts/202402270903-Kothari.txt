evening session which we'll explain a little bit later. Our first talk is actually a super talk split between Probation and Peter about some kind of amazing applications of tools that I guess it's fair to say got their start five years ago in this community but have since really branched out. So take it away guys. And in fact at this location. At this location, yeah, cool. I don't have to go to my coin anything. No, no, you're recorded by these. Excellent. You're recorded by these. Excellent. Okay. All right, yes. Thanks to Sam, Skell, Guy, for organizing yet another fantastic CCSI workshop and inviting me. Brie, glad to be here. And thank you all for making it to the 9 a.m. talk. I know it's difficult when it's minus 20 Celsius outside. I appreciate it. I want to tell you about this line of work on the Qikuchi matrix method, which, like Sam said, very legitimately got its start to. Very legitimately got its start to here. I'll tell you some of the story during the talk. But what I'm going to talk about is based on, oh, yeah, this is like a talk joint by myself and Peter, and based on our joint works with a number of amazing people. So Omar, students at Berkeley, Benkit, my colleague, Tim, my amazing student at CMU, Siddhant, postdoc at MIT, and David and Benny at ETH Suri. So, right, okay, so let me tell you what this method is. So, let me tell you what this method is at a very high level, and then you know, most of this talk is going to be about two specific applications to coding theory. So, at a very high level, this is a method for solving certain extremal combinatorics problems. That's very broad, very general. Let me tell you how we are going to do it by means of a bit that I've got to doing when I give these talks. Doing when I give these talks. So, you know, I think at least a few of you have taught the theory of computation. And there is a really fun lecture to give when you introduce the KSAP problem and prove its empty completeness. And if you are imaginative or careless, you make also some interesting comments about KSAT, like how it's the mother of all problems, how every polymer in MP can be reduced to KSAT, so far. Polynomial can be reduced to k sat, so far is the truth. And then, you know, you also, if you're me, make a comment that, well, you know, in principle, every mathematical conjecture, deciding the truth of the conjecture can be reduced to simply verifying the satisfiability of a sat formula. It's legitimately true. And then we say, therefore, if we had an algorithm to solve KSAT, we would be able to decide the truth of every mathematical conjecture, thereby doing all of math. And then, of course, we go to the room and laugh a little bit, say, okay, fine, that's not actually how you can do math. Fine, that's not actually how you can do math. The punchline of this talk is that that's exactly how we will do math. Okay, that's exactly how we will solve some of these extremal combinatorics problems. And slightly more detailed version of what this method is going to be about is we will reduce some natural, interesting extremal comatox problems to verifying the unsatisfiability of some XOR formulas. These formulas will be random. These formulas will be random-ish in the sense that they will have some amount of randomness in them. And the idea is that even though we don't have algorithms to solve worst-case sort formulas, tools developed in this community for attacking random XOR formulas with some refinements would in fact help us really prove unsatisfiability of these formulas, thereby resolving the question we are asking. That's the very high-level goal. Let me come down a little bit. Let me like come down a little bit. So, you know, what we'll really do is take this goal that we're after, like of deciding whether there is a combinatorial object with certain properties that we care about, and form an equivalent associated set of KxOR formulas. So basically, you know, this object exists if and only if every formula in this family is satisfiable. If I have to prove non-existence of such an object, all Existence of such an object, all I have to do is to prove that there is at least one member of this family that is unsatisfiable. Now, you know, which member should I consider? Well, our plan would be to show that a random member of this family is unsatisfiable. So the word random makes its appearance, making its connection to this talk, you know, to this workshop. Of course, what will really happen is that even though we are used to attacking random KXR formulas with like careful Like careful probabilistic analysis, the amount of randomness that these formulas will have will be way less than the number of variables. So, if you've ever proven unsatisfiability with high probability of a random text or formula by like union bound over all assignments, that's the kind of trick that won't work here. So, what will really work, and the real reason in some sense, you know, we can actually get to the punchline is that there are algorithms that were developed in this community for solving, you know. In this community, for solving random instances of XOR formulas, that with some refinement actually turned out to work even in this supremely randomness start setting. So even though the amount of randomness is significantly less than we are often used to believing, the methods are robust enough that they will succeed with this minimal amount of randomness. That's the punchline. That's basically how this method operates. And I find it like a really cool application of the kind of stuff that this community has been up to in the past. Community has been up to in the past five years or so. Excellent. So let me demystify the word Kikuchi and explain the title to you. So the spectral methods that we will use to prove unsatisfiability of the formulas are based on specific matrices called Kikuchi matrices. They have some connection to statistical physics, but I'm very bad at it, so I won't be able to explain it to you. Ask Chris Moore. But this kind of matrix or the variance of the kind of matrix. Or the variance of the kind of matrix we're going to look at today arose in a very nice work of Alex Weim, who's here, Emma Dalawi and Chris, where they were concerned about Gaussian tensor PCA, which is a natural statistical inference problem. And it turns out that these matrices have wonderful properties, which are very useful in the kind of applications we'll be after today. So the kind of applications that we've had in the past few years include solving some smooth variants of K-Sat formulas, proving a certain extremal Proving a certain extremal comatoids conjecture of phik on the mode bound. These two works on coding theory lower bounds are going to be the main focus of this talk. And you can ask me about any of these applications, including some applications to strong forms of seminarities theorems on arithmetic progressions in dense subjects of integers, often. Okay, any questions about this intro part? Okay, excellent. So let's begin. Okay, this is So let's begin. This is the coding theory part of the talk. I'm going to tell you about some results of coding theory that we can prove using this method. So I'm going to be interested in error correcting codes that admit a really simple decoding or correction algorithm. So I'm going to explain what these objects are and then phrase a question about them. So you all know error correcting codes, there are ways to stretch B bits of message, K bits of message. My message K bits of message, my message will always be in the stock letters like D into a code word which is n bits. N is much larger than K. So you should think of like X as the code word here. This is the kind of letter I always use for code words in this talk. And the map E is the encoding map that sends message into code words. Now what's the fundamental setting in coding theory? Well, you know, this code word suffers from some corruptions. So delta corruption means that delta fraction of the coordinates are modified. This is worst case error model. Are modified. This is worst case error model. There is no randomness in the corruptions. Any delta fraction of the code word bits can be modified. And so what you actually see is this corrupted code word Y. And now the basic question in coding theory is to design the map E so that you can somehow reliably decode the message P even though you know the X the code word you transmitted was corrupted. So formally we require a decoding algorithm and like I said I'm going to be interested today in really simple decoding algorithms. Really simple decoding algorithms. So here is what a usual decoding algorithm takes as input. It takes as input this corrupted code word Y, and the job is to tell me the message. But this time, this local decoding algorithm will, in addition, take one of the message bits as input. So think of my goal as trying to determine the ith bit of the message given the corrupted code word y. The decoder cannot actually even read all of y. Even read all of y. The decoder has only the following amount of freedom. It can choose to query q bits of y and think of q as very, very small. This whole talk is about q equals 3. Think of like, you know, decoders that read 2 or 3 or 4 bits of the corrupted code per y. And based on this responses to the q queries or q bits, it applies some function of this received answers or received bits from y and outputs the IFP. And you know, outputs the ith bit of the message. Okay? Yeah? Now, the delta corruptions, there's no model for the fraction of that. There is no model for the corruption ever in this talk. It's worst case corruptions. Any delta fraction of the bits can be corrupted. Excellent. Yeah, please feel free to stop me and ask any questions. Good? Yeah? Yeah. So, what if all Q. Excellent. Yeah. So, you know, what Chris is observing is also my next line, which is that, you know, the decoder better be randomized. The decoder better be randomized because if the decoder deterministically chooses, let's say, the first qubits to decode, let's say, the first bit of a message, you are in trouble because I have a budget of delta n bits to corrupt, plenty of budget to go around, and I can corrupt the first qubits deterministically. So what really would save you is that the decoder will be a randomized algorithm. The choice of Q queries will be randomized. And because it's a randomized algorithm, you'll fail sometimes. So I will only demand that you decode the IFPI. That you decode the ith bit, which is a little bit better than randomly guessing it. So, you should give me an epsilon advantage over randomly guessing the ith bit of the message. Okay, so this object I'm going to call a Q delta epsilon local decoding algorithm. Q is the number of queries, delta the number of corruptions, or fraction of corruptions you can handle, and epsilon, the advantage over half that it gives for decoding the ILP. Any questions about local decoding? Where is your adaptive? Excellent question. I'm going to come to this. Excellent question. I'm going to come to this. So, queries, you know, in principle, are allowed to be adaptive, but we'll soon see that it doesn't matter. So, you can, without the loss of generality, assume they're non-adaptive. With some drop in the epsilon. So, epsilon, you know, the advantage will change to epsilon over something that depends on Q, but you know, Q is going to be a constant, so we won't care. Yeah. So, if I want to boost my probability, I could run the local decoding out too many times, but there could be correlations between runs, right? You're allowed to repeat the You are allowed to repeat the algorithm just with fresh randomness, which means that you will query Q more bits each time. So you will blow up the query complexity and improve the fidelity of the algorithm. Right, I forgot that the probability is only at the output. Yeah, so let's make that point very, very clearly. The corruptions are worst case. The input is worst case. It's a worst case decoding problem. The only randomness and therefore the only probability is over the choice of the decoding algorithm. Excellent. Okay. Excellent. Okay, so you know, in this talk, delta and epsilon will be small constants, like 0.01. This is the most interesting and most well-studied setting. I will not depart from it in the talk. Okay, so you can just forget about these constants. I want to handle constant fraction of corruptions. I want to get constant advantage. I don't care which constants. Okay? Good. Again, it's a bit, you know, the answer to Chris's question was: okay, you can repeat to Amplify, but then you're not making a good use of your query button, right? Yeah, much better. Apply because it's much better. Yes. So, an alternative formulation is to say we have erasers rather than errors. You want the decoder to be las legas, so you can either say I fail, no, and count the expected number of queries. Right, so you insist on perfect decoding. Right. So, does your negative result, and then it's a better posed question in a sense? Does your negative okay? Maybe in the end of the talk, yeah, let's get to it towards the end of the talk. Yeah, I haven't told you the result, but you want to ask me questions about. I haven't told you the result, but you want to ask me questions about the result. Yeah. Anyway, alright, we get there. It's an alternative definition, that's what it's saying. Right, right, yeah. No, I get it. Yeah. Yeah, let's come to it, I guess, in a few minutes. Good. So, yeah, delta and epsilon will be constants in this talk. So, you know, what I want to do next is generalize this local decoding into something stronger. And towards generalizing local decoding into something stronger, I'm going to make the following simple observation, which, if you've taken any coding theory class, should be. Which, if you're taking any coding theory class, should be obvious, but if not, take my word for it. You can assume that the code, the encoding map E, has the property that the first K bits of the code word simply copy the message, which is what I've done here. So, you know, such a code is called systematic. And, you know, linear codes, which means that E, when viewed as a map from F2 to the K to F2 to the N, is linear, have. 2 to the n is linear, have this property without a loss of generality. There is an appropriate version of it that also holds, even if E is non-linear, but I will not tell you why. You can just take my word for it. But you know, you can assume that the code is systematic, and therefore, a local decoding algorithm is simply equivalently also a local correction algorithm, but it can only correct the first k bits of the code. Local correction means that I take what are the first k bits of the code word as input and tell you what it is. And tell you what it is, even though it might be corrupted by querying some few other locations of the corrupted code word one. If you bought that, then there is a natural generalization which is to correct all bits of the message, all bits of the code word, not just the first k bits. And that's basically the definition of local correction algorithm that I be able to correct every one of the n possible bits. So the only change in this definition is that this time the index i is from any of the first n code word bits instead of the first name as it bits. First game assets. Any questions? Okay, very good. So, you know, formally, local correction is stronger than local decoding. It's a stronger ask on the design of the code. But, you know, I'll tell you the story of how, you know, until about 2007, people didn't really distinguish between local decoding and local correction as much because we didn't have maybe reasons to. But as the story goes, the punchline of this talk is in some sense a strong separation between three-query local decoding and local correction. And local collection. Good. So, excellent. So, you know, I won't spend too much time on the fluff. You know, there are lots of applications of these objects. They were invented first in the context of probabilistically checkable proofs in the early 90s. You know, this is like a core ingredient of PCBs. But since then, they've found really magical applications in private information retrieval, worst case to average case reductions, and so on and so forth. I won't tell you anything about them. Okay. I hope you think of local connection, local decoding as interesting on its own so that we can move on with the talk. So that we can move on with the privilege. Do you need local correction? So PCPs, I think, do require local correction. PCPs are basically locally testable codes, which is hard to stick to. Yeah, okay, yeah. You can deal with local testing, but I guess in the beginning they came up with a correction algorithm right away. Yeah. Great. Okay. So moving on, the central question that is basically the motivation of this talk is: are there any sit-outs? The motivation of this talk is: Are there any such objects? You know, I gave you a definition, I you know, told you all the lofty things about them, but I haven't told you whether they exist. And this is a non-trivial question because, you know, let me clarify that this is a purely combinatorial question. There's nothing algorithmic about this. I'm not asking you for like explicitly constructing these codes. I'm not asking you for efficient algorithms. I'm simply asking you, are there such objects? Okay. And the difficulty here is that the standard way to show such objects is a probabilistic method. Like, choose some random object and argue it has the properties you want. Argue it has the properties you want. It turns out that this method, basically, as far as we know, there is no instantiation of this method to show the existence of LDCs or LCCs. So, the only known ways to prove it is by just basically giving some magical algebraic construction. That's all we have here. So, I'm going to tell you what we know about such objects next. Okay, so basically, I'm going to tell you a story until the Kikuchi matrix method did something about it. So, now let's start with the most basic case q equals 2. q equals 1 is somehow. Basic case q equals 2, q equals 1 is somehow very easy to observe that doesn't really give you anything. So, q equals 2 is the interesting starter case, so to speak. And right off the top, we have actually a very famous code that actually gives you this local correction algorithm. This is the Hadamard code. If you know what that is, basically you treat message as a coefficients of a coefficient vector of a linear function on f2 to the k and just the code is just the evaluation of the linear function on all possible points. This is not important for the talk, but you know, it's you know just. This is not important for the talk, but you know, it's just simple explanation for what Hadamard code is. So it blows up the k-bit message by 2 to the k. So it gives you a 2 to the k length port world. And it allows both local connection and local decodability. In fact, both of them are literally the same algorithm in some sense. And you might be worried that this is exponential lower. That sounds pretty bad, but it also turns out to be essentially tight. So starting with this very beautiful paper of Katz and Teresan, people developed technology to prove lower bounds. Some people developed technology to do lower bounds on such codes, and one application of the technology was confirming that there is nothing better than this that we can do for two query locally correctable codes or locally decodable codes. Okay, great. So, you know, that's 2Q equals 3. That's the next question. Next, next question. And there is a very natural generalization of Hadamard that already gives you an improvement. So, you know, Hadamard was like evaluations of linear functions. If you treat your message as coefficients of a quadratic polynomial, you get basically a quadratic version. You get basically a quadratic version of the code. It's called read muller code, and you can, in fact, like have versions of this for any degree and therefore any query. And so, read muller code of degree 2 can be decoded and corrected, in fact, with three queries. And it gives you a code word length of exponential in square root. So that's a little bit of saving on the Hadamard code. Still exponential, but contributed to savings. And again, you know, a very natural generalization of the Hadamard correction and decoding also. Of the Hadamard correction and decoding algorithm gives you both three-query local correctability and local decodability, more or less no difference between. So, I'm confused by your decode. Yeah. Right. So, I'm thinking of like these codes as proofs that you don't have to take lock length more than the set amount. Yeah, sorry, but you're absolutely right, it's confusing. There is some fixed number for each of these codes. Yeah. Good. Alright, so you know, you can. Good. All right. So, you know, you can ask similarly: is this tight? Is there something better we can do? And so, I'm not an expert. So, when you say things like that, you're sort of like when you talk about lower bounds, the zombie lower bounds on n with their squared they say. Yeah, so the question to me is like, you know, what is the minimum amount of redundancy you need to add to allow local decodability to what etc. Yeah. Okay, so you know, so the question, of course, now is like, is there a better code or is there a better lower bound? And I'm just like, you know, repeating what my colleagues. I'm just like, you know, repeating what my colleague Zedwer explained to me, who is an expert in this area. So, apparently, until 2006 or 7, I guess Yual can also maybe correct me here. But I think until 2006 or 2007, people had maybe controversial opinions, like here, maybe there could be a code, maybe there is nothing better. And there's also this issue of whether LDCs and LCCs are different. And people thought that in some sense, there's not much to gain perhaps by allowing only local decodability, especially given the examples we had. We had this equation for PO equals 4. In 2002, we knew that for Q equals 4, we had a better construction than read Booler for local decoding, but this construction was not locally correct. Oh, really? Is it a very high alphabet construction? No, no, it's binary, but it only works for four queries and it saves a little bit just you know, instead of cubic root and k you get some uh slightly smaller polynomial and it's not specific. Polynomial and it's not three query. I see, I see. So it's still exponential, but it beats Freedmother. Yeah, and it's not locally queried. Not locally corrected, but since we knew in 2000, we had a candidate separation. I see. Excellent, excellent. Yeah, so yeah, so I'll let you all add to my commentary throughout the talk so that you actually learn the real bits of information. Okay, so okay, yeah. How does it change if I just ask for local testimonial? Local testimony is significantly easier. This is also what the point Yuval was making. In fact, we have constant rate. You all was making. In fact, we have constant rate, constant query, and what's the other constant that you're talking about? Constant distance locally testable codes as of two years ago. And the translation of that into his language is that N is like, oh, okay. Yeah. Exactly. Exactly. Okay, good. So, coming back, are there better codes? And, you know, there was in some sense a dramatic change with this very, very cool papers of Yakanin and Efremenko around 2007 and 2008, who showed that, in fact, there is a sub. Who showed that in fact there is a sub-exponential length three-query locally recordable code? So you can see like it's two to the two to the something which is tinier than log k, so it is tinier than two to the any polynomial in k. So it's sub-exponential length. It beats WeedMother, you know, basically out of the park. It's amazing. But this is only a locally decodable code as far as we know. In fact, you know, Yakhanin in 2011 asked the question of whether the same code could admit a local correction algorithm, at least till 2023. At least till 2023, we did not know whether this code itself could, in principle, admit a local correction algorithm. Excellent. So that's the story. You can ask, you know, what's the lower bound, you know, given that we are struggling to construct them, what's the best lower bound? And it turns out that for Q equals C, our lower bound abilities really take a hit. The best thing we know is basically a quadratic lower bound. I'm ignoring logs, factors and K here. So we can prove that N needs to be at least quadratic in K, but that's pretty much it. And this was basically you know the same set of three words that proved the lower bound for Q equals. That proved the lower bound for Q equals 2. So, given that we were struggling in both directions, you know, complexity theory is a very introspective field. We start wondering what's happening, why are we failing? And there were lots of introspective works trying to understand what is the reason for this quadratic barrier. And people, in fact, characterized these barriers, they showed that the lower bound techniques for proving quadratic lower bounds apply to something much more general than LDCs and LCCs. They applies to something called spanoids, for which these techniques are typed. So, that's one explanation for why maybe there was some difficulty in going beyond quadratic work. Going beyond quadratic lower bounds. And there are some very nice connections to the Hamada conjecture from the 70s in designs, which is kind of widely open. So depending on how you think about it, you can think that three LCC lower bounds or LCC lower bounds will give you progress on this conjecture. Or you could say that, you know, it's been open since the 70s. Maybe it's some reason to believe that this problem is non-trivial. But that was Jubal's paper. Great. So against that backdrop, I can tell you what we prove now. So the first application. So, the first application of the method that this talk is about, I think about a year and a half ago, we proved a cubic load bound on the block length of any three-query locally deployable code. This is a factor K improvement on the quadratic thing, but it passes the barrier that I was telling you about, which is kind of nice. It's still like, you know, very, very far from the sub-exponential length codes that we know, so there is a wide gap, but some progress and some application of this Kikuchi matrix method in action. And much more recently, about three months ago, Peter and About three months ago, Peter and I proved that for three-query locally correctable codes, in fact, the block length needs to be exponential in k to the 1/8. So, you know, yeah, there is no sub-polynomial, sub-exponential length, three-query locally creditable code. This one only holds for three-query linear LCCs so far. So, the first result holds for three-query LDCs, non-linear also, but the second one requires linearity in our current proof. Okay, but I'll tell you that maybe three basic points. But I'll tell you, I can give you three basic points about this result, which are in some sense obvious in the control story I told you, but still, just to make it clear. This result establishes a strong separation between 3 L D C's and 3 L C's because there is a sub-exponential length 3 L D C, while clearly, for 3 L C we now need exponential. It in particular shows that these Fremenko-Jakanian codes cannot possibly be locally collectible because they would violate the lower bound. And finally, it kind of suggests that there's nothing better than loaded B polynomials, at least polynomials. Better than low-degree polynomials, at least qualitatively speaking, than 3LCC for 3LCCs. Okay, any questions about the result? So, as you increase the degree of the polynomials into the read-filler code, the fact the power of k in the exponent drops like one over the degree. One over the degree minus one. That's correct? No, one over the degree. Yeah, one over the degree, and the query is degree plus one. Is your lower bound only for q equals three? Yeah, our lower bound is only for q equals three. Lower bound is only for q equals 3. For q equals 4, the method can possibly improve the polynomial lower bound, but nothing better. We cannot get even super polynomial lower bounds with our method. Yeah, it's only for 3. Yeah, any other questions? Yeah? So, do you think that the read Mueller codes should be actually optimal in terms of the power of k? Good question. Yeah, that's going to be my next point. Let's maybe make that point. Okay, first of all, you know, there are generalizations that resulted hold for not just the binary field, but like any small field. I won't go into it. I won't go into it. I think there is a natural sense in which you could try to optimize the techniques and try to get 2 to the K213. There is like a natural breakpoint there. I don't think, well, you know, in principle, you could go to 2 to the K21 half, which is Veed Mother's block length. I think it will require at least one new idea. This one, in principle, I think, could be gotten to with heavy optimizations to approve, even though we didn't quite do it yet. But the nice thing is that just two days ago, I learned that a paper has appeared on ECCC, which has done some of these optimizations and already produced a better. You know, and already produce a better lower bound of 2 to the k to the month water. So I think wait a few months might be the answer. Good, excellent. Any other questions? All right, let's move forward. Let me tell you what I'm gonna do for the rest of the talk. I will tell you a little bit about how this proof is gonna work, and I will tell you how the Suikuchi matrix method works for this problem. I'll begin by actually considering the case of linear fork. Linear for query locally decodable codes, okay, which none of the previous slides were about. I choose four because it's the smallest even number bigger than two, and even numbers are nicer for me, and two is trivial. Okay, so three is like somewhat more sophisticated, complicated, whatever. So, but four, you know, already shows you how conceptually, you know, the idea that is the title of this talk plays a role. So, I'll focus on that. And the best-known lower bound for this case is quadratic, which is basically goes back to the 2004 works, but I'll give you like. Back to the 2004 works, but I'll give you like a new perspective on this proof using this Kikuchi matrix algorithm. Okay, and you know, it will allow me to illustrate this connection to Bayesian X-R formulas and refuting them and so on and so forth. And then, you know, we'll use this proof to derive one key lesson, which is like the key heuristic that governs basically the application of this method, at least so far, which is this density versus level heuristic. And it will be demystified when I get to it. And then I think Peter will take over and tell you how to use this heuristic to actually construct. Heuristic to actually construct this argument for three LCCs. That's the plan. Okay? Good. Let's do it. So I have to tell you now how to go from a four-query locally decodable code into like, you know, some system of satisfiable 4xOR formulas. This is my goal. And then eventually, like I promised you earlier, I'm going to find a way to prove that one of these formulas is unsatisfiable. The way I'm going to do this is by going to a The way I'm going to do this is by going to a combinatorial characterization, a combinatorial description of what a Q query LDC looks like. The arguments that give this characterization turn out to be really simple and elementary, but I will be very sketchy about them. That's some notation for you to remember. You know, E is my encoding map, takes B into X, and I get to see a corrupted version of X called Y. So what does a decoder do? I want to decode, let's say, the ith bit of the message. I-th bit of the message. So, what am I supposed to do? Well, you know, I'm gonna suppose to decide which bits to query, perhaps adaptively, and then what function to apply to these queries in order to compute the IFP. This is basically what I'm after now. I'm gonna make like you know a sequence of claims about some restrictions on what this decoder can do without the loss of generality. So, first, we can assume without the loss of generality that the query, that the decoder is non-adaptive. And let me not even tell you why. And let me not even tell you why, but it turns out to be very simple. Because the decoder is non-adaptive, you can basically just assume that the decoding algorithm for, let's say, the first bit is simply a collection of q-tuples of y's bits, q-tuples of indices in like the code word space, such that I pick one of these q-tuples at random and make the q-corresponding quiz. So that's basically what's written down. So that's basically what's written down, you know, as this hypergraph, like the Q query, the Q tuples that I'm going to query for decoding any given one of the K message bits. Now, if the code is linear, which is what I'm going to focus on right now, you can in fact assume that the decoder is also linear, which means that the decoder's function of the Q things it receives is very simple. Just add mod 2. Equivalently, because I'm going to work in a plus minus 1 basis, I'm simply going to multiply. I'm simply going to multiply the received bits. So, the decoding algorithm, for example, if I'm applying it on X, if I query a tuple, let's say C from H1, I just multiply the corresponding bits, and that's my decoder. And if it is supposed to work, it should give me the ith bit. Okay, good. Finally, we can assume that when there is no corruption, when you know, there is no, like, you know, I get the code one. Like you know, I get the code word that I actually intended to transmit, then the decoder works with probability one, as in like there is no error. The only errors that will arise is because you know, when I query these q-tuples, whatever the bits might be messed up because y has corruptions, but if there is no corruptions, then you know nothing will go wrong. Okay, this is again without the loss of generality. Okay, so far is that obvious? It's not obvious, but it's also not hard. Let me not tell you why. Let me not tell you why. Yeah, ask me later. Okay, good. So, finally, yeah, one final simplification: that you know, intuitively, if you want to somehow, you know, tolerate corruptions, you want to avoid overly querying any given bit. Because if you focus on one bit, then the adversary has all the more motivation to corrupt that particular bit. So, this would mean that there has to be some smoothness in your query sets. In your query sets. No bit should be queried too many times. You can actually take it to an extreme and prove that without the loss of generality by losing something of the epsilon, that advantage over half, just a constant. You can assume that the query sets for any bit are simply a matching, as in like they are pairwise disjoint. No query set is queried more, no query, no bit in the code word is queried more than once. Okay? And now, you know, if you have to tolerate constant fashion corruptions, you can prove that you need. Corruptions. You can prove that you need the matchings to be of omega of n size. Otherwise, you know, I'll just corrupt every bit that appears in your matching. For this talk, in fact, I'm going to assume that the matchings are perfect. You know, it will change nothing in my proof. So for Q L D C's, let's say the matchings are perfect. It's n over Q. Yes? So the distribution is going to be uniform? The distribution is uniform. Yes. You didn't need to be, but you can reduce to this without. Without, yeah. So yeah, so you can do a lot of the simplifications. Oh, I lost my non-adaptivity. Oh, I lost my non-adaptivity. It doesn't mean that it's gone away. It's still there. Good. Good. So, you know, so this, okay, so without loss of individuality, the decoder is really simple. It has a matching for every one of the K of the code, like the message bits. It's a matching is of size, you know, just n over q for each. And the decoder simply queries a q tuple in the matching, adds the corresponding bits, or like multiplies them in the plus minus one world, and that's it. That's my decoder. Any questions? Excellent. If you need to find a matching, you mean it's like a disjoint? Yeah. Definitely mean if they're disjoint? Yeah, pairwise disjoint sets. Yeah, hyperdot matching. Excellent. Any other questions? Very good. All right. So, you know, while I have you, let me also tell you what the corresponding combinatorial characterization of locally correctable code was. Okay? It's really simple. Instead of having a matching only for the first k bits of the code word, I have one now for every bit of the code word. So instead of k matchings, I have n matchings. That's it. Matchings. That's it. Okay. So now you know basically a combinatorial description of both LDCs and LCCs. Alright, so now you know let's build XOR formulas. So what do I want to do? I want to somehow associate this combinatorial description of a perpetrator QQuery LDC into a XOR formula which is supposed to be satisfiable. My formula in fact will be very, very simple. The formula simply encodes the following sentence. The formula encodes Following sentence. The formula encodes that if I were to run local decoding as suggested by these matchings, these query sets on a true code world, then you know I succeed. That I get the right message bit. That's it. That's my formula. Okay, let me write it in notation so that it becomes clearer. My formula simply says that if I fix any message B, k bits, then you know, I take the corresponding I take the corresponding code word x. Now, for any hi, I take any tuple c in h sub i and multiply the bits of x corresponding to the tuple c, then I correctly decode the if bit. That's it. Yeah? Sorry, I got confused at one point. Do we need to care about the sparsity of the code that changes? I'm not a code person, but so I forget which page. Code person, but so I forget which matrix it is, but I mean, don't we need each code in the not the message, the longer thing, the code word to depend on only a few bits of the original message? No, no, no, no. The code word in general is going to be any linear subspace. In a linear code, yeah, it could be any linear subspace. So then, how do we turn this into an XOR formula? Because this is only a statement about sets of bits that are in that linear subspace. No, no, no. That linear source. No, no, no. This is an excellent formula that is really about the decoding sets. We are saying if there is a set of such decoders for any of the k message bits, then you know the code word x should satisfy the fact that if I were to run my decoding on x, it should succeed. That's all we are saying. Oh, yeah, I guess that's whatever it is. You can think of this, yeah, you can think of this as like. Of this, yeah, you can think of this as like some subset of the Pirate Check matrix because you know, in principle, like these are Pirate-Check equations, but you know, we are not interpreting them as such, we are simply saying that they are decoder sets, right? Like, that's that we argue that decoders algorithm looks like you know, pick a tuple from HI and like, you know, multiply the corresponding cover bits. We are simply saying that this check should succeed. That's all we are saying. Does that make sense? You know, so I'll take it offline. I'll take it offline. Okay. Alright. So, you know, also, there's something about deltas. They're confused about where delta went here. Delta went away because I assumed the matchings are perfect. I just killed a parameter. Otherwise, the matching could have been of size some omega delta n. Okay, so these are the ones where the, like, you're saying that for most, for many queries, the decoder works. Exactly, and you threw out the queries where the decoder failed. The delta went away simply because I assume that the, like, you know, delta means that, you know, I want to tolerate delta fraction corruptions. If the matchings are too small, then you know, the adversary could corrupt everything I am going to query in that small matching, right? Like, if it covers a small number of variables. So the matching needs to be large if I have to tolerate some delta fraction corruption. It needs to be at least larger than delta n. Okay, but here, somehow, are you saying that if there were no corruptions? There were no corruptions. This equation is what would happen when there are no corruptions. This is a sanity check equation. We are saying, you know, we started with like this perfect matching. If this perfect matching was supposed to work as intended for decoding, if I were to apply it to, you know, the true code word, I should pass. Okay, okay, okay. So, but is there to be an assumption here about what happens when delta, like you're saying, if I apply my checks to the decoder to the true code word, it succeeds 100% of the time. Yes. Which was not part of the definition. Which was not part of the definition, right? So it was quite this categorization I was building. Okay, okay, okay. Yeah, it is without loss of generality, even though it's not part of the definition. So we're going to succeed if we happen to pick a subset that has not been corrupted. Exactly. That'll happen with some probability that some advantage or something. Exactly. So this in particular implies, you know, sort of specifically what epsilon is going to be, right? It's going to be something. Yeah, like 1 minus 4 delta. Excellent. Yeah. Feel free to interrupt. No, no. Yeah, feel free to interrupt. No, no. Yeah, any other questions about? Okay, very good. Yeah. So notice that, you know, for any fixed k-bits, B, the message, I have a system of XOR equations, right? Namely that, you know, for that message, all the local decoding checks succeed. Yes? So I have a XOR formula size of B, right? So you know, it's just to like side to check, you know, it's some collection of nk over 4 equations because you know each matching has n over 4 equations and there are k of them. Matching as n over 4 equations, and there are k of them, right? So it's n k over 4 x y equations or n variables because you know the variables are doing the code world, the code world bits, x, yeah. And you know, I have such a formula, one for every possible message b. So I have two to the k such formulas. Okay? And notice that, you know, for every message, there is an x that satisfies all the constraints, namely the corresponding code word. So this system of formulas is satisfiable. Okay? Excellent. So, you know, I've given you this reduction to. So you know I've given you this reduction to satisfiable 4xO formulas, yes? If it wasn't linear, I guess you just have an arbitrary reason. Yes, exactly. And then you can still show that you will satisfy more, like some non-trivial fraction of constraint, but not all. Yeah. So it's not that hard, but just potentially in this case. So at the moment, I mean, here there is no, there isn't even really a suggestion that the code is linear. In other words, you're not, you know, this is just saying, please status. This is just saying, please let us find a code word that works. Yeah. It isn't necessarily forcing nothing about the map from B to X b linear or not. Okay, good. That rub was my confusion. But it's just saying that it's a good thing. It's saying that the decoder is linked. I'm sure that's a slightly different. It's saying the decoder is linked. Yeah, it flies without close to the yeah, but I'm just saying the XOR formula is sort of isn't stating that the B. Yeah, right. Okay, excellent. Good. Moving on. So, you know, if I have to choose a random formula from this set, it's also a very easy method. Choose a random message and takes the corresponding slice of B. Okay? So it's a K random bit method to choose a random formula. You can see that it has K random bits. And you know, we are in a core setting. So K is of course much smaller than N. So this, you know, goes back to my comment that we're going to have randomness, which is going to be much tinier than the number of variables in general. Tinier than the number of variables in general. Okay? Good. Excellent. So now let's take one of these formulas and prove that it is false, prove that it's unsatisfiable. That's basically our way to prove that k must be non-trivially small as a function of n. Good. Alright, so we have this formula psi sub b written again. And here's what we're going to do. We're going to say that if there is a 4L DC with some set of With some set of matchings that correspond to the decoding sets, then psi b is sat for every b, but we'll show that for a random b, psi b is unsatisfiable, you know, if k is too large. So together, you know, these two statements will mean that, you know, k has to be small enough if at all there has got to be a pore of this work. Okay, that's the overall plan. In order to, you know, make this plan work, I need to argue things about, you know, what fraction of constraints does a given x satisfy. Does a given x satisfy in my formula? Okay, so I'm going to define the following polynomial in order to reason about what fraction of constraints can some assignment x satisfy in this formula size of b. The formula simply says the following. I sum over i from 1 through k. Then in other words, I am simply taking every possible local decoding check, x sub c equals b i, multiplying both sides by bi. Bi multiplying both sides by Bi, that's like saying BI times X sub C equals Bi squared, which is 1 because we are in plus minus 1 world, right? And I'm just taking this Bi times X sub C and adding them all, okay? I just arrange it in a way that, you know, I pull out Bi in all the tuples that occur in H sub i, and then I write down, you know, the sum of X sub C's, where C's come from the corresponding Hi. Still with me? Like, look at this. Yeah, okay, very good. So, you know, what do we know about this polynomial? Well, we know that, you know, if I choose. We know that if I choose x to be the corresponding code word of b, then it is large. In fact, it satisfies all the constraints, so it's at least nk over 4, right? Very good. And in general, it counts the fraction of constraints satisfied by x minus the fraction of constraints unsatisfied by x. Okay, that's what this formula really counts. Good. So that's, you know, just again summarized for you over here. So what are we going to do next? Now we're going to, you know, try to build an upper bound on this formula. Try to build an upper bound on this formula, and the way we are going to do this is by thinking of size of V as a quadratic form on some appropriate matrix. So, size of V of X right now is a degree 4 polynomial. It's a homogeneous qualtech, right? We are going to somehow write it as a quadratic form of an appropriate matrix and use the spectral norm bound on the matrix as an upper bound on what is a fraction of constraints that any x could possibly satisfy in size. Okay? I'm going to tell you what this matrix is next. This is where. I'm going to tell you what this matrix is next. This is where Kikoji comes in. In order to describe it, I'm first going to look at just this particular fragment of the polynomial, namely the sum of polynomials x subc, where c is in one of the decoding sets, h sub i. Yes, so you're not thinking about kind of the structure of the h's across different i's, is that right? Like, these are some matchings, and when we, and when you pick a random B, you're kind of fixing the right-hand side of this. The right-hand side of this XOR system. But you're not thinking about how do the H's have to relate to each other across different systems. So far, in the argument, I've not thought to be anything. So far in the argument, I've not thought about it. In fact, I think in this argument, eventually I will never get to, never really need to think about it. Right, right. Because it'll be some permutation of the Fukuji matrix, but kind of according to the matching that's used. We'll get there. Yeah, you might actually be on the... Yeah, but yeah, let's get there. Yeah. Good. Okay. So, you know, let's write down this particular fragment in the paragraph. Write down, you know, this particular fragment in the polynomial as a quadratic form. I'm going to build a matrix, really, it's like a parameterized family of matrices, one for every integer L. For a parameter L, the number of vertices I would have is n choose L, where N is the number of code word bits. So it's going to be indexed by a set of all subsets of size exactly L. So the Lth matrix in this sequence is going to be indexed by a set of subsets. Sequence is going to be indexed by a set of subsets of size exactly L. And in fact, this matrix is simply a 0, 1 matrix. So you can think of it as an adjacency matrix of a graph where S and T have an edge between them if the symmetric difference of S and T happens to be a photopole C that lies in H sub I. Okay, that's it. That's the matrix AI here. Okay, good. Let me, you know, say. Good. So let me say some easy facts about this matrix. I'm going to use a L which is going to be eventually very large. Even though we are working with polynomial in n-size objects so far, the L I'm going to eventually use is going to look like n to the epsilon, in fact root of n, okay? Which means that the matrix I'm going to deal with is 2 to the root n. So even though I have a homogeneous quartic polynomial I'm going to look at, I'm going to write it as a quadratic form of a 2 to the root n size matrix. Root and size matrix. Okay, and that's going to be okay. Next, let me also point out that this matrix is in some sense quite sparse. Okay, what does that mean? Well, you know, S and T are sets of size L, so like n to the epsilon, but they have an edge between them only when their symmetric difference is a four-tuple. The symmetric difference of two L size sets is a four-tuple only when they intersect in L minus two of the elements. So even though I have L degrees of freedom, on L minus two of them, they have two exactly. On L minus two of them, they have to exactly match. Okay, so in that sense, this matrix is rather sparse. Okay, good. So, you know, while we are talking about this, let's you know, just make this reasoning go through even one more step. You know, the same C in H sub I contributes multiple edges here because there could be multiple pairs S and T that XOR out, that symmetric difference out to the same C. Let's count how many. It's very simple. You know, in order for S and T to XOR out to C, To XOR out to C, I need them to intersect in L minus 2 variables, and the remaining 2 in each S and T should pair up and give me the resulting 4 tuple C. This is the only way it can happen. So, you know, 4 choose 2 is a number of ways to split C into, you know, pairs. And then, you know, the remaining L minus 2 can be arbitrary as long as I choose the same L minus 2 elements in S and T. So that's L minus 4 times choose L minus 2. Okay? So I'm going to call this number delta. So in total, the number So, in total, the number of edges I have in AI is the total number of C's, which is n over 4, we had a perfect matching, times delta, the number of copies of each C. Okay, so far so good. In particular, the average degree of AI is the total number of edges, which is the expression we just computed, divided by the total number of vertices, which is n choose L. And you know, if you do the math, it will turn out to be L squared over L. The math is easy. Okay? So, this is just for our benefit. So, this is just for our benefit. We're going to, you know, somehow, this number is going to help us reason about this matrix in a bit. Okay? Good? Excellent. So, alright, so that's some information we've learned about these matrices. Next, let's fulfill the promise I made to you that I can take this circle the fragment of the quartet polynomial we're after and relate it to the quadratic form of this matrix on some vector. I'm now going to describe to you. I'm now going to describe to you what vector. I'm going to take the vector on x, so x is basically whatever is the formal variable that appears in that quadratic over there. I'm going to take this, you know, plus minus 1 to the n element x and construct a vector of all possible L-wise monomials in x. So it's a n choose l dimensional vector indexed by subsets of size exactly L and at entry s, it basically has the product of the corresponding bits of x written down. Corresponding bits of X written down. Okay? This like dot circle annotation, this is like different from the tensor because you're deleting the one-two. Exactly. So, you know, yeah, so tensor, like the x tensor to the L, you know, in some sense contains all these entries, but not more. And I only care about the L-wise monomials. That's why, you know, we invented a new notation. Okay, so, okay, so if I take the quadratic form of x circle to the L on A sub i, then what happens? Well, you know, that's what quadratic form looks like. That's what quadratic form looks like. But notice that Ai is basically zero if S and T don't symmetric difference out to some folduple in H sub I. So the only way that happens if S and T have symmetric difference C, but in that case, Xs times X T must be exactly X sub C. So even though formally I have a degree 2L polynomial on the left, I only get a quartet when I like really expand out the quadratic form. And I get basically sum of X sub C's. Basically, sum of x sub c's times the number of times a c appears, which we already estimated to be delta. So, it's delta times you know that circled up a polynomial. Good? Okay, excellent. So now, you know, we are basically more or less done. You know, we look at the top above, right? If I compute the quadratic form of x circle l on a sub i, I get, you know, that homogeneous quadratic monomial there, polynomial over there, but I get delta copies of it. So, I write down delta. Get delta copies of it. So I write down delta here. So if I sum up, you know, Ai's with the coefficients bi, then I get basically size of b up to a scaling of delta. Yes? And I know that there is an x that makes size of b large, which means that there is an x that I could have plugged in here that would make, you know, this quadratic form at least delta times k n over 4. Still good? Okay? But now, you know, so far, you know, everything is true for arbitrary B. In fact, one more step is going to be true for arbitrary. In fact, one more step is going to be true for arbitrary B, which is to just upper bound this quadratic form usual way. Like we just use the spectral norm of some BIAI times the L2 squared length of X circle L, which is a plus minus 1 vector. So its L2 squared length is exactly capital N and choose L. Still good? Okay, so far we haven't used randomness of B, but this is the first time I've invoked that B I can choose to be random. If B is random, then I have a remarker sign sum of arbitrary. Marker signed sum of arbitrary matrices Ai, which is a random matrix. And I can try to understand how this matrix spectral norm behaves by matrix concentration inequalities. The relevant inequality here is matrix Kinchin inequality, which is what I'll use. And if you've seen the scalar Kinchin inequality, which says that the scale of sine sum of scalars behaves, the typical value is going to be the standard deviation, which is like the sum of the squares of the vector's entries. This is basically the matrix analog. The sum of the squares of the coefficients. And all the sum of the squares of the coefficients of bi, they happen to be matrices, no problem, just write it down. And then take the spectral norm and take square root like we do to you know generate standard deviation from variance, okay? So you can, this is basically almost syntactically same as the scale of kinship if you know, except for one key difference, which is that you lose a dimension-dependent factor. The dimension of the matrices plays a role, you get a square root log in the dimension of the matrix. Okay, this is off the shelf, nothing very deep going on. This variance there is also very, very easy. This variance there is also very, very easy to compute. You know, AI, you know, like you remember, is an adjacency matrix, right? For any adjacency matrix, the spectral norm is at most the maximum degree of the graph. So Ai spectral norm is at most d max. So Ai squared spectral norm is at most d max squared. So some Ai squared spectral norm is at most k times d max squared, which is what I'll write it down. Okay, that's it. We are not being clever at. We are not being clever at all. So, we wrote it all down. This is good. Now, what? Just going to summarize what we've done so far and observe the following. If I just rearrange, set n to the side and cancel the root k I have on the right hand side, I basically get this expression. And then I observe that this expression over there is simply the You know, this expression over there is simply the average degree of the graph that we computed earlier. Okay, so I can basically write down this inequality. So I have a square root k times d average is at most d max times square root log n. If I rearrange and square both sides, I get that k is at most the square of the ratio of d max over d average times log capital N. Okay, so far so good. Yeah? Okay. So in particular, yeah. Just in this video summed up the weight mean. In this when you summed up the matrix I mean it's clear that this was not loose because like when you s put some of some of the spheres the matrix is then you can spe uh spectrum normal, but you sort of put the spectrum once individually a priority could be very different. Yeah. Well in one line, you know, after I do one more step at scale, I cannot gain. At this point I can gain and in fact I will gain a little bit but yeah I yeah eventually like this boundary is not actually far Yeah, eventually this boundary is not actually far from being tightened. So I guess in particular, right, in the kind of worst, the situation that would achieve this is that the AIs were all the same, which from the decoding point of view is a bad idea, right? Because you're like... Yeah, or even from the spectral norm point of view, it's a bad idea because the same vector width is the large. I know I'm square we can. So intuitively, these matchings have to be somewhat orthogonal to each other. And then the hope is that the AIs don't commute, and then you might be able to get rid of the. Yeah. You might be able to get rid of the group plugin. Right, yeah. So that is like the plan we've been playing with for at least a year now, trying to get better lower bounds for LDCs. The punchline is that Peter will tell you why you don't need to do that and still get exponential lower bounds for three LCCs. So you can win without beaten kinchin. Anyway, but just to finish this discussion, if AI happens to be roughly regular, meaning its maximum degree is not much larger than it. Maximum degree is not much larger than its average degree. Let's say it's the same, you know, in the extreme case. Then, you know, the first term is just 1, and I get a lower bound which says that k is at most log of capital L. Remember, n, capital N, was n choose L. So, log of n choose L is about small l log L. So, I get a bound that says that k is at most L log N if my AIs happen to be approximately regular. Okay, good. So, now you know, if I need approximate regularity, If I need approximate regularity, I need at least that the average degree of AIs be at least one. Because if not, if the average degree is too tiny, the max degree is of course always at least one, it's a non-empty graph. If the average degree falls too much, then of course the ratio would be too large, right? So in order for the ratio to have even a chance of being controlled, I need d average to be at least 1. D average we already computed earlier, it's L square over m. So d average bigger than 1 happens if L is at least square root of n. least square root of n. So the best lower bound I could hope to get from this technology is k at most square root of small n. It turns out that you can actually make that happen. Turns out that you know even though AIs are not quite regular, you can drop like a negligible fraction of rows and it becomes regular. I won't tell you why, this is a simple argument, but I won't tell you why now. And you know, that basically gives us the lower bound I promised you for four LDCs. Any questions about this? Okay. Okay, great. So let me, you know, take the key lesson we want to learn from this whole proof. Like the proof itself is rather simple. I gave the whole proof to you basically. The key thing to remember is that in order for this technique to work, we needed approximate regularity. For approximate regularity to be possible, we need at least that the average degree of the AIs be at least one. I'm going to call this the density condition, that I need AIs to be dense enough that each row of AI is to be dense enough that each row on average has at least one non-zero entry. But I can always get that by blowing up the level of the Tikuchi. But on the other hand, the best, like the low round I really get is like k at most log capital N, which is little l log N. So it is in my favor to do this at as small a level as possible. Okay? This is the key trade-off. When can I get density at the smallest possible Kikuchi level? This is the game. Okay? Question? Okay, question? Yeah, how close are you to copy brains? Uh in about uh one minute. I also started nine oh five, come on. Nine interrupted. Okay, we're just done. Yeah, this is the lesson. This lesson is the end. Okay, good. So, you know, I just want to like tell you what's going to happen in the next part now. We're going to try to use this lesson that we learned from this proof and try to see if we can get a better lower bound for 3 LCCs. Remember, like you know, the only Remember, like you know, the only extra thing we have for LCCs is that we have n matchings instead of k. Okay, we somehow have to figure out how to use these n matchings to our favor. At first, this seems rather natural, right? If I write down those XOR equations, I have many more of them. Not just nk over 4, but like something like n over 4 times n, which is n squared over 4 or whatever, right? So that seems a lot. But the issue is that only the equations from first k matchings come with a bit on the right-hand side. On the right-hand side. Like the rest of the bits are not independent. Only the first K bits are the message bits. And this is the key reason that, you know, despite the fact that there seems to be a lot more constraints here, we didn't really have any techniques that distinguish between LDCs and LCCs. So what we are going to do now is basically, you know, bank on an idea from proof complexity. We're going to somehow, you know, build additional equations that combine some equations from the first k-matchings, which come with a bit on the right-hand side, the message bit, with, you know, The message bit, with you know the many more equations on the right-hand side. This will like somehow blow up the number of equations while still giving us the randomness that we need. And of course, the game is to somehow get density at a lower level, so this will happen. And the messy part, the complicated part is like arguing why the resulting matrices will be approximately regular. Okay, this is pretty tricky, and I would not tell you how or why. But basically, you know, it does not really hold in general, unlike the proof I just. Really hold in general, unlike the proof I just showed you. You identify some pseudo-random condition in the matchings that makes the condition hold, and when it doesn't hold, you show that there is a way to make progress in a different way. This is part of like a broad paradigm in combinatorics called structure versus animals paradigm. In some sense, this is an instantiation of it. And at this point, we'll go to a coffee break and then the baton goes to the year. So those new clauses form are higher. Peter will tell you how, in fact, they're going to be log n. They're going to be super fun screen. I don't know. Okay, well I don't know it's more than six months, but it's much better in the order I think for very long. So how about does it have any indication? Like one thing that you could say, okay, so suppose that I'm thinking about I'm thinking about doing like a text tomorrow. Somehow, like, no, what do you need? And your matrix is like a big. I know that's not how you're to be code, but I wonder if the second count comes. So, somehow if you're capable of n-dimensional matrix, then you need to do like a logarithmic capital n trace. Trace big time. Yeah, right. And now, like, somehow, if I, if the number of bits of randomness is not enough to have, like, enough randomness for term, then I'm kind of screwing, right? So, okay, so that's like a different way to like for a different way. That's like a different way of looking for the microphone. That's exactly the way you talk about it. That's exactly the rule of anything. So, you know, like you're saying, one natural way to get over this would be to somehow replace the front. Argue that, you know, despite the fact that you have less than. Despite the fact that you have less friends and less, then even like the lessons. Yeah, most of the terms are not like exactly what you mean. So you're saying that the reason that you're going to get created is somehow like the algorithm designer. And it's like you don't, because you don't have any control on the structure of this code, you're trying to trigger the random arbitrary code. So you're trying to say, like, well, if Say, like, well, if the code is like well designed, that is kind of corresponding to this condition of non-commuting, and then you get lost cancellations. And like, the poorly designed code would be like the same matchup for every one of them. If you're totally commuting, I can remove the factors. But honestly, I don't know if there's any arguments for not committing for all of them. Yeah, I'm very glad that you're not. The intuition is that there must be some argument like this in order to go beyond the quadratic or not. Like quadratical. I mean, I should have known about it. I should say exactly the same sentence as Uma. That is a somehow you know. Maybe you can't tell. You know, maybe possibly you should give like highly non-communicable matrices. But the point of view that is remote-type estimates of non-communicability will not work out. And I think it eventually turns out for a really good reason why the. The intuition basically is that some seconds, the one or some, you know, I sense that it looks like that. And when they have the intuition, they can make it happen. In our case, you can prove that that's just the sense of like that the AIs are not going to have some DIY. Even like the microphone. Sure, I mean yeah, that would make sense. I mean somehow because anyway your AIs are like yeah they're they're so structured like uh I think it's even like you know the idealized object in this case. It's like a tensor product like a tensor product. So this is like the conceptual solution is not someone's favorite problem but you put out the parameters. I think that it is like a red. I would love to pick note and make a tensor. Maybe you are interested in myself. Yeah, but you're looking, so for you it's like as though you were working in a tensor space or like not tensor Johnson space, I would do it. So it's not like the same analogy to. But then it's like super annoying because you can imagine in some copies you will like you really need this not confusing to happen in every copy for instance that you have. At some point, like there's some exploration. Oh, there are this workplace and you know, when you have to see where you want to see that, somewhere it has worked. So if it gets changed, yeah, like lots of lots of different change on my phone. I want the whole project on the bottom of the machine on the screen and then That's what I was thinking. It could be farm-right. Especially if you want, you sort of want, like, suppose if you don't have non-trained or something. Probably what you want is you want some kind of elephant of okay, so. Okay, so like, because you're sort of working in an LI sensors. No, you gotta tweak this thing. That's hard to decide. You want to construct the analog of the handle alternative. And then you need to take an L-root of that. I want to see the actual things that are actual things like that. Again, very simple. Again, baby shooters are. Because of her time. But you know, what if you just like, um, I mean, yeah, like here, I am only going to be able to do it. Like, what if what you like? What happens if you just try to so you okay? So the Van Handel style community thing is like you look at like some products like ABAB and everything. So what does it mean? Think of like vectorizing and taking the covariance, like empirical covalence making and solving these vectors. So okay, so suppose we went back to the chalk style thing. Because for the chalk style thing, it's a little bit easier to think about how the tensoring spaces are going to work out. So you want to take, like, one thing you could do is even when the chalk style thing is in sharp, it gives you. And isn't chart that gives you something, right? And you can take the trough-style thing, compute it, and then, because if you think about these as tensor spaces, like you have the products happening in separate, like every, if you think of every tensor space as like working separately, now when you take the L-root of it, this is almost like measuring in every copy, do things look non-communicated. So it's not like I've tried this superfar. Uh of course, but the difficulty we had at some point was like trying to uh argue uh you know based this measure of toned rock hand. Maybe like we didn't see the easy way to control that. Maybe. Actually, like I guess it would end up being, it's I think it's sort of counting like you know. This is my basic idea. I mean like look if any of this off-the-shelf matrix works really. Any of this off-the-shelf matrix constitution what is occurred, but oversetting is combinatorial enough that we should be able to just run the case. Like, why are we doing all of these cleverness? In the end, if this wants to work out, we should be able to do the case proof. You can in principle prove, like, you know, you know, there is a trace proof of kinship. Very transparent, for example. Yeah. Just letting the users, yeah, you get like this products. You get like this products of like making sure that only some products can actually be computed. Yeah, I guess like if you're only saving a product of like four at a time. Well, it might be easier to compute because somehow intuitively what it's doing is it's measuring like pairwise how like how do the two matchings, like how similar are they? And the more similar they are, like the more commuting it looks. But But I guess that the pairwise probably isn't enough resolution, right? You really probably need the K things to interact with the K of H, right? Because otherwise it's as though you were considering K that was smaller anyways. It's a little bit like that. Like somehow, there's also a different problem, which is like, I think there's a different source of non-commutative key. Like, I think the matching sort of saying is not enough to ensure that the AI is not the key. This is because of, like, This is because of additive structure. You're really in some sense, like you could imagine, you could imagine, let's say you partition n into buckets of size of 3. And then take maybe two different ways of partitioning the p into. This will give you matching features very different. But it's still like is that I just reduce the parameters. Right, like when two parts collide, and so this is the reason why you need to look at like longer range. I need to look at longer range things. Yeah, yeah, yeah. So that's why some of the disparities are. Okay, okay, I see. So you're saying it's not even about the fact that there's more than two or four matchings at a time, it's also the fact that you need to use all of the end variables well. Oh man, that's really like. I guess like one thing you could do, okay, like I guess a different thing you could do is you could say uh yeah, but okay, but okay, so I don't I don't really see how to So I don't I don't really see how to do it. If the code is like good and like taking advantage of setting, then you should have this non-commuting problem. Yeah. Yeah yeah. So the intuition seems that somehow you know the object should look at least in some sense super random. It's not in your favor to somehow have