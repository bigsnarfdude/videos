Overview talk: So, like the keywords here are information computation trade-offs, okay, in a specific model of computation, which is the statistical query model. And the focus is going to be on a specific problem, specific statistical task known as non-Gaussian component analysis. So, we'll see how we can use this task as a starting point to prove computational statistical trade-offs for a range of learning problems. For a range of learning problems. All right, so let me begin. So, the high-level motivation of this problem, this NGCA problem, is finding interesting directions in high-dimensional data. Okay, so if a direction is non-Gaussian, then it's considered interesting. If it's Gaussian, it's considered noise. Now, I haven't formally defined the problem, but sort of it's a distribution learning problem. It was proposed first in the signal processing literature. And as the name suggests, And as the name suggests, the objective is to find the non-Gaussian direction in high-dimensional data sets. Okay, and there's, of course, an assumption about the generative model, which I'll describe soon. Okay, so far I haven't really defined the problem. Okay, so this is sort of some of the literature considering this problem. So, is there a question? No, sorry, that was just some random noise. Sorry, I'll be sorry. All right, all right, okay. Just, you know, I don't have sound, so please, please feel free to interrupt me. Okay, so there has been, so let me sort of define what I mean by a non-Gaussian component. So here's the definition. So for a, let B be a unit vector in B dimensions, and A is going to be the probability density function of a one-dimensional distribution. Then I define the high-dimensional distribution P. The high-dimensional distribution P, parameterized by the vector v and the density a, to be the distribution whose projection in the v direction is equal to a, and whose projection in the orthogonal complement, so v perp, is an independent standard Gaussian. All right, so that's the family of distributions I'm going to care about. And the non-Gaussian component analysis problem, so NGCA from now, is going to be the problem of basically finding this hidden direct. Basically, finding this hidden direction v when I give you IID samples from one of those PVs. Okay, and the sort of difficulty of the problem is going to be quantified by this one-dimensional distribution A. So if this one-dimensional distribution A matches many of its low-degree moments with the standard univire Gaussian, then this makes the problem harder, as we're going to see. Now, this definition is not super formal in the sense that, you know, when I say find the hidden direction, what I really mean is like a The hidden direction, what I really mean is like approximate, let's say in L2 norm or in small angle. And you know, in some sense, you know, matching the first m moments doesn't tell you much because you might match more. But basically, usually, algorithmically speaking, people are assuming that, let's say, in the first m plus one moment, there's at least one moment where there's a significant difference. And for the purpose of lower bounds, which is the focus of this talk, really, I'm going to say that if this A distribution matches the first. A distribution matches the first m moments with the standard Gaussian, so the problem is hard in some sense. So, okay, that's the definition of the problem. So, is there any question about the statement? And so, another thing I'd like to say is that we can assume the algorithm knows A. Okay, so the algorithm that's trying to learn this hidden direction V can be assumed to know exactly what this one-dimensional density A is. Is okay now the message of the talk is that this NGCA problem is is um um you know it's quite it's quite general. In particular, it can be very useful as a starting point for proving lower bounds. And when I say lower bounds, I mean computational lower bounds. It can capture instances, interesting instances, not all instances, of several well-studied high-dimensional learning tasks, in particular by appropriately selecting In particular, by appropriately selecting this, you know, univariate distribution A, we can, you know, we can construct hard instances of various problems. Now, one thing I should have said in the previous slide is that really these distributions P V, they're all product distributions. Okay? But they're product distributions with respect to a non standard coordinate system. So if there's the V direction and the orthogonal complement, and the two components there are sort of independent. So A and the Gaussian and the orthogonal subspace. Gaussian in the orthogonal subspace. All right, so let me give you an example to motivate this problem. Now, suppose that, okay, so no, first of all, Pv has this form, okay? It's A of V dot x times some Gaussian in the total complements. Now, suppose that this A is a mixture of univariate Gaussians in one dimension, okay, something looking like this picture, then what is PV? What is going to what is Pv? Can someone tell me? So I take a univariate Gaussian mixture of Gaussians and I multiply it by a Gaussian in the orthogonal complements. So then what I get is basically a mixture of Gaussians in high dimensions. It's not an arbitrary mixture of Gaussian, it's a mixture of Gaussians with a very specific structure. In particular, sort of the high-dimensional components are going to look like parallel pancakes, where basically sort of the Basically, sort of the vector v is going to be basically the line of the means. So, all the means are going to be on the same line, and this line is defined by v. Okay, and it's not difficult to see that in this case, if you actually find or approximate the hidden direction, the planted direction v, then you have essentially learned the mixture. So, the learning problem is essentially equivalent to finding To finding this hidden direction. All right, so is the example clear? Very clear. All right, so let me go back to the general problem. So some basic facts. Okay, so the first observation is that the sample complexity of the problem, if I don't care about computation, the problem can be solved in a few samples. So polynomial in the dimension and the number of matching moments. On the other hand, On the other hand, all known efficient algorithms require many more samples than the information theoretic minimum. So, basically, in some sense, if you're very careful, you can solve the problem with D times M samples and all known efficient algorithms. And by efficient, I mean running time sub-exponential in the dimension, they use D to the M samples. So, again, D is a dimension, and M is the number of matching moments. M is the number of matching moments, so the right answer in terms of samples is d times m, while all known algorithms have a sample complexity d to the m, d to the power of m. And the parcel line of the talk is that this gap may be inherent. In particular, we're going to provide some evidence for a large class of algorithms, including all previous algorithms used for this problem. And here is the informal theorem that for any univalired distribution A that matches. A that matches its first M moments with the standard Gaussian, any algorithm that solves this problem either needs to use D to the M samples or has running time exponential in the dimension. Now, there are two caveats with this statement. First of all, you know, we cannot prove statements about any algorithm. So, this statement is true for any statistical query algorithm, which I'm going to define. Algorithm, which I'm going to define soon enough. And in addition to the matching moments condition, we need another niceness condition from the one-dimensional distribution A, which in some sense is needed. And basically, roughly speaking, we want A to be kind of continuous. We want it to have finite chi-square divergence from the standard univariate Gaussian. Right now, this theorem, so there is a formal version of this theorem, which I just told you what it is, was proved in this paper. It is proved in this paper with Daniel Kane and Alistair Stewart like four or five years ago. There is an analogous statement for sparse vectors. So if you're not searching for directions, you're searching for sparse directions. And basically, the message is that this hardness in the statistical query model allows us to obtain the first evidence of hardness for a wide range of problems that superficially seem unrelated. And here is some. And here is a sort of some partial list. So we can leverage this statistical query hardness to prove optimal lower bounds in this model of computation for a wide range of learning problems. Here is a list. I'm not going to go through them, just look at them. So the running example for the talk is going to be the learning Gaussian mixture models. Now, if you look at all these problems, they superficially might appear to be very different from each other, but it turns out that. But it turns out that you can view them in a unified way under this lens, under the lens of this NGCA problem. Any questions so far? All right, so let me move on. So I was kind of wondering, do you notice these rule out sum of squares at all or how what the relationship with sum of squares upper bounces? So I will comment on that towards the end of the talk. Cool. All right. That's a good question. Okay. So I still argue that the thing. Question. Okay, so I still owe you the definition of this SQ model. So the statistical query or SQ model is a restricted model of computation for learning problem solver distributions. So, you know, an unrestricted algorithm just takes the samples in the inputs, does any computation, polynomial time computation, and gives the answer. So in the SKU model, we have access to some kind of oracle. So we can ask questions. So these are called queries about the distribution. Of course, Distribution, of course, the immediate question is: what type of queries? So, there's some interaction like that between the algorithm and the oracle. The queries are adaptive, so the process of asking questions is adaptive. And the standard query that we use is basically we can pick any function which is bounded, this QI here. This is defined on the support of our distributions. Let's say the Let's say the values are in minus one, one. It has to be kind of bounded, they won't be arbitrary. And the oracle is going to give us an approximation to the expected value of this function under the distribution. Okay. And now, how do we measure the complexity of an algorithm in this statistical query algorithm? So the number of queries is a proxy for the running time and the minimum And the minimum error, this tau parameter is called the tolerance or the error of a query. So the minimum tolerance of a query is sort of a proxy for the sample complexity. So roughly speaking, yeah, we have this type of interpretation, which is not super accurate for upper bounds, but it is quite useful for lower bounds. So let me sort of try to interpret for UNS2 lower bounds. To interpret for unscular bounds. So, suppose we have proved the following statement that any statistical query algorithm for a statistical problem P either requires queries of tolerance at most tau, for example, or makes at least two queries. So this is a statement we can actually honestly prove mathematically. Now, the natural way we interpret this is as follows: that any simulation of this SQ algorithm with samples. Of this SQ algorithm with samples either requires at least one over tau squared samples or has runtime at least Q. Okay. And sort of the way we sort of, the reason we interpret it this way is kind of obvious. It's like, how would you simulate the statistical query algorithm with samples? You would just take samples and approximate every expectation by an average. And the point that you have a bounded random variable to estimate. Bounded random variable to estimate its expectation within some tolerance tau, you need one of the tau square samples. Now, this square here is not that important, but basically like, no, one over tau would be enough. Now, so such a statement can be viewed as some kind of information computation trade-off in the statistical query model. Let me again sort of reiterate that the first statement, the statement in the actual box, is a statement we can actually prove. And the second statement is our interpretation of it. Our interpretation of it. So, are there any questions about the SQ model? No, thanks. All right. So, now, you know, let me sort of give a sort of a necessary generic slide about SQ algorithms. So, first of all, it's a restricted model of computation. Okay, it doesn't capture all algorithms, but it does capture a very wide range of algorithms used in statistics. Of algorithms used in statistics and machine learning. And I'm not going to go through all of these, but basically, the only essentially known exception is Gaussian elimination. The SQ model cannot efficiently do Gaussian elimination, so we cannot learn parities. But this type of sort of disadvantage is a disadvantage for many other restricted models of computation. For example, the sum of squares hierarchy that Gautam alluded to has the same problem. So Gaussian elimination. Problem. So, Gaussian elimination is a very exceptional algorithm, and all these types of restricted families of algorithms that we have that basically cannot do linear algebra. But for all the problems that I'm going to talk about today, this is a minor issue because it doesn't seem that Gaussian elimination could be used to solve them. Now, suppose we consider the following hypothesis testing problem. So, we're given access to distribution D in R to the D. Distribution D in R to the D. And we're promised that either D is equal to some fixed reference distribution D0, or D is selected randomly from some family, call graphic D, according to some prior. And the goal is to distinguish between these two cases. Then there is sort of a standard methodology to prove statistical queue or bounds for this type of hypothesis testing problem. So if you haven't seen this stuff before, I expect most of you have not. Most of you have not. I don't expect to digest it in a minute, but believe me, it's something that you know can be worked out if you sit down for a few hours. And it parallels things that you might have seen. For example, like low-degree polynomial tests have a similar type of methodology. So how do we prove SQL bounds for a testing problem like that? So there is a notion of a pairwise correlation of two distributions P and Q with respect to the reference distribution T0. So this is the distribution in the completeness. The distribution and the completeness of the test. So, what we do, we take the product of the likelihood ratios. So, P with respect to D0 and Q with respect to D0. We take the expectation of that under D0 and we subtract 1. This is just for a normalization, like a shift. Now, so this is basically something, basically it allows us to get, you know, to it gives us an It gives us an inner product basically of the probability distribution. Okay, this, you know, the natural inner product is basically the chi-squared inner product, which is basically defined by this paragraph correlation. And now with this definition, it can be shown that to prove an SQL bound, it suffices to find a large family of distributions in the class, this class calligraphic D, with large pairwise correlation. Okay, and the And the tool to prove such a theorem is basically linear algebra. So there's a basic fact in linear algebra that is a vector cannot have a large inner product with many nearly orthogonal vectors. Okay, so this is what we're really using. So this is by a paper by Feldman, Grigorescu, Rev, Vempala, and Xiao, like four years ago. And it generalizes previous sort of scular bounds for Boolean functions. Now we're going to use such a theory. Now, we're going to use such a theorem in a black box way for our NGCA problem. Of course, the difficulty is how to construct such a family of distributions. So, how to construct a large set of distributions with the large pairwise correlation? How do we prove it? Okay, so I think we have introduced enough background to formally state the main theorem of the talk. So, we consider the natural testing version of NGCA, where we want to distinguish the standard multivariate Gaussian from an unknown hidden direction. From an unknown hidden direction distribution PV. Okay, so that's a problem. The reference distribution is a standard Gaussian in G dimensions, and the family of distributions is going to be Pv for all unit vectors V. And the theorem says that as long as the distribution A matches its first M moments with the standard Gaussian in one dimension, and the chi-squared distance between And the chi-square distance between A and standard Gaussian is finite, then any statistical query algorithm for the testing version of NGCA defined above either requires at least one query that has a very small tolerance, so d to the minus number of matching moments times the square root of this chi-square distance, or requires at least due to the constant many queries. So basically, the interval. So basically, the interpretation of that again is like either the algorithm needs to use something like d to the m, d to the omega omega of m samples, or exponentially the dimension time. So is there a question about the statement? Is the statement clear? Oh, clear. All right. And of course, you know, as long as this one-dimensional distribution A is far from the standard Gaussian in one dimension. Is far from the standard Gaussian in one dimension and other basic reasonable conditions, the sort of testing reduces to learning. So, proving a lower bound for the testing problem basically suffices to prove that the learning problem of finding or approximating the hidden direction is hard. Now, some intuition about why this problem is hard. So, I can provide two pieces of intuition. Okay, the first one is just an observation. It just tells you that to solve the problem, you need to look at the moment. To solve the problem, you need to look at the moment tensor of degree m plus one. Okay, and the reason is you know because you know the the since since a matches the first m moments with the standard Gaussian, then if you take the tensors of order at most m of this p A V, which is a D dimensional distribution, these are identical to the Gaussian moment tensors. So you need to look at high degree moment tensors. And then, you know, you have a very large multi-dimensional array. Very large multi-dimensional array with b to the n plus one entries, you know, what are you going to do? It's no, there's no obvious way to to uh to solve the problem by looking at high-degree uh tensors without looking at the entire tensor in some sense. And the second sort of statement is less obvious. It tells you that you cannot use random projections to solve the problem, so there's a formal way to prove this. So, uh, uh, so basically, uh, what we can show is that that um Show is that solving this testing problem essentially requires exponentially many random projections or a little bit more carefully, either we need sort of very accurate random projections that would require many samples to be able to leverage, or we need exponentially many random projections. And the key lemma The key lemma that essentially shows this statement is here. Okay, so what does this say? Let me try to pass it for you. I think this is like one of the few technical slides. Yes? Was this also a random noise? A random noise, yes. Okay, so yeah. All right, so what's the key lemma? I think this is like an interesting statement mathematically. So take one of these PVs, okay, this hidden direction distribution. Okay, this hidden direction distribution for a vector v. Okay, this is x, and look at the projection of x in a different direction in the direction v prime. So the projection is going to have density q, okay? And the statement is that if I look at the chi-square divergence between q and the standard univariate Gaussian, then this is going to be very small. In particular, it's going to be at most the inner product between these two vectors, v, which Between these two vectors, v, which is the planted one, and v prime, which is the direction where I project, so the cosine of their angle. And this is going to be raised to the power of something proportional to m, okay, times this chi-square distance, which is some finite quantity. It's independent of the dimension. So basically, like, what's going to be this inner product between v and v prime if v is fixed and v prime is random? It's going to be something like one over square root dimension, which means that the right-hand side. Which means that the right-hand side would scale as something like d to the minus m. Okay, so that's the reason why you cannot just use random projections to solve the problem. And here's a picture of what's happening. So V is the planted direction. V prime is some other direction we project, and theta is the angle. And sort of, you know, by definition, if the angle between the two vectors is 90 degrees, then what do we know about Then, what do we know about the projection Q? Okay, great. So, if the two vectors are exactly orthogonal, then the projection is going to be exactly a standard univariate Gaussian, okay, by definition. So, basically, the lemma is a robust version of this fact that if these two vectors, v and v prime, are nearly orthogonal, then this projection q is going to be extremely close to. Projection Q is going to be extremely close to be standard Gaussian chi-squared distance. Okay, now I'm not going to go into the proof of this, but the proof is analytic. It's just like Fourier analysis, or basically like maybe the right term would be Hermit analysis. And so the important observation here that is not difficult to show is, and this motivated this construction, in fact, that if you take this Q, this projection, it has an explicit form, it is basically. An explicit form, it is basically the Ornstein-Ullenbeck operator with parameter theta applied to A. Okay, and roughly speaking, this is like the noise operator, the Gaussian noise operator in computer science. So the idea is that u-theta of A is some kind of smeared out version of A. And as such, it only retains the most prominent features of A, which are basically. Most prominent features of A, which are basically its log degree moments. Okay, now, once you have this type of formula that Q is equal to the noise operator applied to A, we know how to diagonalize these operators. You can just write down things explicitly and prove the lemma. And basically, the proof is not difficult. So the main fact is that Q is equal to U theta of A. From this point, it's an exercise. All right, so now, given this lemma, the proof of the sqular bound is relatively easy. So, recall what we want to do. We want to find a large set of unit vectors v such that these corresponding distributions p sub v are nearly uncorrelated. And it turns out it suffices to take a random set of vectors to do that. So that's the second step. So we can actually show that any pair of such vectors have small correlation. So as long as V and V prime have inner product, which is basically less than a constant, but less than one tenth or something, then it's good enough. And sort of the way to actually prove this is to understand that this correlation in high dimensions. In high dimensions between Pv and Pv prime with respect to the standard Gaussian can be reduced to a two-dimensional quantity. It's literally equal to the correlation between A and u theta of A. And then after that, we can use the lemma that was in the previous slide and some basic sort of bounding, basic Cauchy vars, to show that this two-dimensional quantity is at most the cosine of the angle to the number of matching moments, and the cosine of the angle is just. Moments and the cosine of the angle is just the inner product of the vectors, and that's it. All right, so that's the only proof that this talk has. Any questions before I move on to the applications? No questions. Okay, so now, so let's go back to the main sql bound results. So, suppose that A matches the first M moments with a standard Gaussian, it has finite chi-square divergence from it, then any square. Then any SQ algorithm for the testing version of NGCA either requires a query of very small tolerance, d to the minus m, or requires 2 to the d many queries. So basically, the claim is that we can use this main theorem as a recipe to prove a scular bounds for many learning problems. So let's say we're presented with a generic distributional learning problem, pi, for which Any problem, pi for which we want to prove an escalator bound. So basically, we're going to construct a family of instances of pi by encoding it as NGCA. Okay, and the objective is to do that while matching as many low-degree moments as possible. Okay, so these are the two sort of data we need to construct valid instances of this problem pi that are of this form. That are of this form, and we want these valid instances to match as many moments as possible. So, let me give you an example. So, let's see how we do this for mixtures of Gaussians. So, the lemma here is that there exists a univariate mixture of Gaussians, of k Gaussians, whose components are actually pairwise far from each other, such that this agrees with the standard Gaussian of the first two k minus one moments. So, A is going to have exactly the same two k. The exactly the same 2k minus 1 first moments with n0, 1. And at the same time, a is going to be very far from a standard Gaussian. It's going to have total variation distance, which is going to be at least some constant. And the way that we prove this is not difficult for this particular problem. We can just use a Gaussian quadrature. So we start by constructing a discrete distribution with support, let's say, k, where is the number of components, matching its first 2k minus 1 moments with n0, 1. k minus one moment with n0 one this is what quadrature gives us and then we need to you know this this distribution is not enough okay because um uh its chi-square divergence from the standard Gaussian is infinity so to get a finite chi-square divergence we need to rescale it and add the scale the Gaussian so standard things and we get our you know one-dimensional distribution a and you know this is basically the example I showed you in the beginning so the conclusion so the corollary we get for So, the corollary we get for a mixture of Gaussians is that in the SQ model, either an algorithm needs to use at least dimension to the number of components, many samples, or it needs at least exponential time. Any question? All clear. Okay, so sorry for asking so many times, but I'm not in touch with the room. I'm not sure exactly what's happening. Okay, so and so let me. And so let me note that sort of the instances, the hard instances for this problem, what I described, you know, what comes out of this construction, they look like parallel partics. So basically, this is how the instances look like. So all of the Gaussians are going to have the same covariance. Okay, the means are going to be on a line. And there's only one direction where the covariance is not one, it's smaller than one. And basically, the only thing that you don't know is this hidden direction, which is the direction between the means. Between the means. All right, so let me conclude. How much time do I have? Your time is basically up, so you can conclude. Okay, great. So great. So now the same recipe can be used for many other problems. So here is a list. The point is that NGCA captures SQ hard instances for several well-studied learning tasks. Now, to actually prove each one of those, we need Each one of those, you know, we need to do a lot of work for some of them. In particular, this moment matching thing might be very challenging or even impossible in some cases, and we need to do other things. So to conclude, sort of, you know, like the point is that NGCA is arguably a central problem to study and prove that's computationally hard. It's a natural starting point for hardness results. So for a range of problems, it sort of leads to the first. It sort of leads to the first sort of trade-offs. And sort of the first open problem is: can we obtain alternative evidence of cardinals for some of these problems? There has been progress on this question already, like since that paper, for example, for robust pars mean estimation and for learning mixture models, people have obtained reductions. And in fact, these reductions use essentially the same instances that we saw. Instances that we saw are SQ hard. And the question is: how general is this phenomenon? Okay, so like, can we prove this for other problems as well? So can we use SQ hard instances as gadgets for reductions? The second of the problem is, I guess, what G alluded to. So, can we prove some of squares lower bounds for NGCA? So, it's not clear exactly what this means in this setting because. What this means in this setting because you know NCCA is not a CSP, so this would depend on the formulation of SOS. But yeah, it's an intercomplete problem that the answer should be yes. So I think the same like the same hard instances are hard for SOS as well. I think at this point, I'm going to conclude and take questions, if there are any. Thank you. Sorry, why is open problem three not like totally well defined? Can you say more about that? Yeah, I mean, the answer to that. So, basically, like this sum of squares proof system constructs a family of convex programs, right? And the question is what type of variables and constraints you're going to use depends on the setting. So, all the things that you have seen in the literature, for example, like for CSPs, there is a standard SDP that you strengthen using using. You strengthen using the SOS proof system, by you know, you add more variables from more constraints. So, there's a systematic way to do that. But for a learning problem, you can choose the optimization formulation on your own. And it's not clear, you know, if you add more variables, add more constraints, what's going to happen, right? Yeah, but in general, this is an issue also for CSPs, okay? Like, for example, for the planted click problem, they're sort of, you know, if you change the formulation, Sort of, you know, if you change the formulation a little bit, you know, you need a completely different proof of hardness in the SOS system. Yeah, basically, you're saying the lower bound is against a very specific thing that it can't be certified, but maybe there's some other optimization problem that does work. That's right. Yeah. Anybody else? Thanks so much. Thanks so much. This was really interesting. Thank you. Thanks again. All right, so let me stop sharing. Yes. Cool.