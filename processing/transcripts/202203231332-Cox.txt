Alex Cox, and it's going to talk about the controlled measure palette martingale's a viscosity solution approach. Thank you, and thank you for the introduction. It's very nice to be here. It's very nice to be at a conference rather than following online. So, thank you, Jan and Martin, and the first for the organising. Yeah, so I'm going to talk today about this controlled measure by Lee Martindales. The idea, well, perhaps I'll start by saying what I'm Well, perhaps I'll start by saying what a measure-valued marking guy is. So, the thing I really want to talk about, a stochastic process is taking values in the set of probability measures, and we're going to assume that actually our processes have an additional martingale assumption. So, in particular, there we go. We have this sort of definition that an MVM or probability measure value of Martingale has. Probability measure valued Martingale has this key property that if I test it against, say, continuous and bounded functions, right, the process I get is a martingale. For any continuous bounded function, I can test my measure-valued process against some arbitrary function, and what I get back is a martingale for any function phi. Where do these come from? So the canonical example that is very helpful to think about is to think about taking some random variable in some filtered program. Variable in some filtered probability space, and just taking the conditional law of that process. Okay, so I have, so I take some random variable x, I condition its value at time t being in some set A based on what I know up to time t. That clearly defines a probability measure, but it's also a martingale because of the filtrate and the conditional expectation. So that's the sort of example you have in the back of your mind. So, that's the sort of example I have in the back of your mind. But I'm going to hopefully explain to you that these are very natural things that arise all over the place. And I'll give you some applications and examples in a minute. And Sigrid will also, in the talk, after talk about this a little bit as well, and perhaps talk about one or two other possible directions. But for this talk, I'm interested in basically thinking about controlled versions of measure-valued martingales. So I'm going to think about the formulation of a stochastic. The formulation of a stochastic control problem, and in this formulation, I'm going to have a cost function, for example, which will depend on the position at time t of my MVM. I'm going to use a discount factor to make things nice and well behave, to make sure things are integrable. And our problem basically is to minimize the expected value over some class of measure-valued martingals. Okay, so I want to know which MBM is going to minimize some quantity like this. Is going to minimize some quantity like this. And the basic approach we want to use is the classical stochastic control form sort of dynamic programming approach. So this you could just think of as being some minimization over a class of MDMs. I want to try and understand this from the sort of classical perspective of stochastic control where we're going to apply dynamic programming and hopefully get some sort of Bernoulli principle HJP type equation to help us understand how that might happen. How that might happen. So, in particular, I need to tell you how I'm going to control them. So, I'm not going to do that quite yet, but we're going to introduce some sort of control rho, which is going to be affecting the evolution of the process ψ. And then we may want to actually have some cost function that depends on that control. So, we may want to have some either feasibility or even just sort of cost function depending on the. Sort of cost function depending on the choice of rho that will come in potentially to our problem. Okay, so we can then think of a value function for our problem in the classical sense of given my MVM starts at a particular measure mu and it evolves according to some particular choice of the control. So some admissible pair. So I think of some admissible solution psi together with its control. I want to minimize the cost of the control process over some infinite horizon problem. Over some infinite horizon problem, and effectively, I then want to understand. Well, I'd like to understand what the optimal controls are, but primarily our object is going to be the value function corresponding to this problem. Okay, so this is just some real, so function on the space of probability measures, but taking real values. Okay, and that's sort of what we want to try and understand, hopefully to try and understand values of some of these problems. So, our main aim in this talk is really to try and make sense of all the quantities that come into this object. That comes into this object, right? I mean, it's a very natural thing to look at, but obviously we need to talk about how do I understand this, what can I say about it, and so on in practice. And that's what I want to try and do today, just to make sense of this equation. Okay, why am I interested in these problems? Well, given the topic of the conference, one of the main motivations comes from a slightly simple version of Martin-Gale optimal transport. So one formulation. One formulation here is to find a Martingale n with a given terminal law, and I'm going to start it not in some general distribution but at a fixed point, which is then going to have to be the mean of my distribution. And I want to choose this martingale in a way that, for example, maximizes some functional. So you could take, for example, some function of the running average of the process between now and some fixed time t, and you could say, Fixed time t, and you could say, okay, what is the martigale with this conditional terminal law that maximizes or minimizes, say, some function of its average or some other path-dependent quantity. And the key point is that I can transform this problem here in terms of the market out m with a terminal condition, this one here, to a statement about the evolution of my measure-value process. Okay, so over here I've got the measure-value process, but I'm in. Process. But I'm integrating against a test function, in this case x, which is, well, assuming that these two things are equivalent, right? So assuming I take the measure value process psi, which has the initial law corresponding to the conditional terminal expected value of the Martindale, right? I can condition on this to give this measure value process psi. When I integrate, I get back mt, and these two formulations basically become the same through that velocity. Reliable stuff. So the key, so this is sort of Martinique-optical transport, except here I have to assume a trivial initial law. You can sort of play around a bit if you make your initial filtration non-trivial. This conditioning could give you a non-trivial initial law, but it makes a lot of the other problems quite hard. So I'm going to consider this specific case here. And why is this helpful? Well, if you want to think in terms of To think in terms of classical sort of dynamic programming principles, this terminal condition is not a very nice condition. It's a condition about the average of all of my probability measures. So I have to worry, if I run my problem for a bit and I want to decide, okay, halfway through what's my optimal strategy, not only do I have to worry about my current state, I have to worry about To worry about my current state, I have to worry about all the other ways I could have got there because they're all going to affect how this terminal distribution would have affected things. So, effectively, this formulation here, there is no terminal condition, right? It's just a starting point. So, I move the terminal condition up here to the starting point of my process down here. And so that means that rather than having this nasty sort of distributional constraint at the end, the whole thing becomes part of my state variable. The whole thing becomes part of my state variable at the start. And then the martingale property becomes the sort of thing that's constraining how I'm allowed to change my measure, because it's sort of coming from this condition. So the sort of the, I mean, it's turning what is a sort of one-dimensional problem with a nasty constraint into an infinite-dimensional problem with no constraint. And now, that sort of, okay, the hope is that that. That sort of, okay, the hope is that that switch at least allows us to use much more classical methodology to try and at least think about the problem and gives us different ways of thinking about how to do things. Okay, everyone, any questions? Sorry, please do interrupt if anything's not clear. So, of course, another motivation that's closely connected to that, it's really the same problem, but I guess sort of where a lot of the market get optimal transport has come from is, of course, this is also. Come from is, of course, this is also in a financial context the problem you have in this model independent option pricing. So if you think about the example I had on the previous slide, right, well, your terminal distribution now is what you get from option prices. It's what tells you the law of your asset price at time t. You know there is some risk-neutral measure, so you can condition under that law. Sorry, under that risk-neutral measure, the full prices give you the law of your asset price at some future date. Asset price at some future date, and the fact that you have a risk-neutral measure tells you that the whole process is a martingale. So, you sort of end up with these conditions coming through. And then, in the financial context, this example of an average is basically this problem of finding model-independent bounds for the price of an agent option in this example. Okay, but I mean, this is completely equivalent modulo relabeling all my variables to the formulation I had on the slide before. So, they're really the same. They're really the same problem. So, this is a problem we looked at originally, me and Sigrid looked at a paper in 2017. So, we basically showed that these two things were equivalent. I think I said this, but this risk neutral or the Martingale condition on the core prices, or on the distribution, the MVM condition, is effectively the state variable. So in this case, the measure values. Sorry. If you think about the financial context, call prices should be martingales. So the conditional expected future value of the call option should be a martingale because it's a traded asset price. So under the risk-neutral measure, that tells me that the certain distribution of properties of my terminal value should be martingales. Of my terminal value should be martingales. But if you then try and work out what that means in terms of the conditional law, it's exactly that the conditional law of s at time t is a measure value of Martiga. So you can recover the Martigau properties from the financial context, but you can also just, I mean, it's also very natural when you formulate it the first version. So this one here is the first paper. We sort of generalize some of these ideas to, in this case, American type options. American type options. So, using this measure-valued Martingur framework, I have a paper with Erhan and Jebor Stov where we did this for a slightly different context. And okay, well, other sort of things, well, why might we want to do this? I mean, in the context of option prices, it's very natural to look at the mean. So, you would think if I go back a few slides, If I go back a few slides here, right, I chose the mean as my state variable. So if you have an option that depends on asset prices, you would typically only expect the mean to come in. But you could imagine other sorts of options where you may want to build in payoffs that depend on future call prices, for example. So these are things like. So these are things like VIX options, where in fact it may not be just the current mean of your conditional measure, but other properties of the measure that you might want to minimize or maximize in these payouts. So you could imagine circumstances where actually it's not just the path dynamics of the mean that are important, it's the path dynamics of the conditional measure. Okay, and that could come up in certain things like bits options. So that was So that was one example. Another example that I'm sure most people who know where these come from have are aware that this is also closely related to the marking electron transport problem. The same sorts of things come up in the scoring embedding problem. So given a measure mu, Brownian motion B, the scoring embedding problem, we saw a talk yesterday on this, right? So I want to find a stopping time for my Brownian motion with law mu. With law, such that the process is uniformly integrable, and I may want to maximize some sort of path functional over stopping times. And one very natural class of these sorts of problems occur when this path functional f is invariant to time change. And then I can basically do the same thing. So, and again, we did this in this paper with Sigrid. Basically, you can reformulate the score-high embedding problem, the one-to-one correspondence between the embedding problem, and the optimal. Embedding problem and the optimization problem for measure-valued marking gears, which terminate. So, by that, I mean that as my time goes to infinity, I need to end up converging weakly to something that's a Dirac. All the information in my measure-value process has to basically boil down to one Dirac. And provided I have this condition, then I can make a one-to-one mapping between solutions to the scrollard embedding problem. Solutions to the scrollard embedding problem with this, well, and the optimization problem with this time invariant structure, and MVMs with this termination problem. Okay, so this is equivalent in that case to solutions to the scorer embedding problem. A third example which appears in the literature has been quite well studied. Particularly, there's a series of papers by Cardi Lagay and Rainer from about 10 years ago that goes back to, I think, economics literature. To I think economics literature, Hellman and Mashler in mid-90s. Think of a two-player game. So, in a two-player zero-sum game, you could imagine, okay, there's a reward of my game that depends on some hidden parameter theta. But this parameter is known to player one, but not to player two. So you can formulate continuous time games where the two players are playing some sort of strategy against each other, and because players are playing. And because player one knows the true value of this parameter theta, there is in principle some information leakage to player two. If player one does something which looks like there's a high value of theta, player two might start to believe theta is big, and so their posterior measure will evolve, and so they will update their belief about the law of theta to some new process as the two players play the game. Okay, and as the game evolves, Okay, and as the game evolves, what you get is then, well, there's some stochastic framework, and then in that stochastic framework, what you see is this process psi, which represents the uninformed player's conditional knowledge of the true parameter theta, is again a measure-value market. Okay, and so what's player one doing in this context? Well, now player one will, I mean, they have to choose their strategy because they know everything. Because they know everything about player two's strategy, and they also know theta. They're going to choose their strategy to sort of control, effectively, the evolution of the conditional belief of player two, which is psi. So the player one strategy becomes a control problem, a one-person control problem. This is what's shown in these sort of papers by Cardinal Agay and Rainer. What they show is that what you end up with when What you end up with when you sort of act optimally for player two is player one has to solve a problem which corresponds to a control problem for the state variable psi over some modified payoff depending on the optimal action to play it to. Okay, and so they can formulate all of this set up as a control problem. Sorry, I went the wrong way. A fourth example that I quite like is sort of a very natural one. I've not seen this in. Very natural one. I've not seen this in the sort of maths literature, although if anyone knows any of the references, I'll be very keen to hear them. You have a sort of a problem called a Bayesian search problem. So imagine I have a Poisson process on R with some intensity measure which depends on some unknown location 1. So my Proisson process is a sequence of dots. I'll try and draw this over here. Draw this over here. Right, so over time, what you see is just a sequence of online see me on the board? They should know that people can. Can someone online confirm? Can you see Alex writing on the board? No. No. Is there something I can do to the camera? No. Ah, what about now? The camera seems to have done something. Yes, now we can't see them. Okay. Right, sorry, so my custom process is the one. So, my croissant process is the white dot, which is just the background noise. But behind this, I'm going to have some location y. And this location y gives me an extra sequence of points. So, but so alpha, so alpha delta y means like a balloonie at y. Alpha is the probability of picking y. Probability of picking y? I know I'll forget what y means. So I have a Poisson process with intensity Lebesgue with an additional Dirac at the point Y with mass alpha. So everywhere the Y ones are just the vague density. So the Y can repeat, but the other points of course won't repeat. Yeah. Yes. Yeah, okay, so there's a yeah. So the idea is then you imagine saying something like this, and you want to know where y is, okay, and you want to try and update your posterior of y based on what you observe. And if you imagine some sort of appropriate scaling now, right, where you sort of have to smear this out slightly, but you sort of look at where your process your points come in, you scale everything in your Gaussian, sorry, so you scale the So, you scale the counting process, you scale the noise down, and you sort of observe the number of points according to some probability based on this kernel gamma. So, I choose some site here, say this might be my yt, and what I observe is a count according to some probability of this thing. So, I'm more like to take all I observe is the count of points, it's a single count. Count of points, it's a single count, but I'm more likely to observe, say, this point because it's close to here. Slightly less likely to observe these ones, and I just see some sum of the points I observe in this sort of window. So a simple version would be some sort of box. I see how many I have in the box, but you can imagine some variable case. So six, it should be a symmetric function. So now what I observe is just the count of ones close to the point I. ones close to the point I choose y. And I, well, I don't like Pross and I want to get a Brownian limit, so I scale everything. So I have to scale my signal-to-noise ratio alpha to get a meaningful limit. And you can do this to get a Brownian scaling in the limit. And what we expect to see now is, well, our belief about where this true location y is is a controlled measure-valued process ψ t. I'm going to be an MVM, starting with our initial belief about where Y is. Initial belief about where y is and updated as we see more information. So, what might you try and do? Well, you might say, okay, I'm going to play for, to keep it nice, a random exponential time. Someone's going to come and stop me after some exponential time and say, where is your, what's your posterior belief about the true parameter y. Right, and maybe you want to then minimize the variance at that time. Okay, and so this variance now, well, what my control is this class y. Okay, and if you do. Okay, and if you work out what happens in the Brownian scaling, you can see the conditional law of or the conditional posterior density of the true value of y is going to behave according to some sort of process like this. So I have just a simple Brownian scaling, a simple Brownian SDE, but I have some measure-dependent evolution. Some measure-dependent evolution term depending on my current value - and my control of process 1. Okay, and if I do this, again, effectively because of the Bayesian nature of the problem, my process psi will be an MBM, but I have to sort of choose things like, am I worried about having large tails in my prior? If I want to reduce the size of my tail, I probably have to explore out there, but if I explore in the middle, I'm more likely to detect a signal. Okay, so that's another example where these. Okay, so that that's another example where these sorts of control problems come up. You happy with that? Yeah, I'm still a bit uncomfortable. I understand the point. Uncomfortableness is from the fact that once you put a director mass, of course, Poisson processes must have finite mass over compact sets. In this case, you can't, because take any line, vertical line, it will have a voltage mass, right? Because that's why it can happen. Delta mass, right? And so I'm guessing what you're trying to say is that there is a. I'm guessing what you're trying to say is that there is an extra OSM process at Y, which is in the process. So that's the framework we want to do. These are all examples of particular cases of this. So there's a lot of literature, especially at the moment, on stochastic control, particularly of things like McKean's loss of equations. But I sort of want to point out that actually what we're But I sort of want to point out that actually what we expect here is quite a different sort of dynamic. One of the key things that we're going to see in our dynamics is that we don't have spatial motion. Our measures don't spread out. They tend to concentrate. You can prove this, right? The variance, I'll say it in the minute, but the variance is going to be a supermarking gale, so the variance would always be decreasing. There's also connections to controlled filtering equations, right? So there's quite a Filtering equations, right? So there's quite a historical literature on this. But one of the differences I want to do is not to. So most of this literature relies on embedding our problem into a nice function space. I don't really want to do that. I want to work as we're keeping probability measure values processes as a nice state space. Much more recently, so within about, I think the first one's about the same time as we've brought this paper out, there's been a couple of papers looking much more from the filtering context by Martini. Context by Martini, trying to do some of the things we're doing in a slightly different context, so it's not completely equivalent, but there's a lot more overlap or at least philosophical connection to these papers by Martine. Again, there's a long history of measure-valued processes. These, what we're talking about here are very closely related to the Martingale measures of Dawson, which came about in 93. And again, so probably the direction we came at this, thinking about the score on investment. And we came at this thinking about the scorehard embedding problem, partly inspired by this paper from Ronin El down in 2016, where he did some of these examples for constructing solutions to the scorehard embedding problem. And there's a, I think, a connection to stochastic localization that I'm not sure we've yet worked out, but we'd be keen to see how that develops. So that sort of is the background. Just talking about nice things you can work out about these processes. These processes. So there's lots of nice properties. So, in particular, I can put the Wasserstein P metric on my space of probability measures. I know that if I start with some P growth condition, I will preserve it. If we assume weakly continuous trajectories, which in general I want to do, then again the trajectories will be continuous actually in the special size space. One of the key properties for One of the key properties from our perspective is that the support of the measure value of Martingales is always decreasing. So, as time evolves, I can learn that my process is not in a set, but I can't suddenly acquire information that tells me it's somewhere that I didn't believe it was before. But it's not the case that future values of the MPM are absolutely equivalent to historical values. So you can You can see this. So, one simple way of seeing this: imagine I want to do, I'm going to take a Brownian motion. I'm going to use my trick of conditioning the terminal law. I'm going to run my Brownian motion to time one. At time one, depending on where I end up, I create intervals of, so if this is P1, I create I create a sort of an interval at b1 plus 1, b1 minus 1, and I run into one of these two points. And I now take the mvm, which is the conditional terminal value of my Brownian motion, and it's continuous from here up to here, it's just the weighted sum of two Gaussians. But when I hit this point, it devolves into 2 to rats, and then it evolves as 2D rats. So there's a, I mean, it satisfies all the other properties, but it I mean it satisfies all the other properties, but it it doesn't maintain absolute continuity with respect to historical data where I can devolve to the rats, and that's reasonable in this context. More generally, okay, the variance is a super Martingale, provided I can make sense of the variance. I mean, it's very easy to check this, right? I've got something that's a martingale minus the square of a martingale, so that's always a super martingale, so the variance is always decreasing. Supermarket out, so the variance is always decreasing. So you expect these things to get smaller and smaller. And from a one of the key properties for us is that if I have a continuous MVM, I can localize it in compact sets using the Lavale Bussin. So this is going to be a nice technical property that allows us to say a lot about the future evolution. And the results we're going to have is a helpful way of getting compactness from something. So now, what do I want to tell you? So, now what do I want to tell you? Well, basically, our results follow what is now a very classical structure. So, I want to tell you what I'm going to first of all define an appropriate sense of a controlled MVM. So, I need to tell you how am I going to control them. So, we're going to have to introduce some restrictions. I can't just look at arbitrary MVMs. We don't yet have the technical ability to do that, or at least to get the results in that context. So, we're going to introduce a specific type of control. Of control. We're going to prove the dynamic programming principle for this class of controls. That's relatively straightforward. An Ito formula. Sorry, I'm giving a bit of a head start. So again, an Ito formula for these, right? We can characterize martingales, set up an HJP equation, do sort of verification for smooth value functions. And then I think this is probably the really non-trivial bit is starting to work out what viscosity solution might mean in the sense. Solution might mean in the sense, right, we've got control problems, we don't expect a priori. So, if we're talking about the value function, we don't really expect any a priori regularity of that value function. So, we need to have an appropriate notion of solution to talk about what we mean by our value function. And so, we've got some sort of weak, or viscosity solutions is the appropriate weak sense for these types of control problems. And then, of course, the key thing about on the hard bit about the viscosity solutions is getting a comparison theorem. Is getting a comparison theorem, and we can do that, which allows us then to deduce uniqueness. Okay, so the whole process here is incredibly classical for control theory. The novelty for us is doing it in this framework and being able to particularly get these results out. So, one way. So, probably the key definition here is how do we specify our dynamics? Okay, we want to work in a Brownian setup, so we're going to assume some sort of Brownian. Um so we're going to assume some sort of Brownian filtration. I want to take so in particular I mean just using Martingale representation theorem I can write down the evolution of an arbitrary function phi just as some right sum process sigma but of course we need this now to be appropriately sort of uniform so in fact what we can do is we're going to construct a is we're going to construct a sigma so that this process up here is actually an integral, typically then expecting that this happens. And well what else? Well so sigma, we can assume that sigma evaluated on constant functions is zero because if we I mean this is a probability measure so if I put any constant function here it shouldn't there shouldn't be any randomness. So, in particular, under those conditions, there's some old results of Mark Yule that tell us that we can, in fact, find a function rho for which this holds true. So, the covariance effectively with respect to the measure psi is equal to this process sigma t of rho for all phi, all nice functions phi. So, if we use the notation covariance mu, the covariance with respect to the measure mu of two functions. With respect to the measure μ of two functions, we get an SDE for the measure-value process that looks something like this. So, this is what we expect for a given, so we're going to choose our control row, and we want to then define the measure-value process to solve this STE. Okay, so if we can write our measure-value, well, the assumption we're going to make is that we can write our measure-valued martingale in this form for some appropriate control rate. Okay, and so that's the key restriction. Okay, and so that's the key restriction to the class of MVMs we want to take. So once we have that now, okay, I can be a bit more specific, right? So our weak solution is going to be some probability space, quick with a Brownian motion, everything else, and a measure-value process psi, such that for any choice of, appropriate choice of function phi, we get the SDD solve. Okay, so that's what we mean by solution. Okay, so that's what we mean by solution. And we're thinking in a weak sense now because our solution is the whole of the probability space, the control all together. We're not fixing our probability space and then choosing a control. We're doing it all together. So what do I want to say about admissibility? So obviously we need to put some restrictions on the choice of control in order for things to be well behaved. And in particular, we're going to take some And in particular, we're going to take some p and q satisfying this relation, and some space h of measurable real functions, which is going to be the set that contains our control. And then an admissible control is going to be a weak solution of the MDM such that this process is almost surely in this set H, and we have an integrability condition on the process as well. And okay, when we have some conditions that allow us to get these in nice context. And okay, I've written this just as a class H, but actually, this H here can also depend on psi, which allows sort of state-dependent controls as well. So you can actually have, I mean, that's not very hard. Modulize some conditions on quite how you specify that to include state-dependent controls. Am I going the right way? So once we have this, So, once we have this, basically we can prove a dynamic programming principle. So, if you're not, you haven't done lots of control, what do we mean? Well, I can rewrite my problem, so my value function, as the infimum over a suitable admissible pair of control and process, of up to some stopping time t, I follow the control rho, construct my MVM, and then at the stopping time t, I score the value function start. The value function started again at that new future state. So that's the classical Hamilton Bellman type principle. And we have that here. I mean, the proof is fairly standard. I mean, there's a very nice general framework of Zipkovich that we fit in. So you can basically apply his results with a few, provided you're kept in the right places. One key thing, so thinking now about how we so So thinking now about how we, so, sorry, so let me step back. So now we want to know, okay, actually, can we, we have a Bellman, so we have a Bellam programming principle. Can we say a bit more about what this function v might be now? Right, so we're going to use the sort of, I'm going to try and talk now about on infinitesimal time scales. So we're going to say, okay, what happens if I follow a particular strategy over a small time scale? Can I characterize optimality? So in order to do that, actually, one of the non-trivial things we need to know is Things we need to know is actually: can I actually do that? Can I follow an arbitrary strategy and follow it for a fixed period of time? Because if I can't, it's going to be very hard to make sense of the value or the change in my value for a fixed strategy over a short period of time. And in this context, because it's, I mean, because we've only sat well, the definition of admissibility for rhobar is not quite so straightforward. Quite so straightforward, right? This is a non-trivial statement. But it does turn out to be true. And we can actually construct for a fixed function robot, we can construct a solution to the MVMSDE for a given starting point, which follows the same strategy for a fixed period of time. Okay, and this, I mean, needs some sort of quite careful construction argument. And then, okay, the next step is the eto. And then, okay, the next step is the Ito formula. Again, we can do this. It's not, I mean, there's a lot of literature around based on these sorts of constructions. The right notion of derivative for us, and again, if you know this literature, there's various different notations floating around. We use this version from Carmona and Dela Rue. Okay, so we want to differentiate an arbitrary function f. So it seems to be. So it's used to be smooth function f in our measure. And okay, modulo various growth bounds on this function, right? We need basically a function f that satisfies the fundamental theorem. Sorry, a derivative function of this form, which is now going to be a function of position and measure. And we need to integrate it against the difference in two probability measures to get the difference of the functions. So that's basically the key property we need. Property we need to define our notion of derivative here, and you can do this twice, and second derivative now will be a function of x and y and the current state of the measure. Okay, so that's the notion we want to use here. I think this is the linear functional derivative in the total, almost the linear function derivative of Kamehameha and Delaru. We have a slightly different condition here, modified it slightly, basically the same. Basically, the same. And then, once we have this, it's possible, and I think Sibri is going to say a bit more about this, to prove an Ito formula in this framework. So, again, if we have a weak solution, psi rho, for this SDE process, and under some technical conditions, then provided our function f is sufficiently smooth, we get an Ito formula, so we get a Brownian term plus a classical term, which Pass for term, which again now features the first derivative and the second derivative, and it's linear in the sense I've just talked about. And okay, so this measure s now, sigma sorry, is given by, if you remember, we had the control, we want to normalize, so we get psi s of rho as our new normalization in this sequence. Everyone? Okay. Any questions? So, where is the assumption about so here the existence of sigma is your assumption, right? Well, so sigma is defined here, right? But we have a psi rho as a weak solution of the MBM SDE. I need to have an admissible solution to the SDE in the sense that we already defined in order to be able to make sense of this. In order for this, well, I mean, this might hold in some more general context, right? But the context in which we proved it is this one here. So that's where certain control over. That that's where certain the control then comes into this definition signal. Is that what? Yes. Right, so that gives us some notion of eto, which allows us to start talking about HJB equations. So now our HJB equation is going to look something like this, and we need to be a little bit careful. So this is basically now the operator that we've the operator So this this this one here is the key operation that's going to come out for the drift term. That's the one when we think about our process is the one we want to be able to say something about. Okay, and the standard sort of arguments here give us, okay, we have the beta times u, which comes from the um discounting in our in our formulation. In our formulation, we have a supremum now, which is where we choose our optimal control. So, for our optimal control, this term here should be zero, so the future gain in the value plus the cost should cancel out, modular for discounting. So, that's the standard sort of HJB type term. But we need to be a bit careful about what happens to our MPM. Remember, I was talking about terminating problems. Once we end up at a Dirac, End up at a Dirac, right? Our control doesn't do anything. MVMs can't move from a Dirac. So we also have to have a boundary condition that tells us what happens when we end up in the Dirac, which is this one here. And okay, if we just define our cost function in this way, we can choose an arbitrary row. So we're going to basically end up with some fixed cost once we're in that terminal state, which gives us a boundary condition now for this PDE. So, once we're there, okay, now we have a theorem, right, which tells us basically that given all the stuff we've done before, and under some assumptions, okay, so we have our fixed set of actions H, and the key thing about most of our assumptions is we can reduce to testing them not on the full cl class H, but only on the class of continuous and compactly supported functions in H. Compactly supported functions in H. And because we can do that, that actually technically allows us to do a lot of the checking of conditions, or allows a lot of the arguments to then go through because we can do most of our testing against nice, continuous, compactly supported functions. So, although some of these conditions look quite strong, we're only testing them now not on arbitrary functions but on compactly supported continuous functions. Supported continuous functions actually make some of these statements actually quite a bit weak. But yeah, so provided we have these, so that's the key statement here, then what we do get is our value function is a viscosity solution of the HJT equation. Going the wrong way. So I need to tell you what viscosity solution means. So that's one of the other key things, which is because we have this decreasing support property, right, when we check. Right, when we check viscosity properties, we're only going to check them. So, we need to so normal viscosity arguments, you need to check something about limbSERPs of functions at given points. One of the key restrictions we can make is we only need to check our Limb SERPs against possible measures that might occur in the future. So, in particular, we can only look at ones whose support is either before or after the Or after the one we want to see. So, particularly in this case, typically their support is contained within where we're going. Once we have that, we can basically then get some. So, with the exception of this condition here, most of our viscosity solution, or most of our definitions, are fairly standard. So, we can define now our Hamiltonian, and then our viscosity sub-solution property is pretty much the standard. Is pretty much the standard thing. We can test for continuous functions on the right, but we only need to test against measures with decreasing or increasing support. And that allows us to define a notion of a sub-solution and a super solution, basically the same thing. And we get a solution is both a viscosity sub and super solution, and it's easy to check now. And it's easy to check now using the ECO formula that every classical solution is a viscosity solution. And then the hard bit is basically the comparison theorem, right? It's no good claiming our solution is a viscosity solution if there's lots of viscosity solutions. We want some sort of comparison result. So again, it's mostly classical. The main things are the technical conditions. So we need continuity of the cost function, but only on finite. But only on finitely supported measures. Right? And okay, there's some uniformity. And the other second one is we need some sort of boundedness on the set of controls that we look at. So provided we have those two, then in fact we can get comparison. Okay, and then once we have comparison, we get uniqueness of viscosity solution. So in particular, we get that our value function is unique. The value function is uniquely characterized as the viscosity solution of the H shape in the question. So we can incorporate state-dependent constraints, that's not too difficult. We can generalize onto RD, so basically everything goes through if you go to RD. And I think one of the big open questions we have is how general is this requirement that we General is this requirement that we satisfy this MVM STE. So, if we ask for this sort of controlled structure, how big is the class of measure-valued processes that we can actually recover? How restrictive is it? I think that's something that's still quite hopeful. One of the nice things now is we can actually explicitly solve certain control problems. So, again, under some constraints on the sort of functions we consider, you could imagine a class of states. Imagine a class of state-dependent controls where my control is, I want to restrict to the class of controls whose variance with respect to my current state mu, so that's the integral of rho squared minus the square of the integral of rho against the measure mu, right, is less than or equal to just the variance of the measure mu, which is just putting the identity in here, basically. Okay, so that's a natural class, right? So provided Natural class, right? So provided we have the identity in this class H, we choose some discount rate theta greater than zero. I can take a cost function of this form here, so just the variance of my measure mu. So now what am I trying to do? I'm trying to minimize the variance of my MVM on average, discounted, by choosing appropriate controls, where I have a sort of an upper bound on how variable my controls can be under the measurement. Under the measure mu. Okay, and what you can do in that case, you can write down the value function, so you get something like the square of the mean of the measure mu is the minimum of the cost over all admissible controls. And in that case, so we have a nice value function, we can actually even show that the optimal control in this case is just the linear function or the identity. And of course, those of you who know Eldon's work. I mean, those of you who know Eldon's work, right, he was using exactly this choice of or measure-value process to construct solutions for the scarred embedding paper in this 2016 paper. So, this is sort of an optimality criteria for that class of constructions of solutions to MVMs. I'm not sure I understand what it means, but I mean, it's the sort of one of the directions in which I think hopefully this type of framework could be useful is that. This type of framework could be useful as identifying optimality of some sort of classes of MVMs. Okay, so I think that's basically where I wanted to stop. Was an idea I told you about stochastic control classes, problems for these MVMs? I think in this framework, there's an interesting, I mean, quite a lot of different control problems that come up quite naturally, particularly the sort of connections to Ascard and. Particularly, the sort of connections to the scar and embedding problem, optimal transport formulation we've been talking about, but also coming from other directions, particularly with connections to these Bayesian-type problems. What did we do? We followed this very classical control framework perspective, developed this sort of stochastic representation for our control process, Ito formula, dynamic programming principle, viscosity solutions, and so on. Able to get the classical results about the value functions, so in particular, The value function, so in particular, our value function is the unique viscosity solution to the H2B equation. And in that context, we can even solve some specific control problems. But okay, I'll finish there. Okay, questions, comments? People on the Zoom can speak up, in the room can speak up as well. Maybe I would have a question. Hold on. Sorry, yes. I think Jan is asking a question about the... No, I cannot hear you. Sorry. So I had a question about open versus closed loop controls. Can you understand the nature of the controls that Um it's not something I've thought about to be honest. So what your question is based I mean this comes from me thinking about the when when is this setup sort of endowed with some sort of natural markup structure and where is that And where is it when is it not done actually? First thing that I thought was, you know, when is the control essentially functionally? So, yeah, it's not something I've thought about beyond, I mean, whatever standard theory is out there to help, but so I mean, the most nice problem. But so I mean in most nice problems, right, I would expect to be uh to be to be effectively Markovian, right? Um but I don't want to because I mean there's nothing I mean i i i in the form of the control problem, right, it's a very classical form, right, there's not and we're not doing anything sophisticated and because I mean because you've got the high damage I mean Because you've got the high diameter, I mean all of the difficulty really is in the hierarchy or the non-triviality of the fact that one of your variables is a probability measure. Everything else here, I think, is completely classical. I mean, in the standard control framework sense. So, in that sense, I would tend to expect things to be Markovian, but I mean there. I don't know. I don't know I mean what I don't know is I don't think well I don't think we've even thought about whether or not for example the supremum is attained in most I mean I would guess in most cases it's probably not attained. I don't think we've got any conditions that would tell you when that's attained. Any more questions or maybe that people might ask? I think was it Krista? Somebody from Zoom was asking a question. Can you please repeat the question? Yes, thank you. Thanks a lot for your nice talk. I was wondering how restrictive is the class of measurable marking list with this control structure, how this relates to maybe more general classes. For instance, if you consider just the discrete case. Just the discrete case and diffusions on the syntax, and you consider there then some volatility control. Is this captured by your structure or not? Sorry, when you say diffusions on the simplex, you mean interpreting a finite probability measure as a diffusion? Yes, exactly, yeah. So you have a martingale on the simplex, a diffusion process there. The diffusion process there, and then you could consider some volatility control. Is this captured by your setup, or is it more restrictive? So I think that would be fine, provided there's nothing to resurrect or dead. So once you end up on one face of the simplex, you can't leave that face. Yes, that's meant to stamp the properties. But I think everything else actually, in that case, was already in the paper that means. I mean, that was basically the idea we used in the earlier paper with Sigrit. We showed you could transfer everything to. But this marker with the covariance in row, is this the most general thing that you can consider? Because I think if you're in a Macovian setting and then you write down simply your co-found structure for probability measure volunteer multi-gear, then you could include the controls. Include the controls therein, and yeah, so I'm not sure if this is the most general structure that we can have. So, you mean in here? Yes, exactly, yeah. Because it's only so that the dimension of this Brownian motion is so the dimension of the Brownian motion is one in what we've done. I don't think there's any difficulty in doing it with a higher dimensional Brownian motion. The only reason for not doing that is your For not doing that, is your optimal solutions will always end up in having a one-dimensional sub-solution anyway. There's some sort of convexity that goes on that means, I mean, you know, I may have something that now is a d-dimensional noise, but I will always have some way of restricting to just one of those Brownian directions. Okay. I think that's a good. So there is a gap in the sense I don't think we've written it down in the case of. Think we've written it down in the case of a multi-dimensional Brownian motion, but I don't think there's any reason why I wouldn't go through. But I think you would just all that would happen is when you formulate it like this, you would find that you always just shrink down to a one-dimensional subspace, or you can always shrink down to a one-dimensional subspace. Yeah, yeah. So I was just wondering about could simply in a Macovian setup consider measure-weighted mass. Measure where they think they just write down the generator for this and incorporate controls in there. And I'm not sure if this leads to something more general, but I think that's just maybe. So yeah, I think that's a good question. I don't know the answer to that. Yeah, I think that's a good question. So maybe it could be even generalized a bit. But thanks not to inspiring talk. No, thanks for the question, Christa. No, no, thanks for the question, Christa. No, and I mean I completely agree. I think so. This is the framework we're working in. And it's because things like, I mean, I think I mentioned, you know, we need to check things like if we choose a control, we stay in the set of probability measures. And it's things like that, I think, that would. Well, I suppose if you're writing down the generator, then maybe that would obviously be true. If you're writing down the generator, then you get implicitly restrictions on the controls that you have to inform. Restrictions on the controls that you have to enforce, of course. Yeah, okay. Yeah, I think it's a good question. I don't think I have a good answer. Well, no, no, that's yeah. I mean, certainly I think that's one of the natural directions. I mean, and even, I mean, yeah, obviously you're talking about the generator form, but I think even the stuff that, I mean, like this example I was drawing over here of where you descend to a Dirac, right? I mean, I think even if you have sort of diffusion criteria on your generator, I'm not even On your generator. I'm not even sure it would cover those examples. So I think there's even beyond what, I mean, I agree that what you're suggesting is a natural extension, but I think there's even other extensions beyond that that would be interesting. Yeah, yeah, yeah. Thank you. All right, thank you. Are there any other questions? Very quick one. Otherwise, we move on to the next job. But if not, let's thank Alex again for his nice. If not, let's thank Alex again for his nice talk.