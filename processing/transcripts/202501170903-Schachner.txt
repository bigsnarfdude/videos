A few ongoing collaborations. So my talk will be based mainly on these three papers here. So in the first part, I will tell you a bit about how can we optimize our choice of compactification geometry. I think you heard a lot about that in string theory, starting from 10 dimensions, we have to make a choice of compact geometry on which we reduce our theory. Typically, we don't have a good handle on which Carbier. Handle on which cardias to pick. And so one thing that we did together with Elijah, who is also here, and Nate, another student of Liam, was to use genetic algorithms based on certain maximum priors, can we select CalabiR manifolds from the Coenzyk scale database? And the second part I will tell you a bit about recent applications about the auto-differentiation methods for modulus. Auto-differentiation methods for modular stabilizations, and I will mainly focus on a paper that came out last week where we studied deep observations in the market magnetic cable, and we'll get to that in a couple of slides. So, let me start with a brief introduction. I think the string axiverse doesn't need any introduction anymore, thanks to many of the other speakers. I think you would all agree that a string axiverse. A string axiverse has a rich phenomenology. The general technology naturally contains couplings to all different sectors, in particular to moduli, stellar model degrees of freedom and even other hidden sectors. So this is a really rich class of models that has been studied by many people in this audience. But there's still much more to be learned. And as we discussed, for instance, yes. Discussed, for instance, yesterday quite a lot. The best handle that we currently have on the string axioms is the C4 axioms and type to B. So we compactify type to B super spring theory on a Calabiat manifold, and then we focus on those axions arising as the zero modes of the Rermann-Berman four form upon dimension reduction on this space. And then in the 4D EFT, the typical 4D EFT, the typical potential that we find is of this form. It will have a piece that only depends on the moduli fields that I'm denoting here with tau. The axioms are the phi's. And then we have a bunch of cosine contributions that depend on instanton charges. And here the instanton scales are exponentially suppressed in these modulite elements. And so if you know, starting from this. So, if you now, starting from this scalar potential and all the other couplings in the Lagrangian, compute the masses and the decay constants as functions of the moduli, schematically, what you find is something like this. And then, because, for example, the masses depend exponentially on the modular values tau, this naturally leads to ultra-like bucks and like particles. And this is something that we, for instance, studied recently in our paper on fuzzy.net. Not any questions so far? Any questions so far? Anything that I missed? Good. All right. Now let's get even more specific. I told you we want to make a choice of Calabiao manifold. There is, luckily, a really big database of Calabiao hypersurfaces at our dispense, which arises from triangulations of four-dimensional reflexive polytopes. Four-dimensional reflexive polytopes. These have been classified by two mathematicians called Selen Skarker around 2000. And then, upon triangulating each of these polytopes, you can construct Calabio hypersurfaces very efficiently by using very basic Tauric methods. And in particular, this is very efficient thanks to software developments by Liam in collaboration with some of his former students. Now, the entire data set is actually. Entire data set is actually accessible, and within milliseconds, we can construct FRSTs or colour BLs for any of these polytypes. So, in that sense, it gives us the freedom to study any colour BR that we can find in this database, at least in principle. Now, using these colour BRs arising from polytope triangulations, you can now study the axion EFTs that arise upon compactification. That arise upon compactifications of type 2B on these spaces. And this is a so-called Preza Skalka database. And I think you've seen this plot before, which I think Nicole showed you already that Naomi produced for the fuzzy paper. It's a really nice plot that shows you kind of the pipeline that we typically go through. So we have to make a choice of polytope. We triangulate this polytope, we construct a Calabier manifold, then we choose. Manifold, then we choose an orientifa projection, we choose a QCD divisor, fix them, moduli values, and then we can compute essentially the Axon LaColum. Based on this schematic pipeline, some of these steps may or may not be necessary. Lehman and collaborators have studied many different aspects of this case. Axivers, for instance, black multiple radiance, also in collaboration with Toddy, for example. PQ quality problem, Axon photon cut. Peak equality problem, Axon photon couplings, and most recently Fuzzy Tagmeter, with many people in this room. And so this is the basic pipeline that we typically use for these type of analyses. Now, what I want to talk about now today is how we can make a more informed choice for at least the triangulations of the polytropes that we should be looking at. And so this brings me to. And so, this brings me to this slide. So, this is essentially a combination of two older ideas, partially inspired by the first talk that I've seen in person by Liam in Seattle at Microsoft. There was a meeting on machine learning and string theory. He talked about combinatorial cosmology. And then, also with Gary, I collaborated already on genetic algorithms. So, these two ideas inspired this project with the light. Inspired by this project with Elijah Neg. And so this brings me to Doddy's dream as he was writing it on the blackboard, I think, in the first discussion session or so. And so I think the basic question is that, say you have certain measurements for axiom properties in your EFT, what can you say about the UV? And so one step in this direction, I think, is using numerical methods or Using numerical methods, or in our case, stochastic search optimization. And so these types of algorithms have been applied extensively in the literature in string femurology, for instance, to flux worker intersective brains, or reflexive polytopes, among others, by Ellie, who's also here. And so the idea is now that, say, you're given certain constraints, measurements, whatever, on axion observables. Axion observables, can you solve this inverse problem and find the triangulations that are preferred to give you these types of observations? Again, one, let's say, caveat in this whole procedure is that to apply our algorithms, you still have to fix a polytope. So you have to fix an HM1. At the moment, this is not built in, so you still have to make this choice. Secondly, current. Secondly, currently we are more or less fixing by hand the point immodulus space. So these are two important caveats, but I think one can easily improve upon ours. Sorry. And then one more comment. One thing that was quite instrumental for this collaboration was to figure out what is the best way to encode triangulations. And this is. I think this is very general when you think about applications of optimization or any other algorithms to certain problems. You have to really carefully think about what is the best encoding for your problem. And so in our case, the first thing that we have to remember is that, of course, we are interested in FRSGs. As I said in the beginning, these are bounded by a very, very Bounded by a very very huge number, but now not every FRST gives you a different calabia. In fact, polytope FRST, so FRSTs of the same polytope, with the same induced strangulations on the two phases, gives you precisely the same calabium. And working with these so-called two-phase inequivalent FRSTs, you get rid of a bunch of redundancies, as you can see. Bunch of redundancies, as you can see. These are 500 orders of magnitude. And so now, if you can develop an algorithm that has already this feature built in, I mean that it automatically works on the space of these so-called two-phase and equivalent FRSTs instead of the space of just FRSTs, you make huge improvements because in this case you probably get stuck quite easily in local minima, whereas here you're exploring your space much more efficiently. And so, this is the basic idea for our implementation. We choose a reflexive polytope. We enumerate all the fine regular triangulations of all the two phases. This can be typically done, not for all polytopes, for those polytopes, where this cannot be done. You can still sample the two-phase triangulations. And this will actually happen in one of the applications that I'm about to show you. Then we assign random. Then we assign random labels to each of these two-phase triangulations. And then you can define the DNA or the chromosome C of a calabiao as a set of these labels, one label for each two-phase specifying which two-phase granulation you want to make. And by default, this means that different DNA will already lead to two-phase inequivalent triangulations. Because if you make different triangulations, Because if we make different choices for these labels, of course, they will lead to different FRSTs. And then a choice of DNA can then be lifted to full triangulation of the polypope using the methods developed by Nate, who's also on this paper. Yes, please. I think I don't understand the labels. You have all the two faces. You just enumerate them and 10 minutes or minutes. No, no, no, no, no. No, no, no, no, no. Sorry, this was for if you were to take, let's say, the biggest polytope, there you have two phases with like 170 points or something like that. And there you would get these numbers of two-phase fragments, like really huge and they tend to do something. But for the most part, like these two phases are pretty small. So these numbers range between, let's say, one and a thousand, maybe. So it's not that big age. This spa the space that you're living in is The space that you're living in is n to the n, where the little n is the number of two faces. Yes. And then the, and then it's how many translations of each two faces you have that n to the end of vectors. Indeed. Yes. I think the same question. Assign random labels to each one. That just means number them, and then the chromosome is a list of which one from the other. It's just a list of integers that tracks, okay, which two-phase do I pick from. That tracks, okay, which two-phase do I pick from my different list of all these triangulations? And so, if you have two different triangulations, but they share, like they have one two-phase that is the same, they would have the same entry in the table. Yes, yes, they would have, exactly. They would have the same entry. I'll give a few examples in a second. I hope they're useful. And so, the basic, so this defines the This defines the encoding that we're working with. Now we come to the implementation of our algorithm. As I said, what we're using are genetic algorithms. These are essentially inspired by natural evolution. So what do you do? You pick a population of individuals. In our case, a choice of color dia manifolds. You define a fitness function. So a fitness function could be, for instance, some guy. So a fitness function could be, for instance, some Gaussian that peaks at a desired value for a certain observable that you're interested in. Based on this fitness function, you select individuals, and of course, fittest individuals should be more likely to be selected. So the fitness acts like a probability function here. Then you do certain operations, crossover and rotation, that I will explain in a bit more detail in a second. And then after these steps, you define a new population that will have That will have, on average, a higher fitness. And then you repeat these steps over several times, and the fitness will incrementally increase, and you will get closer and closer to the desired value for a certain observable that you can perceive. So this is the basic idea. Let me give you a few examples for the types of operations that one can do here. So for instance, if you think about crossover, so we have two chromosomes. Two chromosomes or two choices of DNA. They have different labels, and here this means triangulation one of two-phase one, triangulation two of two-phase two, for example. Here, this means triangulation seven for two-phase one, triangulation eight of two-phase two, and so on and so forth. And now, what you can do is you can construct a new chromosome based on different operations. For instance, you can cut these chromosomes at one point, and then you splice them together. And then you splice them together. You can cut them at several points, and then you splice them together at all of these different cuts that you impose. And so there are many different procedures that you can implement. Depending on your problem, not all of these might be actually relevant. Same for mutation. Now, given such a new choice of DNA, you just randomly change certain entries in this list. So this is just as a nature that DNA is randomly altered, and this is quite important. Is randomly altered, and this is quite important also for the search because if you were only to do crossover, that would mean that the genetic information in your population would always stay the same. So, for instance, in this example here, it would always be just triangulation one of two-phase one and triangulation seven of two-phase one, but nothing else. But I mean, there can be ten other triangulations that you never see. But these come into your population upon using, for instance, mutation. Now there's also triangulation six. And two more remarks. So, as I said, the fitness function is typically chosen to have a maximum near the desired value. For instance, you can think of a Gaussian that peaks at the desired value for the absent for the whole. Yes, please. And this only works, the mutation only works if you can enumerate all the triangulations. Yes, I mean, if you don't know how many there are, you can still provide a random. You can still provide a random sample. Like that. You would worry about how fair. And then we get in the whole discussion. You have to dancing menace and it's fine. You would insert some bias, you would restrict your search space to a certain subset. It just means you can still run the algorithm. There's no limitation to that. However, of course, as you say, there's then a question of fair sampling. Any other questions? Any other questions? Probably explain, but maybe it will be the point, but explaining why this is better than the 9-choose 5-choose 5 options, a lot of them for going through this procedure of cross-doable mutation. Doing is like flipping the numbers around, changing the it's better than just going over the list. Yeah. I mean, in most I mean, in most examples, scans are just not feasible. So, yes, we will later start with an example where you can enumerate. All rules are better than just voiding ones. Yeah, I mean, one thing that I was about to say is the following, that, first of all, these are not all the operators that we use. They can be more or less, depending on our implementation. But each of these operations is associated with a hyperparameter, which a priori is also another parameter that you have to choose. Have to choose. And one thing that we did was use Bayesian optimization to fix these to give you the best outcome. So, as you say, it may not be necessary to use all of these for all the applications that you're interested in. Maybe it's just sufficient to use one of them. A priori, you don't know. And I will also provide, I will show you a plot where we compare it against other algorithms. Maybe that wouldn't help. Any other questions? Please? Going with what Dodio was saying, if you say I'm like. Going with what Doddy was saying, if you say on like the 491 case, we can't anyway all the triangulations. Say if you're something like triangulation number one, triangulation number 439, that means you have to create a list from one to 439. How do you decide what triangulation that 439 is? That's basically random. So you just. I mean, the ordering in your list of two-phase triangulations is random. There's no good way. No good way of ordering these triangulations. I think typically Eloni is one, and then the rest is more or less. Maybe Elijah, correct me if that's wrong, if there's a sense of like the first entries are still close by in that space of triangulations, but I'm not totally sure. Usually it's completely random. Then how do you know that your random triangulation generation is actually as random? I mean that this is what I'm saying. I mean, this is what Odi said. We don't know. We really don't know. I mean, it will be a sample, but we don't make any judgment about how fair or unbiased this is. It is definitely biased. It's more like a proof of concept that you can do it. I think this is still a big problem. And Nate has done or has made some progress in the context of random sampling of triangulations, but I actually won't get to get into it. Can you just comment on what the H one one is for the random sampling algorithm? Where the random sampling algorithm matches an enumerative check that you can still perform? So the biggest polyturp that we used here for which we could enumerate all of them was H1 equal to 60. Probably you can go higher than that, but I. You're not saying you could enumerate all the train tags. All the two-phase frameworks. Yeah, you can. Of course, this may not be true, but this is something else I should have stressed. Now, the complexity of the problem is not really measured anymore just by H11, but it's actually two-phase configurations. Of course, there can be a polytope at H1 equal to 60, where there's one two-phase with 100 points, maybe, well, probably not less than 100 points. But some two-phases are more difficult or more complex than others based on the configurations of two-phases. And at that case, 60, the example you looked at, your random sample, did you check it? No, no, no, we enumerated all of them. That was the biggest polytope that we were able to enumerate all of them. Right, my question was, did you check in the largest case where you can inform everything, whether the sampling algorithms are actually... Because you have ground truth for whether the sampling is working in that case. Because you have the counting measure. If I'm understanding correctly, at 60, you were able to energy all the tracking measures. Able to iterate all the calculations, you know the counting matter. And then you have an algorithm related to Ellie's question, which you would, for example, be using at 491 to sample randomly. If you just deploy that algorithm at 60, does it sample flat? Have we checked that? I don't quite remember. I actually think that I don't fully understand the question. I don't know how I would check if I have some subset of the two-paced triangulations for some toothpaste. How do I know that that's a random sample? Well, if you have the complete, maybe I'm not, maybe I'm missing them. If you have a complete list of Maybe I'm missing something. If you have a complete list of a discrete set, so you've enumerated everything, and then you have an algorithm that's supposed to populate that set, then you can just compare what you're getting to the county measure and see if you're only sampling a, like, you know, pick different regions of the discrete space and see if they're all sampled with the same measures. I think that that's the, I don't think that I know how to partition up, like, a set of two-base triangulars into some regions, but I know I'm getting a little from here, a little, you're a little, you're, like, I don't know, but I don't know. But you could use it as a convergence check. You could generate a chain from sampling from the counting measure, but generate a chain using the random algorithm and compute the Galaruba convergence. But we don't have a measure to compute that convergence. If you just have two chains, you can always do that. Just have the countermeasure or like you play? You have countermeasure and your ram and your random one. And you can check the convergence. And then you can check the computers. We haven't checked, honestly, at six. But this is something else that we should think about. Okay, let me continue maybe. Okay. Yeah, this is a very This is a very brief simulation. I'll probably run through this very quickly. So, this is just a rough sketch of the space of all of these NTFE FRSTs. And now you have different choices of triangulations in the space. And now, let's say you know where your optimum is in the space, so here indicated in red. Now you go through these different steps of the GA evolution. What happens here on the left-hand side, you see that this population clusters closer and closer around. Closer and closer around this optimum configuration. And so, a few things that I want to highlight here, apart from, of course, the fitness is maximized. First, in principle, you have access to the entire space. It's a global search algorithm. Secondly, it's non-local. It can jump from one place over here to another place over here. This is just because if you alter one of the two-phase triangulations, you can jump around all of these different chambers of this. Chambers of this external chemical. And then there is this clustering observation that all of these will cluster around this optimal configuration, but typically not all of them will be close to your optimal configuration. Usually some outliers still have. It's very good question. Yes. I'm sure I've just beefed up, but I don't see how it's global if you don't have access to the entire list of two pieces. Good point. Yes, when I said global here, I literally Said global here, I literally mean for a given two-phase triangulation, it can really explore all the NTFEs generated by that. So, assuming that you have the entire base literally means just those defined by the two-phase triangulation that you have at your expense. Okay. And now let's benchmark this algorithm. So, the first example that we looked at was a really simple polytope at H1 equal to 23. Equal to 23, for which we were able to enumerate all the NTFE aphorisms of the polytope, so there are in total 330,000 of them perfect. And the first task that we looked at was very simple, was just maximize the colour BR volume at the tip of the stretched killer. And so, this is the evolution of the G8 population. And also, here in comparison, you have the full distribution for the polytope. So, you see that. So you see that initially at generation 0, the population will have the same distribution as your polytope because it's just a random sample. And now as the GA evolves from generation 0 to generation 20, you see that this distribution shifts more and more to the right. The volume is maximized until we end up with a distribution that is somewhat close to the maximum value. Now to quantitatively Quantitatively say, okay, let's compare this with other algorithms. How good is this J actually in finding the best choice of Calabiaro in this list? So here, I hope you can read all of that, and if not, please let me know. So here, the dashed line is the maximum value. So the volume is 10 to the 6.75. We compared our GA, which is here shown in red, again simulated annealing. This is here in brown. Kneeling, this is here in brown, MCMC, which is here in purple. Those do we have? Best Furge Search, which is here orange, which is very close to the red line. And then random sampling, which is here in blue. This was everything. And you see that the GA easily beats most of these other algorithms, although best search really comes closely. Search only comes close. Are there any appreciable differences in time? Really good question. So, yeah, here on the x-axis, we have the number of samples that we had to go through until we found the best configuration. Another measure would be time. I think best search when it came to time was actually faster. Correct me if I'm wrong, Elijah, because you have done these ones more thoroughly. You have done these ones more thoroughly than me? So, I think that all of the computational operations here are quick enough that basically the x-axis here is a measure of time. Because it's the number of unique times that you're evaluating your cost function or your business function, and that's the expensive step. So I think you can basically say this is time. Yeah, that's a good point. But that could be different if the cost function of the GAs were a Of the GA is more expensive than evaluating like a Gaussian volume. Well, the costly function here is actually, say, for instance, computing the volume. Then evaluating whatever fitness measure you have is very straightforward. Okay, the fitness measure is easy. The fitness is easy. I mean, here, I mean, even the volume is pretty easy to evaluate. But later, for example, when we compute axon photon couplings, of course, that's the most expensive step. Everything else is kind of straightforward. That extended That extended cala cone that you are pulling up there, that's the one of the polytope, not of a single calabi al, right? Yes, it is of a polytope, yes. If you jump around there, you jump among different flanks. Yes, yes. Any other questions? Okay. Sorry. So now let's look at a few more interesting applications. So, as we discussed several times, what we would like to do in these kind of computations, we would like to constrain our moduli space based on certain priors for axions. And let's say we look at the axion decay constant for the lightest axion in our configuration. One thing that you may have asked, and this is here more. And this is here more or less a random number. Let's say you want to find a light axion that has a decay constant of 10 to the 40 GeV at the tip of the stretch scalar cone. I should specifically. So here we picked the polytope at H1 equal to 60. And this polytope had 23 two-phases. Three of them had just one two-phase triangulation, it's just a triangle, it's boring. And then the other ones had either four The other ones had either 4, 64, 168, or 734 two-phase mangulations. You see how many there are. Six with four, five with 64, three with 163, and six with 734 two-phase triangulations. And so you can then bound the number of NTFEF RSTs that you get from this polytope by something like 3 times 3 times 10 to the power 36. So this is the bound because of extending to a triangulation of the format. Extending to a triangular format. Yes, yeah, indeed. Yeah, not all of these configurations will extend to full atmosphere. So, this is a pretty vast search space. It's basically impossible to enumerate all of them. So, here we don't know what the best solution is. But we can still run this GA. And what you find is, and I think the colors are not super nice here, but I think the initial distribution is somewhere around here. I have to search for it myself. Yeah, it's somewhere around here. But what is important is that again. But what is important is that again, over the time scale of the GA over many generations, you noticeably see a change in the distribution, and there's a really sharp peak around the desired value for the axon decay cost. So this is just another proof of concept that you can look for these types of configurations. Of course, this this is again just another toy model setup, but I think we had a stage where now we can use this for more realistic Axel model building. Please. Axel model, please. Can I answer another question about the comparison between the different the different algorithms? Yes, I know. So I would sort of like I would I would think that the splicing step of the GA, I would just naively think that that's sort of so dominant to the random mutations. But then I thought, well, actually, you know, 100 things in your population just doing random mutations is what MTMC would do. Indeed. And the genetic. We do. Indeed. And the genetic algorithm is outperforming MTMC, so that means the squares is important in the... Yes, it is. Sexual reproduction is important in populations. That's absolutely right. Yeah, yeah. I mean, if you were to turn off crossover, it would not be any better than just random sound. Yeah. It's interesting that that makes such an obvious. Was there another comment? Yeah, and I'll actually get to a plot which makes also which is quite interesting, I believe, personally. So, something that you quite generically see in these types of GA evolutions is that there are certain parts of your DNA, of the Calabia chromosome, that are more important for certain features that you're looking for than others. So, what do I mean with this? So, here are eight panels, each for a different two phase. Each for a different two-phase. And the y-axis is the label for the two-phase triangulations. The x-axis are the generations of the DA. So x-axis tells you evolution, y-axis tells you which triangulation you pick from the list. And the colors are the occupation numbers. So how many colour DAOs had this particular two-phase triangulation? And so you make different observations. So for instance, there are two-phases for which there Instance, there are two phases for which there is a single two-phase triangulation that is picked out, that dominates the entire population. So that means, for some reason, this triangulation seems to be more relevant for getting you a certain value, let's say, for the Axon decay constants than other two-phase triangulations. In other two-phases, it's quite different. Here, there's still some freedom. There are many different triangulations that still appear in the final population, as shown here by the colours. So this triangulation, this triangle. So this triangulation, this triangulation, this triangulation, this triangulation, which is still populated. And these are the types of generic features that you observe here. Similarly, there's a second. This particular plot is for one population, and I think if I know where you're heading towards, this plot will of course change if you run it here again. It will look totally different, although, like, if you were to run again, Although, like, if you were to run it many times, you could check: okay, is this outcome universal? Is this always happening? Like, are there certain G's that correspond to that? I mean, one thing, of course, that I have not said here, of course, I mean, you run the GA, you find this distribution, but of course, if you run in the gate, the distribution could still peak around the same volume, but it could peak somewhere completely different in your search space. Like, maybe there are different clusters for local minima for if sorry. For local maximum for your fitness function. And is that true? Like, if you run the algorithm many times with different starting populations, do you get like, you know, if you're optimizing for axion photon populations, do you get a random looking thing? Or how does the final population work? What do you mean with random-looking, in that sense? Like, okay, if you go back to your 491 picture with the Picture with the secondary? Wait, where did I show the 491 picture? Because the cross-section. Ah, sorry. Oh, yeah, yeah, no. Yeah, yeah, so if you... You mean in this plot here? Okay, so if you take different starting populations and then plot the one on the right, the final populations, do you typically get like a uniform-looking final distribution? Or is it always clustered around one or the least? It will be clustered around one. It will be clustered around one place in one more field. And I mean, this is also not so dramatic because something that maybe I should have highlighted in the beginning, these type of algorithms are not meant to give you an honest picture of the distributions in the landscape. It's just meant as you have certain axon price, you want to find a good example in the space. Run this algorithm within five minutes, you get an answer. If you want to have fair samples or fair distributions, Samples of fair distributions, you should use something else. For instance, reinforcement learning. I think Ellie, for instance, has also worked on this. That allows you to generate maybe in a fair or in a uniform way these types of solutions. So that was... Oh, sorry, Glaja, I think, what did you say? Yeah, so the way I understood the question you were asking, not only will it cluster every time, but from run to run, will it cluster around the same place? That need not be true. It will definitely be sufficient. Not be true. It will definitely potentially cluster around different. That's what I was asking. You would hope that if you're going for a global maximum, that it would cluster around the global maximum, but for this problem that Andreas just presented, where you want some decay constant that's in the tail, but not at the very end of the tail, there may be totally different corners. Yeah, that's exactly what I'm asking. Sorry, I thought this is what I said, but maybe I didn't. No, what I meant was: yes, you have these different local maxima, and they can't cluster around any of these. Because in this case, we really don't know. I should say we have a global. Know what it's actually said, but at the total maximum. So there doesn't seem to be any structure to that. Yes. Good. Other remark that I wanted to make is that there is a certain learning progress that can be observed here. So even though TAs are not necessarily machine learning, they are still more efficient in generating new FRSTs. So what is shown here, so again, x-axis tells you evolution, y-axis. Tells you evolution, y-axis is now the number of failed FRST extension. As I said, the given choice of DNA, as defined on my previous slide, does not necessarily or automatically lead to a full FRSTs. Not all of these choices extend to a full FRSTs. At generation zero, essentially what you're doing is random sampling. You see, this is really bad. I need to construct around 5,000 DNAs to find 100 full FRSTs of the post. Full FRSTs of the polytope. But as the GA evolves, this number goes down until eventually all of them extend to full FRSTs. So, in a way, you can also say these GAs are not just good in finding new certain calabiels, but they're also very efficient in generating two-phase inequivalent F-Rusters. Sorry. And this is just highlighting that again. And so the last example that we looked at, and this is again Example that we looked at, and this is again just was meant as a proof of concept that we can look at the entire Kreutzer's calculus at the expense of having to randomly sample two-phase triangulations. So here we looked at H1 equal to 491 and we computed the oxygen photon couplings as was done in KLIMES by Naomi, Dodia, and Liam. And so here the task was very simple. Let's just try to maximize. Simple, let's just try to maximize this accent for the problem. And here we're making a comparison between random sampling and the genetic algorithm. And you see that the GA easily outbeats random sampling by almost an order of magnitude. Why does it track it? So this is because I think the population was of that 1,000. So until this point, the GA is basically just random sampling, and then there's a full population, then you start to implement. There's a full population, then you start the evolution. We talk a lot about whether this is the right thing to plot, but it's hard to come up with a better way of plotting the progress. If you were, to be honest, you should have started, but you should have removed the blue line here and started maybe the blue line at this point. Because this is actually when the GA starts running. Okay, good. So this is all I had to say. Good. So this is all I had to say about G A. How much time we have left? Five, ten minutes? Fifteen? Ah, okay. Okay, yeah. I mean, then let's see how far we get. As I said in the beginning, one caveat in what I just presented is that you had to fix a point essentially by hand in Kelmoduli space. But interesting physics might happen at completely different points in modularized space. So in the So, in the long run, what you have to do is you have to incorporate that also in your searches. And one thing that we did, and this is of course not the main point of our paper, but I want to highlight here because it's one of the computational aspects of our fuzzy paper, is the following. If you look at the relic density for axions in a mass range between 10 to the minus 28 and 10 to the minus 15 EV, and these only depend on the masses and the decay constant of the axions. If you implement If you implement a fully differentiable code that computes these from string theory, you can use gradient-based methods to find points in moduli space for which, for instance, the relic density is maximized. And this is one thing that we did in our paper. So here is a schematic plot. Here you have the fuzzy decay constant, the fuzzy mass. In this direction, basically, the gradient descent went. Of course, it went to higher masses. And this is something that. And this is something that was really useful to find the lightest fuzzy abundance in our course. Please. T here is not time, t is kilometer, is t. Good point. Yes, it's a kilometer because maybe I should have included an index. Good. So maybe this is something that one can incorporate with what I showed you. Now in the Now, in the final part, I briefly wanted to highlight a few ongoing things regarding modularized stabilization. I think it came up in one of the discussion sessions that one thing that in the future we should do is trying to implement typical, and typical remains to be defined, modulate stabilization procedures in some of these Axion collaborations. And so, what we've done in the past with Sven. With Sven and a former master's student, is to develop a framework for stringy EFTs where essentially you just need the topological input, which you can compute from C-WAY tools. And then, luckily in supergravity, of course, you only need to define two potentials, the killer potential and the superpotential. Everything can be computed from just these two objects. You can make your life easy if you develop a code that does that. Develop a code that does that for any input function k and w. So you don't implement by hand all of these functions separately because whenever you change these functions, you have to change these functions. If you have an implementation that is completely modular, that computes all of these EFT properties automatically, no matter what the input is, then you just make your life easy. You're less prone to error. Whenever you have to implement the function yourself, you make a hundred mistakes, you forget minus signs and pi's, and so it's bad. And pies, and so it's a bad, it's a big mess. And in addition to that, this package that we're using, JAX, this is a software package developed by Google for mainly machine learning purposes, but it has also been applied in a lot of different science contexts. It has a lot of features that are just really useful if you're interested in numerical C-boat issues. And so, what we devised was this framework where you construct your models using SIBOR tools. Construct your models using SIBA tools, you compute the EFT, you sample points, you moduli space and other UV parameters, and then you try to find a string vacuum in your 4D EFT using some numerical optimizer. And then you check whether your consistents are consistent. In the interest of time, let me be quick here. So, the main application that we looked at so far is the flux landscape. So, not immediately to axions, but I think. So, not immediately to axions, but I think in the future, as I said, this is an easy extension. So, here we have a superpotential that depends on two flux vectors, f and h. These are just some integer flux vectors. And SUSY minima are then obtained by solving the so-called f-term conditions. These can be rewritten in terms of this linear relationship between f and h in terms of some matrix, some complicated matrix N, called the ISD matrix. Called the IS3 database. And so in the past, we studied many different things. For instance, we tried to improve the numerical optimizer that we're using, comparing sampling biases and so on and so forth. We studied distribution of W0. We looked at non-supersymmetric work here. All of this data is publicly available. And again, in interest of time, let me not say too much about this here, because what I want to focus on today is a paper that came out last week. Paper that came out last week. And so the motivation here, as the title suggests, deep observations of the flux landscape, is the following. Experiments have gotten better and better. Their resolution gets better and better. I mean, if you compare here, Plung to Coby, you see, I mean, we get a much better picture of what there is in the sky. Same with direct observations of supermassive black holes. Now, we can take images of these far distant away objects just by staring at them long and hard. And hard. The question is: Can we apply similar methods also to a flux landscape? And what do I mean with that? So, first of all, simple nearby objects, for instance, would be those that are analytically or numerically straightforward. Things like the rigid, toroidal, things that people have studied extensively in the past. These are typically quite boring, but we understand them well. Pause observation, where we just more or less randomly look at the sky, can be done with appropriate codes. With appropriate codes, so this is what we tried in our previous paper. But what about deep observation, where we stare long and hard at one particular model in one particular part of moduli space? So to rephrase that, how can we systematically generate string value in a given region of moduli space? Or maybe in the context of Axon EFTs? Like which part of your moduli space should you focus on for given priors that you have? That you have. And so, what we did was we devised an algorithm that systematically generates vacuo in a given region of modular space by just using more or less this ISD condition plus bounds on these choices of fluxes. So these bounds to some extent were already derived by Eric and Lawrence. We improved many of them, we found even stronger bounds. And these bounds simply depend on. simply depend on eigenvalues of this ISV matrix M. So for a given region U, you can compute these eigenvalues and you can check by hand which choices of fluxes you should be looking at. And thereby you can generate actually all the flux choices that you need to find solutions in this region. Or if you cannot enumerate all of them, you can still randomly sample. But from the right list and you don't randomly sample just from a really big letters and it becomes Big letters and it becomes really expensive. So these are all the ISD solutions in the region, and in principle, there might be other solutions in the region. No, no. So, I mean, let me show you what I mean here. So, if you make this region small enough, you need to take your tadpole small enough, you're in the game, you can enumerate all of them. If you scale one or the other up, then it becomes tricky. As soon as you enter numbers of like 10 to the 10, 10 to the 20, 10 to the 10, 10 to the 20, becomes hard to enumerate all of them. So, in that sense, our algorithm can do both. Like, if this region is small enough, we were able to make an exhaustive enumeration. If it's not big enough, it's still much more efficient in finding new solutions in that region. Exhaustion means you find all places where the gradient of the of the flux potential is zero, regardless of any ISD. No, no, no, no, implementing ISD. Only ISD solution, there can still be non-ISD solution. Non-ICT solution. There was like that. Oh, sorry, sorry. Sorry, it's not just misunderstood. There's some discrete choices that you're allowed to make, the fictional potential, how much, some introduce it, how much multiplayer is somewhere. And then you're finding, given these choices, where the vacuum of the complex structure modular individual is. Yes, correct. Yes. So this is the basic problem. Good. So in here we generated roughly 170-ish million solutions. And you see these distributions that you get have a quite interesting behavior. And this becomes even more apparent if you color coat them with the right variables. And I should give credit to Nicole here because she said this looks a bit like a seating chart when you cook. Like a seating chart when you book tickets for a concert or whatever. I think she's right. The question is: just where's the stage? I think here it would be in the middle, here it would be kind of awkward. Anyway, so you can color code these distributions now, for instance, by different flux quanta. And you see that there are different symmetries. For instance, here there's C2 symmetry that takes you from the upper to the lower half. Even more interestingly, some of these solutions are singled out. Like, for instance, why do these clusters? Like, for instance, why do these clusters have just this particular value for this flux, or along this line here, there's just zero? And so, from starting from these kinds of intuitions, you can develop an analytic understanding of these fingers. You can actually write down the solutions, you get a better handle on what these types of solutions are. Or here, for example, the radial direction scales with one of the flexes. So, these are the different things that you can now study. Similarly, you can look at distributions here for W0 for all of these different data sets. You find this kind of universal linear follow. I don't know if this is a block plot. And you see that there's this universal behavior. And if you just sample long and hard enough, and this is just standard Dennison Douglas, of course what you find is that there can be fine-tuned cancellation that gets you to small W0. And the smallest W0. And the smallest W0 that we found in our case was 5 times 10 to the minus 5, roughly, given by this solution here. And these are radically different from the standard perturbatively flat solutions that we looked at extensively with Liam and collaborators. So one thing that should be stressed here is that the small w0 arises from a fine-tuned constellation. It exists classically, meaning w is just a polynomial in your fields. In your fields. There's no exponentially suppressed term here that leads to the smallness of the V0. There are, of course, exponentially suppressed terms called to instant concorrections, but these are negligible. These don't contribute here. And more importantly, there's no flat or light direction. In these solutions, you start with a perturbatively flat direction, and that's lifted by a race-strick potential. This is not happening here. All of these fields have masses of the same order. What's the smallest W that you expect there? What's the smallest W that you expect there by Dennis Douglas? That's a good question. Um ten to the m minus seven? No, nine to the ten minus six, I think. So we're close. Yeah. And yeah, I mean in the paper we make much more comparison with Dennifer and Douglas, like for instance the density profile that you get for the for the vacuum density that you find. In our case you see local fluctuations where Dennifer and Douglas is kind of monotonic. Is kind of monotonic. And here, of course, the underlying mechanism is not yet fully understood, but based on these results, we can hopefully find more of these submissions in the future. And so let me conclude. I showed you three different interesting directions for computational aspects in the string axiverse. So on the one hand, I demonstrated that we can have automated geometry finders for stringy oxygen. Geometry finders for stringy oxon EFTs, that we have already developed computational toolkits for moduli stabilization, and now we also have kind of a string theory telescope for moduli spaces. So now can we put all of this together and really look at the interesting part of the string axiverse? And of course, I cannot end my talk without showing this plot once. I mean, everyone seems to be kind of a tradition. And so I think one thing that we have already discussed is extensively. We have already discussed extensively, although I think it's not super clear yet in which places we should be looking in this plot and in this plot. I think that there's a lot that we have learned connecting these two sites when we look at different regions of axiom parameter space, how that translate into on the string theory side into which regions of moduli space or and which choice of of colours we should be making. But I think we are in a in a good place. I should say this is more or less random, so there's no real connection. But still, we can still do. And so with that, let me thank you very much. And in case you're still wondering, like me, where Lake Moulise is, I think we decided it's okay. And with that, thank you very much. I have a general one on the other side.