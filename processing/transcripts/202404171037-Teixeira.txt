And I was saying that it can be intimidating to be in front of this crowd. And imagine for me, new to the party. But one thing gets me a lot more relaxed. Rob Morris at Impact He always says should get to know more the Minotaurics crowd. They're just a test. And this week I've been confirming this since I write. This talk is going to be based on a joint work with Island. He was very kind to make some choices. He was very kind and made some choices during his course to make my life easier, so I had no excuse to give a lot of support. We'll start with the introduction of the models for the fifth time this week. Then we're going to talk about results on thresholds, locality and the bootstrap localistic paradox. We'll try to give some ideas of some proofs and finish with some MySOPEN problems. So let's get started. So let's get started. So we'll focus only on Z2. We fix a parameter P for the initial infection. Every site is going to be declaring that it's dependent on probability P as usual. And we start with the two neighborhood stripe population, where the initial set is the initially infected sites. And at every time step, a site looks at its neighbors. And if two of them are infected, it becomes infected. And we'll be mainly stating our results in terms. Stating our results in terms of the first time that the origin gets infected, which is a random variable with a law that depends on the initial parameter p. I will also be interested in the model that Rivalu explained and talked more about, the Frobosa bootstrap procolation. So, the change in the rule is simple. We are going to declare a site infected if it's the only. If it's the only non-infected site in a one-by-one box. So that's very important. It's not a 0x0, not a 3x3, it's a 1x1 box. So these are the rules if you're thinking in terms of you could track propulsion. And here is a little simulation I did by hand. So yesterday, I think Yesterday, I think uh Una was looking for me in the open sessions. Uh of course I was doing this. Hope you enjoyed. So here you see that this is the mean square of this four and it gets infected with three others as well. And you see they get to meet here, so these also get infected and in the end you're gonna infect this whole square. You're going back to this same simulation. This same simulation later once we do a slight modification to the model. So, this, especially the original two neighborhoods, triples, conjectured to have a non-trivial phase transition. If P was large enough, you'd have finite, almost triggered tau. But if P was small, maybe you'd have an infinite tau with positive probability. And this was supposed to be not the case by Fernet and later. And later generalize by showing the other dimensions. So, in fact, for every P you have a tau dex finite homoshi. So, you see here an inverse robot. This is going to appear whenever simulations fail to reflect what is proved. I can't tell whether the robot is happy or sad, but. Yeah, it's the beautiful robot. Is it gloating? So, this was quantified later in this similar work by Asman and Leibovitz, where they proved that tau behaves like an exponential of 1 over p with constants that could vary. This was extended in several directions by several people. This is definitely an incomprehensible, not comprehensive list. And this was followed by the amazing work by Horoid, where he got the right constant. So this is pi square over 18 that was talked about before. And for proboser, the constant changes slightly, it's pi squared over 6. And again, we have an inverse robot because as soon as Isimon level beats that, this is an exponential. We said this is an exponential behavior. Physicists, of course, didn't believe that it could oscillate between two very different exponentials. There should be a constant. So they tried to measure the constant and they got wrong. But it's important to emphasize how wrong, you know, when you do numerics, usually you measure your success by the number of digits you got r right. I estimated this um x critical exponent with three digits of precision. They got exactly zero digits correct. Exactly, zero digits, correct? They claim like three, or something. This was made more precise in a series of work by Gravan Horroid, Gravan-Horroid-Morris, and then Harthazz-Morris, where they showed the second term correction of order one over square root p. And first, this was the upper bound. First, this was the upper bound was true, and then the lower bound was by law of correction. So this is this was the latest result. And you see, it's uh it's important to remark that the minus sign is in both sides. It's important if you are minus and plus, there could be no deviation at all from the first term. But since it's minus, know that there is an advantage against the first term, so it's easier to infect than what you predict. To effect and what you predict from me the first term. And in particular, you know that the exponent of the correction has to be one-half, right? If it was plus here, it could be one-third. And this again gets a robot because as soon as this was the book in the last constant, they said, okay, so if this is the right constant, I can estimate this power. And estimated it wrongly. The lower bound is the most technical part, and it involves, we saw the upper bound in Frobosa, in Mivalo's lecture, but the lower bound is more technical and it involves corrosive hierarchies with several technical improvements. And here is where And here is where the results that I'm going to present enter. We get a second term with the right constant, but both upper and lower bounds, and you get some estimate of what's left. You see that, unlike in the previous result, we have minus and plus, which means there's no reason to believe that one-third is correct next term. We actually believe it's not. We believe it's one-fourth based on the research. Based on heuristic estimate. And this second term is not as nice in the sense that we don't have an explicit formula for it, like square or something. It's an integral of a negative function that we can numerically tell it's 7 plus 6. For Frobosa, though, the same result is true with different constants. The pi square root of 6 was known. And here it's a pi square root 2 plus root 2, which is a number I did. Which is a number I didn't expect to see again in my life. Okay, so you're saying you think it's a fourth group rather than a third group, but is there a reason to suspect which what the sign is as well? Correction. So we we are we are So we are basically sure we can put 3 minus epsilon here. Sorry, 3 plus epsilon. So it's a rule out. It rules out 3, but it could go all the way to 5 or more. But we believe it's 4. So the top constant, the 7. I'm convinced that that's not. That that doesn't have a closed fault expression. I think that's interesting. Yeah, yeah, you you look at an integral and say yeah I have no idea how to solve until someone goes there and solve. But Someone goes there and so. But you got as an inverse symbolic calculator. Oh, yeah. And it's quite a unit. It's not. If you can compute the number to 20 decimal places. Well, yeah, we put it in one of those tables and it lies between two known things, but it doesn't require. But it's not. There's a version of it. I think it still exists. Version of it, I think it still exists, where it's not just a table. It tries to do, and that's how I found the pi-squared over AT. It tries to do manipulations until it finds something. It's related to a stream that's not productive, but it's pretty taken. Okay, so these two works are built on top of the next thing I'm going to talk about, which is locality. So we're going to make a slightly So we're going to make a slight modification to the model that was introduced before, as Evilo said. So we're going to introduce it in a different way than Evalu did. We're going to consider the good stripe population with two possible states for infections. So an infected site could be just infected or it could be a germ. A germ is like an even more infected site. So germs will be represented by the set X and they follow kind of a superquisit. And they follow kind of a superquisit group of strategy. If you have one neighbor with your germ and you're infected, you also become a germ. So germs spread very fast among infected sites. And the infection will have an extra restriction now. You need at least two of your neighbors to be infected, but at least one of them should be a germ, otherwise you don't get infected. So this is the simulation that I showed you before, and this is the simulation with germs. With germs. So we start with the same configuration, but we declare this to be the germ. And you see in the first step, these two sites have the... this is for both again. These two sites have this missing, are the missing square, so they have a chance to be infected, and they will be because there's a germ of the square there. Oh, yeah. So these two get infected. Now, there's no site that gets infected here. No, this site gets infected. No, this site gets infected, and these three sites become a germ. You see, the germiness spreads very quickly, but at some point it will get stuck because you see before this guy could grow and touch it here and then things would grow. But now the top could not evolve at all because it didn't have a germ, so it didn't become a neighbor of our red block. Red block, so it gets the, in a sense, for local wood thread. Mohammed really has to go to the hill, and he won't go to Mohammed. So we can define, so this model is more constrained than the usual to make the usual corresponding or either to make for those. So the time to infect the origin is going to be bigger. Bigger for the local. But the question is: how big? Is it making it basically possible to infect the urge? And we gave it a huge disadvantage by restricting to locality, so we're going to give it an advantage now. We're going to take the infimum over all the possible places to put the initial jump. But in a sense, we are looking for where the nucleation. The nucleation has a bigger chance to happen. And now that we give it this advantage, it's still bigger or equal than tau, but we expect it not to be that bad. And this is the main result. So we're saying that the local over tau, the non-local, of course, has to be equal to equal to 1, but it's also bounded by an exponential of a polylog, which is very good because these terms are of exponential order, and even the corrections. Order and even the corrections are exponential of polynomials. So if you estimate the tau local with a certain precision, you immediately get the result for the original model, at least while you're talking about interesting things, which are this expansion. So same holds for proposal. And answering the question that I was not here to answer. So the localities in a different sense that I've been emotions. points and so far having to grow from a specific location okay so this where is the initial germ it's uh the infimum one so this tau is the infimum overall initial germs you look for the nutrition so you expect it to be somewhat close to the other but not not that Yeah, if you choose, you're definitely going to find a a region far enough that you I mean it basically we are expecting it to be the nearest nucleation settle to the object, right? You are going to find a place far enough that is going to affect the object eventually, but you work nice for it. The internal there, so you're allowed to choose the term outside A feels a bit strange. Oh no, no, no, it's uh, yeah, I'm sorry, it's in me. You can't help it. Otherwise, you choose George. Thanks. There, all the questions. Just to clarify, like the model for the Germans just one labor, you just need one labor to be. Yeah. So it's very, I mean, you could say it's just sort of virtual percolation, but only one. Sort of virtual percolation, but only one cluster is allowed to grow. Yeah, until that cluster hits it. Yeah, we don't need to have the second dynamics running on top, which is what if I looked in this talk, just took a rectangle instead of the booklet. I like the pictures. So no more questions? Okay, the last advantage you get from this locality it's um that we It's that we managed to give an improvement on the robot's life. So now, so basically, if you wanted to simulate bootstripe population before, you would take a gigantic box, do your initial configuration, and run, see where it stabilizes, whether it affects large or not. And the problem is that the box you have to take is exponentially non-alcane. So everyone knows that the functional curves are fast. So here, if we don't want to estimate tau, but we want to instead estimate tau local, we can do this in a polynomial time instead of exponential. And we know that tau local is very close to tau, so these dots here are estimates of the probability that the origin in a germ infects the whole plane alone. This probability decays as This probability decays as exponentially as p goes to zero, but we can estimate it. And you know, sorry, this is not the probability, this is log of the probability times p, so that it should converge. And you see that this square here is the biggest simulation that was existing previously, and it's even impressive that you can do this with the exponential cost of growing this. Of growing this. Every new dot here, p is divided by 2, so it's a kind of double explanation in a sense. But with this polynomial algorithm, we get all the way there. And you see, this is the theoretical line from the results we have. You see, they match. When you said the biggest simulation, you're talking about simulating the population. And this, the local, this is. The the local I mean this is this has been done with FOPS like several times but but but the the local computing things for the local by me and an undergraduate student once and then by a whole team of people who published it in PNIS so okay so exactly computing the probabilities for the local model so you can understand the result as saying okay we what has been done before is like What has been done before is actually going to converge very little. So, but with our simulation, which was not professional, but we took care of certain things, like precision is a very important aspect. We can't use just floating points and also used the GPU because it's highly parallel. I'm going to explain a little bit how this can be made all the way. I mean, this is quite a gigantic. We estimated all the We estimated all the parameters of the two terms, even the exponents. In red, here you can see them estimated all at the same time. So you see, this is very close to one, this is very close to a half, this is close to its real value, this is for both. It's 1% error, and this is already, okay, not great. But if you do it in a slightly different way, you estimate first alpha, then you say, okay, alpha is 1. So let's estimate lambda 1. Knowing that alpha is 1. Estimate lambda one, knowing that alpha is one, not using this value. Then you get an improvement, and then you estimate this, you get one half, 0.551. Instead, this is one half. You replace it by one half and you estimate this, then you get a very good estimate for long details. You could try to estimate well. Yeah. We have some, I mean we start with the exponent, right? So we start with the gamma here that would be that the Gamma here that would be that the that's pointing the P for the third term, and we have an estimate. I'm gonna show it's pointing towards the wrong direction. So, I'm gonna try to explain a bit the ideas of the proofs of the main results, starting with locality. So we fix We fix a rectangle of width A and height B, and we define two I events that R is internally field, that we talked about in lecture, and I loco is the probability that it's internally filled with a local infection. And we want to estimate the ratio between these two output quotation marks because slightly different for a reason, but it's basically what we want to estimate: the ratio between the probability of the effect. Between the probability of infecting and infecting log. And we're going to prove that this is smaller than an exponential of a polylog for all the boxes that are all the way up to this size, which is extremely super quick. Once you reach this size, you have no question that the local will succeed. So take the numerator there and subtract the denominator. Subtract the denominator. So, what happens if a box is infected but it's not locally infected? What you do is you start to follow the rectangle process backwards and you see, you know, there is an S and a T that get infected. And you keep doing this until you find an S and a T that is what you call non-local. What you call non-local decomposition. So basically, what we are saying here is S is locally infected, T is locally infected, the envelope of them, the smallest rectangle that contains them, will locally grow all the way to the end. But you cannot get this under a local kind of progress. So let's estimate. But it might not be that S and T are locally infected as well. They might be further split later on. Be further split split. Exactly. There could be more underneath, but this is the first, looking backwards, is the first time you see a normal contraction. And these three things will happen, right? There will be this normal local decomposition. Both are infected in a disjoint way using disjoint initial inflection. And in the disjoint way, they are envelope locally all the way to the end. So let's start with the picture I have here. Picture I have here, and one extra assumption which we call case one, which is Holruit's case. So, suppose that all the segments of S and T are bigger than epsilon over P. So they're critical. I mean even small critical, but critical. This means you can already use both upper and lower bounds from Polaroid's paper. So, in the numerator here, you upper bound the probability of infecting. Of insecting by the exponential of the optimal path that you saw in the lecture. And the denominator can lower bound by the same thing, up to a small error that I'm representing here as delta rhythm. So in a sense, what you're saying is you have here the coordinates for S, here the coordinates for T, and you can find the optimal path here that inflects S. The optimal path here that inflates S and the one that inflates T, but you would rather do a concatenation of these two paths and in fact S and T together, this would be a shift. Sorry, I didn't hear. You take the shift of the T guy and put on top of S, and this is much cheaper because the function f. Because the function f is monotone, so you get a clear advantage by doing this. So that's the case one, which is the easier because it builds on top of hierarchies. And by the way, we do need hierarchies in a fundamental way. We cannot do without them, even though we don't need to extend them. We just use them vanilla. And now you see the case one was when all the dimensions were critical, and you have a few more cases. And you have a few more cases. I know I'm not going to get the Christmas gift from Santa Disir because of this light. And you have to... So I'm not going to, of course, explain each of them, but I want to explain a bit the idea. So for example, here, S is critical in both horizontal and vertical, and T is critical vertically, but horizontally is time. So you cannot use Holoid's upper. So you cannot use porroid upper bounds. You have to use what you call a priori upper bound. So we have lemons that give worse upper bounds. They're not supposed to be sharp. They're sharp enough that you can compare to the growth from S to S plus T. So you can use all of them. At some point, things get very delicate. Even we have to consider whether the side lengths of them are other even gets to this point. But if here you saw the ratio being bounded by a small number, we don't need that, we just need the ratio to be bounded by a more gigantic number. But some of these cases will not be as nice. They will also include a sum over pi of smaller boxes. Like you said, they don't need to be locally infected. So sometimes you write pi of s in terms of Phi of S in terms of phi of smaller boxes inside phi of R equals phi. So this is a recursion we can treat and we get to the final result. So this is the idea of the proof of locality. Once you have locality, we don't have to look more into hierarchies, we have to make a careful look at the local group. So this is So this is where we start talking about the thresholds. So we start with the upper bound for tau, which means a lower bound for refraction and local. So we basically, when our dealing is local, we just have this growth path, right? And we want to lower bound, so we can make some requirements on the path, and it is still a lower bound. The main requirement of The main requirement we're going to do is that it's attached to the diagonal from time to time. So if this is 1 over p, we are going to attach every 1 over p to the 2 thirds, we're going to attach it to the dam. And it still gets a lot of entropy. And the entropy is the one responsible for the second term. So we're going to ask for the local thing to effect, not to invest. We're going to ask for it to do. We're going to ask for it to do it like this, and each chunk we estimate through this proposition. So, how to infect, what's the difference between infecting an A rectangle or a B B? And if we get this low bound, we are going to sum over all this. So, this is the term that was known before, and this is the second term, and we're going to get a Riemann sum of the function h that was integral. That whose integral is the second. So it's important to observe a few things here. So then let's wait for one more slide to observe this. So if I load something of this sort in his first lecture, well he didn't get the sharp H, but he got an H that actually gives a negative Gives a negative turn in the second term. So, what was Evilo's strategy? We are looking at only one of these things, one of these chunks. So, the first strategy was let's just grow one by one, like this. And this gives the first term. The second term allowed for the path to have more entropy, so it would grow as much as it would. So it would grow as much as it could in one direction until it was blocked and then it would start the other direction as much as it could until it found if the second direction was blocked then you were dead. But until the second direction found an infection right at the corner, at which moment you are going to destroy this whole barrier and restart from the beginning. So this was the double jumped process that he introduced. Process that he introduced. So here's an illustration: you find a buffer, you go up, maybe you extend a bit up, but at some point you find the corner and you clean the buffer. And you can see that as two-state kind of markup chain. From zero, you jump to one. If you find a buffer, now you're in one that has a buffer. And from one, you can loop or go back to zero. One important observation is that this Is that this composition along these paths is disjoint? What do I mean by this? If you take this exact exploration, when you find one path or another path, you know, if they are different, they have to have a first moment in which they differ. At this very moment, you did something, but at this very moment, you were looking for something in a certain region, looking for an infection in a region, and you found for And you found for one, and you didn't find for the other. So it means there is a region with an infection for you to get to one path, and without an infection for you to get to another path. So it means in the space of all the possible initial infections, these two are disjoint. So omega gamma and omega gamma prime are all two by two disjointed. So we can just sum overall paths and we're going to get. Sum overall paths, and we're going to get our log out. All we have to do now is sum over all gammas, the probability of that gamma. The problem is, you know, it's a the probability of gamma can vary a lot. You know, paths that are like this or like this, they differ a lot in probability. This one you're basically asking to have one infection all the time. Have one infection all the time for a long stretch. This is crazy. But the point is, for the probability of a certain path, it's very well, it basically only depends up to a small error on the number of jumps it takes. Number of times it does the full back and forth. So, this is what evaluated estimated. Evalue estimated. The fact that this only depends on the number of jobs is related to a lemma that he stated but didn't prove, and I'm also not going to prove that if there were two gammas going from here to here, the first term, the omega being responsible for the first term doesn't change much between these two parts. So now that we know the number So now that we know the number, the weight of a path depends roughly on the number of jumps, we can just choose one number of jumps that is optimal because we're doing load bounds and say you want it to do exactly this number of jumps and it becomes a county jump. The problem is this strategy doesn't give you the sharp result and it was observed by Omer. By Omer and Anders, that okay, if you find a buffer here on top, after you found one to your left, you're not dead yet. You could start looking to your left or down. That's exactly what we do. So we get this nice diagram. So if you find the buffer here, you start looking to your left. If you find here, you start looking down. And you could clean this buffer, or you could clean the other buffer, and it's a nice kind of mark of chain. If you get to Markov chain, if you get to four-year badges, you failed. And you want to know what's the probability of surviving. Recall, n was the number of double jumps in the previous circuit. Here we're going to have a similar number, which is k is the number of times we delete a buffer. Deleting a buffer is very costly because we have to find an infection right at the corner. So I take P, not finding an infection in the corner. Not finding an infection in the face. So, as I said, up to a small error, the weight of the trajectory only depends on the number of buffer deletions. So, if you're asking to delete too many buffers, that's going to be extremely costly. But their entropy also grows if you have more buffer deletions, because buffer deletions allow you to turn. So, if you want no buffer deletions, you're going to have only one path. So, you want to balance this. So, if we want to balance these two, we choose an optimal k and we have to count how many trajectories exist with exactly k button predictions. So, let's see, we choose the moments on the x and y axis, and then we get this dropping factor. Dropping factor, and then we, after given the moment of where this buffer deletions occurred, we still have to choose what we did in our matrix. So, this is this matrix that represents the previous picture to a large power. So, there this is how you count how many paths you have in that graph. So, this the the largest eigenvalue of this matrix is two plus root two. This matrix 2 plus root 2. And at this 2 here, that's going to give a square root. And that's roughly how you get the lower bound. So now the upper bound is extremely similar. That's the good news. The problem is that it's way more involved technically. So remember, here is the buffer thing, and then the double jump. Then you had this or the Then you have this for the lower bound to be sharp up to the second term. But I didn't tell you something that could happen here, which is suppose you found two buffers and you are now exploring to the left and you found one infection here. So we're going to delete this buffer. Now you're obliged to inspect here as well. Maybe you'll find one. So there will be a double buffer deletion. We could forget that before because we're finding lower bounds. That before because we're finding lower bounds. So, yeah, if double buffer additions happen, even though we don't need them, but now we have to tell, prove that they are not contributing in the second term. So this is what can happen. Everything can happen. Red is a double buffer deletion, like this red transition. Purple is a triple buffer deletion. You know, your matrix gets uglier, it still has this once, but it has some small turns around. But it has some small turns around, and I'm not gonna do the proof here, but I want to emphasize what gets harder. So the matrix gets harder that you saw already. What else gets harder? So remember that we pinned our walk, our path, to the diagonal from time to time. That's because we were looking for low bounds. We could afford it. Now we have to bound the sum of our paths. So we have to. The sum over all paths. So we have to consider in the cost grain step that the path could decide to do whatever it wants. So if we bound this the same perimeter, we still do jumps of size p to the minus two-thirds, but we have to consider all possible jumps here. We don't consider all of them because some stuff is just too crazy. So we have a cone here that we bounce. That we bounce another way. So, if you exit this cone, there are other reasons why you are not so good. So, this is a lemma that does the course. So, you're just going from S to T, from one point of this to another. And you know, you have these bounds that basically tell you are in the cone, and the semi perimeter is increasing by two-thirds, and you know, you have a. And, you know, you have an ultra bound. This is the first term, giving the lambda one. And here's the second term. It's fixed at the short side of the big square. It's very important. Don't ask me why. So these are the difficulties that get introduced here. We cannot pin to the diagonal. When we are before, I said, okay, any path basically has the first, the same. Basically, we have the same contribution in the first term. Here we have to improve this to also account for the second term, do a more refined analysis, but it's also possible to do. We cannot ignore the double and triple buffer deletions, those errors make the matrix more complicated, but you know, with some theory on eigenvalue perturbations, we showed that they behave. It's so a bit painful, but it's So a bit painful, but it's better than what we would expect trying to get the same thing using strengthening of hierarchies. Questions? Okay, so so yeah, so you do consider drug north for directions? Fairly drift. When your rectangle is growing. So, this is the width and height. Yeah, I understand. So, sometimes increase the width. Yeah, but so you do. And you have to consider second terms and have a second term, and it's interesting because some people asked about the next term. So, if you look at the next term, now you cannot ignore, even in the upper bound, you cannot. Upper bound, you cannot ignore the bubble regulations. So they are going to actually appear in the several things that, so, for example, for the two neighbors, you have this, as you explore, as you start to look for infections to your right, you will very quickly mix into this Markov chain of I have one buffer. Of I have one buffer, I have two buffers, or zero buffers, sorry, you have to mix very fast and can ignore this for the second term. For the third term, we expect to have to understand this mixing to get the right. There are several places that have to be sharper, that's why you expect it to be more evolved, but not hopefully, not at all hopefully. So the two-thirds? I mean, do you know two-thirds is exactly the right? For this, this is uh flexible. Other choices would also work, but two-thirds is not gonna work for next turn. So we either don't pin at all and just say, okay, it's kind of a Browning motion, it's between, you know, the fluctuations of a Browning motion here, and we're not gonna pin. There's all more questions? Okay, so to finish, I would like to tell a bit about the algorithm, how it works. So we only have to estimate the probability that you locally infect a large box, large being constant, local DLP, because after you reach this stage, the rest is basically free. You don't pay anything to go all the way to infinity. All the way to infinity. So the exploration is done through that path, that matrix. So you have several possible states. And we are going to take a path gamma, but also decorate it with the states that you were in that matrix. Because if you are going right, it could be that you have a ball and you are exploring here and you And you are exploring here, and you found an inflection. Or it could be that you already found a buffer to your right, one to your top, and you're exploring to the left, and you grill like this. So we decorate the path with the states that you were in that picture. So now we have a kind of mark of chain that the states are A, width, B, height and S state among all those seven possible states. Seven possible states. So, what's exactly F of ABS? This is something that took us a while to wrap our hands around, is the following probability. Suppose that you explore, the exploration is deterministic. Suppose that you explore the path gamma starting with the origin of the germ. What's the probability that you pass through the point AB during your exploration? And at that very moment, you are at state S. This is F A B F. This is F A B S. And the nice thing about this function is that it can be defined recursively. So if you want to know F of A B S, you only have to understand F of A B, A prime, B S, B prime, S prime in a neighborhood to update F S B S. So it means that we can find a layer 5 band where Band where we keep storing this function and we keep updating it all the way to lambda. So that's very good news because not only the other things polynomial, basically everything you do here is going to be a polynomial over thing. But this is also quadratic in time. So CPU is going to be quadratic, but memory reaches But memory, which is very important here, is gonna be in. If you use quadratic memory, it's a bit much slower. So this is good news. Another good news is that you can do different regions in parallel. So that's why you can use a GPU to move this further. And the last question about the algorithm is: how do we store this probability? Remember, these probabilities are going to be like 10 to the minus 50. So we don't want to store it as a floating point. So, you don't want to store it as a floating point, otherwise, it will degenerate very quickly. So, what we actually store is the logarithm of f. We store log. And now sometimes you have to sum. So, what is the log of r plus s, where you only have stored log of r and log of s. So it's the log of e to the log of. Of e to the log of r plus e to the log of s. You want to do this in a way that you don't lose precision, you just take the maximum of the two, put it multiplying the rotors. And now I have a subtraction here, and it's gonna be it's gonna not not going to degenerate. So it took us a while to get things not to degenerate in terms of precision. And that's the whole simulation. Smart stuff. Did you also use arbitrary precision for the log itself? Because we did. More than 20 years ago, we did exactly what we had on the slide product to the 20 years ago, so we treated it with another purple and so on. But yeah, I think we actually found that even with the log, it may. Even with the log, it made a difference. So we did it with high arbitrary precision rhythmic. So actually, you saw the log to 100 decimal places or 50. And I think that ended up making a difference, actually. By the way, let me note the things to decide. Yeah, well, the trouble is we didn't publish it, which is a great shame because we did it, this was with the summer student, an undergraduate, and we did it even for the standard model. The standard points. So instead of six states, you have like 20 of us, I think. And we work really hard. But yeah, for some reason, at the time, I sort of thought it wasn't worth publishing because it was just immobile. And then, as I say, a few years later, someone else did the same thing, but actually a small fraction of what we'd done and published it in PNAS. So there's a lesson there for everyone. But but yeah, I can tell you I can tell you that. Yeah, yes, one is uh I think it's not this at all. Like they actually get rid of a lot of transitions. But they just do it in the quadrant and they do it for the other photo. It's like it's a simplified version of it. It's like bumps taking account of tomato and account for every T, you get 120. Hundred and twenty different formulas in the matrix. It's a negative. I think we have a slightly different simulation? No, that's what we get in the end, where we got those constants from. And you see it's uh nice. It looks linear, however, it has to be linear. Has to be linear. One, oh, I forgot the one important thing I had to, oh sorry. Any case, so I wanted to show the third term that we tried to estimate and we got the exponent that we don't expect to be correct. So we'll finish with some open problems. Sorry, I think I'm over time. We started a bit late, so before But we started a bit late, so don't worry, please. Just to finish with some nice uh direc future directions to move on. So can we improve on the third term so you won't have to do it for local? Even just the upper bound would be very nice. We have to either get rid or be smarter about the course, the composition. And as I said, the two neighbors should have disparity issues that have to be taken care of. That have to be taken care of. Then, of course, you could try to do the same for other models. We did two major and proposal. But you could say, okay, modified, which Mando was kind enough to draw for us there, is also very similar, so it should behave the same, right? No, for two reasons. So, first of all, the function h that we integrate to get the second term is something that you can Is something that you can calculate and it blows up in a non-integrable way at the order. So it doesn't give a second term that is constant over root p. Actually, we could try, that's a work that Evalu did before, where he showed the upper bound with a blow-up of order square root, sorry, of order log 1 over p over square root p. So there is no constant, it's maybe. So the const there is no constant, it's maybe blowing up as a log. And this is for the upper bound. If you want to do the lower bound, you'd have to show locality like we did. But locality is not true for the modifier. So even a model that's supposed to be extremely similar to neighbor fails to be local in the sense that we mentioned. So it's a warning sign that no more research has. That you know, more research has to go into this to understand what's actually going on. So, basically, the fact that, yeah, I can try to explain why locality is not true for modified. We could try to generalize to other critical bootstrap models. Now, there is this new work by Dominico Plain Evail, so we could try to continue in that direction, or maybe just select. Or maybe just select a few that are easier to handle. And of course, high dimensions, which we don't know, actually, even don't even have a conjecture whether vocabulary puts them. The simulations that we did were not professional. Ever, we took the basic cares, but if someone gets with more serious computers and more skilled code, they could perhaps estimate the next term or the first term. Term and for the first time make a prediction that is expected to hold. And we could simulate other local models, even for example, models that are, we don't know if they are local or not. You know, like you did with your students, we actually believe that these simulations are emphasized on their own, right? Even if the model is not local, it's an interesting model. It's identified. So, thank you very much. Any questions? I mean, maybe you've already done this, I don't know, but if you just take the Frobose model where you know what the first two terms are explicitly, and then compare with the numerical work, then just assume that the first two terms are what they are. The first two terms are what they are, then you should get a pretty good idea of the third term. Yeah, so we did this and we got the exponent in the third term. What we are trying to estimate is only the exponent in the third term. We have lambda one root e, lambda two root e. These are known explicitly. Then you have we were trying to get uh the power. Yes, exactly. And Yeah, exactly. And you see it's something that at every new point, those points take a long time to come out of the computer. And at every new point, we would get a different S for A. And it was around 0.19, which would indicate it's a fifth power, but we expect the fourth. Actually, this was growing. So maybe if we had more computing power. More computing power, this would actually grow across 0.2 and reach 0.25. And the plus, and it's actually a plus rather than a minus in front of the parameters of the general package. So I'm intrigued by the modified model. So you said a calita doesn't hold, but still, do you have the same Same asymptotics for the two tals. So, yeah, that's what we don't. So, I can explain what we know. So, if you look at the function that is going to give us the integral of this function, it's going to give the second term. The second term. It grows in the probose or quantity or two neighbor, it grows up as a square root of p. So it's integrable. It's also integrable on the so on the side, and here it's a different asymptotic. So it's integrable and it gives the right constant. So in the case of modified, it pulls up as a number P. So it's no longer integrable. Oh, sorry, yeah. Oh, sorry, yeah. This is X. So this is the size of the critical. Your critical phi is this. So this blows up as 1 over x, so it's not integral here. But you can try to go all the way to a certain epsilon and just consider the rest, but you have to say what you do here. So if we do something small. So if we do something smart here, it could be that we so you see that that that's where the log blow-up comes from. And if we do something smart in the beginning, we could recover the second term. But the problem is for the local model. No, no, for the mod local model we can recover everything. I see. But for the original term. So the and they do not match. And they do not match. We don't know for the region. Ah, I see. I mean, sorry to interrupt, but I'm wondering whether at the most dumb level imaginable, which is what I'm capable of understanding. So does it sort of come down to this famous puzzle? You have the n by n square, and if you have everything occupied along the diagonal, then that works. So the modified n n squared. So, the modified model, and somehow this is really a different phenomenon which applies at the small scale. Is it kind of all sprouting from this? It's very related to the third question that this picture, just the diagonal, is the first step zero in what you have to understand. You see, below this epsilon, what you have is a strong desire to find corners. Yeah, exactly. Because the corners may be Because the corners are basically growing both directions at the same time. But if you're starting to find corners with a frequency, sometimes you're going to find double corners as well. And that means this was also likely, right? So this will contribute to your turn. So if this happens, you're normal will look. So you'd say, okay, maybe I have to fix a finite neighborhood to do things, but this is also not going to work. Work. Okay, maybe Evilo will be better to explain why, but it's expected that really in the beginning it's more global, polymerization. But somehow, but then if I understand correctly, really what the key challenge may be for this particular little bit of it is to just understand for the modified model just you know and at a scale which is smaller than one in the feet, but Smaller than one FP, like square root or something. Just what is the probability of having an internally span? Very technical. It's a very clean challenge for people. You don't actually have to understand it all. So you will have locality after that scale. And if that scale, you get some scaling limit or some other arguments to bound the probability. Any other questions? If not, that's the best way to get it. So, if you want to follow us to Lake Belize, we're meeting at 12:30 outside of Boyhaw. Let's see the modified.  Yeah, the first idea of how it's going to be.   