So, thanks so much, guys, for having me here today. I'm Joseph. I'll be talking about RAPID, which is our method for improving protein interaction prediction on unseen proteins. If at any point you want to download the slides or you want to see full citations, you can follow the QR link. Oh, let's see here. Oops, there you go. Okay. So, I'm Joseph Shamborski. I'm at McGill University at the Department of Electrical and Computer Engineering. Department of Electrical and Computer Engineering. I'm also a member of the Quebec AI Institute, NILA, and I'm a PhD student in Amin Mad's Combine Lab. So if you're an impressionable biochemistry undergraduate like I once was, you probably think of the biological processes of our bodies as connections of protein-protein interactions. It's an incomplete model. There's a lot of things that go into the things that make us us. Make us, but it gets us pretty far, and it's a pretty good simplification. You can think of these a further simplification, if you really bring it down to as simple as it could be, is thinking of all these processes as a graph of an undirected graph of proteins that connect to each other, and each protein being a node, and the presence of there being an interaction, being an edge in that graph. These graphs can get really large. And so these are just some of the interactions that we know are involved in signaling in some cancers, in most cancers, which is part of the KEGG database, which is a curated map of these protein-protein interactions. And again, if you're an impressionable undergraduate student and you have a very simplistic view of biochemistry, these sort of maps explain everything there is to know. Sort of maps explain everything there is to know. And we're going to continue pretending that we're, you know, this is a model that gets pretty far. So, how do we draw those lines between those nodes? How do we create those maps? So, the way that we typically do that is using wet lab experiments, or so-called wet lab experiments, experiments in the lab, like co-immunoprecipitation or yeast to hybrid systems, where we, by some indirect way, visually, you know, or Visually, you know, or indirectly perceive an interaction between two proteins or many proteins against one another. These experiments have characteristics all alike. These characteristics typically involve them taking days and weeks. They're quite expensive and they often produce a lot of plastic waste. But for paying all these prices, you get very, very often very definitive. Often, very definitive answers to the question: Do these two proteins interact? Computational models try to make predictions that try to answer the same problems, the questions that those lab experiments did. And they try to do so while addressing some of the trade-offs. They take seconds and minutes. They're low to no cost. On the CO2 offset side, you know, they still consume electricity and they produce e-waste, so they're not an answer there. Answer there. And they're not yet definitive. This is the main thing: they're still predictions. And so, as we develop new methods, we try to get closer to making better predictions. So, the question that the PPI prediction, the protein-protein interaction prediction question boils down to oftentimes, the pairwise problem. Given two proteins, do they interact? Because if you could solve that pairwise problem, you can create those big graphs by asking that pairwise problem many times. By asking that pairwise problem many times. Many things have been, many, many approaches have been used to predict these protein-protein interactions. Some of the earliest methods involved homology. So looking at domains that are conserved between proteins that do interact in other species. So there's a great method by Mark Out in 99, same year the matrix came out. Support vector machines are Support vector machines were as big as deep learning methods that were in the mid-2000s for a while. And they had great use for PPIs specifically because you could use kernels that were specific for amino acid space. There's also sequence similarity, which very similarly to homology focuses on the evolution and the idea that many proteins that do interact actually share a lot of sequence similarity. And then finally, like most proteins, And then finally, like most problems in 2022, there are deep learning applications, like the one that we're going to be talking about today, Rapid. So, how, you know, we have all these methods. We have, we've been doing them for a while. How do they perform? And do we need better ones? Because at some point, you solve a problem and you don't need any more methods. So, here is an analysis done by Park and Markot in 2012, so 10 years ago, on Homo sapiens data. On Homo sapiens data. And the results on, they came up with, the answer was to the question, how are we doing? Is it depends on what data you use. They came up with a couple of ways to validate these methods. I hesitate to say naive, the most obvious way to do it is the C1 method. They call it C1, where basically you have, well, we'll forget that. Basically, the idea is that you have all the Basically, the idea is that you have all these edges, these pairwise edges, and you take a random percentage of those and you call those your testing. You hold those out. And I think most people, given a set of pairs, would do things this way. And most methods in 2012 at least did super well. They got in 80s AUROC. And now there's methods that report 0.999 AUROC on such methods. But they also thought, well, there's another way to restrict this validation set. Validation set if you restrict the testing set, the testing pair, such that no proteins that appear in the training set appear in pairs in the testing set more than once. So at least one of the pair of one of the proteins in the pairs have not been seen during training. And when you do that, you see a drastic drop in performance. And they didn't stop there. They identified a third one, C3, where they said, okay, well, let's take it to the furthest where none of the proteins that show up in training. Where none of the proteins that show up in training show up in testing. And we even see a less steep, but still pretty much almost random in a lot of these cases AUROCs. So methods that do pretty well on C1 don't necessarily do well on C3. So depending on the questions that you're trying to answer with protein-protein interaction predictions, you'll have different performance using these methods. So if you're looking to answer these questions about proteins that are not in the training set, you probably won't get the answers that you want. You might as well flip a coin. Answers that you want, you might as well flip a coin. So, what is the problem here? The problem is that PPI data sets, protein-protein interaction data sets, are particularly susceptible to data leakage. There's a lot of data that's leaking from the training data sets into the validation. And the question is, how do we plug that leak? How do we do better on these data sets with unseen data? So, hopefully, towards that problem, we've come up with a method called With a method called RAPID, regularized automatic prediction of protein-protein interactions using deep learning, and we have a preprint now. But the idea is the RAPID architecture is not anything particularly unseen. It's pretty common for methods that use deep learning for PPI prediction to use a twin architecture. But the difference that RAPID brings is that we pay specific attention to the regularization that is appropriate for recurring. That is appropriate for recurrent neural networks and for the PPI data sets that we're using. So, you start out with two protein sequences, amino acid sequences, and we use this tokenizer, the sentence-piece tokenizer, that if anybody's worked with language models or transformers or anything like that, sentence-piece is pretty prevalent there. It's a likelihood-based tokenizer that uses a probabilistic method for diving. Method for divvying up long sequences into chunks where there's no obvious boundaries like spaces between words. And so here the tokens are between one and three amino acids. And these are probabilistic, I might add. So if you tokenize one sequence more than once, you may not always get the same division. It's done by that has a regularizing effect in itself because the sequences that go into the model are not always the same. The encoder. The encoder is an LSTM encoder, but it's a different kind of LSTM encoder. It's an AWD LSTM, which stands for average weight dropped LSTM. And we'll talk more about that in a minute. But what makes this a twin network is that the encoders that the tokens are passed through have shared weights whenever we pass through each sequence. That results in the output of that encoder, results in a latent representation, which we concatenate together. That concatenate. That concatenated latent representation gets fed to a two-layer fully connected classifier. So, this is a shallow classifier and it's actually quite quick. And that's why we call rapid-rapid is because if you want to test one unseen protein against the body of proteins that you already know, you could pre-compute these latent representations and only compute one latent representation and then do a pairwise comparison using that shallow classifier, making it actually quite quick to do to test one novel protein against the whole human proteome. Protein against the whole human proteome, or at least the known human proteome. And that gives you interaction probability between zero and one, one meaning 100% it interacts. So this is the tokenizer and the tokenizer. Sorry, this is another view of this. And so the idea is just that you have a bilayer LSTM and a forward pass. Just to get a zoom in on this. So what makes RAPID different? So, what makes RAPID different? In short, lots of regularizations. AWD LSTM embedding dropout, Ranger 21 optimization. We'll talk all about this in a second. And also, we use Sundance Piece tokenization. I'll talk about this in a sec. So the first thing we use is Drop Connect, which is a major component of AWD LSTM. And so Drop Connect is basically similar to Dropout, except instead of zeroing out the activation. So what Dropout does is you compute the outputs. Compute the outputs, given multiplying the inputs by some weights and biases. And then with a random probability, you assign some hidden outputs to zero. Instead, with drop connect, you do that on the weights. So certain weights with a certain random probability, they go to zero. And this is shown for LSTMs to be massively important because dropout actually really knocks out the ability for LSTM to retain a memory. And so Drop Connect is really important for that. Another thing that we do is embedding dropout. So certain tokens go to. Is embedding dropout so certain tokens go to zero randomly at each step. So that way, no one token is super important to the training step. You don't get these spurious correlations between rare tokens. They randomly go to zero. Weight decay is what deep learning people call L2 weight regularization. So, you, I think most people are familiar with that, so I won't spend too much time with that. Um, but I didn't know what weight decay was, but I knew what L2 weight regularization was. Um, Was. ASGD is average stochastic gradient descent, which is the average part of AWD. And the idea there is that you keep a running weight, running average of the weights that you're optimizing, and you optimize on that average as opposed to the weights directly. This is not what we use, actually. So we use a little bit of a variation. We use something called stochastic weight averaging, which is ostensibly the same idea. The idea is just that you optimize, you keep two sets of weights, one that you optimize. You keep two sets of weights, one that you optimize, and then one after n cycles that you update a running average to. And this has been shown to do a great job of avoiding flat minima and optimizing past them. So how do we perform? So on C1, which aerobate reports all the time, we do pretty well compared to a lot of other methods, contemporary methods. Sprint is in red. It is the In red, it is the sequence similarity method. There are two deep learning methods, and we trade blows with we perform pretty similarly to the sequence similarity method on C1. On C2, which is again the more stricter data set, now our edge becomes a little bit clearer. We have a higher URL C than the leading method. And then if you go to C3, that's when we really maintain our performance where others. Maintain our performance where others really degrade. So, this is all due to all that regularization we spoke to. So, how does rapid performance and the provenance of the data that we're training on, how does that affect one another? And it turns out not too, too much if you have good quality data. So, the data set that we trained on is called string db. And in that data set, there are it's a compilation of a bunch of sources and the And each one is given a confidence value. That confidence value is plotted here. There's a cutoff on the x-axis, and how well we do on those channels is done on the y-axis. And so, as you can see, as you slide the confidence up, we see no real big change. There's no huge change between channels except for the text mining channel, which is in blue. That one is lower than the rest of them, but it makes a lot of sense because the text mining channel is the most spurious of all of them. Mining channel is the most spurious of all of them. Sufficient for there to be an edge in the text mining channel is for two proteins to show up in a manuscript. So they could be saying things like they don't interact or things like that. So as long as it's corroborated by the other channels, text mining does well. But if it doesn't, then you get poor results. We did an ablation study to make sure which of those many parts I've discussed actually contribute to Rapids performance. And you can see here they're sent. Can see here their sentence fees, which is minus 5%. There's a 5% reduction if you remove the tokenizer. If you remove AWD, you drop your AUROC by 2.7%. A lot of people asked about transformers. So we tried transformers in the last two rows and they did terribly because we hypothesized that the data set is just too small for something that big in terms of parameters. We also wanted to see on different data sets whether, you know, if they Data sets, whether, you know, if they are fundamentally different, how well they'll do. So we tried transfer learning on a data set based on X-ray crystallography data. There's only 2,000 data points in this data set that is based on PDB protein-ligand interactions. If you pre-train the encoder that we spoke about on the string data set, and then afterwards train the classifier on that bio-lip data, that small biolip data, we can actually achieve an AURC of 0.9%, 0.9% rather. 0.9 rather. And so we do well on a small data set if we pre-train. And then finally, we were really interested, this is something that we're interested in looking at more in the future, is how RAPID may be used to validate hypothesized interactions between target proteins and candidate therapeutic proteins and peptides. So if I have a peptide that I want to use as a therapeutic, how can I use RAPID or something like RAPID to test it as whether it hits a target? As whether it hits a target protein. And we came up with two anecdotal examples of trastuzumab and pertuzumab, which are recombinant humanized monoclonal antibodies. They're treatments for a specific type of metastatic breast cancer called HER2. And basically what we did is we, as I said before, as I hinted to, we take tristuzumab or pertuzumab and we do a pairwise comparison with all the proteins that we could find in Uniprot and we plot their probabilities. And we plot their probabilities. And so, this is what you get when you do that with Trestuzumab. And the vast majority of them are below 50%. So, we'd like to see that. And we also like to see that HER2, which is what we know Trestuzumab interacts with, is in the first percentile at 98.8% interaction. So, we'd like to see this kind of specificity. It's not perfect yet. There's a lot of things above 50% that I have a hard time believing are above 50%. But, you know, if you were to do a screen, you would find her too. You would find her too in the first percentile. And similarly, for Trastuzumab, you'd find it in the second percentile at 95.1% interaction. So, this is something that we'd like to refine and get a better idea of later on. So, thanks so much. I'd like to thank the members of my lab and all my gracious supporters. So, thank you so much.