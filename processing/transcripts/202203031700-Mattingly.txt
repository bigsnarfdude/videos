And Andrea was a postdoc here who just started a tenure track job. He's just about to discover that things were much better when you're a postdoc in Pisa. So, and I should say the numerics are done with a Duke student, Nikhil Shankar, who spent some time at Duke, but now decided he wanted to go work for Amazon for a while. So, unfortunately, or fortunately, it makes him happy, it makes him happy. Makes him happy, makes him happy. All right. So, you know, this, this, I've actually unfortunately been sucked into doing some gerrymandering work the last five months pretty heavily. So this is still very much work in progress. We have one paper on the archive, but I just thought I would explain some ideas. I've been telling Alex that we are finishing this paper for, I don't know, like five years or something or three years or something ridiculous like that. So here it is, Alex. All right. So, you know, this is a slide that I don't really need to have here. Slide that I don't really need to have here, but uh, you know, we have so we have the 2D Navier-Stokes equation. I'm always going to be talking about 2D. Um, so we're starting off with velocity. I'm going to write it usually in vorticity formation, which I have the right side on the torus, about only about four years. Thanks, Alex. And that's what happens when you're chair, Alex. Someday you'll discover that if you're unlucky enough. And so on the 2D periodic Taurus and And what I'm interested in is, I'm so, you know, a lot of times we've thought about, so we have this, we could think about the Euler equation on the right-hand side without the red terms, or Navier-Stokes with the red terms. I'm actually going to be particularly interested in trying to make models that do a good job of thinking about Euler. So historically, when I've spent a lot of time thinking about this problem, I've thought about the stochastically forced version where you maybe had some body forcing, and then you had some stochastic agitation, which was probably concentrated in the low modes or some mesoscopic. Concentrated in the low modes or some mesoscopic scales. And you were interested in understanding how that randomness transferred through the system to produce ergodic ergodicity. All right. So here's just some cartoon kind of the kind of things that I would like to try to understand. And I'll explain why I'm doing what I'm doing, given all the work I've spent doing other things. All right, so let me try to explain that a little bit. Me try to explain that a little bit. So, I want to think about this stochastic forcing for a second. And so, you can write these equations in Fourier, and then to have this infinite coupled lattice of Fourier coefficients here. I've written them with complex Fourier coefficients and some coupling coefficient that tends to couple L plus J that add up to each, that add up to K, except if the lengths of L and J are the same, or if they satisfy this orthogony condition, then they're coupled, which comes from the incompressibility. Coupled, which comes from the incompressibility. So I don't think I need to belabor that too much in this group. And of course, we have two important quantities: the enstropy and the energy. And in the end, I would like to be able to study systems where I'm keeping an energy fixed and letting the enstrophy go to infinity with some appropriate dissipation equations, so dissipation boundary conditions. So I'm really kind of getting to rough equations that are transferring energy, I mean, transferring encrophed at small scales and energy back up. All right, so. All right, so and if I just study this, I have the deterministic Euler equation, right? That's just again on the Taurus. It's just this equation. I do probability half the time at least, so I can't use omega, so that's why I use Q. I hope everyone will forgive me for that. So, you know, if I can think about this deterministic dynamics pushing forward some measure, and if I do that, I push that forward with. Do that, I push that forward with you know with some semi-groups. So I start off with some initial condition and a test function phi on the velocity on the vorticity field, and I push that forward. And then, you know, since these are conserved, if I have some invariant measure, then an invariant measure is just something which is invariant under this dynamics. All right, that's what I mean by an invariant measure. And, you know, okay. So, of course, there are lots of fixed points. So, of course, there are lots of fixed points. I should have said that's what the last side said. So, then I want to define this stochastic situation. So, now I have some Markov operator. So, I'm pushing and I'm averaging now with respect to the randomness. That's what this expectation is. And look for invariant measures there. So, that's all great. In the historic way, you know, this all has some kind of hypollipticity condition to make sense of when the randomness spreads enough to keep things ergonic. All right, so this is kind of an old story. This is work with Martin Hare and Yakov Sinai and Wayne E. And Jakob Sinai and Wénan Eu and Etienne Pardu. The final kind of piece of resistance of that line of work was finally maybe this final paper with Martin. And then there was lots of other work with Brickman and Kupianen and Kuksen and Sherik Hayan in slightly different settings. All right. So typically in these settings, the noise plays many roles. So, and what I want to get away from that in a way. Get away from that in a way. So, the roles that it plays. So, what does the role play? It plays both the way that energy is being pumped into the system, it's also the way the kind of generic behavior is being guaranteed. So, I want to only use the stochasticity to agitate the system to make the system dynamics generic. And I want to do this in kind of a micro-canonical way. So, I want to preserve energy and enstrophy when I do that. And I want to make the agitation more elliptic in some sense, but. Elliptic in some sense, but respect the dynamics, maybe. And I want to do that because I want to be able to say more things about the system long term. So I want to make my job a little bit easier as a modeler. And then I want to allow keeping the forcing fixed. So I kind of have some idea of what structures I'm perturbing about. So I want to have kind of deterministic body forcing that's keeping the system out of equilibrium. And the randomness is just kind of providing the excitation, the agitation to make the dynamics generic. Agitation to make the dynamics generic. And I also want to isolate kind of simple dynamics in the system to help me improve my intuition and improve my ability to do theorem. So let me say this really, really loudly now. I'm not claiming this is a closure of the right equations. This is not going to give the right answer. This is a model maybe that's closer, more natural than a shell model, but it's still a model. And the model is being chosen not because of its pure physical relevance, but maybe because it'll let me make some steps in the middle. It'll let me make some steps in the middle to prove some theorems. But also, it kind of provides a thermal scale, it can be sketched, it can be tuned in a way to provide more thermalization at the finest scales. Maybe in the way, maybe I'll let Greg speak for himself. Don't be timid, Greg. Tell me what you think. Let me have it. All right. So let me start by showing some simulations here. So here we go. So here's some version of the model. So here's some version of the model running. And, you know, this doesn't look non-fluidy. It's a little smoother on my machine than it looks on your machine with the Zoom. All right, so this is kind of what, let me look, let me switch again. Let me show this now. It is kind of blocky, but, and this is on a log basis. This is Enstrophy is in blue and energy is in red. Blue and energy is in red in logs, scales, all right. And let me show you a little another example. So here we have two pair of vortices interacting. And we kind of see some kind of thermalization down here, very much thermalizing. And some structure left up in the high large scales, although this isn't really. The high large scales, although this isn't really tuned to be exactly where I'd expect the inverse cascade to be dominant, but that's coming soon. My numerics person unfortunately left, and so I'm running the code now, and I just haven't had the time lately. All right, so here we go again, one more time. Alex, what time am I supposed to stop? I forgot. I forgot to look at the clock at half past, right? Half past. That's right. Half past. Half past. That's right. Half past. Half past. Let's see if I can get my clock to come up. Okay, half past. Great. Thank you. All right. So, all right. So, let's think about two model systems. Let's think about Lorentz 96. And so everything I'm going to talk about today is Galerkin. It's not because some of it's doable, some of it's not, some of it's harder, some of it's easier in the non-Galerkin system. I'm happy to suss that out probably, but this is just what we've done so far, and it's in progress. So I'm going to talk about the So, I'm going to talk about the Lorentz system, which is just this periodically coupled ring of nearest neighbor interactions. And I'm going to think about the conservative Lorentz system, which is to say the Lorentz system without the terms in red often. And then also the 2D vorticity formulation of the Euler equation or the Navier-Stokes. I have no idea what just happened there. That was the weirdest thing. One second. Sorry about that. I have no idea what just happened. There is a chance. No, there it came. Okay. There is a chance. No, there came okay. It seemed like my machine crashed for a second, but it came back, which is good news. All right. So, so, what I want to first think about: so, everything is deterministic right now, and I want to just observe that I can rewrite this right-hand side as a sum of interactions, sum of nearest neighbor, nice little interactions, and then some kind of Ornstein-Lebecker, some kind of heat-like dissipation and fluctuation term here. Term here. So here's my interactions. And the same thing can be said about Navier-Stokes. Here's the Navier-Stokes part. And here's the Euler interactions, which I can split into kind of a classical triad model, all right, of three-mode interactions. Yeah. All right. And now, so now the idea, so of course, the idea is going to be what I'm going to do is I'm going to run this model by splitting the dynamics. So I'm going to create, I'm going to think of a I'm going to think of the diffeomorphism φk as just being flowing according to the vk velocity field. All right. So I'm going to do kind of what you would think of as classical operator splitting. I'm going to run each vector field for a short amount of time while everything else is frozen. All right. And of course, since this is a random model, what I'm going to do is I'm going to pick the amount of time that I run each of these models with an independent exponential random variable. All right. Random variable, all right, okay. And so sometimes I'm going to have the red ones, that's when it's Navier-Stokes, or sometimes I'm just going to cycle through the other vector fields one through n with some ordering, and I'm going to run them for some exponential amount of time, okay? With some parameter h. The mean is h, all right? Is that clear to everyone? So, and my capital phi is my one cycle through here. I could do lots of things, I could randomly shuffle the order every time. Randomly shuffle the order every time. I could do lots of different things. I'm not going to do that right now, just for simplicity. I'm just going to fix an ordering that's convenient. None of the theorems really depend on that. Okay. All right. All right. So now I can define a semi-group, which is this just pushing forward this mark, this one step of this kind of dynamics. And of course, although I'm taking n steps here, you really shouldn't think of them as n steps of the dynamic. Of them as n steps of the dynamic, that's just the equivalent of one step of time h of this dynamic. The wall clock, it kind of proceeds by h in that time, okay? And then, of course, I have the deterministic dynamics, and this creates some kind of random dynamical system fibered over the tau and the x space, all right? Just kind of a random helix. All right, so there's a lot of actual work in, I got into this thinking about piecewise. I got into this thinking about piecewise determined random dynamics about 10 years ago, a little under that for some biological reasons, and that was with Mike Reed and Sean Lawley. But there's been, Davis has some work, lots of engineers have some work, Hirth, Bactin, Menimin, and a lot of his colleagues. There's been a lot of thinking about this, but it's often been for biological applications or for statistical sampling purposes. But I want to think about this as a way to introduce and Want to think about this as a way to introduce randomness into this model in a way that gives me some different kind of regularization. And also, it gives me maybe some ability to kind of isolate underlying dynamics in a way that I can deal with them in a nicer way. And I think I can do more here, maybe. We'll see. All right. So, the first obvious theorem is that if you fix a time scale and you take m steps so that m times h is equal to t, then as h goes to zero, Then, as h goes to zero, this deterministic random dynamics actually converges to the deterministic flow. So, on a finite length, on a finite time window, you can make this as faithful as you want to paths of the Navier-Stokes equation. Okay. Remember, I'm always talking Galorkin, so I should always be saying Galorkin, Navier-Stokes, but just hear that if I forget to say it. And I should be saying Euler, actually. Right now, the most interesting case for me is actually where the red one is not there. For me, is actually where the red one is not there, and I'm really just thinking about Euler. So, this is a random system which preserves energy and estrophy, so it lives on the energy and strophy sphere or the intersection of those the ellipse in the sphere. Okay, all right, so uh okay, so we can serve all each, so so I'm gonna so for Lawrence 96, I choose these Vks, and the reason I choose these particular examples is because they conserve energy. Because they conserve energy. So then I end up with a system which is just a simple rotation. This is just a pure rotation, but the speed of rotation depends on this extra coordinate xk minus one. So it has shear. So that's nice. So not surprisingly, this is going to create kind of stretching and folding that we'd expect to create positively up on f exponents and things like that because it has this shear. The splitting we've chosen is kind of an elemental shear, all right? All right. Okay. And then for Navier Stokes, what we're going to do is we're going to, we're actually, I lied to you ever so slightly. Just for simplicity, we're actually going to project these complex coordinates onto their real counterparts. It's not strictly speaking necessary, but one of the points of this is I want to be able to think about things in three dimensions or places where I can see things. So C3 is still hard for my mind to grasp fully. So, but R3 is getting a little friendlier. But R3 is getting a little friendlier, especially a surface in R3. And so, what is the dynamics here going to be? So, first of all, each of these VKs are nice because they conserve energy and enstrophy. An interesting point of view here is I could choose something different that only conserved energy if I wanted to. If I want to think about an equation that only approximately conserved enstrophe, for instance. And so, what the dynamics will be will be these green orbits sitting on these kind of rugby balls. Sitting in high dimensions. This is the intersection of a sphere and an ellipse because energy and instrophy are conserved. All right. Yeah. And so now what I'm going to do is I'm going to switch through all these dynamics and mix them together and run them each for a little bit of random amount of time. All right. So it's not hard. I mean, so it's a nice theorem of Omar's and Andre and mine that this conservative Lorentz diagram has a unique invariant measure. Has a unique invariant measure on the so first of all, you can't start from initial conditions where these vector fields are fixed points. So, it actually, one nice thing about it is it sees the fixed point structure of the problem. So if you don't start on that, the set of fixed points and fixed structures has Lebesgue measure zero or Hasdorf measure zero on this surface of constant energy and energy, constant energy. And so on this set chi, which I define at some point, yeah, there. I define at some point. Yeah, there it is, right there. It's all the points that aren't fixed points of any of these individuals, there aren't fixed points of at least one of these. There are fixed points of, that is not a fixed point of all of these dynamics. So as long as one of them is not fixed, which is what this says, then you're fine. You can start there. And then if you actually put force and dissipation, you can actually prove that you can start from any point. So as long as you pick a fixed body forcing, which is not. Picks body forcing, which is not a fixed point of the conserved dynamic. So it kind of breaks all the fixed points, which is easy to do. As long as you pick some forcing, which breaks all the fixed points, then the system has a unique invariant measure, converges exponentially in total variation norm, et cetera, et cetera. And of course, this statement here, the unique invariant measure on H is for the conservative system. So this is kind of microcanonical. I can just look at the dynamics on the energy shell, which is nice. On the energy shell, which is nice. Okay, so now if I look at this Navier-Stokes equation, there's some set Q, which I'll tell you about, which is the equivalence of these fixed points sets. But if I start on some energy entropy sphere, then the restriction of Lebesgue or Hasdorff measure, I should probably say, to this sphere is the unique measure for this random system. All right, so it's the unique measure on this set with these points removed. Of course, if I start off. With these points removed. Of course, if I start on these bad points that I've removed, which I'll tell you in a second, then I could also get stuck there. And if I pick now a forcing term, if I put the dissipation back in, as long as I fix a point of forcing, a fixed body forcing F, so that's not a random forcing, it's just in one direction. If I fix that one rand body forcing, as long as that breaks all of these fixed points, then you actually have a unique invariant measure for the fluid for the Navier-Stokes equation. All right, and these weird fixed point set is. And these weird fixed point set is something very reminiscent of kind of the conditions you need for hyperlipticity. I don't actually expect you to understand this because, to be honest, I probably didn't write every single thing I need, but the idea is the following is a statement about initial conditions now. So it's kind of what the resonant interacting modes are for this initial condition. As long as somehow, when you start from the ones that are not zero for this particular queue, and then you interact them in the way that the kernel interacts, you actually end up generating. You actually end up generating at least three vectors like this. And this is not optimal, but it's good enough. It's reminiscent of some old work that I have with Etienne and then repeated with Martin. So, you know, as long as these vectors are contained in kind of this set, where this weird addition here is where you take the set vectors you already have and you interact them with the kernel by adding and subtracting in this way that they do on the lattice. So this kind of catches the local resonance structure. Is the local resonance structure? So it's so Q are all the points where this condition here is held. Okay. All right. So now we can go on. So if you go to oscillatics, so you can look at the oscillatics ergodic theorem, which I guess is kind of interesting to be talking right here. I still remember when Kostia first explained all this to me a long time ago. The secret was. The secret was we all used to leave Sinai's office confused and then go down the hall to Kostia's office where he would explain to us what we didn't understand from the last 40 minutes in his Sinai's office. So one day, I can still remember when Kostia and I were talking about Bougero and key first stuff on this. So very much so. So, you know, I don't need to explain this to everyone here, but we have this multiplicative ergodic theorem. So we have these deterministic real numbers, which are the eigenvalues, essentially, and these. Eigen values essentially and these random eigen subspaces. So that if I start in these, in these, in the component of the space, and then I look at as n goes to infinity, and I look at how the linearization grows as a random linearization, then what these converge to. Okay, so I'm actually going to be thinking about this on the Euler dynamics. So I'm going to be thinking about this on the Euler spheres. So I want, so in this case, I actually have zero. So in this case, I actually have zero, I have trace zero linearization. So I could actually see, possibly have all the eigenvalues, all the Lyapunov exponents zero. All right. And so that's what you have to rule out. All right. All right. So, oops, these were, these got messed up. Well, there we go. See, there's, there's Jay, there's Alex. So this, there's definitely. There's Alex. So there's definitely existing related results by Jacob, Alex, and Sam. Those were in the kind of in the limit of the equations going to Euler, and these are actually just directly thinking about Euler, the Euler-like dynamics. And so this is the one we're still just finishing writing, but we're incredibly close to having it all written down completely. So I am pretty confident this is exactly correct. So the conservative Lorentz. So, the conservative Lorentz 96, the randomness conservative, and the Euler equation, the stochastic dynamical system that I mentioned almost surely has a positively up enough exponent for every initial condition on that nice set of points where we can define the dynamics, okay? All right. Yeah, so let me, so, so, I mean, so that's that's the theorem. And the theorem kind of follows, you know, in the basic way of following, you know, take your pick, be LaDrappier or Boxendale. Dropier or Boxendale, essentially, following that, those kind of semi-those classical works here. All right. So maybe when I have this last minute, so maybe I'm going to, let me see, how much time do I have left? Sorry, my clock. So my clock is not showing up. Okay, I have like five minutes. You can take five to seven minutes. Yeah, okay. So let me just let me show a few things. So this is actually a really cute proof. This is actually a really cute proof, which I'll show you because it's one of those things that, like, for a long time, I looked for quantitative ways to control things like operator splitting using Baker-Campbell-Hasdorf formulas and trying to get quantitative bounds on Baker-Campbell-Hasdorf formulas. And it really kind of drove me crazy because I couldn't find a good way. And then I found this in some papers on quantum information theory that I was reading for another reason. And it kind of blew my mind away that somehow I didn't know how to do this. And so this is probably one of those cases where everyone else in the room knew it. And if I just asked one of you guys, you would have told me. And if I just asked one of you guys, you would have told me. But I spent a lot of time trying to figure this out. So let me just point out this really cute argument, which is that if you take this random, this, so there's an expectation missing here, sorry. So you take this random map, which is this kind of flow forward by each of these vector fields for this random amount of time tau, and I kind of fix these to be order one random variables, and then I scale with tau. Then the differentiation t. Then, then the differentiation T is just you just find where you just apply the chain rule and differentiate in there inside each of them. And then you do the stupidest thing you could ever do: you just do the commutators and flip them all out in front. And when you get them all out in front, then this is actually the real true vector field, not the random, not the split one with random orders. And so you write that here, and then you just apply Duhamel. You just do variation of constants, and you get a bound on this, just like you would always do. And you get. Just like you would always do, and you get exactly a bound on the deterministic one and the random one by some error terms, which you can show are the right order. And so, it's really kind of an analyst proof of this, which doesn't need any kind of discussion about formal power series or algebraic row things. It's quite actually beautiful. I don't take credit for it, but so you can, so that's how we prove the convergence as h goes to zero. But I emphasize we don't want to take h goes to zero. We actually want to study this model with positive h. Actually, we want to study this model with positive H because we think it's an interesting model to study as a halfway point. Okay, so what do the measures look like? Let me maybe show you again these pictures. So this is a longer time run. So we kind of get this thermalization. So what that unique invariant measure, of course, is Lebesgue on this ellipsoid. And we get some energy being concentrated up at the high frequencies. And maybe let me, since I'm running out of time, let me just go here maybe. Let me just go here, maybe. So, this is now the one where I'm going to force only, I'm going to dissipate on the edges at the this is Fourier space. So, I'm going to dissipate at high frequencies and I'm going to force at low frequencies. But remember, the forcing is deterministic now. It's just some fixed body forcing of about four or five modes, a little band. And so, now if I do this, what I'll see, and this is a low viscosity here, and so I get some kind of, you know, this is not. Get some kind of, you know, this is not that big. This is like a hundred by hundred box in Fourier. Um, it's not huge, um, but I get some thermalization like I expect at the low scales. And then here I have a little bit stiffer viscosity, and I see that I get a bit more of a structure here showing up. Um, the movies look a little nicer on my screen, they get a little, they get a little pixelated under Zoom. So here we are again. Under zoom. So here we are again. This is just, this one's fortunately at the rather low viscosity. I wish I'd done it at the higher viscosity to suck the energy out the end. I think I'm getting a lot of backscattering here more than I would like. But, you know, this looks, I get some nice structures building up here with the randomness, which is nice. And maybe, you know, if one movie was good, whoops. Let's see. Okay. Two movies is better. Let's see, two movies is better. All right, so here you go. One more time. And so, I don't know. We have a number of things we want to do with this model, a number of things we think we can prove. I mean, right now we've done kind of the basic things that have been done before, but we think we can actually do some other things with this model, which are interesting. And there you go. So, hopefully, just to start, thanks a lot for your attention. Great. Thank you very much, John. So, you know, clap in your own way or if that like reacts. Great. So.