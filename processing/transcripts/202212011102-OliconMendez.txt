I speak about the research that we've been developing over the last year at Fraier Universität in Berlin. Maximilian and I have been working on a very long term now, it's becoming like a very long term project now. There are so many interesting things happening in a very simple, very old model, and I hope that I can convey a bit of why is this relevant and of interest. Yeah, so let me just begin. So, I'm going to talk about a very old friend of the chemical community, so the Brusselator. So, this was a model developed for, or yeah, proposed by Ilio Pégogin and Lefervo back in the 60s, sorry. And it's just a very simple chemical reaction network which involves four reactions and one, two, three, four, five, six. Four, five, six chemicals, right? But the relevant ones here are only two, which we denote by X and Y, because we make a couple of assumptions. So, first of all, we're just going to assume that everything happens following a law of mass actions, which is like one of the simplest ways of modeling or interaction between chemicals. And well, here, for example, you can notice that D and E are basically decoupled, they just are. Are basically decoupled, they just are like an outcome or a byproduct of the whole interactions. And we assume that there's this is like what people call the infinite pool assumption. So we assume that there's a lot of A and B, so we can assume it's just a constant in the system. Parameters, of course. And of course, there are different ways of modeling this chemical reaction or this system of chemical reactions. Or this system of chemical reactions, a very microscopic one is what people call the reaction jump processes. So basically, it means we remain, like the system remains constant until randomly one of the four reactions happen. And when that happens, then the concentration or the number of molecules obviously changes because they interact, they produce something new, and so on. And then it remains constant until randomly something new happens, and so on. So, this is one approach. So, this is one approach that I will talk about hopefully by the end of the talk. There are two other approaches. So, something that involves large volumes, but not so large. So, this is what people have called the chemical Langevin equation. And I know it looks daunting and awful, and it is indeed. But you can, I mean, the people who are already close to this kind of models, you can see a bit of a relationship to the Of a relationship to the ODE model, which is the famous one, right? So this now looks very familiar, very simple, polynomial, and so on. This basically is just the limit when v, the volume, goes to infinity of v sky. So there are like three different scales in terms of the volume or the densities that people have studied thoroughly. However, However, the second one, so this one creates lots of complications that I'm going to talk about in a couple of slides. But first, let's just remind ourselves what happens with the original model, the Brosolator, like the proper Brosolator. So this is a model or a HODE which exhibits Hopf vifertation, a Hopf vifertation. So depending on the parameters B and A, we can find sustained oscillations. So in particular, Oscillations. So, in particular, if b is greater than this value, so for a fixed a, if we take b greater than 1 plus a squared, we can observe, or well, there is a limit cycle emerging due to the hop bifurcation. Okay, so this is the deterministic figure. What happens with the stochastic version, so the chemical Langevin equation, the biggest problem for us who want to work with dynamical systems is that it doesn't admit solutions for all times. For all times. And then, this, of course, creates complications. And since we want to do dynamical systems, then we just turn around and say, okay, which could be a sensible model for a stochastic procellator, but which also has some chemical flavor, right? Because there are many suggestions like, oh, we impose reflecting boundary conditions or we kill the system and so on, but it lacks a physical interpretation. So, one proposed model. One proposed model was by Ludwig Arnold, Bleker, and Jean-Copet, which is assuming that the parameter B, the one that actually, or one of the, the one we can consider, the bifurcation parameter, is random, is a random parameter now. So if we assume that it looks more like B plus some noise, then we strattenopich noise, then we include these two terms in the system. So here's So here, sigma is going to be a measurement of the noise strength, right? And Wt, as usual, it's Brownian noise. And this actually does define a random dynamical system. So in the sense that solutions exist for all time, future time, sorry. And our question as a dynamicist is: well, now that we have the playground for us to just do our stuff. To just do our stuff, then can we observe what kind of interesting behavior we can observe depending on the parameter bodies? So, in particular, does it always happen that there's noise-induced synchronization that there was a talk earlier today? Or can we actually observe unpredictability with respect to initial conditions? So, just as a reminder, what we mean by this kind of behavior is the following. We fix a noise realization and we compare order. Realization and we compare orbits with different initial conditions. This is what we have in mind when we work with random dynamical systems. So yeah, I mean, just to make everything more pictorial, these are two different realizations of the, or two different trajectories of the system with the noise, same noise realization. It's in red and blue. Blue. And I mean, it does resemble a bit what happens in the deterministic system. However, there is something interesting happening here. So it seems like somewhere in this arc is accumulating and it's a little bit more mixing than here. Here it looks like everything is coming down in a very straightforward manner. And this is, of course, beyond the bifurcation point. What happens before the bifurcation point, however, is that everything Bifurcation point, however, is that everything is just deviating around the attracting fixed point, the deterministic one, which is rather expected. So, as I said, we're interested in the convergence or divergence between orbits, so we calculate the distance between them, and these are the two features that we observe. So, of course, when we have an attracting fixed point, we observe like a uniform contraction between those orbits. However, in the In the oscillatory regime, we observe like constantly, very frequently we observe deviations from these orbits. Now, this is something that one has to take, you know, one has to make a little bit more of an analysis. Of course, these deviations between the orbits happen, but they eventually die off. But there is a large time scale where these deviations even amplify eventually. Even amplify eventually. So, our first approach was to do something that Alexandra has already presented. So, what happens in the finite time scale? So, what if we study the Lyapunov exponents without taking any limits, just in a finite time window? And this is what we did. This is just like a slide for the formalization. We have a general SD with sufficiently smooth vector fields and so on. We take the variational equation. We take the variational equation in its matrix form, and then we just take the logarithm of the solution here and divide it by t. And this is the finite timely upon exponent with, of course, with this initial condition. And then, of course, this depends again on the time window or the time scale. This depends on the initial condition and also the noise realization. But this one, we fix it, we generate it right. But this one, we fix it, we generate it randomly. And then what we can explore is what happens throughout state space. And this is more or less the figure we have. So if we choose just as a reference, there's nothing particular about the choice of T, but if it's something close to the period of the deterministic limit cycle, this is more or less the landscape of the finite time Lapping of exponent for this choice of the parameters. This. So we can observe that throughout state space we have positive values of these bots. And this is an interesting figure because it is not really capturing the essence of the instability of the repelling fixed point, right? So in the Hopf bifurcation, the deterministic one, there's a deterministic fixed point which becomes unstable. And this figure says that this instability is actually spread. Instability is actually spread throughout certain regions of the state space. So, this is in its own an interesting figure, and we would like to understand a little bit better what happens. But if we go back to what happens here, I should also say that by tuning the parameters b and sigma, we can make this window as large as we want. I mean, with some magic, you know, playing with the numerics. Of course, it's not like a With the numerics, of course, it's not like something straightforward, but this is also an indication of: well, there are two different possibilities perhaps. Either we can make it for fixed finite B and sigma, we can make it as long as we want, meaning that there could be potentially noise-induced chaos. Or, well, in its own, you know, if we can make it as long as we want, well, this is like finite time chaos in some sense. So, the choice, or what we have observed numerically, is that if B is large enough, these windows are actually extending somehow. So, we take B of this shape, like A divided by epsilon, and then, of course, immediately we get to something that looks like a slow-fast system in non-standard form. And of course, this. And of course, this is also in a different coordinate frame. So we actually take, so we have an equation for x, an equation for y. I just take capital X, or yeah, this one, to be X plus Y, and Y to be the little X that we had before. What we have here is something that is bounded in this half-octant. And these are some realizations or some orbits of the system. I apologize for the colors, perhaps it's not very clear for some people, but you have two different orbits, one in yellow, one in blue, and we do observe like a slow dynamics here. And then the interesting part happens here. So somehow, as soon as we start approaching this turning point of the limit cycle, if I may use the term, it seems like because of randomness, It seems like because of randomness, the orbits actually can jump before or actually can jump after. And because of the noise, one of the trajectories, even though it's the same noise realization, it might be jumping before or after, creating like this kind of deviations, these large discrepancies in the orbits. But is the direction of the flow is a corporate? Yes. It so it grows uh very close to the uh identity line, then it jumps to the left. Then it jumps to the left and then it goes down. Thank you. So this is an interesting, I mean, this is what we think is creating like finite time instabilities at least. And when we see the equation, we say, okay, what has been done for the Brosillator without noise? And surprisingly, there was not many things done, except. Things done except some, yeah, like this transformation in this new coordinate system. So then we started analyzing the system on its own. Of course, if you see the state space here, it just screams a bit like, oh, bringing me back to polar coordinates, because the angle is just between pi-fourths and pi-halves. And well, the r can be any positive number. And this is how it looks like. It looks Looks like it looks not so nice perhaps, but it just has like very simple terms for the sign and polynomial in R. Now, if you go, if you, you know, well, I'll help you. If you see the limit cycles here, as soon as we start increasing b more and more, we see that the limit cycles just grow and grow and grow. So, as opposed to what happens, for example, in Fitzugna Gumo or Fander Paul, there is Funding Paul, there is like a critical limit cycle. One can identify, we observe faults, and we construct, like, well, these are examples that you already know very well. But here in the Brazilator, we don't have this. So the idea we had was, well, if everything starts blowing up to infinity as we make epsilon going to zero, why don't we bring infinity to play? So we rescaled the system, or we transformed the system as this, right? So that infinity. This, right? So that infinity is mapped to one. And now we are playing instead of this octant or half-octant, we're playing in a piece of pipe, like the ones we had a couple of days ago. And this corresponds to infinity. And the equations for this are given here, where the f and g, of course, are just the transformations of these guys. The velocity of the vector or the vector. The velocity of the vector or the vector field here is not well defined, but we can rescale the time so that it's just an invariant set now. And this is more or less like what we've been working over the last two or three weeks. What we aim to do is just use similar techniques as in the paper by Peter and Ruba. So we want to show. So, we want to show that the Brose Elector exhibits relaxation oscillations when we have large B, but in this new coordinate system. This is our aim. We have already many things done, and we just need to fill in the details. And of course, as I said, our aim in the long term is to understand the random dynamics as well. So, can we use all these approximations of the limit cycles and so on so that we can give And so on, so that we can give some information about finite time Lyapunov exponents and perhaps the proper Lyapunov exponents. So I should mention that this is actually an open question. Does the stochastic Brosolator, as I presented it before, does it exhibit noise-induced synchronization? Everyone, or well, Arnold believed and colleagues believed that this was true, but there is no proof for this. So this might be like a first approximation for giving an answer. Answer. Yes, of course. And if it doesn't, either it has positively up enough exponents, meaning the conjecture was false, which is also a nice result, or we can actually give some information about the finite time dynamics. So I have just 10 minutes left. I'll try to spend five talking about the other perspective that I presented at the beginning, which was the Markov jump process. So the Markov jump process, as I said, Of jump process, as I said, just we can just think about this. If we know the states of the system at each jump and how much time it spends in this constant regime, then we can reconstruct the whole trajectory. And this has been done in the past. So these are the variables that we're interested in. This has been done in the past via the so-called Gillespie's algorithm. So this was done mostly for reconstructing the trajectories and doing statistics of it, but when Statistics of it, but when we saw this formulation when we were discussing more or less one year ago, well, this is just a random difference equation. And this is exactly the setting for doing random dynamical systems. Here, the Qn and Rn are random variables between 0 and 1, uniformly distributed. So we can just do, I mean, there are explicit expressions for these guys. It's really not that important. Guys, it's really not that important. You can consult the expressions and the basics here in this reference by Stefi Wingenman and Christoph Schute. But the important thing here is that if you look at the first equation, this is independent of t. And this is just a Markov chain given by a random difference equation. So on its own, it's a random dynamical system as well, which somehow is... Well, which somehow is forcing, perhaps I don't know how to say it, the equation for T. So, one interesting question is to analyze this one on itself. The good thing is that here we're talking about a discrete state space. And not many people have worked on random dynamics in discrete state space as we have done in the past. So, we started analyzing the essence of what dynamics. The essence of what dynamicists are looking for, right? So, attractors. And we, in the random setting, there are different notions of attraction: weak attraction, forward attraction, pullback attraction. Most of them are essentially the same in the discrete state space. So, there's no big differences between them. I mean, there are subtleties, but typically, this is what happens. So, weak and forward attractors are equivalent. Forward, you know, you have an invariant. Forward, you know, you have an invariant object and everything starts approximating it. The weak attraction means this attraction happens in probability, not almost surely. So these guys are essentially the same. We know that there is an attractor if the Markov chain is positively recurrent and irreducible, which is a very standard setting. And of course, the random attractor has to be compact, but compactness in a discrete state space. In a discrete state space is just finite. So, this is really nice because then we can talk about combinatorics and so on. And this is actually work contained in this paper that we submitted last summer. And of course, the generalizations and further studies about it. This is work in progress also with Holly Kemnitz. And how do we connect with the previous slides? So we have, of course, this Markov jump process. Markov-Jump process, they like both the ODE or the SDE or the Markov-Jump process come from the same chemical reaction. And there's some kind of way to relate them via some convergences with respect to the volume. And these convergences are actually well characterized. So if we have like a, I mean, this is a Markov-John process, but in densities, so rescale with the volume, and this is the solution of the OD, there is a correction intermediary. There is a correction term here that depends on the volume as well. And this is how it looks like. Of course, this happens in probability. So, our ultimate goal as well, or a separate project if you want, is once we understand what happens with the slow-fast system here, how do we translate this into the Markov jump process? Can we get some information, for example, about the attractors? What else is it screaming out? Out. Yes, so this is exactly what I wanted to share with you. Thank you very much. Thank you. For this interesting talk, there's time for questions, please. When you have two random trajectories and they are far away from each other, that's where what where I stand right, you show this plot of the distance between the two. They can be very close to the prairie orbit already, but Very close to the period orbit already, but they're not synchronized yet, right? That's actually the issue, right? So I wonder whether you know issues like isochrons and so forth would not be of great interest, right? The regions where they push you together and others where it's neutral. Yes, so the most colourful picture almost screamed out. Yeah, it does. It does. Yeah, this is something we discussed a few months ago, actually. But the mechanism, though, But the mechanism, though, that's. Well, what I'm saying is it would be interesting to have the picture of the isochrons for the jump process and put your dynamics on it to understand. Because if what you're saying is right and eventually they synchronize, it means they eventually have to synchronize, which is sort of a surprising thing. So such a phases, right? Why would it do this? Where does this contraction in the phase come from? And geometrically, it's of course somehow the structure of the isochromes in. The structure of the isochrons in combination with your noise. With your ramp, with your noise. But it's surprising, but not. It is, yeah, yeah, yeah. Yeah, no, this is a good point. Yeah, thank you very much. Questions? I'm not quite sure I deserved it correctly. You had this proscillator, you had, you know, on the y equal to x axis, you move and then depending on the noise, you kind of jumped at different spots. kind of jump at different spots. So is the dynamic slow there? So is it just the effect that you know that the noise has enough time to generate sufficient perturbations that it then eventually will jump? Or what's going on there? I would say it's kind of like a combination. So here you have this low manifold and of course the orbits are so this is the deterministic setting, right? So here we have an outline where the Have an outline where the vector field points upwards, and once you jump here, once the orbit reaches this point, then it immediately jumps to the left. So as you say, if we have a random perturbation of it, it's kind of like a combination of the randomness somehow pushing you through the knot line before and then it jumps here or avoiding it. Yes, but it is slow, so right? Yes, that means you have to. There, right? So that means you have enough time to wait for large excursions that will push you across the blue line, don't you? Yes, it is low. Yes, exactly. Yeah, this is the noisy term. So, of course, yeah, you have to wait a long time for this to happen. Okay, there's another there's an online question, so let's ask contribution. Oh, you can speak. Can you hear me? Yes. Thanks for the interesting talk. I just have a quick question when I see your system you have the noise included in both equations, but the somehow the the intensity depending only on x rather than On X rather than occult, at least, that's what I remember from the system. Yeah, so I mean, the original question looks like this. So bx equals a minus 1 plus bx plus x squared y, and y equals bx minus x squared y. And then here we assume that b is random, so we just change this guy by b plus sigma topium, right? W, right? So here we have plus, sorry, minus sigma x dw. And here, since you have p as well, you have it with the opposite sign. So the randomness coming from this p, that's why it is depending on your x. Rather than that you say that there are some randomness of the system that amplified by the state of the system itself. Okay. Yes, yes, exactly. So the randomness comes from assuming now that B is a Comes from assuming now that B is a random parameter. Nice. Thank you. Please. So I was thinking about the setup of your jumping process. You started out saying there were four equations. And then we were supposed to ignore the D and the E, and then there was infinite A and infinite B, and then you reduced the whole thing to essentially two equations. So with your dump system, are you thinking in terms of four equations? Four equations, and so you're waiting, and then A is reacting, and so nothing happens. No, we're also thinking in two equations as well. I mean, the D and E are just byproducts, so these are just the consequence of whatever happens with the others. And A, I mean, so for example, you have the first reaction, which is A gives you X with a certain reaction K. And we just make law of mass action K1A times X is the term that is involved. times X is the term that is involved here. And you can put this term both in the ODE or in the Markov jump process. So this will be a term that is inside a Poisson process and so on. So yeah, you can make the same assumption on all levels of the modeling. Yeah, because also with the equations that follow, there is a certain number of molecules that need to somehow react. You would think that, I don't know, I'm not sure whether this is naive, I'm not a chemist, but some. I'm not a chemist, but some reactions you think are more likely to take place than others. And so there's a whole stochastic process with some particular probabilities that you could put on there. But that's not what you're doing, right? We are. I mean, yeah, so I mean, yes, of course, there's stochasticity. I mean, yes, of course, there's stochasticity in these reactions, if I understand your comment correctly. This assumption is also relevant because it brings you out of equilibrium. That's the essence. So it's no longer a closed system, right? It's second law of thermodynamics, I think. If it were closed, then everything in the long run goes to equilibrium. But we want to avoid this by assuming A and B are constants as opposed to, you know. As opposed to dynamical variables as well. Is that in the line you were saying? Yeah, I think so to some extent. Maybe we could talk about it afterwards. Well, I think the probabilities U and U2 are hidden in the reaction constant, which are very simple here. They are not constants, so these are just ones. There should be k's, actually, but they are assumed to be ones. You're correct, I think. There are hidden probabilities in these Markov functions, for sure. Yeah, of course. Sorry, yeah, these are all rescale variables. Just for understanding, so when you have the noise on your parameter B, is this the one which controls also the Hof bifurcation so that your noise would make the system more or less jump between non-HOF and HOF? So that you're always in between whether so it's oscillatory or this is this is a good uh This is a good