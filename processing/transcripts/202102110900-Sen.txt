Okay, so welcome to this morning session. We are going to have two talks and then we're going to have a discussion session. And the first speaker is Bodi Sen from Columbia University, where he is a professor of statistics. So Bodi, please start. Thank you, Johannes. Thank you, Johannes. Thanks a lot for the very kind invitation. It's a pleasure to be here, although virtually, and I hope to learn a bit more from all of you. So what I will be talking about is a talk that is mostly statistical, but there is an optimization aspect to it. So the statistics part, the first part of the talk is joint work with Jake Soloff, a PhD student at UC Berkeley, who is working with Aditya, who is again. Who is working with Aditya, who is again one of my main collaborators over the years at UC Berkeley? And the optimization part that we have just started working is with Yang Jin Zhang, a postdoc at National University of Singapore, and Ying at the University of Minnesota. And I must confess, this is mostly their work. I'm just presenting it. So, before I get started, I would just like to ask the audience: if you have any questions, please let me know. Please let me know. I know it's very difficult to communicate virtually, but I will try my best to stop and explain something if you have any questions. Okay. Okay. So let's get started. What is this talk about? This talk is about non-parametric maximum likelihood estimation, right? So here's the first slide, and here's the basic model that I'll focus on. So my data is y1 through yn. I guess you can see the I guess you can see the hand pointer that I'm trying to move to indicate which part of the slide I am in, right? You guys can see it, I guess. Okay, so I have data y1 through yn, and it's opted from this model. So yi is theta i plus zi. Theta i's are unknown, and they are the main quantities of interest. Zi's are errors, are normally distributed errors. Are normally distributed errors. So you only observe the yi's. This is in d-dimensional Euclidean space. Of course, as I said, the interest is in estimating the theta i's. But the heterogeneous part comes from the fact that the errors, z i's, have known variances, sigma i's, which are different for every observation. So, in some sense, this is somewhat different from classical statistics where the errors are not known, but here the errors are actually known. Not known, but here's the errors are actually known. And I'll give you many, many examples. My main motivation comes from astronomy, where this is ubiquitous in any data set in astronomy. They have observations and observations are corrupted because of measurement errors. So you can think of theta i's as the actual observations, so the velocity of stars or the chemical abundance of a star, but you don't get to observe it. You have measurement error, the instruments are not calibrated well enough. Sometimes there is dust. Well enough, sometimes there is dust, interstellar dust, and so on, which contaminates your data or corrupts your data in some sense. And all you have is YI, and the goal would be to recover the theta eyes. Is that okay? Is the setup kind of clear at this point of time? Please let me know if you have any questions. Okay. So now, so classically, this is a very standard model. This is a very standard model in statistics, and most of the focus has been on trying to understand what happens under sparsity. So, sparsity in this situation might mean that most of the theta is zero. So, the probability under this prior distribution G star. So, I should say a bit more about G star. So, one of the crucial assumptions we are making here is that the theta is, although unknown, are drawn IID from this distribution G star. So, it's unknown, it's completely unknown. And the goal. Completely unknown, and the goal is in some sense to recover g star and the theta i's if possible from the y i's this has enormous applications in clustering so in clustering you can imagine suppose you have k clusters then g star is a discrete distribution with k atoms and the theta i's are coming from these discrete distributions so once you have a theta i coming from the i-th cluster then you basically have observation from that particular cluster the The application that I would be most interested in is what happens when G star has some structure. And what do I mean by structure will become evident as I go on in the talk. Quite often it will mean it has a low-dimensional structure, a manifold structure, and so on. And the kind of questions that I would be interested in answering would be, okay, so in this model, the unknown parameters are the theta i's, of course, but estimating the theta i's consistently is never going to be possible. Consistently is never going to be possible because I just don't observe theta. There is so much of error here. But maybe trying to estimate G star is a reasonable goal, and that is what we are going to focus at. Can I estimate G star non-parametrically, which means without making any assumptions? And if we can do that, maybe we can hope to, as the Y i's have measurement errors, can we denoise them at least? Can we somehow estimate the theta i's? Although, as I said, it is strictly not possible to consistently estimate the theta i's. To consistently estimate theta i's. And of course, as we want to do it non-parametrically, we will be dealing with estimators that are going to be difficult to compute, and we are going to talk about that. Okay, so any questions about the setup and framework? Let me pause for a minute here and give you the opportunity to ask a question or two. Okay, so if there are no questions, then I can go forward. No questions, then I can go forward and I can show you my first figure, which actually was quite motivating and revealing. So, this is my data set. I have a hundred thousand stars. I have, this is the color magnitude diagram. So, in the color magnitude diagram, in the x-axis, you have the color of a star, which is basically obtained from spectrographic measurements about individual stars. And I have the brightness in some scale. And this is the observed data. Data okay, from a Gaia satellite study, and you see there is a lot of scatter, there is a lot of measurement error. And if you do the empirical-based strategy that I am going to describe, you do see that you see a lot of structure here. You see a lot of tightening around this part. You see the main sequence stars are also much more structured than it is here. And why I find this project very exciting is that this is ubiquitous in any. Exciting is that this is ubiquitous in any astronomy data set. And all we can try to do is to have any such data set, denoise it, and then hopefully give it to the astronomer to do any downstream analysis that he or she might be interested in. As you can see here, the hope is the amount of measurement error is much less compared to this data set or this raw data. And the focus of the talk would be: how do I go from this picture to this picture? To this picture, okay, okay. Any questions? So, here is another astronomy data set. This has actually 30,000 data points from another data set, which was a collaboration that I was involved in. And I have tried to indicate some of the measurement errors in each of the stars. Here, there is more measurement error along the y-axis. By the way, this is chemical abundance of stars. So, basically, you observe a star. So basically, you observe a star, you try to find out the chemical abundance in terms of magnesium, iron. There are about 19 such chemical abundances that you take for every star. And here I've just plotted the two dimensions, the two most important dimensions that the astronomers are interested in. And they want to denoise it. And here is some plot that you can get after denoising. And this is okay, but there are still some artifacts that I'm not completely happy with. And I must say here that. It and I must say here that this is very much an ongoing work. I would be, I really, this is in some sense a start of the work. So I'm really not sure about some of the algorithmic issues that we are facing. And any advice or any help would be greatly appreciated. Anyway, so that's the denoise versions we get. Of course, I have to tell you how am I doing this? And that's what the talk is about. So here's the rough outline of the talk. So here's the rough outline of the talk. I'm going to develop a non-parametric maximum likelihood estimation strategy for this problem. I'm going to describe it in detail. And I'm going to describe some theoretical results, statistical results that justify the use of this non-parametric maximum likelihood estimator. And then I'm going to talk about this empirical base estimation and denoising part. I'm going to give again some statistical guarantees. And lastly, I'm going to address the hard problem of directly comparing G star, the distribution. Comparing G star, the distribution I wanted to learn, with G hat, the distribution I can learn from the data. And as I said, this is joint work with Jake and Aditya. And then, and the next part of the talk would be about computing the MLE, where we develop a semi-smooth Newton-based augmented Lagrangian method. And this is joint work with Yang Jing and Ying. Okay, so let's get started. So, as I said, this is the model. I observe the Y i's, theta i's are actually. The yi's, theta i's are actually unknown. zi's are normally distributed errors, but they have a known sigma i as the variance. I have assumed that the theta i is a drawn iid from a population G star, and my interest, as I said, is in understanding the distribution g star. So how do I estimate this? So this has a very long history in statistics. Kiefer and Wolfowitz in 1956 studied this problem in a slightly simpler setting, and they basically said, Setting, and they basically said they made this crucial observation that, okay, this is okay, but if you look at the YIs marginally, the YI's are actually independent marginally, and marginally, they have a convolution distribution. So you're convolving a Gaussian distribution with G star. That's what the distribution of Yi is marginally. And here, G star is involved in Yi. And you can imagine, okay, so. And you can imagine, okay, so my data is YI, YIs are not IID because they're not IID because the sigma is different for every observation, but it's not a big deal. You still have this as your distribution of your data points, and you can just try to do a maximum likelihood in this context. So maximum likelihood, as most of you may know, is basically what theoretical statisticians and mostly statisticians in general employ in any problem they find, which is their right. Problem they find, which is they write down the joint density of your data, take the logarithmic fit, and try to maximize the sum of it. Okay, so this is the maximum likelihood estimator. Now, what is interesting is this is a non-parametric maximum likelihood estimator. What do I mean by that? As you can see here, G, all I'm using in G is that G is some distribution in Rd. And I want to maximize over all possible distributions in Rd. In RTD. That seems to be a daunting task, but this is a sort of what Kiefer and Wolfowitz proposed, and they showed actually this strategy can lead to consistent estimators. And this has been studied quite a lot in the 80s and 90s by Bruce Lindsay and some others. There are books written on this. And recently, there has been some more renewed interest, and that's why I'm also involved in some of these projects. So, let's talk about. So let's talk about the computation first, briefly. So, this is the problem that I want to solve. This is actually a convex optimization problem. Note that the constraint set is convex and the objective is actually concave. You're maximizing it. So, this is actually a convex optimization problem. Can I study it theoretically? Yes, indeed, I can. You can show that the fitted values of the densities at the point yi are actually unique. actually unique although g hat we still don't know whether g hat is actually g hat need not be unique but you can always take as a solution g hat to be a solution that has at most n atoms so it's a discrete distribution with at max n atoms and quite often what you see is the number of atoms in g hat is much much smaller than n and that will become crucial while developing Become crucial while developing our estimation, our optimization strategy. For D equal to one and homoscedastic errors, when sigma is all the same, it was known that actually G hat is also unique. But we have been able to show that actually this uniqueness need not hold beyond the greater than one. But you can always take as a solution a g hat which has at max n atoms. Now, I would again like to highlight the fact that here I'm maximizing supposedly over all distributions. Now, this is important, and this is what leads to a convex optimization problem, because if we know, if we say, for example, constrain G to have, say, at max k atoms, then I'm looking at the clustering problem, which we know, like fitting a k component Gaussian mixture, which we know is non-convex in nature, and will involve tuning parameters. Whereas this is completely unconstitutional. Whereas this is completely unconstrained, and the fact that it's unconstrained, there are no constraints, apparently, except the fact that G is just a distribution, leads to a convex optimization problem. So now, if you think about this harder again, although it's a convex optimization problem, it's an infinite-dimensional convex optimization problem. So, to implement, there has been many, many strategies proposed. One of the earliest was by Nan Lair, the inventor of the EM algorithm, and she used the EM And she used the EM algorithm to solve this problem. There have been many, many different ideas coming from the machine learning literature, the exemplar method, Frank-Wolf type methods have been proposed and studied for this particular problem. But recently, there seems to be one method that is quite popular, and it's popular because it's quite easily implementable, which is this direct discretization, right? Which is this direct discretization, right? So you have to optimize an objective over all distributions. Now, of course, all distributions is too large a space. Why don't I look at a discrete distribution with m atoms, and I fix the m atoms to be theta 1 to theta m for a large m. And then the discrete analog, if I fix that, then the discrete analog is exactly this for this problem. And then I can try to maximize over now, I have fixed the atoms of the distribution. I can only play around with the probabilities the distribution. Play around with the probabilities the distribution puts on these atoms that's the x1 through xm. So these are probabilities that sum up to one, of course, probabilities mean they're non-negative, and you can try to optimize this. And that would be an approximation to the infinite dimensional problem I'm originally interested in. Okay. Any questions? Let me pause here for a minute and see if there are any questions. So so can I so can I understand the Can I understand the discrete, the direct discretization as you're essentially putting in an empirical approximation to the F's and then you're approximating the empirical approximation? So the F has this structure, which is a convolution structure. I'm not approximating F directly. I'm approximating G, right? So G was the class of all distributions. Now the class of all distributions is too large a space. So I'm saying let's Space. So I'm saying let's look at distributions that put mass at the points theta one to theta m. And you can imagine if theta one to theta m is a very fine grid in, say, r to the power d, then you would get a natural discretization of this infinite dimensional problem. Does that make sense? Yeah. Thank you. Okay. Thank you. Thank you for your question. Any other questions or concerns or comments? Concerns or comments? Okay, so let's start with some pictures. So, here, this is a simulation study. I have assumed that all the sigma's are basically the identity matrix. G star, I have assumed, is a discrete distribution that puts atoms at four points. So, your data is really a finite mixture, Gaussian mixture model with four Gaussian mixtures, and that's why you see the four modes here. That's your data distribution. That's your data distribution. Your YIs are now actually IID because the errors are the sigma's are the same. And this is what, if you do the MLE, the non-parametric MLE strategy that I just described, using a sort of a fine gridding approach, you get at something like this, which is almost indistinguishable from f of g's. So f of g star is the marginal distribution of the yi's. F of g hat is kind of the estimated distribution. That is kind of the estimated distribution of the YI's, and they're almost indistinguishable. As we will show, indeed, statistically, they are also almost indistinguishable. So, here is another plot. Here, G star is basically uniformly distributed on a circle. So, G star is on the circle. And then for every point in G star, I have a Gaussian error that leads to this strange-looking density. So, that's f of G star. G star and this is the estimated f of g hat. So here, as I said, g star has a structure. It's basically putting, although it's a distribution in R2, it is putting all its mass on the circle of radius 3. Here is another picture where G star has structure. It now puts mass on two concentric circles of radius 3 and 6, respectively. And this is the observed, this is the density of the observed. Observed, this is the density of the observed data, and this is what is the fitted density. I have used 10 to the power 4, and you can see the estimation is not as good as it was before, because it's a more complex problem, as you can understand. So, hopefully, these figures can motivate that indeed I can think about. So, the distribution of yi was f g star sigma i, so it's the convolution of g star with a normal distribution. Of g star with a normal distribution with mean zero and covariance matrix sigma i. And its estimate, its natural estimate, is f g hat sigma i. And you can ask, okay, how accurate is this estimate? Right now, so in statistics, that's what a lot of us do. We try to this is a density estimation problem. We want to estimate how good the density estimation procedure is. The Hellinger loss is a standard thing to use. Off is a standard thing to use, but we have to be slightly bit careful. We have to realize that your data is not IID per se, because as I changes, the distribution of YI changes. So, really, there are no one density, there is no one true density. As I varies, the true density also changes. So, really, the appropriate metric is what is called the average Hellinger metric. So, this is the Hellinger distance of the true density of Yi with its estimated density, and then I average over all the n-many observations. Average over all the n-man observations I have. And as a statistician, you may want to know how small is this loss or the expected value of the loss, which is the risk. And what we realize is that in this metric, this is a very good estimator of the marginal density of the YIs. Okay, so our proof is very much inspired by a recent work by Aditya and his student. His student, and which is in turn inspired by a seminal work by Chun Hui Zang in 2009. So, here is our main result. So, to simplify our result, let me just state it. What happens when G star is just a compactly supported distribution? So, it's supported on a compact set S. I have to assume something about the sigma i's. The sigma i's can change with every observation, but I essentially assume that it's a maximum and minimum. Assume that its maximum and minimum eigenvalues are kind of bounded. That's the A lower bar and the A upper bar. And then you see that the average Hellinger loss, the expected value of it, the risk is in fact diminishing as the term 1 on n, barring some log factors, which is somewhat remarkable in some sense. We are doing density estimation, non-parametric density estimation. Still, we are getting almost parametric rates. Almost parametric rates barring the log factors. Now, this is indeed remarkable at first sight, but we are not the first people to observe it. It's known that Gaussian mixtures are very smooth, and when you want to estimate a smooth density, a very smooth density that infinitely times differentiable, you can get almost parametric rates, and that is what we have here. But what is so interesting is that you do have a method which is completely tuning-free here. There is no tuning. Completely tuning free here. There is no tuning to do. Of course, there is a discretization of the infinite-dimensional convex optimization problem that you are doing. But in terms of tuning parameters in the system, there is really nothing at all. This estimator does not use the knowledge that it's compactly supported, nor does it use the knowledge that the sigma is are somewhat regular. And it gives you this almost parametric rate. And you can show that this rate is actually. This rate is actually mini-max optimum. You can ask the question: okay, so it performs well when G star is compactly supported. What happens if G star is a discrete distribution? So in clustering, you might imagine G star is a distribution with K atoms, which means you really have a K component Gaussian mixture model. And in this case, my G hat, which does not use the knowledge of K star, which does not use the use of the use of the use Does not use the knowledge of k star, which does not know the knowledge of k star, shows again almost parametric rates of convergence. And that is again quite remarkable, as I said again, because here I'm not using the fact that it's known that G star has K star atoms, but somehow the NPMLE adapts and gives you results or rates that behave as if you know the true value of K star. Okay, so again, you can show that. So again, you can show that the mini mini max lower bounds are there. There cannot be any estimator which has better risk than K on N. And here we have K star over N. The only price we are probably paying is in additional log factors. Okay, so this again tries to show some of the adaptive properties and how the MLE G hat behaves particularly well when GG starts. When g star has some structure. Here, the structure was that it's a discrete distribution with k atoms. So now let's move on to denoising. So let's try to think about the problem. Okay, so good. So we know that G star could be estimated well if you think of it as estimating the marginal distribution of the y i's. But that was not the real goal. The goal was to denoise my observations. I have assumed that the theta. I have assumed that the theta i's are iidg star, but and I'm interested in the theta i's. Now, if if how would a Bayesian solve this problem? The Bayesian would say, okay, you have a prior g star on the theta i's. By the Bayes theorem, I can define this oracle posterior means, which is the mean of theta i given on my observations, that has the simple expression, and these should be reasonably. And these should be reasonable, quote-unquote, estimators of the theta i's. That's what a Bayesian would do in this problem, actually. But you have to realize that G star is unknown. And empirical base, which is a concept that again goes back to Herbert Robbins in the 1950s, is being there's renewed interest by a lot of statisticians. Bradley Efren at Stanford has worked a lot on this recently, is because Recently, is because what a frequentist can do is an empirical-based strategy, which is, okay, I don't know the prior, I can estimate the prior. And that is precisely what we did. G star was the prior. The NPMLE allowed us a way of estimating G star by G hat. And once I have an estimate of G star, I can estimate the oracle posterior mean by the empirical estimator of theta i stars, which is this. And this is exactly the picture. And this is exactly the pictures that I had showed you in my plot. These are the Denoise versions. And this strategy is laid out in great detail in this, again, seminal paper by Jiang and Zhang in 2009, who call it the general maximum likelihood empirical base estimator. Now, of course, many people would like to use a different estimator for G star, and you can do that, but here we are going to confine ourselves to the non-parametric maximum likelihood estimator of G star. Estimator of the star. And you can see that this again estimator is tuning free, no tuning involved, and it provides excellent shrinkage. So here are some plots in one dimension. So in one dimension, these are my data points. Now, these data points are generated from where G star has just five is a discrete distribution with five components. So your observations are coming from a five component Gaussian mixture model. One has mean around minus five, one has Around minus five, one has been around minus two, the other one at zero, the other one at three, and the other one at six. And all the sigma's have been assumed to be one here. So this is your data. Of course, there is no way of recovering the G star very easily, but you can hope to recover the oracle-based estimators. The oracle-based estimators assume the knowledge of G star, and these are the blue points that you see. And as you can see, that they. And as you can see, that they provide shrinkage and they are very close to the true value of G star that you can hope to try to attain, but you can never really attain these values, minus 5, minus 2, 0, 3, 6. And what is remarkable is this empirical base estimator, which does not use G star, uses MLE strategy, is essentially exactly the same. So compare the red points with the blue points, and you see that they are very similar. And you see that they are very similar. So, with this data, you get enough shrinkage, and these could be taken off as the denoised versions of the observations you have had here. Similarly, you can do it again for a different model. Here, G star is itself a mixture of Gaussians. And here again, there's remarkable similarities with the oracle base estimators, that's in blue, with the empirical base estimators. With the empirical base estimators that use the MLE G hat. So here are some pictures from two dimensions. So in two dimensions, G hat, now this is an example where you had two concentric circles and the true signal, the true theta i's are all on these two concentric circles, right? And you have corruption by noise, so that's the raw data you observe. And if you do And if you do Oracle Bays, which assumes the knowledge of G star, then you get denoised estimates that are actually very close to the true signal itself, but that of course crucially uses what G star is. You can never compute it in practice. And as an empirical-based strategy, a frequented strategy that uses the data to estimate the prior, that's what we are doing here, we do get shrinkage and we. Get shrinkage, and we do recover the two concentric circles quite well given this data set. The same phenomena is observed for different structures. Here's 8, so the G star, all the theta i's are lying on this 8. This is the data you observe. And you can say this is complete garbage, but the reason you can actually do it is because the sigma is unknown. You know that each of the data has been corrupted. Each of the data has been corrupted by Gaussians with identity matrix, and that helps you recover the true underlying structure. And that's how the oracle-based estimator looks like. And the empirical Bayes estimator actually very much mimics the Oracle Bay estimator. Here's how you can denoise the structure thiam. So this is your true signal is uniformly spread on. Signal is uniformly spread on these four letters. That's what you observe. If you do oracle base denoising, which you cannot compute, that's pretty good. And with the empirical base where you're estimating your G star, that's what you get. So this to me looks quite promising. And you may ask, okay, can I quantify the accuracy of my empirical base estimators, theta i hats? So theta i hats are Hats are what I was plotting. And really, the goal that I want to achieve is the oracle posterior mean. That's the closest I can get. And you may ask, how close is theta i hat to theta i star? Like, so in the figures, how close is this are these data points to this, sorry about that, to this sets of points, theta i stars. And here is a first result, which again assumes some structure. If G star is compactly supported on a compact set S in Rd, you can again show that in mean squared error, your theta i hats are actually damn good estimates of these theta i stars. In fact, the estimation accuracy again shrinks at an almost parametric rate one on n up to some log factors. And you must realize that there is no And you must realize that there is no knowledge about the set S that is being used to construct this estimator. It's a tuning-free procedure. You can say what happens if your underlying G star was discrete with K star atoms. A similar phenomena you observe, even theta i star, theta i hats are accurately estimate theta i stars, and you get an almost parametric rate k star on n. On n. And as I said before, the NPMLE does not use K star, it's in some sense maximizing overall distributions and it kind of realizing that G star is discretely supported. And it's giving you a rate that is almost optimal if you knew K star. So we must say that this has, of course, implications in clustering and so on. There are various ways of doing convex clustering, but I don't think. Clustering, but I don't think such strong guarantees can be provided for the denoised estimates using convex clustering methods. And our proof techniques are, again, very similar to that of this paper by Shaha and Kuntuboina. Okay, so now any questions? Let me pause for again a brief minute and see if there are any questions or concerns. Hi, Bodhi. Could you say a little bit about the size of the constant in front there? In particular, how it depends on D, the C. How does C depend on D? Yes, that's a very good question. We do not have explicit constants, but it depends very badly on D. It's probably exponential in D. So all the analysis we are doing here, you are very right to point out that this is. To point out that this is, in some sense, you have to think about as D fixed and n growing. So, yes, I am hiding the fact that it does, that I'm not really trying to advocate this for really high dimensional X distributions or when D is really large. I don't know. Really, I don't really know how to even compute it when D is very large. And all my simulations are in one and two dimensions, although I hope to address a few problems in a. Although I hope to address a few problems in astronomy, which I'm working with right now, which have close to 20 dimensions. But I really don't know how to do it and how to even discretize the infinite dimensional complex problem. So thank you. And thank you, Johannes. We are going to get to some of those computational questions in a minute. But I want to just briefly point to this deconvolution section. So you see, so far, I've been trying to say that G hat is a good estimate of G star, but I'm not directly comparing G hat with G star. G hat with G star. In the first part of the talk, I was comparing the densities, the marginal densities of the Y i's, which involved G star, with the estimated density of the Y i's, which also involved which involve G hat. In the second part of the talk, I was comparing the theta i's with the theta i hats, which involve the g star and the g hats, but we were not comparing them directly. You can ask, okay, why don't this is a distribution and this is your estimated distribution, why don't you compare them directly? Distribution, why don't you compare them directly? This is what is called the deconvolution problem in statistics. And it's known it's a hard problem in statistics. And how would you like to compare them? Realize that G star could be a discrete distribution, could have structures, a low-dimensional structure, whereas G hat is always can be taken as a discrete distribution. So under what metric do you want to compare them? It's very natural to compare them using the Washerstein distance because Using the Wascherstein distance, because this again is very popular and it has significance for estimating mixtures of Gaussians particularly. And it can handle the fact that you are comparing a discrete distribution with a continuous distribution. So what is the Washerstein distance? I guess there have been many talks in the last couple of days about Washerstein distance. I just won't want to say much except that you really want to minimize the distance between two distributions. The distance between two distributions u and v, and you're minimizing over all joint distributions with marginals as u and v, and that's the Washerstein 2 distance. Now, if you do that, of course, we don't really have a lot of tools to handle the or to control the Washerstein distance between G hat and G star. Long Iguen, in a sort of a very nice paper, In a very nice paper, connected this deconvolution error in terms of the Washerstein distance with respect to the density estimation errors of the mixture. So, this is the quantity we tried to control in the first part of the talk. And this paper gives us a link. If you can control this, maybe you can control this as well. With that, we can show that indeed we can control the Wascherstein distance between G star and G hat, and you get a logarithmic rate. So, this is, of course, a slow rate. So, this is of course a slow rate, but this is not surprising. Again, we know that deconvolution is a hard problem. And for any arbitrary g star, there is no way of getting an estimator that is faster, that has faster rate than one on log n. But the important question to me is, okay, I don't want a result which is such a worst case result. What happens if G star is structured? What happens if G star is a discrete distribution? Is a discrete distribution with k atoms. What happens if it's lying in a one-dimensional manifold in a 20-dimensional space? So, of course, we don't have all the answers yet, but this is, as I said, this is joint work and this is very much ongoing work. So, for the simplest case, when you assume that G star just puts mass at one point, if you assume that, you can show that this estimated g n hat, what you get, achieves a pattern. You get achieves a parametric rate now. It is basically n to the power minus one-fourth. And that is all we have been able to prove so far. But this hints at this adaptive property of the MLE, and this is well known in other shape constraint problems wherever you use the MLE or the non-parametric MLE, that depending on the underlying structure of the data, the rates of your MLE could be different. And here is a classic example. The worst case rate is very slow, but it Worst case rate is very slow, but it can indeed perform very well when G star is a Dirac measure. That gives me hope that when G star is actually a discrete distribution, you may get basically n to the power minus one fourth also. So let me pause here for a minute and see if there are any more questions. And let me see how I am doing with time. I don't have much time. I'm doing with time. I don't have much time, but I'm going to talk about the computation now. But are there any questions? Okay, so let's now dive into computation. So as I said, what we really want to compute is an infinite dimensional convex program. The most natural strategy, which is very popular nowadays, is direct discretization, which you fix a grid, theta 1 to theta m for m large, and you solve the discrete analog, where you are just The discrete analog where you are just maximizing over the probabilities xj's. Now, to write it succinctly, this is the optimization problem you are trying to solve where this L matrix, this N cross M matrix, so this is the Gaussian density with variance covariance matrix sigma i at the point yi minus theta j. Note that yi's are my observations, they are known. Theta j's are my fixed grid, so they are also known. So there is nothing to optimize in the L matrix. nothing to optimize in the L matrix. It's a given matrix which is large, which is large dimensional when m and n are large, but there is nothing to be optimized here. So that's the problem I have at hand. And Coenker and Misera essentially demonstrated that interior point methods are quite good in solving this problem. And they are much more reliable than the EM algorithm, which converges very slowly. So the internet point methods used using MOSEC and Using MOSEC and so on are good, but they don't scale very well. So they get stuck at 10 to the power 4 essentially, and M being 10 to the power 3. So the question you ask, so just before I go there, so now this is the picture I started my talk with. So this is the raw data and these are my empirical base estimates. How did I get this? So of course I got this by computing g hat first. How did I compute? G hat first. How did I compute g hat? I took an initial grid of points that these are the theta j's, 100 cross 100 grid of points. And this is my g hat, which puts point mass at some of these grid points. And as you can see, it just puts point mass at a relatively fewer number of points than the initial grid. So this is all the theta j's that have positive x j's, and with these. X js and with these g star I computed the empirical base estimators that gave rise to this. Actually, I must also confess that there was some binning involved here because here this sample size was 10 to the power 500,000, but I don't want to go to that at this point of time. So let's talk about the discretization and the optimization problem. So the constraint is you have a probability vector, so they have to add up to one and it's non-negative. And it's non-negative. You can, using a simple Lagrangian technique, bring it in the objective function, and you can know that the Lagrange multiplier has to be one. You can show that very easily. So what this recent paper by Matthew Stevens and his collaborators at UChicago did was they studied this problem and they proposed, as we will see, a sequential quadratic program to solve this. But we take the dual route. So we look at the dual problem to this problem. Look at the dual problem to this problem. And we use an augmented Lagrangian strategy. So, this is the dual problem. You can write down the augmented Lagrangian to this problem. And we want to use a semi-smooth Newton method. So, we would like to keep this projection onto the non-negative octane. And you will see that it helps us quite a bit because it gives you the sparsity that you have already seen in this variable. In this variable, you know that most of the xj's would be zeros, and so you would have a enormous augmented Lagrangian will be solved iteratively. The primal variables are now easy to solve. It's only the dual is not that easy. But of course, u and v are very related. Fixing v, u is very easy to compute using the easy to compute using the proximal mapping that's what we do here and to do to to to optimize over v we have we use a semi-smooth newton method and i don't want to go into the details a lot but uh to to solve the semi-smooth newton i would have to compute the gradient and the and the hesitan of this problem but by dunskin's theorem we can actually Dunskin's theorem, we can actually characterize the gradient and the Hessian quite easily. So this is an n by n Hessian matrix, but what happens is that you have in this large matrix L, L is n cross m, you have sandwiched a sort of a diagonal matrix with entries 0 and 1. And as the diagonal matrix is sparse, very sparse, you really have to deal with only a few columns of this L matrix. matrix and that reduces optimize the the complexity uh burden in in in using the hessian considerably so i just want to compare it with the recent key metal paper where they look at the primal problem and they used a sequential quadratic program to solve it but they are computing the Haitian was order nm squared whereas we can leverage on the sparsity of the solution The sparsity of the solution that we know is going to happen, and we can compute the Hessian faster in this particular case. So, let me just end with a summary and some questions. So, what have we done? We have studied the non-parametric maximum likelihood estimate in a Gaussian mixture problem. It's a non-parametric estimator. It involves no tuning parameters. We see that it has very good Hellinger accuracy. It can be used. Accuracy. It can be used to denoise the theta, the Y i's, to estimate the theta i's. We realize also that in the Washer style loss, this NPMLE exhibits adaptive rates, and we have developed a semi-smooth Newton-based augmented Lagrangian method to solve it. We haven't really implemented a lot because this is very much ongoing work, but we plan to do so. I want to end with a few questions. I want to end with a few questions that I think I don't know, and maybe some of you may know the answers to this. So the first principle of computation that we have used is a naive gridding. And I can see the gridding being a decent strategy when you have dimensions two or three and so on. But when D is large, how do we go beyond naive gridding? Can we have methods that go beyond? Can we have methods that go beyond this, which really are provably a good approximation to the infinite-dimensional problem that we really wanted to solve? Now, the NPMME is really a convex problem. And the reason why it's convex is because you are really fitting n components, an n-component Gaussian mixture to your data, to your n data points. And that's what gives you a convex problem. But it's an infinite-dimensional convex problem. We know that k complex. We know that K-component Gaussian mixtures are actually NP-hard. So the original problem, which if you don't discretize, is it an NP-hard problem? Now, how do we scale beyond a million observations? Now, this astronomy data sets are becoming really, really big. If we want to make an impact in astronomy, we have to be able to scale these methods to more than a million observations. And our current approach to And our current approach to doing that is to bin some of the observations and reduce the sample size drastically at the first step. Is there a principled way of doing such binning that can handle heteroscedastic errors? So these are some questions that we still don't know an answer to, and I would be happy to hear your thoughts and comments. Thank you very much. Thank you. Thank you for your time. Thank you, Bodhi. That was an excellent talk. That was an excellent talk. Really enjoyed that. I think some of the questions that you're bringing up is really to the very right audience here, because I think many here will be very, you know, certainly expertly prepared to address some of this issue regarding substituting gridding with some more efficient techniques. And so I'll open up. And so I'll open up to the audience to see if any comments, questions, suggestions. Thank you, Johannes. I have a few questions, but I don't want to take up all the time. Can I ask one and then I'll wait? Yeah, thanks. That was very interesting talk. So can I just check if I understand the main problem correctly? Understand the main problem correctly. So you have a deconvolution problem for the distribution G, right? And then because your additive noise is Gaussian, the phi sub sigma i's are Gaussian, right? Yes. So I can also see this as like a kernel density estimation problem where the loss is like the MLE loss, right? Yes. Right. Yes, so you can. So, indeed, if the sigma i's were all the same, this is very similar to a kernel density estimator. But what does a kernel density estimator do? It puts one on n mass at all the data points. Yeah, yeah. We, of course, don't want to do that, and we want to optimize that. So, in that sense, there is a difference, yes. But okay, so then my I guess my first question. I guess my first question would be: Have you, or is this interesting in the field to think about cases where the sigma i's are not known? So maybe the additive noise itself is unknown, and then you will have hyperparameters there. Basically, the phi's have to be parametrized as well, so it becomes like I guess in imaging you call it blind deconvolution, right? Where the convolution. Where the convolution kernel is unknown as well? Yes, so that's a very good question. So, using the maximum likelihood strategy would fail in that case, as you can imagine, because if sigma is unknown, you cannot do it. But you are very right. In many applications, that is of primary importance. You do not know sigma. And in fact, some of the really remarkable success of, if you like, the figures I wanted to show, they They quite crucially hinge on the fact that I know sigma. So, the reason I like this problem and this approach is that this is ubiquitous in astronomy. Every astronomy data set has known measurement errors for every single observation they take. So, for astronomy, this is, I think, a very, very standard application. But yes, you are indeed right. For other applications, handling sigma, even homoscedastic but unknown, would be a big step. But unknown would be a big step forward. And it's not clear to me if some of the, if the end, if the non-parametric maximum likelihood approach can be immediately used, because you cannot really maximize the likelihood in that case, right? Yeah, I mean, I was thinking more like, you know, there are many methods now for tuning hyperparameters in, you know, in statistics and machine learning, people are doing it all the time as well. I was just curious. As well. I was just curious, like, you can just throw those methods at the problem and see if they work well. So that's a very good suggestion. But again, this is my ignorance. I probably don't know some of this work. So if you may want to share some of these papers with me, I can try to have a look. Sure. Yes. Indeed, I guess this is just my ignorance. I don't know the other literature so well. Okay. I'll wait. I have more questions, but I'll wait. I'll wait. I have more questions, but I'll wait. Can I have a question? Please. Okay, great. Yeah, so again, also thanks for the excellent talk. I was wondering if, I mean, you mentioned you identified already higher D being an interesting direction here. And probably my naive understanding is that there are some other ideas needed than gridding the thetas and estimating. The thetas and estimating the density. So, in your area, I mean, it's been the null space of my reading, but are people using, for example, transport-based methods there or Karman filter techniques to tackle those high-dimensional problems? So, I must say, people have not really studied it so much. So, as you can see, so this is an old problem, at least the literature. Problem, at least the literature that I'm more familiar with, it came up in the 1950s. But then, most of the time, till the last 10 years, people were confined in the one-dimensional scenario. And this has a lot of connections to multiple testing, multiple hypothesis testing. And that's what empirical base has been recently used by statisticians a lot. And they have not really explored this problem, at least using the non-parametric maximum. Least using the non-parametric maximum likelihood estimate, even forget the non-parametric maximum likelihood estimate, using any kernel estimators for G-hat and then using it for large dimensions. The particular applications I am interested in, as I said, some of the plots are from a joint paper in astronomy. So, there, my student was working with a 19-dimensional space. This chemical abundance is very 19-dimensional. Dimensions and what I would like to do first of all is to go from two dimensions to 10 dimensions. And we haven't gone that far yet. Because if I want, because we are still using off-the-shelf solvers like MOSIC and so on, if you have, as I said, say 50,000 data points, it doesn't really work that well. So we have to first develop algorithms that work at this large scale. At this large scale. And I guess even the most recent paper that I tried to cite, the Ski Metal paper that talks about, they talk about n being a million, but they are restricted to dimension one. And in dimension one, there is a nice result that shows that in dimension one, if you just have square root of n points in your grid, then you can essentially lose much in terms of the statistical accuracy of this estimator. Accuracy of this estimate. So that is, I guess, one of the reasons why these authors focused on the case n is equal to 10 to the power 6 and m is equal to just 1,000. But going beyond that, I don't think there has been much work, really. Sorry, but that's the literature I am familiar with. Yeah. No, I think that that's really interesting. So, but basically, then in higher Basically, then in higher dimensions, would people consider variational inference, for example, as a substitute for non-parametric methods? Or is that information? No, that is very much the case. Yes. So I think this is also a problem with the statistics field that somehow, even in the field of statistics, the applied statisticians don't talk as much with the theoretical statisticians, and that is a problem, I have to admit. So I think there, if really, so I view it from because I'm interested in astrostatistics and I have this really interesting data sets at hand, I want to analyze it. And although I have a more theoretical bent, I would like to see if MLE works well in these. So that's how I came to this field. But I am very interested in seeing how other methods would behave. And I have, as I said, examples. As I said, examples with sample sizes varying from 30,000 to a million with dimensions ranging from 2 to 20. Okay, cool. Thank you so much. Time for one more quick question. Anybody? I can ask my second question if Luzon wants to go. You go ahead. Who wants to go? You go ahead. You go ahead. No, no, you go ahead. It's fine. Okay, sorry. So, mine is a very quick one, maybe. So, so, Bodhi, in the example with the grid for your astronomy data set, I guess one other thing that, yeah, so something that one would do is just why not subsample the raw data set and just put the thetas there? And just put the thetas there because it seems like you have a lot of grid points that are not used at all, right? Yes. So I should have said this. So, this is the strategy of Lashkari and Goland. I think that is a NEEPS paper in 2009. They call it the exemplar method, where they essentially use the data points as the theta j's. And in fact, we can certainly do that. And that is, to me, the only idea I have. The only idea I have of how to scale it as you go up the dimensions. You are very right, but I guess from a purely theoretical perspective, I don't know if any guarantees can be given in that sort of because you are kind of double-dipping in your data to fix your atoms. And I'm not sure if maybe that is the only reasonable thing to do in large dimensions, but it In large dimensions, but it has this unpleasant quantity that it's not guaranteed to work. So, whereas here we know if the initial grid gets dense, we are going to come closer and closer to solving the infinite dimensional problem. And in these cases, we have also tried things like if you have a region where there are no data points, and then there is no point in having a grid point, we would kind of throw away that grid point and so on. Yes. And so on, yes, thank you. That's a good point. Yeah, I'm on that. I'm wondering. This reminds me of when we do optimization, like sample average. Like, I mean, in some sense, you do random, but you can also do grid something. It's not the same problem, but like something like Lattin hypercube will scale a lot better so that like you, you know, you do your grid, but only put the points in, you know, with this scrambled way of doing Latin hypercube, that will reduce and scale much better. That will reduce and scale much better. And then you can use the current data points, I guess, together with this, which would scale better. So just to ask you, so when you said Latin hypercubes, do you mean quasi-Monte Carlo sequences? Not quasi-Monte Carlo, but Latin hypercube will take a grid, right? Instead of randomly sampling. Yes, yes. So we'll take a grid and then think about putting it across the diagonal or something, and then it will. The diagonal, or something, and then it will, like, how to say on each grid point, it will sample one from each row and then one from each cross. I see, yes, yes, okay, okay, okay, okay. So that will like reduce this problem when you go to high dimension, but still sort of capture. And then there are well-known results in terms of as you increase, right? The properties have been studied. So I wonder it could also help with the theoretical. But you would also get, you need to have like, yeah, as the number of. Uh, you need to have, like, yeah, as the number of grid points increase, something like that. Um, so that's no, that is something I did not know about. So, uh, I will try to do a search also. And if you have some references that you think are appropriate, please let me know. So, Lars has made this remark in the chat window where he says basically whether people are using optimal transport-based ideas, and he tries to say that. And he tries to say that it actually works quite well for dimensions up to 100. So, no, I don't know about using optimal transport in this particular. So, I would certainly would like to see. And he has already forwarded an archive link, which I'll try to see. Yeah, we can chat more about this. I'm just wondering. I mean, this problem that you have raises a lot of memories working on these ideas. Working on these ideas. And I'm happy to chat anytime. Thank you. Thank you.