For inviting me here, and uh, yeah, um, can everyone hear me and see my screen? Yes, yes, all good. Okay, great, thank you. So, yeah, today I'm going to share some our recent work on Graph ODE with everyone here. So, the general motivation is like: if we look at our work, the entire world, right? So, lots of systems actually they Word, right? So, lots of systems actually can be considered as dynamic system from like physics system. For example, this one actually is like our solar system, and this is a trajectory of the planets. And I think it also involved the war of planets. So, we can see that the different form is embodied dynamic system. Now, we also can see some other system, like including epidemic system, like social dynamic system. Social dynamic system, this traffic transportation system, or like a brain network, right? So, all of them can be considered as some sort of dynamic systems. But then from graph perspective, those dynamic systems can be considered as dynamic graphs. So, similar to regular graph, we also have nodes and they also have the kind of interactions between them. But of course, like they also have this temporal information. Is temporal information associated with nodes. And sometimes they're also associated with edges. So that's the spatial part. They have the temporal information. So here, just a very simple example for the five-body spring system. So here, every node here can be considered as like object, and then they connect with each other using a spring, using this like specific graph. And then just assuming that probably like That probably like starting from some position, right? So we let them go, but then you can imagine that those objects will definitely be dragged by each other, and then they will have their trajectory. But then the questions like, can we based on those observed trajectory for those individual agents and then try to predict what will happen to them in the future? So then we can imagine. Right, so then we can imagine that this dynamic system, um, yeah, in many cases, uh, where we don't really know the underlying mechanism, so there's like a magic black box system, and that will turn kind of the past trajectory, including the interaction graph, into some future trajectory. So, this yt could be part of this xt because sometimes you only interest in part of those observations. In part of those observations. But then, what we try to do here is like whether we can utilize some model, right? So, based on the data that we have observed, including this graph, then we can predict the future. So, definitely, it can only be an approximation, but we wish that in this case, we can really uncover lots of black box, just based on the observed data. Now, let's try to take a look what's happening. What's happening for the existing literature, how people try to solve the problem, try to simulate dynamic system, and try to based on the observe the trajectory to predict what's happening in the future. So according to our understanding, right, so there are like roughly two types of approaches. The first approach relies heavily on the knowledge. Okay, so it's like the domain expert, for example, in the n-body problem in our solar system. In the n-body problem in our solar system, right? The people know that what will be the basic law behind the system, but then based on that, they can write down this ODE, and then based on the ODE, then they can predict the future. Of course, for the ODE, there might be some parameters that they need to fit based on observed data. But then, after that, then everything should be good. So, here are just a bunch of examples of ODEs that have been utilized in some of the systems. Of the systems, like this one is like in the ecological system, this one's like in the epidemic system, right? And by looking at this ODE, probably it looks very familiar to us because this simply says that the derivative of this the position of an ice object, right, so it depends on something that related to itself, but also depends on some sort of aggregation of its neighbors. Aggregation of its neighbors. So that's very similar to the method passing framework that we're using in the graph neural network. But anyhow, this is like the first type of approach, OD-driven approach, and it requires lots of kind of domain knowledge. And it's very difficult to write down those ODEs specifically. It's very complicated. So the second type of approach is more like data-driven, right? So it's more like I just treat those data as like, suppose we have end data. Data as like suppose we have n data points and we have t time stamps, then it's just a yeah, a regular n by t data, and we wish that try to utilize, for example, time series models. And more recently, of course, we try to utilize deep learning to do the spatial temperature data prediction. And more recently, right, so people also try to utilize graph neural network to capture the battery interactions, spatial interactions. The better interactions, vasual interaction between objects. So, now let's just have a little bit of a cap of graphic neural networks and their applications to modeling dynamical systems. So, I think probably I can skip these pages. Okay, so essentially, graph neural network right now is very popular, and many of the approaches actually based on message passing framework. And then, if you look at a specific like individual nodes, right? Individual nodes, right? So we have to like collect messages from your neighbors and also combine it with the nodes, the self-information, and then do some sort of transformation. So then when applying to this dynamic system, so typically it follows this kind of strategy. So first of all, we have a specific assumption that we want to do this one step ahead prediction. So it's like, suppose we have already have Suppose we have already have all the data points at timestamp t, then we wish that we can predict what will happen at timestamp t plus one. And so that's something that we already can do. So essentially for this part, right, so the modeling part will replace all the possible models with the graph neural network. And recently we have MRI and also GNS. Also, a GNS always is kind of a philosophy to solve the problem. But there are a couple of limitations of the current way of applying graph neural network to modeling the dynamic system. The first thing, of course, is like the assumption that we need to collect or assume that the data at timestamp t is available, either it's like observed or like predicted from previous. Previous calculation. And then we just do one step ahead of prediction. Okay, the second limitation is like we still assume that the data points are kind of discrete. Okay, so it's like the timestamps are very regular samples. We have time stamp one, two, three, four, until like timestamp capital T. So all the data are like very well organized, no missing data, everything is perfect. Data, everything is perfect. But somehow, if we look at the real-world data, right? So it's very tricky. Maybe you have a dynamic system, okay, due to some reason, you can only observe partial of the kind of observation of the object. And also, if you look at the timestamp, that when you can collect the observation, right? So they are not regular at all. It's just at a random. At a random time step, you probably observe this too. At a different time stamp, you observe a different object. Okay. And when you can observe them, it's completely irregular. So that's the kind of real world data, partially observed and irregularly sampled. Then how we can based on this kind of messy data and try to predict what's going on in the future? So in order to do that, we really need to utilize this kind of We really need to utilize this kind of messy data to model the underlying mechanisms. So that's why we try to utilize the so-called graph OBE to replace this ordinary graph neural network to do this job. So the hope is like instead of modeling this kind of discretized dynamics, we wanted to just model the continuous dynamics directly. Directly. And also, if we have that continuous dynamics, right, so then we can directly predict kind of long-term future because we can predict the entire function instead of just the one step ahead prediction. So inspired by the recent advances in neural ODE, which is our new design for the sequence data, we think right now it's time to move it from sequential data to this more general graph. A general dynamical graph data. So the general idea is like assuming we have a dynamic system and we can represent each agent in a latent space, and those objects have to follow some dynamics that can be described by this ODE in the latent space. So by using this formula, let's try to understand here. So for a specific object I, For a specific object I, right? So it's like derivative at times t, then can be encoded by a graph neural network. Okay, so in this graph neural network, of course, it has to collect messages from its neighbors, right? And then do some sort of aggregation and then do some sort of transformation. Okay, for this part, it's just a very simple graph neural network. And what kind of graph neural network can be utilized here? So I think it's still here so i think it's still um it's open question because it really depends on what kind of the what kind of properties that are required for the specific dynamic system okay so um but then just to come to uh our solution right so this is uh like our first attempt try to try to propose the first graph code e try to solve the dynamic system modeling problem okay so then uh our solution can be considered Can be considered the three steps. The first step is like based on the observed data, we try to encode it into the latent initial state, which is known as Z0. And once we have that, we can assume that we use a versional approach, right? So then we can sample a specific initial state and also assume that our graph ODE-based, yeah, graph neural network-based ODE is already there, right? Based OD is already there, right? So then we can utilize this derivative and then also utilize the initial state. Then we can utilize ODE solver to infer what will be the lateness state for node I at any time stamp T. So then the next step is like once we really can infer this Z T at any time stamp for any individual nodes, then what we need to do is like we just calculate. you do is like we just calculate this zt for the nodes at timestamp t well we do have the observations right so here then it doesn't matter whether the observation is regularly sampled or not whether it's partially observed or not it doesn't matter just try to tell us where we can get this observed value and then we'll do the decoding from this latent state so the loss can be considered as two parts the first Two parts. The first part is like we try to, it's just a reconstruction loss, right? So just that following this generative model, we can predict this observable, predict the position for those objects at some time t, and then we compare them with the observed ones. And then we wish that the difference is small. And the second part is generated due to this kind of variational encoder. Encoder. So that's why we have this KL divergence to make sure that, okay, the zero is coming from some nice distribution. So this is an overall framework. And the encoder actually is more complicated than we thought earlier because the encoder essentially will try to encode the initial state for the system and for each object. Actually, we need to have another graph neural network. Have another graph neural network that is designed for the temporal dynamical graph. And then in the end, let me directly move to here. Okay. So then this essentially is another graph neural network that tries to collect messages from all its neighbors. And then for each node here, actually, it's not an object, but vectorance of that object at some time step. And we're going to collect the message from. And we're going to collect the message from its surrounding neighbors. And then we do some attention mechanism. So then it will be represented with this weighted average of all the representations. So once we got a representation of the object at different timestamps, then we can build another like attention mechanism because we wanted to kind of squash the different observations for the same object into the For the same object into the into just the one initial state, okay. So we just apply another attention mechanism. So, so in the end, right, so we just have this big model, okay, for the encoder for the initial state is just the one graph neural network, and then we're going to kind of utilize OD solver to infer the observation of any object at any time and then decode it into. And then decode it into the values that have been observed. And then we can have a function, and then all the parameters can be updated. So then we come to the evaluation stage. We test our model into three data sets, motion data, spring system, and charged particles. So actually they are for these two, right? So their dynamics are known, but for the first one, it's a human body, actually, do not really know. Body actually do not really know its dynamics. And we test over two tasks. The first task is interpolation, meaning that we just try to train the test in the same time period. We wanted to fill in the missing data, try to predict the missing data. And the extrapolations like we want to train on some time period and then want to test on same time period. Okay, so then based on our results, we will see that. Results, we will see that our LGODE actually performs much better than the baselines. And the more interesting things like it performs even better if the observation is sparser, because we don't have to have that many data points. And also it works better in the extravelation task. It can predict the kind of future dynamics much better because it's more generalizable. Is more generalizable and also we show some case studies here, for example, for the five-body spring system, right? So the left-hand side is just a ground truth, and the right-hand side is like a prediction. And in this case, we still need like long observation time lengths to predict the future. Later, we will show that in more recent work, we actually can generize very well for extreme long future dynamics. And this is another case study on the motion capital. Another case study on the motion capture data. And the left-hand side, ground truth, and right-hand side is like prediction results. But then we realized that there are a couple of new challenges under this graph ODE framework. The first challenge, like in many cases, so our graph is not fixed. So in the previous problem, we assume that the motion data, right? So the connection between different joints that are fixed. Different joints that are fixed in the spring system, right? So, which kind of object and which object they are connected through a spring that is also fixed. So, the graph actually does not change. But in some cases, okay, for example, the traffic systems, right? So, you can imagine that the kind of edge properties actually change significantly. And also, we try to also consider this epidemic system. System, which is a COVID-19 spreading US into a dynamic into a graph, dynamical graph. So, in this graph, actually, we just treat each of those 50 states in US as a node, and then they are connected to each other. It's a complete graph, but for each edge, on each edge, we have different like a traffic flow. And this traffic flow actually is evolving along with the time, especially if. Along with the time, especially if, for example, some state has a very severe cases, right? So then we have a more strict traffic ban. Okay, so then you will say that it will have direct impact to the inflow and outflow for that specific state. Okay, so that's why this is a very typical scenario that in the dynamic system, both the graph structure and this node properties, they are evolving. So then, so that's So that's why when we try to think about this dynamic system, right, instead of just treating this GT as a fixed input and only predict what will happen to the XT, actually also need to predict what will happen to the GT. So our model is still roughly follow the same framework as in the LGODE. But then the difference is like we, in addition to model this node property dynamics, we also need to Property dynamics, we also need to model the edge dynamic. So that's why in the encoder state, right? So instead of just encode the initial state for those nodes, we also need to model or encode the initial state for those edges. And in the generative model, instead of just trying to utilize this single ODE to write down or to infer what will happen to node I at a time stamp t, we also needed to write We also needed to write another separate ODE, try to infer for the edge i to j, what will be the status at timestamp t. So for the decoder, similarly, instead of just a decoder, the position of a node from the latent state, we also need to kind of try to decode from the latent state for the edge and try to predict its edge property. Property. Okay. And so that's the difference. But then let's come to this more details of the cooperative ODE record. So this part, right, is just the ODE for the nodes. So then this part is related to this message passing from its neighbors. And this part is called a natural recovery. So in the COVID-19 case, it's quite natural. It's like if we have more people that have been affected in the Affected in the last time stamp. Okay, so then there will be less people could be affected in the next time step. Okay, so that's why we have this natural recovery. We also have the natural physics. So they are kind of borrowed from the typical epidemic model. And for the edges, right? So we also need to write down this ODE for the edge evolution, which will be affected by, of course, itself in the previous timestamp, but also will be. Previous timestamp, but also will be affected the status of the current nodes. So, we also need to include that because, um, like in the COVID-19 case, right, so if those two states are very severe, right, it's more likely that there will be less traffic between those two states, okay? And then we can decode from that latent representation for that edge to this kind of. For that edge to this kind of agency matrix, okay. And then with this agency matrix, we can fit back to this node ODE because in the node ODE, we really need to utilize this updated graph to kind of update our message passing. So that's the general idea. And then by utilizing this CGOD, the couple neural DE, it turns out that the prediction for the COVID-19 is For the COVID-19 is much, much better, okay, especially for the long-term prediction. So we have compared to there's a website, it hosts the kind of contributor from different universities, different parties. They just contributed their prediction. And turns out that our prediction has a like a big advantage when it goes to longer term. For example, if we want to do three weeks ahead prediction, right? So our accurate error. So, our accurate error steers similar to this one week ahead or two week ahead prediction. But then, if you look at some of the baselines, right? So, the performance actually deteriorates significantly. Okay, so that's a very clear advantage that why we wanted to model continuous dynamics directly instead of just do this one step ahead of prediction. Okay, and this is just animation, and on the left-hand side, it's just a ground truth of the cases. Actually, this is. Cases actually is the best cases for each state, and the right-hand side is the predicted one. Okay, yeah, so actually we have another challenge, and this one is still ongoing work, so I just mentioned very briefly. So the general idea is like now we are collaborating with people from material science, and it turns out that in the material science scenario, right, so this is we don't really have this like a graph. Already have this like a graph, a fixed graph. But different from the COVID-19 prediction case, it's unrealistic that we assume the graph is complete. So somehow we wanted to still leverage the sparse graph structure so that we can make the prediction much, much faster. Otherwise, if we have millions of nodes in the material system, particle system, then it's just impossible. So that's why we try to. So that's why we try to combine this graph ODE with structured learning. We try to make it really scale to this very large scale dynamic system. And also we want to make it marginalizable. So yeah, I'm going to skip the details, but I really want to share the results with you. So based on our recent algorithms, it turns out that, right, so for example, in this case, we can definitely accommodate. We can definitely accommodate much much more actuals, and also just based on like 10 time slots, okay, for observation, then we can predict very long future dynamics. Because if you look at the observation lens and the prediction lens, right, so you can see the clear difference. And similarly, yeah, for this one, yeah, same thing. So this is like a water system. So each node here is just a water part. Here is just a water particle. Okay, so we just can predict what will happen based on just the first few steps. And this is like sand system. Okay, so we can see that according to the dynamics, the sand system is more rigid than the water system. Okay, so I think probably what stop here, just a very brief summary. So I think it's Summary. So I think it's very interesting that we can say that we can use graph neural network and also use graph neural network-based ODE to model the dynamics of interacting systems. And we can say that according to the preliminary results, this kind of approach is very promising. And also, we wish that we can really go deeper in some of those dynamic systems, like material science, and try to. Some like material science and try to understand what will be the limitations of the current graph ODE or graph neural network. So then we can try to seek for new problems. Okay, so that being said, I will stop here. Okay, thank you.