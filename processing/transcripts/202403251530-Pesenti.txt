Students, so today I would like to present a work in progress that I have now, which is identification of potential outcome distribution under selective attention. So the question or the problem of identification of potential outcome distribution is very relevant when you care about heterogeneous effects of treatment. So in practice, you can rely on assumptions like Can rely on like instrumental variable assumptions coupled with some kind of ranking variance assumptions or also selection on observables. However, when you have in practice the fact that you can only observe the outcome for a non-random subpopulation, there is this problem called sample selection that you need to deal with also because you don't only have to worry about. You don't only have to worry about the fact that treatment can be endogenous, but because of the fact that the outcome will be systematically unobserved for a non-random subpopulation, it's also an issue. So think about a question where you are interested in the effect of college attendance on earnings. So you have to first take into account the fact that individuals self-select into college. Self-select into college and also they self-select into the labor market. But you only observe the wages, the income for those who are in the labor market. So, yes, you are interested in only the effect of college attendance, but you will only observe the wages for those who are also selecting into the labor market. So, you need to address this double and regenerative problem jointly. Jointly. If you do it like separately, you may have biases. And in the literature, there are several attempts to solve this problem, relying on parametric assumptions or structural models. And also, like when they are in use of instruments, you generally need to have instrument with large support. Now, in this paper, what I'm trying to do is to provide identification results for a marginal distribution of potential. Result for a marginal distribution of potential outcome without relying on bias assumption, strict assumption. And I also don't need instrumentative support. I actually need a binary instrument. My approach also accommodates multivariate outcome, discrete outcome, multination, binary treatment, and I have point identification. So the way I approach the question, I first derive a representation result. A representation result that I will show after this slide. So, this is a model. Consider an outcome that can be multivariate and a treatment D that can be binary or continuous. Now, I have the sample selection indicator, which is one if your outcome is observed, zero otherwise. So, here, y star is a latent outcome, and as I said, I observe y, I don't observe y star. And I'm interested in a marginal distribution of y this star. marginal distribution of y d star which is equal y d star is y star if d equals 1. So here I have t can be endogenous also s is endogenous so this is why I've said you have a double ontogeneity or now what I'm interested in is the distribution function which is the C D F of Y distal. Now to answer that question I have this first result so I think this result So, I did this result, and that is a first contribution of my paper because this is a generalization of a result in Andros in Columb 2005. So, it's a local representation of a multivariate copula. So, suppose you have a n-dimensional copula and you fix like some point in 0, 1 power n. So, I showed that for every j between 1 and n, you can always find. You can always find a unique parameter, which is like a correlation parameter between one and minus one and one, such that this representation holds. So, meaning that you can always separate for a given j, this is a joint distribution of u1 until uj, and also this is a joint of uj plus 1 until n. So you can always separate both. Now, this one, Now, this one, which is local, meaning that for each vector of point that I'm trying to evaluate the popula at, I have a different parameter here. So rho is a function of two variables, not a function of n. Otherwise, you don't have any constraint there. So rho is a function of the copula of the first js. And the copula of the n minus j. Yes. Everything like this, there's no constraint, right? There's no constraints. Oh, I saw. Okay. Yeah. So this parameter always exists and is unique. And as I said, it's just the person correlation coefficient between these two product of integrated. So as I said, this allows me to tackle the problem with double ontogeneity because now I have, since I have two ontogeneity. Since I have two endogenetic problems, I can use this result. And the first lemma that I have before the identification result is this one. So this object can be identified from the data. And I prove using theorem one that it equals this. So here, this object I'm looking for, this correlation parameter that I also don't know, but I know this probability here. probability here which is just what I observe here so what I have here is one equation with two unknown so the use now of the binary instrument will give me two equations and two unknown giving some stability so the previous theorem uh can I interpret it as you are writing a coherence into a the current class the coherence yes yes in the divide case is exactly that Is exactly that. Now, in the multivariate case, it needs to be independent between the two. So, here, when I suppose now that I can observe a banning instrument, so I rewrite the mark one conditioning of that instrument. So, I have this. And the identify assumptions are these two assumptions: exclusion restriction and local coordination stability. So, exclusion restriction is very common. Exclusion restriction is very common in the IV literature. They are just saying that the potential outcome is not affected by the instrument. Now, this assumption can look like new, but it's not really new in the literature. You can actually relate that to already existing assumptions in the causal inference framework, like monotonicity or rank invariance, or also copular stability. Stability so this assumption can be related to those, and I do that in the paper. Now, with these two assumptions, I can identify the distribution and the local parameter. Because again, what I'm doing is like I'm creating two equations for 21. So, I have a system, and I just prove that the Jacobian of that system is a P matrix. So, by global universe theorem, I have a unique solution. So, this is what I have here in this theorem here. So, this is what I have here in this theorem here. That under assumption one, I identify the marginal distribution and the local parameter. When I have continuous treatment, also, I just need assumption one, which is exclusion or assumption local correlation stability. With that, I can rewrite things that way. And using the properties of the conditional distribution, I are again. Conditional distribution. I have again this system because Z is binary, so I will have again a system of co-equation and polymorphism. And again, this new system has a Jacobian, which is epymetric, so I have a unique solution. So these objects are identified even in continuous treatment case. So here, the one thing that I do is I allow Y to be multivariate. So, when I get the CDF, I can get the quantile in a straightforward manner. Now, when it's multivariate, I need to rely on optimal transport to get a quantile. Not the quantile, but I would say quantile type object because there is no clear definition of what is the quantile you want to know. But with optimal transport, you can always come up with a definition, and the papers already did that. So now. So now assumption one that I use to identify my parameters is not testable when Z is binary. If Z is more than binary, I can test it. But in the case Z is binary, I try to come up with an idea, which is like I should have these two distributions. I mean, actually, these two random variables should be equal. That Y given S and D and Y is. Y given S and D, and Y star given S plus 1 and D plus 3. So from the data, I can get a distribution of this one. And when I, for my identification result, I can also compute the distribution of this one. So I call them PDA QD, and I can come up with a test comparing these two distributions. And if they are different, I'm sure that assumption one cannot work. So I just finished. So, uh, I just finish here. So, the main contribution of what I'm doing is to provide conditions under which I can identify the marginal distribution of counterfactual or potential outcome with the binary instrument without relying on parametric assumption, structural forms, and instrument with large support. I also, to achieve that, I developed a new representation theorem, which is a generalization of the prevalent case. Of the pivot case in this paper, and I show that I just need the same assumption to identify the objects of interest in the binary treatment and the continuous treatment. So, thank you very much if you have any questions. Are there any questions? Explain a little bit what this means that rho is the same for treatment and control for the different values of Z. It seems that the O is involving not only the outcome, but also the treatment and the selection mediator. Yes. So it seems that the assumption is involving different underlying equilibrium restrictions, statistical restrictions for these three aspects. For these three, that's correct. Yes, I will come back here. So, here, this row is a correlation between this indicator and ST. So, yeah, what you are saying is right. So, if you think about the case where Y is univariate, so and I will say this row equals minus one. So, what I will say is like up to the level Y, wage level Y, every individual is not treated and selected at the same time. Selected at the same time. So it's like I consider the like treatment and selection as one indicator from what I derive from here. So they see the instrument can affect treatment and selection, both, but it doesn't affect the correlation between potential outcome and the product of both. And the product of both. This is what the local population stability assumption is. So, yeah, as I said, I discussed the relevance of this assumption in relationship with other assumptions of the data. But now I don't have time. Another quick question. So let's then join you again. This one breaks? I mean you can always send an email if you want to have your recording taken out. So the next speaker for this afternoon is Joshio Hyu. You're going to speak about an ordinary differential equation for entropic optimal transport and its linear constraint variance. Go ahead, Jules. Yes. Hi everyone, I'm Josua. And this work is joint work with Luke and also myself. Luca Lena and also my supervisor Bennett Pras. And for this project, we try and use ODE, declaratize the entropy optimal transform on a discrete domain. And originally, we just want the first motivation only on two marginal, but actually found out that this method can be extended to multi-marginal and actually optimal transport with extra linear constraint. So let's talk about what is the ex o other methods of you. The optimal transform extra linear constraint. The setting is quite similar to the optimal transform. We have the, this one is the multi-marginal one. First of all, you have the corresponding x1 and xn marginals. We have the corresponding space for the marginal and the corresponding marginal, the mu1, mu2, and mu n. And also we have the coupling, this set of all coupling. We also have the Of all cover. We also have the cost. This is just general cost. But we have one extra thing. It's this Q. This Q is this Q is a linear subspace of the boundary continuous function defined on the product space of the whole X1 to understand. So, after resetting, so we can talk about what is the optimal transport with an extra-linear constraint. Is that they have also tried to optimize this one because integrating the This one costs integrating with respect to the coupling. But now, the feasible set is a little bit smaller, not on the whole or every coupling. But we request that if this of gamma, this coupling is inside this visible set, we request that for each q inside this length of space, the integral has to be equally zero. Actually, this setting is including many. Including many interesting measures, the CFA support, including the marking automotors for which it has application in finance. Also, it can relate to the adaptive automotance. So some people are, I think, AI also learning about this. And the most important, actually, is optimal transport itself. The marginal constraint is the linear constraint that makes Catalogue when it's trying to. Makes candid lovitch when we're trying to generate moving from launch to this so successful because it makes the marginal, it's a linear constraint. So, therefore, in some formulation, people treat the multi-marginal optimal transform is the two-marginal optimal transform with extra-linear constraint. They treat the extra dimension, that is in extra linear constraint. So, we say we try to work it on the entropy optimal transport. So, we define what's there. So we define what is the entropy optimal transport. So we have, assuming we have the important measure, which is independent coupling, then if we have some epsilon, which possible could be positive, then we equal zero. And eta has a strictly positive, then the entropy optimal transport problem with extra linear constraint is that, okay, we have still have this integrated cost, the C integration with respect to the gamma. We have been restricted on this. We have it restricted on this small set, a couple of sets, but we're also interfering this magic entropy type. This is the formula, the terrible magic entropy type. And this is the primal side. And this is the so-called dual side. Just like the optimal transport, we have the potential, but also we have this exponential. And if you are familiar with entropy. If you are familiar with entropy optimal transport, you will see that originally, usually there's only one parameter for the entropy recognization stator. But here, I introduced one more extra, it's the epsilon. But actually, epsilon, introducing this new epsilon is in some sense equivalent because we think about if we fix the eta, just on this picture, if we fix eta, when epsilon goes to zero, then the first chart vanishes, right? So this is equivalent to when we add. Equivalent to when we fix the epsilon and then we try to make the eta go to infinity. So usually the solution will be the measure which has the highest relative entropy. And on the other hand, if we fix the eta, letting the epsilon go to infinity, which is equivalent to the eta, when we fix the epsilon, when we make the eta go to zero. In this case, this is going back to the original. This is going back to the original optimal transport, our original optimal transport problem. So, this is the entropy optimal transport. And now we said that we are working on the discrete k. So, this makes the marginal to be the discrete marginal. And in this case, also the constraints of space also become only a finite dimensional space. So, we can read it as this form of the linear programming problem with the entropy cause. So, you can see that this is the original cause. This is the original cost, this is entropy terms. This corresponding to the marginal constraint, this corresponding to the extra linear constraint. And to me, this work, why I can say that, okay, this extends from two-marginal, multi-marginal, and we've also extra marginal, because actually, if we rearrange it, define it well, actually we can look at this curve platform. Just a mini a matrix. A matrix, in a matrix, and then it's a distraught, and the corresponding dual problem, originally a distort. And we can, adapting the condensed notation, we can also add this form. This in general is a concave, uh this function is a concave function. In general, it's not a straight concave, this is a concave function also. Yes? So it can have a solution, but it n may not be unique. But That may not be unique, but we in this form, and in this particular kind of problem, I mean, the optimal transfer problem with extra-linear constraints, actually, we can make this A, the A, to become a full-color matrix. Global linear full column matrix. Actually, this is a very common trick in the original optimal transfer problem. In the original transfer problem, the dual problem usually is the potential is not unique until we fix some of them to be exactly. We fix some of them to be exactly as like for example, at some point it's equal to zero. This is one way to make the dual potential to be unique. This is one trick. And this trick, I find that we can extend to not only the multi-marginal, but also all those extra linear constraint detectors. So we can have this memory just saying that, okay, this tattoos can extend to this linear string constraint. Stoning constraint. And now we're going to the main focus is about the ODE itself. So we have this Eurofon function. I twist a little bit, I put a negative side to it. So it becomes a convex function. Previously, it's a concave function. I'm putting it in a convex function. And we are trying to minimize this convex function, this convex function, which corresponding to minimizing the dual formula functional. Dual formula functional. And one property about this one is: as I said in the previous page, I can do some trick to make A to be a full column rank. After using this trick, then this functional is not only commerce, but strictly commerce. And this is crucial, because we only want to have one solution. If it's multiple solutions, it's difficult for us to parametrize it. Actually, by the first order optimal condition, if it is obviously the function, it's differentiable. Obviously, the function is differentiable, so you can try to differentiate by the first automobile conditions. The gradient should be equal to zero, right? It's obvious. And then, as I said, because it's straight concave, so straight, so it's straight convex, so it should be unique. In the sense that for each epsilon, there should only a unique corresponding dual function, dual potential. And then, this, and then it means that actually by the implicit function field, they can make. The implicit function field that can be written as a function, a smooth function, this phi can be written as a function of epsilon. And then also we try to differentiate again after we can deduce the corresponding Cucci problem. This is the ODE we want to have. In general, this is also we can find it from the implicit function field. Say that if as far as this is a solution of this optimization problem, it should satisfy this ODE. It should satisfy this ODE. But actually, we also go even further. We prove the well-ponence. We say that, okay, this ODE has one unit solution. In a sense that if you want to understand, we try to prioritize the dual potential, we can actually study the ODE stuff because ODE, you need to determine the potential. So, therefore, this gives us one application. Like the entropy regularization problem, usually in the optimal transfer problem, it's for the many times using for the numerical method. It's trying to solve for it using the entropy regularization to approximate the original OT, optimal transfer. Usually people using synchron method to approximate. Now we provide another way to approximate. We can use solving this ODE to approximate. So this is one of a new method to do the To start to do the alternative, to solve this entropy optimal transport problem. Later on, I will give you some examples. And one more thing is actually quite surprising. Actually, I say that originally last week, I said that, okay, people are using SINCOR, we are using ODE. But if doing the proof, actually, I can extend that SINCAR algorithm not only to optimal transform with two module. Previously, there are some single methods for Synchron method for optimal transfer is famous. Recently, people also said that okay, or for the multi-mart transfer. But for synchron method with the expedited constraint, at first I know I see two papers using Synchron on the multi-mole transform and also the adapted multi-move transform. But they need to be only two constraints because they rely on some convergent result that is worked only on two margins. And in our And in our project, actually, we found that the convergent result can extend to multi-marginal and also extend to different whatever extra linear constraint. So actually, we exactly, although we only want to automate the ODE, we actually contribute also to the single method. So this, and actually, for this is the numerical part, we have actually for the numerical, and we also have some. And we also have some contribution to the theoretical part. Is that after you think if we are the optimal cost for each epsilon corresponding to optimal cost, different epsilon, we expect that they will just give the different optimal cost. And for the ODE actually give us the way to interpolate between when epsilon equals 0, which corresponds to the fully recognized, and that when epsilon goes to infinity. And that when epsilon due to infinity is corresponding to the original optimal transform problem. And especially when epsilon equals zero, when we do it here, working on the classical two-marginal optimal transform, we know that when it's only two-marginal, then for a fully organized one, it means that it has a minimum attribute respect to the potentially. Then, in this case, the optimal solution is just the potential itself, which corresponding to the U and V should be equal to. The u and v should be equal to zero. So actually, we can calculate the C0, but not only C0, we can actually calculate the higher ordering. So actually, we can do the T expansion of this course around the zero. And actually, we can, using the ODE here, we can calculate the higher order. And we can key observation is that because the derivative of the potential no longer exists inside on the right-hand side. Exists inside on the right hand side. Actually, it's just a derivative. So, actually, we can calculate it for every order we want. Have we an explicit solution? Yeah, exactly. We have here just demonstrating this first two theory. So, this is some new for failure contribution. And next, I'm talking about the new map example. Yeah, the numerical example. Here, in this page, we just want to demonstrate. In a failure class. In a very classical, the two-marginal case, with the distant cores, here we are assuming both marginal, just identical to be uniformly distributed between 0 and 1. And everyone should know that when it's fully regularized, then the solution is just for the measure. When it's the original automotive transfer problem, then the identical map, the identical map is also the solution. So actually, this picture was. So actually, this picture, what's the picture? It's like this, by solving this ODE, we can see the interpolation between the full rapidization to the unreparable one. Actually, I would call it some sort of like entropic interpolation between them. This is for the transporting or transport 2 module. Another example I want to demonstrate is that, as I said, this worked with the next. This work with the next constraint. This is a multi-period marking multi-period martin for optimal transform. And I said that there's people before this only working on the marginal transform with only two multiple. But as far as I know, no one is working. I do not know that it's a single method, it's a fly to multiple. And this will be let's say that the upper one is the coupling uh that uh given by our OD method, the lower part is the coupling. Method, the lower part is the coupling given by the central method. Actually, they agree. So, yeah, okay. That's all for my presentation. And if you just ask any questions from the audience? Yeah. What's the contribution of the entropic stuff? The entropics. What do you mean by entropy? Yeah, I will go back to the question. Yeah, um so is your only exist because you're writing in my yeah the entropy because why we are entropy plants it's like why we need the extra entropy usually in general optimal transfer problem the solution may not be unique there could be multiple solutions but with this entropic terms it become extremely com uh complex Be complex functions, functional. So there should be only one unit. So that's why we want to make terms. And another thing is that because in LSN2 module, people well known that a single method is a very fast way to calculate. So actually it's very common to people in the AI, they need to calculate the waste anticipation. So they were using the single method to calculate the waste anticipation. So they call it the light speed rate. Do you have any restrictions on your set of linear constraints? Actually, as far as I'm no, as far as this is a linear constraint. The main actually, because we need to it to be on the discrete set, is is very big. set is is maybe a big uh constraint on a discrete set is only it's only a finite dimensional and finite dimensional as long as it's finite it's okay yeah the integral is because the integral is actually has to be zero so every point has to be zero right everything has to be finite actually we don't we don't even have a big uh restriction on the cost as long as it's finite everywhere that's okay you're saying the set has to be finite uh the The cost has to be finite, and also the value entropy has to be finite. But this is automatic for this big marginal. Well, I guess you also need to have an interior point of the constraints. Like, you can't, you're saying that the linear functions have to be integrated to zero and that it has to be feasible. Oh, yeah, yeah. It has to be an interior point, right? Yeah, the feasible set has to be non-empty. Say the martin optimal transport like. Empty, say the martin, optimal transform, like it has to be martin between the first module and second module. Then, yeah, this is main constraint. Yeah, yeah, thank you. I have to move your question to the next break. So the third speaker of the session is Li Zhuang Lin and she's going to talk about optimal transport under uncertainty on the components. Yeah, thank you. So hello everyone, my name is Yuan Li, I'm from University of Waterloo, a PhD student supervised by Test Me Huan. And this topic actually comes from one of our work called John's Mixability and the conclusions of And the conclusions of negative dependence. So, you see, there is no optimal choice pot in my work. So, in the name of this work, so I remade this title to have optimal choice spots here. So, in this title, we say we are discussing about the uncertainty on the components. So, we are first begin with sorry. Okay, so Okay, so I have this one. I'm not very familiar with this. So let's first look at Snaplon is marginal optimal transport problem. So what we are looking at is how to minimize the At is how to minimize the expected cost, which is major bio convex function on the sum of some random variables from x1 to xn. And each xi, we know their marginal distribution, but we do not know their dependent structure. So actually, we are choosing a dependent structure to minimize this expected cost, which is just like you choosing a transport between each component in an optimal transport problem. So, this question, of course, we This question, of course, we sometimes also call it like dependence uncertainty problem, a risk aggregation problem. And it's clear that if you can get a random vector with a given marginal and its sine equal to a constant, since we take a convex cost function, then obviously this should be a minimizer. And we call such kind of random vector with sign equal to a constant as a joint mix proposed by Wang Wang 2011. And from now on, when we discuss about marginal distribution constraints, we always refer to those can support a joint mix random vector, so which is called joint mixed ball. And actually, if you select some very nice marginal, you will get several different kinds of junct mix. So, for example, if you take your marginal as the standard normal distribution, the very simple one you can say. The very simple one you can think about like n minus n minus 1 because this n is a standard normal risk. And since we have n and minus 1 appears alternatively, so we call it alternating joint mix. And it's clearly one sums after zero, so it's a joint mix. And another way you can think about how to build a joint mix is to take a multinomial distribution. As long as you take the correlation matrix with the sum of all those elements. With the sign of all those elements equal to zero, then you will get the balance for the sine from xi12 mn would be just zero, right? So it's also just mixed. And here we take a very spatial correlation metric where we have the correlation between each pair of random variables in this vector are equal. So since it's also a joint mix and has the same original distribution, you can directly calculate the covariance for. That the covariance for this random vector would be just minus 1 or minus 1. And this one would be an exchangeable random vector. It means that if you change the position of those random variable in this vector, you will not change your distribution. So the position doesn't matter. And we are wondering where these two kinds of judgments express different laws in terms of minimizing the expected cost. So we look at the cost of the subset for this. The subsets for this route. So, for the expected cost inside this cost function, we are not going to sum up all those x, 1, 2, x, we just sum up a small group like group k inside it. And now we take a very simple example to consider this question. We take this as a quadratic function and we look at a group with only two components inside. And the one is like, it can support. own is like you can support an alternating joint mix and we have very we have only two own side you can copyright that for a group with two components this alternating joint mix will give you like four or zero subset of cost of subset it depends on you choose own or own and own or own and find some but for exchangeable junctrics it's more stable you just get one cost of subset so it seems like this Subsets. So it seems like this exchangeable joint mix will give you a more fairy distribution of the cost to every small group. So we are wondering where this one can be used to solve like worst-case problem for to minimize the subset of cost, worst-case of success cost. So directly obviously from our example before, if we take n equal to 4, you can calculate that for this alternating joint major. For this alternating joint mix, this worst case would be equal to 4, and for exchangeable joint mix, this worst case would be just 4 over 3, so it's much smaller. But we do not know where it is where we are minimizing here, right? So this is a question we are going to solve later. But of course, we want to solve a more general question for this optimal choice cost problem under uncertainty for all those components. So we first foremost that this question. First formally that this question. So, since we have some uncertainty about who will participate in this process, so we probably can get a probability distribution for each small group, like how likely this group will participate. And our objective function will be changed to the expected average of all the subsets weighted by this probability. And of course, you might have no idea about which kind of probability. Might have no idea about which kind of probability distribution you are going to use. So we have also uncertain set of the probability measures, which is M. And we say if uncertain set of the probability measures is symmetric, if when you pagation the label of all those components, like just change the labels, the result probability measure will still be inside this setup. So it means that the label of this. Let the label of this agent does not matter. You just care about who it is. You do not care about their name. Okay, so this is a very natural assumption. And our first result is here. So assuming that M is a symmetric uncertainty set and we take identical modulal distribution, then for a joint mix, you can do a reorganization of it. So it's like you put all those random variables into a box. Random variable into a box, and you take its house randomly without replacement, and form a new random vector. So, a random permutation. So, this new random vector would be exchangeable since you take them out without replacement randomly. And it's still joint mix because the original random vector is joint mix. And it still has the marginal F since we have the identical marginal. And with all those features, you can explain. Those features you can exactly calculate this correlation metric would be the PN style we introduced before. And our first result shows that after doing this reorganization, you will get a better worst case. And so we can always find an exchangeable joint mix that can improve your orange mode random matter. But we still don't know where this method will give you a minimizer yet. So it will highly depend on your choice. It will highly depend on your choice of this f function, pulse function f and your choice of like certain sets f. So we here choose a simple cost function, quadratic function f equal to x squared. And with this cost function, you can find that a joint mix with the marginal f and the corrosion metric, spatial corrosion metric Pn star, we introduced before, would be a minimizer for this question. Minimizer for this question, but we do not fully categorize it. It means that not all minimizer here would be such a kind of joint mix. It will depend on like which kind of M you choose. So now we are going to introduce two very special cases of this choice of M. So first, the first case is that we choose M as the set of all probability measures defined on 2 to the power. So it means that you have no information about like. That you have no information about which kind of subgroup will tech will be in this set, you need to consider. So, in this case, the worst case would be just like the worst case of all of the cost of all of subsets. And of course, you can just limit your discussion to the subsets with K components, but you know which group, the number of the group, the number of components in this group. Of components in this group, but I do not know exactly how likely they can be inside the probability of they participate in this minimized process. And in this case, this worst case would be the worst case of cost for the set with k components. And for the first case, we show that if we further take a remote node distribution with mean zero, then we can calculate. With mean zero, then we can categorize the all those minimizers. So a random vector would be a minimizer if and only if it is joint mix with this correlation matrix. And similarly, if we limited this number of subgrouping to a number of k, as long as you do not take this k as the spatial number like 1, a minus 1 or n, you will still get this if and only statements. And for the spatial case like k equal to 1, The spatial case like k equal to n, it would be degenerate just for one dimension. And for k equal to n or n minus one, it will go back to the non-uncertainty case. So this already solved. And for about discussion, we are limited marginal with identical marginal. But for heterogeneous marginal, what we can get is only four speed dimension. So we show that if you have three marginal distributions, you have three modern distributions with mean equals zero, then this minimized worst case problem, all the joint peaks would be the minimizer for this minimizer for worst case problem. And one very interesting result we present here is that if you can get a joint mix with all those covariance between each pair are non-positive, then any random vector with positive covariance. Vector with positive cohesion metric cannot be a minimizer. So that's why we think that the negative dependent joint mix a minimizer might be a very identical, very nice, like desirable solution for more general question. And that's why in this paper, the title we started with joint mix and notions of negative dependence. But since I'm run out of time, so I do not have time to present our work about the negative dependence. Present our work about negative dependence joint mix. And this is the reference we use in this paper. Thank you so much. Thank you very much. Are there any questions? Oh yeah, of course. Yeah, otherwise you cannot stand up. Yeah, otherwise you cannot send R essence R p equals to zero. Yeah. I'm sorry I forgot to heterogeneous case keeps very yeah so this is still a very open question. We do not have the results for very general F, variable M and also the hypogenic parameters. We do something that gives you normally distributed. Orbitally distributed of it explicitly? You mean the ethiogeneous case? So in a mandible distribution, we do not have constitution for a mandible distribution. Yes, yes, we I mean, yeah, when you say heterogeneous, but if you just think fi being different numbers. We do not take it to be normal distribution. It can be any kind of distribution as long as it's possible to. As long as it's called, yeah. Thank you. What's the motivation of studying this problem? So this problem is like originally it is a risk aggregation problem. And for a risk aggregation problem, we always discuss like how to minimize the sum of own risk. Like you are given a number of. But of course when you have some uncertainty about who will be this, you might have like if you go to a party, maybe some of your friends suddenly say, I'm not going to your party, right? Binary. So you may have some uncertainty about the participation. That's why we have such kind of question. Okay, so the last speaker for this session today is Alice Keegan, and she's going to talk about target. She's going to talk about targeted policy learning. My name is Alice Chi, and I'm a four-year ECOL PhD student at the University of Washington, and I'm supervised by Professor Ying Ting Fen. So today I will present one of my ongoing projects, Target Balls and Learning. And this is joint work with Professor Fen and also my father, Kauti, who's also here. So here are a snapshot of this work. Basically, the problem we're considering is the inability of authoritarian optimal policy in addressing issues like social inequality. So the idea here is that utilitarian policies are only maximizing the population average outcome instead of targeting specific groups. Yeah, so a good motivating example here is that consider prescribing medication to Consider prescribing medication to individuals at risk of diabetes. And here, the medication assignments can be based on some observable individual characteristics. And if we only adopt a utilitarian optimum policy, then it is optimizing the population average outcome. For example, minimizing the average blood sugar level of the individuals. And we will expect that such a rule or policy can be driven by the majority in the sample. In the sample. So, a problem can arise if, for example, the majority consists of low-risk individuals who can benefit from the medication, whereas there can also be high-risk individuals who can be hurt by the medication. But since we adopt a utilitarian policy, it can fail to extract or pick out the characteristics of those high-risk individuals. So it's possible that those high-risk individuals can end up in even worse situations. Worse, even worse situations. But that's the motivation of our paper. So instead, we are taking a group agnostic point of view, meaning that we're not explicitly defining subgroups based on characteristics. And that can be helpful because in many empirical settings, we don't have demographic labels available for use. And our paper is also risk-averse, meaning that we aim to learn an off-field policy that targets the individual. An optimal policy that targets the individuals on the lower tail of the post-treatment outcome distribution. So, we're only looking at the individuals who are worst affected after implementation of the policy. And the idea behind our paper really stems from the conditional value risk, which is a popular risk measure in risk management. Yeah, so we'll adopt the conditional value risk as our welfare function instead of the average population outcome. The average population outcome. In Yang paper, we also study balance on the welfare regret, which is measured as the difference between the welfare loss or welfare difference between our optimal policy and the best policy in a given policy class. And we also study for inference for the optimal welfare. So we can conduct tests. So, as an inferred building block, we can first introduce the standard policy learning. The standard policy learning framework, which is the protocol welfare maximization approach, in the paper by Kitagawa and Tetanov. So, this studies the utilitarian optimal policy. So, here we assume that the underlying population from which the sample is drawn, like basically the two potential outcomes, the treatment assignment and the individual characteristics, is drawn from this population distribution P. And then the welfare function is similar. And then the welfare function is simply the average outcome after this binary premium rule or policy pi. So if an individual gets treated, then pi will be one, then we take the treated potential outcome. And conversely, if pi is zero, then this individual is not treated, so we take the untreated potential outcome. So if we prefer larger wise, then the utilitarian output policy just maximizes this average population outcome. Population outcome. And we can also rearrange this welfare function into this form by defining the conditional average treatment effect as the difference between the two potential outcomes given x. And then we consider the untreated potential outcome y0 as the baseline. So the intuition here is that the first best policy is to treat the individuals who can benefit from the treatment, like those who have positive COT. Have positive COT. And of course, as I said, the empirical welfare maximization approach by this paper does not consider any distributional impacts. So our paper instead is the risk averse by looking at the distribution, post-treatment outcome distribution, and we only focus on the lower tail. So first we can express outcomes We can express out the post-stream and outcome distribution by averaging or by integrating the conditional CDF of y given x in view swept policy pi over the marginal distribution of x. And the alpha conditional value risk welfare is simply the expected outcome among the worst affected size alpha subpopulation. So it's going to be a conditional expectation. Going to be a conditional expectation of the post-treatment outcome yiÏ€, given that this outcome is less than the alpha quantile in the post-treatment outcome distribution. Yeah, so this quantile also has a name, which is the value at risk, and it is also a popular risk measure in the risk management literature. But we note in the paper that conditional value risk is more popular because it is a coherent risk measure. It is a coherent risk factor, but valid risk is not. So here's a simple visualization. So if we assume that the outcome after treatment follows a standard normal distribution, then we're only looking at this lower tau of the outcome distribution, and we are maximizing this conditional mean of these outcomes. So here's our optimization function. So this capital pi is a pre-pro Capital pi is a pre-specified class of policies. For example, linear policies were policy trees, and we're just selecting a policy in this class to maximize the conditional value risk. And here, the choice of alpha is problem specific, so it can depend on the policymaker, like how much we want to target the worst affected. So, if alpha is extremely small, then we're getting close to the essential equipment of the outcome. Of the outcome distribution. And it also corresponds to the Raussian welfare in the welfare literature. And if we take alpha to be large, like close to 1, then this quantile will just be the maximum of the outcome distribution. So this whole thing will just be the average of 1. So if alpha is equal to 1, then we are just going back to the standard imperval of our maximization problem. Welfare maximization problem. So we can interpolate between the Rausen welfare and the empirical welfare maximization by varying alpha. And here's an alternative interpretation of our conditional value risk welfare using the distributionally robust idea. So here we can define an uncertainty set centered at the observed distribution, observed post-outcome, a post-treatment outcome distribution. outcome a post-treatment outcome distribution. So this consists of all potential, like policy, potential distribution in Q that has distance upper bounded by log 1 over alpha. And this distance function is defined as the essential supremum of log dq over pf. And here, the conditional risk welfare is equivalent to the worst case welfare. The worst case welfare under the perturbed distribution of the study population. So, this is equivalent to learning a distributionally robust policy that maximizes the average welfare under the worst-pace perturbation of the study population. So, we can also think of this as a distributionally robust version of empirical welfare maximization. So, that's also an implication of the conditional value of risk. And then, in terms of estimation, we exploit the dual form of conditional value risk. So, instead of using the conditional expectation, we can write it in terms of a optimization problem. And also, this dual optimum is attained at the value risk of this outcome variable. Of this outcome variable, which is the alpha quantile of this post-stream outcome distribution. So, this form is convenient because we can directly solve this optimization problem instead of looking at the conditional expectation. And also, we can redefine nuisance parameters like the outcome parameter and the conditional average treatment effect. And I call them like pseudo outcome. them like pseudo outcome and pseudo CATE because they will depend on these this quantile data. So identification is similar to estimating the average treatment effect. So we can assume selection on observables namely unconfoundedness and overlap. Then in this case we have the welfare function will be identified. Welfare function will be identified in this form. So this looks pretty similar to the estimate of the average treatment effect. So we just have the nuisance parameters mu and also have L. So we can also, similar to ATE estimation, we can design prosperity nuisance estimators and also the doubly robust multiplication terms to estimate. Estimate this welcome. So, in the end, what we do is we maximize this simple analog of welfare over the policy and also this data. And we show that asymptotic normality is achieved, and the asymptotic variance would just be the variance of those doubly robust. Of those doubly robust scores. How much time I have left? So, yeah, I'm open to questions. So, assumption tool is not a need because you want to identify the joint distribution of Y0 requirement. But if you can But if you can get it in another way, you don't need this assumption. Yeah, so there can be other identification assumptions. For example, the difference in differences are identification. So it doesn't have to be selection non-systems. This is very interesting. Thank you for that. And I want to make sure I understand. So uh there's just a couple of questions that I want to clarify on the technology and I look forward to reading your paper. There's two strands that come to mind There's two strands that come to mind in economics related to that. First is the idea of social aggladiation of natural welfare. And the second is the social welfare. They're not exactly the same. But that's why I want to make sure I understand what you mean at the beginning by a utilitarian approach. If that means like sort of the Hartsan literature on aggregating individual preferences, or does it mean really showing that optima could be attained by activism? Party money could be obtained by maximizing your social welfare function. I believe you're leading towards the second approach, correct or not? Yeah, so the utilitarian welfare policy is maximizing the average welfare policy. Right, so with that said, that means that you're assuming loose neutrality, I would agree. Otherwise, you would be a sum of weighted expected utilities, right? Not just the sum of expectations. What do you mean by sum of weighted utilities? So what I mean is in a population with, let's say, expecting activity maximizer, but it doesn't have to be. Let's say they're risk average. You can show that very to optima in this exchange equipment can be found by maximizing the social welfare function in real particular rates. But if you risk much of these rates can be taken to be one over n, and so it would just be the sum, right? That would give you the entire function, right? And your premise is. And your premise is that this is not an acceptable way to, say, find a social optimum for what reason? This is the point that I missed. Is that you need to introduce risk aversion? Is that what it is, or is that more it's true? So I think what we're talking about in terms of risk aversion is that there can be different weights for the outcomes. So there's another paper also by Kitabela that talks about putting more weights on lower ranked outcomes. More weights on lower-ranked outcomes. So they are putting more weights on the worst affected individuals, and then for the top individuals, they have less weight. But our weight function is different because we're only considering the lower tail. So all individuals in that lower tail will receive a weight of one and the individuals above the upper part will receive weight zero. Right, but but uh going back to the previous observation, uh a choice of a particular set of weights give you one point. Particular set of ways give you one point on the barrier to frontier. So, and rarely that barrier frontier is a single. So, I mean, you have infinitely many points on that utility possibility frontier, right? And so, this is the point that I'm missing. So, what you're suggesting is picking a particular sort of set of weights to choose one point, one particular point on that feature, and arguing that this would be the preferred one among all the possible planet optimum. Is that what it is? Is that what it is? So we are not choosing weights. The weight is given if you have alpha. Because alpha is pre-specified. If you have alpha, then you know which individuals we're targeting. So those individuals will have positive weight. And our policy is just designed for that weighted average. The weights are risk-classified. I'm sure I'm missing something. I need to read a paper, but thank you. Well, that concludes the session. Thank you. So this