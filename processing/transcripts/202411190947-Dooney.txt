I was supposed to be giving this talk on Thursday, but I actually think it's nice to give it now because it'll very much fall in after Derek's presentation. And hopefully, this research can take a step towards solving some of the problems that we have with glitches in particular. So we'll get right into it with some motivation. So gravitational waves are obscured by detector background and glitches further hinder data analysis. Further hinder data analysis by increasing false positives, biasing PSD estimation, and in the worst of cases, overlapping with signals like we've seen with GW170817. And it would be nice to reconstruct these things wherever they happen. So some benefits of time-domain reconstruction of signals include the reconstruction to serve as precursors for parameter estimation pipelines and looking towards next generation detectors. I think we can even think about trying to isolate unlocked signals from stuff like supernovae. Signals from stuff like supernovae. And with regards to glitches, if we can successfully reconstruct glitches, we can aim at doing some sort of glitch mitigation and then build data challenges and other simulations that can help people do their research. So with regards to model agnostic approaches for glitch reconstruction in the collaboration, it's Bayeswave. It's very effective, but it can be costly. It takes on the order of one CPU hour to model one glitch, and this might be infeasible going forward towards the end. Be infeasible going forward towards next generation, and even at the moment, in the case of binary neutron star emerges, where time is in the essence in terms of localizing in the sky where the signal is coming from. So, here's a loose goal about what we're trying to do. We want some fast and accurate approach for time domain reconstruction. We want to be able to reconstruct any power excess above the gaussian background, which is typically assumed during data analysis. We want to be agnostic of the source and the morphology of these events. Morphology of these events, and we would like to compete with Bayes wave accuracy while reducing the inference time. And if deep learning models can be trained effectively in this type of problem, they're a good candidate because after the training phase, they're very, very fast. So you can see some diagrams about what we're trying to do here. Like basically, given any sort of excess power event hidden by the background, we want to be able to reconstruct that, whatever it is. So here's a high-level overview about what we're trying to do. So in the top row here, this is like the typical approach. So, in the top row here, this is like the typical approach in terms of deep learning, at least for denoising. And you have your mixture input with signal plus noise, or with what have you, and you train your deep learning denoising model to map to the underlying waveform that's underneath it. And this is very effective when you understand what that waveform looks like. Our approach is a little bit different. Instead, we are training our deep learning model to map our mixture input to the Gaussian background underneath rather than the loop. Underneath rather than the waveform. And this is motivated by the fact that we want to be agnostic of what that waveform is. We don't want to just provide that as a target, and we want to just be able to hit the Gaussian background, whatever that is underneath it. And using the additive assumption, we can then simply just take the model output, subtract it from the model input, to hopefully reveal whatever the excess power is underneath. So here's a look at the gravity spike glitches with Derek just went through. With Derek just went through, and you can see a wide range of time-frequency morphologies here. It's very diverse, and we want to be able to build something that can generically be applied to these type of events. So, we have our work cut out for us. But we're going to try to do that using some sort of proxy in GLET injections, where we just populate this space we care about full of stuff. And we learn to map that mixture input, noise plus injections, to the background. To the background realization that's underneath these events. And this is how we do that. We basically start with a whole bunch of gauss and background samples, two seconds long, 4096 sampling rate, a mean of zero, standard deviation, 50, and this is all simulated with numpy. And this is actually going to be the target of our network. This is what we're going to try and hit every single time. And you can see what we populate it with up here. We're just going to stuff a bunch of these anywhere between one and Anywhere between one and thirty of these five different waveform classes into these background samples. They each have their parameters which we sample randomly during their generation, but some important parameters at the minimum frequency we consider is one-thirds maximums and likewise frequency, half the sampling rate. So we scaled the sum random SNR and 0 to 250 and show it all in there. And we're going to inject some sort of random time lag just to have diversity in time dimension as well. And you can see the results of doing And you can see the result of doing these types of steps here. You have some crazy mixture input with a bunch of injections, and we want to try and map that to that. Some assumptions in doing this. So we assume that signals and glitches can interact additively with the protector background and that the signal and glitch space can be modelled by interpolating between these above waveform classes and their linear combination. This isn't too crazy because Bayeswave does something similar. Bayeswave assumes that. Similar. Bayes-Dave assumes that the detector strain can be modelled using some noise model, signal model, and glitch model, and the glitch model can be approximated as Derek went through using a bunch of sign Gaussian wavelets to conceive what that kind of looks like up here in the middle plot. And we just consider some additional wavelet classes just to really beef up the diversity in our data set to improve the interpolation ability of our deep learning models. Here's an example of what these samples look like in the time domain on the right, on the left-hand side here. We learn to map. Left-hand side here. We learned to map what's above to what's below. So, yeah, you can't really see these gout and background features in here because of the loud injections that we put into it. But we have 250,000 of these samples. And some of these samples can have, for example, one injection in them. And what we're looking at here is another time frequency representation outside the Q scan. And this is just calculated using the short-time Fourier transform. And you can see the mixture input above and then the Gaussian background size. Above, and then the Gaussian background samples underneath it. We can model both of these representations, so we want to try to understand which one is better. So, in terms of the time domain, this is what this looks like. So, the model which we use is called the U-Net. I'm not going to spend a lot of time on this, but if anyone's curious afterwards or at the coffee break, we can talk about it. We're basically mapping some sort of input with this dimension to the same type of dimension of the output. And this is just 8192 as our two-second sample, basically. Is our two-second sample basically, and BS is just the batch size, and we're basically mapping this mixture input to the Gaussian background underneath. And that's what this looks like in the time domain. And for the spectrogram domain, it looks like this. So we start with our signal, our mixture input, and we use the STFT, the short-time Fourier transform, to give us our complex spectrogram, which we divide out into magnitude and phase components. So humans can't understand the phase, we can't really hear it, is what they say. But it's important that we model it silently. But it's important that we model it simultaneously so that when we actually map this to the magnitude and phase spectrogram of the Gaussian background, we can just take the inverse SDFT to bring us back to our time domain. And then we can do the subtraction like we were doing before, just by modeling client series. We're going to use three different experiments to show the efficacy of this approach. So in the first case, we just use a purely simulated experiment. We're concerned with the reconstruction accuracy, which is the time domain match. Which is the time domain match with simulated glitch injection, the simulated white noise. And we'll use this experiment to compare different deep learning architectures. But differently to our training data, we're actually going to include unseen glitch signal classes from glitch generators, namely Gangli, which was developed by Melissa Lopez in the audience here, and CDB GAN, which I developed, which is a conditional GAN, quite similar to Gangli. Gangli generates blit glitches, CDB GAN generates blit glitches in two other classes. The nice thing about And two other classes. The nice thing about these conditional GANs is that you can sample this class vector up at the top here to get all sorts of weird hybrid samples just to increase the diversity of what we're going to test our reconstruction ability above these models. So that's a nice feature for those. And then we're going to take the best model that we found in the simulated experiment and do a one-to-one comparison with Mayswave for the state of the arc. So we'll apply both of the same data. And we'll finish up with some examples of applying this approach to gravity spike literature, which includes analysis of real data. Which includes analysis of real data. It'll be more of a qualitative result for today. So we'll start with the simulated experiments. So we're looking at a table here. This is basically the mean match across all our different classes which we considered. The up down to ring down were seen during training. All these ones were info. I'm just going to draw your attention to the mean row here down at the end. And you can see the different models we tried. These three are 1B models, just our time series. And these three, they're all units, but different resolutions. They're all units, but different resolutions of these magnitude and phase spectrograms, basically. And this table says a few different things, but let's just talk about the main ones. 2D, if you look at the 2D spectrogram here with the high resolution, it's better than the 1D case. And higher resolution spectrograms lead to better reconstruction ability of this model, which is intuitive because you get finer details in higher resolution spectrograms. Just a note on the inference speed, so we can reconstruct one sample in 0.05. Reconstruct one sample in 0.05 seconds, and that's on a CPU. If we use a GPU, it's even faster. So, let's do some plots about what this model can do. So, you can see in dark blue here the mixture input, and then I have the injection in orange and the reconstruction in dotted grey. And these at the top row are the full samples, the two seconds which we consider for our model. And then I just zoom in in the middle, interesting features. And you can see this is an example of the CBB GAN Tom. This is an example of the CBB GAN Tompty glitch. And then this is one of these weird hybrid mixture glitches that we inject in. And you can see we do a good job that hid most of the features even when they're pretty obscured. And then we increase the difficulty. This is just to show the flexibility of the model within this two-second window. And I zoom in in the red circles here at some of these interesting features. And you can see that we're matching the injection pretty well in these type of hybrid samples. So now let's So now let's take the best model which we could and let's compare it with Bayeswave in a one-to-one comparison. So again, purely simulated data. You can see this is the input for both approaches and then Bayes Vapus on the left, deep extractor on the right. So this is a BBH signal generated by the GAN, but we just consider it like some glitch-like event, pretty much. Event pretty much. So Bayeswave isn't given any information on the signal model here, just so you're aware of that. And yeah, but you can see the match here on top of each plot from both. And this is an interesting example because Genghi was trained on Bayeswave glitches from O2. So pretty much Bayeswave has to get out what it learned to get out in some other event, pretty much. So yeah, it's interesting that even then, Deep Extractor can surpass it in terms of the match. Okay, here's just a sign gaussian and just to show some examples. Ausion and just to show some examples of what this can do. And yeah, one of these weird hybrid samples from the CDB GAN. Like, it's okay, there's some discontinuities in here, maybe not so realistic, but it's just to show the flexibility of the deep extractor model in its ability to interpolate within this space of basically trying to capture anything beyond the Gaussian noise flow. So here's just some numbers. This is the for both Bayeswave and Deep Extractor. For both Bayeswave and Deep Extractor, the minimum and maximum match. The reason why I didn't just take a mean or something is because we only have six samples, and I just wanted to show that a lot of the time Bayeswave does very well, but just has sometimes trouble converging on the actual underlying injection, whereas deep extractor remains to be flexible. So, just some caveats here. This is a purely simulated setting, and maybe a more realistic setting could yield different results. So, Tegwood, the grain of salt to some degree, but we certainly have. To some degree, but we certainly have good flexibility with the deep learning approach. And there's some things that Bayes Eye can do that we cannot do. So, Bayes Eyeve can incorporate signal models directly and, as a result, handle when glitches overlap with signals, which is a very nice feature. And we can't do that yet, but maybe we can have some discussion and the conclusions about how we can try to do that. Baswave also can do things like PSD estimation, windowing and lightening, and these are all things that our model already assumes are done before we do some sort of analysis. We do some sort of analysis. So let's have a look at applying this approach to some of the gravity spy data. Before I do that, I just want to draw your attention to PSDs. So up until now, our model has only seen this realistic, sorry, unrealistic, very well-behaved simulated white noise, and that's what this looks like. And you can see just real detector noise over 60 seconds above 3B in Hanford. And here's the original PSD, and when you whiten it with Welch's method. Whiten it with Welch's method. You can still see a lot of these line features at the lower frequency range, a lot of this stuff going on. And this is challenging. This is like a challenging noise environment and it goes beyond what we've done so far. So we've really made the idealistic case in at least in the initial stages of the experiments. So before we can go applying this stuff to real data, it's very important to handle the BSDs. And we can talk a little bit about how I've done this. This is a very imperfect approach. This is a very imperfect approach to gathering the real data, but just as a first step, just to see what the model can do. And I'm just going to draw your attention to the diagram here. Basically, we have this pre-trained peep extractor, this U-Net model, and it's only seen this very well-behaved PSD. So, we're going to inject additional knowledge about the PSD in a fine-tuning stage. We'll take our pre-trained model, transfer it with a real PSD using the similar type of injection scheme that we had, and then we will eventually end up with a And we will eventually end up with a fine-tuned deep extractor model. And I just want to show the effects of doing that. So here's like an example of a blip glitch. And when you don't transfer learn on the top, you can see that this 512 hertz line, we only go up to 1024 hertz here. This is actually being removed from the data. So that's a problem. Like you can see in the orange here, all these high-frequency artifacts, probably from these lines. The model just removes them because it didn't see it. It only saw the really nice flat PSD. Really nice flat PSD before. So, this is excess power to that pre-trained model. But when we actually do transfer learning, you can see that we can remove the glitch while maintaining these five hundred and twelve hertz lines. So, we get just a reconstruction that doesn't have all these weird artifacts in it. So, this shows the importance. Yeah, it's very important to be able to handle these things. I'll just show a few examples here of some of these reconstructions. Tompy from OTRIA Hanford. This is a whistle glitch. So, yeah, you can see basically this is just pretty much qualitative. You can see basically, this is pretty much qualitative because we don't know what these things look like underneath. But you can see that we do preserve a lot of the gaussian background type features here. And some helix, so just to show the flexibility. Anything within the two-second window above that noise floor, hopefully we should be able to reconstruct it pretty well. So I just want to show some border cases. So this is like an extremely loud glitch, and you can see some zeroing effect here underneath these really, really loud glitches. You can see how loud they are, like the background is barely noticeable. The background is barely noticeable. So that's just a limitation of this approach. And I don't know if there's maybe some ideas of how we can maybe improve that, but maybe this, at least with this approach, it's the best we got. And then also, the low frequency range can be a problem. So you can see here in the realistic PSD, you can see all sorts of stuff going on here. And this is supposedly a no-glitch gravity spy event. And you can still see some stuff going on down here. And what I'm showing here is actually the input and the output of the file. Actually, the input and the output of the fine-tuned model. The fine-tuned model couldn't do it because this wasn't above what it normally saw in the backgrounds it was trained to hit. But when we used a simulated model that had that lovely flat PSD curve, it does take it out to, it looks like it takes it out well, but of course it also takes out some of these high-frequency lines. And you can see, like, it does seem to do something. So, in the grey here, um, um. Our fine-tuned model couldn't take out anything, but you can see some of these sinusoidal behaviours that we associate with scattered light glitches. But we also have the artifacts as well. So, some conclusions of the research. We've been able to develop at least some sort of semi-model agnostic time domain reconstruction approach using deep learning, specifically UNESCO. We've done that by injecting a bunch of different waveforms to have some sort of way that our model can interpret. Some sort of way that our model can interpolate to real signals and glitches. And we are learning to map the background. So this problem really corresponds to learning PSDs, pretty much. And if you can learn PSDs very well, you can learn what's supposedly normal. And if you can learn what's normal, then you can learn to reconstruct what isn't normal. And I think it's important to draw attention to this representation of using both and simultaneously analysing both magnitude and phase in these SDFT spectrograms, which are some. SDFT spectroands, which is something that the Q scans that are popular with the gravity spot data set lack. And we've been able to surpass Bayes Wave performance in the simulated setting by yielding a 10,000 times speed up. So definitely if this approach can be developed upon, maybe we can find ways to handle the next generation problems. And yeah, we can by incorporating real PSD, we can transfer learn at least to approximate these these real events as they occur in the in the detectors. They occur in the detectors. But yeah, of course, low frequency range and extremely low glitches can be a problem. So keep an eye out. We can't have a paper and a package coming out soon, so hopefully you can use it in your own analysis. We would like to be able to handle the real training data and the real cases better. So any help or feedback is appreciated there. How we can better handle these PSDs and real data to really remove these excess power events. There's been work done on removing the lines from the data. Maybe that's one approach. Approach. But yeah, with regards, we cannot handle overlapping signals and glitches. The model doesn't care. It'll take out anything. So that's going to be an extra challenge to do that. But I do have some ideas on how it might be done. Again, with just simulations, injections, all this stuff. Audio source separation problems. There's a big field in deep learning, those type of things that have very similar. Yeah, there's a lot of similarities with what we're trying to do here with overlapping signals. But in the 3G era, hopefully, if the Einstein telescope has a triangular Hopefully, if the Einstein telescope has a triangular design, this is good motivation for that. Because we'll have the null stream, and then we can just reconstruct glitches there. And then we don't have to worry about at least glitches overlapping with signals in that case. And as a next step, it would be nice to test if this model, if it can be trained at least in the null stream, if it can help parameter estimation there, and then hopefully later on, focusing on overlapping signals in the current error. So that's it. Thank you very much for listening. If you have any questions, I'll do my best to answer them. Questions, I'll do my best to answer them. Questions?