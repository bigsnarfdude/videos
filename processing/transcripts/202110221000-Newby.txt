             a Newtonian fluid, it could also tell you about the viscosity of the cytoplasm. So the reason we kind of hesitate to talk about effective viscosity or viscosity is that we don't really know. In fact, the cytoplasm probably isn't a Newtonian fluid. It has elastic properties to it. Elastic properties to it because you know it's a very crowded space and there are lots of biopolymers. So we'll just talk about the diffusivity of gems, which is a well-defined thing. So it's basically the variance of those delta x's with maybe a scale factor. So we're going to try and spatially resolve this parameter, the diffusivity. And more generally, the approximation And more generally, the approach that we take in my group is to use stochastic models of the motion of something, bacteria, particles, diatoms, and then try and connect that stochastic model, the parameters in that stochastic model, to environmental factors that are hidden. Okay, so what kind of data are we dealing with? So I showed you a nice movie at the beginning. That's a turf microscopy video. It's not actually what we're going to use. We're actually going to use 3D videos from a spinning disk microscope. And I can say those words and pretend like I know how to use a microscope. But of course, I just know how to use a laptop, right, and a computer in Python. So, this is a max projection. So, we take that 3D video. It's really hard to visualize a 3D video, right? Especially one that's as noisy as this is. Look how much sort of noise there is, and that's because the gems are powered by GFP, and they're not very bright, right? And they're very fragile, biologically expressed fluorophores. So, we just have to deal with these low signal-to-noise conditions that. Low signal to noise conditions, that's just part of an inherent part of this data set. So it's ZStack, right, that you're dealing with, and then you've projected it just for visualization, or do you project it for analysis too? Good question, just for visualization. We're going to... So these red spheres that hopefully you can see are that's a representation of our tracking, our estimate of the three-dimensional position of a particle. Position of a particle. So I'm overlaying that. That's not, of course, part of the actual video. I'm overlaying that on the top of a max projection just for visualization purposes. Because it's nice to say, oh, is this particle that I've tracked, I get a nice XYZ number. Does it actually correspond to a particle in the video? We like to make sure that these things are working properly, right? So this is, I'm going to use these sort of layers. So, this is, I'm going to use these sort of layered visualizations to try and understand what we're seeing as we move along. So, one of the problems that we caught early on, so yes, we can get these particle positions, but XYZ doesn't necessarily tell you where inside the cell you are. Because remember, the statistics of what we're interested in estimating, the diffusivity, depends on how far between. On how far between two frames a particle jumps. So I have to connect the dots. I have to take a particle in one frame and connect it to a particle in the next frame. And what if I create these jumps over completely different compartments of the cell? That's going to give me bad data. And it turns out we are actually interested in regions like tips, branches, and these are places in the cell. And these are places in the cell that are prone to get right up next to. So we call these hypha, just these distinct tubes in the cell. So we want to be able to separate our measurements between hypha. But not only that, we want to be able to say, oh, here are particles that are at the tip of one of these hypha, or these are on a branch point, or how far, in terms of the axial distance. Axial distance along this tube are two observations. So we need like a geometry of the cell, right? So that's one of the first things that we had to tackle. So what we did is we took all of our x, y, z positions, right? So I did like a time projection. I just put them all in like this point cloud, right? And then we had to segment them. Had to segment them, those points, into distinct hypha, distinct tubes. So that's a very hard problem, and the way we ended up having to do it is by hand. And that's one of the first things that Grace figured out how to do. And she worked on this project as a, not an undergraduate because she had graduated, but before she started graduate school. So this is one of the first things that she figured out how to do. So once we have How to do. So once we have the points separated out, what we can do is use computational geometry to work out a surface mesh. This is like something you would see in like a video game, right? Maybe like an 80s video game. This is because they figured out how to use polygonal meshes to represent three-dimensional graphical scenes. So we're using the very same tools. And then, so we have a representation of the surface of the cell with this. Of the surface of the cell with this polygonal mesh, and then we can use skeletonization to get a medial axis curve. And these two things we can basically put our observations in the context of a cell. We know where the tip is because we know where the end of the medial axis is. We know where branch points are because it's not shown here, but we actually get a graph with these. Graph with these medial axis curves. And we can take our estimates, right? We're estimating the variance of our delta x's spatially. So we can project that onto the surface. That's what you should see here. Or we can project that onto the medial axis curve. And we just get like a spatial profile moving down the central axis. Down the central axis, the medial axis of these tubes. Those are the two things we're going to look at. So, this was all done with the computational geometry library, which is just a standard open source C ‚Åá  library with lots of really useful sub-modules and packages. Anyway, so I want to talk about the problem of spatially resolving something like diffusivity. Something like diffusivity and the approaches that have been taken in the past, just sort of broadly speaking, with particle tracking. So, this is a completely different data set. I honestly don't even remember what this is. Auger, this is a micro-rheology data set, so it's a 2D thing. But the point is that this is something that we've done in the past with Greg Forrest's group. These particles, they just have their own private little territories. Own private little territories. They don't interact, really. Maybe there's one here that kind of gets close to another particle, but I could just estimate the diffusivity of each of these tracks independently, right? And then sort of like sew those, like spatially localize those diffusivity estimates and then create like a landscape, right? But that doesn't work for us in these hypha because these Because these gems are small. The smaller a particle is, the faster it diffuses. So they are mixing, right? This is like, you know, it's a crowd in a party. Everybody is just walking all around and exploring lots of overlapping territory. So we need a different approach. So, what we decided to do is use Use a kernel regression estimation and maximum likelihood estimation. I already showed you the formula for maximum likelihood estimation of the diffusivity. But remember, it only depends on the delta x's. So I don't take a particle over its entire track and estimate a diffusivity because it's gone halfway down all the way to the end of the tube, the hypha, and back again. The tube, the hypha, and back again. I need to spatially localize these. So I just take the delta x's, which they haven't traveled that far, and I pin those down to a spatial location. And then I give each one of those, these delta x's again, I give each one of those a Gaussian, its own little Gaussian. And so each one gets a Gaussian wherever it happens to be. Wherever it happens to be, and sigma is like how far away that Gaussian is going to persist. And then you get a large superposition of these Gaussians, and then that gives you the spatial profile of the diffusivity. So it's like taking my maximum likelihood estimator, which is basically the variance, and just spreading it out with this kind of smooth histogram. A word about how we A word about how we did the actual how we localized where the gems are. We get this, like I said, this large point cloud of positions at times. This is based on some earlier work where we developed a convolutional neural network that can pinpoint the centroid position of a particle. So it's a feed-forward. It's a feed-forward network of convolutions that starts with the image data. So, this is grayscale integers. Each pixel is an integer, right? The bright ones are larger integers, and the black ones are like zero, right? And those get fed into these convolutions, and there are non-linearities between these different layers, and you train them. You train them, and when you train them somehow, by the magic of machine learning, they begin to recognize patterns. And we have a very simple pattern, and that is bright blob, right? But it's a difficult problem because we have low... How is this working? Oh, it's already automatic. Okay, I didn't know that. All right. So these are, this is not gems. This is, I think, virus and mucus. It's just a standard example issue. It's just a standard example. I show some of the difficulties that you face when doing particle tracking. Low signal to noise, it's much brighter on one side or one corner of the image than the other. And anyway, so the neural network does a pretty good job of just pinpointing where these particles are. And then there's a task that you have to perform after that, which is get the delta x. The delta x. We have to connect the dots, go between. Is it giving you a full 3D shell? This is just a 2D video. Yeah, so we get three-dimensional positions. I'll show you more of the actual visualizations for the gems tracking as we move along. Anyway, the neural network is, it's actually pretty simple as far as the neural networks that are designed to categorize. Networks that are designed to categorize your cat pictures on your phone are probably much more complicated with more layers, but our patterns are very simple, grayscale, bright blobs again. But we do do, I often get the question, do we build time into this inference? Like when you watch a video and you let it play, you can see the spots, but if you just stop it, right, then it's Just stop it, right? Then it's just a sea of noise. So we do what's called a recurrent neural network, which takes temporal information through time. We have these very large videos. 3D videos can be 10 gigabytes, 100 gigabytes, and more. And neural networks take a lot of memory to run. So one of the challenges that we had to do is we had to store. Had to do is we had to store this terabytes worth of videos, and we had to deliver those videos to processors, lots of individual processors with their own memory, in order to run the neural network on that data. So we did this with this standard map-reduced kind of algorithm, and cloud is in this case Apache Beam on Google Cloud. They're designed to do this, right? Take virtual. This, right? Take virtually unlimited storage and deliver it to processors. So that's what we used. So we get this pipeline which involves, again, the particle tracking and these green ones here are hand processing. So we had to segment the localizations, right? We did that by hand, and then we had to also touch. By hand, and then we had to also touch up those surface meshes I showed you. So we get the geometry and the tracking together, that's the point. And then the products are those surface projections and the medial axis projections. So here's some surface projections overlaid on top of the max projected video with the particles there. So this is just like a stack, right? A layered stack of results. Stack of results just so that you can see the different things that are coming together to create these estimates. So I'm going to come back to this because I think first, it's important to validate what we're doing. Can we trust any of these results? One of the things we do is we look at stuff like this just to see if things make sense with the data and the tracking and the diffusivity estimates. So that's important. Estimates. So that's important. But I think what's also important is to do some other kind of validation. What we like to do is synthetic data where we control the ground truth and we can really investigate how the uncertainty propagates through the entire pipeline. So when I say synthetic data, I mean images. I made 3D images. This is a max projection. Images, this is a max projection. Notice how it looks a little too perfect, right? Because it's fake, right? I made this. The real hypha curve a little bit, right? Or it's a little bit messier than this. So what I wanted to test is a really basic question. If I have a region in the hypha that has very low diffusivity, so that is very viscous, and it's right next to a very high diffusivity region, will my tracking be able to pick up both? Pick up both simultaneously. So, this is the medial axis projection. Remember, I told you we could get just curves. And this is the surface projection. And it does. It's able to get that. But it didn't at first. When we tried this at first, it completely failed at doing this. We could get one or the other. And so we had to actually build in some heavy... Build in some heavier machinery based on something called the expectation maximization algorithm. So we iteratively created better estimates for ourselves. And I just don't have time to really get into the details on that. Long story short, we can simultaneously resolve low and high diffusivity regions that are next to each other. We underestimate diffusivity if the diffusivity is large. If the diffusivity is large. And that's partially because we have boundary effects and just because tracking is inherently biased to fast-moving particles that move very far. The farther they move, the harder it is to estimate them. This is perfect tracking fed through the entire pipeline. So if we did particle tracking perfectly, that's what the answer we would get. So you can kind of compare these and see the answer. Of compare these and see the uncertainty that just comes from the particle tracking itself. What's the difference between that image and this one? Notice the scale. It's like 0.1 down to 0.02. This is in microns squared per second. This is 0.02, so this is 10 times less. We wanted to know if we could do this resolution across orders of magnitude in diffusivity. In diffusivity. So we can still resolve very small diffusivity differences as well as larger ones. Here's some results. We were able to quantify the heterogeneity that exists within a single cell, which you can see here, these all come from the same cell, and even between different cells. We wanted to see if there are any interesting regions of the cell that have low diffusivity. Remember our original question was, can we locate any diffusional barriers within the cell? Unfortunately, we didn't get much of a result for diffusional barriers in the interior, but we did see a strong signal for Did see a strong signal for lower diffusion at the growing Heinfeld tips, where you have a lot of actin dynamics and other things occurring. So the way that GRACE quantified this is by taking the average diffusivity along the entire hypha and then comparing that as a ratio to the diffusivity of a region of interest like the hypha tip. And so we see that. And so we see a very strong signal of lower than average diffusivity at the hypha tips compared to the rest of the hypha. Can I ask you a quick question, Jay? This is the Edmund. What about around nuclei? Okay, we got some unconvincing basically lack of any difference around nuclei. And there's a couple of reasons for this. This, there may actually be diffusional barriers, but they're too small for us to really pick up with this method. We need to somehow increase the resolution of our spatial resolution, I guess, of diffusivity. And I'll talk about some extensions and things we're thinking about in a bit. But we did find an interesting difference, the difference is. The difference is small, 69%, but that small difference is statistically significant. And that's at the SG2 checkpoint, which is kind of interesting, suggesting that you get some increased crowding there, but it's also a checkpoint for the cell cycle that would be probably a nice target to delay or advance the division time if you wanted. The division time, if you wanted to say, control synchrony across the population. We have another quick question in the chat window. This is Joel's question. Can gems be excluded from the regions of interest around the nuclei? Can gems be excluded? Well, they're certainly excluded from the nuclei themselves. And we did consider sort of the volume fraction of organelles. There's been some There's been some EM studies looking at that, and we believe that gems should, their small size should be less than any sort of gel pore size, and the organelles in Ashbia are never dense enough to prevent something small like gems from going wherever they need to go. So, we're pretty sure that they should be able to access any region. Be able to access any region of the cell except for inside the nuclei themselves. So, just to summarize these results, we're able to quantify and spatially resolve diffusivity within the cytosol. We were able to correlate it with cellular structures like the growing tips of the hypha and around nuclei. We used a convolutional neural network. We used a convolutional neural network to automate the particle tracking. And we used Google Cloud for storage and processing of large video data sets. Okay, so I want to tell you a little bit about some ongoing work that we're doing. So, to address and enhance our ability to spatially resolve properties with this sort of particle tracking data. Sort of particle tracking data, I've been exploring a different approach, a new idea, and that is based on not tracking particles. Okay, so remember, we have to connect the dots. We need the delta x's. Well, I'm tired of delta x. I want to do something else because these are very dense with gems. It's very hard to control the density of gems. Gems, and the signal-to-noise ratio is very low. So, what we wanted to do, and this is with Max Hirsch, who's an undergrad at Carnegie Mellon, who came out of this super high school in Chapel Hill or in North Carolina, where they apparently all come out as experts at Python. You're getting into your 10 questions. Okay, that's fine. So, anyway, this is just that same This is just that same movie I showed you before, but I'm just supposed to say, look how dense it is, right? It's really hard to connect the dots between these, and it is. So instead, what I want to do is I want to count how many particles are in bins, and then I want to model molecular motion using a stochastic reaction diffusion equation. We saw this in Sam's talk earlier, I think that was yesterday, and also in I think that was yesterday, and also in Tim Elston's talk, he talked about what did he call it, the spatial Gillespie, right? So the basic question is, can we train a neural network to count and not just count all the particles in the whole image, but to count in like bins, right? So this is some preliminary results. These aren't published, but here's a very simple training image. A very simple training image, and the neural network does seem to still be able to spatially localize. Well, this is the neural network output here, right? It does still seem to be able to spatially localize where the particles are. And it counts, but that's not obvious from this image. So what Max did is he ran a whole bunch of experiments with differing numbers of particles and looked at just how many particles the neural network counted. The neural network counted, and it does a pretty good job, though not perfect. So, this is, I think, a great start for being able to extract data from these microscopy videos that we could feed into a reaction, a stochastic reaction diffusion model, and then do inference with that model. All right, so another project that's ongoing that Grace introduced yesterday is tracking. Yesterday is tracking the position and the lineage of nuclei in living cells. And so, these, I think these videos we've already seen, but I'll show them again because they're very nice. We have hundreds slash thousands of these nuclei moving in complicated ways, not just Brownian motion, and they're dividing, they're spatially organizing within the cell in fascinating ways. Fascinating ways. They're doing it in order to grow and to be able to optimize their growth spatially. So there may be different concentrations of food in different places. And they're also doing it to, I don't know, remain asynchronous. The synchrony of the divisions has some intrinsic sort of role. Sort of role to play in all of this. But also, they maintain a very deliberate spacing, a distribution, and they also maintain a very deliberate number of nuclei to the overall volume of the cell. Now, Grace, this is a unicorn video taken by. Video taken by a grad student years ago, and it's really hard to get anything approaching this good. And she doesn't remember how she did it. So we can still use it to show in presentations, and that's great. But they don't like to blade nice and flat like this in 2D. So Grace has been working hard to get three-dimensional video. To get three-dimensional videos of these. And there are all kinds of issues that you have to deal with. Like, look, they're growing over the time scale that we need to actually, because the division time is like 100, what she say, 140 minutes on average. So during the time scale of hours, these cells grow. And you need to be able to sort of aim this three-dimensional region that you're imaging on. You're imaging on so that they grow into it and not out of it. Oh, yeah, and you have to do this without killing the cell because the laser light that you use to image these things kills the cell. Oh, and it also bleaches the thing that you're trying to look at. So it's a really difficult thing. The way that we're going to approach the tracking, because we want to be able to track these nuclei as they divide, and we want to track As they divide, and we want to track the lineages so that we should be able to track daughters to mothers and so on. In order to get estimates for the phase along the cell cycle, because remember Grace's model is a Kuramodo model of phase, coupled phase oscillators. So we want to be able to connect that to data, and the best way I think for doing that is to come up with good models and then use Bayesian. Up with good models and then use Bayesian methods. So we've been developing some work that's been done recently on Foley Bayesian multiple particle tracking. So the multiple particle is important and the Foley Bayesian is important. Those two things together make the problem extremely difficult just to come up with a Bayesian model. Not to mention all of the tools that you would need to actually That you would need to actually access the posterior distribution. And there's been some amazing work from, I think this is a father and son team in Perth, Australia. And that's all I have. And I'm happy to take questions. Got time for a couple. Can you do the room while the audio switch is over? So just a yeah, just a couple of uh questions first. Um can you comment on the recommended neural networks and how you know in the training I had I think that's usually a common problem is usually when you have like you get these long videos you know it's a problem in the training so something that is potentially a problem for you and my second question is about the train the last function that you mentioned I just want to learn all the answers that you guys think about. Okay, so for the first question, there is a problem with recurrent neural networks getting long-range temporal correlations. And I don't want long-range temporal correlations. I think the temporal correlations we want are very short anyway. Just one frame, really. And so. Really. And so the recurrent neural network already does that quite well. In our training videos, we can make them as long as we want. So we can also limit, like not many people want to limit the temporal correlation, but we didn't have to. If we used something like an LSTM, which is sort of designed to have these long-range correlations, then maybe we would have had to actually limit that more. Had to actually limit that. And your second question is the loss function for the neural network? I think for your students, it's the loss function in Lexus. Oh, you mean for the counting? Yes, yes, yes. Okay. So are you asking how the the stochastic reaction diffusion model plugs in like a like kind of like a binary percentry though? Yes, so we're training like a pixel dense classifier and we just use cross entropy to train it. Yeah. And that's just on the image side, right? Yeah. Question? So you've got the high voltage are growing, and it looks like the nuclei are kind of getting transported as well. So does that confound? Because there's going to be a direct flow. So does that confound? Ducted low. So does that come down the estimate of the fusion channel? Oh, that's a good question. Well, I mean, in theory, no, because we subtract off the mean, so we're basically estimating the variance. So if there is any knockdown flow, then it shouldn't affect the estimates. But the flow is the time scale I showed in those last minutes is hours, right? So that's like a very long time. And it looks like those nuclei are just trucking at like 60 miles an hour. On the time scale of the GEMS videos, those nuclei are not moving at all. That fluid flow is much slower than what we do with the GEMS videos. Yeah, because it may be that there's some spatial heterogeneity to that effective term, which might. Yeah. One of the things we wanted to do was estimate that fluid flow. To do was estimate that fluid flow with the gems, but it turns out that that's very difficult. The gems are very diffusive, so you need a lot of signal to be able to. It's basic standard error, right? If the noise is very large, you need a whole lot of square root of n to get accurate estimates. So, And, you're up next. All right, great. Thanks. Yeah, really, thanks. This is a really, really interesting talk. I'm just sort of thinking out loud. I'm just sort of thinking out loud, but I'm just wondering: do you think it would be possible to do something almost like an FCS analysis on the discrete gems particles just by looking at fluctuations in the fluorescence intensity within small volumes as a way to back out estimates of the local diffusivity? That would be interesting. I haven't tried it. This is basically what they do with particle tracking all symmetry. With particle tracking ball symmetry. So, this is something they do in fluid dynamics with passive tracers. And it's interesting to think about whether it would work here. So I'm skeptical because of how noisy. How could that possibly not depend on just the intrinsic signal to noise in the video itself? But I haven't tried it, so I can't. I can't say definitively that it wouldn't work. That would be pretty cool to do. But even if you're like combining it with your particle counting neural network, so that you're basically just looking at fluctuations in the number of things, assuming that each one has equal brightness. Wouldn't that to some extent ameliorate the noise, the signal-to-noise problem? Assuming your counting is good. That's a really good point. I guess, in a way, you can. Good point. I guess, in a way, you can say that that's exactly what I'm doing with that neural network and using reaction diffusion master equation. You could sort of think of that as a way of doing exactly what you described, right? Looking at correlations of how they fluctuate through time. In a way that can deal with the intrinsic high noise conditions that we have. Conditions that we have. Yep. Yep. Yeah. Well, so unfortunately, we're going to need to move on. We've got a couple of folks that I think are like 12:30 bus today, so we can't go too late. So I think what we'll do is we'll just go ahead and move straight over to the virtual. And if anybody needs to stretch their legs in the room, we can to just kind of get us back on the time. Apologies to those who didn't answer your questions there. Questions here.