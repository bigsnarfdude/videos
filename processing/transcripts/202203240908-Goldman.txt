Thank you very much and thanks for the invitation. It's my first time in Bath, but not exactly as the usual style. So it's always a bit stressful to talk in front of co-authors because you hope you're not going to say too many things which are wrong and still manage to say something interesting. So I apologize to Martin and Dario if they don't agree with all of what I'm going to say. Okay, so I want to talk about the optimal matching problem and it covers a few recent results. With a few recent results with Luigi Ambrosio, Martin Hussman, Felix Otto, and Dari Trizan. Actually, if I don't talk too fast, which I sometimes tend to do, I'm not sure I will be able to cover the results with Martin and Felix. So I guess many of you already know what is the optimal matching problem. I'm still gonna recall what it is. So what it's called the b-parthite optimal matching problem. Partite optimal matching problem is the following one. So you take n red points randomly distributed in the unit Q and n blue points. So the red points will be the Xi and the blue points will be the YJ. And so you can choose whichever distance that you prefer. But for instance, you can look at the Euclidean distance to the power P and you want to minimize among all the possible matchings. Possible matchings. So that means among all the possible permutations, sigma, the matching cost, which is the sum between 1 and n of xy minus y sigma i to the p. And that's a quite natural problem, which appears in many instances. And it's related to other related combinatorial problems, such as the traveling salesman problem or the minimum spelling tree. And the typical questions you can ask yourself is: what is the usual size of the minimum energy? So, let's say what is the expected value of this minimum? And can you say something about the shape of the minimal connection? So, as many of you already know, this is related to optimal transport, and I guess that's why I'm here. So, this is a side which is probably quite useless in this audience, but at least. Useless in this audience, but at least it will serve to take notation. So if I define the p-vassocient distance between two measure mu and lambda by so this WP of mu and lambda where I minimize among all match among all couplings pi between mu and lambda the Passochian cost. Then of course this is a linear programming problem. So at least in theory it's much simpler than the previous one. But what I can see is that if But what I can see is that if I have n points xi and n point yi, and I look at the measure mu, which is just one over n times the sum of the delta xi, and lambda, which is one over n, sum of delta yi. So they are both priority measures. Then by a theorem of Birkov, looking at this optimal matching problem on the left, it's exactly the same as looking at the vasso-strand distance between my two measures, mu and lambda. So this is, of course, So, this is of course great news because the first problem was a combinatorial one. So, you don't have too many tools at hand to study it. But then once you move to a large distance, then you have much many more analytical tools, and so you can hope to say more. And in particular, you can extend it to more general measures. And actually, what I'm going to talk about today is a quite natural variant of this problem where now Variant of this problem where now there are only red points. So I take randomly, so I the uniformly distributed red points in the cube. And now I want to match them to the reference measure. So I want to, so I call mu to be, as before, one over n sum of the delta xi. And I want to look at the vast search and distance on the unit cube. So that's where I put this index 0, 1 to the d, between my measure μ and the constant measure 1, so the Lebesgue measure. Measure one, so the Lebesgue measure. And I take the occasion to introduce this notation here, which I'm going to use throughout the talk. What I call the WP index omega of mu and kappa, so here kappa is just a constant, is the Vassarstein distance, so the P Vasserstein distance to the P of the measure mu restricted to omega towards the constant times the. So basically, it's the restriction to omega of this problem. Omega of this problem. And here I don't really have the choice. The only constant for which this makes sense is mu of omega divided by the volume of omega. So of course, if you don't like vast soft spaces, which I guess it's not the case of anybody here, you can also see this as a purely geometric problem. It's a problem of optimal tessellations. So you want to find the, it's called the Lager cells. So basically you want to find the sets AI. The sets Ai, so you want to partition your cube zero into the D in a union of sets Ai, where uh so the sets are disjoint, so Ai intersected with Aj is empty. Each of these sets have the exact mass one over n, and you want to minimize the sum over all the cells of the integral of Ai of x minus xi to the p. Of course, the optimal transport map will send each of the Transport map will send each of the point X inside the cell Ai to the point Xi. So, this problem has many connections to various fields, such as random graph theory, probability. I mean, of course, you recognize the empirical measure here. It also has connections to theoretical physics. So, this type of problem has attracted a lot of attention in the last 40 years, maybe. And I've put here a list of Uh, and I've put here a list of people which worked on that. And there are actually a few books on this problem, for instance, by Telegram. And you see that there are people both from the mathematical community, but also from the theoretical physics. And as an example, Giorgio Parisi, which recently earned the Nobel Prize, also worked on this problem. And so for the rest of the talk, I'm just going to focus on this matching to the reference measure. Measure and also the case d equals to one is very special. So, you know, in dimension one, the optimal transport map is just the monotony rearrangement. So, there everything is explicit. I mean, it's still not always easy to describe what's happening, but it's a quite special case. So, I'm only going to talk about dimension two and higher. Other questions so far? I think there's not much new. New okay, so uh so I've presented the problem on zero one, but actually, I think it's a little bit easier since we're going to look at different scales to blow up the picture and look at what's happening at a scale where the points are typically a distance one from one each other. So I'm letting L to be n to the power one over d, and I make the change of variable y equals to lx. And so that's Lx. And so that means that now points are typically at distance one from each other. And my original problem is the same as looking at the following one. So I get one over L to the D times the vast ocean distance between mu and one, where now mu is just the sum of the Dirac masses at the point xi. And xi again are IID uniformly distributed in this big cube. So in So, in these variables, what I'm going to call the micro scale is scales which are of order one. The system size or the macro scale is big L. And everything which is in between is what I would call mesoscales. So, the first question you can ask yourself is that, okay, if I look at the expectation, what is the typical, so what is the scaling? Typical, so what is the scaling law for this expectation? So, how does it depend on L? So, first naive guess is that okay, points are at distance one from each other. So, each point should pay order one of transport cost to be sent to corresponding set AI. And so, since I have LD points, the total vaster shining cost should be of order LD. And so, when I divide by the volume, I should get something of order one. I should get something of order one. However, this is not exactly the case, or at least it is the case in dimension three and higher. But in dimension two, something very interesting happens, is that you have a logarithmic collection. And I think it's really this logarithmic collection which attracted a lot of interest on this problem because it was somewhat unexpected. And in particular, the fact that you have this P over two and not P. I mean, I think if you think a bit less naively, Less naively, you can come up with the fact that in dimension two, something different should happen, and maybe you have this log to the P L, but actually, the right power is P over two. And that's because there are some cancellations over the scales. And so this behavior was first proved and observed by Haiti, Commotion, and Tusnadi. This is really one of the founding papers in this field. This field, I would say it suffers a bit from its shortness. I mean, it's not very easy to digest a paper. Some statements are quickly phrased there, but okay. So after that, there was a lot of work, for instance, by Talagrand and others to study this type of scaling. So once you know that this is the leading order behavior, you can move. Is the leading order behavior? You can wonder if L is very large. Are this bounds sharp? So, for instance, in dimension three and higher, does one over L D Wasserstein distance between mu and one converges to a given constant when L goes to infinity or not? And same thing if I divide the dimension two, my cost by a log to the p over two l, does it converge to something? So, to talk about that, I want to. I want to talk about PD on that, which was recently proposed in this paper by Caracho and collaborators. So, this is a physics paper, so things are not completely rigorous. But what they claim is that at scales which are much bigger than one, so I say mesoscopic scales, if I look at my measure mu, so the sum of Dirac masses, since the points are IID, this should. IID, this should look like the Lebesgue measure. And so all I should be in some kind of linearization regime. And therefore, all the quantities which are depending on mesoscopic or macroscopic quantities or mesoscopic scales should be very well described by the linearized problem. So I remind you that at least when p equals to 2, the Two. The theorem of Jan Brunier tells you that the optimal coupling has a very specific form. So it's t times identity push forward levy, where t is the gradient of a convex function. So this function is solving, at least formally, the Mongeon-Paire equation, which is a determinant of Hessian of psi equals to one over mu. And of course, here a mu is a sum of Dirac masses, so this doesn't make too much sense, but okay, this is just a formal computation. Formal computation. So let's see what is the linearization of the Monge-Pi equation in this case. So since here μ is close to 1, we expect the transport map to be almost the identity. So you can write that size of the counter of each potential is equals to half x squared minus phi with phi, which is small. And assume also grad phi is small. So then you can so you write that determinant of the Hessian of psi. That determinant of the Hessian of psi, this is an equality, so it's the terminal of identity minus Hessian of phi. And now, if you do the Taylor expansion of the terminant of identity, which is one, minus the trace of d2 phi, so that's the Laplacian of phi. And on the other side, you write that mu is one plus mu minus one. And then if you do a Taylor expansion here, that's more or less two minus mu, and you obtain the And you obtain the well-known fact that the linearization of the Montgomery equation next to around the Lebesgue measure is just the Poisson equation. So your Montreal-Pair equation becomes La Passion of phi equals to mu minus one. And then the cost, so the integral of T minus X squared, becomes the integral of grad phi squared. So this is a, I don't know, maybe a complicated way to say that the linearization of a var solution space around Of a vast space around Lebesgue is the space h minus one. So that's a very well-known fact in optimal transport. You can already find it in the first book of Villani, for instance. And it has been used a lot lately. So there are ideas that they want to use this Anzat, so you replace the vast soft shine distance by the H minus one distance to compute what should be the cost, at least in Cost at least in dimension two, and at least for p equals to two, I would say. So, doing that, they predict among many, many other things that, so in dimension two, you should have that the expectation of the this Lasserstein distance is one over two pi times log L plus a correction which is of order one, but they don't know what's the value, plus something which is a little low of one when L goes to infinity. When L goes to infinity. And in higher dimension, they have something which is very funny for us as mathematicians. So they say that there is a first order which they don't know, and some value which they are not able to predict, but they are able to predict the second order and say that second order should be this zeta of one constant divided by four pi squared times. So now the correction should be like one over L to the D minus two. be like one over l to a d minus two of course you see that when d is equal to two this is of order one and then you get this logarithmic plus something small and the this zeta d of one it's the analytic continuation of the zeta function at one and there are some related predictions for p different than two but that's even less clear what it means so of course this is a very appealing on that This is a very appealing onset, especially if you like PDEs, but it's quite hard to make riborous. In particular, if you have a Dirac mass, it's not in H minus one. So it's H minus one norm is infinity. So if you solve a Laplacian of phi is equal to mu minus one, then the interval of grad phi squared is plus infinity. So it's not very clear what you should do there. And maybe more importantly, the Monje-Per equation is a fully non-linear and very degenerate. Fully non-linear and very degenerate elliptic equation, and so it's difficult to justify this linearization. But so, nevertheless, so the main theorem, so what we know so far about this asymptotic behavior of the expected value is contained in a series of works. So, in dimension two, this is essentially due to Ambrosio Stra and Crevison. Uh, Stra and Crevison is that at least if you look at the case p equals two, then they found the leading order term, which is exactly the one predicted by the physicist. And then there is a correction, which unfortunately is still of order log log L. They're not able to prove that this should be of order one. And even less so proving that if you subtract one over two pi log L, you converge to a constant. And in higher dimension, you can prove that. higher dimension you can prove that the this expected value is going to converge to a finite value f so which depends on p and d and this holds now for every p larger than one and every d larger than three so basically that's what we did with dario okay so i want to describe a bit uh the proof of ambrogio's tri and trevison because uh it really builds on the this uh characello and i'll uh on that so it's I'll Anzat, so it gives a good justification, at least on the macroscopic scale, of this Anzat. So, what they came up with is that the good way to go around this difficulty that the H minus one norm is infinite is that you need to modify your measure, and they use to modify them the heat equation, and you have to modify it at scales which are much larger than one, and so this. And so this suggests somehow that if you look at the displacement, so at t minus x, then this guy should be very close to your grad phi, so the one from the linearized problem, in some weak topology. So after convolving, they are close by. And we'll see that this will appear also later on in the talk. And to construct a coupling, so let's say for the upper bound in the theorem, they use a In the theorem, they use the so-called Daco-Ronier Moser construction. And for this, I remind you the Benamou-Ronier formulation of optimal transport. So you can write this Lagrangian problem in Eulerian formulation. So it's the same thing as minimizing over all pairs rho and j the kinetic energy. So it's the integral on my cube, zero L to the z of the integral in time of one over rho j square. square and what are this rho and j so they are connected through the continuity continuity equation sorry so dt rho plus divergence of j is equal to zero and what's the link with uh mu and lambda is that your curve rho so it's a curve of measures it has to start from the measure mu so at time zero you are equal to mu and you have to end up at time one in your measure lambda okay and you can prove that the two problems coincide Two problems coincide, and so using that, we can construct a competitor for the Beninou-Brunier problem using the solution to the Poisson equation. So, for this, you just look at the linear interpolation between Lebesgue measure and mu. So, you take rho to be one minus t mu plus t. And then you take j, so the flux, which is the gradient of phi, where again phi solves Laplacian of phi is equals to mu minus one, I guess. Mu minus one, I guess. And so you can check that if you look at d t rho, that's one minus mu. And if I didn't make a mistake in the signs, then divergence of j is La Persian of phi, and then the sum should be equal to zero. Okay, so that gives you an estimate from above of the Vasso-Than distance by the H minus one norm. So of course, here there is a constant, and in theory depends on. In theory, it depends on if you don't do it very carefully, you could think that that could blow up because μ is singular, but actually, you can pull that up to a constant. You can bound it by this red phi square. And so, to get the sharp constant, you have to do this more carefully, but that's basically the idea. So, in higher dimension, you expect that the leading order cost to the transport is given by. Order cost to the transport is given by the small scale displacement. So you expect that things are going to move of order one. So you cannot really expect that this Anzatz will give you really the answer. At least it will not be able to give you the sharp constant. So the idea, which is quite natural, is to use sub-additivity. So something which has been used in other contexts in mathematical physics when you want to prove the existence of. Of thermodynamic limits is to use the sevalativity property of the problem. So here we want to use the fact that if I look on a cube of size 2L, then if I look at a subcube of size L, then my measure mu, so the restriction to the subcube, looks very much like the The random measure on size L. So, if I don't know if this is very clear, but basically, if I just cut out a cube, then what I see there as my random measure, this is very similar to what I had if I just took, so let's say the cube is of size L. This is very similar to what would happen if I take a L D random point uniformly distributed in this box. And so you can see that somehow the And so you can see that somehow the problem has this invariance on the scales, but it's not exactly, I mean, it's not completely exact. And so for this, you would like to say that, okay, I will take, so I mean, formally, you would like to say that in order to construct a competitor on the big cube, I would like to paste together competitors on the smaller cubes. That would give me an upper bound because I have the stability property of the Vassocian distance. So in order to have really this investment. So, in order to have really this invariance over the scales of the random measure, it's easier to replace the deterministic number of points. Because, of course, if I look at a subcube, then the number, so even if I have a deterministic number of points on the big cube, so if I have two to the L D points on the big one, there is absolutely no reason that on any of these subcubes I will have exactly L to the D points. So, in order to have something which is more invariant, it's easier to work with. Invariant, it's easier to work with Poisson number of points. So, this is often called replacing the canonical ensemble by the grand canonical one. So, the number of points on the big cube is following a Poisson random variable. So, now if I restrict a Poisson point process to a subcube, then it's still a Poisson point process. But now the problem is that, sorry, let's say. Let's say, let's think for a second again about the deterministic problem. Even if I had two to the L D points on the big cube, so I wanted to on the big cube, I want to match it to the Lebesgue measure with constant one. Then on the smaller cube, there is no reason to have exactly L D points. So then there I cannot really match I mean basically the constant to which I want to match it don't don't match. Excuse me the the joke. Excuse me, the joke. And so the solution for that is to relax the problem, not impose that you are going between one and the sum of the direct masses. And so indeed, the idea is to treat the defect in the number of points as a local mass defect. So say that on each of the subcubes, I'm not going to match it to one, but I'm just going to match it to the corresponding constant. So maybe they are slightly different, but on each of the subcubes. Different, but on each of the sub cube here, and you see it in the next slide. So, in each of the cube, I will match it to the number, to the constant which corresponds to the number of points in that cube. Then I will try to patch things together. So, I define kappa as before as the number of points on this cube of size L divided by the volume of the cube. And now my measure, so my function, sorry, f of L is this. Of L is this expectation of one over L D times the vast ocean distance between mu and kappa. And of course, in case that has exactly L to the D points, that's just the problem I started with. And so, as I said, the idea is that I have a cube of size 2L and I divide into two to the D cubes, Q1, Q2. Let's say here there are four of them. And so on first on Q1, I send my points to their My points to their corresponding Lebesgue measure with density kappa one. On Q2, I do the same thing to kappa two and etc. So now I have these four big cubes and each of them has a constant density on each of the cube, but they could be slightly different. And then from this guy, I go to the one with, let's say, uniform constant. And now I'm claiming that doing that. Claiming that doing that, I can estimate very well the cost because since to go from here to here, I use the fact that I am back to a matching problem on a cube of size L. So I use the that I already know how things went on that scale. And then here, this will be very cheap because now the constants are very close to each other because the number of points is very close to the volume. So in formula, So, in formulas, so if I want to estimate the cost on a cube of size 2L using triangle inequality, okay, since here I look at a vast organs p to the p, I need to then do a young inequality. So I have an extra parameter epsilon. But you can think of epsilon being equal to zero and just here have a constant, but that doesn't exactly work. So the energy, so the yeah, the energy on the cube of size 2L. Energy on the cube of size 2L. I can bound it by one plus epsilon times what's happening on the size L. So this is just from the sub additivity on the top part plus the cost to go from this locally constant to constant, which is what I will call a global term. So this is kind of the local error on the left and the global error on the right, which is the expected value of this Wasserstein distance to go from locally constant. From locally constant measures to the constant one. So now the aim is to estimate this global term. So to do that, sorry, this is a slightly technical slide, just to give an idea why it's working. So to do that, we can use again this Beninou-Brunier formula and the same type of estimate. So to estimate the Wasserstein distance by the, here will be the W minus 1p distance, but okay. So you can estimate it using the same. So, you can estimate it using the same trick as before by the integral of grad phi to the p, where phi solves the corresponding Poisson equation. So, Laplacian of phi equals to the sum of the kappa minus kappa on each of the q. And here, for instance, using a Calderon-Sigmund estimate, we can estimate this L P norm of the gradient by the L P norm of the right-hand side and just by scaling. And just by scaling, you see that you need to have a L to the P appearing. So, okay, if you don't like Calderon-Sigmund and elliptic estimates, you can just forget that. But basically, you can estimate the vast ocean distance by L to the P plus D times the sum of this kappa I minus kappa to the p. So you know by, for instance, central limit theorem or things like that, or concentration inequalities, that the number of points in a cube of size L, so if you have A cube of size L, so if you have a Poisson point process, it should be exactly the volume plus the correction, which is of the order of the volume to the one-half. So, here, if I look at the number of points divided by the volume, this should be like one plus the volume to the minus one half. And so kappa i minus one should be of the order of L to the minus d over two. So, if I plug this in here, so I was divided by the volume, so I was Divided by the volume, so I have so I have this one over L D which cancels with the L plus L to the P plus D. So now I have L P times L to the minus P D over 2. And the nice thing is that now the P, you can factor it out and you get exactly L to the minus P over 2 times D minus 2. And you see that this term, so for L large, is going to be small as soon as D is larger than 3. So 3 or larger. So now So now I can plug it in my sub-additivity estimate, and I get the estimate on the top here. Then, okay, you can optimize in epsilon if you want, and you get that f of 2L is less than F of L plus a small error term. So, if there was not this error term, you know that a function which is decreasing and bounded from below is converging. So, here we can adapt a bit this fact and obtain the same. A bit this fact and retain the same thing that this limit exists. Okay, other questions so far? No. So like 20 more minutes, I guess. So using the same type of ideas and actually mixing a bit the sub-additivity arguments with arguments from this paper of Ambrosio Strait and Tribison, we were then able to prove with which Ambrosio. Prove with Ruigi Ambrosio and Diet Raison the following facts. So now this is in dimension two. If I take omega, a connected and bounded Lipschitz domain, then I can also study the case when the points are not taking uniformly, but are taking according to another law, which is rho. And here the hypothesis that rho is older continuous and bounded from above and below. So I think maybe elder. Below. So I think maybe Elder is a technical assumption. I would assume that if it's just bounded from above and below, this should be true, but not able to prove it right now so far. Is that if I look at the same, so okay, since here my law is not uniform, I need to go back to the original scale when the macroscopic scale is of order one. So if I do the correct rescaling, so I knew that the energy was of order 11. So, I knew that the energy was of order log n over n. So, if I remultiply by n over log n, then the limit of the number of points going to infinity, then this converges to a volume of omega divided by 4 pi, which is exactly the same as in the case of the uniform measure. And this was actually conjectured not that long ago by Benedetto and Cayotti. And in that paper, they proved the upper bound. So they proved that this thing is less than omega over 4 pi, but only. Of a four pi, but only in the case of the unit cube. And actually, their proof is quite similar to the proof we had with Dario, and it used also these server utility ideas. So maybe a couple of words about the proof. So for the upper bound, we want to use the sub-additivity a bit in the same spirit as before. And we want to say that locally, rho should be close to. Locally, rho should be close to constant. So, locally, you want to use this ambrosia-strain-trevison result that on 0, 1 squared, you know exactly the right constant. But the difficulty, as compared to the setting of Benedetton Colliotti, is that in general, if I have a Lipschitz domain, I cannot divide it exactly in a finite number of cubes. So I cannot exactly do this stability argument. This stability argument because it actually works really well on cubes and not so well if you have different objects. And so the idea was to use a Whitney partition. So it's a partition of your domain, which is refining close to the boundary. So you have a partition in cubes, but now the number of cubes is infinite. And if you look at the error term which you get by doing this Francari or Calderon-Zyngmund type estimate, Basically, you have an error term which is something small, but times the number of cubes. And of course, if the number of cubes is infinite, this error term is infinite. So this forces you to be much more careful. So it's quite more technical. And so when you treat this global term, you need to understand much better and do something quite strong. Maybe the more interesting part, I think, is the lower bound. Is the lower bound. And for this, the idea is to use a sub-addit, sorry, super-additive quantity. So do the same type of arguments as before, but with a super-additive quantity. So this is something which has already been used in this topic by other authors. For instance, if you go to see the paper by Bart and Bardenard, there is something similar. And so for this, we use this modified Basso-Stein distance. So B is for Barnes. Pass of steam distance, so B is for boundary, which was actually introduced in a paper by Figali and Gilli quite some years ago. So now, so it looks very similar to the usual Vasertan distance, but now on the cost I go all the way to the boundary. But still, my conditions on the marginals are just inside the domain. So that means that I'm allowed to exchange mass with the boundary, and what I'm going to pay is just the distance to the boundary. And how much mass. To the boundary, and how much mass I send to the boundary or take from the boundary that's up to me, and it's not very hard to check that this quantity is super additive. And so, using that, okay. And so if you can define the quantity which is kind of the analog of this thermodynamic limit, so the one which okay, so in the paper of Ambrosio Strain Trevison, they're. Paper of Ambrosio Stream Trevison, they proved that one over log L is expectation of Vassofstein 2 divided by L squared goes to 1 over 2 pi. And now you can look at the same quantity, but for this boundary problem. And so you can use this concept. Sorry, I'm a little confused about the boundary problem. Should the constraints be inequalities instead of equalities? No. Sorry. No. No, why? Okay, so the mu and the lambda have the same mass or not? Well, they don't have to. Here they have, but they don't have to. And the mu and the lambda are defined on the interior of the domain. Yes. Basically, you can think that you are allowed to add some measure. I don't know how you call it. Let's say eta one on the boundary of omega, which is going to be a source, which you can. To be a source which you can use to send more mass inside, and then you can add another uh measure lambda eta2 on the boundary, which is going to be a sink, and you're allowed to send some mass to that. Okay, what you're going to pay is just kind of the distance to the boundary squared. Is that clear or? Uh okay, so the the main point here is to prove that the two constants uh agree. To prove that the two constants agree, so to look at the original problem or the one with where you allow change of mass with the boundary, and so this way you prove that your lower bound and upper bound coincide. So this kind of ideas have been used in other concepts, more like when you use this h minus one type of distance. And this goes by the name of JK-Neumann bracketing. So basically, you can think of this You can think of this boundary version as a kind of a Dirac Lay analog of the Vasso-Shan distance, which should be thought as a Neumann type problem. And okay, I don't have too much time to explain why, but one of the ways you can see that is that if you look at the Versostein, so the gradient flow of the entropy for the usual Vassarstein distance on the domain, you would get. On a domain, you would get the heat equation with Neyman boundary conditions. And if you do the same thing with this new boundary version, then you get heat equation with the directly boundary conditions. That's actually the motivation for introducing this quantity in this paper of Figarian G. And so that was kind of the first time that this type of result was proven in the context of vast ocean distances. So okay, so uh are there more questions on that part? It's more or less okay. Okay, so uh in the last uh 10-15 minutes I want to I want to go back to this uh onset of uh Caracolo and collaborators and we would like to uh justify it on a let's say mesoscopic scale so prove that uh if I look at the displacements so t minus x that it should be close to my graph. It should be close to my grad phi, where phi solves the Poisson equation Laplace and of phi equals to mu minus one. So now I'm looking at a scale R, which is a mesoscopic, so it's very large compared to one. And so in order to be able to state my result, it's easier to scale back so that that scale is of order one. So I change, sorry, I'm changing again. So now I said that my points are R times points Yi. points yi and in this way i'm looking at scale one and i want to send this r and l to infinity so now the optimal map and then by scaling so i'm now looking at the optimal map tr between uh to this uh new measure mu r which is just one over r d so the the parameter one over r d is just so that you have something which is of density one locally so i want to send this guy to Locally. So I want to send this guy to LeBron. And the answers of Caracolo and collaborators tells you that. So I use this R d over 2 to be the CLT scaling. So if I multiply my displacement T minus X by R to the D over 2, this should be close to the grad of phi R. Where is phi R now solves Laplacian of Phi equals to R to the D over 2 times this mu r minus 1. And so I did this scaling here. So, I did this scaling here because, by a central limit theorem, I can see, at least formally, that this measure r to the d over 2 mu minus 1 should be close to white noise. So, I would call w here when r is very large. So, for instance, if you test this against a test function, it's not very hard to see that you can apply a standard CLT theorem to show that you converge to a white noise applies to a test function. know is applies to motors function and therefore if i define the curl free gaussian free field grab psi so i want so what i'm interested not in really in psi but more in this quantity grab psi where psi is the solution at least formally of the equation laplacian of psi is equals to white noise and what we expect is that for r very large my displacement so t minus x multiplied by r to the t over two should be Over two should be close in some sense to grab psi. Okay, I hope this was not too confusing. And so basically, our main result with Martin was that this is true in dimension two and three. So, okay, so we did it in a slightly different context because we didn't want to have to deal with boundary conditions. So we look at the periodic setting. So we replace the cube by the tor. Cube by the torus so R divided by L Z to the D. And I have to introduce some more notations. So I am fixing a smooth cutoff function eta. And what I call eta r is the usual rescaling of eta. And what I denote by phi index r is the convolution of the function phi by eta r. So it's the convolution at the By eta r, the convolution at the scale eta r of phi. So maybe the statement is slightly easier to understand in dimension three because kind of independent of the notation which I just introduced. So in dimension three, you can prove that this displacement, so R to the d over two t minus x, is converging locally in law to my curve-free Gaussian-free field Krebsi. And now in dimension two, there is this logarithm. Now, in dimension two, there is this logarithmic divergence. So it's impossible that R to the D over 2 T minus X converges to something because it's going to be logarithmically diverging. But what I can prove is that if I subtract the right shift, so this, so here I recall that phi r is the solution to that equation, so Laplacian phi r equals to r d over 2 mu minus 1. So if I subtract the convolution of grat phi at scale 1. Of grad phi at scale one from my displacement, then this guy is going to converge to exactly this grad psi minus the same subtraction of the shift, so minus grad psi one of zero. So here I'm cheating a bit. Actually, the statement is for, let's say, the inverse map of T, that does not really exist, so you have to write it for the optimal coupling. The optimal coupling. This is for technical reasons, but it's just a bit easier to state it like that. And what I mean by convergence in law is convergence in some appropriate negative Sobolef space. And of course, this leaves the open question of what happened in dimension four and higher. And for those it is still not clear. I still believe it's true for every dimension, but I think Martin had some doubts. I don't know. Still an open question. So. So, what's the main ingredients of the proof? So, the proof is made by two parts, which are quite independent. So, the first one is really a linear part. So, you look at this object rat phi r. This is just the solution to this Poisson equation. And you can prove that this object converges in law to the corresponding graph psi. And this actually works in every space dimension. And this actually works in every space dimension. And of course, in dimension two, you have to subtract the correct shift. And then you need to prove the linearization part, which tells you that my displacement is close to this quad phi r. And so for this, the main estimate is the one here, which is in the middle of the slide. And so for this, you can, for simplicity, you can just write it for a Just write it for a big r equals to one. Then you just read as follows: that if I look at my optimal displacement, so t minus x, this is a function, I convolve it at scale r, and I look at what happens in one point, let's say zero. So this object here is stationary, so it's going to be the same thing at every point, but for simplicity, let's look at zero. Then this guy is close to the convolution at the same scale r of grad phi. Scale R of grad phi and the rate of so how close it is. So here I look at the L P n of this guy is less basically than one over R. So here beta it's one for dimension three and higher. That comes from the fact that the expected value of my Wasterstein distance divided by L D was a four to one. And of course in dimension two you need to have this logarithmic correction. So in dimension two and if r is large enough this is just a log of r. Large enough, this is just a log of R, and using that and a bit of negative sober spaces machinery, you can go from this to a quantitative bound on the distance between T minus X and grad phi in this negative sober spaces. But this is really the main estimate. And so now the question is: why does this tell you that t minus x is close to grad phi? And also, And also, why is this restricted to dimension two and three? Because, okay, if you want to say that these two guys are small, then one over R needs to be much smaller than the typical value of grad phi r. And it's not very hard to check that the expected value of the L P norm of grad phi r should go like one to the r to the one half d minus two for all bigger than one. Much bigger than one. So, if you want that my error term here, one over r is much smaller than the typical size of gratify r, you want that one over r is much less than one over r to this one half d minus two. And if you check, this directly gives you that you want d to be less than four. So the thing is that if d is larger than four, you still have this estimate, which is still interesting because a priori you don't know much about the typical size of this t minus x. But in the estimate, But in the estimate, you can just cancel the graphier R because this guy is actually going to be much smaller than the right-hand side. So, this doesn't really tell you anything about how close they are. So, if you want to go to higher dimension, you need to improve this bound in dimension four and higher. That's the main point where we don't know how to go to higher dimension. And so, in a couple of words, the main ingredient for this quantitative Quantitative linearization bound is actually a deterministic result which we obtained with Martin and Felix a couple of years ago. And the idea is to kind of inspired by quantitative stochastic homogenization. And it's to obtain a large-scale regularity theory for Mahjon-Pair equations. So this is a purely deterministic statement, which says that if I have a measure mu, which is QL periodic, and let's say it has. QL periodic, and let's say it has a correct volume. So the volume is, so the number of, sorry, here that it's not a question of number of points, but the total mass of mu is L to the D. And if I look at the solution to the Poisson equation on my torus, the Laplace Fi closed to mu minus one, if I assume that on the larger scale, so on the scale L, my measure mu was close to one, so that means that I'm the linearization regime at the largest scale. At the largest scale, so in this form here, so if you want intervention three and higher, just that one over the volume times the Vassocian distance is of order one. And if now at every scale between one and L, my measure mu is flat in the sense that it's locally close to the appropriate constant with the same type of estimate. Then you can prove the same statement as before, meaning that. Same statement as before, meaning that if I look at t minus x or the displacement and I convolve at scale r, then it's co it's closed close to grad phi convolve at same scale r and with this rate one over r so uh what's the very rough idea of the proof? So it goes through a component of scheme and the main point is to propagate a global information. So you start with the fact that your map, so t minus x. map. So T minus X is small in the L2 sense on the BQ. And then you want to go down the scales and use that. You can propagate that on smaller scales, provide you are still in the linearization regime. And you encode that in the fact that you assume that locally your measure mu is still close in Vashan distance to one. And you have to combine this with a quite delicate linearization argument. Delicate linearization arguments to identify the resulting global shift and prove that you can really take this gratifier R0. Probably I said too much in this talk, so I think it's a good place to stop. And thank you for your attention. Thanks a lot for the talk. Do we have questions or remarks? So I don't know if you see the chat or only the questions left. Okay. Thanks for the nice talk, Michael. Um so so do do I understand this this the Richlin Neumann bracketing correctly that you can transport over the boundary for free? Transport over the boundary for free? No. I mean, for this Dirichlet-like problem, because I mean, you can add, kind of, you can move mass to the boundary for both. So my question, can you put it differently? Is it kind of the periodic version? Is it in between this would be in between the two, right? Because you can. So this means that the. You can so this means that the periodic version converges to the same constant, which uh yes, yes, yes. But I mean, in the periodic version, they already proved it in the first paper by Daniel and so on. Yeah, yeah, but I mean, this would be another consequence here, yeah. But I mean, this is a very uh somewhat particular because, in this case, uh, you can use this uh kind of H minus the linearized problem, and there you can do all the computations, and then you just. And then it just happens to be the same constant in some way. But yeah, right in the first paper, they could prove that on the torus you get the same constant. I mean, basically, they proved something much more general: that if you take a manifold. Manifold, any compact manifold or one boundary, yes. Uh, then you get the same constant, and then they treat a bit differently the case of the square. And one other question: so, essentially, you need the kind of lower and upper bound in this result with Dargo and Ruji. You need the lower and upper bound on the density to exclude clumping. Clumping. Well, yeah, I mean, if your measure vanish, so imagine you have two connected components, even if you have row equals to one, but on two balls, digital, then the scaling is not the right one. Yes, yes, yes, exactly. You have a fluctuation of square root. Yeah, square root of the volume of points, and then you need to send them far away, and that's much more expensive. Far away, and that's much more expensive than you get here. So, this is inevitable. So, yeah, actually, maybe I can use this occasion to point out something. So, to prove the yeah, so at some point in this argument, you need to. Yeah, so at some point in this argument, you need to show that if you have a measure which is close to constant in some sense, then the problem with taking the points randomly according to this law or taking them uniformly are close by. And to do that, we used the construction which is exactly the same as this, how it's called, the one by Milman and Kier. The one by Milman and Kim, which was already described, I think, by one of the talks by Dan. So we discovered after that that was the same map. I saw a talk of Max Fatih like two weeks ago, where he was talking about a joint work in progress with two of the speakers of this week. We discovered that we used something which is already known before. Something which is already known before for this map. Just find that it resurfaced there. And because here it was important that to have a transport map, not necessarily optimal, but whose Lipschitz norm was close to one. Yeah, okay. Thanks. Are there more questions? So, if not, thanks a lot again for the nice talk. And I guess we resume in half an hour.                               