Thank you very much, and thanks a lot for organizing such a wonderful workshop and for inviting me to come. Today I'm going to talk about cost efficiency. This is a joint talk with Guerrot, who's a professor, a GM professor in Germany. Thibault was a postdoc at some point with us and moved to the industry in Switzerland, and Steven who's also here. And Steven who's also here in the room. So, what I'm going to talk about today is the first part is about cost efficiency and trying to explain to you if you're not familiar with this notion and the way we define it in normal math finance. And then, cost efficiency: the idea is to try to find a payoff that has the smallest costs but achieve a given distribution. You're interested in a distribution, you want to achieve this distribution at some time in the future, and you want to do that at the smallest cost. And this, the standard setting, when we talk about cost efficiency, we have a fixed distribution we want to achieve, and we have only one probability measure, so there's no probability measure on this distribution. And the goal of this paper is to try to extend this notion and say, what if you don't have one product? If you don't have one probability measure, but several of them. And if you're unsure about the probability measure, then you're also unsure about the distribution of your future assumption or future claim. And so you have a family of distribution, depending on the probability measure that will be your key. It will give you a different distribution. And so, what we try to do then is to construct. Do then is to construct a pair of that is not achieving a distribution F0 because we don't know, there's no unique distribution at the end, but that is dominating F0 for some specific order. So we have kind of a dominance already. So this problem looks maybe strange to you to try to find this optimization and try to solve this cost efficiency problem. But then I will show you in the last part that this is directly related to robust. Is directly related to robust portfolio choice, like Gilbert Smeiler, where you have a maximum expected utility, for instance. Then, the solution to a maximum expected utility problem is also a robust cost-efficient payoff. So, the solutions that we will find here by introducing some ambiguity on the probability P are going to be solutions to this robust portfolio choice. So, that was mostly our motivation. Our motivation. So, first thing I want to clear is that there is no problem with arbitrage. So, you can have two payoffs. Think of if you don't like payoffs, you can think of consumptions, slightly different setting, but it works the same. That have the same distribution in the future, and you're looking at their price today, and they can have different price. So, in the real world, you can have two times the same distribution in the future. Have two times the same distribution in the future and two different prices simply because, like, you can take, for instance, SP 500, its median. The probability that it goes above the median and below its median is 50%. But still, they can have different costs because this is the real world probability. In the real world, there's 50% chance to be above or to be below. So, if you take these two payoffs, one that pays if the ST goes down, one that pays if the SAT goes up, these two. If the SLP goes up, these two have exactly the same distribution, they're only 50%, but they have different costs because, in general, the Wisconsin probability will be different from the real world probability. So, and there's no arbitrage. You can have these two pairs. So, it has nothing to do with arbitrage. So, if I clear this, now I can go to my main assumption and to the yes, feel free to interrupt if you want. No, no, no. Okay. So, but if you have So, but if you have any questions, you can interrupt me. So it is better if you interrupt me and to try to keep following getting lost. So what are the assumptions I'm going to make? The one period setting. Time zero, investment horizon is T. I'm going to assume the market is arbitrage-free, but not that it's complete, so there might be multiple prices. Multiple pricing risk measures. But at least at the beginning, without ambiguity, there's a unique probability measure, and everybody agrees on it. This unique real-world probability measure, there's no ambiguity on it. I'm assuming you cannot dynamically trade, but that all calls and puts with this maturity are traded. And if you know that all puts and all calls are available, then you can. Available, then you can show, in fact, under some assumption like square integrability or things like that, that if you take a function of the underlying price, there is a decomposition as a linear combination of these calls and puts. And so if the cause and puts are in the market, then you can approximate and replicate any of these functions of the other eye. Of the other right. Yes. Just not to clarify. Do you assume this probability matter in the real world is observable? Yes, for now it is observable. So we know exactly the stock price, what distribution it has. Maybe it's log-normal, you know the drift, you know the volatility, so you know perfect, you have no ambiguity on it. At least at the beginning, in my first part when I defined constitutions. And this S is just a function of Just a function of this maintenance delivery. So, S is the stock on the market, an index, it could be SP 500. And then what we are going to consider is all payoffs that are linked, like all options, if you want, on SP 501, root links that are linked to the S P 501 at the time T because all of these are replicable. You can Replicable. You can replicate as a static portfolio of calls and puts, and you can also write them because you can replicate them, their price is unique. And so even though there might be different probability measure Q to price this payoff, so this is a withdrawal probability where you price the discounted payoff, for any Q, if this thing is replicable, the price is unique. The price is unique and is equal to this price, which I'm not going to write it as an expectation under the risk-led probability. I'm going to write it as an expectation under the real-world probability, where I use likelihood ratio to reflect the change of measure between P, the real-world probability, and Q, the with control probability. And so one of these terms, they Q. So one of these terms that Q can be written as a function of st. And so the price can be written this way. And so now I can come to my problem, which is to find over all possible claims that have a given distribution in the future, which one has the minimum price. So minimum cost that is calculated as this expectation of the Expectation of the future claim discounted, if you want. 1 over L T is also the stochastic discount factor if you want. So more used to this stochastic discount factor, you can interpret 1 over L T as the stochastic discount factor. I'm going to say that a payoff claim is cost efficient if it solves such a problem for a given probability distribution. This comes back to a paper in 1988 by Kim. In 1988, by Phil DeBick, he had two papers that were related to this simple problem that is solved in a complete market by normal model. Yes? Q is the primitive model. You can't influence Q. You cannot influence Q. Q is given by the price of gold and puts in the market. So prices are given and you can infer the wisdom probability from the prices that you From the prices that you observe, the option prices. So there's no issue with this one. And so now this problem is actually straightforward to solve. As soon as L is continuously distributed, there is a unique solution that can be written this way. So if you see something like this, I'm most certain that most people find it really easy to to see. It's really easy to see, just in case. This expectation, you can see it as the expectation of a product. And so it's linked to a correlation, if you want. And so if you see this, it's more or less minimizing the correlation between consumption and 1 over L. And this will be obtained if the two are negatively correlated as possible, anti-monotonic. So we want anti-monotonic coupling between the two X and one over L. So a commonotonic coupling. So, a commonotonic complaint between X and L. And so, if you look at this formula, L is continuously distributed. This is the C D F of L. So, this is a uniform. And F minus 1 of a uniform has the right distribution. And in particular, that's the only way to be increasing in L. So, this is increasing in L. It's a unique solution. There's nothing really complicated here. So, that's how you solve the cost efficiency problem with no omega. With no ambiguity. So now, what if we have more than one probability measure P? So we don't know exactly the probability measure P. We have now a set of probability measure. And we want to solve this problem, except that this problem doesn't mean anything because it depends on the probability measure that you're looking at. Now we have different probability measures, so the distribution of X changes depending on the probability measure P. So you can't solve this. So you can't solve this problem. So what we do, we construct a bigger set than this one. We look at the distribution of x under P. We assume we have a reference distribution, which will be our F0. And we're going to say that with this ambiguity on P, we want to be at least as good as our probability distribution for some order. And we use a stochastic order, and it tables stochastic order. integral stochastic order. What is an integral stochastic order? Same, I think lots of people would know that. It's simply that you have a family of functions. So for instance there are non-decreasing functions. And then you say for all f in this family, this integral is less than this integral. And then you have an order, which is the first order stochastic dominance between f and b. If you take increasing and concave, you get second order stochastic. Increasing and concave, you get second-order sugars, and so on. So, what is the problem we want to solve? In fact, is this one where we look at all payoff such that for all probability p in our uncertainty set, we dominate the target distribution in first-order stochastic domains. And so now the cost-efficiency problem, which I call the robust cost-efficiency problem, is still a minimum cost. We have only one. Minimum cost. We have only one reset for probability that comes from the options traded in the market, and we have a family of payoffs that belongs to this set. This thing is actually related, which I was telling in the introduction, there is a relationship with the maximum utility setting. So assume that you can find the least favorable measures, and then we've Favorable measures. And then we've, I think, this is something that also appears in the literature that we discussed in the previous days. So the idea is to have a probability L star, like you do ratio, such that it's minimum in this ambiguity set with all the probability measure P. So I look at the distribution of L star and all P's, and I All p's, and I'm looking for a probability p star in this set such that this thing is minimum in the first-order stochastic domains. If I can find this least favorable measure, I can actually solve the problem of robust cross-efficiency by using exactly the same construction as before. So, to get, so I have to assume that this thing is continuous, and under some conditions, we can show that. Some condition we can show that we can solve it and that it's continuous, that this is a uniform and we get a pair that has a white distribution. Yes. Just to clarify, there's only uncertainty in P, so Q is fixed. Q is, yes. There's no uncertainty on Q. I mean, there are many Qs, but all of them give a unique price to all calls and puts that traded in the market. They have a unique price. All these calls and puts give a unique general rep. Puts give a unique can replicate all the payoffs that depends only on the stock price. And so all this path-independent payoffs have a unique price. And so there might be many cues, but they all agree on the pricing of our claims that we are considering here that are statistically uh replicable. Yes. Always true on the it's uh It's if you can replicate something, then if you have different risk-detrop probability, they will all coincide and there will be a unique price. By absence of arbitrage. So in fact, the assumption that leads to this conclusion is the fact that we assume the market is arbitrage. Sorry. So it means that the queue, maybe we have a queue, the pricing kernel, but when you project to the price, it's unique. The queue can be non- That is unit, but Q can be non-unit by exactly. But in this case, then different Q in your DP, DQ, are they different? Because you use the L star. No, because the one we consider is the path independent p of so yes. So for all path independent payoff, this LT can be written this way. So I'm going to use Written this way. So I'm going to use for all the cues, the dump CT under Q of S T is unique. Even though there are many cues, this thing is unique. So this is our common thing, in fact. So we can solve explicitly this problem. And so, for instance, and this was a problem that was solved with a different approach in the literature by Sheed. If you have a log-normal market. If you have a log-normal market and the only thing that is different, so under Q, you have a drift is always the interest rate. Under P, in the real world, you have mu, but you're uncertain about the drift in your market. So mu is in a family of mu1 and mu2. Then we can show that if, for instance, the interest rate is less than mu1, then there is a least favorable measure, which is LT mu1. So mu1. So mu1 is the minimum drift. It's kind of the, if you're looking at the worst case scenario, the drift is the smallest. And LT mu1 will be your least favorable measure. And if you use this likelihood ratio, you can construct the robust cost efficient payoff. So what it means, it means that even if you have uncertainty on mu, if you want this robust cost-efficient and you want to construct a payoff that is at least as good That is at least as good as F0, you need to use the minimum drift. So it's quite intuitive. If you have uncertainty on the drift between mu1 and mu2, you would choose the minimum drift to construct your optimal pair. And at least all this pair, this x star, if you calculate its distribution under another p from another pu, you will see that it dominates in first order. That it dominates in first odostochlasic dominance, this distribution, FCO minus 1. Yes? Just a clarifying question, Carol, on the previous slide, when you talk about the lead-favorable measure and the structure of the X-star in this case, this only works for FSD, right? Because essentially it's a point-wise ordering on the CDFs. If you go beyond FSD, what happens? So, second-order stochastic dominance, it still works. Else. It still works, so the minimum in this case can be soft. And you also have a measure, a disparable measure, but you need to find it in the sense of second-order stochastic limits. So whenever you have a lattice structure on your order, this still would work, right? Because essentially you can find a a minimal element. That's all you need, right? You need a minimal element on your order. So the structure would be the same regardless of the ordering as long as you have a minimal element. Some yes, you have a minimal element. Maybe. That's maybe that could be a generalization. That's maybe. Okay, see if it works. Sorry, related to that, do we actually have necessary and sufficient conditions? To have the existence? Yeah, I mean, I think that's. We have some sufficient conditions, but they are not necessary and they are very difficult to satisfy. In fact, so we have not so many examples for which it works. I could show you kind of the basic example for which it works. It's the case where you're still looking at log-normal description. Where you're still looking at log-normal distribution, and now instead of having a certainty on the mean, you just are uncertain on the volatility. In fact, you can deal with mean and the volatility together, which we do in the paper. But on the slide, I only report the uncertainty on the volatility. So, what you know is that the real-world probability, the real-world volatility is smaller than the principal-world relativity. So, there is a relativity premium there, which is great. Premium there, which is well documented, but the volatility in the real world could be in a range. And what we show is again: if you're doing this robust cost efficiency, what you will do is you will pick the worst volatility, the maximum volatility to construct your payoff. And this payoff, using the maximum volatility, will lead to a dominating payoff that will dominate the distribution F0 over all these. All these probability distributions with different segments. And this is for second-order stochastic numbers in this case. So you have the second-order stochastic numbers, yeah, it's because it's log-normal and you have a nice order on log normals. But it's not so easy to find. So in the last minutes of the talk, I want to show you why we think that this is an important result because it's not It's not just solving this minimum, it's really also characterizing the optimal solution of robust portfolio selection problem. And this was inspired by the fact that cost efficiency and portfolio selection are very closely related. So, portfolio selection, I see it this way. V is my objective function, I have preferences, and I'm going to assume they are low-invariant. So, if I have the same distribution, let's assume for now there's Distribution, let's assume for now there's no ombigbit, there's only one p. So I have the same distribution under p, then I'm equally happy with x or y. And I have increasing preference if x is always better than y, p almost surely, then x is better with my objective function. So these are very basic assumptions on the p and the And the problem I'm going to solve when I think about portfolio selection is to maximize this objective function over a set of admissible payoffs that I can achieve with some budget, initial budget W. Initial budget I'm investing in there. And the claim is that any solution of this problem must be cost efficient. So the way to see it is to think of it is to think of it as by contradiction. Assume that you can find that the maximum exists and is not cost-efficient, then it would mean that it has some distribution F, but because it's not cost-efficient, you can find another pair with the same distribution. It is strictly cheaper. If it has the same distribution, it would have the same objective function. So it also achieved the maximum. But it's strictly cheaper, which means But it's strictly cheaper, which means that you didn't use your full budget. So then you just take this budget, you put it in your bank account, and of course you will dominate in first-order successive dominance, like almost surely, your optimal payout. And that violates the optimality of this original payoff. So we know that all optimal solutions of this problem must be cost efficient. And so we have an equivalence that says that if you are optimal for some R optimal for some objective function that is low invariant and increasing, it's if and only if you're increasing in L, and it's if and only if you are cost efficient, and also if and only if you can find, in fact, a concave utility for which you are optimal. So, you have this full equivalence between these four statements. We have the same thing for robust problems. So, how do I define a robust problem? So how do I define a robust problem? I'm looking at now I have a family of P, I look at W P's P lower invariant of P increasing, because now I need to specify which probability P. And I'm defining robust preferences as the minimum, the infimum of all possible probabilities, kind of the worst case of my preference for X. And so for instance, this could be a mean expected utility, like this worst case expected utility. Expected utility, and then you would lead to a max mean problem when you maximize over all possible payoffs what happens in the worst probability. And so, this is my setting to have this robust preferences. And we have the following result for FSD. So, if I have X star that solves this problem, solve. So, this is solving this maximum utility. In fact, x star needs to be increasing in L star, and L star here is this least favorable measure for the first order stochastic dominance. And we can also write it as X star solves this first-order stochastic dominance, robust cost-efficiency problem. So it is robust, cost-efficient, in the sense of first-order stochastic dominance. Which means that to solve this problem, it has nothing to do with cost-efficiency in some. Nothing to do with cost efficiency in some sense. You could actually transform it and then look at it as a minimum cost for this robust setting where you dominate the distribution L C So conclusion, what we do, we characterize this robust cost efficient. We find this analytical expression and we relate this robust cost efficiency through portfolio selection. If you're interested, the payoff is actually the paper. The paper is actually available on Archive and was recently accepted in Finance and Stochastics. You can find it for now on Archive and soon at one point. Finance and Stochastics. Thank you. Thank you. Carlon, on the previous slide, just a quick red frank question. Can you do the same construction and get something similar but with? And get something similar, but with SSD, since the structure is basically the same. Yeah, but there's a completion of the utility. Yes, that's what I was going to say. I mean, so here you're having an uncreasing competitive utility. What else do you need for that? You mean the replicating economy, right? Where you construct the utility and a solar economy. Some foundation on the expected utility setting, on the ambiguity. Remember this ratio with the relative risk aversion and there is some condition linked to the credence is something also in this ratio I think. Okay, that's fine. But there are some conditions. It's actually not so straightforward to find the equivalence beyond this simple case. In fact, yes. Can you go back to your consumption page? They say that no dynamic trading is allowed. How is this assumption affecting your conclusion? If dynamic trading is allowed, is that still holding? So mostly is because we want to be sure we can, because we didn't want to go in a complete market, then you would not have this problem. So if you set up your market in So if you if you set up your market being complete, all payoffs can be achieved and then if you add dynamic trading you will achieve all payoffs. But here we we wanted to have a set of admissible payoff that can be replicated. So that's why we need this if we could better if there is also possibility to trade this adds more payoff, but I don't think it will change the solution. It will change the solution. But still, work effects. Yeah, because under that paper, I remember, like, if your payoff is only depending on the terminal, if you are allowed to trade semi-statically, it's just semi-statically paid off and that's only de depending on the term. But yeah, I say but in this case you say there will be no dynamic trading, so it means that no intermediate trading. No intermediate trading is the article. So I'm just wondering how is that? But you say that it's related to the copy market? What we mean is what we need really to make everything work is that all bi-financial playoffs are achievable, attainable by a strategy. Or B that's that's enough, in fact. You agree? Go ahead, B? Well, to get the results, you just need a Python kernel, like with this form which you have there, which you want to replicate. Multi-replicate, then you of course need a structure. Yeah, if you're in a complete market setting, it would work. But except that now you cannot use that. 