These sort of part four performance guarantees, but I did promise to talk a bit about optimization proper. So before I get to that, I'd like to at least sort of finish a bit this train of thought about how these kinds of entropic notions can also hopefully. Can also hopefully begin to say something interesting about refutation, not just for Markov chains, you know, equilibration questions, but also questions about sort of optimization proper, you know, not just for something like simulated annealing, but maybe sort of more standard methods like approximate message passing and gradient descent. So, okay, so with that in mind, you know, the question, so just a reminder: what was the question we were interested in? You know, we're asking a question about, you know, suppose I wanted to maximize a homogeneous polynomial of degree p. In our normalization, sort of the natural way to write it is like this. And subject to sort of simple constraints, like either the entries are discrete. The entries are discrete, or maybe you know the L2 norm squared is say n. And the question I want to just sort of touch on very briefly is, you know, can stable algorithms Algorithms find the maximum quickly. And specifically, you know, in some of the algorithms we'll be seeing later today, in a lot of interesting questions, you'll have order one-time algorithms. One-time algorithms, and what I'll be interested in today is just say log-time algorithms. And what do I mean by stable, or more precisely, noise? What do I mean by noise stable in this setting? What I'll mean sort of informally is that if I take, you know, the optimization problems we were considering, you know, the simplest ones are of the form, you could write them sort of in the span. You could write them sort of in this fancy tensor product notation as a coefficient tensor interproducted with sort of the point you are in the state space. And then what we'll do is we'll say, suppose you have some output of your algorithm. Informally, when I say the algorithm is noisy stable, I mean the following. I mean the following. If I consider the smart path or the OU flow, which sort of heuristically is I'm resampling a small fraction of the entries, but if you think about things discretely, but continuously, it's more of an interpolation between the original coefficient matrix and an independent copy of it. What I want is that I'll say that the algorithm is stable if the distance between the Distance between the output for the original coefficient matrix and the output for the coefficient matrix at time t of this flow has some sort of reasonable modulus of continuity in time in this interpolation parameter t. So, some examples of stable algorithms. Of stable algorithms would be, so for example, by sort of standard well-posedness arguments, you can see that things like Langevin dynamics or gradient flow are stable in the continuum in discrete time. You can think about sort of Lipschitz iterative schemes. You know, what I mean by that is your output at time, the tth iteration is some Lipschitz function applied entry-wise to the guy at time t minus one, time t minus two, all the way time to time t zero. You know, so some examples of this would be gradient. So, some examples of this would be gradient descent or approximate message passing schemes. Sort of a broad class of stable methods that were introduced recently in the theoretical CS literature are what are called low degree methods. Sort of heuristically, the idea of a low-degree method is: you know, so suppose I think about the actual optimum of my problem. Well, the idea of a low-degree method is, well, you know, if this map from the coefficient matrix to the optimum is, you know, at least vaguely reasonable, you might expect that you can approximate this optimum. This optimum, you know, by an expansion, sort of a Taylor expansion. And so in general, we will say that a method is low degree if the output of your method of your algorithm is a polynomial if each sort of entry is going to be a vector. If each entry is Is going to be a vector if each entry is a polynomial of degree at most d. And so when I say polynomial though in low degree, what I'm going to mean is not bounded degree, but even say, you know, degree is the degree is say not quite n, but say n up. n up to logarithmic factors. So n over log n, say. And so what we'll talk about here is that, you know, again, what you can show, and this is joint work with David Gemarnick and Alexander Wine, is that again, you know, these low-degree methods. Low degree methods also will fail at producing near optimizers. So in terms of AMP, that would say things like approximate message passing wouldn't work in log n time, but it also sort of was introduced to capture more complex things like spectral type methods. And so again, so the heuristic here is as follows. Here is as follows. Sort of what's going to be the obstruction is what's called the overlap gap property. So there's going to be two parts to the obstruction. They're both going to be a kind of overlap gap property. So the first kind of overlap gap property, we've actually kind of seen already, right? Because one thing we know Right, because one thing we know is that you know, at low temperature, or at least we've discussed that at low temperature, you expect replica symmetry breaking to occur in the sense that the overlap distribution should have multiple points, but in fact, you can go a bit further. You expect there to be a gap in the support. And so, why do we care about the low temperature Gibbs measure? Sort of intuitively, the idea would be that, well, I fix the dimension and send the temperature to zero, which is to say I send beta to infinity. The Gibbs measure is going to concentrate on the true optimum. And so then, you know, the hope would be or the And so then, you know, the hope would be, or the idea is that if you look at the sort of overlap distribution and send the temperature to zero, well, by itself, it would converge to one, just a Dirac mass at one. But if you sort of look at it in just the right topology, you'll get sort of a non-trivial measure that's going to be sort of a measure that minimizes. Measure that minimizes a zero temperature formula zero temperature Parisi type functional. And then the hope would be that this would have something, or what you can actually show is that this would have something like O. Show is that this would have something like OGP, would have a gap in the overlap as well. And so, you know, you can, there's been a lot of work recently on making this picture rigorous, sort of starting with this original paper, these two papers that came out simultaneously, one by myself and Ian Tabasco, and one by Chen and Arnab Sen. They came out in 2017, and then there's been sort of And then there's been sort of follow-up works, sort of important ones being one with Ofinger and Chan, and then myself with Stribabrata-san. So, you know, the upshot being that you can sort of get a replica symmetry breaking type picture at zero temperature. Temperature. And then, you know, what would this mean? Sort of, okay, there's all this fancy, you know, analytical stuff. But the point is, you know, by looking at this sort of living measure and, you know, picking just the right topology, you get this upshot that the near minimizers of H Of H should either be very close to each other or quite far from each other. So that's this is sort of the zero temperature analog of the free energy barrier picture we talked about. Oh, sorry, there is this question about random initialization. Yeah, the gradient descent thing would be from, for example, uniform at random start. Sorry. All right, so just getting back to it. So, okay, so there's first this sort of zero temperature version of the free energy barrier property. Version of the free energy barrier property, but that by itself, again, isn't going to be quite enough to imply sort of interesting obstructions for algorithms. You'll also need one more thing, which is called disorder chaos. So, you need one more ingredient. The idea is this. So, disorder chaos is defined as well. Let's let x0 be a near minimizer of the problem unconsider. And what I'll do is I'll let x be a near minimizer. Of well, we can think about A as also being a zero from that flow of matrices or tensors. And I can think about the flow at time t under the OU flow. And then the idea is like this. You know, the OU semi-group mixes quickly. Right now, OU dynamic mixes quickly. Right, you have a uniform log-Soblev inequality in all dimensions. And so the idea is that maybe the minimizer should, once you flowed a little bit in time, the landscape should be essentially refreshed. Now, right, so the hope would be that, you know, if I look, for example, at the inner product between these two minimizers. The inner product between these two minimizers. Once I flow for a little bit of time, let's say in order one time in t, the landscape should have completely refreshed. So the inner product is basically zero. Two independent vectors on the sphere and on the discrete hypercube should have normalized inner product essentially zero. But actually, making this work out is quite a bit of work. And there's been a lot of work. Lot of work in this direction by Chatterjee, Ding and Eldan, Weikwa Chen with collaborators, and again more recently, Ronan Eldan has another paper in this direction. What we'll be using will be based off of a work of Wei Kuo Chan with his PhD student, Madeline Hansi, and Gilad Lehrman. Who showed that this picture holds in the discrete setting? That you do actually have this property for near optimizers, say in the discrete setting. And there's similar results for spherical models. But the point is that what is this telling you? This is telling you another kind of overlap gap property, but it's actually sort of a strong form of the overlap gap property. For the for the pair. Oops. Right, because these guys will actually be orthogonal to each other, not just sort of either close or far. So, to capture all these together, what ends up being important is sort of two notions. So, the first one is sort of a notion of overlap gap property that captures sort of all these different kinds of overlap gaps. And what it is, is the following. So, suppose I have a family of functions. Of functions with a common domain on our end, and we'll say that they have the overlap gap property. With parameters mu positive are mu and zero is less than or equal to nu one is strictly less than nu two which is almost one if for any For any two functions in the family and any two points x and y such that f of x and g of y are in the mu superlevel set you have That they're normalized in a product is contained either in the interval from 0 to new 1 or the other disjoint interval from new 2 to 1. And sort of there you're going to need And sort of, you're going to need one more property to finish things off. And what we'll say is the following: so, two functions f and g are new separated above mu if for any two points x and y in the domain they're X and Y in the domain, in their common domain, with that are in the super level sets for F and G respectively, you have that their normalized inner product is in fact less than and so what we can show you know just to sort of summarize. So, just to sort of summarize everything we've just seen, you get the following theorem. So, suppose I have two independent copies of the disorder tensor, and let's look at the OU flow. And then I'll consider the family of functions, which is our objective function evaluated at the various coefficient tensors given by this family. Finally, I'll fix some epsilon positive. And then what I'll do is I'll let Egs denote the limit of the mass. Of the maximum of H. And it's sort of a non-trivial fact that's in one of the exercises to show that this is a non-random number. And this convergence happens almost surely. And what we can show is the following. is the following. If we let f be the family of functions given by this h of t, then this whole family has the overlap gap property with some parameters mu and zero is less than new one is less than new two is less than or equal to one and two And two H0 and H one are new one separated, where new one is the same new one. Above, and both of these happen, sorry. So the OGP happens with some parameters new one and new two. Parameters one and nu2, but the mu, which was the super level set, is the ground state energy minus a little bit of wiggle room. And here, these two functions are new ones separated above the same mu, which is the ground state energy, with a little bit of wiggle room. And this happens with high probability. happens with high probability which say with probability one minus e to the minus constant times the dimension right so what i'm saying is if i take this whole flow then the near minimizers of any two points in this flow will either be close to each other or far from each other and then if i look at the two endpoints which correspond to two completely independent copies of the problem you know that their near minimizers are at least new one are at least At least new one apart. So that's what these two things are sign. And so then the question is: you know, how would you go from, you know, having this OGP and new one separation to refutation? The idea is, again, to, you know, look at a kind of replication of the problem, a kind of replicated dynamics. What you do. What you do is let xt denote the output of your algorithm corresponding to AT, which is the flow at time t for the interpolation. This t, maybe I should use a different letter just to emphasize that this t is not the runtime of the algorithm. Maybe I'll call it a row. Time of the algorithm. Maybe I'll call it a row. It's just the amount of time I run my OU flow. And then, you know, the idea is that if this algorithm performs well with high probability, if this algorithm outputs a solution with reasonable probability, then you can suppose. Suppose that all of these, or at least sort of a very large number of them, are near minimizers, which is to say that each of the x sub t's are near optimizers. Oh, sorry, not minimizers, maximizers. And so then the idea is: well, this is going to lead to a contradiction. Why? Well, let's just look at the overlap between the output at time zero, at interpolation time zero and interpolation time row. So here's the overlap, and here's how well at time zero, if I take two, if I look at the solution and I look at the solution. Look at the solution, and I look at the solution again, the inner product's going to be one. And so, by a continuity argument, you expect: okay, maybe it'll decrease a little, but it's not going to change so much as you vary rho. And this continuity is coming from the noise stability. And similarly, again, if I look at the overlap at one by a new one separation, you know, it's at most some new one. It's at most some new one. Let's just pretend that's zero for the moment. And so, in particular, after you give yourself a little bit of wiggle room, you'll have a continuity argument by noise stability near one. So they're going to be sort of nearly orthogonal at time one. So this is rho is equal to one. And so then, but then you know by OGP that the overlap of all of these things, just by using OGP and continuity over and over again, these overlaps are all going to sort of stay put. And so this is going to lead to a contradiction because this OGP property happens for any positive time. It'll even happen for sort of very small times if you give yourself. Very small times, if you give yourself, since we're giving ourselves a little bit of room. And so, in particular, you have this sort of contradiction because the continuity said that the function was nearly one, let's say, this time here, but the OGP is saying that it's nearly zero in this whole window here, and so you have a contradiction. So, okay, so just to sort of recap, you know, even though replica symmetry breaking is sort of in and of itself an equilibrium concept, or even, you know, we turned it into a zero temperature concept. You know, replicosymmetry breaking, if it's sort of connected with the overlap gap property and Overlap gap property and something a little bit more like positivity of a replicant eigenvalue, or this sort of disorder chaos, which would again be implied by this positive replicon eigenvalue, then you can get sort of refutation for things like sampling and at least noise stable algorithms. And so, I think that's about all I want to say for refutation right now. And then the last sort of chunk of time I'd like. And then the last sort of chunk of time, I'd like to talk about sort of the other side of things, performance guarantees, and then maybe sort of give a bit of a segue into the sort of upcoming lectures. But I guess now is a good time to take a quick but shortish break. So if you have questions, please ask them in the chat. Have questions, please ask them in the chat. I don't know, you might want a real break. So, any questions for Rokash? It doesn't look like there is any question for now. If you want to take a two-minute break to breathe a little bit, feel free. Otherwise, we'll start back whenever you want.    All right. So I guess maybe there are no questions. It looks like you're good. All right. So, in that case, just to sort of close things out, I'd like to talk about performance guarantees for a bit. Guarantees for a bit. So, just sort of as a pointer, you know, we're going to hear a bit more about performance guarantees for specific algorithms, but in Ali Ronsubach's talk in a couple of days, and then Andrea Montanari's Montanari's talk next week, I believe. Week, I believe. And so, what I'd like to talk about is sort of something a bit different from all of those, from both of those results. What I'd like to do is think about what more we can say about Langevin dynamics. So, again, Langevin dynamics, right? You had this DXT was there's spherical Brownian motion. Spherical Brownian motion in a potential given by my random energy. And I'll focus specifically on the p-spin models here. So one kind of thing you can think about is spectral gaps. And so there's a lot of interesting things here in terms of guarantees, a lot of interesting questions. But two things I want to point it out. So there's results on Log-Sobolev inequalities in the discrete setting for p is equal to 2. P is equal to 2. There's been this recent really nice result of Roland Bauerschmidt and Terry Baudineau using sort of ideas from supersymmetry. And for the sort of non-discrete cases, So, for the spherical models, you can use sort of more classical Bakery-Amri arguments. And this is discussed a bit in a paper with Reza. But, what I want to talk about is a bit, you know, sort of a question that's sort of looming. Know sort of a question that's sort of looming around in the background of all of these sort of talks on dynamics and spin glasses, which is specifically what's the dynamical role or dynamical relevance of the complex landscape? What does sort of all this whole story about critical All this whole story about critical points and everything that I'd sort of alluded to at the beginning says about the behavior of the dynamics. So, for example, you know, suppose I start my dynamics near a well, or perhaps more interestingly, near a saddle point of high index. Now, the question is: you know, where do I go? Do I increase in energy? Do I decrease in energy? And as you might imagine, you know, connected to this question is, you know, do you do, suppose I start from somewhere, say random, do you even enter? And enter a ball around critical points. Do you see critical points at all? And then, you know, connected to this is, you know, how does my How does my initialization affect this? So, just as sort of a warm-up to understand why there could be something sort of really sort of unusual going on here for Langevin dynamics, let's consider the following warm-up exercise. So let's consider. I start my dynamics at some point. And this point is in a ball of radius epsilon times root n, where epsilon we think about as small in this sphere. And what I'll do is I'll suppose a couple of things. So one, So one, we'll assume that y is a critical point with index, which is a constant times n, where this constant is, say, strictly between 0 and 1. So that is, you know, I have order n positive eigenvalues, order n negative. Order n negative eigenvalues. So I have order n up directions and order n down directions at the side. And the question I want to ask is the following. So suppose one more piece of information I give you is that let's look at the normalized energy. And just for the And just for the sake of convention, I'll just put a sign here. Suppose the normalized energy is just a bit negative on the order n scale. The order n scale isn't super atypical, but that's sort of where the interesting things start to happen in the landscape. And the question I just want to ask is: you know, does my energy Does my energy decrease? Let me do it this way. Let me remove this for now. Does my energy increase or does my energy decrease? Now, one thing you could think about is: you know, there was this whole question about initialization. So, you know, let me look at the spectrum. So, you know, let me look at the spectral projection onto the top, onto the positive part of the spectrum. And let me suppose that my, you know, I can work in local coordinates where my function has a good quadratic approximation. And this coordinates, well, I can suppose that, you know, my x naught is entirely sort of in the positive part of the space. Right, so if I have this sort of doodle of this crazy high-dimensional saddle, you know, these. Saddle, you have these order n up directions, you have these order n down directions. And I'm saying, well, suppose that I'm starting, say, somewhere like here, I'm a bit in the up direction. Or, you know, conversely, suppose I start a little bit in the down direction. So suppose I start a little bit here. My question is, now just sort of in order one, sort of short amount of time, what happens? Do you go up or do you go down? Now, you know, what you might expect from, you know, What you might expect from a metastability perspective is maybe if beta is really, really large, but still, say, order one, it would go down if it should go down in this saddle, right? Especially if you start, say, in this down direction at least. That's one thing you might think would happen. And you know, your reasoning would be it, you know, for very large beta, you know, from this whole metastability picture that you should eventually sort of follow that kind of a trajectory. But it turns out this intuition is wrong, as is often the case in high-dimensional probability. In fact, the energy will always increase, and it will always increase by a macroscopic amount in a shorter to one time. A shorter one time. So it'll increase for some t for a whole interval of t's, which is bounded above by some t, which is order one. So no matter where I start near the sad in the saddle. In this saddle, I'm going to increase in energy. So even if I'm at this blue point, I'm going to shoot up the saddle. So at first glance, this might seem kind of crazy, right? But the issue is that there's sort of two effects going on in the dynamics here. And there's sort of a second-order high-dimensional effect, right? High-dimensional effect, right? So, one thing that's sort of not so hard to see is that if you started at the critical point, you know you're going to move away from it at least, because Brownian motion is not going to want to live near any given point. So you should move away from that point. But the fact that you're going up is maybe a bit surprising until you sit there and you just calculate and see what happens. And what you find is the following. And what you find is the following. Well, suppose I look at this normalized energy. What does it look like? If I do Ito's equation, there's the generator hitting H divided by N plus there's a grad H on N d M. And so then you can think: okay, so how would I analyze this? Well, you said the first. Okay, so how would I analyze this? Well, you said the first thing you can think is: well, okay, the gradient, what is it? It's a bunch of derivatives. This is a Gaussian process. And so each of the entries are going to be Gaussian. And you can sit there and you can compute. And they're going to be each going to be individually sort of standard Gaussians with variance one. So that would tell you, you know, with very high probability, you'll have a law of large numbers. So this, you know, this gradient should be of. Should be of order at most square root n. But what does that mean? Well, for the martingale part of the problem, now what that tells you is by Dube's inequality, the Martingale part of the problem is irrelevant on order one times because it's basically like square root t on square root n. So it's not that the diffusion is. So, it's not that the diffusion is doing something crazy. It's actually coming from the drift itself. So, okay, so how could the drift, what could funny, what funny thing could be going on with the drift? Well, the idea is, well, okay. So, okay, now I know that dHn is basically L Hn D T on N, I need to think a bit more about what this is. Think a bit more about what this is. Okay, so there's two parts: there's the Laplacian of n, and then there's beta grad h squared on n. And so, you know, here's the thing, right? If I'm too close to the critical point, right, what's going to happen here is I'm going to have, you know, beta times essentially epsilon because the gradient's going to zero. Epsilon because the gradient's going to zero, so this will be quite small, you know, depending on the trade-off between beta and epsilon. And so you can think about this as being quite small. But on the other hand, what's going on with this hashin? Well, the hashin is in fact Is in fact, I mean, sorry, the Laplacian. The Laplacian is the trace of the Hessian. And the idea is there: well, you're summing all the eigenvalues. And so if the sum over the positive part of the spectrum is a bit more than the sum over the negative part, you have a sign. And similarly, if the sum over the negative part of the spectrum is a bit more than the other part, then you have. The opposite sign. So it depends sort of on this trace. Now, in the p-spin case, what's going to happen is that, you know, because the p-spin model is essentially homogeneous, it's a homogeneous polynomial of degree p, well, remember the homogeneous polynomials are eigenfunctions for the Laplacian. So this piece. Laplacian. So, this Pieceman model is going to be essentially an eigenfunction of eigenvalue p up to a lower order term. And so what is this telling you? That, you know, if this energy is just a little bit negative, say macroscopically negative, this Laplacian term is actually macroscopically positive. So, the Laplacian is actually helping you, right? So, sort of in words, what's happening is, you know, even though you have low temperature, you know, it's what's what you're winning off is not sort of a random effect from the Brownian motion, but instead there's sort of a concentration of measure phenomenon in actually how the walk moves, right? The Brownian motion is picking around. Moves right the Brownian motion is picking a random direction and then making a step. And the random direction it's picking is based off of this Hessian. And then the point is that if you have a few more up directions than down directions, you're in high dimensions, you're way more likely to go up than down. So that's at least, you know, the point I wanted to make here is at least, you know, these questions in high dimensions can get, you know, really sneaky. So So, sort of the upshot I wanted to bring out, bring up now is: you know, you could try to start to understand things a bit more precisely then. You know, if you're starting to get a bit curious about this, you could say, okay, well, if I want to understand the energy, what's nice about the fact that it's almost a spherical harmonic on the order one times, the energy is basically a function of the energy and the gradient. And so then you could try to understand. And so then you could try to understand the two-dimensional system, which is the energy of the gradient. Now, of course, that system is not going to close, but it turns out that it nearly closes. And so what you can show is that if you look at the two-parameter family, which is the energy. Which is the energy, the negative energy is better for sort of parameterization for just to make expressions nicer. If you look at the energy and the modulus of the gradient, what you can show is, so firstly, this sequence is tight and its limit points are and C. R and C1, and they satisfy the following system of differential inequalities. So the energy term is a linear function in u and v and the modulus of the gradient term is Going to be a quadratic function in these variables. Where this last term here is an error term you get from sort of truncating at only looking at the two variables. And so you get these sort of two upper bounding and lower bounding systems. And in particular, what you can do is you can develop an approximate phase portrait for the system. So if you look at the limiting dynamics. So, if you look at the limiting dynamics, what you find is that if you look in energy sort of momentum space, what ends up happening is that there's an absorbing set defined in terms of the sort of lower bounding flow and the upper bounding flow. There's going to be sort of the transit from one to the other fixed. To the other fixed points for the two different dynamics. And then what you have in here is an absorbing set. And what's really interesting is that this absorbing set is bounded away from criticality. Criticality. So, in all-order one times, you're going to actually be somewhat far from these saddle points. Okay, so there's a whole lot more we could say about these kinds of dynamics and performance guarantees. In particular, some very interesting things I left off here are connections to what are called the Crosanti summary. Summers Crossanti Herner Somers Cugliandelo and Courchon equation where the idea is to try to develop a closed system of equations for just the right observation. Of equations for just the right observables. And there again, the idea is going to be to look at sort of replicated kind of dynamics. But now instead of looking at two copies of the system, you look at one copy of the system at two different points in time. But I think that's sort of, I'm sort of hitting up on my time here. So, you know, as promised, I just wanted to end this with sort of. Of end this with sort of a high-level discussion of what's next over the next couple of weeks. So far, in the first day's lecture, we talked about sort of the replica symmetry breaking picture and its connection to the Mazard Parisi-Virasoro onsets. Virusoro Anzatz. And we talked a bit about pure states. Now, you know, there are sort of more questions you can go. You can start asking questions about, you know, can you come up with the mean field equations for these systems? And it turns out, you know, there are two different kinds of mean field equations. Base, they're sort of all called in the literature, the physics literature, unfortunately, they're all called the Talas-Sanderson-Palmer equations. The Tawis-Anderson-Palmer equations But there's sort of two types of these equations. One based off of what's called the Talas-Anderson-Palmer free energy and what's going to happen What's going to happen in a couple of days is Elirond Subag is going to tell you about his recent work on understanding the Tallison-Anderson-Palmer of free energy and how this connects again to questions about optimization. Another direction we could talk about, but we won't have the time, is sort of what are called the Mazard-Virasoro multi-scale tap equations for Cavity fields for the cavity approach. And if you're interested in more on this, you can see a paper by myself with Ofinger. Another direction is you can connect the replica symmetry breaking picture. Well, if you think about the SK model, where you're thinking about a metric, a matrix, you realize that this sort of very quickly You realize that this sort of very quickly connects to questions of random graph theory. And then you can start asking yourself about all these sort of interesting optimization problems on random graphs, both in the dense and the sparse setting. And this will be sort of Amin's going to be talking about this, how this whole story sort of translates to the random graph setting. Graph setting. And then finally, you know, one more thing you could start thinking about is sort of, you know, more interesting algorithms, which are, say, motivated by this whole sort of preesy sort of story. And this connects to sort of a really nice class of algorithms called the approximate message passing schemes. And this is going to be sort of the starting point for under. Be sort of the starting point for Andrea's talks. And then finally, you know, in this sparse graph, random graph setting, there is an analog of this whole, you know, Mazard-Perzi-Virasoro picture, and it'll connect again to things like these Talas-Anderson-Palmer equations that Subag will be talking about. And in that case, you know. And in that case, you know, things are going to connect to what's called the beta free energy approach. And now, Elkhan and Mosul will be talking, I think, a bit about this and its connections to algorithms again. So I just wanted to thank you all for taking the time to listen. You know, taking the time to listen, and hopefully, you'll enjoy the next few weeks of this summer school. Okay, so sorry, if you can unmute everyone, I think we should give a big round of applause to Okos for his four hours of lectures. His great four hours, and he was also kind enough to link to everything that's coming up, which was much more than I hoped for. So, thanks a lot, Akosh. Hope for so. Thanks a lot, Okosh.