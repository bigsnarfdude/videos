I was going to start with dynamic distributional robustness. Two causal transports, robustness and transport. So right to the theme of the workshop. Thank you very much for the time invitation. It's a very nice workshop. And I'm going to talk about the distribution lattice in a dynamic setting. It's kind of different from some of the talk we have seen so far, which was on tech setting. I present the version of the talk to some of the audience, but I'm going to try to present. But I'm going to try to present it in a different way. So, these are works with some of my students at US Austin, Vijay's PhD student. Jing Jung is now taking hostel at Chicago, and we have John Mijo and John Topics. So, when we talk about sequential decision-making on certainty, people from different communities really have different languages. By that I mean that, for example, in organization or or more broadly, in authority research, multi-stage test programming. Multi-stage success programming kind of standard paradigm can be traced back to, for example, 50 or 70 years ago when Georg Dentic started two-stage success programming. For people from machine learning or computer science, Markov decision process is a standard way for the model-based learning. I guess for applied mathematicians and maybe CAS option transport is more standard language for this. In a non-robust setting, more or less these frameworks are the same. Namely, for one problem, you can model inner ways and translate the language solution on one side. But the tricky thing comes in if you have the robustness, because the way we model the ambiguity of the problem is different depending on which framework we work on. For example, in Markov SIM process, the ambiguity will be on the probability transition matrix. But for reasons learned, Metrics, but for reason learning, for stress auto control, it will be on some stress processes. So, in the interest of the audience, I'm going to focus on the last one, the stress auto control. And essentially, it will follow the dream, the regime of the problems that discussed in today's keynote this morning. So, let's set up stage. So, let's focus on a very simple state dynamics. So, the current state is ST. State is ST, and together with the control UT, the randomness omega t together will influence what we have in S state. It can be represented by function f, depends on R data as well. The running example you can think of for this part would be an example of the inventory timing problem. The current state inventory plus the ordinant quantities trap the demand gives you the infrared level for nesting. Right, uh, it's important to note that. Right, uh it's important to note that there's a random variable negative key which performs a successful processes. I'm using the bracket to represent the history up to a certain point, is really uh decision independent. So whatever the control we make does not affect the distribution of the status quo. It's a limitation of this approach, but it's kind of standard setup. It applies to some application process, for example. Right? And we are putting user capital to represent a set of fitable. Apple needs to represent a set of visible regions for a problem and the goal to find a control output control from this admissible set and minimize the long-range attitude cost. Here, the control is history dependent, namely it could be dependent on all the random variables, all the randomness of time t cost. So there are kind of standard setup, and in a mini mass robot control, the story goes as follows. The idea is our deployed environment we are available. Deployed the environment where we are actually implementing the control policy could be different from the environment where we learn for the output. And which sometimes calls a nominal environment. So therefore, study a minimizing game between a controller and some adversarial distribution represented by a single set F. So the center is denoted by a real happy distribution, and this data measures the level of Data measures the level of robustness with respect to some discrepancy measure. The classical tools in the mean mass robust control based on vertical entropy, or more generally, divergence. Essentially, the rate was the normal distribution, which works nice if the considerable distributions are all absolutely continuous with respect to some nominal ones. So the thought today is really motivated by application of the more data-driven Application in a more data-driven setting, where instead of having a parametric continuous distribution, here we have a data-driven distribution which finance for. There are different ways for dealing with multi-state problems with the data. One of them is based on scenario T. For example, in this one, you can use monitor sampling, condition on what you have observed at every stage, and sample from the initial future for the next stage. For an estimation. And this comes up with a decision tree. And for example, this one fully connected represents the stage-wide independence case. The other choice in the chat would be based on some clustering idea, you can cluster a sample path that gives you a better representation of the output function. So the genre math, I'm going to try to kind of convince you why causal transform would be the good way to model the anniversary in a data dream example. In a digital drinking setting, and then to present our main results, which gives us a dynamic programming reformulation of the problem. I'm going to try to present it in the simplest setting so that it's going to be easier to interpret, but I'm going to talk about some extensions at the very end. Alright, so the motivation of using cultural transfer really from this simple application in time series. So let's consider simply setting where we have this AR1. And we have this AR1 time series that's represented by this total. I can think about this omega as the demand process. If you think about the data uncertainty, say at any given time t, the error of the distribution could either directly because of the maybe assembly error or error in the environment that occurs at the state t, or because of the modeling error that occurs at the very beginning of the horizon. Beginning of the horizon and propagate it to the time point. But in the same time, the future randomness will not affect your uncertainty about that time stage. So that is to say that when you think about multi-data uncertainty of this type originating from the time series, there is some restriction on what kind of perturbation of the data are allowed in the set of distribution. Alright, so let me illustrate more details in the Let me illustrate more details using this very simple two-stage binary tree where the second stage have two relations and the conditional and second stage have another two additions. Up, down, and up for each. This is a nominal distribution, the blue ones. And let's say how to perturb into different ones. Let's think of it as there are some data perturbation around this nominal tree. So let's start from, for example, from the second stage. Let's perturb the upper one to give a larger value. Upper one to even larger value, and the bottom one to even more value. As it gives a perturbation of the distribution in the second stage, in the first stage. Now, conditionally on what you have observed, what I have done for the first stage, let's look at the condition distribution in the second stage. For example, this power sub tree gives a conditional binding distribution for the second stage and similar for the bottom. So, in the second stage, So in the second stage, let's do another perturbation. Compare it either to an upper level or perturb it to a downer level. You see that this perturbation is really stage by stage. First, you perturb the distribution in the first stage, then do the conditional distribution in the second stage, and thus preserve the information structure of those two. So they kind of have a similar structure, but with different locations. It seems a very natural idea when you come to how to perturb a scenario key in this model. To how to perturb a scenario key in this model case. But surprisingly, if you think about the vanilla optimal transport that defines, say, the Waterstein responds, it actually violates this so-called causal constraint. To think about it, let's just look at the same example. So what the Waterstein preparation does is instead of looking state by state, it's look at the sample path in Harvey. So we have four sample paths, up, up, up, down, down, up, and down. Up, up, up, down, down, up, and down, down. So let's perturb, what the worst one does is it first perturbs the first semi-path, then the second one, the third one, and the last one. So you perturb a four-semo path to another four-semal path without considering how the conditional information evolves in these processes. So namely, if you look at the filtration, for example, is a purple perturbable tree, this becomes a kind of a few A trivial future, namely, knowing the uncertainty in the first page will tell us exactly what will happen in the future. Alright, so therefore for our considerable problem, it will be interesting to look at the setup distribution defined by the colour transform. So to make it more performally, let's define this vector value transform map that maps from the nominal semi-path. From the nominal semi path to another semi-path. And it's a metric value of every component specified the perturbation at the T-station of the hertz. And the causal perturbation really just represented here because this transform map depends on the history of the nominal stress process up to a time. In comparison, if you define the waterstein perturbation, then every argument Then every argument in each of the components here would depend on in-person. And given this vector value of the transform map, that's by the non-sensitivity, which induces a family of distributions. So that's just a constraint in the perturbation. And perhaps the simplest one, just to let's say the maximum perturbation for each of the nodes from a scenario tree in a nominal case is a constant. In a nominal case, is constrained by a number theorem. And this actually leads to the so-called infinite causal transport distance in the future. And for those of you who are already familiar with the infinite causal transport distance, actually this is not the original form. I'm already showing you this kind of equinox reformulation for the problem. But the reason I'm doing so is simply because I think this one is much easier to interpret. Although the actual The actual integer causal transport distance set actually contains more disturbance than what this set describes. I'm going to come back to the more general setup when we talk about extension of this curve. So given this setup, we can see that we are essentially maximizing over the distributions now because maximizing over the set of causal transform maps. And it is non-stativity enabled. Now, non-adductivity enables us to write down the dynamic programming formulation in an efficient way because you can essentially write down this summation as a sum of conditions expectation thanks to the non-intensificity. And it immediately gives us the dynamic programming formulation for our problem. So I'm going to present the dynamic programming formulation for two problems. One, the positive elevation that gives us the worst case of cost for fixed cost. For fixed cost. And second one, the process learning involves automation market flows. The reason I present them separately is because they are going to rely on different set of assumptions. So, for a policy evaluation, as we said earlier, because of the non-anticipativity of the setup, we can write down the evaluation of the worst case cost in a recursive form in a rather straightforward way. So, here, let me interpret this for you. This value function. This value function, similar to the non-robot case, it depends on the current state NST, as well as the history from the deployed environment, represented by the orange color. It depends on the history because our control ut, our right-hand side, is a function of the deployment environment. So inside this expectation, we have this worst case problem. Evaluates the worst case cost over only with the Over only with the uncertainty at time t. Note that the history from deep environment is p. So we only optimize our current stage only. And the first one is the current cost, the second one is magnetic. The center that we take to the stream on with the kind of a neighborhood around this omega hefty represents a sample from historical. From historical data using the nominal environment. And it also gives the history of the nominal environment. So therefore, when we look at this value function, it not only depends on the sample path from the deployed environment, but also depends on the sample path in the non-environment. It helps us to compare the in-sample and out-sample tree. That really means the sample. And L sample trajectory means the trajectory from the nominal ones and the trajectory from the point ones. If you're familiar with the dynamic consensus literature, it has a similar form, but the important difference here is that this dynamic recursion involves the trajectory from both environments. And it is this difference that enables us to compare our distributions with different spots. In the standard literature, for example, those one involved. Literature, for example, those ones involved in relative entropy. Actually, it only involves the dynamic recursion, only involves blank coefficient because generally they are working on the same probability space, but just to change the measure and a weight of it. Who is the center data I mean over time? The center, you mean the blue ones? I mean it's a given setup where you have a nominal scenario and you can based on orange colour sampling or different classrooms. Sampling of the classroom. You get this only once. But this is given, it's more a data-driven approach where you say you have some data. But I mean, the duality falls in a more general way. So it does not have to be a finite scenario, it could be general problems. So for example, you have a boring motion, and this theorem also applies, but computational-wise, it could be more completely difficult to solve. Uh any other questions? Any other questions? Alright, so this gives us a DP formulation for the process evaluation. So now let's talk about the obligation over the controls. A natural thought would be just add an infimum over a control L time D for this problem, right? And that gives this problem. But unfortunately, this problem does not actually give us an implement policy simply because. Policy simply because the optimizer of this problem also depends on the history from the nominal environment. But the policy itself is only defined on the deployed environment. So that's why it's not implementable. So to remove this dependence on the history from the nominal environment, let's just say, assume the nominal tree is a stigma. So actually, for literature, some For literature on robust control based on reticle entropy, this is kind of a common assumption. So, with that, we can remove the dependence on the historical sample path from the normal description, but in the same time, by induction, the dependence on the deployed environment doesn't happen. Let's try and co-design with the nominal case where the function only depends on current state. The band of functions only depends on the current state if you have a statewide independence. But so let's say that although the ball centered at the statewide independent scenario tree contains non-statewide independent trees, but if you look at the worst case, it only involves the statewide independent case tree. And another way to interpret this right-hand side is you can think of it as the worst case cost over a one-period workstone model. And the center of this ball is just the Of all is just the distribution at the T stage, when we assume serial independence, it does not handle history, but just if you can partition products of the distribution and F stage. And this represents kind of infinite work symbol, but only for the one period. So let me make a small remark on the recateginarity, which kind of Which are kind of an important concept of building economics and finance. If you only look at the still components, the previous result does not depend on specific structure of the problem. And actually, for any random sequence, the volume holds. So on the left, we have this infinite causal transport distance that would give us evaluates the worst case loss. And on the right, Birth case loss, and on the right we decompose that for supreme output expectations or a bunch of independencies. And in the literature, this literature is from a stochastic programming literature where people define the rectangular in this way. Just like if you have this form, it's a rectangular with respect to a family of distributions and with family of house functions. But in another context, people define the record analytic in different ways, and I'm happy to discuss further. And I'm happy to discuss further if you have any, because I may miss some important literature in the economic format finance. Basically, in order to make it work, the difference between this one and standard ones is that you have to consider two sets of equations, standard one. Right, let me make a comparison to the red entropy set as well. So the red entropy set constraints like The red is the entropy set constraint, the length page ratio, well the cold transport constraint, the transport distance. So essentially, in the worst case, one particle weight is the other particle location, while fixing the weight. So therefore, in the worst case, the support of the problem will be different. In particular, in a colour transport, very likely the support will be different. That white helps to hedge against the unseen sample happens. So, the key to prove this dynamic programming recursion, Dynamic programming and recursion, in our case, we rely on non-attestability, so that I write out the energy transpections in summation form. But in the red entropy case, it relies on the chain rule decomposition of reduced entropy. Basically, I represent the red-free entropy between two successful processes as a sum of the red field entropy and optimization. But in any case, the decomposition of the probability distribution and probability distance cell helps to rewrite the dynamic problem. Helps to rewrite the dynamic code. Right, so in the rest of the couple minutes, let me discuss some extensions for this framework. This is actually the original definition for the caudal transport distance, and not only applied for infinite order, but also for any p from 1 to unit. So, whether that this is the original formulation for Waterstein, but with additional constraints. But the idea is actually. But the idea is actually similar. So you can think of this gamma as a concentration lamb between two regions. And what this conditional independence requires is that given the history of the nominal path, then what you're going to perturb for the t-stage variable is going to be independent to the future of the nominal stress process. So if you do not allow splitting of hard mass, namely you're going to put one node with another node, To put one node to another node, then reduce to our case variable. Essentially, we don't care about the module. And let's look at this p-cause of Washarstein, sorry, P causal transport constraint set, which leads us to the corresponding robust control problem. And Kuhn representation for this problem, that kind of connection to what I have done earlier, is that you can think of this problem as also a mechanism over Marge Max. Over March maps. But instead of looking at the deterministic map, let's allow some randomness splitting, which just represented by this random speed. So namely, in that case, by introducing the random seed, you can allow some random splitting for each other. So that every node could be split into another distribution. But other than that, the problem would be quite similar. So in case of p equals infinite, whether you allow randomized Infinite, whether you allow randomization on the particular mapping or not does not affect the problem. But with final RFP, we have this insurance and combustion of the problem and equivalent to the ratio. For final RP, it's often convenient to work with the soft penalty algorithm form, sometimes called the multiplier problem economics or variational parameters. If you have an additional penalty here. Penalty here and the penal coefficient remember. So we no longer have the radius but have the lambda here. But they kind of sort of have the tuna penalty in most cases. In this case, we can write down the dynamic program recommendation for the robust policy validation and policy learning as well. They're mostly the same under the same set of assumptions. The only difference is they are going to change the higher perturbation within a small The higher perturbation within a small neighborhood of the nominal node has a soft penetration. And if you really want to work on this higher content problem, you can just minimize overlap multiply in the very end plus the lambda time space that you can do a formulation as well. But that's for a finite order peak. And the other extension would be, say, let's call a considerable infinite random problem. So in this case, let's assume the nominal stochastic process. Assume the nominal success process of the id and each time follows from the distribution p has grown. We can write down and assume CF their stationary does not depend on time and the flexible set also does not depend on time. And we can write down the corresponding dynamical programming recursions which now actually can show us a stationary optimal robust policy that can solve by six points average. Actually in a literature evo sometimes people actually start from the problem. Sometimes people actually start from this problem right away and develop an acceleration. So that's a kind of quote and provide a different interpretation of what we have been working on in computational studies. Alright, let me summarize the takeaway. So we look at the mean mass robust control with COVID transport. The idea is we are going to look at some non-attestable configuration of the nominal scenario tree. And this enable us to write an introvert in the process. us to write down the data and data proposition. For policy evaluation in the whole very general setting, but for process learning we have to impose data by dependence to make sense work. In terms of interpretation, you can think of the original problem as a multi-period risk measure that measures the risk of the entire semal path for with a certain loss. And the right hand side simply means that you can decompose them as a composite dynamics measured defined by a single pair of numbers. Define file single pairing or something. With that, I thank you very much for the talk. I'm wondering if there's other types of sources of certainty that can be bedded into the framework. For example, counting rationality of the decision maker? Yes, I guess people kind of need different ways to model their. Kind of a different way to model the boundary rationality. Maybe distance-based one is one of them. I think there are other ones maybe can be incorporated, but I guess the first thing we need to just know is whether this distance can or other choices can be had this natural decomposable form so that we can add otherwise in terms of accessibility. The problem would be true. Yes, to the point, to the point. So there are several responses to this one. First one, if you work with a bicausal transfer distance, the first thing is the distance, the authentic set itself does not come up. So therefore, you won't rely on the So therefore you won't rely on arguments to prove the recursion, it fails. However, if you assume certain continuity on the space, on assembled space, namely there's no isolated point, then either having causal transfer distance or bicausal, that will lead to the same equivalent reformation. The idea is that even for a map that's only one side causal, you can perturb a little bit to make it. You can perturb a little bit to make it in fine constant. And this perturbation can be arbitrarily small, so that the two problems will be the same. But this only applies to the case where you have a space that has no isolated points. Because for the space with isolated points, you can't have, you don't have an opportunity to perturb with a small neighborhood. So in this case it means that you your state speed disobey is not working? Uh in this case I mean the results hold for general, not methodologist, but this is basically so far. But I I mean I uh but I I mean uh the the R D that that is a continuous basic essence or any our manifold. I wanted to ask about the policies. So is it really that bad? So as I if I understood correctly, where you solve this and it depends on P and P depends on Q in some sense, right? Depends on and not just on uh Q measure. That's why you introduced this stage-wise independence? Yes. So the reason why we have state-wise independence really wants to have this dynamical recursion. Because without that, for example, here, we cannot exchange. So the minimization for policy really. So the minimization of our policy really at very outside at the beginning, but this involves the interchange of the minimization of a policy at every stage and the expectation, the potential expectation at every stage. And we have to have statewide independence, and in general, we have to have it in order to make the exchange pole. Otherwise, the part itself, if you look at this one, if you minimize it over the control, it will depend on the nominal trajectory, which is Which cannot hold because it can only be a function of the deployed environment. You're going to implement a pause in the deployed environment, which is a function over only the oriented part, the oriented stream, but not the good stream. Yeah, I'm still a bit confused. I mean, you could just make it a function of two variables. No, you're going to d you are going to implement your policy in this environment. I mean, you choose a function of omega omega only. Omega only. It cannot be a function of arbitrary reference distribution that is available to us. I mean, you're making it robust, right? So anyway, you have to know what the center of your goal is. But in reality, you can only observe, when you actually implement it, you only observe one trajectory. If you have two trajectories in the nominal history, then is free, that will give you two different powers. But you can implement only one number, which one should be choosing. As you go along, you are creating... So imagine a static problem. In the static problem, you have P0. By static, you mean the original Max problem? Oh, okay. So at the beginning, right, you have only one environment, which is P0. Okay. Okay. And then you extract the worst case distribution from there. Now you have W and W bar. Right? And so you know you have a shadow environment that is kind of following you all the time. You are solving this problem, this dynamic problem itself. But if you really want to solve the original problem, that the the game formulation, min max formulation, that is that that will give you a That that that that will give you a a suitable policy or implementable policy. I mean uh then a problem here, I mean, you can implement it, but it's not no longer equivalent to our original problem. Thank you very much again. Speaker Roundtable