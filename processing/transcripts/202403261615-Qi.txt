Okay, so the last speaker for this session today is Alice Ki, and she's going to talk about targeted policy learning. My name is Alice Chi, and I'm a fourth-year e-call PhD student at the University of Washington, and I'm supervised by Professor Ying Tifen. So today I will present one of my ongoing projects. I will present one of my ongoing projects, Target Policy Learning, and this is joint work with Professor Fen and also my co-author Coach, who is also here. So here are a snapshot of this work. Basically, the problem we're considering is the inability of utilitarian optimal policy in addressing issues like social inequality. So the idea here is that utilitarian policies are only maximized. That utilitarian policies are only maximizing the population average outcome instead of targeting specific groups. So, a good motivating example here is that consider prescribing medication to individuals at risk of diabetes. And here, the medication assignments can be based on some observable individual characteristics. And if we only adopt a utilitarian policy, then it is optimizing the population average outcome. Population average outcome, for example, minimizing the average blood sugar level of the individuals. And we will expect that such a rule or policy can be driven by the majority in the sample. So a problem can arise if, for example, the majority consists of low-risk individuals who can benefit from the medication, whereas there can also be high-risk individuals who can be hurt by the medication. But since we adopt a utilitarian policy, We adopt a utilitarian policy, it can fail to extract or pick out the characteristics of those high-risk individuals. So it's possible that those high-risk individuals can end up in even worse situations. But that's the motivation of our paper. So instead, we are taking a group agnostic point of view, meaning that we're not explicitly defining subgroups based on characteristics. And that can be helpful because in many empirical settings, Because, in many empirical settings, we don't have demographic labels available for use. And our paper is also risk-averse, meaning that we aim to learn an upfield policy that targets the individuals on the lower tail of the post-treatment outcome distribution. So we're only looking at the individuals who are worst affected after implementation of the policy. And the idea behind our paper really stems from. Our paper really stems from the conditional weather risk, which is a popular risk measure in risk management. So we'll adopt the conditional weather risk as our welfare function instead of the average population outcome. In our paper, we also study balance on the welfare regret, which is measured as the difference between the welfare loss or welfare difference between our optimal policy and the best policy in a given policy class. Policy class. And we also study for inference for the optimal welfare so we can conduct tests. So, as an inferred building block, we can first introduce the standard policy learning framework, which is the protocol welfare maximization approach in the paper by Kitagawa Tetanov. So, this studies the utilitarian optimal policy. So, here we assume that the underlying population from which We assume that the underlying population from which the sample is drawn, like basically the two potential outcomes, the treatment assignment and the individual characteristics, is drawn from this population distribution p. And then the welfare function is simply the average outcome after this binary premit rule or policy pi. So if an individual gets treated, then pi will be one, then we take the treated potential outcome. And conversely, if pi is zero, then this If pi is zero, then this individual is not treated, so we take the untreated potential outcome. So if we prefer larger wise, then the utilitarian optimum policy just maximizes this average population outcome. And we can also rearrange this welfare function into this form by defining the conditional average treatment effect as the difference between the two potential outcomes given x. And then we consider the untreated potential outcome. Consider the untreated potential outcome Y0 as the baseline. So the intuition here is that the first best policy is to treat the individuals who can benefit from the treatment, like those who have positive COT. And of course, as I said, the empirical welfare maximization approach by this paper does not consider any distributional impacts. So our paper. Impacts. So our paper studies be risk averse by looking at the distribution, post-treatment outcome distribution, and we only focus on the lower tail. So first we can express out the post-treatment outcome distribution by averaging or by integrating the conditional CDF of y given x in view swept policy pi over the marginal distribution of x. X. And the alpha conditional welfare risk welfare is simply the expected outcome among the worst affected size alpha subpopulation. So it's going to be a conditional expectation of the post-treatment outcome, yi pi, given that this outcome is less than the alpha quantile in the post-treatment outcome distribution. Yeah, so this quantile also has a name, which is the value at risk. Has a name which is the value at risk, and it is also a popular risk measure in the risk management literature. But we note in the paper that potential value at risk is more popular because it is a coherent risk meter, but valid risk is not. So here's a simple visualization. So if we assume that the outcome after treatment follows a standard normal distribution, then we're only looking at this lower tail of the outcome. At this lower tail of the outcome distribution, and we are maximizing this conditional mean of these outcomes. So, here's our optimization function. So, this capital pi is a pre-specified class of policies. For example, linear policies were policy trees. And we're just selecting a policy in this class to maximize the conditional value risk. And here, the choice of alpha is problem specific, so it can depend on. Specific. So it depends on the policymaker, like how much we want to target the worst affected. So if alpha is externally small, then we're getting close to the essential equipment of the outcome distribution. And it also corresponds to the Raussian welfare in the welfare literature. And if we take alpha to be large, like close to one, then this quantile will just be the mass. Quantile will just be the maximum of the outcome distribution. So, this whole thing will just be the average of what? So, if alpha is equal to 1, then we are just going back to the standard empirical welfare maximization problem. So, we can interpolate between the Rosen welfare and empirical welfare maximization by varying alpha. And here's an alternative interpretation of our conditional weather risk welfare using the distribution. Using the distributionally robust idea. So, here we can define an uncertainty set centered at the observed distribution, observed post-outcome, post-treatment outcome distribution. So, this consists of all potential, like policy, potential distribution in Q that has distance upper bounded by log 1 over alpha. This distance function is defined as the essential supreme function. As the essential supremum of log PQ over PF. And here, the conditional risk welfare is equivalent to the worst case welfare under the perturbed distribution of the study population. So, this is equivalent to learning a distributionally robust policy that maximizes the average welfare under a worst-paced perturbation of the study population. Perturbation of the study population. So we can also think of this as a distributionally robust version of empirical welfare maximization. So that's also an implication of the conditional value of risk. And then in terms of estimation, we exploit the dual form of conditional value risk. So instead of using the conditional expectation, we can write it in terms of We can write it in terms of an optimization problem. And also, this dual optimum is attained at the value risk of this outcome variable, which is the alpha quantile of this post-stream outcome distribution. So this form is convenient because we can directly solve this optimization problem instead of looking at the potential expectation. And also, And also, we can redefine nuisance parameters like the outcome parameter and the conditional average treatment effect. And I call them like pseudo outcome and pseudo-CATE because they will depend on this quantile data. So, identification is similar to estimating the average treatment effect. So, we can Effect. So we can assume selection on observables, namely unconfoundedness and overlap. Then in this case, we have the welfare function will be identified in this form. So this looks pretty similar to the estimate of the average treatment effect. So we just have the nuisance parameters Î¼ and we also have L. How? So we can also, similar to ATE estimation, we can design prosphited nuisance estimators and also the W request modification terms to estimate this welfare. So in the end, what we do is we maximize this simple analog of welfare over the policy and also this. This eta. And we show that asymptotic normality is achieved, and the asymptotic variance will just be the variance of those double-robust scores. How much time I have left? So, yeah, I'm open to questions. 